# Visual Instruction Tuning with Polite Flamingo

## What is the central research question or hypothesis that this paper addresses?

The key research question addressed in this paper is how to effectively use a diverse collection of annotated vision-language datasets to improve visual understanding capabilities of large language models, while preventing the raw annotations from negatively impacting the response style and politeness of the model. To address this, the paper proposes a new method to rewrite the raw annotations from vision-language datasets into more natural and polite responses. It trains a "Polite Flamingo" model as a multi-modal response rewriter, and applies it to convert a large collection of vision-language datasets into high-quality instructional data. This rewritten dataset "PF-1M" is then used to train the final multi-modal model "Clever Flamingo" via a novel U-shaped multi-stage tuning approach.The central hypothesis is that using the proposed annotation rewriting and multi-stage tuning methodology will allow the model to leverage diverse vision-language data to improve visual understanding, while avoiding the "multi-modal alignment tax" that typically degrades response quality when using raw annotations directly. Experiments verify improved performance on vision-language tasks along with higher human preference ratings.In summary, the key research question is how to utilize the richness of vision-language datasets for visual instruction tuning, while preventing the model from learning undesired annotation styles that reduce response politeness - and the proposed Polite Flamingo rewriting approach provides a solution.
