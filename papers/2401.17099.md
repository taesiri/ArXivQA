# [MT-Ranker: Reference-free machine translation evaluation by inter-system   ranking](https://arxiv.org/abs/2401.17099)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Traditional machine translation (MT) evaluation metrics treat the task as a regression problem to predict a quality score. This has limitations in terms of interpretability of scores and reliance on reference translations. 
- In practice, we often care more about relative comparisons between MT systems rather than absolute scores. Also, reference-free evaluation is important for real-world usage.
- These practical considerations have not been jointly explored in prior work.

Proposed Solution:
- The paper formulates reference-free MT evaluation as a pairwise ranking problem. 
- Given a source sentence and two candidate translations, the proposed model predicts which translation is better.
- This simplifies the task and eliminates the need for reference translations.

Model:
- Uses a multilingual T5 encoder to attend over the concatenated input text containing source sentence and two translations. 
- Adds a mean pooling layer and binary logistic regression head.
- Trained using three stages:
   1) Indirect supervision from cross-lingual NLI
   2) Discriminate between human/machine translations
   3) Weak supervision from synthetic better/worse translation pairs

Main Contributions:
- First work to model reference-free MT evaluation as pairwise ranking
- Achieves state-of-the-art correlation with human judgments on WMT and MQM benchmarks without using any human annotations
- Eliminates reliance on reference translations and quality score annotations
- Shows improved practical utility over traditional regression-based evaluation

In summary, the paper presents a new problem formulation and modeling approach for reference-free MT evaluation that demonstrates excellent real-world applicability and performance. The pairwise ranking design is intuitive and the training methodology cleverly avoids the need for human judgments.


## Summarize the paper in one sentence.

 Here is a one-sentence summary of the paper:

The paper proposes a new reference-free machine translation evaluation approach that formulates the task as a pairwise ranking problem, comparing two candidate translations to predict which one has better quality, and shows this approach can achieve state-of-the-art correlation with human judgments without using any human-annotated training data.


## What is the main contribution of this paper?

 The main contribution of this paper is proposing a new formulation of reference-free machine translation evaluation as a pairwise ranking problem. Specifically, given a source sentence and a pair of candidate translations, the proposed system called MT-Ranker predicts which translation is better, instead of assigning quality scores to individual translations.

The key benefits highlighted in adopting this pairwise ranking approach include:

(1) It simplifies the evaluation task compared to traditional regression-based quality scoring. 

(2) It enables reference-free evaluation, which is more practical since reference translations are often unavailable. 

(3) It relies less on high-quality human annotations, instead leveraging synthetic training data more easily.

The paper shows MT-Ranker achieving state-of-the-art correlation with human judgments on several benchmarks without using any human-annotated training data. By removing dependencies on reference translations and human judgments for training, the proposed formulation demonstrates increased practical applicability.


## What are the keywords or key terms associated with this paper?

 Based on my review of the paper, some of the key terms and keywords associated with it are:

- Machine translation evaluation
- Pairwise ranking
- Reference-free evaluation
- Direct assessment
- Synthetic data generation
- Multilingual pretraining
- State-of-the-art performance
- WMT benchmarks
- Human judgments
- Kendall's tau correlation
- BLEU
- COMET
- BERTScore
- Multidimensional Quality Metric (MQM)

The paper proposes a new approach of formulating machine translation evaluation as a pairwise ranking problem in a reference-free setting. It uses indirect supervision from cross-lingual natural language inference data and synthetic data generation to train the models. The proposed approach, called MT-Ranker, achieves state-of-the-art performance on several benchmarks like WMT DARR and MQM datasets in correlating with human judgments, without using any human-annotated data. Some of the other key ideas explored are multilingual pretraining, scaling model sizes, analyzing model limitations and comparison with baseline methods.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in this paper:

1. The paper proposes a new formulation of reference-free machine translation evaluation as a pairwise ranking problem. How does framing the problem as pairwise ranking instead of regression simplify the task? What are the advantages of simplification?

2. The paper uses indirect supervision from cross-lingual natural language inference in the first stage of training. Why is this a sensible pretraining task? How do entailment and contradiction translate to the concept of better and worse translations?

3. The second stage of training involves discriminating between human and machine translations. Why is the assumption that human translations are better than machine translations a reasonable one for this task? When might this assumption fail?

4. The paper uses simple heuristics like word dropping and replacement to generate synthetic training data. What is the intuition behind why artificially perturbed translations are likely to be worse? How could the perturbations be made more sophisticated? 

5. The paper demonstrates state-of-the-art performance without using any human annotations. How much further could performance be improved by incorporating human judgements? What are the challenges in collecting good quality human judgements?

6. The paper shows that the system generalizes well to unseen language pairs. Why is generalization capability important for practical applications? What factors affect generalization - model architecture, training data diversity, etc.?

7. The system struggles with phenomena like untranslated words. Why are these particularly challenging for reference-free evaluation? How can the system be improved to handle these better?

8. The pairwise decisions made by the system have low inconsistency. Why is having no contradictions necessary when using the predictions for system level evaluation? How does providing system level scores eliminate contradictions?

9. How suitable is the proposed segmentation ranking method for system level evaluation? What are other methods to derive system scores from segment rankings? What are their pros and cons?

10. The paper benchmarks extensively on WMT and MQM datasets covering translation from English to multiple languages. How challenging would it be to apply the method for translation between language pairs that do not involve English?
