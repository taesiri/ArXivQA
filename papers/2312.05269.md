# [LifelongMemory: Leveraging LLMs for Answering Queries in Egocentric   Videos](https://arxiv.org/abs/2312.05269)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper "LifelongMemory: Leveraging LLMs for Answering Queries in Egocentric Videos":

Problem:
The paper focuses on the natural language queries (NLQ) task in egocentric videos, which involves localizing a temporal window in a long video that answers a query posed in natural language. This task has applications in building personalized AI assistants but faces challenges due to long and information-rich egocentric videos. Prior methods rely on end-to-end training on sparsely annotated datasets and struggle to capture long-range dependencies. 

Proposed Solution:
The paper proposes LifelongMemory, a novel framework that utilizes multiple pre-trained models - specifically multimodal large language models (MLLMs) - to answer natural language queries from egocentric videos in a zero-shot setting without training on annotated data. 

It first uses a pre-trained captioning model to generate detailed narratives for the video, condensing the lengthy footage into key textual descriptions. These narratives are fed to a frozen large language model (LLM) which predicts coarse time intervals that potentially contain the answer. Finally, a pre-trained natural language query (NLQ) model refines the LLM's predictions into fine-grained locations.

Main Contributions:
- Proposes a new framework for the egocentric video NLQ task combining pre-trained MLLMs in an effective pipeline without end-to-end training
- Achieves competitive performance to supervised methods on the Ego4D benchmark
- Provides design analysis and insights on key components like captioning, prompting LLMs, confidence calibration etc.
- Demonstrates the potential of leveraging MLLMs for complex vision-language tasks through comprehensible reasoning via natural language

The paper delivers an important proof-of-concept for using pre-trained models in this challenging domain. The framework is flexible to incorporate better captioning and reasoning models in the future.
