# [Joint Video Multi-Frame Interpolation and Deblurring under Unknown   Exposure Time](https://arxiv.org/abs/2303.15043)

## What is the central research question or hypothesis that this paper addresses?

 The central research question this paper addresses is how to perform joint video multi-frame interpolation and deblurring under unknown exposure time. 

Specifically, the paper aims to tackle the challenging task of reconstructing a high framerate sharp video from a low framerate blurred video where the exposure time is unknown and varies across frames. The unknown and varying exposure time makes this problem very difficult, as traditional methods assume a fixed known exposure time.

The key hypothesis is that by learning an exposure-aware feature representation and using it to adapt computations in the motion analyzer and reconstruction network, the model can handle varying levels of blur and motion better. The exposure-aware features allow the model to adapt to different exposure settings and motion patterns.

In summary, this paper proposes a new method called VIDUE to address the problem of joint video interpolation and deblurring without knowing the true underlying exposure time, which is more realistic for real-world videos. The main novelty is in learning to adapt the model computations to estimated exposure features.


## What is the main contribution of this paper?

 The main contributions of this paper are:

1. It proposes VIDUE, a unified framework for joint video multi-frame interpolation and deblurring under unknown exposure time. Previous works assume known/fixed exposure time, which is not realistic. 

2. It extracts an exposure-aware feature representation from the input video using a variant of supervised contrastive learning. This allows adapting computations in later stages to different exposure conditions.

3. It designs a motion analyzer with two U-Nets to analyze intra-motion (within frames) and inter-motion (between frames) in an exposure-aware manner. 

4. It develops a video reconstruction network that enables exposure-adaptive convolution and progressive motion refinement for high-quality frame interpolation and deblurring.

5. Extensive experiments show VIDUE outperforms state-of-the-art methods on joint ×8 and even ×16 interpolation and deblurring on both synthetic and real datasets. It also runs faster than competing methods.

In summary, the key innovation is formulating and solving the more realistic problem of joint video interpolation and deblurring without assuming known exposure time. This is achieved via an exposure-aware and motion-aware computational framework.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence TL;DR summary of the paper:

The paper proposes a method called VIDUE for joint video frame interpolation and deblurring under unknown exposure time, which learns an exposure-aware feature representation to adapt computations in intra- and inter-motion analysis and video reconstruction for improved performance.


## How does this paper compare to other research in the same field?

 Here are a few key ways this paper compares to other related research:

- This paper addresses the joint problem of video frame interpolation and deblurring under unknown exposure time. Most prior work tackles these as separate problems or assumes known/fixed exposure time. Jointly addressing interpolation and deblurring under unknown exposure is more challenging but also more realistic.

- The proposed method adapts its computations to learned exposure-aware and motion-aware representations. Other methods typically don't account for differences in exposure or motion patterns. This adaptive computation is a notable contribution.

- Experiments demonstrate state-of-the-art performance on challenging benchmarks for joint 8x interpolation and deblurring. The method also shows promising results on 16x interpolation, significantly outperforming other techniques. 

- The approach does not rely on any auxiliary data (e.g. events from specialized cameras). Some recent methods leverage such extra data sources, but the proposed technique works with standard consumer video input.

- The implementation is end-to-end deep learning-based. Many previous deblurring techniques use hand-crafted priors or shallow learning. Recent learning-based methods tend to address interpolation and deblurring separately.

Overall, this paper pushes forward the state-of-the-art in joint video interpolation and deblurring by formulating a more realistic problem setup and developing an adaptive deep network to address it effectively. The experiments verify improved performance over current approaches.


## What future research directions do the authors suggest?

 Based on my reading of the paper, some of the main future research directions suggested by the authors include:

- Further reducing the computational complexity of VIDUE while retaining (or improving) the performance. The paper notes that VIDUE involves relatively heavy computation compared to some other methods. Reducing computation cost while maintaining performance would be useful.

- Optimizing VIDUE using perceptual quality metrics with emphasis on temporal coherence. The current VIDUE method is optimized on standard metrics like PSNR. Using perceptual metrics and focusing more on temporal coherence could further improve visual quality.

- Extending VIDUE to handle videos where the exposure time changes more abruptly over short durations. The current method assumes the exposure time is roughly constant over the input frames. Handling larger exposure time variations across input frames is noted as a limitation. 

- Applying the core ideas of VIDUE, like exposure-aware feature learning and adaptive computation, to related video processing tasks beyond joint interpolation and deblurring. The exposure-conditioned computation approach could potentially benefit other tasks.

- Further improving the performance on very large interpolation factors like x16. Despite substantially outperforming other methods on x16 interpolation, there is still room to improve absolute performance.

In summary, the main future directions are around computational efficiency, perceptual optimization, handling dynamic exposure conditions, generalization to other tasks, and continuing to push the boundaries of extreme interpolation factors. The overall approach shows promise for joint video processing tasks involving unknown image degradation conditions.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the paper:

This paper proposes a method called VIDUE for joint video multi-frame interpolation and deblurring under unknown exposure time. The key idea is to extract an exposure-aware feature representation from the input video using contrastive learning. This representation allows the method to adapt to different exposure settings. Two U-Nets are then trained to analyze intra-motion within frames and inter-motion between frames, with their computations adapted to the exposure-aware features. Finally, a video reconstruction network progressively refines motion and performs exposure-adaptive convolution to output a high framerate sharp video. Experiments on synthetic and real datasets show VIDUE achieves significant gains over state-of-the-art methods on joint 8x interpolation and deblurring. It also demonstrates strong performance on the very challenging 16x interpolation task. The main conclusions are that explicitly modeling unknown exposure time and adapting computations accordingly leads to improved video interpolation and deblurring.
