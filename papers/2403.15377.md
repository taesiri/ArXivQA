# [InternVideo2: Scaling Video Foundation Models for Multimodal Video   Understanding](https://arxiv.org/abs/2403.15377)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Learning effective video representations is critical for advancing video understanding and enabling diverse applications like video searching, game control, robot learning, etc.
- Recent advancements in large language models (LLMs) and their multimodal versions (MLLMs) provide new opportunities to enhance video representations by embedding them in learned world models. 

Method:
- The paper proposes InternVideo2, a new video foundation model (ViFM) trained with a progressive learning scheme combining:
  1) Masked video token reconstruction 
  2) Video-audio-text contrastive learning
  3) Next token prediction using videos

- It emphasizes spatiotemporal consistency in the training data by:
  - Semantically segmenting videos into clips
  - Generating independent video, audio and speech captions and fusing them to recalibrate video-text alignment

- The model is scaled up in size and trained on 412M multimodal video entries.

Contributions:
- InternVideo2 achieves SOTA results on 65 out of 74 evaluated video/audio benchmarks, covering action recognition, video-text tasks, video dialogues etc.

- It demonstrates superior performance on video captioning, QA, and long video understanding, highlighting its ability to reason about and comprehend temporal context.

- The improved training data and progressive learning scheme effectively enhance both video perception and semantics, leading to a qualified video encoder for advancing video understanding.

In summary, the paper presents a strong video foundation model InternVideo2 that leverages masked reconstruction, contrastive learning and next token prediction in a multi-stage training process. By improving video-text alignment and reasoning capacity, it significantly advances the state-of-the-art in diverse video tasks.
