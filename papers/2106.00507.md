# [Towards Quantifiable Dialogue Coherence Evaluation](https://arxiv.org/abs/2106.00507)

## What is the central research question or hypothesis that this paper addresses?

 The central research question this paper aims to address is how to train an automatic dialogue coherence evaluation metric to produce quantifiable coherence scores that align with actual human ratings. Specifically, the paper points out two main limitations of existing metrics:1) They simplify the evaluation task into a binary classification of coherent vs incoherent dialogues, whereas human ratings are on a multi-level Likert scale. 2) Their training objectives deviate from actual human rating standards due to the absence of human-annotated scores.To address these limitations, the paper proposes a new training framework called QuantiDCE that includes:1) Multi-Level Ranking (MLR) pre-training to enable learning coarse judgments of dialogue coherence degrees.2) Knowledge Distillation (KD) fine-tuning to align the metric with actual human rating standards using minimal annotated data. The key hypothesis is that by adopting this two-stage training approach of MLR pre-training and KD fine-tuning, the resulting metric will produce quantifiable coherence scores that better correlate with true human judgments, compared to existing metrics.


## What is the main contribution of this paper?

 The main contribution of this paper is proposing a novel training framework called QuantiDCE for learning quantifiable dialogue coherence metrics. Specifically, the key contributions are:1. Proposing a quantifiable training framework with two stages - MLR pre-training and KD fine-tuning - to train dialogue coherence metrics that can quantify multiple levels of coherence like humans, instead of just classifying coherent/incoherent binary levels. 2. A new multi-level ranking (MLR) pre-training loss is proposed to enable models to learn coarse judgements of coherence degrees. This allows distinguishing context-response pairs of different coherence levels.3. A knowledge distillation (KD) fine-tuning strategy is used to align model predictions with actual human ratings, using just a small amount of human-annotated scores. This helps retain knowledge from pre-training and avoid overfitting the scarce human-annotated data.4. Experiments show QuantiDCE significantly outperforms previous state-of-the-art metrics in correlating with human judgements of coherence.In summary, the key novelty is formulating dialogue coherence evaluation as a quantifiable problem with multiple levels like human ratings, instead of binary classification. The proposed QuantiDCE framework with MLR pre-training and KD fine-tuning is able to effectively train quantifiable coherence metrics.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the key points from the paper:The paper proposes QuantiDCE, a novel two-stage framework to train quantifiable dialogue coherence metrics, including multi-level ranking pre-training to learn coarse coherence judgments and knowledge distillation fine-tuning to align scores with human ratings using minimal annotated data.


## How does this paper compare to other research in the same field?

 Here are a few key ways this paper compares to other research in dialogue coherence evaluation:- It addresses the "quantifiable" problem - existing metrics simplify coherence evaluation as a binary task, while humans give more nuanced Likert-scale ratings. This paper proposes a framework to train metrics that can quantify multiple levels of coherence.- It uses a two-stage training approach with pre-training and fine-tuning. The pre-training stage uses a new multi-level ranking (MLR) loss to learn coarse judgements of coherence. The fine-tuning stage aligns the model with human ratings using only a small amount of annotated data, avoiding overfitting. - The proposed QuantiDCE framework achieves significantly higher correlation with human judgements compared to prior state-of-the-art methods like GRADE. This demonstrates the benefits of modeling coherence evaluation as a quantifiable, multi-level task.- Most prior work has focused on unsupervised training of coherence metrics. A key contribution here is incorporating human annotations to fine-tune the model to fit human standards. The knowledge distillation approach helps avoid overfitting the small annotated dataset.- The idea of using multi-level ranking losses has been explored for some tasks like image classification, but this paper is one of the first to apply it to dialogue evaluation and propose modifications tailored to this task.Overall, this work makes notable progress in bridging the gap between automatic dialogue coherence metrics and true human judgements. The quantifiable training framework and use of human annotations differentiates it from prior unsupervised approaches. The gains over previous state-of-the-art demonstrate the promise of methods that better model the nuances of human coherence assessments.


## What future research directions do the authors suggest?

 Based on my reading of the paper, here are some of the future research directions suggested by the authors:- Investigate more efficient ways to obtain multi-level training data. The authors used a dataset constructed from DailyDialog by adding different quality levels of responses, but suggest it would be useful to explore more efficient methods to get multi-level training data. - Extend the multi-level training approach to general evaluation for natural language generation tasks beyond just dialogue. The paper focuses on dialogue coherence, but the idea of using multi-level training objectives could be beneficial for other NLG evaluation tasks as well.- Experiment with using more than 3 coherence levels during training. The authors used 3 levels in this work, but suggest using more levels could help the model learn a finer-grained quantification of coherence that is closer to human ratings.- Improve the dialogue evaluation benchmark. The authors point out some limitations with the existing benchmark they used for evaluation, such as only providing a single reference response. They suggest enhancing the benchmark could be useful for more robust evaluation.- Apply the proposed training framework to other dialogue tasks beyond coherence evaluation. The overall idea of using multi-level pre-training and knowledge distillation fine-tuning could be beneficial for other dialogue metrics as well.In summary, the main future directions are exploring ways to obtain more multi-level training data, extending the approach to other NLG tasks, using more coherence levels, improving evaluation benchmarks, and applying the training framework to other dialogue tasks.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the paper:This paper proposes QuantiDCE, a novel framework for training quantifiable dialogue coherence metrics that can align with actual human ratings. The framework has two stages - Multi-Level Ranking (MLR) pre-training and Knowledge Distillation (KD) fine-tuning. In MLR pre-training, a multi-level ranking loss enables the model to learn coarse judgements of dialogue coherence degrees by separating context-response pairs of different coherence levels. In KD fine-tuning, the pretrained model is further tuned on a small amount of human-annotated data, using a knowledge distillation loss to retain the pre-trained knowledge while aligning scores to human ratings. Experiments show QuantiDCE metrics achieve much higher correlation with human judgements than previous state-of-the-art metrics. The main contributions are proposing the quantifiable training framework to bridge the gap between training objectives and human ratings, and showing its effectiveness empirically.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the key points in the paper:This paper proposes QuantiDCE, a new framework for training quantifiable dialogue coherence metrics that can align with actual human ratings. The framework has two stages: Multi-Level Ranking (MLR) pre-training and Knowledge Distillation (KD) fine-tuning. In the MLR pre-training stage, a new multi-level ranking loss is proposed to enable the model to learn coarse judgments of coherence degrees by separating context-response pairs of different coherence levels. In the KD fine-tuning stage, the pretrained model is further tuned on a small set of human-annotated scores. To avoid overfitting on the limited annotated data, a knowledge distillation regularization loss is used to retain the knowledge learned during pre-training. Experiments show QuantiDCE significantly outperforms prior state-of-the-art metrics in correlating with human judgments of dialogue coherence. The MLR pre-training and KD fine-tuning are both shown to contribute to QuantiDCE's improved performance over baselines. The results demonstrate QuantiDCE can effectively train a quantifiable dialogue metric aligned with actual human ratings, overcoming limitations of prior work that simplified evaluation as a binary task and lacked human guidance. Key contributions are the new pre-training and fine-tuning methods enabling quantifiable evaluation reflecting human standards.


## Summarize the main method used in the paper in one paragraph.

 Here is a one paragraph summary of the main method used in the paper:The paper proposes QuantiDCE, a novel framework for training quantifiable dialogue coherence metrics that can align with actual human ratings. The framework has two stages - Multi-Level Ranking (MLR) pretraining and Knowledge Distillation (KD) fine-tuning. In the MLR pretraining stage, the model is trained with a multi-level ranking loss to learn coarse judgements of dialogue coherence across multiple levels. This includes a separation loss to separate features of context-response pairs at different coherence levels, a compactness loss to compact features within each level, and an ordering loss to ensure proper rank ordering of coherence levels. In the KD fine-tuning stage, the pretrained model is further finetuned on a small amount of human annotated data, using a knowledge distillation regularization loss to retain the knowledge learned during pretraining while adapting to the human ratings. This allows training a quantifiable metric aligned with actual human ratings using minimal human annotations.


## What problem or question is the paper addressing?

 The paper is addressing two key limitations of existing automatic dialogue coherence evaluation metrics:1. Existing metrics simplify the evaluation task into a binary classification of distinguishing coherent vs incoherent dialogues. However, human ratings are actually multi-level on a continuous scale (e.g. a Likert scale). The paper refers to this as the "quantifiable problem".2. Existing metrics are trained without direct supervision from human ratings, so they fail to align with actual human rating standards.To address these issues, the paper proposes a new training framework called QuantiDCE (Quantifiable Dialogue Coherence Evaluation) that aims to train a metric that can quantify multiple levels of coherence like humans and align better with actual human ratings.Specifically, QuantiDCE has two stages:1. Multi-Level Ranking (MLR) pre-training: A new MLR loss enables the model to learn coarse judgments of multiple coherence levels without human labels. 2. Knowledge Distillation (KD) fine-tuning: The pretrained model is further fine-tuned on a small amount of human-labeled data, using a KD regularization loss to retain the pre-trained knowledge and avoid overfitting to the limited labeled data.In summary, the paper proposes a new quantifiable training framework to address limitations of existing dialogue coherence metrics and better align automatic metrics with true human ratings.
