# Towards Quantifiable Dialogue Coherence Evaluation

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper aims to address is how to train an automatic dialogue coherence evaluation metric to produce quantifiable coherence scores that align with actual human ratings. Specifically, the paper points out two main limitations of existing metrics:1) They simplify the evaluation task into a binary classification of coherent vs incoherent dialogues, whereas human ratings are on a multi-level Likert scale. 2) Their training objectives deviate from actual human rating standards due to the absence of human-annotated scores.To address these limitations, the paper proposes a new training framework called QuantiDCE that includes:1) Multi-Level Ranking (MLR) pre-training to enable learning coarse judgments of dialogue coherence degrees.2) Knowledge Distillation (KD) fine-tuning to align the metric with actual human rating standards using minimal annotated data. The key hypothesis is that by adopting this two-stage training approach of MLR pre-training and KD fine-tuning, the resulting metric will produce quantifiable coherence scores that better correlate with true human judgments, compared to existing metrics.


## What is the main contribution of this paper?

The main contribution of this paper is proposing a novel training framework called QuantiDCE for learning quantifiable dialogue coherence metrics. Specifically, the key contributions are:1. Proposing a quantifiable training framework with two stages - MLR pre-training and KD fine-tuning - to train dialogue coherence metrics that can quantify multiple levels of coherence like humans, instead of just classifying coherent/incoherent binary levels. 2. A new multi-level ranking (MLR) pre-training loss is proposed to enable models to learn coarse judgements of coherence degrees. This allows distinguishing context-response pairs of different coherence levels.3. A knowledge distillation (KD) fine-tuning strategy is used to align model predictions with actual human ratings, using just a small amount of human-annotated scores. This helps retain knowledge from pre-training and avoid overfitting the scarce human-annotated data.4. Experiments show QuantiDCE significantly outperforms previous state-of-the-art metrics in correlating with human judgements of coherence.In summary, the key novelty is formulating dialogue coherence evaluation as a quantifiable problem with multiple levels like human ratings, instead of binary classification. The proposed QuantiDCE framework with MLR pre-training and KD fine-tuning is able to effectively train quantifiable coherence metrics.
