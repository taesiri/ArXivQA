# [Distilled Self-Critique of LLMs with Synthetic Data: a Bayesian   Perspective](https://arxiv.org/abs/2312.01957)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a summary paragraph of the paper:

This paper proposes a new method called distilled Self-Critique (dSC) for aligning large language models (LLMs) with human values. dSC builds on prior work framing reinforcement learning from human feedback as Bayesian inference. The key idea is to refine LLM outputs through a Markov Chain Monte Carlo sampler that alternates between critique and revision steps, using only synthetic data. The samples are then used to fine-tune the LLM to distill the refined responses. Experiments on improving safety, sentiment, and privacy control demonstrate that dSC can effectively steer LLMs at low cost. The method incorporates an explicit likelihood model based on an external reward (unlike Self-Refine), includes explicit critique and revision steps (unlike ReST), and has a distillation phase (unlike both). dSC provides a novel perspective of criticizing and revising an LLM's own generations as Bayesian inference, opening avenues for further improvements.


## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Large language models (LLMs) have shown great capabilities, but there are still concerns around their safe and ethical use. 
- Prior work has focused on aligning LLMs to human values through reinforcement learning from human feedback (RLHF) or model feedback (RLAIF).

Proposed Solution:  
- The paper proposes a new method called "distilled Self-Critique" (dSC) to align LLMs by refining their outputs. 
- dSC interprets RLAIF as a Bayesian inference process using a Gibbs sampler with synthetic data.
- It has explicit critique and revision steps where the LLM critiques its own output and provides a revised output.
- The refined outputs are then distilled back into the LLM through self-training.

Main Contributions:
- Provides a Bayesian perspective to interpret RLAIF methods.
- Introduces the dSC framework requiring only synthetic data to align LLMs through MCMC sampling and self-distillation.
- Evaluates dSC on experiments related to safety, sentiment modification and privacy control.
- Shows improved performance over baselines in avoiding harmful content, removing negative sentiment, and reducing personally identifiable information.
- Demonstrates the approach can work with both synthetic data from the model itself as well as transfer learning from another model's synthetic data.

In summary, the paper proposes a novel distilled Self-Critique approach to align LLMs that provides both a useful framework and strong empirical results across multiple alignment tasks. The method is self-contained only requiring synthetic data.
