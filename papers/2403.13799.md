# [Reverse Training to Nurse the Reversal Curse](https://arxiv.org/abs/2403.13799)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Large language models (LLMs) suffer from a "reversal curse" where they fail to generalize facts to the reverse direction. For example, if trained that "Paris is the capital of France," LLMs cannot infer that "The capital of France is Paris."
- This is a serious flaw as it shows LLMs do not properly learn relational knowledge, despite all the facts they have been trained on. The curse persists even when training on huge datasets due to Zipf's law.

Proposed Solution:
- Introduce a "reverse training" scheme that trains the LLM bidirectionally. Along with standard left-to-right training, some training examples have their word order reversed while preserving entities. 
- Four reversal schemes are proposed: token reversal, word reversal, entity-preserving, and random segment reversal. These act as an auxiliary "second language" task.
- Hypothesize this helps resolve the curse as models can leverage knowledge from reversed examples to aid forward prediction, similar to multitask learning gains.

Contributions:
- Show complete reversal curse failures on symbolic tasks and celebrity QA, whereas reverse training succeeds.
- Demonstrate strong gains from reverse training in pre-training and finetuning setups, eliminating the reversal curse.
- Analysis shows entity-preserving and random segment reversal work best. The reversal unit correlates to target "concepts" that need to be reversed.
- Reverse training even outperforms standard training on some forward prediction tasks when data-bound. Hence useful beyond the reversal curse.

In summary, the paper proposes an effective reverse training scheme to mitigate the reversal curse in LLMs. The method is simple to apply and provides gains in both reversal and standard capabilities.
