# [Context Diffusion: In-Context Aware Image Generation](https://arxiv.org/abs/2312.03584)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a detailed summary of the key points from the paper:

The paper proposes Context Diffusion, a novel diffusion framework for in-context aware image generation. It enables pre-trained diffusion models to leverage visual context examples, presented alongside a query image and optional text prompt, to control the appearance of the generated image while preserving structure from the query image. The key aspects are:

1) It separates the encoding of visual context from the query image's structure to better control different aspects of image generation - context for appearance and query for structure. 

2) It supports using multiple context images in a few-shot manner to provide more visual cues, unlike prior works that just use a single context image pair.

3) Experiments on diverse tasks, like using segmentation maps, sketches, etc as queries and real images as context, show it handles both in-domain and out-of-domain settings better than Prompt Diffusion. 

4) When conditioned on just context images or just prompt, unlike Prompt Diffusion, Context Diffusion still generates high quality outputs showing its balanced reliance on visual and textual inputs.

5) Overall, Context Diffusion demonstrates improved image quality and fidelity in capturing visual characteristics from context through both automated metrics and human evaluation, across various settings. This confirms its applicability for flexible in-context aware image generation.
