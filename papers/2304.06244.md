# [Asymmetrically-powered Neural Image Compression with Shallow Decoders](https://arxiv.org/abs/2304.06244)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the main research question seems to be:

How can we reduce the decoding complexity of neural image compression methods while maintaining competitive rate-distortion performance?

The key hypotheses proposed and tested are:

- The learned synthesis transforms in nonlinear transform coding (NTC) define a manifold that is relatively "flat", in the sense that linear interpolations in the latent space map to nearly straight lines in the reconstruction space. This suggests that a linear or shallow nonlinear synthesis transform may be sufficient.

- By exploiting the asymmetry between encoding and decoding, one can use a powerful encoder (e.g. iterative optimization) to compensate for a simple decoder. The theoretical analysis quantifies this intuition via the "inference gap".

- A JPEG-like linear synthesis transform can emulate a deep convolutional decoder if given a large enough kernel size, while being much more efficient. 

- With a powerful encoder, even a JPEG-like linear synthesis transform can achieve competitive rate-distortion performance to more complex learned synthesis transforms. Adding a small amount of nonlinearity can further improve performance.

In summary, the main hypothesis is that decoding complexity can be dramatically reduced in NTC by using shallow transforms, without sacrificing too much RD performance if paired with a powerful encoder. The experiments support this hypothesis.
