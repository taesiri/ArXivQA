# [Language Models with Conformal Factuality Guarantees](https://arxiv.org/abs/2402.10978)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem: Large language models (LLMs) tend to generate non-factual and hallucinated content, making their outputs untrustworthy. However, precisely quantifying and controlling the factuality of LLM outputs is challenging. 

Proposed Solution: The paper proposes "conformal factuality", which connects ideas from conformal prediction to provide high-probability correctness guarantees on LLM outputs. The key insight is that the correctness of an LLM output can be framed as an uncertainty quantification problem, where the uncertainty set is defined as the statements entailed by the LLM output. Using this connection, conformal methods can provide guarantees by implicitly representing uncertainty sets and backing off the specificity of claims.

The paper gives an algorithmic instantiation by:
(1) Breaking down LLM outputs into sub-claims 
(2) Scoring sub-claims by uncertainty  
(3) Removing low-scoring sub-claims to give guarantees

This algorithm leads to an interpretable system where users can still understand the remaining factual claims.

Main Contributions:
- Establishes connection between conformal prediction and LLM factuality using entailment
- Gives algorithm for conformal factuality by scoring and removing sub-claims  
- Empirically demonstrates high-probability factuality guarantees on LLM outputs while retaining most sub-claims

The method applies to any LLM, requires few human annotations, and leads to usable systems with quantitative correctness guarantees significantly higher than base LM performance. Evaluations on QA and reasoning datasets validate these claims.
