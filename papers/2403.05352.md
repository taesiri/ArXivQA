# [Enhancing Plausibility Evaluation for Generated Designs with Denoising   Autoencoder](https://arxiv.org/abs/2403.05352)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Evaluating generated images is an unsolved challenge in deep generative models (DGMs). Commonly used metrics like Fréchet Inception Distance (FID) rely on pre-trained ImageNet models and are often inconsistent with human judgment, especially for structured images like product designs.  
- Human designers care more about structural plausibility rather than absence of visual artifacts. But FID and other metrics over-penalize visual noise and are not sensitive to structural failures like missing components.
- Hence there is a need for a metric tailored to evaluating generated designs that focuses on structure rather than textures/noise.

Proposed Solution:
- Replace the ImageNet-trained Inception-V3 model in FID with a Denoising Autoencoder (DAE) trained on ImageNet. DAEs are robust to noise and focus on reconstructing the underlying structure.  
- Propose Fréchet Denoised Distance (FDD) which encodes real and fake images with the DAE and computes Fréchet distance between them like FID. Also propose variants using other distances.
- Additionally train a DAE on the target dataset (e.g. bikes) itself to get better feature representations.

Experiments & Results:
- Test FDD on structured datasets like BIKED (bikes), 3DChairs (chairs) and general dataset FFHQ (faces).
- Sensitivity test shows FDD distinguishes between visual vs structural disturbances unlike FID/DINO-FID. More consistent across groups.
- Consistency test shows FDD reliably detects increasing disturbance levels.
- Model ranking experiment shows FDD aligns better with human rankings than SOTA metrics.
- Visualizations using Grad-CAM show DAE focuses on structure while Inception-V3 focuses on textures.

Conclusion:
- FDD outperforms SOTA metrics in evaluating plausibility of generated designs.
- Robust to visual noise, sensitive to structure, consistent across experiments and aligns better with humans.
- Could guide DGMs to generate more reliable and plausible designs.
- Potential to assess 3D generative models too by using 3D DAEs.


## Summarize the paper in one sentence.

 Here is a one-sentence summary of the paper:

The paper proposes a new metric called Fréchet Denoised Distance (FDD) for evaluating the structural plausibility of generated designs by replacing the Inception-V3 model in Fréchet Inception Distance (FID) with a denoising autoencoder trained on ImageNet to make the metric focus more on design structure than visual artifacts.


## What is the main contribution of this paper?

 The main contribution of this paper is proposing a new metric called Fréchet Denoised Distance (FDD) for evaluating the structural plausibility of generated designs. Specifically:

- FDD replaces the Inception-V3 model in Fréchet Inception Distance (FID) with a denoising autoencoder (DAE) that is trained on ImageNet. This allows FDD to focus more on the underlying structure of design images rather than visual artifacts like noise.

- Through experiments on datasets like BIKED and Seeing3DChairs, FDD is shown to be more sensitive to structural failures in designs and more consistent with human judgment compared to metrics like FID, Kernel Inception Distance (KID), and recent variants like DINO-FID.

- Visualizations using Grad-CAM indicate that the DAE focuses more accurately on the shape and structure of design images compared to Inception-V3 which tends to just detect the presence of the object.

In summary, FDD provides a new automated metric specialized for evaluating the structural plausibility of generated designs, outperforming prior general-purpose metrics like FID. This can be useful for developing and assessing generative models focused on design tasks.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and keywords associated with this paper include:

- Evaluation metric
- Design generation 
- Structural plausibility
- Fréchet Inception Distance (FID)
- Denoising autoencoder (DAE)
- Fréchet denoised distance (FDD)
- Kernel denoised distance (KDD) 
- Topology denoised distance (TDD)
- Sensitivity test
- Increasing disturbances 
- Model ranking
- Grad-CAM visualization

The paper proposes a new metric called Fréchet denoised distance (FDD) for evaluating the structural plausibility of generated designs. It replaces the Inception-V3 model in FID with a denoising autoencoder to make the metric more robust to visual artifacts and more sensitive to structural failures. Experiments like sensitivity tests, tests with increasing disturbances, and model ranking are conducted to demonstrate the effectiveness of FDD over metrics like FID, KID, DinoFD, and TD. Concepts like KDD, TDD and Grad-CAM visualization are also explored in relation to FDD.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions I would ask about the method proposed in this paper:

1. The paper mentions that FDD is more robust to visual artifacts like noise compared to structural failures like missing components. But at what point would adding progressively more noise start to significantly impact the FDD score? Is there a noise threshold where FDD starts penalizing more?

2. Have you experimented with other types of autoencoders besides DAE for the FDD framework, like Variational Autoencoders (VAEs)? Could there be benefits to using a VAE over a DAE?

3. The 1948-dimensional latent space used matches that of Inception-V3. Did you experiment with other latent space dimensions, both larger and smaller? How sensitive is FDD to this hyperparameter?

4. For the model architecture, you mention the encoder/decoder have 5 convolutional layers each. Did you experiment with adding or removing layers? At what model complexity does overfitting start to become an issue? 

5. You show FDD generalizes well to other datasets besides ImageNet like BIKED and FFHQ. Did you test performance on more complex 3D design data? Would FDD work for evaluating generated 3D chair/table designs for example?  

6. You demonstrate the “focus” of the DAE better captures shape information than Inception-V3 using Grad-CAM. Are there other visualization techniques besides Grad-CAM that could give more insight into what the DAE learns?

7. For real-world application, how long does it take to encode a batch of images and compute the FDD score in practice? Is run-time efficiency comparable to FID?

8. The DAE model needs to be trained first before the FDD can be calculated. What strategies did you use to ensure the training was robust and that your model didn't overfit to the training data? 

9. For model ranking, what validation techniques did you use to establish quality of the human evaluation as ground truth? Could issues like evaluator bias impact conclusions?

10. A key benefit of FDD is better alignment with human assessment. But human evaluation of quality is subjective. Do you have plans to rigorously quantify inter-rater reliability of human evaluators?
