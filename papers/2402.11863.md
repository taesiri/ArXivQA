# [How Interpretable are Reasoning Explanations from Prompting Large   Language Models?](https://arxiv.org/abs/2402.11863)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Interpretability of explanations from large language models (LLMs) prompted with techniques like Chain-of-Thought (CoT) is important but not well studied. Specifically, the faithfulness, robustness and utility of such reasoning explanations need comprehensive evaluation. 

- Prior works assess faithfulness of CoT explanations but don't consider other prompting techniques or other dimensions of interpretability like robustness and utility.

Methodology:
- Proposed a new prompt called Self-Entailment-Alignment CoT (SEA-CoT) to improve interpretability of explanations by selecting reasoning chains based on maximizing entailment and overlap with context.

- Evaluated CoT, SEA-CoT and other prompting techniques (self-consistent CoT, question decomposition, self-refine) on 3 axes:
   1) Faithfulness: Using paraphrasing, mistake insertion and counterfactuals
   2) Robustness: Via paraphrasing 
   3) Utility: Using forward simulatability

- Experiments were done on OBQA, QASC and StrategyQA datasets using 70B Llama-v2 model.

Key Contributions:

- First comprehensive multi-dimensional evaluation of interpretability of reasoning explanations from LLMs across different prompting techniques

- Proposed SEA-CoT method substantially improves interpretability over other methods on faithfulness, robustness and utility

- Utility improvements indicate SEA-CoT produces explanations that are more useful for teaching student models

- Showed interpretability drops at lower model sizes but SEA-CoT still outperforms other prompts 

In summary, this paper presented an extensive analysis of interpretability of LLM explanations using new techniques and metrics, and introduced a novel prompt that yields significantly more interpretable reasoning chains.
