# [Learning Trajectory-Word Alignments for Video-Language Tasks](https://arxiv.org/abs/2301.01953)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading, the central research question this paper tries to address is: How can we build better video-language representations by capturing trajectory-word alignments in videos? The key points are:- Objects in videos form trajectories spanning across multiple frames. Previous methods like patch-to-word attention may overlook these trajectories and focus too much on static spatial contexts. - The paper proposes a Trajectory-to-Word BERT (TW-BERT) to learn trajectory-word alignments using a novel Trajectory-to-Word (T2W) cross-attention. This helps capture richer temporal contexts.- They also propose a Hierarchical Frame Selector (HFS) during fine-tuning to select more informative frames and better model trajectories of varying lengths.So in summary, the paper focuses on improving video-language representation learning by explicitly modeling object trajectories and their alignment to words, through the proposed T2W attention and HFS module.
