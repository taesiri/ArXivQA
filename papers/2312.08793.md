# [Forbidden Facts: An Investigation of Competing Objectives in Llama-2](https://arxiv.org/abs/2312.08793)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a summary paragraph of the key points from the paper:

This paper studies how Llama-2 chat models resolve competing objectives, using a "forbidden fact" task where the model must answer a factual recall question without stating the correct answer. The authors find that forbidding the right answer drastically reduces the probability of Llama-2 stating it correctly. By decomposing Llama-2 into over 1000 components, they identify around 35 key components that reliably implement this suppression behavior. However, analysis shows these components use messy, heterogeneous heuristics rather than a simple suppression algorithm. One faulty heuristic was exploited to craft an "California attack" that fools the model by innocuously placing the correct answer in the prompt. Overall, the complex mechanisms behind even simple behaviors pose challenges for interpretability and providing guarantees about model behavior. The results suggest goals like human-understandable explanations may be difficult for advanced AI. However, this could be because Llama-2 uses unintelligent heuristics, rather than such complexity being inherent to intelligence.


## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem Statement
- The paper studies how large language models (LLMs) like Llama-2 resolve competing objectives or pressures, using a "forbidden fact" task. 
- In this task, the model is asked to complete a factual statement (e.g. "The Golden Gate Bridge is in the state of __") but is forbidden from stating the correct answer (e.g. "California"). This creates a tension between answering correctly and obeying the constraint.

Methodology
- The authors create a dataset of factual statements paired with competing and non-competing forbidden words. 
- They decompose Llama-2 into over 1000 components and rank each component based on its importance for suppressing the correct answer in the competing case.  
- The top 35 components are enough to replicate the full suppression behavior. The authors analyze these components in detail.

Key Findings
- The most important suppression components are heterogeneous and appear to use complex, faulty heuristics rather than an straightforward algorithm. 
- These components privilege semantic information about what token is most likely to be the correct answer. 
- One faulty heuristic can be exploited via an "California Attack", where innocuously mentioning California in the prompt causes Llama-2 to wrongly suppress that token.

Main Contributions
- Provides evidence that even simple behaviors in LLMs may be implemented in messy, heterogeneous ways rather than clean algorithms.
- Develops a novel "forbidden fact" task and dataset to study competing objectives.
- Ranks over 1000 components in Llama-2 based on their importance for suppression using a first-order patching methodology.
- Discovers attack exploiting faulty heuristic in Llama-2's suppression circuitry.
- Suggests fundamental goals like interpretability and behavioral guarantees may be very difficult for advanced AI systems.


## Summarize the paper in one sentence.

 The paper investigates how Llama-2 chat models resolve competing objectives of helpfulness and harmlessness in factual recall by analyzing the components involved in suppressing the correct answer when forbidden, finding messy heuristics rather than clean algorithms.


## What is the main contribution of this paper?

 The main contribution of this paper is an analysis of how the Llama-2 language model resolves competing objectives or pressures, using a "forbidden fact" task where the model is instructed to answer a factual question truthfully but is forbidden from stating the correct answer. Specifically:

- The paper introduces the "forbidden fact" task and dataset where models face a conflict between truthfulness and not stating forbidden words. On this dataset, forbidding the correct answer reduces the odds of Llama-2 stating it correctly by over 1000x.

- The paper decomposes Llama-2 into over 1000 components (residual streams) and identifies around 35 components most important for suppressing the correct answer in the forbidden fact task. However, these components use complex, heterogeneous mechanisms rather than a simple suppression algorithm.

- Analysis reveals the components tend to privilege attending to correct answers over incorrect ones, use suppressive transforms, and rely on faulty heuristics. One such heuristic was exploited to craft an "California attack" that fools the model.

- The complexity of the suppression behavior, despite the task having a simple algorithmic solution, highlights difficulties in interpreting advanced ML systems and achieving goals like trustworthiness guarantees or engineering insights. The results suggest progress in these areas may require different techniques or focusing on stronger models.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper summary, some of the key terms and concepts associated with this paper include:

- Forbidden fact task - A task where models are instructed to perform factual recall but are forbidden from stating the correct answer
- Competing objectives - Models often face tradeoffs between different goals, like helpfulness vs harmlessness
- Llama-2 - The language model studied in this paper
- Decomposition - Breaking down Llama-2 into over 1000 components to analyze which contribute to suppression 
- First-order patching - A method used to attribute importance to components by swapping them between competing and non-competing prompts
- Suppressor heads - Attention heads found to implement suppression behavior 
- Heterogeneity - The paper finds the mechanisms used for suppression are complex and varied rather than following simple algorithms
- California attack - An adversarial attack taking advantage of faulty suppression heuristics
- Interpretability - Understanding how models like Llama-2 work and make decisions


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in this paper:

1. The paper decomposes Llama-2 into over 1000 components to study the suppression behavior. What are some limitations of this decomposition approach? Could there be higher-order interactions between components that this misses? 

2. The paper uses first-order patching to determine component importance. How valid is the assumption that components act independently? Could interactions between components undermine the attribution method?

3. The paper finds heterogeneity in the mechanisms used by the most important suppression heads. What implications does this have for interpretability and providing guarantees about model behavior? Does it suggest fundamental limitations?

4. The California Attack exploits faulty heuristics in Llama-2. Does the fact that similar attacks worked on multiple model sizes suggest fundamental issues with the architectures/training, or just limitations of insufficient scale?

5. The paper hypothesizes the suppression heads privilege correct answers when determining what to attend to. What further experiments could be done to validate this hypothesis? Are there other ways to test what information the heads use?  

6. The analysis studies attention but not much of the feedforward layers. What role might the MLPs play in the suppression behavior? What experiments could help determine this?

7. Query attention was found to not be semantically specific while key attention was. Why might this asymmetry exist? What underlying mechanisms could explain it?

8. The paper filters the dataset to focus on facts Llama-2 can answer correctly. How might the findings differ on a less filtered dataset? Would the mechanisms be more complex?

9. How does the simplicity/interpretability of the suppression behavior compare to that of factual recall studied in other work? Might suppression inherently require more complex mechanisms?

10. The paper suggests interpretability goals like human understandability of advanced AI may be unachievable. Do you agree with this conclusion? How could the findings be extended to make a more definitive determination?
