# [CL2CM: Improving Cross-Lingual Cross-Modal Retrieval via Cross-Lingual   Knowledge Transfer](https://arxiv.org/abs/2312.08984)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a summary paragraph of the key points from the paper:

This paper proposes a new cross-lingual cross-modal retrieval framework called CL2CM that improves the alignment between visual data (images/videos) and target language text without needing annotated visual-target language training data. The key idea is to leverage a cross-lingual (CL) network to model more reliable semantic correspondence between source and target languages using multi-level alignment, and transfer this knowledge to the cross-modal (CM) network that aligns visual and target language data. Specifically, the CL network uses instance-level and proposed self-supervised word-level alignment objectives, while the CM network focuses on instance-level alignment. Knowledge distillation is then used to transfer the comprehensive CL network knowledge to the CM network. This helps address two key challenges - noisy machine translations and effectively establishing visual-target language correlations. Experiments on three datasets demonstrate CL2CM's effectiveness over state-of-the-art methods, while retaining efficiency as only the CM network is used during inference. The multi-level alignment and knowledge transfer allow it to learn better fine-grained cross-modal correspondence and alleviate issues with noisy translations.
