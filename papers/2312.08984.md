# [CL2CM: Improving Cross-Lingual Cross-Modal Retrieval via Cross-Lingual   Knowledge Transfer](https://arxiv.org/abs/2312.08984)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a summary paragraph of the key points from the paper:

This paper proposes a new cross-lingual cross-modal retrieval framework called CL2CM that improves the alignment between visual data (images/videos) and target language text without needing annotated visual-target language training data. The key idea is to leverage a cross-lingual (CL) network to model more reliable semantic correspondence between source and target languages using multi-level alignment, and transfer this knowledge to the cross-modal (CM) network that aligns visual and target language data. Specifically, the CL network uses instance-level and proposed self-supervised word-level alignment objectives, while the CM network focuses on instance-level alignment. Knowledge distillation is then used to transfer the comprehensive CL network knowledge to the CM network. This helps address two key challenges - noisy machine translations and effectively establishing visual-target language correlations. Experiments on three datasets demonstrate CL2CM's effectiveness over state-of-the-art methods, while retaining efficiency as only the CM network is used during inference. The multi-level alignment and knowledge transfer allow it to learn better fine-grained cross-modal correspondence and alleviate issues with noisy translations.


## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem: 
Cross-lingual cross-modal retrieval aims to retrieve relevant images/videos using queries in a target language, without requiring annotated data between the vision and target language. This is challenging due to:
1) Noisy translations when using machine translation to create pseudo training pairs.  
2) Large gap between vision and language modalities making alignment difficult.  
3) Requirement to balance alignment quality and computational efficiency during inference.

Proposed Solution:
The paper proposes a Cross-Lingual to Cross-Modal (CL2CM) framework to improve vision-target language alignment by transferring knowledge from a Cross-Lingual (CL) network. Main ideas:

1) CL network aligns source and target language with multi-level alignment - instance-level and self-supervised word-level alignment to handle noisy translations. 

2) Knowledge distillation transfers CL network's more reliable cross-lingual knowledge to the Cross-Modal (CM) network. This comprehensive correspondence helps improve vision-target language alignment.

3) Only the efficient CM network is used during inference.

Main Contributions:

1) Proposes using cross-lingual transfer to improve vision-target language alignment in cross-lingual retrieval. First work to explore this direction.

2) Introduces multi-level alignment strategy in CL network to model more reliable cross-lingual correspondence and handle noisy translations.

3) Demonstrates state-of-the-art performance on Multi30K, MSCOCO and VATEX datasets, showing effectiveness for large-scale retrieval.

The summary covers the key problem being solved, the main ideas of the proposed CL2CM solution, and the primary contributions made in the paper. It highlights the core elements to provide a high-level understanding.
