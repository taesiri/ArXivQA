# [Proving the Lottery Ticket Hypothesis: Pruning is All You Need](https://arxiv.org/abs/2002.00585)

## What is the central research question or hypothesis that this paper addresses?

This paper sets out to prove the strong lottery ticket hypothesis, which states that within a sufficiently overparameterized neural network with random weights (e.g. at initialization), there exists a subnetwork that achieves competitive accuracy without any training. The key research questions addressed are:1) Can we theoretically prove the existence of good subnetworks within randomly initialized neural networks?2) How does pruning weights compare to pruning entire neurons in terms of expressive power?3) Can pruning a random network achieve competitive performance to optimizing the weights through training?The main hypothesis is that pruning a randomly initialized overparameterized network can find a subnetwork that matches the accuracy of a target network, without any further training. The paper proves this formally for deep and shallow neural networks.Overall, the paper provides theoretical evidence for the strong lottery ticket hypothesis. It shows the surprising result that competitive accuracy can be attained by simply pruning weights of a random network, without needing to optimize their values through training. The paper compares weight-based and neuron-based pruning, and demonstrates the greater expressive power of the former. The results motivate developing efficient algorithms for finding good subnetworks in large random networks.


## What is the main contribution of this paper?

This paper presents theoretical evidence for the lottery ticket hypothesis, which states that a randomly initialized neural network contains a small subnetwork that can match the performance of the full network when trained in isolation. The main contributions are:1. Proving that a randomly initialized overparameterized ReLU network contains a subnetwork ("weight-subnetwork") that can approximate an arbitrary target ReLU network of any depth, without any training of the subnetwork. This is a stronger version of the original lottery ticket hypothesis.2. Showing that pruning entire neurons in a network ("neuron-subnetwork") is equivalent in power to training only the last layer of a network (random features model). Hence neuron-subnetworks are more limited than weight-subnetworks.3. Demonstrating that even neuron-subnetworks can efficiently fit finite datasets and functions in a neural network RKHS, despite their limitations compared to weight-subnetworks.4. Proving hardness results, showing that finding the optimal subnetwork is computationally hard, both for weight and neuron-subnetworks.5. Discussing implications on the universality and limitations of pruning methods. Overall, the results provide theoretical justification for the lottery ticket hypothesis and pruning methods in general. The paper sets the ground for further research on efficient pruning algorithms with provable guarantees.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key point made in the paper:The paper proves that a randomly initialized overparameterized neural network contains a subnetwork that achieves competitive accuracy compared to the full network, without any further training needed - supporting the lottery ticket hypothesis that trainable subnetworks exist at random initialization.


## How does this paper compare to other research in the same field?

Here are a few thoughts on how this paper compares to other research on neural network pruning and the lottery ticket hypothesis:- This paper provides theoretical evidence for the lottery ticket hypothesis by proving that randomly initialized overparameterized networks contain subnetworks ("winning tickets") that can match the performance of the original network. Most prior work has focused on empirical evaluations, so this theoretical analysis helps explain why the phenomenon occurs.- The paper differentiates between two types of subnetworks - pruning weights vs pruning entire neurons. It shows that weight pruning is more powerful and can approximate arbitrary target networks, while neuron pruning is equivalent to random features. This sheds light on the relative power of different pruning techniques. - Compared to other theoretical lottery ticket papers like Zhou et al. 2019 and Ramanujan et al. 2019, this work makes fewer assumptions and proves a stronger version of the conjecture from Ramanujan et al. It shows the existence of winning tickets without any training, rather than just good initialization.- The paper gives a constructive proof by showing how to find good subnetworks via pruning. This is in contrast to some statistical/information-theoretic arguments on model compression. The pruning view could inspire new algorithms.- The techniques are relatively simple (based on concentration bounds) compared to some statistical learning theory proofs. This makes the arguments more accessible.- The paper links pruning to known hardness results for learning neural networks. This highlights interesting parallels between optimization and pruning.Overall, this paper enhances our theoretical understanding of lottery tickets and pruning in significant ways compared to prior work. The pruning view and connections to optimization hardness are novel contributions to the field. The simple and accessible analysis helps clarify why the phenomenon occurs.


## What future research directions do the authors suggest?

Based on my reading of the paper, some of the main future research directions suggested by the authors are:- Developing efficient heuristic algorithms for pruning randomly initialized neural networks. While the paper proves the existence of good subnetworks, finding them algorithmically is still an open challenge. The authors suggest developing heuristic pruning algorithms and analyzing them theoretically under certain distributional assumptions. - Improving the polynomial dependencies of the network size before pruning. The paper shows the existence of good subnetworks in networks that scale polynomially with the problem parameters. Reducing the degree of these polynomial dependencies could make the theoretical results more practical.- Generalizing the theoretical results to other neural network architectures like convolutional networks and ResNets. The current results are proved for fully-connected feedforward networks. Extending the analysis to more complex and modern architectures is an important direction.- Studying the relationship between pruning and optimizing weights theoretically. The paper shows pruning can achieve competitive results to weight training, but more work is needed to formally characterize and contrast these two paradigms.- Providing more formal evidence for the lottery ticket hypothesis and related empirical phenomena about good subnetworks in randomly initialized networks. The current paper proves a version of this hypothesis but more theoretical work is needed to fully explain these intriguing observations.In summary, the main themes are developing more practical algorithms, improving the theory, and extending the analysis to other neural network architectures in order to better understand random pruning and the lottery ticket hypothesis. Formalizing the connections to optimization and weight training is also an important direction suggested. Overall, the paper opens up many interesting avenues for future work at the intersection of neural network pruning, randomization, optimization, and generalization.


## Summarize the paper in one paragraph.

Here is a one paragraph summary of the paper:The paper proves the strong lottery ticket hypothesis, which states that a sufficiently over-parameterized neural network with random weights contains a subnetwork that achieves competitive accuracy on a task without any training. The authors show that for any target network with bounded weights, a larger random network of polynomial size contains a weight-subnetwork that approximates the target network. They also show that pruning entire neurons from a random network is equivalent in power to a random features model. The results provide theoretical justification for the lottery ticket hypothesis and suggest that neural network pruning may be as effective as optimizing weights. The paper demonstrates the expressive power of subnetworks within over-parameterized random neural networks.


## Summarize the paper in two paragraphs.

Here is a two paragraph summary of the paper:The paper proves the lottery ticket hypothesis, which states that a randomly initialized neural network contains a small subnetwork that can match the accuracy of the original network when trained in isolation. The authors show that for any target network with bounded weights, a sufficiently overparameterized neural network with random weights contains a pruned subnetwork that achieves similar accuracy to the target network, without any further training. Specifically, the paper differentiates between pruning weights and pruning entire neurons from the network. For weight pruning, they show that any target ReLU network can be approximated by pruning a random network of twice the depth. The size of the pruned network is similar, up to a constant factor, to the original target network. For neuron pruning, they prove it is equivalent in power to the random features model, and cannot efficiently approximate even a single ReLU neuron. Their results provide theoretical evidence for the lottery ticket hypothesis, and suggest that neural network pruning is a competitive alternative to weight optimization for finding highly accurate small networks.
