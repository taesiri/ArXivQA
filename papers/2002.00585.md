# [Proving the Lottery Ticket Hypothesis: Pruning is All You Need](https://arxiv.org/abs/2002.00585)

## What is the central research question or hypothesis that this paper addresses?

This paper sets out to prove the strong lottery ticket hypothesis, which states that within a sufficiently overparameterized neural network with random weights (e.g. at initialization), there exists a subnetwork that achieves competitive accuracy without any training. The key research questions addressed are:1) Can we theoretically prove the existence of good subnetworks within randomly initialized neural networks?2) How does pruning weights compare to pruning entire neurons in terms of expressive power?3) Can pruning a random network achieve competitive performance to optimizing the weights through training?The main hypothesis is that pruning a randomly initialized overparameterized network can find a subnetwork that matches the accuracy of a target network, without any further training. The paper proves this formally for deep and shallow neural networks.Overall, the paper provides theoretical evidence for the strong lottery ticket hypothesis. It shows the surprising result that competitive accuracy can be attained by simply pruning weights of a random network, without needing to optimize their values through training. The paper compares weight-based and neuron-based pruning, and demonstrates the greater expressive power of the former. The results motivate developing efficient algorithms for finding good subnetworks in large random networks.


## What is the main contribution of this paper?

This paper presents theoretical evidence for the lottery ticket hypothesis, which states that a randomly initialized neural network contains a small subnetwork that can match the performance of the full network when trained in isolation. The main contributions are:1. Proving that a randomly initialized overparameterized ReLU network contains a subnetwork ("weight-subnetwork") that can approximate an arbitrary target ReLU network of any depth, without any training of the subnetwork. This is a stronger version of the original lottery ticket hypothesis.2. Showing that pruning entire neurons in a network ("neuron-subnetwork") is equivalent in power to training only the last layer of a network (random features model). Hence neuron-subnetworks are more limited than weight-subnetworks.3. Demonstrating that even neuron-subnetworks can efficiently fit finite datasets and functions in a neural network RKHS, despite their limitations compared to weight-subnetworks.4. Proving hardness results, showing that finding the optimal subnetwork is computationally hard, both for weight and neuron-subnetworks.5. Discussing implications on the universality and limitations of pruning methods. Overall, the results provide theoretical justification for the lottery ticket hypothesis and pruning methods in general. The paper sets the ground for further research on efficient pruning algorithms with provable guarantees.
