# [Proving the Lottery Ticket Hypothesis: Pruning is All You Need](https://arxiv.org/abs/2002.00585)

## What is the central research question or hypothesis that this paper addresses?

This paper sets out to prove the strong lottery ticket hypothesis, which states that within a sufficiently overparameterized neural network with random weights (e.g. at initialization), there exists a subnetwork that achieves competitive accuracy without any training. The key research questions addressed are:1) Can we theoretically prove the existence of good subnetworks within randomly initialized neural networks?2) How does pruning weights compare to pruning entire neurons in terms of expressive power?3) Can pruning a random network achieve competitive performance to optimizing the weights through training?The main hypothesis is that pruning a randomly initialized overparameterized network can find a subnetwork that matches the accuracy of a target network, without any further training. The paper proves this formally for deep and shallow neural networks.Overall, the paper provides theoretical evidence for the strong lottery ticket hypothesis. It shows the surprising result that competitive accuracy can be attained by simply pruning weights of a random network, without needing to optimize their values through training. The paper compares weight-based and neuron-based pruning, and demonstrates the greater expressive power of the former. The results motivate developing efficient algorithms for finding good subnetworks in large random networks.
