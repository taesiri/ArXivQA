# GLM-130B: An Open Bilingual Pre-trained Model

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be: How can we train and optimize a large-scale open-source bilingual language model to be highly accurate yet affordable and accessible?The key goals of the paper appear to be:- To train an open-source language model with over 100 billion parameters that matches or exceeds the performance of similar proprietary models like GPT-3.- To make this large language model affordable and accessible by optimizing its training efficiency, quantizing it to reduce memory usage, and accelerating its inference speed. - To train the model to be bilingual (English and Chinese) and evaluate it on benchmarks in both languages.- To share insights, code, and lessons learned to help the broader research community train better large language models.So in summary, the central goal is to advance research on training open-source, high-performance, yet affordable and accessible large language models, using their 130B parameter bilingual model GLM-130B as a case study. The paper discusses the model architecture, training techniques, inference optimizations, and evaluates the model extensively to showcase its capabilities.


## What is the main contribution of this paper?

The main contributions of this paper are:1. It introduces GLM-130B, a 130 billion parameter open bilingual pre-trained language model. This provides an alternative to the privately-owned large language models like GPT-3.2. It describes the unique design choices and training strategies for GLM-130B, including using the GLM architecture, DeepNorm layer normalization, embedding gradient shrink, and multi-task instruction pre-training. These enable high performance and stability during training.3. It achieves INT4 weight quantization for GLM-130B without loss of performance. This allows the large model to be run on relatively affordable GPUs like 4x RTX 3090. 4. It evaluates GLM-130B extensively, showing it outperforms GPT-3 and other large language models on a range of English and Chinese benchmarks while having less bias and toxicity.5. It open-sources the model, code, training logs, evaluations, and lessons learned to promote openness and reproducibility in large language model research.Overall, the main contribution is introducing an open, high-performance, and inclusive large language model to facilitate research and applications of models at this scale. The paper shares valuable insights and engineering techniques to train and run such large models.
