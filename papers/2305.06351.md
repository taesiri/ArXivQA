# [Reconstructing Animatable Categories from Videos](https://arxiv.org/abs/2305.06351)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is:How can we build animatable 3D models of deformable object categories from monocular videos while disentangling variations across instances from time-specific variations within an instance?The key challenges they aim to address are:1) Modeling the morphological variation across instances within a category (e.g. different dog breeds). This is difficult to disentangle from time-varying variations within a single instance (e.g. a dog's articulation and deformation over time).2) Dealing with the limited and impoverished nature of in-the-wild videos, where objects may be partially observable from limited viewpoints and with inaccurate segmentation masks. The model needs to listen to common structures across the category while staying faithful to the input views. Their proposed approach RAC introduces three main ideas to address these challenges:1) Specializing skeletons to instances via optimization, allowing better disentanglement of morphology and articulation.2) A method for latent space regularization that encourages shared structure across the category while maintaining instance details.3) Using 3D background models to disentangle objects from the background.The overall goal is to learn category-level 3D models from monocular videos that can generate different instances and their motions, without requiring 3D supervision or registrations.In summary, the paper aims to address the problem of building animatable category-level 3D models from in-the-wild monocular videos, while disentangling variations across instances and motions over time.


## What is the main contribution of this paper?

This paper presents Reconstructing Animatable Categories from Videos (RAC), a method to build animatable 3D models of object categories from monocular video collections. The key contributions are:1. Learning category-level shape and skeleton models from videos without 3D supervision. This allows capturing variations across instances within a category (e.g. different dog breeds).2. Disentangling between-instance morphological variations from within-instance pose and motion over time. This allows motion transfer across instances. 3. Modeling the background scene as a 3D NeRF to improve object segmentation.4. Achieving state-of-the-art reconstruction quality on humans, dogs and cats using only 50-100 casual internet videos per category.The main novelty is building category-level models without 3D supervision that can disentangle shape, appearance and motion variations across a category. This is enabled by optimizing instance-specific skeletons and shape codes, regularizing the shape space, and modeling the background. The results show the potential to scale articulated 3D reconstruction to new categories from in-the-wild videos.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper presents RAC, a method to reconstruct animatable 3D models of deformable object categories like cats, dogs, and humans from monocular videos by disentangling variations across instances (e.g. morphology) from variations within instances over time (e.g. articulations), enabling motion transfer across instances in a category.
