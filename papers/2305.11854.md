# [Multimodal Web Navigation with Instruction-Finetuned Foundation Models](https://arxiv.org/abs/2305.11854)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be: How can we develop an autonomous web navigation agent that can be trained offline using multimodal inputs (HTML and screenshots) and leverage large pretrained language models?The key hypotheses appear to be:1) Grounded spatial understanding from multimodal inputs (HTML + screenshots) will improve a web navigation agent's ability to complete tasks, especially those involving dynamic page transitions or requiring global context. 2) Using an instruction-finetuned language model (Flan-T5) as opposed to a standard pretrained LM (T5) will improve the agent's HTML comprehension and multi-step reasoning abilities.3) Scaling up the training data (to 347K demonstrations) and model size will lead to better performance on web navigation tasks. The overall goal seems to be developing a practical offline training approach for web agents that can leverage inductive biases from large foundation models like Flan-T5 and achieve strong performance without needing massive online interaction. The paper aims to demonstrate this through experiments on MiniWoB and WebShop benchmarks.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contributions seem to be:1. Proposing a multimodal web navigation agent called WebGUM that uses both HTML and screenshot images as input. This allows the agent to leverage both semantic and visual information.2. Using an instruction-finetuned foundation model (Flan-T5) as the base for WebGUM. The authors hypothesize and show empirically that starting with a model finetuned on instruction-following improves the agent's ability to follow instructions and do multi-step reasoning for web navigation tasks. 3. Demonstrating strong performance of WebGUM on the MiniWoB and WebShop benchmarks, including:- Outperforming prior offline trained methods by a large margin- Approaching the performance of online RL methods with purely offline training- Outperforming prompting large models like PaLM on WebShop4. Performing analysis and ablations to demonstrate WebGUM's advantages in:- Leveraging multimodal inputs - Better semantic understanding of HTML- Multi-step reasoning ability- Scaling with dataset and model size5. Releasing a new large-scale multimodal dataset of 347K demonstrations for MiniWoB, to enable further research in this direction.In summary, the main contributions are presenting a new offline trained multimodal agent WebGUM, demonstrating its capabilities, and releasing a large multimodal demonstration dataset to facilitate future research on web navigation agents. The results show that instruction-finetuned foundation models combined with multimodal inputs are a promising approach.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence TL;DR summary of the paper:The paper proposes an instruction-following multimodal agent called WebGUM that leverages both webpage screenshots and HTML to perform web navigation tasks by finetuning a pretrained vision-language model, and demonstrates improved performance over prior methods on web navigation benchmarks.
