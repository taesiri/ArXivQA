# [Multimodal Web Navigation with Instruction-Finetuned Foundation Models](https://arxiv.org/abs/2305.11854)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be: How can we develop an autonomous web navigation agent that can be trained offline using multimodal inputs (HTML and screenshots) and leverage large pretrained language models?The key hypotheses appear to be:1) Grounded spatial understanding from multimodal inputs (HTML + screenshots) will improve a web navigation agent's ability to complete tasks, especially those involving dynamic page transitions or requiring global context. 2) Using an instruction-finetuned language model (Flan-T5) as opposed to a standard pretrained LM (T5) will improve the agent's HTML comprehension and multi-step reasoning abilities.3) Scaling up the training data (to 347K demonstrations) and model size will lead to better performance on web navigation tasks. The overall goal seems to be developing a practical offline training approach for web agents that can leverage inductive biases from large foundation models like Flan-T5 and achieve strong performance without needing massive online interaction. The paper aims to demonstrate this through experiments on MiniWoB and WebShop benchmarks.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contributions seem to be:1. Proposing a multimodal web navigation agent called WebGUM that uses both HTML and screenshot images as input. This allows the agent to leverage both semantic and visual information.2. Using an instruction-finetuned foundation model (Flan-T5) as the base for WebGUM. The authors hypothesize and show empirically that starting with a model finetuned on instruction-following improves the agent's ability to follow instructions and do multi-step reasoning for web navigation tasks. 3. Demonstrating strong performance of WebGUM on the MiniWoB and WebShop benchmarks, including:- Outperforming prior offline trained methods by a large margin- Approaching the performance of online RL methods with purely offline training- Outperforming prompting large models like PaLM on WebShop4. Performing analysis and ablations to demonstrate WebGUM's advantages in:- Leveraging multimodal inputs - Better semantic understanding of HTML- Multi-step reasoning ability- Scaling with dataset and model size5. Releasing a new large-scale multimodal dataset of 347K demonstrations for MiniWoB, to enable further research in this direction.In summary, the main contributions are presenting a new offline trained multimodal agent WebGUM, demonstrating its capabilities, and releasing a large multimodal demonstration dataset to facilitate future research on web navigation agents. The results show that instruction-finetuned foundation models combined with multimodal inputs are a promising approach.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence TL;DR summary of the paper:The paper proposes an instruction-following multimodal agent called WebGUM that leverages both webpage screenshots and HTML to perform web navigation tasks by finetuning a pretrained vision-language model, and demonstrates improved performance over prior methods on web navigation benchmarks.


## How does this paper compare to other research in the same field?

Here are a few thoughts on how this paper compares to other related research:- The paper presents a multimodal web navigation agent that uses both webpage screenshots and HTML content as inputs. This is different from some prior work like WebN-T5 that only looked at HTML inputs. Using multimodal inputs seems to improve performance, as shown in the results. - The proposed model WebGUM is trained in a fully offline manner on a large dataset of 347K demonstrations. This is an advantage over methods like CC-Net that require online reinforcement learning with billions of exploratory interactions. Offline training is more feasible for real-world deployment.- WebGUM simplifies the web navigation problem by using a standard transformer architecture based on pretrained models like T5 and ViT. It outputs executable actions as text. This contrasts with prior specialized models that encode HTML structure explicitly. The simplification seems to allow better leveraging of pretrained foundations.- The paper shows WebGUM significantly outperforms prior offline trained methods like WebN-T5, and comes close to online RL methods like CC-Net. Scaling model size and data improves WebGUM further.- The use of an instruction-finetuned foundation model like Flan-T5 appears important. This model seems to provide stronger inductive bias for multi-step reasoning and alignment with instructions.- WebGUM also does well on the WebShop benchmark, even outperforming very large models like PaLM, suggesting its reasoning skills transfer.Overall, the simplifications and offline training approach seem novel and powerful compared to prior work. The results demonstrate strong performance from instruction tuning and scaled data/model size.


## What future research directions do the authors suggest?

Based on my reading of the paper, some of the key future research directions suggested by the authors include:- Developing methods to allow agents to generalize more broadly to diverse real-world websites or instructions beyond the MiniWoB benchmark. The authors note that human-level generalization is still a challenging open problem.- Incorporating online reinforcement learning into the training process to further enhance performance on certain long-horizon, multi-step tasks. The authors suggest this could help close the remaining performance gap compared to state-of-the-art online trained agents.- Collecting internet-scale behavioral datasets to train more capable generalist models. The authors note their multimodal dataset of 347K episodes is still far from internet-scale.- Applying the approach of simplifying web navigation and leveraging foundation models to other advanced LLMs or more open-ended situations. The authors highlight their approach has minimal task-specific assumptions.- Deploying trained policies on real websites, which requires addressing challenges like handling no reward signal and costly interactions. The authors suggest iterative data collection and deployment could enable practical interactive agents.In summary, the key directions involve scaling the datasets, models and training techniques to achieve broader generalization, while also moving towards real-world deployment which poses additional challenges. Leveraging online RL, expanding the scope beyond MiniWoB, and harnessing internet-scale data seem to be the core suggestions.


## Summarize the paper in one paragraph.

Here is a one paragraph summary of the paper:The paper presents a multimodal web navigation agent called WebGUM that can understand both webpage screenshots and HTML code to follow natural language instructions to complete web-based tasks. WebGUM combines a vision transformer (ViT) to encode screenshot images and an instruction-finetuned language model (Flan-T5) to handle text, allowing it to leverage both visual and textual information. It is trained offline on a large dataset of 347K expert demonstrations on the MiniWoB benchmark. Experiments show that WebGUM significantly outperforms prior offline trained methods, improving success rate from 48.4% to 80.3% on MiniWoB. Ablations demonstrate WebGUM successfully leverages the multimodality for spatial understanding and temporal task information. Using an instruction-finetuned language model boosts performance by enhancing HTML comprehension and multi-step reasoning capabilities. Results also highlight the importance of scaling model size and training data. Overall, the work presents an effective framework to leverage vision-language foundation models for offline training of capable web agents.


## Summarize the paper in two paragraphs.

Here is a two paragraph summary of the paper:The paper presents a multimodal web navigation agent called WebGUM that can understand webpages and complete tasks through interacting with them. WebGUM uses a transformer-based model that takes in multimodal observations of webpages, including raw HTML code as well as screenshot images. The model is pretrained on a large corpus of 347K expert demonstrations of completing web navigation tasks. During training, the language model component is initialized with an instruction-following model called Flan-T5 in order to provide strong inductive bias for following natural language instructions. The image model component is a vision transformer pretrained on ImageNet. By leveraging both the textual HTML code and visual screenshot images, WebGUM is able to understand webpages and reason about them better. This allows it to complete challenging multi-step web navigation tasks successfully.Experiments show that WebGUM significantly outperforms prior web navigation agents, especially other offline trained agents. It achieves 80.3% success rate on the MiniWoB benchmark, compared to 48.4% for the previous best offline agent. Detailed analysis reveals WebGUM's advantages in grounded visual understanding of webpages, comprehending raw HTML code, and multi-step reasoning abilities. The instruction-finetuned language model is also shown to be more robust to unfamiliar webpages. On the WebShop e-commerce benchmark, WebGUM also achieves state-of-the-art performance compared to methods like reinforcement learning and prompting large language models. The results demonstrate the effectiveness of WebGUM's offline training approach and use of foundation models for sample-efficient web navigation.


## Summarize the main method used in the paper in one paragraph.

Here is a one paragraph summary of the main method used in the paper:The paper proposes an instruction-following multimodal agent, WebGUM, for web navigation tasks. WebGUM observes both webpage screenshots and HTML pages as input and outputs executable actions like click and type. It is based on jointly finetuning an instruction-finetuned language model (Flan-T5) and a vision transformer (ViT) on a large corpus of demonstration data. The Flan-T5 encoder-decoder handles text tokens from action history, instructions, and raw HTML, while the ViT encodes visual tokens from screenshots. Multiple visual tokens are extracted from image patches to capture richer spatial features. The multimodal tokens are fed into the Flan-T5 which predicts actions in text format. This allows WebGUM to leverage the strong inductive biases of foundation models pre-trained on vision and language data to solve challenging web tasks in an offline, data-driven manner, without needing online reinforcement learning.
