# [PeLLE: Encoder-based language models for Brazilian Portuguese based on   open data](https://arxiv.org/abs/2402.19204)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- There is a need for large language models (LLMs) specialized for Brazilian Portuguese that are trained on open, reproducible data. Existing models are either multilingual, trained on non-open data, or smaller in size.

Proposed Solution:
- The authors introduce PeLLE, a family of encoder-based LLMs for Brazilian Portuguese based on the RoBERTa architecture. 
- PeLLE models are pretrained from scratch or by extending weights of mBERT and XLM-R, using the open Carolina Corpus (823M tokens).
- Three models are presented: pPeLLE (from scratch), mPeLLE (extends mBERT), xPeLLE (extends XLM-R).

Experiments:
- PeLLE models are evaluated on textual entailment, semantic similarity, hate speech detection and document classification tasks.
- Compared against Portuguese models (BERTimbau, Albertina-PTBR) and multilingual models (mBERT, XLM-R).

Results:
- Larger models generally perform better, but smaller curated models can compete. 
- pPeLLE is competitive, showing value of Portuguese-only training.
- Extending mBERT and XLM-R (mPeLLE, xPeLLE) also works well.
- Performance on legal documents suggests value of law texts in Carolina Corpus.

Main Contributions:
- First family of encoder LLMs for Portuguese based fully on open data
- Analysis of tradeoffs between model size, curated data, language specificity
- New pretrained models available for use, reproducible and unrestricted
- Showcase of using the Carolina Corpus for pretraining

The paper makes progress towards open, reproducible models in Portuguese while analyzing different approaches to pretraining such models. The models and analysis provide a basis for further work on optimization and enlargement of models for this language.
