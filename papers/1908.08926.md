# [Efficient Deep Neural Networks](https://arxiv.org/abs/1908.08926)

## What is the central research question or hypothesis that this paper addresses?

This paper does not seem to have a clearly stated central research question or hypothesis. It appears to be presenting a new method called Differentiable Neural Architecture Search (DNAS) for automatically searching neural network architectures. The key ideas presented in the paper are:- Formulating neural architecture search as optimizing the distribution over architectures rather than optimizing individual architectures. This allows gradient-based optimization of the distribution parameters.- Using a stochastic super-network with Gumbel-Softmax relaxation to represent the architecture distribution and enable differentiable optimization. - Designing a latency-aware loss function that incorporates actual latency measurements on the target hardware to guide the search towards efficient architectures.- Applying DNAS to two problems - mixed-precision quantization and efficient mobile ConvNet search - and showing it can find models that exceed state-of-the-art in terms of accuracy and efficiency.So in summary, there is no single clear hypothesis being tested. The main contributions are introducing the DNAS method itself, the techniques like Gumbel-Softmax and latency-aware loss to make it work efficiently, and demonstrating its effectiveness on the two application tasks. The high-level goal is to enable fast and automated neural architecture search for finding accurate and efficient models.
