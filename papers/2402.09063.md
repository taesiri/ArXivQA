# [Soft Prompt Threats: Attacking Safety Alignment and Unlearning in   Open-Source LLMs through the Embedding Space](https://arxiv.org/abs/2402.09063)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Most prior work on adversarial attacks against LLMs focuses on manipulating discrete input tokens. However, for open-source models with full model access, attacks can directly manipulate the continuous embedding space. This threat model is currently under-explored.  

- As open-source LLMs advance rapidly in capability, ensuring their safety and security becomes imperative. However, their vulnerability against embedding space attacks tailored to open-source models remains unclear.

Method:
- The authors propose embedding space attacks that directly perturb the continuous vector representations of input tokens to trigger unsafe model behaviors.

- Two attack settings are studied: (1) Individual attacks optimized for a single input, (2) Universal attacks optimized over multiple inputs to improve generalization.  

- The attacks are evaluated on two tasks: (1) Breaking safety alignment to trigger harmful responses (2) Extracting supposedly deleted information from unlearned LLMs.

Key Results: 
- Embedding attacks break safety alignment substantially faster than prior attacks, removing alignment in just 9 iterations on average. They also generalize better than fine-tuning.

- Embedding attacks successfully extract more deleted information from unlearned LLMs than direct prompting, even when using multiple prompt attempts. This demonstrates their ability to rigorously interrogate unlearned models.

- A novel cumulative success rate metric is proposed to quantify information leakage from unlearned models over multiple prompt attempts. Surprisingly, simple top-k sampling also proves an effective attack. 

Main Contributions:
- First demonstration of the effectiveness of embedding space attacks to compromise safety of open-source LLMs and unlearned models.

- Introduction of new metrics tailored to continuously attack and evaluate unlearned models over multiple attempts.

- Findings highlight need to re-evaluate defense methods against this overlooked but highly potent threat model as open-source LLMs continue advancing rapidly.
