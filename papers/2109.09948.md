# [Neural networks with trainable matrix activation functions](https://arxiv.org/abs/2109.09948)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is: How can we construct neural network activation functions that are adaptive and trainable, in order to improve the performance and robustness of neural networks? The key ideas and contributions are:- The paper proposes a systematic approach to constructing matrix activation functions whose entries are generalized from ReLU. The activation functions depend on parameters that can be trained along with the weights and biases.- The activation functions are based on matrix-vector multiplications using only scalar multiplications and comparisons, making them simple and efficient.- The paper introduces diagonal and tridiagonal matrix activation functions where the diagonal and off-diagonal entries are piecewise constant functions represented with trainable parameters. - This allows different activation functions to be learned for each neuron and layer, adapted to the specific problem. The approach includes ReLU networks as a special case.- Neural networks with the proposed trainable matrix activation functions are shown to be robust and effective on function approximation and image classification tasks, outperforming networks with fixed activation functions like ReLU.In summary, the central hypothesis is that making the activation functions adaptive and trainable can improve neural network performance. The paper proposes an approach to achieve this and validates the hypothesis through numerical experiments.
