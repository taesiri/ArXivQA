# [The Unreasonable Effectiveness of Easy Training Data for Hard Tasks](https://arxiv.org/abs/2401.06751)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Hard to train models to give correct outputs on specialized domains of knowledge since it is difficult to correctly label training data (scalable oversight problem). 
- Want to understand how well models generalize from easy (correctly labeled) data to hard (difficult to label correctly) data to assess difficulty of scalable oversight.

Proposed Approach: 
- Assess "easy-to-hard generalization" - model performance on hard test data when trained on easy training data according to some hardness measure. 
- Use 6 human hardness measures (grade level, expert rating, skills, question/answer length, reasoning steps) and 1 model-based measure.
- Compare "easy-to-hard" models to "hard-to-hard" upper bound (models trained on hard data) and unsupervised lower bound. 
- Main metric is Supervision Gap Recovered (SGR) - % of gap closed between lower and upper bounds.

Experiments:
- Test on 4 QA datasets, with model scale 7b-70b params.
- Surprisingly find 70-100% SGR, meaning easy data is almost as good as hard data for supervision. 
- Show it's better to use easy data when: (1) More easy data can be collected for a budget, (2) Easy data has less noise.
- SGR is robust across model scale, starts declining when hardness gap is very large.

Main Conclusions:
- Language models generalize well from easy to hard data, suggesting scalable oversight problem may be easier than thought. 
- Often better to train on easy data due tobudget/noise tradeoffs.
- Easy-to-hard generalization surprisingly consistent, though declines with large hardness gaps.

The summary covers the key points on the problem being studied, the proposed approach, the main experiments and results, and the high-level conclusions. Let me know if you need any clarification or have additional questions!


## Summarize the paper in one sentence.

 This paper finds that large language models can often perform almost as well on difficult questions when trained on easier questions compared to when trained on the difficult questions themselves, suggesting the scalable oversight problem may be more tractable than previously thought.


## What is the main contribution of this paper?

 This paper studies the problem of easy-to-hard generalization in language models. The main contributions are:

1) Demonstrating that language models generalize surprisingly well from easy training data to hard test data, often recovering 70-100% of the performance gap between an unsupervised lower bound (zero-shot prompting) and an oracle upper bound (models trained on hard data). This suggests the scalable oversight problem may be easier than previously thought.

2) Showing it can be better in practice to collect and train on easy data rather than hard data, since hard data tends to be more expensive to collect and can have noisier labels. Models trained on more easy data can outperform those trained on less hard data.  

3) Analyzing easy-to-hard generalization over multiple datasets, task types, training methods, hardness measures (including human measures like grade level and model-based measures), and model scales in order to establish the robustness of the main conclusions.

In summary, the paper provides evidence that language models exhibit surprisingly strong easy-to-hard generalization, and it may be preferable in practice to train models on easy data. This has implications for the difficulty of the scalable oversight problem.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and concepts are:

- Easy-to-hard generalization: Generalizing from models trained on easy data to performance on hard data. This is the main focus of the paper.

- Scalable oversight: The problem of how to properly supervise and evaluate models when labeling training data is difficult. Easy-to-hard generalization provides context on how challenging this problem is.

- Supervision gap recovered (SGR): The metric that compares easy-to-hard performance to an unsupervised lower bound and hard-to-hard upper bound. 

- Multiple notions of hardness: The paper examines several measures of data hardness based on properties like education level, expert ratings, cognitive skills required, etc.

- In-context learning: One of the training methods used, which leverages the few-shot learning abilities of language models.

- Question answering: The paper tests models on multiple question answering datasets with different domains and hardness levels.

Some other potentially relevant terms: compositional generalization, curriculum learning, parameter-efficient tuning, chain-of-thought reasoning. Let me know if you need any clarification or have additional questions!


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in this paper:

1. The paper introduces a new metric called "Supervision Gap Recovered" (SGR). How is this metric defined and what does it aim to capture about easy-to-hard generalization? What are the strengths and potential limitations of using this metric?

2. The paper finds surprisingly high SGR values from easy-to-hard generalization. What hypotheses might explain why current language models exhibit this degree of generalization? How might these hypotheses be tested?

3. The paper suggests easy data supervision could be preferable to hard data supervision when hard data is costlier or noisier. What open questions remain about optimally balancing data collection budgets and priorities between easy and hard training data?

4. The paper mostly focuses on a fixed number of easy training examples - e.g. 10 examples with in-context learning. How might easy-to-hard generalization trends change if far more easy examples were used? What tradeoffs might come into play?

5. The paper shows SGR values are high across multiple training methods. What differences did you notice between methods and what might explain them? When might certain methods be preferable over others?  

6. The paper focuses on question answering and reasoning tasks. What other challenging domains might it be worthwhile to study easy-to-hard generalization for? What unique challenges or opportunities might those domains present?

7. The paper uses datasets with hardness annotations from humans. What are some of the challenges and open questions around more automatically determining or even predicting datapoint hardness levels?

8. The paper examines model scale up to 70B parameters. How if at all might you expect easy-to-hard generalization trends to change at the 100B-1T parameter scale and beyond? What experiments could shed light on this?

9. The paper finds easy-to-hard generalization declines when the train-test hardness gap grows very large. What hypotheses might explain this decline? How could this phenomenon be studied further?  

10. The paper focuses on supervised learning. How well might you expect unsupervised or self-supervised models to generalize from easy to hard data? What benefits or limitations might they have compared to supervised approaches studied here?
