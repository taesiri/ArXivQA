# [On Building Myopic MPC Policies using Supervised Learning](https://arxiv.org/abs/2401.12546)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem: 
- Model predictive control (MPC) requires solving an online optimization problem at each time step, which can be computationally expensive. 
- Approximating the MPC policy using supervised learning (like neural networks) can reduce online computation but loses performance guarantees.
- Myopic MPC with short prediction horizon reduces computation but degrades performance.

Proposed Solution:
- Learn the optimal "cost-to-go" function using supervised learning offline. 
- Cost-to-go represents future cost associated with control actions.
- Replace cost-to-go in myopic MPC with learned function approximator.
- Generates optimal state-value pairs for supervised learning by solving long-horizon MPC problems offline.
- Proposes data augmentation scheme to efficiently generate more state-value pairs using sensitivity analysis. 

Main Contributions:
- Novel way to build myopic MPC policy by learning cost-to-go function offline using supervised learning.
- Differs from existing reinforcement learning approaches for this problem.
- Sensitivity-based data augmentation scheme to generate training data at low computational cost.
- Analysis on stability with approximate cost-to-go function.
- Demonstrated on inverted pendulum example, shows significant speedup with minimal performance loss.

In summary, the paper proposes a method to learn an approximate cost-to-go function offline using state-value pairs from an optimal MPC expert. This is then used in a myopic MPC scheme to get a fast online controller without compromising stability or performance. The data augmentation scheme makes this approach scalable by reducing the offline computational burden.


## Summarize the paper in one sentence.

 This paper proposes learning a myopic model predictive control policy by approximating the optimal cost-to-go function offline using state-value pairs generated from an expert long-horizon MPC controller, and presents a sensitivity-based data augmentation scheme to efficiently generate the training data.


## What is the main contribution of this paper?

 The main contribution of this paper is a myopic MPC policy, where the cost-to-go function is learned offline using supervised learning. Specifically:

- It proposes to learn the optimal value function (cost-to-go) offline using state-value pairs generated by solving long horizon MPC problems, instead of learning the MPC policy directly. 

- It presents a sensitivity-based data augmentation scheme to efficiently generate multiple state-value training samples using only a few offline MPC solves. This helps reduce the computational cost of collecting the training data.

- It provides error bounds on the learned value function when using inexact augmented data samples. 

- It shows that despite approximation errors, the learned myopic MPC policy can still recover the optimal policy or at least stabilize the system under certain conditions.

- Compared to standard approximate explicit MPC schemes, the proposed approach can better handle online changes to constraints or cost function.

So in summary, the key novelty is using supervised learning to train a myopic MPC policy offline by approximating the cost-to-go function, enabled by an efficient data augmentation strategy to reduce data collection costs.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, here are some of the main keywords and key terms:

- Model predictive control (MPC)
- Value function approximation
- Parametric sensitivity
- Data augmentation
- Supervised learning
- Myopic MPC policy
- Bellman's principle of optimality
- Cost-to-go function
- Reinforcement learning
- Approximate dynamic programming (ADP)

The paper proposes a framework to learn an approximate cost-to-go function using supervised learning with data generated by solving an MPC problem offline. This learned function is then used in a myopic MPC policy with very short prediction horizons. A key contribution is a sensitivity-based data augmentation scheme to efficiently generate training data. The approach differs from typical reinforcement learning methods for value function approximation by using open-loop rather closed-loop data. Overall, the goal is to develop a computationally cheap myopic MPC policy by approximating the cost-to-go term using supervised learning on MPC-generated demonstrations.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in this paper:

1. How does the proposed method of learning the optimal cost-to-go function using supervised learning differ from existing approaches like approximate dynamic programming (ADP) and reinforcement learning (RL)? What are the relative advantages and disadvantages?

2. The paper argues that by learning the cost-to-go function instead of directly learning the policy, the performance guarantees from the online optimization are retained. Can you elaborate on why this is the case? How does this specifically help with constraints and stability?

3. Sensitivity-based data augmentation is proposed to generate additional training data. Explain the key idea behind this approach and how the parametric sensitivity of the MPC problem is leveraged. What assumptions are needed for this to work?

4. What is the effect of using inexact/inaccurate data samples generated through data augmentation on the quality of the learned cost-to-go function? The paper provides an error bound - explain the bound and its implications.  

5. The choice of sampling strategy for collecting the initial offline data set is mentioned to be important. What are some commonly used sampling strategies and what are their pros and cons? How does data augmentation complement the sampling?

6. Under what conditions can the myopic MPC policy with approximate cost-to-go function recover the optimal policy exactly? The paper mentions some sufficient conditions - summarize those and explain why those conditions lead to optimality.

7. The proposition on stability with approximate cost-to-go function relies on a specific condition being satisfied. Explain what that condition is, why it ensures stability, and how it can be enforced in practice when designing the function approximator.

8. Contrast the ability of the proposed approach to handle changes in constraints/cost function online against approximate explicit MPC schemes. Why is the online optimization in myopic MPC more robust in this aspect?

9. The optimal actions can also be stored while generating the training data. How can this be leveraged to further accelerate the online computation of the myopic MPC policy?  

10. Instead of state-value pairs, the training process can also leverage state-action pairs from an expert policy. Briefly explain how this can be achieved using ideas like inverse optimization.
