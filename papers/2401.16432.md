# [Improving conversion rate prediction via self-supervised pre-training in   online advertising](https://arxiv.org/abs/2401.16432)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- The paper focuses on improving conversion rate (CVR) prediction models used for bid optimization in online advertising. Accurate CVR prediction is critical for advertisers to set the right bids to achieve their campaign goals.
- However, directly training CVR models suffers from data sparsity - clicks and attributed conversions used as labels for training are very rare compared to ad impressions. Adding more unlabeled data like conversions unattributed to clicks impairs model calibration/accuracy.

Proposed Solution:
- The paper proposes using self-supervised pre-training with an autoencoder model to learn meaningful representations from all conversion events, including unattributed ones. 
- The pre-trained encoder is used as a feature extractor for the main CVR prediction model, which is trained only on attributed conversions. This retains calibration without the data sparsity issue.

Key Contributions:
- Novel application of self-supervised pre-training to improve CVR prediction models, using additional unattributed conversion data without affecting calibration.
- Custom design decisions to integrate pre-training into the strict latency constraints of real-time bidding systems with factorization machine models and incremental training.
- Evaluation of autoencoder based on reconstruction loss, stability, generalization ability, etc. to ensure benefit for downstream CVR model.
- Demonstrated CVR model lift both offline during training and online in A/B tests, resulting in increased advertising revenue. The solution is now fully deployed.

In summary, the paper makes a valuable contribution in leveraging more data to improve conversion models via self-supervised pre-training, adapted specifically for real-time bidding systems through custom modeling and training techniques. The offline and online experiments validate the effectiveness of the approach.


## Summarize the paper in one sentence.

 This paper proposes using self-supervised pre-training with an autoencoder on all conversion events to generate additional features that improve the accuracy of click-to-conversion rate prediction models used for bidding in online advertising, while adapting the technique to the constraints of real-time auctions and incrementally trained factorization machines.


## What is the main contribution of this paper?

 Based on my reading of the paper, the main contributions are:

1. An application of self-supervised pre-training to utilize additional data from conversion events that a CVR model cannot access directly during training. Specifically, they pre-train an autoencoder on all conversion events, and then use the encoder's output as an additional feature for the main CVR prediction model, which is only trained on click-attributed conversions. This allows them to leverage non click-attributed conversions without impairing the calibration of the CVR model.

2. A set of techniques for training and using autoencoders with incrementally trained factorization machine variants that are used in ad ranking under strict latency constraints. This includes choices like the loss function, feature selection, model architecture, and incorporating nonlinear transformations of the encoder's output to boost expressive power.

So in summary, they adapted the idea of self-supervised pre-training to the specific application of conversion rate prediction in online advertising, with constraints and requirements unique to that setting. The offline and online experimental results demonstrate the effectiveness of their approach.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, here are some of the key terms and keywords associated with it:

- Conversion rate (CVR) prediction
- Online advertising
- Factorization machines
- Self-supervised pre-training 
- Autoencoders
- Incremental training
- Data sparsity
- Click attribution 
- Model calibration
- Latency constraints
- Native ads
- Cost per acquisition (CPA)
- Cost per thousand impressions (CPM)

The paper focuses on improving conversion rate prediction in online advertising by using self-supervised pre-training with autoencoders. This helps address the data sparsity issue for attribution models. The solutions are designed to work with factorization machine models under strict latency constraints. Key aspects include incremental training, model calibration, and optimizing for advertiser goals like CPA. The context is native advertising auctions at Yahoo.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in this paper:

1. The paper mentions that directly training the CVR model on non click-attributed conversions impairs model calibration. Why does this happen and how exactly does it impair calibration? 

2. The autoencoder uses an architecture with embeddings followed by MLP encoder and classifier-based decoder. Have the authors experimented with more advanced architectures like transformers? If so, what were the results? If not, how can transformers potentially help?

3. The paper uses stability and generalization metrics to evaluate the autoencoder. Are there other useful metrics that can provide more insight into the autoencoder's ability to learn meaningful representations? 

4. Random Fourier features are used to allow the CVR model to learn non-linear functions of the code. Are there other techniques besides kernels that can achieve this? How do they compare to RFFs?

5. The encoder only uses high-level taxonomy features of the ad. Does incorporating more fine-grained ad features improve performance despite needing more inferences per auction? 

6. How exactly does the projector network proposed in the future work help squeeze more performance out of the encoder? What are some projector architectures that strike a good balance between performance and inference time?

7. Can the autoencoder framework be extended to a seq2seq model that takes histories of user actions as input? Would the resulting representations contain more useful signals?

8. The paper uses an online learning approach. How do the stability results extend to a batch training methodology? Are there other benefits batch training can provide?

9. Beyond model architecture changes mentioned in the future work, how can advances in self-supervised learning like masked autoencoders and Barlow twins be adapted to this framework?

10. The autoencoder pre-training provides a fixed signal to the CVR model. Can we make it an adaptive signal by continuing to train the autoencoder online while training the CVR model?
