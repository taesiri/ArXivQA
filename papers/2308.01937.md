# [Training Data Protection with Compositional Diffusion Models](https://arxiv.org/abs/2308.01937)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is:How can we train diffusion models on disjoint datasets and compose them to match the performance of a model trained on their union, while enabling various forms of data protection?The key ideas and contributions are:- Deriving a closed-form expression to compose the reverse flows of diffusion models trained on disjoint data distributions. This allows generating samples by combining models without loss of performance.- Using prompt tuning to efficiently train compartmentalized diffusion models, avoiding the need to train separate full models. - Demonstrating selective forgetting, continual learning, and differential privacy via compartmentalization.- Quantifying the contribution of individual data sources to generated samples.- Empirically showing that the compartmentalized model can match the performance of a model trained on the combined data, owing to the diffusion modeling objective and use of a safe training set.So in summary, the paper introduces compartmentalized diffusion models as a way to decompose diffusion models into components that can be composed to enable various forms of data protection and attribution, while maintaining generative performance.
