# [Rethinking and Simplifying Bootstrapped Graph Latents](https://arxiv.org/abs/2312.02619)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a high-quality paragraph summarizing the key points of the paper:

This paper provides both empirical and theoretical analysis to uncover the underlying success factors and potential redundancies in the Bootstrapped Graph Latent (BGRL) framework for graph contrastive learning without negative samples. Through experiments, the authors find that graph augmentations and the predictor module are crucial for BGRL's effectiveness, while components like distinct augmentations, exponential moving average update, and asymmetric architecture provide negligible benefits. Theoretically, they reveal BGRL's predictor implicitly assists in instance-level decorrelation of node representations, which is key for producing discriminative embeddings. However, learning the predictor parameters can cause slow convergence. Hence, the authors propose to estimate the predictor directly from the covariance matrix of representations without extra parameters. Based on these findings, they design a simplified framework called SGCL that only requires a single graph augmentation, encoder, and inferential predictor per iteration to maximize similarity of representations from consecutive iterations. Extensive experiments demonstrate SGCL achieves competitive performance to BGRL on various benchmarks with significantly fewer parameters, lower memory and time costs, and faster convergence speed. The simplification and analysis provide valuable insights on the success factors of contrastive learning on graphs.
