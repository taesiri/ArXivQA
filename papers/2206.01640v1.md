# [PROMISSING: Pruning Missing Values in Neural Networks](https://arxiv.org/abs/2206.01640v1)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question/hypothesis appears to be: 

Can neural networks effectively handle missing data without relying on data imputation techniques? The authors propose and evaluate a method called PROMISSING that directly handles missing values in neural networks by learning to "prune" the missing values and replace them with learned neutralizers rather than imputing them. Their key hypothesis seems to be that PROMISSING can compete with or outperform standard imputation techniques for handling missing data in neural networks. The experiments on simulated and real-world data are designed to evaluate whether PROMISSING achieves similar or better performance compared to various imputation techniques across different data settings.

In summary, the main research question is whether their proposed PROMISSING method can effectively handle missing values in neural networks without needing to impute missing data, which is the standard approach currently used. Their experiments aim to demonstrate that PROMISSING can achieve competitive performance with imputation techniques across various data scenarios.


## What is the main contribution of this paper?

 The main contribution of this paper is proposing PROMISSING, a novel technique for handling missing values in neural networks without imputing them. The key ideas are:

- Replace missing values with "neutralizers" that prune the effect of missing inputs on neuron activations. This allows neural networks to handle missing values without changing the activations or needing to impute data.

- Learn a problem-specific numerical representation for unknowns (missing values) rather than imputing them with arbitrary values. 

- Models trained with PROMISSING become more indecisive and cautious when facing samples with many missing values, similar to human decision making.

- PROMISSING adds minimal overhead and can be easily incorporated into existing neural network code.

The authors show through experiments on simulated and real-world classification/regression datasets that PROMISSING performs competitively with various imputation techniques. A key advantage is flexibility with different missing data mechanisms and data types.

They also demonstrate an application of PROMISSING on a clinical dataset for psychosis prognosis, where the model trained with PROMISSING appropriately becomes more indecisive on patients with missing data. This could be important for trustworthy clinical decision support systems.

In summary, PROMISSING offers a simple yet effective approach for neural networks to handle missing values without imputation, with the advantages of modeling unknowns and calibrated decisiveness. The ease of incorporating PROMISSING into existing networks is also a notable contribution.
