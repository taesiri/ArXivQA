# [Sparse Attention with Linear Units](https://arxiv.org/abs/2104.07012v2)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the key research questions and hypotheses addressed in this paper are:- Can we achieve sparsity in attention naturally and efficiently by replacing the softmax activation with a rectified linear unit (ReLU)? The authors hypothesize that ReLU will inherently induce sparsity as it drops negative activations.- Can such a rectified linear attention (ReLA) model achieve comparable performance to standard softmax attention and other sparse attention methods on machine translation tasks? The authors hypothesize it can.- Will ReLA have advantages over other sparse attention methods like sparsemax and entmax in terms of efficiency and flexibility? The authors hypothesize it will require no specialized operations for inference unlike softmax variants and allow more flexible behavior like null attention.- Will the induced sparse attention patterns from ReLA provide useful linguistic insights? The authors hypothesize the sparsity will increase interpretability and the ability to analyze information flow.In summary, the key hypothesis is that replacing softmax with ReLU can achieve sparse yet flexible attention in an efficient way, maintaining strong performance while improving interpretability. The authors test this on machine translation tasks and analyze the induced attention patterns.


## What is the main contribution of this paper?

The main contribution of this paper is proposing a novel sparse attention mechanism called Rectified Linear Attention (ReLA). The key points are:- ReLA replaces the softmax activation in standard attention with ReLU, which induces sparsity in the attention weights. This allows the model to automatically learn sparse attention patterns. - ReLA adds normalization (RMSNorm) and an initialization scheme or gating to stabilize training. This results in an easy-to-implement and efficient drop-in replacement for standard softmax attention.- Experiments on machine translation tasks show ReLA achieves comparable performance to strong baselines, while having similar training and decoding speed as standard attention.- Analysis shows ReLA delivers high sparsity rates, head diversity, and better alignment to source words than other sparse attention variants. - Uniquely, ReLA allows "null" attention where a head assigns zero scores to all context items. This leads to highly specialized heads and the null rate shows potential to indicate translation quality.In summary, the key contribution is proposing the simple but effective ReLA mechanism for sparse and flexible attention, with both modeling and computational benefits over prior sparse attention approaches.
