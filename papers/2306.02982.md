# [PolyVoice: Language Models for Speech to Speech Translation](https://arxiv.org/abs/2306.02982)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the main research question is:How can we develop an effective speech-to-speech translation (S2ST) system that can handle both written and unwritten languages and also preserve the voice characteristics of the original speaker?The key points are:- The authors propose a novel framework called PolyVoice that consists of two main components:  - A speech-to-unit translation (S2UT) front-end that converts speech in the source language to semantic units in the target language.  - A unit-to-speech (U2S) back-end that converts the target units into speech while preserving the source speaker's voice.- The S2UT component uses a cross-lingual language model called U-XLM that translates between semantic units extracted from speech in an unsupervised way. This allows the system to handle unwritten languages.- The U2S component leverages the VALL-E X method to do in-context learning and clone the speaker's voice using the source speech and target units as context.- Overall, PolyVoice aims to develop an S2ST system that works for both written and unwritten languages, and also preserves voice characteristics, unlike previous pipeline systems. The key hypothesis is that the proposed architecture with its two language model components can achieve these goals.In summary, the main research question is how to develop an effective S2ST system for written and unwritten languages while preserving voice quality, which the authors hypothesize can be achieved through the proposed PolyVoice framework. The paper presents experiments and results to evaluate this hypothesis.


## What is the main contribution of this paper?

The main contributions of this paper are:1. Proposing a language model-based framework for speech-to-speech translation (S2ST) consisting of a translation language model and a speech synthesis language model. 2. Using unsupervised discretized speech units instead of phonemes as the intermediate representation between source and target speech. This allows the framework to handle unwritten languages.3. Adopting the VALL-E X approach for the speech synthesis component to build a unit-based audio language model, enabling voice cloning and preservation of speaking style.4. Evaluating the system on Chinese -> English and English -> Spanish translation tasks, showing improved performance over previous systems in terms of BLEU score, ASV score, and naturalness.5. Demonstrating the ability to handle translation between a written source language (English) and an unwritten target language (Spanish) without any Spanish text data.In summary, the key innovation is proposing a fully language model-based S2ST framework that leverages unsupervised semantic units for cross-lingual speech translation and voice cloning, applicable even for unwritten languages. The experiments validate the effectiveness of this approach.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

The paper proposes PolyVoice, a new language model-based framework for speech-to-speech translation that uses discrete semantic units as an intermediate representation between the source and target speech. The key idea is to use a decoder-only language model for translation and an autoregressive language model for speech synthesis while preserving speaker voice. The main advantage is the ability to handle unwritten languages by using unsupervised semantic units.


## How does this paper compare to other research in the same field?

This paper presents a novel language model-based framework for speech-to-speech translation using discrete semantic units. Here are some key ways it compares to other research in this field:- Most prior work in speech-to-speech translation uses an encoder-decoder architecture, while this paper proposes using only a decoder model (U-XLM). This is inspired by recent advances in language modeling showing the power of decoder-only models like GPT. The results show their decoder-only model outperforms an encoder-decoder baseline.- The use of discrete semantic units from self-supervised models like HuBERT is becoming more common in speech translation tasks. However, this paper shows how these units can be integrated into a language modeling framework more seamlessly than prior approaches that relied on multi-task learning.- For speech synthesis, this paper adapts the in-context learning approach of VALL-E/VALL-E X to use unsupervised semantic units rather than phonemes. This is a novel way of applying these types of models to potentially lower-resource or unwritten languages.- The paper demonstrates the approach on both written (Chinese-English) and unwritten (English-Spanish) language pairs. Showing strong performance on an unwritten setup is still relatively unique in speech translation papers.- In addition to standard metrics like BLEU, the paper evaluates speaker similarity (ASV) and naturalness to show their model can preserve voice characteristics well. Assessing these prosodic/speaker aspects is an active area of research.Overall, the key novelties seem to be the language model architectures for end-to-end speech translation and synthesis, the use of discrete units for unwritten languages, and the thorough evaluation. The results look promising and suggest this is a useful direction for speech translation compared to prior sequence-to-sequence style models.
