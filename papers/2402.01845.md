# [Multi-Armed Bandits with Interference](https://arxiv.org/abs/2402.01845)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- The paper studies the problem of experimentation (A/B testing) with interference, where the outcome of one unit depends on the treatments assigned to other units. This violates the common assumption of no interference. 
- Specifically, it introduces the problem of Multi-Armed Bandits with Interference (MABI), which considers the cumulative rewards over a time horizon. There are $N$ units arranged on a grid, $T$ rounds, and $k$ treatment arms.
- The reward of each unit in each round depends on the assignments of all units, with a decaying interference assumption - namely the interference effect decays with spatial distance between units.
- The goal is to maximize cumulative rewards by judiciously assigning treatments over the $T$ rounds. This is challenging since the reward functions can arbitrarily vary across units and rounds.

Proposed Solution:
- First, it shows switchback policies (which assign all units the same arm in each round) have optimal $\tilde{O}(\sqrt{T})$ expected regret. However, they suffer high variance. 
- To address this, a novel policy is proposed that integrates ideas from adversarial bandits (EXP3-IX algorithm) and causal inference (clustered randomization). Key ideas involve:
  - A Robust Randomized Partition to increase worst-case exposure probabilities. This helps reduce estimator variance.
  - A Horvitz-Thompson-IX estimator that unifies estimators from EXP3-IX and Horvitz-Thompson while allowing heterogeneity.
  - Analysis via concentration inequalities to bound and control variations.

Main Contributions:
- Formulates a novel and challenging MABI problem that connects adversarial bandits and experimentation under interference.
- Provides optimal expected regret policy and lower bound that expected regret cannot improve by considering multiple units. 
- Develops a policy whose regret (i) is optimal in expectation (ii) has high probability bound vanishing in number of units $N$. In contrast, regret of any switchback policy does not vanish in $N$.

The main innovation is in developing techniques to control and leverage variability arising from interference and non-stationarity, while retaining optimality guarantees. This helps bridge the theory and practice of experimentation.


## Summarize the paper in one sentence.

 This paper introduces the problem of Multi-Armed Bandits with Interference (MABI), where the reward of each experimental unit depends on the treatments assigned to all units with a decaying influence in spatial distance, and presents policies that achieve optimal expected regret as well as high-probability regret bounds that vanish with the number of units.


## What is the main contribution of this paper?

 Based on my understanding, the main contributions of this paper are:

1. It introduces a new problem formulation called Multi-Armed Bandits with Interference (MABI), which bridges the field of experimentation under interference and online learning. The key features are: (i) it assumes the unit-to-unit interference level decays in distance, capturing settings like ride-sharing platforms; (ii) it uses a very general setup allowing reward functions to be adversarial and non-stationary.

2. It shows that the minimax expected regret for MABI is Θ(√T), matching standard adversarial bandits without interference. Specifically:
(a) Any switchback policy induced by an adversarial bandits policy achieves O(√T) expected regret. 
(b) There exist MABI instances on which any policy suffers Ω(√T) expected regret.

3. It proposes a policy based on EXP3-IX and clustered randomization that achieves: (i) O(√T) expected regret, and (ii) high-probability regret bounds that vanish as the number of units N grows large. In contrast, the regret bounds for switchback policies do not vanish in N.

In summary, the paper introduces a novel problem bridging experimentation and online learning under interference, establishes fundamental limits on the expected regret, and provides an algorithm with vanishing high-probability regret. The policy uses some nice ideas like a robust randomized partition of units and an estimator interpolating between Horvitz-Thompson and EXP3-IX.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, here are some of the key terms and keywords that seem most relevant:

- Multi-armed bandits (MAB)
- Interference
- Regret bounds
- Expected regret
- High probability regret bounds
- Switchback policies
- Decaying interference property (DIP)
- Spatial interference
- Cluster randomization
- Horvitz-Thompson (HT) estimator
- Implicit exploration (IX)
- Batch policy
- Adversarial bandits

The paper introduces the problem of "Multi-Armed Bandits with Interference" (MABI), which connects the fields of online learning/MAB and causal inference with interference. Key aspects include analyzing regret bounds in expectation and high probability, using ideas like switchback policies, the decaying interference property, cluster randomization, and modified estimators like the HT-IX estimator. The regret is compared against the best fixed-arm policy. Overall, it provides regret analyses for a batched MAB problem with spatially decaying interference between units.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 potential in-depth questions about the method proposed in this paper:

1. The paper proposes a Horvitz-Thompson-IX (HT-IX) estimator that combines ideas from the EXP3-IX algorithm and the Horvitz-Thompson estimator for interference problems. How exactly does the HT-IX estimator balance variance reduction and bias introduced by the IX parameter?

2. The robust randomized partition (RRP) is introduced to ensure a lower bound on the exposure probabilities. How does the RRP construction guarantee a lower bound and why is having a good lower bound on exposure probabilities important?

3. The regret analysis decomposes regret into three terms - bounding $R-\hat{R}$, $\hat{R}-\hat{R}_a$, and $\hat{R}_a-R_a$. What are the key steps and results used to bound each of these terms?

4. The high probability regret bound makes use of a checkerboard 9-coloring to induce negative correlation between estimator terms across clusters. Explain why this coloring ensures negative correlation and why negative correlation is needed for the regret bound.  

5. How does the choice of parameters like the learning rate η, IX parameter β, and RRP parameters l and r impact the regret bound? What are good guidelines for setting these parameters?

6. The cluster-level randomization is crucial for obtaining a regret bound that vanishes with N. How exactly does the randomization methodology enable regret scaling with N? Compare to a simple Bernoulli randomized experiment.  

7. Discuss the connections and differences between the regret bounds for expected regret versus high probability regret. Why does considering high probability regret require more sophisticated analysis?

8. How does the decaying interference assumption enable efficient learning? What would happen without this or similar assumptions?

9. Compare the MABI setting to other related settings like batch bandits and networked bandits. What aspects of the problem and analysis carry over or need to be significantly modified?

10. Can you think of ways the analysis might be tightened further? What are the bottlenecks in converting the high probability bounds to exponential tail bounds?
