# [Attention Prompt Tuning: Parameter-efficient Adaptation of Pre-trained   Models for Spatiotemporal Modeling](https://arxiv.org/abs/2403.06978)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Fine-tuning pre-trained transformer models on video-based downstream tasks like action recognition leads to parameter inefficiency, high computational demands, and overfitting risks.
- Existing prompt tuning methods like Visual Prompt Tuning (VPT) require a large number of prompts for video tasks, increasing latency and FLOPs.

Proposed Solution:
- The paper introduces Attention Prompt Tuning (APT) which directly injects prompts into the keys and values of transformer attention. 
- This eliminates redundant computations on prompts compared to concatenating prompts with input tokens as in VPT.
- A novel prompt reparameterization based on scaling is also introduced to enhance robustness and accelerate convergence.

Key Contributions:
- APT reducesprompts required for video action recognition from 800 for VPT to 400 while achieving better performance.
- Computational complexity in terms of GFLOPs is lower for APT compared to VPT.
- Reparameterization makes training faster and less sensitive to hyperparameters.
- APT achieves state-of-the-art results on SSv2, UCF101 and HMDB51 video datasets compared to existing parameter-efficient methods like adapters and VPT.  
- With only 0.45% of tuned parameters, APT matches or exceeds full fine-tuning performance, demonstrating the method's efficiency.

In summary, the paper makes video-based transformer tuning more practical through attention prompt injection and reparameterization innovations that improve efficiency and performance.
