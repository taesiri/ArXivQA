# [Binary and Ternary Natural Language Generation](https://arxiv.org/abs/2306.01841)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central research question this paper addresses is: How can we quantize both the weights and activations of transformer models down to very low bits (ternary or binary) while still maintaining good performance on natural language generation tasks like summarization and translation?The key hypotheses appear to be:1) A combination of statistics-based weight quantization and learned activation quantization can enable stable training of low-bit generative transformers, which was not previously possible.2) With this approach, they hypothesize they can quantize transformer encoder-decoder models down to fully ternary or binary while achieving reasonable accuracy on summarization and translation benchmarks.3) They further hypothesize their method can outperform prior work on mildly quantized (8-bit) generative transformers and set new state-of-the-art results.So in summary, the main research question is how to quantize generative transformers to very low bits, and the key hypotheses are around using a combined weight and activation quantization approach to achieve this effectively for the first time.
