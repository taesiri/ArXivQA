# [Binary and Ternary Natural Language Generation](https://arxiv.org/abs/2306.01841)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central research question this paper addresses is: How can we quantize both the weights and activations of transformer models down to very low bits (ternary or binary) while still maintaining good performance on natural language generation tasks like summarization and translation?The key hypotheses appear to be:1) A combination of statistics-based weight quantization and learned activation quantization can enable stable training of low-bit generative transformers, which was not previously possible.2) With this approach, they hypothesize they can quantize transformer encoder-decoder models down to fully ternary or binary while achieving reasonable accuracy on summarization and translation benchmarks.3) They further hypothesize their method can outperform prior work on mildly quantized (8-bit) generative transformers and set new state-of-the-art results.So in summary, the main research question is how to quantize generative transformers to very low bits, and the key hypotheses are around using a combined weight and activation quantization approach to achieve this effectively for the first time.


## What is the main contribution of this paper?

The main contributions of this paper are:- It proposes a novel combination of statistics-based weight quantization and learning-based activation quantization to enable training transformer encoder-decoder models like BART in very low-bit settings (ternary and binary). This was not possible before.- It significantly advances the state-of-the-art for low-bit text generation models. The authors demonstrate the first non-trivial fully ternary and fully binary transformer models for summarization and translation. - For commonly used settings like ternary weights + 8-bit activations, their method outperforms prior SOTA by up to 2.3 ROUGE points on summarization and 1.2 BLEU points on translation.- Their binary weight models also exceed or match ternary weight SOTA, while being even more efficient.- Overall, the paper shows for the first time that transformer-based text generation models can be quantized to very low bits without catastrophic accuracy loss. This has implications for efficiently deploying large generative models.In summary, the key contribution is a novel quantization scheme that for the first time enables training competitive ultra low-bit (ternary/binary) transformer models for text generation tasks like summarization and translation. The proposed models significantly advance efficiency and accuracy over prior work.
