# [LORS: Low-rank Residual Structure for Parameter-Efficient Network   Stacking](https://arxiv.org/abs/2403.04303)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Deep learning models often employ numerous stacked structures which have identical architectures and perform similar functions, such as the decoders in Transformers. 
- While stacking enhances model capacity, it also leads to a substantial increase in parameters, posing challenges for practical applications.

Proposed Solution: 
- The paper proposes LORS (Low-Rank Residual Structure) to reduce parameters in stacked structures while maintaining performance.
- LORS decomposes the parameters in each module into shared parameters capturing commonality across modules and private parameters capturing module-specific characteristics.
- Shared parameters are trained jointly by all modules while private parameters are owned separately by each module.  
- This is achieved by having the private parameter matrix be a low-rank adaptation to the shared parameter matrix, inspired by LoRA.

Main Contributions:
- Proposes LORS, which enables stacked modules to share most parameters while retaining only a small number of unique ones per module to match or exceed standalone performance.
- Introduces methodologies to reduce both static and adaptively generated parameters in stacked structures.
- Validates LORS by applying it to stacked decoders of AdaMixer object detector, reducing decoder parameters by 70% while maintaining or improving performance over standalone AdaMixer.
- Demonstrates versatility of LORS across tasks (detection), models (AdaMixer), backbones (ResNet, Swin), and components (decoders).
- Shows potential for LORS to facilitate implementation of large models affected by excessive stacking-related parameters.

In summary, the paper makes stacked structures more parameter-efficient by exploiting commonalities across stacked modules via shared low-rank parameter matrices. Extensive experiments demonstrate LORS enables using substantially fewer parameters to match or exceed standalone model performance.
