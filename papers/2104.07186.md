# [COIL: Revisit Exact Lexical Match in Information Retrieval with   Contextualized Inverted List](https://arxiv.org/abs/2104.07186)

## What is the central research question or hypothesis that this paper addresses?

 Based on my understanding, the central hypothesis of this paper is that lexical exact match systems can be substantially improved by incorporating contextualized representations from deep language models. 

Specifically, the paper proposes that using contextualized representations of tokens, instead of surface form matching and heuristic scoring, can help address semantic mismatch issues in traditional lexical retrieval systems like BM25. The paper introduces a new retrieval architecture called COIL (COntextualized Inverted List) that encodes both query and document tokens with contextualized representations from a pretrained language model. It then performs efficient retrieval by matching query and document tokens that have the exact same surface form, but using vector similarities between their contextualized representations rather than heuristic scoring.

The central hypothesis is that by introducing contextualized representations into the lexical exact match retrieval framework, COIL can capture more semantic matching signals while retaining the efficiency benefits of inverted indexes and surface form matching. The paper aims to show that this approach can substantially improve over both classic lexicalretrieval systems as well as recent dense retrievers based on global text embeddings.

In summary, the key hypothesis is that lexical retrieval can be greatly improved by incorporating contextualization, and COIL is proposed as a way to efficiently achieve this while preserving efficient surface form matching. Experiments on standard IR benchmarks aim to validate that COIL outperforms both traditional lexical and recent neural retrieval systems.


## What is the main contribution of this paper?

 Based on my understanding, the main contribution of this paper is proposing a new contextualized exact lexical match retrieval architecture called COIL (COntextualized Inverted List). 

Specifically, the key contributions are:

- Introducing the idea of using contextualized representations from deep language models like BERT to enable semantic-aware token matching in lexical retrieval systems. This helps address the issue of semantic mismatch in traditional lexical matching models like BM25.

- Proposing the COIL architecture that stores contextualized token representations in inverted lists and performs efficient search through lexical exact match. This brings together the benefits of semantic matching from deep LMs and efficiency of inverted index retrieval.

- Demonstrating through experiments on MSMARCO passage and document ranking tasks that COIL outperforms both classical lexical retrievers like BM25 and state-of-the-art dense retrievers. The results show the potential of improving lexical retrieval with contextualization.

- Analyzing the tradeoffs between effectiveness and efficiency with different vector dimensions in COIL. Lower dimensionality representations are shown to retain most of COIL's gains while speeding up retrieval.

- Providing qualitative analysis showing COIL's ability to match tokens based on context and handle lexical variation.

Overall, the main novelty of the paper is in proposing contextualized inverted lists for efficient semantic matching in lexical retrieval systems. The results indicate this is a promising direction to build efficient and effective next-generation semantic search systems.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the paper: 

The paper proposes a new retrieval architecture called COIL that uses contextualized representations from deep language models to enable efficient semantic matching between query and document terms within an inverted index retrieval framework.


## How does this paper compare to other research in the same field?

 Here are a few key points on how this paper compares to other related work:

- The paper focuses on improving lexical matching in information retrieval by using contextualized representations. Most prior work has focused on going beyond lexical matching entirely by using neural soft-matching models. This paper explores improving lexical matching itself with contextualization.

- Existing lexical matching models like BM25 rely on exact surface form matching and heuristic term weighting schemes like TF-IDF. This paper replaces those with contextualized token representations from BERT to capture semantic similarity.

- Prior work has augmented lexical models like BM25 with contextualized expansions (e.g. DeepCT, DocT5Query). This paper directly replaces term statistics in inverted lists with contextualized vectors. 

- Compared to dense retrievers like DPR, this paper preserves lexical matching signals while adding contextualization. DPR uses a single CLS vector losing token interactions.

- The proposed COIL model is more efficient than soft-matching models like ColBERT that compare all combinations of query and document tokens. COIL only matches on overlapping tokens.

- The results show COIL outperforms both lexical and dense retrieval baselines, approaching the effectiveness of ColBERT soft-matching while being significantly more efficient to compute.

In summary, the key novelty is in improving lexical matching with contextualization while retaining efficiency, outperforming both traditional lexical and recent neural models. The results demonstrate the viability of this approach.


## What future research directions do the authors suggest?

 Based on my reading of the paper, some potential future research directions the authors suggest include:

- Improving the efficiency and scalability of the COIL system through optimizing the inverted index structure and retrieval algorithms. The paper mentions using approximate nearest neighbor search methods like Faiss to speed up retrieval from the dense contextualized representations in each inverted list.

- Exploring different contextualization functions beyond BERT, such as other transformer language models, to encode semantics.

- Studying the effect of dimension reduction and regularization in the contextualized representations more systematically. The authors found lower dimensions sometimes helped due to regularization effects.

- Incorporating query expansion and document expansion techniques to further improve vocabulary mismatch issues in COIL.

- Evaluating COIL on additional retrieval tasks and datasets beyond the MSMARCO passage and document collections used in the paper.

- Comparing with and integrating COIL with other sparse retrieval methods like ANCE and MeBERT that use multiple representations.

- Extending COIL to multilingual retrieval by using multilingual language models for contextualization.

- Developing unsupervised or weak supervision methods to train COIL without requiring large amounts of human relevance judgments.

- Combining COIL with traditional feature-based models like BM25 in an ensemble system.

- Exploringtransformer models beyond BERT as the contextualization function.

So in summary, the main future directions are improving efficiency and scalability of COIL, exploring different contextualization models and representations, and evaluating the approach on more tasks and datasets.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the paper:

The paper presents COIL (COntextualized Inverted List), a new retrieval architecture that brings semantic matching into lexical IR systems. COIL replaces the term statistics in traditional inverted lists with contextualized vector representations of tokens to enable more effective search. Specifically, COIL encodes both query and document tokens into contextualized vectors using a Transformer language model like BERT. These token representations are grouped by surface form into inverted lists. At query time, COIL looks up the inverted lists for the query tokens and computes vector similarities between query and document tokens as relevance scores. This allows COIL to do fast retrieval focused on documents with overlapping tokens, while also capturing semantic matching between tokens using contextualized representations. Experiments on passage and document ranking benchmarks show COIL substantially outperforms lexical retrievers and dense retrievers, with performance approaching an all-to-all soft match retriever like ColBERT. The results demonstrate the potential of improving lexical retrieval with contextualization while retaining efficiency.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the key points from the paper:

The paper presents COIL, a new contextualized exact match retrieval architecture that brings semantic matching into traditional lexical retrieval systems. COIL encodes both query and document tokens into contextualized vector representations using a transformer language model like BERT. At index time, the contextualized representations for each document token are grouped by surface form token into inverted lists. At query time, contextualized representations are generated for each query token and used to retrieve from the inverted lists, computing vector similarity for only overlapping query-document tokens. This allows efficient search while overcoming issues like semantic mismatch that limit traditional lexical retrieval. 

Experiments on the MSMARCO dataset show COIL significantly outperforms both classical lexical retrieval systems like BM25 and state-of-the-art dense retrievers. The token-level contextualized matching provides fine-grained signals that improve ranking precision. Adding an additional CLS vector match further improves vocabulary coverage. The resulting system approaches the effectiveness of computationally expensive all-to-all soft match models like ColBERT, while retaining efficient inverted index search. The paper demonstrates the potential for lexical retrieval enhanced with contextualized representations.


## Summarize the main method used in the paper in one paragraph.

 Based on my reading of the paper, the main method used is as follows:

The paper proposes a new retrieval architecture called COntextualized Inverted List (COIL). COIL aims to bring semantic matching into traditional lexical retrieval systems like BM25. It does so by replacing the heuristic term frequency scores used in systems like BM25 with vector similarities between contextualized representations of overlapping query and document terms. 

Specifically, COIL encodes both query and document tokens into contextualized vector representations using a transformer language model like BERT. At indexing time, the document token vectors are stored in inverted lists grouped by surface term. At query time, query tokens are encoded and used to look up their corresponding inverted lists. The similarity between the query token vectors and the document vectors in the lists is computed as the matching score. The scores are summed over the overlapping query and document terms to produce the final ranking.

Additionally, COIL can compute a similarity between [CLS] sentence representations of the query and documents to account for vocabulary mismatch between queries and relevant documents. The token matching scores and CLS matching score are combined for the final ranking.

So in summary, COIL brings semantic matching into fast inverted index retrieval by using contextualized representations of tokens and computing vector similarities over inverted lists, while retaining efficiency through exact term matching.


## What problem or question is the paper addressing?

 The paper is addressing the problem of semantic mismatch in classical information retrieval systems that rely on exact lexical matching between query and document terms. The key questions it seeks to answer are:

1) Can we improve exact lexical matching systems by incorporating contextualized representations to handle semantic mismatch? 

2) Can an efficient retrieval architecture be built using contextualized inverted lists that enables both semantic matching and fast retrieval through inverted index lookup?

3) How does the performance of contextualized exact matching compare with state-of-the-art neural ranking models that use soft/dense matching?

Specifically, the paper proposes a new retrieval architecture called COntextualized Inverted List (COIL) that brings semantic matching into lexical retrieval systems. Instead of heuristic term statistics, COIL uses vector similarities between query and document tokens' contextualized representations for scoring. This allows it to differentiate the same surface term in different contexts and address semantic mismatch issues in traditional lexical systems. The paper shows COIL significantly outperforms classical lexical retrievers and competitive or better than state-of-the-art dense retrievers while retaining efficiency through inverted index lookup.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper abstract, some of the key terms and concepts are:

- Classical information retrieval systems: The paper refers to traditional search systems like BM25 that rely on exact lexical matching between query and document terms.

- Neural IR models: The paper discusses recent neural network based models for information retrieval that perform soft semantic matching between query and document terms.

- Computation efficiency: The paper notes that while neural models improve accuracy, they lose the efficiency of exact match systems like BM25. 

- Contextualized exact match: The paper proposes a new architecture called COIL that brings semantic matching into lexical retrieval by using contextualized representations from deep language models.

- Inverted lists: COIL stores contextualized token representations in inverted lists, enabling efficient retrieval through exact match while incorporating semantic matching.

- Evaluation: Experiments show COIL outperforms both classical lexical systems and state-of-the-art dense retrievers on passage and document ranking benchmarks.

In summary, the key ideas are bringing semantic matching into efficient inverted index retrieval via contextualized representations, and showing this COIL architecture beats both classical lexical and recent neural retrievers.
