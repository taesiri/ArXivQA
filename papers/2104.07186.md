# [COIL: Revisit Exact Lexical Match in Information Retrieval with   Contextualized Inverted List](https://arxiv.org/abs/2104.07186)

## What is the central research question or hypothesis that this paper addresses?

Based on my understanding, the central hypothesis of this paper is that lexical exact match systems can be substantially improved by incorporating contextualized representations from deep language models. Specifically, the paper proposes that using contextualized representations of tokens, instead of surface form matching and heuristic scoring, can help address semantic mismatch issues in traditional lexical retrieval systems like BM25. The paper introduces a new retrieval architecture called COIL (COntextualized Inverted List) that encodes both query and document tokens with contextualized representations from a pretrained language model. It then performs efficient retrieval by matching query and document tokens that have the exact same surface form, but using vector similarities between their contextualized representations rather than heuristic scoring.The central hypothesis is that by introducing contextualized representations into the lexical exact match retrieval framework, COIL can capture more semantic matching signals while retaining the efficiency benefits of inverted indexes and surface form matching. The paper aims to show that this approach can substantially improve over both classic lexicalretrieval systems as well as recent dense retrievers based on global text embeddings.In summary, the key hypothesis is that lexical retrieval can be greatly improved by incorporating contextualization, and COIL is proposed as a way to efficiently achieve this while preserving efficient surface form matching. Experiments on standard IR benchmarks aim to validate that COIL outperforms both traditional lexical and recent neural retrieval systems.


## What is the main contribution of this paper?

Based on my understanding, the main contribution of this paper is proposing a new contextualized exact lexical match retrieval architecture called COIL (COntextualized Inverted List). Specifically, the key contributions are:- Introducing the idea of using contextualized representations from deep language models like BERT to enable semantic-aware token matching in lexical retrieval systems. This helps address the issue of semantic mismatch in traditional lexical matching models like BM25.- Proposing the COIL architecture that stores contextualized token representations in inverted lists and performs efficient search through lexical exact match. This brings together the benefits of semantic matching from deep LMs and efficiency of inverted index retrieval.- Demonstrating through experiments on MSMARCO passage and document ranking tasks that COIL outperforms both classical lexical retrievers like BM25 and state-of-the-art dense retrievers. The results show the potential of improving lexical retrieval with contextualization.- Analyzing the tradeoffs between effectiveness and efficiency with different vector dimensions in COIL. Lower dimensionality representations are shown to retain most of COIL's gains while speeding up retrieval.- Providing qualitative analysis showing COIL's ability to match tokens based on context and handle lexical variation.Overall, the main novelty of the paper is in proposing contextualized inverted lists for efficient semantic matching in lexical retrieval systems. The results indicate this is a promising direction to build efficient and effective next-generation semantic search systems.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper: The paper proposes a new retrieval architecture called COIL that uses contextualized representations from deep language models to enable efficient semantic matching between query and document terms within an inverted index retrieval framework.


## How does this paper compare to other research in the same field?

Here are a few key points on how this paper compares to other related work:- The paper focuses on improving lexical matching in information retrieval by using contextualized representations. Most prior work has focused on going beyond lexical matching entirely by using neural soft-matching models. This paper explores improving lexical matching itself with contextualization.- Existing lexical matching models like BM25 rely on exact surface form matching and heuristic term weighting schemes like TF-IDF. This paper replaces those with contextualized token representations from BERT to capture semantic similarity.- Prior work has augmented lexical models like BM25 with contextualized expansions (e.g. DeepCT, DocT5Query). This paper directly replaces term statistics in inverted lists with contextualized vectors. - Compared to dense retrievers like DPR, this paper preserves lexical matching signals while adding contextualization. DPR uses a single CLS vector losing token interactions.- The proposed COIL model is more efficient than soft-matching models like ColBERT that compare all combinations of query and document tokens. COIL only matches on overlapping tokens.- The results show COIL outperforms both lexical and dense retrieval baselines, approaching the effectiveness of ColBERT soft-matching while being significantly more efficient to compute.In summary, the key novelty is in improving lexical matching with contextualization while retaining efficiency, outperforming both traditional lexical and recent neural models. The results demonstrate the viability of this approach.
