# [Memory-Efficient Differentiable Transformer Architecture Search](https://arxiv.org/abs/2105.14669)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question is: 

How can we perform differentiable architecture search (DARTS) with Transformers in a memory-efficient way for sequence-to-sequence tasks?

The key points are:

- DARTS shows promise for neural architecture search, but is memory intensive when applied to Transformers due to storing many intermediate activations during searching. 

- The authors propose using multi-split reversible networks as the backbone for DARTS with Transformers. This allows reconstructing activations during backpropagation, reducing memory usage.

- They devise a backpropagation-with-reconstruction algorithm to enable gradient calculation while reconstructing inputs. 

- Experiments show their proposed "DARTSformer" searches larger Transformer architectures under memory constraints and achieves strong results on machine translation tasks.

So in summary, the main research question is how to make DARTS memory-efficient for Transformer architecture search on sequence tasks, which they address through reversible networks and a custom backpropagation algorithm.


## What is the main contribution of this paper?

 The main contribution of this paper is proposing a memory-efficient differentiable architecture search method for sequence-to-sequence tasks. Specifically:

- They propose a multi-split reversible network as the backbone, which enables reconstructing the intermediate layer outputs from the top layer's output. This significantly reduces memory usage during architecture search.

- They devise a backpropagation-with-reconstruction algorithm to calculate gradients and reconstruct layer inputs simultaneously. 

- They combine the reversible network with differentiable architecture search (DARTS) by designing mixed operation nodes in each split. This results in an end-to-end architecture search method called DARTSformer.

- They validate DARTSformer on machine translation tasks. The found architecture consistently outperforms standard Transformers across different dataset and model sizes. It achieves comparable performance to Evolved Transformers with much lower search cost.

In summary, the key contribution is proposing a memory-efficient way to enable differentiable architecture search in sequence-to-sequence models like Transformers. The reversible network backbone and associated algorithms relieve the memory burden and allow searching larger models.


## How does this paper compare to other research in the same field?

 Here are a few key points about how this paper compares to other research in neural architecture search:

- The paper focuses on applying neural architecture search to sequence-to-sequence models like Transformers. Much existing NAS research has focused on convolutional networks for image tasks, so exploring NAS for sequence models is still relatively new. 

- The main contribution is a memory-efficient approach to enable differentiable NAS with Transformers. Standard DARTS has high memory costs when applied to Transformers due to storing the outputs of all candidate operations. The proposed reversible network backbone reduces this memory burden.

- Compared to other NAS methods for sequence models like reinforcement learning or evolution, this approach is much more computationally efficient. The search cost is orders of magnitude lower than methods like the Evolved Transformer.

- The found architectures consistently outperform standard Transformers across multiple translation datasets. This helps demonstrate the generalization ability of the NAS method beyond just the search task.

- The improvements are more pronounced at smaller model sizes. At big model sizes, the performance gap narrows between the NAS models and standard Transformers. This suggests data augmentation or other regularization may help further.

Overall, the paper makes nice progress in enabling efficient and effective NAS for sequence-to-sequence learning. The reversible modeling approach helps make DARTS feasible for Transformers. There's still room to improve over standard Transformers, especially as model size increases. More work can be done to push the improvements from NAS in large-scale settings.


## What future research directions do the authors suggest?

 Based on my reading of the paper, some of the main future research directions suggested by the authors include:

- Exploring the use of their reversible network architecture and differentiable architecture search method on more tasks beyond sequence-to-sequence. They mention their approach is generic and can likely be applied to other network structures and tasks as well.

- Trying additional data augmentation techniques when training the larger models found through architecture search. The authors observe the performance gap between standard and searched models decreases at very large sizes, which they hypothesize could be due to overfitting effects that data augmentation could help with.

- Experimenting with different numbers and arrangements of reversible blocks in the architecture. The paper focuses on a simple 2-split encoder and 3-split decoder, but mentions architectures with more splits could be explored.

- Applying other memory saving techniques in conjunction with their approach, such as using localized attention mechanisms as in Reformer rather than standard full attention.

- Reducing the computational overhead during the search process from the input reconstruction needed for backpropagation. The authors note this makes the search slower but doesn't significantly hinder convergence - still this could be optimized further.

- Evaluating the impact of searching directly in the target model size rather than transferring architectures from smaller searches. The paper shows searching in larger sizes improves results, suggesting further optimization could be done.

In summary, the main directions are around expanding the approach to new tasks and network architectures, scaling up the search and models, and reducing the memory and computational costs of the search process even further.


## Summarize the paper in one paragraph.

 The paper proposes a memory-efficient differentiable architecture search (DARTS) method for sequence-to-sequence tasks. The key ideas are:

1) A multi-split reversible network is devised where the intermediate layer outputs can be reconstructed from the last layer, reducing memory usage. 

2) This reversible network is combined with DARTS by having each split contain an operation search node. A backpropagation-with-reconstruction algorithm is developed that only needs to store the last layer outputs.

3) This allows searching deeper networks with more candidate operations under the same memory constraint. The method is evaluated on machine translation tasks where it consistently outperforms standard Transformers, achieving the same performance as a big Transformer with 69% fewer parameters. It also exceeds Evolved Transformers with much lower search cost. The memory efficiency enables architecture search with large hidden sizes, leading to better final performance.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the paper:

The paper proposes a memory-efficient differentiable architecture search method called DARTSformer for sequence-to-sequence tasks. DARTSformer combines a multi-split reversible network with differentiable architecture search (DARTS). The multi-split reversible network allows intermediate layer outputs to be reconstructed from the last layer's output. This significantly reduces memory usage during architecture search compared to standard DARTS with Transformers, enabling larger hidden sizes and more operations to be searched over. A backpropagation-with-reconstruction algorithm is developed to enable gradient calculation during architecture search. 

Experiments are conducted on three machine translation tasks. DARTSformer finds architectures that consistently outperform standard Transformers, achieving comparable performance to “big” Transformers with 69% fewer parameters. DARTSformer also exceeds Evolved Transformers in performance while having orders of magnitude lower search cost. The method demonstrates strong generalization ability across tasks. Overall, DARTSformer enables memory-efficient neural architecture search for sequence-to-sequence problems.


## Summarize the main method used in the paper in one paragraph.

 The paper proposes a memory-efficient differentiable architecture search (DARTS) method for sequence-to-sequence tasks. The key ideas are:

1) They devise a multi-split reversible network, where the input can be reconstructed from the output. This avoids storing intermediate activations except for the last layer. 

2) They develop an algorithm for backpropagation with input reconstruction. The gradients can be calculated without storing intermediate outputs. 

3) They combine the reversible network with DARTS by using mixed operation search nodes in each split. This enables searching larger models under the same memory constraint.

4) They apply the method to search architectures for neural machine translation. The resulting architecture is evaluated on WMT English-German, English-French and English-Czech translation. It consistently outperforms standard Transformers across different model sizes.

In summary, the paper proposes a memory-efficient DARTS approach by using multi-split reversible networks. This allows transformer architecture search with limited memory resources. The searched architecture generalizes well to multiple translation tasks.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the paper:

The paper proposes a memory-efficient neural architecture search method called DARTSformer that combines a multi-split reversible network with differentiable architecture search to enable searching larger transformer models while reducing memory usage.


## What problem or question is the paper addressing?

 The main problem addressed in this paper is the high memory cost of applying differentiable architecture search (DARTS) to Transformers for sequence-to-sequence tasks. Specifically, the paper identifies two key issues:

1. Storing the outputs of all candidate operations in DARTS leads to high memory consumption when using Transformers as the backbone model. This limits the size of models that can be searched.

2. There is often a performance gap between the architectures found by searching in small sizes versus evaluating in large sizes. Searching directly in large sizes is infeasible due to memory constraints.

To address these issues, the paper proposes a memory-efficient DARTS method for sequence models based on multi-split reversible networks. The key ideas are:

- Using reversible networks allows reconstructing intermediate activations during backpropagation, reducing memory usage.

- A multi-split design further reduces memory by splitting activations into smaller chunks. 

- A backpropagation-with-reconstruction algorithm enables gradient calculation without storing intermediate outputs.

- This approach reduces the memory burden of DARTS, allowing larger models to be searched directly.

In summary, the paper aims to enable memory-efficient neural architecture search for sequence-to-sequence tasks using Transformers, by proposing a reversible network design that reconstructs activations during backpropagation. This allows larger models to be searched compared to standard DARTS.
