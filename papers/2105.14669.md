# [Memory-Efficient Differentiable Transformer Architecture Search](https://arxiv.org/abs/2105.14669)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is: How can we perform differentiable architecture search (DARTS) with Transformers in a memory-efficient way for sequence-to-sequence tasks?The key points are:- DARTS shows promise for neural architecture search, but is memory intensive when applied to Transformers due to storing many intermediate activations during searching. - The authors propose using multi-split reversible networks as the backbone for DARTS with Transformers. This allows reconstructing activations during backpropagation, reducing memory usage.- They devise a backpropagation-with-reconstruction algorithm to enable gradient calculation while reconstructing inputs. - Experiments show their proposed "DARTSformer" searches larger Transformer architectures under memory constraints and achieves strong results on machine translation tasks.So in summary, the main research question is how to make DARTS memory-efficient for Transformer architecture search on sequence tasks, which they address through reversible networks and a custom backpropagation algorithm.


## What is the main contribution of this paper?

The main contribution of this paper is proposing a memory-efficient differentiable architecture search method for sequence-to-sequence tasks. Specifically:- They propose a multi-split reversible network as the backbone, which enables reconstructing the intermediate layer outputs from the top layer's output. This significantly reduces memory usage during architecture search.- They devise a backpropagation-with-reconstruction algorithm to calculate gradients and reconstruct layer inputs simultaneously. - They combine the reversible network with differentiable architecture search (DARTS) by designing mixed operation nodes in each split. This results in an end-to-end architecture search method called DARTSformer.- They validate DARTSformer on machine translation tasks. The found architecture consistently outperforms standard Transformers across different dataset and model sizes. It achieves comparable performance to Evolved Transformers with much lower search cost.In summary, the key contribution is proposing a memory-efficient way to enable differentiable architecture search in sequence-to-sequence models like Transformers. The reversible network backbone and associated algorithms relieve the memory burden and allow searching larger models.


## How does this paper compare to other research in the same field?

Here are a few key points about how this paper compares to other research in neural architecture search:- The paper focuses on applying neural architecture search to sequence-to-sequence models like Transformers. Much existing NAS research has focused on convolutional networks for image tasks, so exploring NAS for sequence models is still relatively new. - The main contribution is a memory-efficient approach to enable differentiable NAS with Transformers. Standard DARTS has high memory costs when applied to Transformers due to storing the outputs of all candidate operations. The proposed reversible network backbone reduces this memory burden.- Compared to other NAS methods for sequence models like reinforcement learning or evolution, this approach is much more computationally efficient. The search cost is orders of magnitude lower than methods like the Evolved Transformer.- The found architectures consistently outperform standard Transformers across multiple translation datasets. This helps demonstrate the generalization ability of the NAS method beyond just the search task.- The improvements are more pronounced at smaller model sizes. At big model sizes, the performance gap narrows between the NAS models and standard Transformers. This suggests data augmentation or other regularization may help further.Overall, the paper makes nice progress in enabling efficient and effective NAS for sequence-to-sequence learning. The reversible modeling approach helps make DARTS feasible for Transformers. There's still room to improve over standard Transformers, especially as model size increases. More work can be done to push the improvements from NAS in large-scale settings.


## What future research directions do the authors suggest?

Based on my reading of the paper, some of the main future research directions suggested by the authors include:- Exploring the use of their reversible network architecture and differentiable architecture search method on more tasks beyond sequence-to-sequence. They mention their approach is generic and can likely be applied to other network structures and tasks as well.- Trying additional data augmentation techniques when training the larger models found through architecture search. The authors observe the performance gap between standard and searched models decreases at very large sizes, which they hypothesize could be due to overfitting effects that data augmentation could help with.- Experimenting with different numbers and arrangements of reversible blocks in the architecture. The paper focuses on a simple 2-split encoder and 3-split decoder, but mentions architectures with more splits could be explored.- Applying other memory saving techniques in conjunction with their approach, such as using localized attention mechanisms as in Reformer rather than standard full attention.- Reducing the computational overhead during the search process from the input reconstruction needed for backpropagation. The authors note this makes the search slower but doesn't significantly hinder convergence - still this could be optimized further.- Evaluating the impact of searching directly in the target model size rather than transferring architectures from smaller searches. The paper shows searching in larger sizes improves results, suggesting further optimization could be done.In summary, the main directions are around expanding the approach to new tasks and network architectures, scaling up the search and models, and reducing the memory and computational costs of the search process even further.


## Summarize the paper in one paragraph.

The paper proposes a memory-efficient differentiable architecture search (DARTS) method for sequence-to-sequence tasks. The key ideas are:1) A multi-split reversible network is devised where the intermediate layer outputs can be reconstructed from the last layer, reducing memory usage. 2) This reversible network is combined with DARTS by having each split contain an operation search node. A backpropagation-with-reconstruction algorithm is developed that only needs to store the last layer outputs.3) This allows searching deeper networks with more candidate operations under the same memory constraint. The method is evaluated on machine translation tasks where it consistently outperforms standard Transformers, achieving the same performance as a big Transformer with 69% fewer parameters. It also exceeds Evolved Transformers with much lower search cost. The memory efficiency enables architecture search with large hidden sizes, leading to better final performance.
