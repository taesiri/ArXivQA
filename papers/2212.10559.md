# Why Can GPT Learn In-Context? Language Models Implicitly Perform   Gradient Descent as Meta-Optimizers

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper addresses is:What is the working mechanism behind in-context learning (ICL) with large pretrained language models like GPT?Specifically, the authors aim to explain ICL as a process of meta-optimization and analyze the connections between ICL and explicit finetuning. Their central hypothesis is that ICL can be understood as implicit finetuning of the original pretrained model.In summary, the key research question is about explaining the inference mechanism behind in-context learning with large language models. The central hypothesis proposes that ICL works by implicitly finetuning the model parameters through a meta-optimization process during forward propagation.


## What is the main contribution of this paper?

The main contribution of this paper is proposing a novel perspective to understand the working mechanism of in-context learning (ICL) with large language models. Specifically:- It figures out a dual form between Transformer attention and gradient descent, and explains ICL as a meta-optimization process where the model produces meta-gradients and applies them through attention. - It analyzes connections between ICL and explicit finetuning, and proposes to understand ICL as implicit finetuning.- It provides empirical evidence from multiple perspectives to support the understanding of ICL as implicit finetuning.- Inspired by the dual form, it designs a momentum-based attention mechanism that outperforms vanilla attention. This further supports the understanding of meta-optimization and shows the potential to utilize this understanding for model design.In summary, the key contribution is explaining ICL with large language models as implicit meta-optimization and finetuning. This understanding is supported by both theoretical analysis and comprehensive experiments. The momentum-based attention designed based on this understanding also verifies its reasonability.
