# [Foundation Policies with Hilbert Representations](https://arxiv.org/abs/2402.15567)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
Finding scalable and effective ways to pre-train reinforcement learning policies in an unsupervised, offline manner remains an open challenge. Prior offline pre-training objectives like behavioral cloning, goal-conditioned RL, and unsupervised skill discovery have limitations in terms of data requirements, behavior diversity, or versatility for downstream tasks. 

Proposed Solution:
This paper proposes Hilbert Foundation Policies (HILPs), a novel unsupervised framework to pre-train policies that capture diverse, long-horizon behaviors from offline data. The key ideas are:

1) Learn a Hilbert state representation that preserves temporal distances between states. This representation maps temporally similar states to spatially similar latent states.

2) Train a latent-conditioned policy to optimally "span" the latent space using an inner product-based intrinsic reward. This results in behaviors that can move long distances in any direction.

3) The latent-conditioned policy can be efficiently adapted to new tasks in a zero-shot manner via prompting the policy with reward functions or goals. The inner product structure also enables test-time planning.

Main Contributions:

- Introduction of HILPs, a new versatile policy pre-training framework that subsumes prior offline GCRL methods and enables zero-shot RL.

- Using Hilbert representations to capture temporal structure and enabling efficient prompting strategies.

- Empirical demonstration that a single HILP model can outperform prior state-of-the-art methods specialized for zero-shot RL, offline GCRL, and hierarchical RL across 7 simulated robotic environments.

The main benefit of HILPs is efficiently learning a variety of long-horizon behaviors that facilitate rapid downstream task adaptation, without needing online data collection or additional training. The structured representations also enable test-time planning.
