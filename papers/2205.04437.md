# [Activating More Pixels in Image Super-Resolution Transformer](https://arxiv.org/abs/2205.04437)

## What is the central research question or hypothesis that this paper addresses?

 The main research goal of this paper is to design an image super-resolution transformer network that can better utilize long-range dependencies in the input image to reconstruct more accurate and better quality HR images. 

The key hypotheses are:

1) Existing transformer-based SR networks like SwinIR do not fully exploit long-range dependencies, utilizing only local patches rather than the full image context.

2) Combining channel attention and self-attention in a hybrid architecture can allow utilizing both global statistics and local dependencies for better SR. 

3) Adding an overlapping cross-attention module can enhance interactions between neighboring image patches/windows to reduce blocking artifacts.

4) Large-scale same-task pre-training is beneficial for unlocking the full potential of transformers for the SR task.

So in summary, the main goal is to design a hybrid attention transformer network for SR that can utilize global context better through channel attention, self-attention on larger windows, cross-window attention, and large-scale pre-training. The hypothesis is that this will allow capturing longer-range dependencies in the image to reconstruct higher quality HR images compared to existing methods.
