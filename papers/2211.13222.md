# [SVFormer: Semi-supervised Video Transformer for Action Recognition](https://arxiv.org/abs/2211.13222)

## What is the central research question or hypothesis that this paper addresses?

 The central research question this paper addresses is how to effectively utilize Transformer models for semi-supervised video action recognition. Specifically, the paper investigates:

- How to adapt semi-supervised learning methods like FixMatch to be suitable for video transformers instead of CNNs. 

- How to design effective data augmentation strategies tailored for video transformers in the semi-supervised setting, rather than using standard image augmentations.

The key hypotheses are:

- Video transformers can achieve strong semi-supervised action recognition performance despite lacking inductive biases like CNNs, if properly adapted.

- Novel token-level mixing like Tube TokenMix and temporal warping augmentations can better leverage the capabilities of transformers for semi-supervised video tasks compared to pixel-level mixing or spatial-only augmentations.

In summary, the paper explores how to unlock the potential of transformers for semi-supervised video action recognition via adapted training frameworks and customized video-specific data augmentations. The central goal is pushing the state-of-the-art in this domain by effectively leveraging transformers.
