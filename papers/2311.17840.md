# [A quasi-polynomial time algorithm for Multi-Dimensional Scaling via LP   hierarchies](https://arxiv.org/abs/2311.17840)

## What is the main contribution of this paper?

 Based on my understanding, the main contribution of this paper is an improved algorithm for the Kamada-Kawai formulation of multi-dimensional scaling (MDS). Specifically:

- The paper gives an algorithm that runs in quasi-polynomial time and achieves an approximation ratio of O(OPT^{1/k} log(Δ/ε)) + ε for the Kamada-Kawai objective, where OPT is the optimal cost, Δ is the aspect ratio of the input distances, k is the target dimension, and ε is the error parameter. 

- This improves upon prior work by Demaine et al. that gave a polynomial time approximation scheme but took time exponential in the aspect ratio Δ. In contrast, this new algorithm runs in quasi-polynomial time even for super-logarithmic Δ.

- The key ideas involve formulating a Sherali-Adams LP relaxation that exploits the geometry of low-dimensional Euclidean space, along with a novel analysis of a conditioning-based rounding scheme. In particular, the analysis shows how to bound the "pseudo-uncertainties" that arise from the LP relaxation by relating them to geometric properties.

- This is positioned as an important step toward developing general techniques for efficiently optimizing over metric spaces using convex programming hierarchies. The techniques are not specific just to the Kamada-Kawai objective and may extend more broadly.

In summary, the main contribution is a faster approximation algorithm for MDS under the Kamada-Kawai objective, with running time that is exponentially better in the aspect ratio parameter Δ compared to prior algorithms. This is achieved via new analysis techniques for rounding Sherali-Adams relaxations that exploit geometric properties.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and keywords associated with it are:

- Multi-dimensional scaling (MDS)
- Kamada-Kawai objective function
- Dimension reduction 
- Metric embeddings
- Approximation algorithms
- Sherali-Adams hierarchy
- Linear programming relaxations
- Conditioning-based rounding
- Aspect ratio
- Low-dimensional geometry

The paper proposes a new approximation algorithm for the Kamada-Kawai formulation of multi-dimensional scaling, which is a popular technique for dimension reduction and data visualization. The key contributions include using the Sherali-Adams hierarchy of linear programs along with a novel, geometry-aware analysis of a conditioning-based rounding scheme. This allows the algorithm to achieve significantly better dependence on the aspect ratio parameter compared to prior works, by exploiting properties of low-dimensional Euclidean geometry. The analysis relies heavily on bounding quantiles of expected pairwise distances both in the LP solution and optimal solution.

Overall, the main focus is on developing improved, provable approximation algorithms for MDS objectives using the tools of discrete optimization and convex programming. The key terms reflect this, highlighting MDS, approximation algorithms, dimension reduction, rounding schemes based on linear and semidefinite relaxations, metric embeddings more broadly, and quantifying metric distortion.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 potential in-depth questions about the method proposed in this paper:

1. The paper proposes a novel conditioning-based rounding scheme for the Sherali-Adams LP hierarchy. How does this rounding scheme differ from prior approaches for rounding Sherali-Adams relaxations? What specifically makes the analysis here more geometry-aware?

2. A key innovation is bounding pseudo-deviations of points after conditioning. Explain how pseudo-deviations are defined and why conditioning reduces these deviations. How does this connect to controlling the quantiles of pairwise distances?

3. The discretization result shows that restricting to a coarse net only loses an additive ε in objective value. Discuss the proof approach here. In what ways does it exploit properties specific to the Kamada-Kawai objective?

4. Lemma 3 relates the ε-quantiles of pseudo-expected distances to those in an optimal embedding. Explain why losing a √OPT fraction of pairs is necessary and how the subsequent analysis handles this loss.  

5. For the line/plane, the number of levels of the Sherali-Adams hierarchy used is Õ(log(Δ)/ε). Compare this to the bound that would follow from a black-box application of known Sherali-Adams rounding schemes. Where does the improved dependence on Δ originate?

6. How is the geometric Lemma 1, about pairwise distance quantiles in low dimensions, used to prove the main result? What insight does this lemma provide about the metric structure?

7. Discuss the approximation factor tradeoff compared to prior work, in terms of dependence on Δ versus OPT and ε. When is the guarantee here stronger or weaker?

8. The paper suggests the convex programming approach may extend to other MDS objectives. What modifications would be needed to handle weighted least squares or Kruskal stress? What barriers remain?  

9. Corollary 1 shows the LP size can be reduced to poly(n) while maintaining the approximation guarantee. Explain the subset-based Sherali-Adams relaxation used and why conditioning on random subsets preserves key properties.

10. Beyond MDS, what other open problems in dimensionality reduction, visualization, and metric embeddings could potentially benefit from the convex programming perspective? What modifications or new analyses might be needed?
