# [Fully Decentralized Cooperative Multi-Agent Reinforcement Learning: A   Survey](https://arxiv.org/abs/2401.04934)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem: 
This paper provides a comprehensive survey on fully decentralized cooperative multi-agent reinforcement learning (MARL). In fully decentralized MARL, agents cannot obtain any information from other agents during both training and execution. This poses significant challenges as the environment becomes non-stationary from the perspective of individual agents. Two common settings of fully decentralized cooperative MARL are studied: (1) shared reward setting where agents maximize a shared global reward, and (2) reward sum setting where agents maximize the sum of individual rewards. The goal is to develop algorithms that can find the optimal joint policy or convergence to an equilibrium in these two settings. However, research in this area has been limited due to the difficulties caused by non-stationarity.

Solutions and Contributions:

Shared Reward Setting:
- Reviewed value-based methods (e.g. IQL, fingerprint, hysteretic IQL, I2Q, BQL) that try to mitigate the impact of non-stationary transition probabilities. Proposed new value iteration operators or focus on ideal transition probabilities.
- Reviewed policy-based methods (e.g. IPPO, DPO, TVPO) that guarantee monotonic improvement of joint policies under decentralized optimization. Proposed objectives based on f-divergence or KL constraints.

Reward Sum Setting: 
- Reviewed value-based methods that use Q-learning to find best response policies (decentralized Q-learning) or weaker equilibria like CCE (V-learning). Provide asymptotic convergence guarantees.
- Reviewed policy-based methods that rely on gradient dominance for convergence proofs. Algorithms use independent actor-critics.

The paper provides a systematic organization of prior work, identifies key open questions around optimality, sample complexity, coordination and partial observability, and suggests future research directions like offline decentralized MARL and unified RL applicable to both single agent RL and MARL.


## Summarize the paper in one sentence.

 This paper provides a comprehensive survey of fully decentralized cooperative multi-agent reinforcement learning algorithms for both the shared reward setting and the reward sum setting.


## What is the main contribution of this paper?

 This paper provides a systematic review of fully decentralized methods for cooperative multi-agent reinforcement learning. The main contributions are:

1) It categorizes existing work into the shared reward setting and the reward sum setting. For each setting, it reviews both value-based and policy-based algorithms.

2) For the shared reward setting, it discusses algorithms that try to mitigate the impact of non-stationary transition probabilities in the value-based methods, and algorithms that try to guarantee monotonic improvement of joint policies in the policy-based methods. 

3) For the reward sum setting, it introduces algorithms that aim to find Nash equilibria based on Q-learning or policy gradient methods.

4) It also discusses limitations of current methods, open questions, and future research directions in fully decentralized multi-agent reinforcement learning. For example, devising policy-based methods with optimality guarantees, analyzing sample complexity, enabling coordination without communication, and combining with partial observability.

In summary, this paper provides a broad overview to assist researchers in understanding the progress, challenges, and opportunities of this relatively less studied area in multi-agent reinforcement learning.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper's content, some of the key terms and concepts associated with this paper include:

- Fully decentralized cooperative multi-agent reinforcement learning
- Shared reward setting
- Reward sum setting / General sum setting
- Nash equilibrium (NE)
- Best response policy
- Coarse correlated equilibrium (CCE) 
- Non-stationary environment
- Centralized training decentralized execution (CTDE)
- Independent Q-learning (IQL)
- Distributed IQL
- Hysteretic IQL  
- I2Q
- Best Q-learning (BQL)  
- Independent PPO (IPPO)
- Decentralized policy optimization (DPO) 
- Transition-aware policy optimization (TVPO)
- Weakly acyclic game
- Decentralized Q-learning
- V-learning
- Gradient dominance condition
- Unified reinforcement learning 

These concepts relate to formulating the decentralized MARL problems, analyzing algorithm convergence, handling non-stationarity, and comparing to centralized methods. The key focus is on developing MARL algorithms that can find optimal or equilibrium policies in a fully decentralized manner.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 potential in-depth questions about the methods proposed in this paper on fully decentralized multi-agent reinforcement learning:

1. The paper categorizes methods into value-based and policy-based. For the value-based methods in the shared reward setting, what are the key challenges in handling non-stationary transition probabilities? How do methods like I2Q, BQL and Hysteretic IQL address these challenges?

2. For policy-based methods in the shared reward setting, what is the intuition behind using surrogate functions like in DPO? How does the surrogate function help monotonic improvement of the joint policy?

3. The paper proves convergence of several algorithms like I2Q, BQL and DPO. What are the key assumptions needed for these convergence proofs? How realistic are these assumptions and what happens when they are violated? 

4. In the reward sum setting's value-based methods, explain the concept of weakly acyclic games. Why is this concept important for convergence guarantees of Decentralized Q-Learning?

5. For V-Learning in the reward sum setting, explain how optimistic and pessimistic value function approximations are used. How do these impact the policy update and convergence to CCE?

6. Explain the concept of gradient dominance used in several policy-based methods for the reward sum setting. Why is controlling this term important for convergence to Nash Equilibriums?

7. For methods like DPO and TVPO, adaptive coefficients are used instead of large constants in their objectives. Explain why this is done and what impact it has.

8. The paper shows convergence for stochastic games is still an open question for some methods. What makes handling stochasticity difficult compared to deterministic environments?

9. Partial observability and decentralized coordination are discussed as open challenges. Explain why these problems are difficult and what modifications are needed in existing methods.

10. Compare CTDE and fully decentralized methods - in what scenarios is each approach preferred? Could both approaches be combined in a unified RL framework?
