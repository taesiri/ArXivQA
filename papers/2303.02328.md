# [Decompose, Adjust, Compose: Effective Normalization by Playing with   Frequency for Domain Generalization](https://arxiv.org/abs/2303.02328)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central research question this paper addresses is:How can we improve existing normalization methods for domain generalization (DG) by addressing the issue of content change/variation? The key points are:- Existing normalization methods like batch normalization (BN), layer normalization (LN), and instance normalization (IN) are commonly used for DG to extract style information. However, they have an issue of changing/losing content information when removing style information. - This content change/variation has not been explicitly analyzed before. - The paper proposes to address this issue by combining normalization with spectral decomposition from frequency domain perspective. Spectral decomposition allows separating style (amplitude) from content (phase).- The authors first mathematically analyze and quantify the content change caused by normalization. - Based on this analysis, they propose novel normalization methods - PCNorm, CCNorm, SCNorm - that preserve or adjust content and style changes for more robust DG.- They also propose ResNet models DAC-P and DAC-SC using these methods that achieve state-of-the-art performance on DG benchmarks, highlighting the importance of controlled content preservation.In summary, the central hypothesis is that explicitly analyzing and addressing the content change issue in normalization methods can lead to better techniques and performance for DG. The proposed frequency domain-based normalization methods validate this hypothesis.
