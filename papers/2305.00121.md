# [Learning Locally Editable Virtual Humans](https://arxiv.org/abs/2305.00121)

## What is the central research question or hypothesis that this paper addresses?

 The central research question this paper addresses is how to create locally editable 3D human avatars that have high-fidelity detail and modeling capability. The key ideas are:

- Proposing a novel hybrid human representation that combines the advantages of neural fields (flexibility and modeling power) with explicit skinned body models (ease of deformation and control). 

- Integrating this representation into an end-to-end trainable generative framework that allows fitting to 3D scans, randomly sampling, and editing the avatars via local feature manipulation.

- Enabling avatar customization workflows like transferring details across subjects, drawing textures on images, and consistent reposing. 

The hypothesis is that by disentangling global body deformation from local surface details, and learning feature dictionaries in a generative model, it is possible to create controllable avatars that retain complex geometric and appearance information even for unseen poses and subjects. The experiments aim to validate the modeling accuracy, generalization ability, and editing flexibility of the proposed approach.

In summary, the key innovation is in the hybrid human representation that facilitates avatar generation and editing within a single framework trained on multi-subject scan data. This allows creating customizable avatars not possible with previous human modeling techniques.


## What is the main contribution of this paper?

 This paper proposes a novel hybrid representation and end-to-end trainable framework to model fully editable and customizable neural avatars. The main contributions are:

- A hybrid representation that combines neural fields with skinned meshes. This exploits the modeling power of neural fields and the consistency of skinned meshes under deformation. 

- An end-to-end trainable generative auto-decoder architecture. This enables fitting to unseen 3D scans, random sampling of avatars, and local feature editing across subjects.

- A new high-quality dataset "CustomHumans" containing diverse 3D human scans for training and evaluation.

In summary, the key innovation is the hybrid representation that supports the creation and customization of detailed and animation-ready avatars via local editing of disentangled geometry and texture features. This also enables capabilities like model fitting, cross-subject feature transfer, and texture editing. The experiments demonstrate the approach's advantages over prior work in generative avatar modeling and editing.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the key points from the paper:

The paper proposes a novel hybrid representation for 3D human avatars that combines skinned meshes and neural implicit fields, enabling generating diverse detailed avatars with the ability to perform local editing and transferring of geometric and texture details across subjects.


## How does this paper compare to other research in the same field?

 This paper presents a novel method for creating and customizing 3D human avatars with locally editable geometry and texture. Here are some key points on how it relates to prior work:

- Representation: It proposes a hybrid representation that combines the flexibility of neural fields with the consistency of skinned body meshes. This allows representing complex geometry while enabling control and editing. Other works like NeuMesh and Neural Body explored similar hybrid representations, but were limited to single subjects. 

- Generative modeling: The method trains an auto-decoder generative model with adversarial losses for multi-subject avatars. This is a unique contribution compared to prior work on parametric avatars (CAPE, SMPLicit, SMPLpix) or single-subject neural fields (PIFu, SNARF, Neural Body) that cannot create diverse random samples.

- Avatar editing: The disentangled local features allow swapping details across subjects and editing via images. This level of control is not supported in other learning-based avatars. The editing capabilities go beyond previous works on clothing/detail transfer or texture editing.

- Fitting: Experiments show the model fits unseen scans better than state-of-the-art methods like SMPL+D and gDNA. This demonstrates the representation power of hybrid neural fields over purely parametric (SMPL) or global latent codes (gDNA).

- Dataset: A large-scale multi-subject high-quality scan dataset is contributed for generative avatar research. This addresses limitations of current datasets.

In summary, the key novelty is the combined generative modeling framework, hybrid representation for avatars, and editing capabilities - advancing the state-of-the-art in deep generative models for controllable human avatars. The experiments also validate the design choices over alternatives.


## What future research directions do the authors suggest?

 Based on my reading of the paper, some key future research directions suggested by the authors include:

- Improving the representation to better handle clothing deformations caused by pose-dependent factors and motion dynamics. The current approach assumes static poses for each subject during training. Modeling dynamics could allow for better pose-dependent deformations.

- Exploring alternative body models beyond SMPL/SMPL-X. While these are effective, other models could provide benefits.

- Extending the approach to model hand and face details in addition to the body. Hands and faces are important for avatars but not modeled currently. 

- Enabling control over more attributes like body shape and facial expressions in addition to pose, texture, and geometry. This could allow avatars to be customized in more ways.

- Evaluating how editable avatars could be used in interactive 3D applications, like games and virtual worlds. Testing the approach in downstream tasks could reveal new challenges. 

- Improving diversity and detail of generated avatars as model capacity increases. Scaling up training data and models could enable higher quality and more varied avatars.

- Considering techniques to make training more efficient and avatar editing interfaces more intuitive. Improving speed and usability could be important for real applications.

- Investigating social impacts and ethical issues surrounding highly realistic and editable avatars. Responsible research practices will be important as avatar technology advances.

In summary, the authors point to several promising research avenues related to representation, modeling, control, applications, scaling, interfaces, and ethics of learning editable virtual human avatars. Advancing these could move the field closer to flexible and practical avatar creation.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the paper:

This paper proposes a novel hybrid representation and end-to-end trainable network architecture to model fully editable and customizable neural avatars. The core idea is to combine the modeling power of neural fields with the consistency and control of skinned meshes. Specifically, the method constructs a trainable feature codebook on the vertices of a deformable body model to store local geometry and texture features, exploiting its consistent topology under articulation. This representation is then employed in a generative auto-decoder architecture that enables fitting to unseen scans and sampling realistic avatars with varied appearances and geometries. Furthermore, the representation allows local editing by swapping features between assets. Experiments show the method achieves better model fitting than state-of-the-art methods and generates diverse detailed avatars. The code and a new high-quality dataset CustomHumans are released to facilitate research on controllable avatars.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the paper:

This paper proposes a novel hybrid representation and end-to-end trainable network architecture to model fully editable and customizable neural avatars. At the core of this work is a representation that combines the modeling power of neural fields with the ease of use and inherent 3D consistency of skinned meshes. Specifically, they construct a trainable feature codebook to store local geometry and texture features on the vertices of a deformable body model, exploiting its consistent topology under articulation. This representation is then employed in a generative auto-decoder architecture that enables fitting to unseen scans and sampling of realistic avatars with varied appearances and geometries. Furthermore, the representation allows local editing by swapping local features between 3D assets.

To verify the method for avatar creation and editing, the authors contribute a new high-quality dataset called CustomHumans. Their experiments quantitatively and qualitatively show that the method can generate diverse detailed avatars and achieves better model fitting performance compared to state-of-the-art methods. The code and dataset are made publicly available. Overall, this work presents a novel hybrid representation and training framework that enables creating controllable and editable neural avatars with high visual quality. The local editing capabilities in particular are unique and could be useful for many graphics applications.


## Summarize the main method used in the paper in one paragraph.

 The paper proposes a novel hybrid representation and end-to-end trainable framework to model fully editable and customizable neural avatars. The key idea is to combine the modeling power of neural fields with the consistency of skinned meshes. Specifically, the method constructs a trainable feature codebook to store local geometry and texture features on the vertices of a deformable body model (SMPL), exploiting its consistent topology under articulation. This representation is used in a generative auto-decoder architecture that enables fitting to unseen 3D scans and sampling realistic avatars with varied appearances and geometries. The local conditioning in the representation also allows for local editing by swapping features between avatars. The method is trained on a newly collected high-quality 3D human dataset CustomHumans. Experiments show the approach achieves diverse and detailed avatar creation, better model fitting, and consistent avatar editing compared to prior arts.
