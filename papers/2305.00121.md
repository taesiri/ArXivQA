# [Learning Locally Editable Virtual Humans](https://arxiv.org/abs/2305.00121)

## What is the central research question or hypothesis that this paper addresses?

 The central research question this paper addresses is how to create locally editable 3D human avatars that have high-fidelity detail and modeling capability. The key ideas are:

- Proposing a novel hybrid human representation that combines the advantages of neural fields (flexibility and modeling power) with explicit skinned body models (ease of deformation and control). 

- Integrating this representation into an end-to-end trainable generative framework that allows fitting to 3D scans, randomly sampling, and editing the avatars via local feature manipulation.

- Enabling avatar customization workflows like transferring details across subjects, drawing textures on images, and consistent reposing. 

The hypothesis is that by disentangling global body deformation from local surface details, and learning feature dictionaries in a generative model, it is possible to create controllable avatars that retain complex geometric and appearance information even for unseen poses and subjects. The experiments aim to validate the modeling accuracy, generalization ability, and editing flexibility of the proposed approach.

In summary, the key innovation is in the hybrid human representation that facilitates avatar generation and editing within a single framework trained on multi-subject scan data. This allows creating customizable avatars not possible with previous human modeling techniques.


## What is the main contribution of this paper?

 This paper proposes a novel hybrid representation and end-to-end trainable framework to model fully editable and customizable neural avatars. The main contributions are:

- A hybrid representation that combines neural fields with skinned meshes. This exploits the modeling power of neural fields and the consistency of skinned meshes under deformation. 

- An end-to-end trainable generative auto-decoder architecture. This enables fitting to unseen 3D scans, random sampling of avatars, and local feature editing across subjects.

- A new high-quality dataset "CustomHumans" containing diverse 3D human scans for training and evaluation.

In summary, the key innovation is the hybrid representation that supports the creation and customization of detailed and animation-ready avatars via local editing of disentangled geometry and texture features. This also enables capabilities like model fitting, cross-subject feature transfer, and texture editing. The experiments demonstrate the approach's advantages over prior work in generative avatar modeling and editing.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the key points from the paper:

The paper proposes a novel hybrid representation for 3D human avatars that combines skinned meshes and neural implicit fields, enabling generating diverse detailed avatars with the ability to perform local editing and transferring of geometric and texture details across subjects.
