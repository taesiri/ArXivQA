# [Perseus: Removing Energy Bloat from Large Model Training](https://arxiv.org/abs/2312.06902)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a high-quality paragraph summarizing the key points of the paper:

This paper proposes Perseus, a unified optimization framework that aims to reduce the energy consumption of large neural network model training. The authors identify two types of "energy bloat" that lead to excess energy usage - intrinsic bloat caused by imbalance across pipeline stages, and extrinsic bloat caused by straggler pipelines. Perseus formulates the pipeline energy minimization problem, proves it is NP-hard, and provides an efficient approximation algorithm to generate the complete "iteration time vs energy" Pareto frontier for a training pipeline. It starts from the minimum energy schedule and iteratively speeds up execution time while minimizing energy increase to trace the frontier. At runtime, Perseus can quickly adapt to stragglers by looking up the appropriate energy schedule from the precomputed frontier table. When evaluated on models like GPT-3, BERT, T5 and Wide ResNet, Perseus is able to reduce per-iteration energy consumption by up to 30% in real GPU experiments and large-scale emulations, without noticeable slowdown. The paper makes notable contributions in formally defining and addressing the intrinsic and extrinsic energy bloat problems in large scale distributed training.
