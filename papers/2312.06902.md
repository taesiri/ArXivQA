# [Perseus: Removing Energy Bloat from Large Model Training](https://arxiv.org/abs/2312.06902)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
Training large AI models on multiple GPUs consumes massive amounts of energy. However, not all of this energy directly contributes to training throughput. Specifically, the paper identifies two sources of "energy bloat":

1) Intrinsic energy bloat: Due to imbalance in computation time across pipeline stages when using pipeline parallelism for distributed training. Stages not on the critical path run faster than needed, wasting energy. 

2) Extrinsic energy bloat: When multiple pipeline replicas run in parallel (data parallelism) but some pipelines (stragglers) run slower than others. Non-straggler pipelines wasting energy by running faster than the slowest pipeline.

Proposed Solution:
The paper proposes Perseus, a system that creates a unified optimization framework to minimize both intrinsic and extrinsic energy bloat in large model training, without slowing down training.

Key ideas:
- Represent the computations in a training iteration as a DAG 
- Annotate each computation node with a planned time and energy schedule
- Formulate an optimization problem to minimize energy under a time constraint 
- Prove the problem is NP-Hard, provide an approximate algorithm
- Algorithm efficiently finds all points on the Pareto frontier of iteration time vs energy 
- Select the optimal point based on straggler status to minimize energy bloat

The system has two components:
1) A framework-independent server that runs the optimization and lookups optimal schedules
2) A client integrated into the training framework that profiles computations online and realizes schedules by tuning GPU frequency

Main Contributions:  
- Identify intrinsic and extrinsic energy bloat in large model training
- Formulate a principled graph optimization algorithm to minimize both sources of bloat 
- Introduce the idea of an "iteration time vs energy" Pareto frontier for training
- Design and implement an end-to-end system Perseus that can reduce energy consumption by up to 30% for large model workloads without affecting training throughput.

In summary, the paper makes important contributions in optimizing the energy efficiency of large model training by systematically identifying and minimizing unnecessary energy waste. The proposed system Perseus and algorithms demonstrate significant energy savings on real workloads.
