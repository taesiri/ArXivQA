# [Knowledge of Pretrained Language Models on Surface Information of Tokens](https://arxiv.org/abs/2402.09808)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Pretrained language models (PLMs) lack knowledge about surface information of text tokens, such as character lengths, substrings, and character positions. This limits their performance on tasks requiring surface understanding like information extraction, length-specified generation, and extractive QA. 

- It is unclear what types of surface information are captured in the subword and word embeddings learned by PLMs. Analyzing this is an important first step towards developing "surface-sensitive" PLMs.

Methodology:
- Analyzed surface knowledge of PLMs from 3 perspectives: token length, substrings, word constitution (character locations)
- Trained MLPs to extract these types of surface information from subword and word embeddings of PLMs
- Compared 6 English PLMs (Word2Vec, BERT, T5, LLaMA, etc) and 3 Japanese PLMs (rinna, Swallow)

Results:
- PLMs capture partial knowledge of token length and substrings but not precise character locations
- Recent large PLMs (LLaMA) outperform older PLMs (Word2Vec) in capturing surface properties 
- Observed decoder-side bottlenecks in utilizing the surface knowledge, based on generations

Conclusions:
- Surface knowledge of PLMs is incomplete, lacking precise positional information
- Continued analysis of surface properties is important for improving PLMs and developing "surface-sensitive" models
- There are decoder-side limitations in effectively using the acquired surface knowledge

The key contributions are the in-depth analysis quantifying surface knowledge in PLMs from multiple facets, comparisons across models and languages, and insights on decoder-side bottlenecks. The results motivate developing enhanced architectures to capture surface properties.


## Summarize the paper in one sentence.

 This paper analyzes the surface knowledge of pretrained language models regarding token length, substrings, and token constitution based on experiments with English and Japanese models, finding that while recent models acquire some knowledge of length and substrings, they lack knowledge of token constitution.


## What is the main contribution of this paper?

 Based on my reading of the paper, the main contribution is an analysis of the knowledge that pretrained language models (PLMs) have regarding surface information of tokens, specifically token length, substrings, and token constitution. The key findings are:

- PLMs do have some knowledge of token length and substrings, especially recent large language models like LLaMA2. However, they lack knowledge of token constitution - which characters appear in which positions.

- There seem to be bottlenecks in utilizing the surface knowledge PLMs do have, both in the embeddings/encoders and especially on the decoder side for generation tasks. For example, models could not effectively generate the length of words, even though this information seems to be present in the embeddings.

- The analysis was conducted systematically across multiple PLMs in English and Japanese, showing consistent trends. This highlights gaps in surface knowledge as a common issue faced by current PLMs.

In summary, the main contribution is surfacing and analyzing the lack of surface knowledge in PLMs through multiple experiments, and identifying decoder-side bottlenecks in utilizing such knowledge. The authors aim for this analysis to motivate progress in developing "surface-sensitive" PLMs.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and keywords associated with it are:

- Pretrained language models (PLMs)
- Large language models (LLMs) 
- Surface information
- Token length
- Substrings
- Token constitution
- Word embeddings
- Subword embeddings
- Knowledge analysis
- Bottlenecks

The paper examines the knowledge of surface information, such as token length, substrings, and token constitution, acquired by pretrained language models. It compares several PLMs, including BERT, T5, LLaMA, CANINE, rinna, and Swallow models trained on English and Japanese text. The analysis focuses on what knowledge is contained in the word and subword embeddings learned by these models. The paper concludes that while PLMs acquire some knowledge of lengths and substrings, they lack full knowledge of token constitution. It also identifies decoder-side bottlenecks in utilizing the acquired surface knowledge. So those are some of the key concepts and terms that feature prominently in this work.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 potential in-depth questions about the method proposed in this paper:

1. The paper examines surface knowledge from three perspectives: token length, substrings, and token constitution. What are the key differences between these three types of surface knowledge? What additional types of surface knowledge could be studied?

2. The paper trains MLPs to extract different types of surface knowledge from embeddings. What are the advantages and disadvantages of using MLPs for this task compared to other machine learning approaches? 

3. The paper finds that recent large language models (LLMs) capture more surface knowledge than smaller pretrained models. What properties of LLMs allow them to better acquire this knowledge during pretraining?

4. The paper shows a discrepancy between surface knowledge contained in embeddings versus what models can generate. What factors could explain this decoder-side bottleneck in utilizing the acquired knowledge? 

5. The paper focuses on English and Japanese models. What differences would you expect in surface knowledge acquisition for other languages with varying writing systems? What experiments could be run to test this?

6. The paper excludes the largest LLMs like ChatGPT. How do you think ChatGPT might perform on the tasks in this paper to evaluate surface knowledge compared to the smaller models tested? What experiments could determine this?  

7. The paper finds better knowledge of shorter tokens close to string ends. Why might models struggle with longer subwords/words or interiors of strings? How could this be improved?

8. The paper focuses on surface form. What other types of linguistic knowledge are important for language understanding and generation? How could these be evaluated?

9. The paper analyzes existing models. How could models be designed to better acquire and utilize surface knowledge during pretraining and fine-tuning?

10. What downstream NLP tasks do you think would benefit most from improved handling of surface knowledge in LLMs? How significant might gains be? What benchmarks could be proposed?
