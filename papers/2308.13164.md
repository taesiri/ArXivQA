# [Diff-Retinex: Rethinking Low-light Image Enhancement with A Generative   Diffusion Model](https://arxiv.org/abs/2308.13164)

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper addresses is how to rethink low-light image enhancement as a generative image modeling problem rather than just a restoration problem. The key hypotheses are:1. Formulating low-light image enhancement as a decomposition into illumination and reflectance components followed by generative diffusion modeling of each component can improve results compared to direct end-to-end mapping or simple restoration approaches. 2. Transformer-based decomposition can better separate illumination and reflectance compared to CNN-based methods.3. Diffusion modeling can generate missing details and textures in a conditioned way better than discriminative CNN or simple optimization approaches.4. Combining decomposition, Transformer attention, and diffusion generative modeling provides a new way to tackle low-light image enhancement that outperforms previous methods.The overall goal is to show that thinking of low-light enhancement as a conditional generative modeling problem guided by a physical image formation model (Retinex) can enable recovery of missing scene details and improved results compared to viewing it as just a discriminative mapping or basic restoration task. The Transformer and diffusion components specifically aim to improve the decomposition and generative conditional modeling parts of this framework.


## What is the main contribution of this paper?

The main contributions of this paper are:- It proposes a new method called Diff-Retinex for low-light image enhancement. Diff-Retinex combines a Retinex model with a generative diffusion model. - It introduces a Transformer decomposition network (TDN) for decomposing images into illumination and reflectance maps in the Retinex model. TDN uses multi-head depthwise convolution attention to efficiently process high-resolution images.- It applies generative diffusion models to adjust the illumination and reflectance maps by restoring their probability distributions. This allows the method to compensate for lost information and details in low-light images through generation.- Experiments demonstrate Diff-Retinex's ability to infer and complete missing details and textures in low-light images, which other methods cannot. The results also show strong quantitative performance and generalization ability.In summary, the key innovation is the combination of Retinex decomposition with generative diffusion models. This allows Diff-Retinex to not just enhance but actually generate missing content in low-light images in a controllable way. The Transformer decomposition network also improves Retinex model applicability.
