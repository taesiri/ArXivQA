# [Deep Language Networks: Joint Prompt Training of Stacked LLMs using   Variational Inference](https://arxiv.org/abs/2306.12509)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the main research questions/hypotheses appear to be:1) Can a deep language network with a single layer (DLN-1) outperform existing methods like automatic prompt engineering (APE) and in-context learning (ICL)?2) Does adding depth to create a two-layer deep language network (DLN-2) provide further improvements in performance over a single-layer DLN-1?The authors seem to be investigating whether their proposed deep language network architecture, which stacks multiple language model layers and learns prompts at each layer, can boost the performance of smaller language models to match or exceed larger more powerful models. Specifically:- They first show how prompt optimization can be effectively done for a 1-layer DLN-1, outperforming APE and ICL baselines on some tasks. - They then demonstrate that a 2-layer DLN-2 can achieve even higher performance by learning to decompose problems into subtasks, with each layer solving an easier subtask. - Their experiments aim to show that DLN-2 can match the few-shot performance of GPT-4 even when using smaller LM components, by learning the prompts jointly.So in summary, the central hypotheses appear to be around the capabilities of the proposed DLN framework compared to existing methods, and specifically whether depth helps to boost the performance of smaller constituent LMs.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contributions appear to be:- Proposing Deep Language Networks (DLNs), which view large language models (LLMs) as language layers that can be stacked to form deeper networks. The learnable parameters of each layer are natural language prompts.- Demonstrating an effective prompt optimization method for 1-layer DLNs that outperforms prior work like APE on several language understanding tasks. The optimized prompts learn to include both task instructions and verbalizations of training examples. - Developing a variational inference training algorithm for learning the prompts at each layer of a 2-layer DLN. This allows joint training of the prompts by treating the middle layer output as a latent variable.- Showing that 2-layer DLNs can improve over 1-layer DLNs, achieving performance comparable to GPT-4 few-shot learning on some reasoning tasks, even when using smaller/less capable LLMs in the layers.- Framing prompting techniques like chain-of-thought as special cases of DLNs and discussing how DLNs suggest opportunities for more complex prompt learning.- Providing an open source implementation of DLNs.In summary, the main contribution appears to be proposing the DLN framework for stackingPrompt-programmed language models, developing effective prompt optimization algorithms tailored for 1-layer and 2-layer DLNs, and demonstrating their capabilities on a variety of language understanding and reasoning tasks. The results suggest this is a promising direction for building more customizable and capable modular language systems.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper proposes Deep Language Networks (DLNs) which view large language models as language layers in a network, with natural language prompts at each layer as the learnable parameters; it shows how to optimize prompts for one layer, then presents a variational inference method to jointly learn prompts in a two layer DLN, demonstrating improved performance over single layer DLNs.


## How does this paper compare to other research in the same field?

Here are a few thoughts on how this paper compares to other research in the field:- This paper presents a new framework for stacking and jointly training multiple language models using natural language prompts. Most prior work has focused on single large language models, knowledge distillation to create smaller models, or prompting techniques for a single model. The idea of composing multiple language models with learned natural language prompts is novel.- The proposed Deep Language Networks have some similarities to other compositional prompting techniques like Chain-of-Thought or tool-assisted prompting. However, DLNs treat the intermediate outputs as latent variables and use variational inference to learn the prompts, which is a unique approach.- Compared to state-of-the-art few-shot learning results, the performance of the 2-layer DLN networks is competitive on some tasks and underperforms on others. More analysis would be needed to determine if DLNs can match or exceed the capabilities of the largest LLMs.- The DLN framework is shown to provide a benefit over single layer prompt optimization, demonstrating the value of depth even when using smaller constituent models. This could be a promising direction for making LLM systems more affordable, adaptable and transparent.- The idea of learning to generate synthetic examples for inclusion in prompts is noteworthy, as this allows dynamic selection of useful training examples during optimization. This relates to meta-learning techniques for few-shot learning.- The open source release of the system is valuable for reproducibility and extensions by other researchers. More rigorous ablation studies could further elucidate the effects of the different optimization techniques proposed.Overall, this paper introduces a novel compositional framework for language models that combines ideas from prompting, knowledge distillation, variational inference and meta-learning. While performance does not uniformly exceed state-of-the-art few-shot learning, the ideas show promise for assembling capable LLM systems from smaller parts. Further analysis and extensions of the approach could better reveal its strengths and limitations compared to other methods.
