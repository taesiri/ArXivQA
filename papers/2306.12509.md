# [Deep Language Networks: Joint Prompt Training of Stacked LLMs using   Variational Inference](https://arxiv.org/abs/2306.12509)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the main research questions/hypotheses appear to be:1) Can a deep language network with a single layer (DLN-1) outperform existing methods like automatic prompt engineering (APE) and in-context learning (ICL)?2) Does adding depth to create a two-layer deep language network (DLN-2) provide further improvements in performance over a single-layer DLN-1?The authors seem to be investigating whether their proposed deep language network architecture, which stacks multiple language model layers and learns prompts at each layer, can boost the performance of smaller language models to match or exceed larger more powerful models. Specifically:- They first show how prompt optimization can be effectively done for a 1-layer DLN-1, outperforming APE and ICL baselines on some tasks. - They then demonstrate that a 2-layer DLN-2 can achieve even higher performance by learning to decompose problems into subtasks, with each layer solving an easier subtask. - Their experiments aim to show that DLN-2 can match the few-shot performance of GPT-4 even when using smaller LM components, by learning the prompts jointly.So in summary, the central hypotheses appear to be around the capabilities of the proposed DLN framework compared to existing methods, and specifically whether depth helps to boost the performance of smaller constituent LMs.
