# [Deep Language Networks: Joint Prompt Training of Stacked LLMs using   Variational Inference](https://arxiv.org/abs/2306.12509)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the main research questions/hypotheses appear to be:1) Can a deep language network with a single layer (DLN-1) outperform existing methods like automatic prompt engineering (APE) and in-context learning (ICL)?2) Does adding depth to create a two-layer deep language network (DLN-2) provide further improvements in performance over a single-layer DLN-1?The authors seem to be investigating whether their proposed deep language network architecture, which stacks multiple language model layers and learns prompts at each layer, can boost the performance of smaller language models to match or exceed larger more powerful models. Specifically:- They first show how prompt optimization can be effectively done for a 1-layer DLN-1, outperforming APE and ICL baselines on some tasks. - They then demonstrate that a 2-layer DLN-2 can achieve even higher performance by learning to decompose problems into subtasks, with each layer solving an easier subtask. - Their experiments aim to show that DLN-2 can match the few-shot performance of GPT-4 even when using smaller LM components, by learning the prompts jointly.So in summary, the central hypotheses appear to be around the capabilities of the proposed DLN framework compared to existing methods, and specifically whether depth helps to boost the performance of smaller constituent LMs.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contributions appear to be:- Proposing Deep Language Networks (DLNs), which view large language models (LLMs) as language layers that can be stacked to form deeper networks. The learnable parameters of each layer are natural language prompts.- Demonstrating an effective prompt optimization method for 1-layer DLNs that outperforms prior work like APE on several language understanding tasks. The optimized prompts learn to include both task instructions and verbalizations of training examples. - Developing a variational inference training algorithm for learning the prompts at each layer of a 2-layer DLN. This allows joint training of the prompts by treating the middle layer output as a latent variable.- Showing that 2-layer DLNs can improve over 1-layer DLNs, achieving performance comparable to GPT-4 few-shot learning on some reasoning tasks, even when using smaller/less capable LLMs in the layers.- Framing prompting techniques like chain-of-thought as special cases of DLNs and discussing how DLNs suggest opportunities for more complex prompt learning.- Providing an open source implementation of DLNs.In summary, the main contribution appears to be proposing the DLN framework for stackingPrompt-programmed language models, developing effective prompt optimization algorithms tailored for 1-layer and 2-layer DLNs, and demonstrating their capabilities on a variety of language understanding and reasoning tasks. The results suggest this is a promising direction for building more customizable and capable modular language systems.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper proposes Deep Language Networks (DLNs) which view large language models as language layers in a network, with natural language prompts at each layer as the learnable parameters; it shows how to optimize prompts for one layer, then presents a variational inference method to jointly learn prompts in a two layer DLN, demonstrating improved performance over single layer DLNs.


## How does this paper compare to other research in the same field?

Here are a few thoughts on how this paper compares to other research in the field:- This paper presents a new framework for stacking and jointly training multiple language models using natural language prompts. Most prior work has focused on single large language models, knowledge distillation to create smaller models, or prompting techniques for a single model. The idea of composing multiple language models with learned natural language prompts is novel.- The proposed Deep Language Networks have some similarities to other compositional prompting techniques like Chain-of-Thought or tool-assisted prompting. However, DLNs treat the intermediate outputs as latent variables and use variational inference to learn the prompts, which is a unique approach.- Compared to state-of-the-art few-shot learning results, the performance of the 2-layer DLN networks is competitive on some tasks and underperforms on others. More analysis would be needed to determine if DLNs can match or exceed the capabilities of the largest LLMs.- The DLN framework is shown to provide a benefit over single layer prompt optimization, demonstrating the value of depth even when using smaller constituent models. This could be a promising direction for making LLM systems more affordable, adaptable and transparent.- The idea of learning to generate synthetic examples for inclusion in prompts is noteworthy, as this allows dynamic selection of useful training examples during optimization. This relates to meta-learning techniques for few-shot learning.- The open source release of the system is valuable for reproducibility and extensions by other researchers. More rigorous ablation studies could further elucidate the effects of the different optimization techniques proposed.Overall, this paper introduces a novel compositional framework for language models that combines ideas from prompting, knowledge distillation, variational inference and meta-learning. While performance does not uniformly exceed state-of-the-art few-shot learning, the ideas show promise for assembling capable LLM systems from smaller parts. Further analysis and extensions of the approach could better reveal its strengths and limitations compared to other methods.


## What future research directions do the authors suggest?

Based on my reading of the paper, here are some of the main future research directions suggested by the authors:- Exploring DLNs with different widths (number of layers), depths, and connection architectures beyond the linear networks tested so far. The framework could potentially support more complex graph structures.- Learning parts of the forward and backward templates rather than engineering them. This could help tighten the variational lower bound and enable more effective joint training.- Fine-tuning the LLMs at each layer to improve individual layer performance, in addition to optimizing prompts. This could boost the capabilities of the full DLN system.- Going beyond discrete prompts to explore continuous prompt representations that could be optimized via gradients.- Developing more adaptive approximate posterior distributions for the hidden variables, potentially by learning suitable prompts. This could improve the tightness of the evidence lower bound.- Leveraging additional modalities beyond text as inputs/outputs to the layers.- Exploring whether DLNs could be used to generate training data for adapting LLMs to become more "stackable" components.- Testing DLNs on a wider range of tasks and environments beyond the benchmarks used so far.- Considering additional objectives beyond accuracy, such as model controllability, interpretability, and reliability.- Investigating societal impacts and how to responsibly deploy modular systems like DLNs in real-world applications.In summary, the authors point to many possibilities for extending DLNs to more complex architectures, learning additional components, integrating other data modalities, generating training data, evaluating on more tasks, and responsibly deploying the technology.


## Summarize the paper in one paragraph.

Here is a one paragraph summary of the key points from the paper:The paper proposes Deep Language Networks (DLNs), which view large language models as layers in a network whose parameters are natural language prompts. They first show how prompt optimization can be effectively performed for a 1-layer DLN, extending prior work on Automatic Prompt Engineering. They then propose a 2-layer DLN which allows decomposing problems into subtasks, with each layer focused on a simpler task. The output of the first layer is treated as a latent variable and joint training of the two prompts is done via variational inference and approximate posterior sampling. Experiments on reasoning tasks show DLN-1 can surpass strong baselines like in-context learning, and DLN-2 provides further gains, sometimes approaching GPT-4 performance even when using a smaller LLM. The DLN framework suggests new ways to optimize prompts in stacked LLMs.


## Summarize the paper in two paragraphs.

Here is a two paragraph summary of the key points from the paper:The paper proposes Deep Language Networks (DLNs), which view large language models (LLMs) as stochastic language layers in a network. The learnable parameters of each layer are the natural language prompts. The paper first shows how prompt optimization can be effectively performed for a 1-Layer DLN, extending prior work on Automatic Prompt Engineering. This 1-Layer DLN matches or exceeds the performance of other LLM prompting techniques on several language understanding tasks. The paper then proposes a 2-Layer DLN which feeds the output of the first layer LLM into the input of the second layer LLM. To train the two prompts jointly, a variational inference algorithm is devised. Evaluated on reasoning tasks requiring decomposition into subtasks, a 2-Layer DLN outperforms a 1-Layer version, and even reaches performance comparable to GPT-4 in a few-shot setting, despite using a smaller LLM backbone. In summary, the key contributions are:1) Framing LLMs as language layers with learnable prompt parameters 2) Devising effective prompt optimization for 1-Layer DLNs3) Proposing a novel variational inference approach to jointly learn two prompts in a 2-Layer DLN4) Demonstrating that with proper prompting, stacking and chaining smaller LLMs can match or exceed larger individual LLMs on reasoning tasks.


## Summarize the main method used in the paper in one paragraph.

The paper proposes Deep Language Networks (DLNs), which compose multiple language models (LLMs) in a stacked architecture to jointly learn natural language prompts at each layer. Specifically, DLNs view LLMs as parametrized by their prompt, which serves as a natural language instruction. A shallow DLN with one LLM layer (DLN-1) is shown to optimize prompts more effectively than prior discrete search methods like APE. This is done by using the LLM to both generate candidate prompts and score them based on data likelihood. Further, a two LLM layer DLN (DLN-2) is proposed to capture reasoning decompositions. The prompts at each layer are learned jointly using variational inference, which treats the first layer LLM's output as a latent variable. An approximate posterior distribution is maintained over this latent variable, and used to derive a variational lower bound that can be optimized to learn the prompts at both layers. Experiments show DLN-1 matching the best baseline LLM system on many tasks, and DLN-2 providing further gains on complex reasoning tasks, sometimes exceeding the performance of GPT-4 despite using a smaller LLM backbone.
