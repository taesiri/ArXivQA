# [Deep Language Networks: Joint Prompt Training of Stacked LLMs using   Variational Inference](https://arxiv.org/abs/2306.12509)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the main research questions/hypotheses appear to be:1) Can a deep language network with a single layer (DLN-1) outperform existing methods like automatic prompt engineering (APE) and in-context learning (ICL)?2) Does adding depth to create a two-layer deep language network (DLN-2) provide further improvements in performance over a single-layer DLN-1?The authors seem to be investigating whether their proposed deep language network architecture, which stacks multiple language model layers and learns prompts at each layer, can boost the performance of smaller language models to match or exceed larger more powerful models. Specifically:- They first show how prompt optimization can be effectively done for a 1-layer DLN-1, outperforming APE and ICL baselines on some tasks. - They then demonstrate that a 2-layer DLN-2 can achieve even higher performance by learning to decompose problems into subtasks, with each layer solving an easier subtask. - Their experiments aim to show that DLN-2 can match the few-shot performance of GPT-4 even when using smaller LM components, by learning the prompts jointly.So in summary, the central hypotheses appear to be around the capabilities of the proposed DLN framework compared to existing methods, and specifically whether depth helps to boost the performance of smaller constituent LMs.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contributions appear to be:- Proposing Deep Language Networks (DLNs), which view large language models (LLMs) as language layers that can be stacked to form deeper networks. The learnable parameters of each layer are natural language prompts.- Demonstrating an effective prompt optimization method for 1-layer DLNs that outperforms prior work like APE on several language understanding tasks. The optimized prompts learn to include both task instructions and verbalizations of training examples. - Developing a variational inference training algorithm for learning the prompts at each layer of a 2-layer DLN. This allows joint training of the prompts by treating the middle layer output as a latent variable.- Showing that 2-layer DLNs can improve over 1-layer DLNs, achieving performance comparable to GPT-4 few-shot learning on some reasoning tasks, even when using smaller/less capable LLMs in the layers.- Framing prompting techniques like chain-of-thought as special cases of DLNs and discussing how DLNs suggest opportunities for more complex prompt learning.- Providing an open source implementation of DLNs.In summary, the main contribution appears to be proposing the DLN framework for stackingPrompt-programmed language models, developing effective prompt optimization algorithms tailored for 1-layer and 2-layer DLNs, and demonstrating their capabilities on a variety of language understanding and reasoning tasks. The results suggest this is a promising direction for building more customizable and capable modular language systems.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper proposes Deep Language Networks (DLNs) which view large language models as language layers in a network, with natural language prompts at each layer as the learnable parameters; it shows how to optimize prompts for one layer, then presents a variational inference method to jointly learn prompts in a two layer DLN, demonstrating improved performance over single layer DLNs.
