# [TENPLEX: Changing Resources of Deep Learning Jobs using Parallelizable   Tensor Collections](https://arxiv.org/abs/2312.05181)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem: 
Modern deep learning (DL) jobs leverage multi-dimensional parallelism across multiple GPUs, combining data, model, and pipeline parallelism strategies. This results in complex parallelization configurations that are tailored to a specific set of GPU resources allocated to the job. However, the GPU allocation may need to change dynamically at runtime for reasons such as elasticity, redeployment, or failure recovery. Current DL systems lack abstractions to change the parallelization of a running job efficiently and in a model-independent manner. This is needed to maintain training convergence and performance when resources change.

Proposed Solution:
The paper proposes Tenplex, a state management library for DL systems. Tenplex represents the model and data state of a DL job as a "parallelizable tensor collection" (PTC). The PTC expresses the hierarchical tensor structure along with metadata capturing the multi-dimensional parallelization strategy. When GPU resources change at runtime, Tenplex transforms the PTC by re-partitioning and re-allocating tensors based on a new parallelization configuration from the DL framework. This ensures consistency of the training dataset and hyperparameters. Transformation operations are computed in a distributed, parallel fashion across GPUs to minimize data movement.

Contributions:
- Formalizes the notion of a PTC to externalize and transform the model/data state of DL jobs
- Presents algorithms to reconfigure PTCs dynamically based on changes to parallelization  
- Implements distributed state transformers to apply reconfiguration efficiently at scale
- Integration with DL frameworks like PyTorch and model libraries like Megatron
- Experiments show support for fully dynamic multi-dimensional parallelism changes with low overhead

In summary, Tenplex enables DL jobs to change parallelization and GPU resources at runtime while maintaining efficiency, consistency and convergence. The PTC is a novel abstraction for DL state management across configuration changes.
