# Enabling Intelligent Interactions between an Agent and an LLM: A   Reinforcement Learning Approach

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be: How can an agent interact with a large language model (LLM) in an intelligent and cost-effective way to solve complex embodied tasks?More specifically, the paper explores using reinforcement learning to train a mediator model that can determine when it is necessary for the agent to query the LLM for high-level instructions. The goal is to enable the agent to solve target tasks with only minimal necessary interactions with the LLM, thereby reducing the interaction costs associated with using a large external LLM.The key hypothesis seems to be that by learning an explicit asking policy through RL, the agent can learn when it should ask the LLM for new plans versus adhering to its current plan. This allows the agent to balance task performance and interaction costs.So in summary, the central research question is around enabling efficient and intelligent agent-LLM interactions, and the hypothesis is that an RL-trained asking policy can achieve this goal. The experiments then aim to evaluate this hypothesis across different embodied environments like MiniGrid and Habitat.


## What is the main contribution of this paper?

The main contribution of this paper is proposing an RL-based approach called "When2Ask" to enable efficient and cost-effective interactions between an embodied agent and a large language model (LLM). Specifically:- They introduce a mediator module within the Planner-Actor-Mediator framework that decides when it is necessary for the agent to query the LLM for high-level instructions. This mediator contains an asking policy trained with RL to balance task performance and interaction costs.- They evaluate When2Ask in MiniGrid and Habitat environments requiring planning sub-goals. Results show it achieves high task success with only a few necessary interactions with the LLM, significantly reducing costs compared to baselines.- Experiments suggest the learned mediator makes the agent's performance more robust against partial observability by enabling it to handle newly acquired information and unexpected errors when providing subsequent plans. - They open source their code to facilitate research on leveraging LLMs for embodied agents.In summary, the key contribution is using RL to learn an intelligent asking policy that mediates interactions between the agent and LLM. This enables effective and cost-efficient utilization of the LLM's reasoning abilities for complex embodied tasks.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper proposes an RL-based approach called When2Ask that learns when it is necessary for an embodied agent to query a large language model for high-level instructions in order to accomplish a target task efficiently while minimizing interaction costs.


## How does this paper compare to other research in the same field?

Here are a few thoughts on how this paper compares to other related work in using large language models (LLMs) to assist embodied agents:- It builds on prior work that explores using LLMs as planners/reasoners for embodied agents, such as Ahn et al. 2022, Wang et al. 2023, and Dasgupta et al. 2023. However, it focuses specifically on designing an intelligent mediator to facilitate efficient interaction between the agent and LLM.- Compared to methods that use hard-coded rules for querying the LLM (e.g. Ahn et al. 2022), this paper proposes learning an adaptive asking policy using RL. This allows more flexible and intelligent interaction compared to hand-designed rules.- Unlike Dasgupta et al. 2023 which queries the LLM at every timestep, this work aims to minimize unnecessary LLM interactions to improve efficiency. The learned asking policy determines when interaction is needed.- The approach is evaluated extensively in both simple grid worlds (MiniGrid) and more visually realistic environments (Habitat). Results demonstrate the learned policy enables efficient task completion across environments.- The proposed mediator model makes the overall system more robust to uncertainties and flaws in other components like the translator. This is a useful property lacking in some prior work.Overall, this paper makes a nice contribution in designing the mediator to enable more intelligent and efficient interaction with LLMs. The RL-learned asking policy is a key innovation compared to prior work. The comprehensive experiments highlight the strengths of the approach in terms of minimizing LLM queries while maintaining good task performance. The code release also makes this a useful contribution for researchers in this area.
