# [On the Practicality of Deterministic Epistemic Uncertainty](https://arxiv.org/abs/2107.00649)

## What is the central research question or hypothesis that this paper addresses?

This paper compares different uncertainty estimation methods for deep neural networks on the tasks of semantic segmentation and depth estimation. The key research questions it addresses are:1. How do different uncertainty estimation methods like MC dropout, deep ensembles, SNGP, MIR, DDU, etc. compare in quantifying uncertainty on semantic segmentation and depth estimation? 2. How does uncertainty estimation change with varying levels of noise and domain shift? Specifically, how robust are these methods when tested on corrupted or out-of-distribution data?3. Can proper uncertainty estimation improve model performance and reliability for real-world applications like autonomous driving?The central hypothesis is that explicitly modeling uncertainty can improve robustness and safety for vision-based systems like self-driving cars. The paper evaluates different uncertainty estimation techniques on semantic segmentation and depth estimation to understand their characteristics and determine which methods are most effective. A key focus is evaluating performance on noisy and shifted data to simulate real-world conditions. The goal is to provide guidance on selecting appropriate uncertainty methods and demonstrate their benefits for critical applications like autonomous driving where uncertainty awareness is crucial.In summary, the paper provides a comprehensive benchmark and analysis of uncertainty estimation techniques on key vision tasks under varying data conditions relevant for autonomous driving systems. The central aim is assessing these methods' ability to quantify uncertainty and enable safer, more robust vision models.
