# [What has LeBenchmark Learnt about French Syntax?](https://arxiv.org/abs/2403.02173)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Pretrained acoustic models like LeBenchmark are trained on raw speech signals only, with no explicit lexical knowledge. Yet they perform surprisingly well on higher-level linguistic tasks that seem to require syntactic knowledge. So an open question is - do these models implicitly learn syntax? 

- This paper probes LeBenchmark, a pretrained French acoustic model, for syntactic information to shed light on this question. Specifically, the tasks are part-of-speech (POS) tagging and unlabeled dependency parsing.  

Methodology:
- LeBenchmark is probed by extracting token representations from the model using time alignments, and training linear classifiers on top to predict POS tags and dependency heads. This is done separately for each of the 24 layers.

- Experiments use the Orfeo French treebank of spontaneous conversational speech. POS tagging uses a 20 tag set. Dependency parsing uses a relative position encoding to cast it as sequence labeling.

Results: 
- LeBenchmark encodes syntactic information, seen via higher accuracy than random baseline. POS tagging accuracy peaks at 65.5% (layer 15), parsing at 52% (layer 14).  

- Syntactic information peaks in the middle layers, then drops sharply in later layers, contrasting with BERT where the drop is more gradual.

- The model does better at predicting local dependencies, as seen in labeled results for layer 14 parser. Still, it has some accuracy for longer distances too.

Contributions:
- First probing study of syntax for a French acoustic model, and on spontaneous speech
- Finding that syntactic information peaks in middle layers before disappearing in later layers

Overall, the paper demonstrates that despite training only on raw speech, LeBenchmark learns some amount of implicit syntactic knowledge. This peaks in the middle layers and then fades rapidly.
