# Teaching Small Language Models to Reason

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper addresses is: can the reasoning capabilities of large language models be transferred to smaller language models via finetuning?The authors explore using knowledge distillation to transfer the reasoning abilities that emerge in large language models (LLMs) with over 100 billion parameters to smaller language models. Specifically, they finetune smaller language models on chains of thought generated by the LLMs to teach the smaller models to reason in a similar step-by-step fashion. Their experiments aim to determine if this chain of thought knowledge distillation can improve the reasoning capabilities of smaller models across tasks like arithmetic, commonsense and symbolic reasoning.


## What is the main contribution of this paper?

The main contribution of this paper is exploring the transfer of reasoning capabilities from large language models (LLMs) to smaller models via knowledge distillation of chain of thought (CoT) reasoning. Specifically, the authors propose finetuning smaller student models like T5 on CoT data generated by large teacher models like PaLM 540B and GPT-3 175B. Their key findings are:- Finetuning smaller models on LLMs' CoT improves task performance across arithmetic, commonsense, and symbolic reasoning datasets. For example, finetuning boosts T5 XXL's accuracy on GSM8K from 8.11% to 21.99%.- The proposed method is robust to different teacher model architectures. Finetuning on GPT-3 175B's CoT also improves T5 XXL's performance. - There is a trade-off between model size, dataset size, and accuracy. Smaller student models can match the baseline accuracy of larger models when trained on CoT data. The method is also more data-efficient than regular finetuning.- The benefits are task-dependent. For tasks requiring factual knowledge, improvements are limited as smaller models likely lack the knowledge compared to larger, higher capacity models.In summary, the key contribution is a method to transfer reasoning abilities from huge LLMs to smaller models via CoT knowledge distillation and an analysis of how model size, dataset size, and task nature affect the transfer.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

The paper proposes a method to transfer the reasoning capabilities of large language models to smaller models via knowledge distillation. Specifically, it generates chains of reasoning from large models and uses them to finetune smaller models, thereby teaching the smaller models to reason in a similar way.


## How does this paper compare to other research in the same field?

Here are a few key ways this paper compares to other research on teaching reasoning skills to small language models:- The approach of using knowledge distillation from large pretrained language models is novel compared to prior work on improving reasoning in small models. Most prior work has focused on approaches like prompting, while this paper explores transferring reasoning skills from large teacher models.- The paper provides a more comprehensive evaluation on multiple reasoning tasks (arithmetic, commonsense, symbolic) compared to related works that often focus on just one task. This allows better analysis of how the approach transfers across different types of reasoning.- The ablation studies on model size, dataset size, and teacher model choice provide useful insights into how these factors impact transfer of reasoning skills. This helps understand the trade-offs and limitations of the knowledge distillation approach.- The gains in accuracy, particularly on arithmetic reasoning datasets like GSM8K and MAWPS, are quite substantial compared to prior works. The paper shows bigger improvements on these tasks than related concurrent works exploring knowledge distillation for reasoning.- The paper explores knowledge distillation in a multi-step pipeline rather than just using soft target distillation. The two-step process of generating chain of thought data from the teacher then training the student on that data is novel.Overall, I would say the comprehensive analysis on multiple tasks, extensive ablation studies, and novel distillation pipeline make valuable contributions compared to related works on improving reasoning abilities of small language models. The gains on arithmetic reasoning are particularly noteworthy.


## What future research directions do the authors suggest?

Based on my reading of the paper, some of the main future research directions suggested by the authors are:- Explore improving the reasoning of small models in multi-task settings. The current work focuses on individual reasoning tasks, so it would be interesting to see if similar improvements can be achieved when training the student model on multiple reasoning tasks at once.- Generate new training data using large language models, rather than just annotating existing datasets. The authors suggest that large LMs could potentially be used to generate novel reasoning chains from scratch, providing more training data to further improve student models.- Experiment with different student and teacher model combinations. The current work focuses on distilling knowledge from PaLM 540B and GPT-3 175B into T5 models. Testing other model architectures as teachers and students could reveal new insights. - Improve prompting approaches for generating the teacher's reasoning chains, such as exploring self-consistency. The authors note they only used the original CoT prompting method, but other prompting innovations could further enhance the teacher's reasoning abilities.- Explore the model size and data size trade-offs in more depth. The authors provide some initial analysis, but more work could be done to find the optimal balance between these factors.- Test the approach on a wider range of languages and task formats beyond English and single-task reasoning. This could enhance the generalizability of the method.In summary, the main suggested future work revolves around broadening the approach to more tasks, models, and data; and further optimizing the student-teacher training process.


## Summarize the paper in one paragraph.

Here is a one paragraph summary of the paper:This paper explores transferring the reasoning capabilities of large language models (LLMs) like PaLM 540B and GPT-3 175B to smaller language models through knowledge distillation. The authors propose finetuning student models like different sizes of T5 on the chain of thought (CoT) reasoning generated by the LLMs on question answering datasets covering arithmetic, commonsense, and symbolic reasoning. Their key finding is that finetuning smaller models on CoT data from LLMs significantly improves task performance across datasets compared to just finetuning on the original data, especially for arithmetic reasoning. For example, finetuning T5 XXL on PaLM 540B's CoT improves accuracy on GSM8K from 8.11% to 21.99%. The authors also show tradeoffs between model size, dataset size, and accuracy. Overall, the work demonstrates that CoT knowledge distillation can transfer reasoning abilities from huge models to smaller ones.
