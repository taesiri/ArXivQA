# [CTVIS: Consistent Training for Online Video Instance Segmentation](https://arxiv.org/abs/2307.12616)

## What is the central research question or hypothesis that this paper addresses?

Based on my understanding, the central research question this paper tries to address is: How can we improve the discrimination of instance embeddings for better associating instances across time in online video instance segmentation (VIS)?The key hypothesis is that recent online VIS methods do not learn highly discriminative embeddings because they rely on contrastive items sourced only from single reference frames. To learn more robust embeddings, the authors propose to align the training and inference pipelines in terms of how contrastive items are constructed.In summary, the paper focuses on improving instance embedding learning for better instance association in online VIS. The core idea is to build more reliable contrastive items during training by incorporating techniques used in the inference pipeline like memory banks, momentum-averaged embeddings, and noise. This allows the model to learn from more stable historical representations instead of just instantaneous embeddings from single frames.


## What is the main contribution of this paper?

Based on my understanding, the main contributions of this paper are:1. It proposes a simple yet effective training framework called CTVIS (Consistent Training for Online Video Instance Segmentation) for online video instance segmentation. The key idea is to align the training and inference pipelines in terms of how contrastive items (sets of anchor, positive and negative embeddings) are constructed. 2. It introduces several techniques to build better contrastive items during training: using a memory bank to store more stable momentum-averaged embeddings, adding noise to simulate ID switching, and using long video clips to provide more context. This results in more discriminative instance embeddings.3. It proposes a method to generate pseudo-training videos from images using data augmentation techniques like rotation, cropping, and copy-paste. This allows training competitive models using only sparse image annotations rather than dense video annotations.4. The proposed CTVIS framework achieves new state-of-the-art results on standard video instance segmentation benchmarks like YouTube-VIS 2019, 2021 and OVIS, outperforming previous methods by large margins.In summary, the key novelty is in aligning training and inference for online VIS via better contrastive learning on long videos, and showing strong performance even when trained on pseudo-videos from sparse image annotations.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper proposes a new training strategy called Consistent Training for Online Video Instance Segmentation (CTVIS) that aligns the training and inference pipelines to learn more robust and discriminative instance embeddings for improved video instance segmentation performance.
