# [Learning the Expected Core of Strictly Convex Stochastic Cooperative   Games](https://arxiv.org/abs/2402.07067)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- The paper studies stochastic cooperative games where the reward function is unknown and modeled as a random variable. The goal is to learn a stable allocation (point in the expected core) that satisfies the core constraints in expectation.

- Prior works assume knowledge of the reward distribution or full-information feedback. This paper considers the challenging bandit feedback setting where only the reward of the queried coalition is observed.

- Learning the expected core with bandit feedback and without additional assumptions requires exponentially many queries, making it intractable. 

Proposed Solution:
- The paper focuses on strictly convex games and makes assumptions to ensure the core has sufficient width. These help restrict the search space and enable tractability.

- A new "Common-Points-Picking" algorithm is proposed that estimates the vertices of the core by sampling cyclic permutation orders. It constructs half-spaces based on confidence sets to find a common stable point.

Main Contributions:

- First algorithm to learn the expected core of strictly convex games under bandit feedback with high probability using only polynomial samples.

- Novel extension of the hyperplane separation theorem to show existence of common points among multiple confidence sets.

- New results in convex geometry relating width of permutation polytopes to sample complexity.

- Analysis showing sample complexity scales as O(n^5) with constants capturing width and convexity parameters of the game. Highly adaptive to complexity of game instance.

- Algorithm and analysis technique may find use in other multi-agent learning problems involving convex geometry.


## Summarize the paper in one sentence.

 Here is a one sentence summary of the key points from the paper:

The paper develops a sample efficient algorithm for learning stable reward allocations in stochastic cooperative games with uncertainty, by leveraging assumptions of strict convexity and bounded width of the expected core polytope to enable polynomial time identification of a common allocation satisfying core constraints with high probability.


## What is the main contribution of this paper?

 The main contributions of this paper are:

1. It introduces an algorithm called \texttt{Common-Points-Picking} to learn a stable allocation (a point in the expected core) in a stochastic cooperative game. This algorithm is able to return a point in an unknown simplex given access to noisy estimates of the simplex's vertices.

2. It provides an analysis of the sample complexity and stopping condition for the \texttt{Common-Points-Picking} algorithm to guarantee that it returns a point in the expected core with high probability. 

3. The analysis involves developing new results in convex geometry, including an extension of the separation hyperplane theorem and bounding the width of the core polytope. These geometric results may be of independent interest.

4. Under the assumptions of strict convexity and bounded width of the core polytope, the algorithm is shown to return a stable allocation with high probability given a polynomial number of samples in the number of players. Specifically, $O(n^5 \log(1/\delta) / \varsigma^4)$ samples suffice where $n$ is the number of players, $\delta$ is the failure probability, and $\varsigma$ quantifies the degree of strict convexity.

In summary, the key contribution is an algorithm along with its analysis to enable stable allocation learning in stochastic cooperative games under certain convexity assumptions. The analysis relies on and develops some new results in convex geometry as well.


## What are the keywords or key terms associated with this paper?

 Based on my review of the paper, here are some of the key terms and keywords that seem most relevant:

- Stochastic cooperative games
- Expected core
- Credit assignment
- Convex games
- Strictly convex games
- Generalized permutahedra 
- Sample complexity
- Common points
- Separating hyperplanes
- Core constraints
- Stability
- Reward allocation

The paper focuses on the problem of learning the expected core in stochastic cooperative games, specifically for the class of strictly convex games. Key concepts include the expected core, which refers to core constraints/stability satisfied in expectation, credit assignment for determining stable reward allocations, convexity assumptions on the cooperative game, and notions from convex geometry that are leveraged in the analysis like generalized permutahedra. The main technical contribution involves an algorithm called Common-Points-Picking for identifying points in the expected core given uncertain knowledge of the reward distribution, along with a sample complexity analysis using tools like separating hyperplanes.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 potential in-depth questions about the method proposed in this paper:

1. The paper assumes the game is $\varsigma$-strictly convex. What challenges would arise if we relaxed this assumption? Could the algorithm still work if strict convexity was violated for some coalitions? 

2. Assumption 2 bounds the width of the cyclic permutation matrix $W$. What techniques could be used to theoretically prove or disprove whether $c_W$ remains small for most convex games?

3. The Common-Points-Picking algorithm estimates the marginal vectors for a set of $n$ permutation orders. What impact would using a different set of permutations have? Could adaptive permutation selection improve performance?  

4. Theorem 1 shows low dimensional cores cannot be learned with finite samples. Does this impossibility result hold for other solution concepts like the Shapley value? What core dimensionality guarantees learnability?

5. The stopping condition involves constructing separating hyperplanes. What computational and geometric insights allowed the construction of this novel condition? 

6. How does the sample complexity scale in terms of $n, \varsigma,$ and $c_W$? Can we further optimize dependencies on these parameters? 

7. Rather than returning the average marginal vector, can we provide guarantees for other core selection rules like the least squares solution?

8. The paper focuses on deterministic stopping rules. Could statistical stopping criteria utilizing confidence intervals improve performance? 

9. No regret/best arm identification bandit techniques are utilized. Could these further reduce sample complexity?

10. The uniform sampling scheme could be improved via adaptive sampling. What existing adaptive sampling techniques are best suited for this problem?
