# [VidLA: Video-Language Alignment at Scale](https://arxiv.org/abs/2403.14870)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper "VidLA: Video-Language Alignment at Scale":

Problem:
- Video-language alignment is challenging due to lack of large-scale aligned video-text data and inability to capture both short and long-range temporal relationships in videos. 
- Existing methods use complex hierarchical architectures that cannot leverage powerful pretrained image-text models.

Proposed Solution:
- Construct YT-VidLA-800M, the largest video-text dataset with 800M clips using LLMs to generate better aligned descriptions.
- Propose a simple two-tower model with hierarchical temporal attention to capture multi-scale semantics. It uses "concept tokens" to summarize video content hierarchically.  
- Keep model simple to initialize from pretrained image-text models like CLIP.

Key Contributions:
- Novel data curation strategies to create large-scale video-text data using LLMs for caption generation and text summarization.
- Hierarchical temporal attention design that models local short-range motion as well as global long-range semantics at different temporal scales.
- Achieves new SOTA on multiple retrieval benchmarks, especially for longer videos. Better generalizes across temporal scales.
- Demonstrates effectiveness of large-scale pretraining for video-language tasks. Simple architecture efficiently utilizes powerful image-text foundation models.

In summary, the paper introduces simple and effective solutions for large-scale video-language alignment to advance research in this direction. The ideas can provide guidance for future efforts aiming to leverage foundation models for video understanding.
