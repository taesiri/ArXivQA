# [Indoor Millimeter Wave Localization using Multiple Self-Supervised Tiny   Neural Networks](https://arxiv.org/abs/2311.18732)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a high-quality paragraph summarizing the key points of the paper:

This paper proposes using multiple small neural network models, each trained on a different section of a large indoor environment, for accurate device localization. Rather than training one large model to cover the entire space, several "tiny" models are trained in a self-supervised way using noisy angle of arrival measurements and location labels from a separate localization algorithm. To determine which model to use as the device moves, the authors develop Kalman filter-based and out-of-distribution detection schemes to dynamically switch between models. Simulation results in a U-shaped room with three sections show that reconstructing the device's trajectory by switching models can achieve 90% sub-meter accuracy, outperforming both the bootstrapping algorithm alone and a single neural network. The strengths are distributed training, simplified models, and robust switching logic to leverage the multiple tiny networks. This approach is promising for scalable and accurate device tracking across large, complex indoor spaces.


## Summarize the paper in one sentence.

 This paper proposes using multiple self-supervised tiny neural networks, each trained on different sections of a large indoor environment, along with schemes to dynamically switch between them for accurate millimeter wave-based localization of a mobile client.


## What is the main contribution of this paper?

 Based on reviewing the paper, the main contribution is proposing to use multiple tiny neural network models, each trained on a different section of a large indoor environment, to localize a mobile client. The key ideas include:

1) Training multiple small neural networks in a self-supervised manner using labels from a bootstrapping localization algorithm, instead of a single large neural network model. This reduces complexity.

2) Proposing two schemes to dynamically switch between the neural network models as the client moves through different sections:
- A Kalman filter-based scheme that uses state innovation to detect when to switch
- An out-of-distribution detection scheme that uses the statistical distribution of the training labels 

3) Showing through simulations that switching between multiple neural network models using the proposed schemes achieves higher localization accuracy compared to using a single model or standard geometric localization algorithms.

In summary, the main contribution is the idea of using multiple self-supervised tiny neural networks with intelligent switching schemes to enable accurate and lightweight localization across large indoor spaces.


## What are the keywords or key terms associated with this paper?

 Based on my review of the paper, some of the key terms and keywords associated with it are:

- Millimeter wave (mmWave)
- Indoor localization
- Tiny neural networks
- Self-supervised learning
- Multiple neural networks
- Switching schemes
- Kalman filter
- Out-of-distribution detection
- Angle difference of arrival (ADoA)
- Angle of arrival (AoA)
- Angle of departure (AoD) 
- Time of flight (ToF)
- Multilayer perceptron
- Regression
- Mean square error (MSE) 
- Innovation
- Mahalanobis distance

The paper proposes using multiple tiny neural networks trained in a self-supervised manner to localize a mmWave client moving in a large indoor area. It introduces two switching schemes - one based on a Kalman filter and the other based on statistical distribution of labels - to determine when to switch between the neural networks to maintain localization accuracy. Key terms like mmWave, indoor localization, tiny neural networks, self-supervised learning, switching schemes, Kalman filter, ADoA, AoA, MSE are integral to understanding the techniques and contributions of this work.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in the paper:

1. The paper mentions using multiple tiny neural networks instead of a single deep model. What are the key advantages and disadvantages of this approach? How does it help with scaling to larger environments?

2. The paper exploits two schemes - Kalman filter (KF)-based and out-of-distribution detection (ODD) - to determine when to switch between neural networks. Compare and contrast these two schemes. What are the tradeoffs involved?

3. The KF-based scheme uses the innovation $\hat{\mathbf{y}}_t$ and normalized innovation squared (NIS) metric $\beta_t$. Explain the significance of these two metrics and how they are useful for deciding when to switch neural networks.  

4. The ODD scheme relies on computing the Mahalanobis distance between the location estimates and the distribution of the training labels. Intuitively explain why this distance metric is relevant for detecting when the neural network is providing out-of-distribution estimates.

5. Both schemes seem to require setting certain key thresholds - $\eta$ for KF and $\zeta$ for ODD. What impact do these thresholds have? How can they be set optimally?

6. The neural network architecture uses only 4 layers. Discuss the considerations in designing such a shallow model. What hyperparameters were tuned for model selection?

7. The paper obtains self-supervised training labels using the JADE algorithm. Why is JADE well-suited for this purpose? What are its limitations?

8. Fig. 3 analyzes the behavior of the NIS metric $\beta_t$. Explain the trends observed in $\beta_t$ as the client moves across different sections. How does this support the working of the KF-based scheme?

9. How reliable is the ODD scheme in environments where different sections have overlapping visibility to the same set of access points? When does it break down?

10. The results show improved accuracy over using a single neural network. Could there be scenarios where a single deeper model may be more appropriate? What would be the tradeoffs?
