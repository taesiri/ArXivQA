# [DreamHuman: Animatable 3D Avatars from Text](https://arxiv.org/abs/2306.09329)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is: How can we generate realistic, animatable 3D human avatars from just textual descriptions, without any paired text-to-3D training data?The key points are:- The paper presents a method called DreamHuman to generate 3D human avatars from text prompts. - The goal is to create animated 3D models that match the textual description, with realistic appearance, clothing, and body shape.- The key innovation is doing this without any supervised text-to-3D training data. - The method combines text-to-image diffusion models, neural radiance fields, and 3D statistical human body models.- The human body model acts as a strong prior to regularize the avatar structure and appearance during optimization.- This allows generating animatable avatars that can be reposited in new poses.So in summary, the main research question is how to leverage recent advances in generative models to create controllable, animatable 3D human avatars from text alone, without paired text-to-3D supervision. The paper presents a novel approach and demonstrates high quality 3D human generation.


## What is the main contribution of this paper?

The main contribution of this paper is presenting a method called DreamHuman to generate realistic and animatable 3D human avatar models solely from textual descriptions. The key aspects are:- DreamHuman combines large language models, neural radiance fields, and statistical human body models to generate 3D human avatars with realistic appearance, clothing, and body shapes from text prompts. - The method incorporates human body priors to regularize the avatar structure and ensure anthropometric consistency of complex structures like limbs and fingers. This helps avoid undesirable artifacts like missing limbs that general text-to-3D methods can produce.- The avatars can be animated and placed in different poses based on a set of 3D poses or motions, without needing additional training or fine-tuning. This is enabled by modeling non-rigid, pose-dependent surface deformations.- Semantic zooming is used to add detail in perceptually important regions like the face and hands by modifying the prompts and rendering higher resolution views of those areas.- The method is trainable without any supervised text-to-3D data by relying on score distribution sampling losses from a text-to-image diffusion model.In summary, the key contribution is a novel framework to generate realistic, animatable 3D human avatars from text through the combination of neural radiance fields, statistical body models, and advancements in text-to-image synthesis. The results significantly outperform prior work in 3D avatar generation.
