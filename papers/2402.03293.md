# [Flora: Low-Rank Adapters Are Secretly Gradient Compressors](https://arxiv.org/abs/2402.03293)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
Modern deep neural networks require large amounts of memory to store optimization states like momentum and gradient statistics during training. For example, the 175B parameter GPT-3 model requires an additional 1.4TB memory just to store these states. This poses scaling challenges for training even larger models. Existing solutions like the low-rank adaptation (LoRA) method constrain model updates to be low-rank, harming model performance. 

Proposed Solution:
The paper proposes Flora, a method that views LoRA through the lens of random projections. Specifically, Flora shows LoRA can be approximated as: (1) compressing gradients via a random projection, (2) updating model weights using the compressed gradients, and (3) decompressing the gradients before the next step via another random projection. Based on this view, Flora directly compresses gradients and decompresses them before weight updates, allowing high-rank updates while using low-rank gradients internally. This is done by resampling the random projection at each step. The compressed gradients enable sublinear memory for optimization states like momentum and gradient accumulation.

Main Contributions:
- Provides a novel view of LoRA as approximating random projections, giving theoretical justifications.
- Proposes Flora which resamples projections at each step, enabling high-rank updates with low-rank gradient statistics stored internally.
- Achieves similar performance as uncompressed, full-matrix updates while using sublinear memory for optimization states.
- Empirically demonstrates memory savings and strong performance across tasks like summarization and translation compared to baselines.

In summary, the paper interprets LoRA as gradient compression via random projections, and uses this view to design Flora, an optimizer that compresses optimization states to sublinear memory while allowing high quality updates. Experiments verify Flora's effectiveness.
