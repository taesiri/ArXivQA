# [HaMuCo: Hand Pose Estimation via Multiview Collaborative Self-Supervised   Learning](https://arxiv.org/abs/2302.00988)

## What is the central research question or hypothesis that this paper addresses?

 The central research question this paper addresses is how to perform 3D hand pose estimation from a single RGB image in a self-supervised manner using multi-view data during training. The key ideas are:- Leveraging multi-view images as a form of weak supervision to train a single-view 3D hand pose estimator without requiring ground truth 3D labels. - Designing a cross-view interaction network to distill more reliable pseudo labels for the single-view estimator by enforcing multi-view consistency and capturing complementary information across views.Specifically, the paper proposes a framework called HaMuCo that contains:- A single-view estimator that predicts 3D hand pose from a single RGB image. It uses a MANO model to provide hand shape priors.- A cross-view interaction network that takes multi-view images as input and conducts feature interaction across views. It supervises the single-view estimator via multi-view distillation.The core hypothesis is that by utilizing multi-view consistency and cross-view feature interaction during training, the framework can learn an effective single-view 3D hand pose estimator without requiring ground truth 3D annotations.
