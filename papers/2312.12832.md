# [Turning Dust into Gold: Distilling Complex Reasoning Capabilities from   LLMs by Leveraging Negative Data](https://arxiv.org/abs/2312.12832)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Large language models (LLMs) like GPT-3 show strong reasoning capabilities, but their large size makes deployment difficult. 
- Prior work has tried distilling reasoning skills from LLMs into smaller models using chain-of-thought reasoning paths. However, they only use positive samples and discard negative ones with incorrect answers.

Proposed Solution:
- The paper proposes a framework to distill knowledge from both positive and negative samples from the LLM. 
- It consists of 3 main steps:
   1. Negative Assistant Training: Train a separate model on negative samples and use attention to selectively integrate its knowledge when training on positive samples.
   2. Negative Calibrated Enhancement: Use the negative model as a baseline to highlight critical knowledge when doing self-distillation on the positive model.  
   3. Adaptive Self-Consistency: Train a ranking model on positive/negative samples to score multiple candidate answers and more accurately select the final answer.

Main Contributions:  
- Show the value of negative samples for distilling reasoning skills, which is overlooked in prior work
- Propose a comprehensive framework with 3 complementary steps to effectively exploit knowledge from both positive and negative LLM samples
- Extensive experiments on math reasoning datasets demonstrate significant gains over baselines by leveraging negative information

In summary, the key idea is to fully utilize the knowledge and behaviors from both correct and incorrect LLM outputs to better transfer reasoning capabilities to smaller models across training and inference. Both positive and negative views are important for distillation.
