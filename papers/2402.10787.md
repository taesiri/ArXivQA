# [EdgeQAT: Entropy and Distribution Guided Quantization-Aware Training for   the Acceleration of Lightweight LLMs on the Edge](https://arxiv.org/abs/2402.10787)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
Despite the remarkable progress of large language models (LLMs), their deployment on resource-constrained edge devices faces challenges due to massive model parameters and computations. Quantization can generate lightweight LLMs for efficient inference, but suffers from accuracy loss, especially when quantizing weights, activations and key-value caches together to very low bitwidths like 4-8 bits. Existing quantization-aware training (QAT) works focus only on weight quantization, lacking acceleration from activation quantization.  

Proposed Solution - EdgeQAT:
The paper proposes EdgeQAT, an entropy and distribution guided QAT framework to optimize lightweight LLMs and achieve acceleration on edge devices. The key ideas are:

(1) Identify that performance drop mainly comes from information distortion in quantized attention maps, visualized via different distributions in quantized query and key matrices.

(2) Propose entropy and distribution guided optimization during training to maximize entropy of query/key (minimizing quantization error) and minimize difference between quantized and FP16 attention maps.  

(3) Design token importance-aware adaptive quantization to assign more bits to important tokens and fewer bits to unimportant ones, further reducing quantization bitwidth.

Main Contributions:

(1) Novel entropy and distribution guided QAT method to mitigate information loss in quantized query, key and attention maps.

(2) Token importance-aware adaptive mixed-precision quantization for further optimization.

(3) State-of-the-art accuracy comparable to FP16 models, and up to 2.37x on-device speedup across smartphones, Raspberry Pis and FPGAs, enabling efficient LLM deployment.

In summary, the paper makes significant contributions in guided QAT and mixed-precision quantization to accelerate LLMs on resource-constrained edge devices with minimal accuracy loss.
