# [GIVT: Generative Infinite-Vocabulary Transformers](https://arxiv.org/abs/2312.02116)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a detailed paragraph summarizing the key points of the paper:

This paper proposes generative infinite-vocabulary transformers (GIVT), a new type of generative transformer model for continuously modeling sequences of real-valued vectors instead of discrete tokens. The key ideas are: 1) Replace the input embedding lookup table with a linear projection layer to embed the real-valued input vectors, and 2) Replace the categorical output distribution with a continuous distribution like a Gaussian mixture to generate real-valued vectors. This avoids issues with standard VQ-VAE tokenization like limited codebook capacity and large embedding matrices. The authors apply GIVT to model the latent space of a Gaussian VAE trained on images, adapting techniques like temperature sampling and classifier-free guidance. Without needing specialized VQ-VAE training, they show GIVT variants competitive with or superior to VQ-VAE-based transformers like VQ-GAN and MaskGIT on class-conditional image generation. They also demonstrate promising results on dense prediction tasks like segmentation and depth estimation by using GIVT within the UViM framework. The simplicity of the approach combined with strong performance across tasks highlights the potential of this method for generative sequence modeling.
