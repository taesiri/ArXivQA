# [Fastformer: Additive Attention Can Be All You Need](https://arxiv.org/abs/2108.09084)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is:How can we design an efficient Transformer model that can effectively model long text sequences?The key points are:- Standard Transformer models have quadratic complexity with respect to sequence length due to the self-attention mechanism. This makes them inefficient for long sequences.- Prior efficient Transformer variants using sparse attention or low-rank approximations have limitations in fully capturing global contexts or being efficient enough on very long sequences.- This paper proposes Fastformer, an efficient Transformer variant based on additive attention, which can model long sequences with linear complexity while capturing contextual information effectively. - The core ideas are using additive attention to summarize the query and key matrices into global vectors, and modeling the interactions between global vectors and individual token vectors via element-wise product.- Experiments on various tasks like text classification, news recommendation, and summarization show Fastformer is much more efficient than Transformer baselines while achieving competitive accuracy on long sequences.In summary, the central hypothesis is that using additive attention and element-wise products for global-local interactions can enable both efficiency and accuracy in long sequence modeling for Transformers. The paper provides empirical evidence to validate this hypothesis across different tasks.
