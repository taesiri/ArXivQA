# [Fastformer: Additive Attention Can Be All You Need](https://arxiv.org/abs/2108.09084)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is:How can we design an efficient Transformer model that can effectively model long text sequences?The key points are:- Standard Transformer models have quadratic complexity with respect to sequence length due to the self-attention mechanism. This makes them inefficient for long sequences.- Prior efficient Transformer variants using sparse attention or low-rank approximations have limitations in fully capturing global contexts or being efficient enough on very long sequences.- This paper proposes Fastformer, an efficient Transformer variant based on additive attention, which can model long sequences with linear complexity while capturing contextual information effectively. - The core ideas are using additive attention to summarize the query and key matrices into global vectors, and modeling the interactions between global vectors and individual token vectors via element-wise product.- Experiments on various tasks like text classification, news recommendation, and summarization show Fastformer is much more efficient than Transformer baselines while achieving competitive accuracy on long sequences.In summary, the central hypothesis is that using additive attention and element-wise products for global-local interactions can enable both efficiency and accuracy in long sequence modeling for Transformers. The paper provides empirical evidence to validate this hypothesis across different tasks.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contributions of this work are:1. The proposal of Fastformer, a new Transformer architecture based on additive attention that can process long sequences efficiently in linear time complexity. 2. The use of element-wise products to model the interactions between global context vectors and individual token representations. This allows Fastformer to incorporate global context information efficiently.3. Extensive experiments on multiple datasets demonstrating that Fastformer is much faster than many existing Transformer variants while achieving competitive performance on tasks like sentiment classification, news recommendation, and text summarization.In summary, the key novelty is the design of Fastformer using techniques like additive attention and element-wise products to enable modeling long sequences with global contexts in linear complexity. The experimental results validate that this approach can speed up Transformers substantially without sacrificing much accuracy. The efficient incorporation of global context is the main technical contribution of this work.
