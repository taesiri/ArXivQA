# [FFSplit: Split Feed-Forward Network For Optimizing Accuracy-Efficiency   Trade-off in Language Model Inference](https://arxiv.org/abs/2401.04044)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Large pre-trained language models (LLMs) have high computational and memory costs, making deployment on commodity hardware challenging. 
- The feedforward network (FFN) component is a key efficiency bottleneck, accounting for ~2/3 of parameters and inference latency.

Observations:
- The paper observes that only a few "heavy hitter" neurons in the FFN have large output norm for any input token, while most neurons are sparsely activated.  
- Removing heavy hitter neurons causes a large drop in accuracy, while removing other low-activation "light hitter" neurons has little impact.

Proposed Method: 
- Explicitly split the FFN into two parts based on heavy hitter neurons. 
- When applying compression techniques like low-rank decomposition or quantization, allocate more resources (less compression) to the FFN part with heavy hitters.
- This allows compressing the FFN while preserving accuracy.

Contributions:
- Show heavy hitter neurons exist and are crucial for accuracy even for non-ReLU LLMs.  
- Propose a method to split FFN based on heavy hitters and allocate resources accordingly during compression.
- Achieve 1.25-1.56x inference speedup with 43.1% fewer parameters and negligible accuracy drop.
- Integrate method with different compression techniques like low-rank decomposition and quantization to improve their efficiency-accuracy trade-off.

In summary, the key idea is to identify important heavy hitter neurons in the FFN and preserve them during compression to optimize for efficiency while maintaining accuracy. The method explicitly splits the FFN to allow allocating resources in a more granular way.


## Summarize the paper in one sentence.

 This paper proposes to split the feed-forward network in transformers into two parts based on "heavy hitter" neurons, and allocate more resources to the part with heavy hitters during model compression to improve the efficiency-accuracy trade-off.


## What is the main contribution of this paper?

 Based on my understanding, the main contributions of this paper are:

1) The authors found that only a few neurons (called "heavy hitters") in the feed-forward network (FFN) of transformers have large output norm, while most other neurons are sparsely activated. 

2) Leveraging this observation, the authors propose to explicitly split the FFN into two parts - one containing the heavy hitters and another containing the remaining neurons.

3) When applying compression techniques like pruning or quantization, the authors allocate more resources (less compression) to the FFN part with heavy hitters in order to preserve accuracy. The FFN part without heavy hitters is compressed more aggressively.  

4) Experiments on BERT and large language models show that splitting FFN and protecting the heavy hitters results in better accuracy-efficiency trade-offs compared to vanilla compression techniques. Specifically, the authors are able to reduce parameters by 43.1% and achieve 1.25-1.56x inference speedup with negligible accuracy drop.

In summary, the key idea is to identify and preserve the computationally critical heavy hitter neurons in order to improve the accuracy-efficiency trade-off of compressed transformers. The explicit FFN splitting provides a simple but effective way to achieve this.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and concepts are:

- Feed-forward network (FFN): The feed-forward network component of transformer models, which accounts for a large portion of parameters and computation. Identified as a key efficiency bottleneck. 

- Heavy hitters: The observation that only a small subset of FFN neurons have consistently large outputs across tokens. The paper refers to these as "heavy hitters".

- Explicitly splitting FFN: The proposed method of explicitly splitting the FFN into two parts based on the heavy hitters. This allows allocating more resources to the heavy hitter part.

- Accuracy-efficiency tradeoff: Optimizing the balance between model accuracy and efficiency (size, speed etc). A central goal when deploying large language models.

- Model compression techniques: Methods like pruning, quantization etc that aim to make models smaller and faster but usually hurt accuracy.

- Low rank decomposition: One specific compression technique explored. Decomposes weight matrices into low rank approximations.

So in summary - heavy hitters, explicitly splitting FFN, accuracy-efficiency tradeoff, model compression techniques aimed at FFN module.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 potential in-depth questions about the method proposed in this paper:

1. The paper hypothesizes that "heavy hitter" neurons exist in non-ReLU based transformers like BERT. What is the mathematical intuition behind this hypothesis? How does it relate to the characteristics of activation functions like GELU?

2. The paper proposes explicitly splitting the FFN into two parts based on heavy hitter neurons. What is the motivation behind retaining a dense matrix formulation instead of immediately sparsifying the FFN? How does this connect to potential hardware efficiency gains? 

3. What mathematical explanation is provided in Equation 4 for why removing heavy hitter neurons causes a larger accuracy drop compared to removing light hitter neurons? Can you explain the terms in more detail?

4. When applying compression techniques like low-rank decomposition, why does the method allocate more resources to the FFN component with heavy hitters? What is the hypothesis tested here?

5. For transformer models beyond BERT, what types of quantization and pruning techniques could be integrated with the proposed explicit FFN splitting? What hyperparameters may need to be tuned?

6. The experiments show wall clock time speedups on CPUs and GPUs from the method. What underlying hardware or software factors contribute to these gains? How do batch size and sequence length impact gains?

7. How was the set of heavy hitter neurons determined in the experiments? Was any sensitivity analysis conducted on the percentage chosen? What impact would this have?

8. Could the proposed method offer accuracy or efficiency benefits for tasks requiring autoregressive decoding like language modeling? What modifications would be needed?

9. For larger transformer models, how might the heavy hitter phenomenon change? Would the method still be applicable or would modifications be required?

10. The method improves accuracy vs efficiency trade-offs for BERT compression. How well might it transfer to compressed versions of models like GPT-3 or T5? What architecture differences need to be considered?
