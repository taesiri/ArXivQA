# [EfficientViT: Memory Efficient Vision Transformer with Cascaded Group   Attention](https://arxiv.org/abs/2305.07027)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central research question addressed in this paper is: How to design efficient vision transformer (ViT) models that achieve a good trade-off between accuracy and inference speed? Specifically, the paper aims to explore principles and methods to improve the efficiency of ViT models in terms of memory access, computation redundancy, and parameter usage. The goal is to develop ViT models that have fast inference speed while maintaining competitive accuracy compared to state-of-the-art CNN and ViT models.The key hypothesis is that by systematically analyzing and addressing the speed bottlenecks in standard ViT models, it is possible to design ViT models that are significantly faster yet without compromising much on accuracy. The paper proposes a new family of models called EfficientViT that implements techniques like sandwich layout blocks, cascaded group attention, and parameter reallocation to improve model efficiency. The central question is whether these techniques can strike a good balance between speed and accuracy for ViT models.In summary, the main research question is how to design fast yet accurate ViT models, and the central hypothesis is that through specialized architecture designs that improve memory, computation and parameter efficiency, it is possible to achieve such an efficiency-accuracy trade-off. The EfficientViT model family is proposed to validate this hypothesis.


## What is the main contribution of this paper?

The main contribution of this paper is proposing a new family of efficient vision transformer (ViT) models called EfficientViT. The key ideas and contributions are:- Analyzing the factors that affect inference speed of ViTs, including memory access, computation redundancy, and parameter usage. The analysis provides useful guidelines for designing fast ViT models.- Proposing a new EfficientViT building block with a sandwich layout that uses fewer memory-inefficient multi-head self-attention (MHSA) layers and more memory-efficient feedforward network (FFN) layers. This reduces memory access overhead.- Introducing a cascaded group attention (CGA) module that feeds different splits of features to each attention head to improve diversity and save computation. The cascaded design further enhances representations.- Reallocating parameters by expanding dimensions of critical components like value projections while shrinking redundant ones like FFN hidden dimensions, improving parameter efficiency.- Based on these ideas, designing a family of EfficientViT models with different width/depth trade-offs. Extensive experiments show they achieve much faster inference speed compared to CNNs and ViTs with similar accuracy.In summary, the key contribution is designing a set of fast yet accurate vision transformer models and providing useful principles and analysis to guide efficient ViT architecture design. The proposed EfficientViT models demonstrate superior efficiency-accuracy trade-offs.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper proposes a new family of efficient vision transformer models called EfficientViT, which achieves a good trade-off between speed and accuracy by designing memory-efficient operations, addressing computation redundancy, and reallocating parameters.
