# [ContraQA: Question Answering under Contradicting Contexts](https://arxiv.org/abs/2110.07803)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the main research question seems to be: How do question answering (QA) models behave under contradicting contexts that contain a mix of both real and fake/misleading information?The key aspects are:- The paper investigates the impact of misinformation/disinformation on QA models by constructing contradicting contexts that mix real information with fake information created both manually (by humans) and automatically (by neural models). - The hypothesis seems to be that existing QA models will struggle or be misled when presented with these kinds of contradicting contexts containing misinformation. - The paper aims to analyze the vulnerability of QA models to misinformation and propose methods to build more robust QA systems that can handle contradicting contexts.So in summary, the central research question is investigating and analyzing how QA models perform on contradicting contexts with real + fake information, in order to understand their vulnerability to misinformation and explore ways to improve robustness. The key hypothesis is that current QA models will struggle with such contexts.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contribution seems to be:1) Constructing a new dataset called ContraQA, which contains over 10K human-written and model-generated contradicting pairs of contexts based on the SQuAD dataset. 2) Proposing a model called BART-FG for generating fake contexts by iteratively masking and re-generating constituents in the original context.3) Analyzing the vulnerability of QA models under contradicting contexts mixed with real and fake information. The results show QA models suffer significant performance drops on ContraQA compared to SQuAD.4) Proposing a misinformation-aware QA system that integrates question answering and fake context detection, which improves performance under contradicting contexts. In summary, the key contribution is creating a new dataset and threat model to analyze the impact of contradicting contexts and misinformation on QA systems, and proposing methods to make QA systems more robust under such conditions. The paper focuses on studying contradicting information that brings conflicting facts to the QA context.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper introduces ContraQA, the first large-scale dataset for studying the impact of misinformation on question answering systems, containing over 10K contradicting context pairs written by humans and generated by models, and shows that existing QA systems are vulnerable to such misinformation but can be made more robust by integrating misinformation detection.


## How does this paper compare to other research in the same field?

Here are a few key points comparing this paper to other related research:- This paper focuses specifically on studying the impact of misinformation and contradicting contexts on question answering systems. Much prior work has focused on adversarial attacks or robustness in QA, but not through the lens of misinformation. This provides a new perspective on QA robustness.- The paper introduces a new dataset, ContraQA, containing over 10k examples of contradicting contexts mixed with real and fake information. This is the first dataset of its kind for studying QA under misinformation. Prior work has not looked at contradicting contexts or included both human-written and AI-generated misinformation.- The proposed BART-FG model for controllably generating fake contexts is novel. It uses constituency parsing and masking to iteratively modify key phrases in the original text. This allows for controlled generation of contradicting information. Other text generation models like GPT-2 don't have this fine-grained control.- The misinformation-aware QA framework jointly trains a QA model with a fake context detector. This is a new architecture aimed at improving robustness to misinformation. Prior work has not explored joint QA - misinformation detection models. - Experiments are comprehensive in evaluating major QA models on ContraQA and analyzing impact of different misinformation types. This provides new insights on where models struggle with contradictions.Overall, the core ideas around contradicting contexts, new dataset, controlled generation model, and joint QA-detection model offer unique contributions not explored by prior work. The paper carves out a novel problem space at the intersection of QA, misinformation, and robustness. More work is still needed to address these issues in open-domain QA.


## What future research directions do the authors suggest?

Based on my reading of the paper, some of the main future research directions suggested by the authors are:- Study the problem of QA robustness to misinformation under more realistic open-domain QA settings. The current work focuses on closed-domain QA using a modified version of the SQuAD dataset. The authors suggest extending the research to open-domain QA where retrieving relevant contexts introduces additional challenges.- Develop more effective countermeasures and defenses for building robust misinformation-aware QA systems. The authors show that integrating misinformation detection can help, but it requires large amounts of labeled in-domain data which may not be feasible. More advanced techniques need to be explored.- Explore other types of misinformation beyond contradictory information, such as hoaxes, rumors, false propaganda etc. The current work focuses specifically on contradicting contexts, but there are other forms of misinformation that need to be considered. - Evaluate the impact of different kinds of misinformation edits, beyond just entity substitutions. The authors mainly consider entity substitutions to create contradictions, but more complex edits could reveal further challenges.- Study the threat of neural-generated misinformation at scale. The authors argue misinformation can be easily mass-produced using neural models, exacerbating the risks. Defending against neural misinformation needs more attention.- Release datasets to promote research on robust QA. The authors plan to release their dataset to facilitate progress on this important problem. More datasets could spur further advances.In summary, the authors argue much more research is needed to handle misinformation in QA, suggesting directions like more complex settings, improved defense techniques, alternate misinformation types, analysis of different edits, contending with neural-generated misinformation, and releasing datasets.


## Summarize the paper in one paragraph.

Here is a one paragraph summary of the key points from the paper:The paper introduces ContraQA, the first dataset for studying question answering under contradicting contexts mixed with both real and fake information. The dataset contains over 10k human-written and model-generated contradicting context pairs built on top of SQuAD 1.1. The authors propose BART-FG, a novel framework to generate the fake contradicting contexts by iteratively masking and regenerating constituency spans from the original context. Experiments demonstrate that current QA models suffer significant performance drops on ContraQA, revealing their vulnerability to misinformation. To mitigate this threat, the authors build a robust QA system integrating question answering and misinformation detection, which improves performance. However, defending against misinformation remains challenging without sufficient in-domain training data. The authors plan to release ContraQA to spur more research into building robust QA systems that can handle real-world misinformation.


## Summarize the paper in two paragraphs.

Here is a two paragraph summary of the paper:The paper proposes a new dataset called ContraQA for evaluating question answering systems under contradicting contexts. The dataset is constructed from SQuAD 1.1 by creating fake contradicting versions of the original context paragraphs. Both human annotators and a novel generative model called BART-FG are used to produce the fake contexts. BART-FG iteratively masks and regenerates constituency spans in the original context to create fluent contradicting texts. Experiments show that state-of-the-art QA models suffer significant performance drops on ContraQA compared to clean SQuAD data. Both human-written and BART-FG generated contradicting contexts are challenging, but human edits are subtler and more deceiving. The authors propose integrating question answering with misinformation detection as a defense. A detector trained on sufficient labeled real/fake paragraphs boosts performance, but may be infeasible in practice.In summary, this paper constructs a new QA dataset containing contradicting contexts to evaluate model robustness to misinformation. Both human-written and model-generated misinformation significantly degrade QA performance. The proposed defense of combining QA with misinformation detection helps, but relies on large labeled data. Further research is needed to build robust QA systems that can handle real-world misinformation.
