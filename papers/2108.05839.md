# [Logit Attenuating Weight Normalization](https://arxiv.org/abs/2108.05839)

## What is the central research question or hypothesis that this paper addresses?

This paper proposes a new training method called Logit Attenuating Weight Normalization (LAWN) to improve the generalization performance of deep neural networks. The central hypothesis is that controlling the growth of network weights, especially in the final layers, can help prevent loss of adaptivity during training and allow the network to find better optima. Specifically, the authors argue that unconstrained training of overparameterized networks leads to loss flattening - the training loss becomes very small as the weights and logit outputs grow large. This makes it difficult for the network to escape suboptimal minima and continue exploring the loss landscape. LAWN constrains the norms of the weights, especially in the final homogeneous layers of the network. This is hypothesized to prevent loss flattening, improve the network's adaptivity, and enable finding better minima during training. Experiments across image classification, recommendation systems, and with various optimizers demonstrate improved generalization performance with LAWN compared to regular training.In summary, the central hypothesis is that controlling weight norms through LAWN can improve network adaptivity and generalization compared to unregularized training. The method is shown to be widely applicable across tasks, architectures, and optimizers.


## What is the main contribution of this paper?

This paper appears to present a new training method called Logit Attenuating Weight Normalization (LAWN) for improving the generalization performance of deep neural networks. The key ideas are:- Controlling the growth of logits (output scores) during training by constraining the weight norms of homogeneous layers in the network. This helps avoid "loss flattening", where the loss and gradients become very small, making training less adaptive. - Using projected gradients during the weight-constrained phase to remove the ineffective radial component and focus training on the lateral component that improves classification margins.- Switching from a free training phase to a constrained phase once weights have reached a sufficient norm to avoid early distortion of training.The main contributions seem to be:- Proposing the LAWN method and explaining intuitively why it should improve adaptivity and generalization. - Showing LAWN can be easily added to existing optimizers like SGD, Adam, LAMB.- Demonstrating experimentally that LAWN improves generalization over strong baselines across image classification, recommender systems, batch sizes. - Particularly impressive gains are shown for Adam on CIFAR and ImageNet, where LAWN helps it match/exceed SGD performance.- Showing LAWN enables graceful scaling to large batch sizes compared to optimizers without it.In summary, the main contribution appears to be the proposal and experimental validation of the simple but effective LAWN technique to boost adaptivity and generalization of existing optimizers. The method is intuitive, broadly applicable, and brings consistent gains across tasks.
