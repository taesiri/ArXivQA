# [Let Storytelling Tell Vivid Stories: An Expressive and Fluent Multimodal   Storyteller](https://arxiv.org/abs/2403.07301)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem: 
Existing storytelling models struggle to generate human-preferred narratives that are both expressive (integral, interesting, correlated) and visually consistent across the story. They tend to generate simplistic storylines or lack diversity in the plots. 

Solution - LLaMS Pipeline:
The paper proposes a new pipeline called LLaMS (Large Language model assisted Multimodal Storyteller) with two key components:

1. Expressive Story Generation and Prediction
- Introduces a sequence data enhancement strategy to automatically rewrite existing story data to be more detailed, integral and interesting using pre-trained language models. This enhanced data is then used to train a textual storytelling model based on LLaVa architecture.

- The model is trained on two tasks - story generation (images to text) and story prediction (partial images to continued story text/images). This allows generating factual stories from images as well as predicting reasonable future developments.

2. Consistent Vision Storytelling 
- Proposes SQ-Adapter module that captures common visual features from a sequence of images and guides the image generation process for consistent illustrations that match the text plots.

Main Contributions:
- Novel LLaMS pipeline to address limitations of existing storytelling models through automatic data enhancement and new model architecture.

- Sequence data enhancement strategy to automatically improve expressiveness of existing VIST dataset.

- Textual storytelling model trained on joint story generation and prediction tasks.

- SQ-Adapter module to maintain visual consistency across generated illustrations.

- Achieves state-of-the-art performance on VIST dataset through human evaluation. Demonstrates expressive, integral and consistent multimodal story generation.
