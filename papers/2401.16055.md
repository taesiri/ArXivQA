# [Stolen Subwords: Importance of Vocabularies for Machine Translation   Model Stealing](https://arxiv.org/abs/2401.16055)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Model stealing is an attack where an adversary tries to replicate a victim model's functionality using only its inputs and outputs. It is unclear how the choice of subword vocabulary like BPE affects the performance of stolen translation models.

- The paper examines two research questions:
   1) Is it important for the stolen model to match the victim model's BPE vocabulary? 
   2) Can the attacker recover the victim model's vocabulary given black-box or gray-box access?

Methods:
- Victim model: WMT19 winning English<=>German model. Student models tested.
- Datasets: 10M parallel En/De sentences from ParaCrawl, EuroPat patents, CommonCrawl.
- Access: Black-box (only translations) or gray-box (subword segmented outputs).

Results:
- Matching victim vocabulary gives only a small boost over using a domain BPE vocabulary. But better than unrelated domains or BPE on all data. 

- With gray-box access collecting output subwords recovers 98% of victim vocabulary. From black-box, training BPE on available data gets 57% overlap.

- With only a single seed sentence, an iterative vocabulary expansion algorithm can recover some of the vocabulary.

Conclusions:
- The BPE vocabulary itself is not crucial for model stealing, domain specificity is more important. 

- With subword-level outputs, the victim's vocabulary can be extracted with high accuracy. With only translations, best to train a domain BPE locally.

Main Contributions:
- Quantification of the effect of matching vs. mismatched BPE vocabularies for MT model stealing/distillation.

- Analysis of approaches to recover victim vocabulary under different levels of model access.

Limitations:
- Focused only on MT models and BPE algorithm. More tokenization-sensitive tasks could show higher dependence.


## Summarize the paper in one sentence.

 Here is a one-sentence summary of the paper:

This paper explores the importance of subword vocabularies in machine translation model stealing scenarios, finding that matching the victim's vocabulary provides little benefit over using a domain-specific vocabulary, but the victim's vocabulary can be extracted from model outputs if gray-box access is available.


## What is the main contribution of this paper?

 Based on my reading of the paper, the main contributions are:

1) Showing that the choice of BPE vocabulary has only a minor effect on the performance of a stolen machine translation model, as long as the vocabulary matches the domain of the stolen model or student model. Specifically, using the victim's vocabulary is marginally worse than using a domain-specific vocabulary.

2) Demonstrating several approaches for extracting the victim model's BPE vocabulary, under different assumptions about the level of access to the victim model (black-box or gray-box). The best approach with gray-box access can recover 98% of the victim's vocabulary.

3) More broadly, providing guidance for choices regarding subword vocabularies in black-box knowledge distillation scenarios. Showing that either using the teacher's vocabulary or a student-specific vocabulary trained on the appropriate domain are reasonable approaches.

In summary, the paper explores the importance of subword vocabularies in machine translation model stealing scenarios, finding the choice has a minor effect, while also showing methods for extracting the victim's vocabulary.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and concepts are:

- Model stealing - The paper focuses on "model functionality stealing", where an attacker tries to replicate a victim model locally based on its outputs.

- Machine translation - The specific task examined is machine translation, translating text from one language (e.g. English) to another (e.g. German).

- Byte-pair encoding (BPE) - A subword tokenization algorithm that is widely used in machine translation. A key question is whether matching the victim's BPE vocabulary matters for model stealing.

- Vocabulary inference - Recovering or extracting the victim model's BPE vocabulary based on its translations. This is explored under different levels of access to the victim model.

- Knowledge distillation - Model stealing is related to knowledge distillation, where a smaller student model tries to mimic a larger teacher model. But the goals are different (attacking vs efficiency).

- Black-box vs gray-box access - Black-box means only access to translations, gray-box means access to subword units in the translations.

So in summary - model stealing, machine translation, BPE vocabularies, vocabulary inference, knowledge distillation, black-box attacks.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 potential in-depth questions about the method proposed in this paper:

1. The paper explores both the importance of matching victim and student BPE vocabularies as well as approaches for extracting the victim's vocabulary. Which of these two aspects do you think is more impactful in practical model stealing scenarios and why?

2. When presenting the results of using different BPE vocabularies for the student model, the paper does not explore the effect of vocabulary size. What effect do you think using smaller or larger BPE vocabularies would have on student model performance?

3. The paper finds that using a BPE vocabulary tailored to the student model's training data performs nearly as well as matching the victim vocabulary. Do you think this conclusion would still hold for more morphologically complex languages or other tokenization algorithms besides BPE? 

4. When extracting the victim's vocabulary via gray-box access, the paper shows better results when translating authentic sentences compared to unique words. Why do you think this is the case? What are some potential ways to address this?

5. The cyclic translation algorithm for extracting victim vocabulary from a single seed sentence did not perform as well as other approaches. How could this algorithm be improved to increase vocabulary overlap with the victim model? 

6. Beyond machine translation, what other natural language processing tasks do you think would show a high dependency on matching victim and student vocabularies for model stealing? Why?

7. The paper hypothesizes that model stealing dependence on victim vocabulary is related to the tokenization-sensitivity of the task. What experiments could be run to definitively test this hypothesis?

8. What are some potential negative societal impacts that could arise from the ability to effectively steal machine translation models and vocabularies demonstrated in this paper?

9. The victim model studied uses BPE tokenization. How do you think results would differ for models using other subword tokenization methods like WordPiece or Unigram?

10. What additional information would be needed to determine real-world attack feasibility of the vocabulary extraction methods presented? How costly do you estimate it would be to apply these attacks?
