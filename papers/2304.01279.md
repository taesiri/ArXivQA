# [Long-Tailed Visual Recognition via Self-Heterogeneous Integration with   Knowledge Excavation](https://arxiv.org/abs/2304.01279)

## What is the central research question or hypothesis that this paper addresses?

Based on my understanding, the central research question this paper addresses is: How to improve the performance of MoE-based methods for long-tailed visual recognition by better utilizing the intermediate features from different depths of the network?The key hypotheses are:1) Different parts of a deep neural network (shallow vs deep) may have different preferences in fitting the data from a long-tailed distribution. The shallow parts may be better at recognizing some tail classes.2) By fusing intermediate features from different depths with the high-level features in each expert branch of a MoE model, the experts can become more diverse and skilled at different parts of the distribution. 3) Knowledge distillation among diversified experts can help reduce the influence of hardest negatives, especially for tail classes, and improve overall performance.To validate these hypotheses, the authors propose two main components:- Depth-wise Knowledge Fusion (DKF): Fuses intermediate features from different depths with the high-level features of each expert to encourage feature diversity.- Dynamic Knowledge Transfer (DKT): Transfers knowledge among diversified experts by suppressing the hardest negative predictions to reduce their influence, especially on tail classes.The overall proposed method is called Self-Heterogeneous Integration with Knowledge Excavation (SHIKE). Experiments on several benchmarks demonstrate the effectiveness of SHIKE compared to other state-of-the-art methods.
