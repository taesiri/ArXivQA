# [SpanSeq: Similarity-based sequence data splitting method for improved   development and assessment of deep learning projects](https://arxiv.org/abs/2402.14482)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Deep learning models can memorize and overfit on the training data, leading to overestimated performance on the test set. This issue is exacerbated when using biological sequence data like genes, proteins or genomes due to evolutionary relationships causing redundancy and similarity between sequences. 
- Standard practice is to randomly split data into train/validation/test sets. But this causes data leakage and similarity between sets, enabling memorization and obstructing proper assessment of generalization performance.

Proposed Solution:
- The authors develop SpanSeq, an alignment-free method to split sequence data into partitions while avoiding similarity between partitions. It uses Mash or KMA for fast k-mer comparisons to measure sequence similarity, then clusters similar sequences and distributes the clusters across partitions.

Key Contributions:  
- SpanSeq scales to cluster and partition large biological sequence datasets in a reasonable time. It is not limited by sequence lengths like alignment methods.
- Case study on DeepLoc model shows SpanSeq partitioning causes no data leakage between splits. This avoids memorization, provides realistic validation set performance for hyperparameter tuning, and enables proper assessment of generalization ability.  
- For the DeepLoc case, SpanSeq partitioning also substantially reduces training time required compared to random splitting.

In summary, SpanSeq enables properly constructing train/validation/test splits for biological sequence data to avoid overestimating deep learning model performance due to memorization from data leakage between splits. It is reasonably fast, scales to large datasets, and is alignment-free so sequence length is not limiting.
