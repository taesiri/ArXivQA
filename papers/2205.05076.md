# Reduce Information Loss in Transformers for Pluralistic Image Inpainting

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper addresses is how to reduce information loss in transformers for pluralistic image inpainting. Specifically, the paper identifies two main sources of information loss in existing transformer-based image inpainting methods:1. Downsampling the input image to lower resolutions, which causes loss of fine details and misalignment of region boundaries when upsampling back to the original resolution. 2. Quantizing the RGB pixel values into a small number of discrete tokens before feeding into the transformer. This causes loss of information in the pixel values.To address these issues, the paper proposes two key solutions:1. A patch-based autoencoder called P-VQVAE that encodes the image into non-overlapping patches at the original resolution, avoiding downsampling. It uses a dual codebook to separately represent masked and unmasked patches. 2. An unquantized transformer (UQ-Transformer) that takes the continuous encoder features as input without quantization and only uses quantized tokens as prediction targets. This avoids information loss during transformer processing.The overall framework combines P-VQVAE and UQ-Transformer to maximize information retention for the transformer, in order to achieve better fidelity and diversity for pluralistic image inpainting, especially on complex images and large masked regions.


## What is the main contribution of this paper?

The main contributions of this paper are:1. It proposes a new transformer-based framework called PUT for pluralistic image inpainting. PUT contains two key components:- P-VQVAE: A patch-based autoencoder that converts the input image into non-overlapped patch tokens using an encoder, quantizes the patch tokens with a dual-codebook, and reconstructs the image from the quantized tokens using a decoder. This avoids input image downsampling and disturbance between masked and unmasked regions.- UQ-Transformer: A transformer that takes the unquantized features from P-VQVAE encoder as input and predicts quantized tokens as targets. This avoids the information loss caused by feature quantization.2. Compared to existing methods, PUT reduces the information loss from the input image in two aspects:- By using a patch-based encoder, it avoids input downsampling and thus preserving full resolution information.- By feeding unquantized features to the transformer, it avoids quantization loss.3. Extensive experiments show PUT achieves state-of-the-art performance on image fidelity and diversity, especially for large masks and complex datasets. It outperforms previous transformer-based methods by a large margin.4. Ablation studies validate the effectiveness of the proposed P-VQVAE and UQ-Transformer components. The unquantized transformer is shown to be critical for preserving information and achieving high inpainting quality.In summary, the main contribution is proposing the PUT framework that reduces information loss compared to prior arts by avoiding input downsampling and feature quantization, leading to improved performance on pluralistic image inpainting.
