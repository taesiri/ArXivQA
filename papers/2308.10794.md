# [MGMAE: Motion Guided Masking for Video Masked Autoencoding](https://arxiv.org/abs/2308.10794)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is: How can we improve the performance of video masked autoencoding by incorporating motion information into the masking strategy?The key hypothesis is that using motion information to guide the masking process will help create more challenging and meaningful pretraining tasks, leading to better learned video representations. Specifically, the authors propose using optical flow to track masking maps across frames and generate temporally consistent masking volumes. This allows sampling a consistent set of visible tokens over time rather than masking randomly or using a "tube masking" approach. By constructing consistent masking cubes that track objects over time, the authors hypothesize this motion guided masking approach (MGMAE) will:- Further reduce the risk of information leakage between frames, since objects cannot be unmasked in some frames and masked in others.- Encourage learning more useful spatiotemporal structure information by enforcing reconstruction from limited consistent tracks of visible cubes. - Adapt better to motion variations compared to tube masking approaches that assume small/no motion between frames.In summary, the central hypothesis is that explicit motion guided masking will create a more challenging pretraining task and result in improved video representations compared to prior work like VideoMAE. The experiments aim to validate if MGMAE does improve action recognition accuracy after pretraining.


## What is the main contribution of this paper?

This paper proposes a motion guided masking strategy for video masked autoencoder pre-training, called MGMAE. The key contributions are:- It introduces a motion guided masking approach to generate temporally consistent masking volumes for video clips. Specifically, it uses optical flow estimated by RAFT to warp the initial random mask to adjacent frames. This motion guided masking can better suppress information leakage in time during pre-training. - Based on the temporally consistent masking volumes, it samples a set of cubes to keep visible across the entire clip. This allows building a more challenging reconstruction task by enforcing temporal consistency on the visible cubes. - It implements the motion guided masking strategy on top of the VideoMAE framework. Experiments on Something-Something V2 and Kinetics datasets demonstrate MGMAE consistently outperforms VideoMAE with tube masking.- It provides detailed ablation studies on the design choices of motion guided masking, such as base frame selection, warping methods, initialization, etc. This verifies the effectiveness of each component in the proposed masking strategy.- It also visualizes the generated motion guided masking volumes and reconstructed videos to showcase that MGMAE can effectively track objects and sample coherent cubes for pre-training.In summary, the main contribution is proposing the motion guided masking approach for video MAE to build a more challenging pre-training task by incorporating temporal consistency. This simple yet effective strategy further pushes the limit of video masked autoencoder.
