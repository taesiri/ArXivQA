# [Towards Continual Learning Desiderata via HSIC-Bottleneck   Orthogonalization and Equiangular Embedding](https://arxiv.org/abs/2401.09067)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Deep neural networks suffer from catastrophic forgetting when trained sequentially on multiple tasks. Many continual learning (CL) methods rely on storing past data (rehearsal buffers) or expanding model size, compromising privacy and efficiency.

- The paper considers a strict but realistic CL setting with no access to past data, no substantial network expansion, while still balancing stability and plasticity. Achieving all these desiderata simultaneously is challenging.

Method: 
- Proposes CLDNet which synergizes two components - HSIC-Bottleneck Orthogonalization (HBO) and EquiAngular Embedding (EAE).

- HBO transforms CL into a statistical dependency problem, updating parameters using an orthogonal projector constructed by Hilbert-Schmidt Independence Criterion (HSIC). This enables non-overwritten updates, mitigating feature bias and layer-wise overwriting.  

- EAE replaces the trainable classifier with predefined equiangular basis vectors that optimize distance metric on hypersphere. This enhances decision boundary adaptation between old and new classes.

Contributions:
- CLDNet reaches multiple CL desiderata - no rehearsal buffers, no substantial network expansion, balances stability and plasticity.

- Extensive experiments show CLDNet outperforms prior state-of-the-art methods under the realistic setting. On CIFAR-100, it improves 7.54% over best baseline with 0 buffers and 1.02x parameters.

- First orthogonal projector in CL to use HSIC. First to bring parameter-free classifier to CL which enhances model decision ability.

In summary, CLDNet sets a strong baseline forStrict yet practical CL setting through the synergy of statistical dependency and distance metric based training objectives.


## Summarize the paper in one sentence.

 Here is a one-sentence summary of the paper:

The paper proposes a continual learning method called CLDNet that reaches multiple desiderata such as no reliance on rehearsal buffers, minimal network expansion, and a good balance between stability and plasticity, through the synergy of two components - HSIC-Bottleneck Orthogonalization (HBO) for non-overwritten parameter updates and EquiAngular Embedding (EAE) for decision boundary adaptation.


## What is the main contribution of this paper?

 The main contribution of this paper is proposing a new continual learning method called CLDNet that reaches multiple desiderata:

1) It does not require storing or accessing previous task data (rehearsal-free).

2) It keeps the model size relatively constant without substantial network expansion.  

3) It strikes a good balance between stability (retaining knowledge from previous tasks) and plasticity (ability to learn new tasks) through the synergy of two components:

- HSIC-Bottleneck Orthogonalization (HBO) which implements non-overwritten parameter updates to address catastrophic forgetting.

- EquiAngular Embedding (EAE) which enhances decision boundary adaptation between old and new tasks using predefined basis vectors.

The key insight is to transform the continual learning process into statistical dependency and distance metric optimization problems that mitigate parameter overwriting and confusion in decision boundaries. Experiments show CLDNet achieves strong performance across benchmarks while satisfying the desiderata.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and concepts include:

- Continual learning (CL): Learning sequentially from a stream of data distributions without forgetting previously learned knowledge. The paper focuses on addressing catastrophic forgetting in CL.

- Desiderata: Desired properties or criteria. The paper considers multiple CL desiderata involving no access to previous task data, unchanged model size, and stability-plasticity balance.  

- HSIC-Bottleneck Orthogonalization (HBO): A component of the proposed CLDNet method that implements non-overwritten parameter updates using Hilbert-Schmidt independence criterion and orthogonal projections.

- EquiAngular Embedding (EAE): Another component of CLDNet that enhances decision boundary adaptation between old and new tasks using predefined equiangular basis vectors.  

- Rehearsal-free: Not requiring storing or replaying data from previous tasks.

- Minimal expansion: Keeping the model size relatively constant without expanding with new parameters per task.  

- Stability-plasticity dilemma: The tradeoff between retaining old knowledge (stability) and learning new knowledge (plasticity).

So in summary, key terms include continual learning, desiderata, catastrophic forgetting, HBO, EAE, rehearsal-free, minimal expansion, and stability-plasticity.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in this paper:

1. What is the motivation behind proposing the Hilbert-Schmidt independence criterion (HSIC) bottleneck orthogonalization (HBO) component? How does it help mitigate catastrophic forgetting?

2. Explain in detail the formulation of the HBO optimization objective. What is the intuition behind minimizing and maximizing the HSIC terms? 

3. The paper states that HBO addresses the layer-wise parameter overwriting issue. Elaborate on how optimizing the HSIC terms helps mitigate feature bias and construct a more accurate orthogonal projector.

4. What is the purpose of designing the EquiAngular Embedding (EAE) component? How does using predefined basis vectors instead of a trainable classifier help in continual learning?

5. Discuss the process of generating the equiangular basis vector matrix W* in detail. What constraints need to be satisfied? How is the optimization function tweaked?

6. Explain how the overall CLDNet training objective based on HBO and EAE is formulated. How do the two components synergize in the proposed approach?  

7. Elaborate on how CLDNet satisfies the three continual learning desiderata mentioned, especially in terms of rehearsal-free learning and minimal network expansion.

8. How does the rank of orthogonal projectors change on using HSIC terms compared to existing methods? What does this indicate about model plasticity?

9. Analyze the t-SNE visualization provided in Figure 4. How does HBO help in discriminative feature learning across incremental sessions? 

10. Critically analyze the limitations of the proposed CLDNet approach. What are possible future research directions that can build upon this work?
