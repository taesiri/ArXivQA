# [How Many Validation Labels Do You Need? Exploring the Design Space of   Label-Efficient Model Ranking](https://arxiv.org/abs/2312.01619)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a detailed summary of the key points from the paper:

This paper introduces LEMR, a novel framework that reduces the need for costly manual annotations and enables accurate model selection under low-resource settings. It leverages an ensemble of models to generate pseudo-labels on an unlabeled validation set, selects instances for labeling based on uncertainty sampling strategies, dynamically adjusts the model ensemble based on updated labels, and ultimately ranks the models. To evaluate LEMR, the authors propose MoraBench benchmark, comprising diverse model outputs from 23 weak supervision, semi-supervised learning, and prompt selection tasks across various NLP datasets. 

The experiments demonstrate LEMR's ability to significantly lower labeling costs for model selection without compromising accuracy. With a suitable combination of methods from the proposed design space, LEMR reduces the required labeling to even below 10% in some tasks compared to using the full labeled validation set. Key insights show that soft ensemble works better when model performance is poorer and hard ensemble is superior otherwise; uncertainty sampling consistently outperforms random acquisition; and selecting high-quality model committees via the Z-score method improves results.

Overall, LEMR offers a highly cost-effective, accurate approach to model selection, especially valuable under resource constraints. The methodological considerations and insights based on the extensive analysis using MoraBench provide valuable guidance for research in this domain.
