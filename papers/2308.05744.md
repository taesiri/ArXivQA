# [PlankAssembly: Robust 3D Reconstruction from Three Orthographic Views   with Learnt Shape Programs](https://arxiv.org/abs/2308.05744)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be: How can we develop a robust 3D reconstruction method from orthographic views that is tolerant to imperfections in the input drawings?The key ideas and contributions to address this question appear to be:- Using a deep generative model based on Transformer architecture rather than traditional reconstruction pipelines. This allows more flexible mapping between input and output.- Designing the model to output shape programs that assemble planks, incorporating domain knowledge of furniture modeling.- Creating a large-scale dataset of cabinet models to train and evaluate the method.- Demonstrating significantly improved robustness to input noise and errors compared to traditional reconstruction methods.In summary, the paper develops a learning-based approach to 3D CAD reconstruction from orthographic views that is much more robust to imperfect inputs than prior geometry-based techniques. The use of deep generative models and domain-specific shape programs are key to achieving this robustness.


## What is the main contribution of this paper?

The main contributions of this paper are:- They propose a new method for reconstructing 3D CAD models from three orthographic views using deep generative models. Traditional methods establish explicit correspondences between 2D and 3D entities, making them sensitive to errors/noise in the inputs. In contrast, the proposed method uses attention mechanisms to learn more flexible mappings between the inputs and outputs.- They design a shape program representation suitable for generating cabinet models. The shape program resembles how a human would construct the model in CAD software. Using this representation improves reconstruction accuracy and enables downstream editing operations. - They create a large-scale dataset of over 26,000 cabinet models and systematically evaluate the proposed method against traditional methods. Experiments show their method is much more robust to imperfect inputs like missing/noisy lines.In summary, the key novelty is the use of deep generative models for 3D CAD reconstruction from orthographic views, which is more robust than prior geometry-based methods. The shape program representation is also a useful way to incorporate domain knowledge.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

The paper proposes a new method to automatically convert 2D line drawings of objects from three orthographic views into 3D CAD models. The key idea is to use a Transformer-based sequence model to generate shape programs that assemble simple geometric primitives like planks into complex objects like cabinets. This learned approach is more robust to imperfections in the input drawings compared to traditional reconstruction methods.


## How does this paper compare to other research in the same field?

This paper presents a novel deep learning approach to reconstructing 3D CAD models from three orthographic views. It makes several key contributions compared to prior work:- It is the first to use a deep generative model for this task, specifically a Transformer-based sequence-to-sequence model. Previous methods relied on traditional computational geometry pipelines to explicitly establish correspondences between 2D and 3D. The generative modeling approach allows for more flexible mapping and greater robustness to imperfect inputs.- It incorporates domain knowledge by designing a simple domain-specific language to represent the 3D models, tailored to the furniture domain. This shape program output boosts accuracy and supports downstream CAD applications like editing. Prior learned methods output generic representations like meshes or point clouds. - It introduces a large-scale dataset of over 26,000 cabinet models, most created by professional designers. Previous datasets were smaller and synthetic. The scale and realism enable thorough benchmarking.- Experiments demonstrate substantially higher robustness to input noise and errors compared to both traditional methods and other recent learned approaches. The method also runs efficiently on GPUs.Overall, this work makes significant advances in learning-based 3D reconstruction from orthographic views. The novel deep generative modeling approach, combined with domain-specific shape programs, enables reconstruction that is far more robust and practical for real-world engineering. The ideas could generalize well to other CAD model types beyond furniture.


## What future research directions do the authors suggest?

Based on my review of the paper, some of the key future research directions suggested by the authors are:- Extending the approach to more complex CAD models beyond cabinet furniture, such as mechanical parts. The authors suggest that the main idea of using a learned generative model is generalizable, but extending it to domains without large-scale training data remains an open challenge.- Incorporating additional information available in CAD drawings, such as layers, text, symbols and annotations. The authors note that their current approach does not leverage these and integrating them could help reconstruct more complex drawings.- Going beyond reconstructing 3D geometry to also inferring relationships like kinematic joints. The current work focuses only on modeling geometry but many mechanical CAD models also contain non-geometric information that could be useful to capture.- Exploring different neural network architectures beyond Transformers. The authors demonstrate the benefits of using attention for this task but other network architectures may have complementary strengths.- Improving robustness to imperfect inputs and reducing failure cases. The authors show their method is more robust than prior approaches, but handling noise and incomplete data remains an open problem.- Enabling intuitive user interaction for error correction and editing. Leveraging the learned generative model for user interactions like correcting mistakes or editing models is suggested.In summary, the main future directions pointed out are: extending the approach to new domains, incorporating more CAD information, inferring non-geometric relationships, exploring alternative network architectures, improving robustness, and enabling user interactions. Advancing any of these aspects could build nicely on the authors' proposed generative modeling approach.
