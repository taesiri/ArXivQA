# [Revisiting the Power of Prompt for Visual Tuning](https://arxiv.org/abs/2402.02382)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
The paper focuses on visual prompt tuning (VPT), which is a promising method for adapting pre-trained vision models to downstream tasks by inserting learnable prompt tokens. However, VPT faces several challenges:
1) Difficulty with prompt initialization - random initialization performs poorly.
2) Sensitivity to prompt length hyperparameter.  
3) Subpar performance when using self-supervised pre-trained models as the backbone.

These challenges hinder the successful application of VPT for contextual adaptation.

Key Idea: 
The authors first explore how the distributional relationship between prompts and image patch tokens evolves during VPT training. They find that the prompts converge towards the distribution of the patch tokens, i.e. their mutual information increases.  

Based on this, the paper hypothesizes that initializing prompts to already have high mutual information with patch tokens can improve tuning. The proposed method uses the downstream task's inferred patch token prototypes to initialize prompts.

Proposed Method:
The authors propose Self-Prompt Tuning (SPT), which leverages downstream inferred token prototypes to initialize prompts.

Specifically, patch tokens are first inferred by passing the downstream task images through the pretrained model. Then prompt tokens are initialized by either:
1) Clustering inferred tokens into prototypes (computationally expensive)  
2) Efficient approximations: mean/max pooling or random sampling of inferred tokens.

The initialized prompts are inserted into the model and fine-tuned on the downstream task while keeping the backbone model frozen.

Contributions:
- SPT effectively addresses the challenges of prompt initialization, robustness to length, and adaption for self-supervised models.
- Experiments show SPT improves VPT by 10-30% in accuracy, achieves highly competitive results to full fine-tuning, and advances self-supervised model adaptation.
- Analysis provides insights - SPT shows better robustness to prompt lengths, scales better with model capacity, and is most beneficial when downstream data is small.

In summary, the paper makes significant contributions around improving visual prompt tuning through a simple but effective prompt initialization method based on mutual information between prompts and image embeddings.


## Summarize the paper in one sentence.

 Here is a one-sentence summary of the paper:

The paper proposes a novel visual prompt tuning method called Self-Prompt Tuning (SPT) that initializes prompts with downstream token prototypes inspired by the observation that prompt tokens tend to share high mutual information with patch tokens during proficient training, achieving significant performance gains especially under self-supervised pretraining.


## What is the main contribution of this paper?

 According to the paper, the main contributions are:

1. Proposing a simple yet effective approach called Self-Prompt Tuning (SPT) that initializes prompt tokens with downstream token prototypes inspired by the observation that prompts tend to share high mutual information with patch tokens during proficient training.

2. Showcasing for the first time that a prompt tuning method can achieve highly accurate results on par with full fine-tuning, especially under self-supervised pretraining backbones like MAE and MoCo-v3. SPT improves average accuracy over VPT by 10-30% and outperforms full fine-tuning in 19 out of 24 cases while using less than 0.4% of parameters.

3. Demonstrating that SPT is robust to hyperparameters like prompt length and scales well to larger models and more training data. It also significantly advances adaptation performance for self-supervised pretraining over previous prompt tuning methods.

In summary, the main contribution is proposing an effective prompt initialization strategy called Self-Prompt Tuning that leads to significant improvements in accuracy and adaptability over prior prompt tuning approaches, especially for self-supervised pretraining.


## What are the keywords or key terms associated with this paper?

 Based on my review of the paper, some of the key terms and concepts related to this work include:

- Visual prompt tuning (VPT) - Inserting learnable prompt tokens into vision transformers to customize them for downstream tasks while keeping the backbone model frozen.

- Self-supervised pretraining - Pretraining vision models like MAE and MoCo on large unlabeled image datasets.

- Parameter-efficient fine-tuning - Methods like VPT that can adapt pretrained models to new tasks without changing many parameters.

- Normalized mutual information (NMI) - A metric used in the paper to quantify the relationship between prompt tokens and image patch tokens.

- Token prototypes - Cluster centers of image patch tokens that are used to initialize prompt tokens in the proposed SPT method. 

- SPT (Self-Prompt Tuning) - The proposed approach to initialize prompt tokens using token prototypes from the downstream task for more effective tuning.

- Shallow vs deep prompt tuning - Inserting prompts in only the first or all layers of a vision transformer.

So in summary, the key ideas involve analyzing VPT, proposing a better prompt initialization strategy called SPT using downstream token prototypes, evaluating on self-supervised and supervised models, and showing gains over prior VPT work.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in this paper:

1. The paper proposes initializing prompt tokens with downstream token prototypes constructed by clustering image embeddings. What is the intuition behind this initialization strategy? How does it help with the training convergence and performance compared to random initialization?

2. The paper computes normalized mutual information (NMI) between prompts and image embeddings to show that they converge in distribution during training. What exactly does NMI quantify in this context? And how does higher NMI at initialization impact downstream tuning?

3. The paper proposes several practical prompt token sampling strategies like mean pooling, max pooling, and random sampling. How do these methods work? What are the trade-offs between them in terms of performance vs computational efficiency? 

4. What are the differences between SPT-Shallow and SPT-Deep variants proposed in the paper? What influences the choice between them for a given downstream task? How sensitive is performance to prompt length?

5. The paper demonstrates SPT works much better than VPT for self-supervised models like MAE. What intrinsic properties of self-supervised models make VPT underperform? And how does SPT alleviate some of those issues?

6. How suitable is SPT for low-data regimes compared to full fine-tuning? What amount of downstream data makes full tuning outweigh SPT? What insights does Figure 5 provide regarding this trade-off?

7. The paper shows SPT scales better with increased model capacity compared to VPT. What factors contribute to the degraded scaling behavior of VPT? And how does appropriate prompt initialization in SPT help resolve this?

8. What role does the similarity of distributions between prompts and image embeddings play in the transferability of SPT across diverse datasets? How can we quantify this effect?

9. Could conditional computing approaches like expert models help provide better prompts in a data-driven way instead of predefined clustering? What challenges need to be resolved to enable such dynamic prompt generation?

10. What other prompting approaches like continuous prompting or prompting in hidden spaces could potentially allow better tuning without destroying pretrained representations? How can we design prompts to maximize information preservation?
