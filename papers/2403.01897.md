# [Fostering the Ecosystem of Open Neural Encoders for Portuguese with   Albertina PT* Family](https://arxiv.org/abs/2403.01897)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- There is a lack of large, open-source language models specifically tailored for the Portuguese language. Existing models like BERTimbau (335M parameters) and Albertina (900M parameters) represent good first steps, but the ecosystem needs to be further expanded and improved. 

- There is also a shortage of good benchmark datasets for evaluating Portuguese language models.

Proposed Solution:
- The authors contribute two new encoder models to expand the ecosystem of open Portuguese language models:
  - Albertina 100M: A smaller, efficiency-focused model with 100 million parameters.
  - Albertina 1.5B: A larger, high-performance model with 1.5 billion parameters.
  
- Both models are based on the DeBERTa architecture and were pre-trained on Portuguese web crawl data from OSCAR and CulturaX datasets.

- To address the dataset shortage, the authors also constructed new Portuguese test sets by machine translating the GLUE and SuperGLUE benchmarks into European and Brazilian Portuguese.

Main Contributions:

- Albertina 100M and Albertina 1.5B - two new open-source encoder models for Portuguese that push the state-of-the-art for this language.

- New translated test sets for Portuguese based on GLUE and SuperGLUE benchmarks.

- Evaluation showing strong performance from the new models across a range of NLP tasks, with Albertina 1.5B achieving new state-of-the-art results.

- The models and datasets are openly released to continue expanding the ecosystem for Portuguese language AI.
