# [The Need for Speed: Pruning Transformers with One Recipe](https://arxiv.org/abs/2403.17921)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
Modern transformer architectures like BERT and ViT have enabled significant progress in domains like NLP and computer vision. However, they also have very high computational costs, posing challenges for adoption, especially on edge devices. Many prior works on compressing transformers have limitations like dependence on architecture specifics, expensive retraining procedures, or being focused only on a narrow set of tasks. There is a need for a general compression framework that works well across transformer architectures and application domains without requiring retraining.

Proposed Solution: 
The paper proposes the OPTIN (One-shot Pruning Technique for Interchangeable Networks) framework for efficient compression of pre-trained transformers in a one-shot manner, without requiring retraining. OPTIN introduces a "trajectory" metric that measures the effect of pruning a weight on deeper layer embeddings and logits, capturing long-range dependencies to determine weight importance. The most unimportant weights are pruned to meet a FLOPs constraint. Intermediate feature distillation via manifold and KL divergence losses is used to compute trajectory. For vision models, token reduction is also incorporated.

Main Contributions:
- Proposes OPTIN, the first one-shot pruning method that generalizes across NLP and vision transformers without retraining
- Introduces the trajectory metric to capture long-term weight importance via distillation 
- Achieves state-of-the-art results on NLP benchmarks and image classification tasks with BERT and ViT models
- Shows strong performance even after aggressive pruning, e.g. retains 79.01% ImageNet accuracy for DeiT-Small at 23.7% FLOPs
- Demonstrates versatility across tasks like segmentation and CNNs, and with transfer learning
- Provides good speedups in inference throughput along with compression

The main impact is enabling efficient deployment of modern transformers without expensive retraining, with versatility across architectures, domains, and applications. This facilitates adoption in resource-constrained environments.
