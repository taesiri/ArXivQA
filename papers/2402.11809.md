# [Generation Meets Verification: Accelerating Large Language Model   Inference with Smart Parallel Auto-Correct Decoding](https://arxiv.org/abs/2402.11809)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Large language models (LLMs) like ChatGPT are autoregressive, generating tokens sequentially during inference which limits parallelism and slows down inference speed. This is problematic especially for very large models with billions of parameters.

- Methods exist to accelerate inference like speculative decoding, but rely on auxiliary models which have overhead. Semi-autoregressive models can parallelize decoding but usually see degradation in output quality and are still hard to train.

Proposed Solution:
- Propose Smart Parallel Auto-Correct Decoding (SPACE) to accelerate inference of autoregressive LLMs in a lossless way, without needing extra models.

- Uses semi-autoregressive supervised fine-tuning (SAR-SFT) to give LLMs ability to predict multiple future tokens at once. This retains quality while making training easy by just modifying the dataloader. 

- An auto-correct decoding algorithm lets the LLM concurrently generate and verify candidate tokens in a single model invocation. Integrates draft-then-verify speculative decoding with SAR in an efficient way.

Main Contributions:
- SAR-SFT scheme to empower LLMs to generate multiple tokens in parallel while retaining model quality
- Auto-correct decoding algorithm allowing concurrent generation and verification of tokens within a single LLM forward pass
- Experiments on LLMs of varying sizes validate SPACE accelerates inference 2.7-4x on HumanEval while maintaining output quality
- Simple to implement and eliminates need for extra models like previous speculative decoding methods

In summary, SPACE introduces an innovative way to accelerate inference for autoregressive LLMs by adapting them to predict and verify multiple tokens concurrently, while maintaining output quality. The method is model-agnostic and demonstrates substantial speedups across different models and dataset tasks.
