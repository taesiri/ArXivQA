# [From Knowledge Distillation to Self-Knowledge Distillation: A Unified   Approach with Normalized Loss and Customized Soft Labels](https://arxiv.org/abs/2303.13005)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading, the central research questions/hypotheses that this paper addresses are:

1) How can knowledge distillation (KD) be improved to better utilize the soft labels from the teacher model for guiding the student model? 

2) How can effective soft labels be generated for self-knowledge distillation when a teacher model is not available?

Specifically:

- For KD, the paper hypothesizes that normalizing the non-target logits in the distillation loss will allow better alignment between the teacher and student distributions, improving knowledge transfer. They propose Normalized KD (NKD) to test this.

- For self-KD, the paper aims to develop a general and efficient method to generate customized soft labels incorporating both target and non-target class information without requiring a teacher model. They propose Universal Self-Knowledge Distillation (USKD) to achieve this.

So in summary, the central research questions are around improving KD through normalized loss and generating effective customized soft labels for self-KD without a teacher model. The NKD and USKD methods are proposed to address these questions.


## What is the main contribution of this paper?

 This paper proposes two main contributions:

1. A new knowledge distillation method called Normalized Knowledge Distillation (NKD). NKD improves knowledge distillation by normalizing the non-target logits in the distillation loss. This allows better alignment between the teacher's and student's distributions. 

2. A self-knowledge distillation method called Universal Self-Knowledge Distillation (USKD). USKD generates custom soft labels for self-distillation without needing a teacher model. It creates soft target labels by smoothing the student's predictions and soft non-target labels based on the training data.

The key ideas are:

- NKD normalizes the non-target logits so their sum matches between teacher and student. This better transfers the teacher's knowledge.

- USKD generates custom soft labels for self-distillation. The soft target labels are smoothed student predictions. The soft non-target labels use the training data and Zipf's law.

- Both NKD and USKD achieve state-of-the-art results on CIFAR and ImageNet datasets. NKD boosts student accuracy in knowledge distillation. USKD provides effective self-distillation for CNNs and vision transformers.

In summary, the main contribution is improving knowledge distillation with normalized logits (NKD) and proposing an effective self-distillation method (USKD) that works for diverse model architectures. The techniques achieve excellent results on benchmark datasets.
