# [From Knowledge Distillation to Self-Knowledge Distillation: A Unified   Approach with Normalized Loss and Customized Soft Labels](https://arxiv.org/abs/2303.13005)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading, the central research questions/hypotheses that this paper addresses are:

1) How can knowledge distillation (KD) be improved to better utilize the soft labels from the teacher model for guiding the student model? 

2) How can effective soft labels be generated for self-knowledge distillation when a teacher model is not available?

Specifically:

- For KD, the paper hypothesizes that normalizing the non-target logits in the distillation loss will allow better alignment between the teacher and student distributions, improving knowledge transfer. They propose Normalized KD (NKD) to test this.

- For self-KD, the paper aims to develop a general and efficient method to generate customized soft labels incorporating both target and non-target class information without requiring a teacher model. They propose Universal Self-Knowledge Distillation (USKD) to achieve this.

So in summary, the central research questions are around improving KD through normalized loss and generating effective customized soft labels for self-KD without a teacher model. The NKD and USKD methods are proposed to address these questions.


## What is the main contribution of this paper?

 This paper proposes two main contributions:

1. A new knowledge distillation method called Normalized Knowledge Distillation (NKD). NKD improves knowledge distillation by normalizing the non-target logits in the distillation loss. This allows better alignment between the teacher's and student's distributions. 

2. A self-knowledge distillation method called Universal Self-Knowledge Distillation (USKD). USKD generates custom soft labels for self-distillation without needing a teacher model. It creates soft target labels by smoothing the student's predictions and soft non-target labels based on the training data.

The key ideas are:

- NKD normalizes the non-target logits so their sum matches between teacher and student. This better transfers the teacher's knowledge.

- USKD generates custom soft labels for self-distillation. The soft target labels are smoothed student predictions. The soft non-target labels use the training data and Zipf's law.

- Both NKD and USKD achieve state-of-the-art results on CIFAR and ImageNet datasets. NKD boosts student accuracy in knowledge distillation. USKD provides effective self-distillation for CNNs and vision transformers.

In summary, the main contribution is improving knowledge distillation with normalized logits (NKD) and proposing an effective self-distillation method (USKD) that works for diverse model architectures. The techniques achieve excellent results on benchmark datasets.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence TL;DR summary of the paper:

The paper proposes methods to improve knowledge distillation performance by normalizing the non-target logits for better alignment and generating customized soft labels for self-distillation without needing an explicit teacher model.


## How does this paper compare to other research in the same field?

 This paper presents two main contributions to the field of knowledge distillation:

1. Normalized Knowledge Distillation (NKD): This improves upon classical knowledge distillation by normalizing the non-target logits before calculating the distillation loss. This allows the student model to better match the teacher's logit distribution. Experiments show this provides consistent improvements over standard KD and other advanced KD techniques like DKD.

2. Universal Self-Knowledge Distillation (USKD): This proposes a method to perform self-supervised knowledge distillation without a teacher model. It generates custom soft target and non-target labels for each image using the student's own predictions. This allows self-distillation to work well for both CNNs and vision transformers. 

Some key differences compared to prior work:

- Compared to standard KD, NKD better transfers the teacher's knowledge by normalizing the logit distributions. This simple change consistently outperforms standard KD.

- Compared to other advanced KD methods like DKD, RKD, CRD, etc., NKD achieves state-of-the-art results with a simpler and more direct modification to the loss function.

- Compared to prior self-KD methods, USKD works for both CNNs and ViTs, while many others are designed only for CNNs. USKD also adds minimal overhead unlike methods requiring auxiliary networks.

- Compared to recent self-KD methods like Zipf's LS, USKD uses both target and non-target knowledge, instead of just non-target classes. This provides further improvements.

So in summary, this work pushes state-of-the-art for both teacher-student KD and self-supervised KD, with simple yet effective techniques to better transfer and generate knowledge in the models. The standardized frameworks of NKD and USKD are applicable to many model architectures.
