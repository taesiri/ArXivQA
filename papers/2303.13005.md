# [From Knowledge Distillation to Self-Knowledge Distillation: A Unified   Approach with Normalized Loss and Customized Soft Labels](https://arxiv.org/abs/2303.13005)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central research questions/hypotheses that this paper addresses are:1) How can knowledge distillation (KD) be improved to better utilize the soft labels from the teacher model for guiding the student model? 2) How can effective soft labels be generated for self-knowledge distillation when a teacher model is not available?Specifically:- For KD, the paper hypothesizes that normalizing the non-target logits in the distillation loss will allow better alignment between the teacher and student distributions, improving knowledge transfer. They propose Normalized KD (NKD) to test this.- For self-KD, the paper aims to develop a general and efficient method to generate customized soft labels incorporating both target and non-target class information without requiring a teacher model. They propose Universal Self-Knowledge Distillation (USKD) to achieve this.So in summary, the central research questions are around improving KD through normalized loss and generating effective customized soft labels for self-KD without a teacher model. The NKD and USKD methods are proposed to address these questions.
