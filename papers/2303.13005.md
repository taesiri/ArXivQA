# [From Knowledge Distillation to Self-Knowledge Distillation: A Unified   Approach with Normalized Loss and Customized Soft Labels](https://arxiv.org/abs/2303.13005)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading, the central research questions/hypotheses that this paper addresses are:

1) How can knowledge distillation (KD) be improved to better utilize the soft labels from the teacher model for guiding the student model? 

2) How can effective soft labels be generated for self-knowledge distillation when a teacher model is not available?

Specifically:

- For KD, the paper hypothesizes that normalizing the non-target logits in the distillation loss will allow better alignment between the teacher and student distributions, improving knowledge transfer. They propose Normalized KD (NKD) to test this.

- For self-KD, the paper aims to develop a general and efficient method to generate customized soft labels incorporating both target and non-target class information without requiring a teacher model. They propose Universal Self-Knowledge Distillation (USKD) to achieve this.

So in summary, the central research questions are around improving KD through normalized loss and generating effective customized soft labels for self-KD without a teacher model. The NKD and USKD methods are proposed to address these questions.


## What is the main contribution of this paper?

 This paper proposes two main contributions:

1. A new knowledge distillation method called Normalized Knowledge Distillation (NKD). NKD improves knowledge distillation by normalizing the non-target logits in the distillation loss. This allows better alignment between the teacher's and student's distributions. 

2. A self-knowledge distillation method called Universal Self-Knowledge Distillation (USKD). USKD generates custom soft labels for self-distillation without needing a teacher model. It creates soft target labels by smoothing the student's predictions and soft non-target labels based on the training data.

The key ideas are:

- NKD normalizes the non-target logits so their sum matches between teacher and student. This better transfers the teacher's knowledge.

- USKD generates custom soft labels for self-distillation. The soft target labels are smoothed student predictions. The soft non-target labels use the training data and Zipf's law.

- Both NKD and USKD achieve state-of-the-art results on CIFAR and ImageNet datasets. NKD boosts student accuracy in knowledge distillation. USKD provides effective self-distillation for CNNs and vision transformers.

In summary, the main contribution is improving knowledge distillation with normalized logits (NKD) and proposing an effective self-distillation method (USKD) that works for diverse model architectures. The techniques achieve excellent results on benchmark datasets.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence TL;DR summary of the paper:

The paper proposes methods to improve knowledge distillation performance by normalizing the non-target logits for better alignment and generating customized soft labels for self-distillation without needing an explicit teacher model.


## How does this paper compare to other research in the same field?

 This paper presents two main contributions to the field of knowledge distillation:

1. Normalized Knowledge Distillation (NKD): This improves upon classical knowledge distillation by normalizing the non-target logits before calculating the distillation loss. This allows the student model to better match the teacher's logit distribution. Experiments show this provides consistent improvements over standard KD and other advanced KD techniques like DKD.

2. Universal Self-Knowledge Distillation (USKD): This proposes a method to perform self-supervised knowledge distillation without a teacher model. It generates custom soft target and non-target labels for each image using the student's own predictions. This allows self-distillation to work well for both CNNs and vision transformers. 

Some key differences compared to prior work:

- Compared to standard KD, NKD better transfers the teacher's knowledge by normalizing the logit distributions. This simple change consistently outperforms standard KD.

- Compared to other advanced KD methods like DKD, RKD, CRD, etc., NKD achieves state-of-the-art results with a simpler and more direct modification to the loss function.

- Compared to prior self-KD methods, USKD works for both CNNs and ViTs, while many others are designed only for CNNs. USKD also adds minimal overhead unlike methods requiring auxiliary networks.

- Compared to recent self-KD methods like Zipf's LS, USKD uses both target and non-target knowledge, instead of just non-target classes. This provides further improvements.

So in summary, this work pushes state-of-the-art for both teacher-student KD and self-supervised KD, with simple yet effective techniques to better transfer and generate knowledge in the models. The standardized frameworks of NKD and USKD are applicable to many model architectures.


## What future research directions do the authors suggest?

 Based on my reading of the paper, some future research directions suggested by the authors include:

- Exploring other methods to generate customized soft labels for self-knowledge distillation. The authors propose using the student's intermediate features and predictions to generate soft labels, but other approaches could also be investigated. 

- Applying the proposed methods to other vision tasks beyond image classification, such as object detection, segmentation, etc. The authors demonstrate the benefit of their self-distillation method for an object detection task, but more exploration on other tasks would be useful.

- Adapting the proposed distillation methods for other model architectures besides CNNs and vision transformers, such as recurrent neural networks. The authors show their method works for both CNN and ViT models, but extending it to other architectures could further demonstrate its general applicability.

- Investigating how to better transfer knowledge from early layers in deeper teacher models. The authors focus on distilling knowledge from the teacher's final predictions, but transferring representations from early layers could also be beneficial.

- Exploring whether the proposed normalized distillation loss could be adapted for regression tasks beyond classification. The authors hint at potential benefits for regression but do not extensively validate it.

- Analyzing the theoretical underpinnings of why the proposed normalization and customized label generation improves knowledge transfer. While the methods are empirically shown to work well, providing further insight into the theory could be valuable.

- Continuing to improve the efficiency and minimize the overhead of self-distillation methods. The authors' method adds little computational overhead, but reducing it further or developing more efficient knowledge transfer approaches would be useful.

In summary, the authors propose several promising future directions such as generating soft labels in new ways, applying the methods to new tasks and architectures, theoretically analyzing the techniques, and improving efficiency. Advancing research in these areas could further develop the ideas presented in this paper.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the key points from the paper:

This paper proposes a unified approach for knowledge distillation (KD) and self-knowledge distillation (self-KD) by decomposing and reorganizing the KD loss. They introduce Normalized KD (NKD) which normalizes the non-target logits in the KD loss to better utilize the teacher's soft labels. For self-KD without a teacher, they propose Universal Self-KD (USKD) which generates customized soft labels for both target and non-target classes using the student's own predictions. The soft target label is obtained by smoothing the student's target logit. The soft non-target labels are generated using the rank from an intermediate feature layer and Zipf's law for the distribution. Experiments show NKD boosts ImageNet accuracy of ResNet18 from 69.90% to 71.96% with a ResNet34 teacher. USKD brings gains on ImageNet for CNNs like 1.17% for MobileNet, and also works for ViTs like 0.55% for DeiT-Tiny. The methods require negligible extra time and resources. NKD and USKD provide a unified way to improve model performance, achieving state-of-the-art for both knowledge distillation and self-knowledge distillation.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the paper:

This paper proposes a unified approach for knowledge distillation (KD) and self-knowledge distillation (self-KD) using normalized loss and customized soft labels. For KD, the authors decompose the distillation loss into target and non-target components. They find that normalizing the non-target logits allows better alignment of the teacher's and student's distributions, leading to improved performance. They introduce Normalized KD (NKD) which significantly boosts accuracy in KD scenarios. For self-KD without a teacher, the authors generate customized soft labels for both target and non-target classes. The target label is created by smoothing the student's own prediction while the non-target labels use the rank from an intermediate feature layer and Zipf's distribution. This Universal Self-KD (USKD) method works for both CNNs and vision transformers with negligible overhead. Extensive experiments on CIFAR and ImageNet validate the effectiveness of NKD for teacher-student KD and USKD for self-KD. For example, NKD improves ImageNet accuracy of ResNet18 from 69.90% to 71.96% with a ResNet34 teacher. USKD gains 1.17% and 0.55% on ImageNet for MobileNet and DeiT-Tiny respectively.

In summary, this paper presents a unified framework to improve both knowledge distillation and self-knowledge distillation. The key ideas are normalizing the distillation loss and generating customized soft labels. NKD boosts model accuracy when training with a teacher model. USKD allows effective self-supervised learning without a teacher model for diverse network architectures. Both methods advance the state-of-the-art in their respective domains.


## Summarize the main method used in the paper in one paragraph.

 The paper proposes a unified approach for knowledge distillation (KD) and self-knowledge distillation (self-KD) by decomposing and reorganizing the generic KD loss. The main contributions are:

1. They propose Normalized KD (NKD) which normalizes the non-target logits in the KD loss to enable better usage of the teacher's soft labels. This results in state-of-the-art KD performance. 

2. For self-KD without teachers, they propose Universal Self-Knowledge Distillation (USKD) which generates customized soft labels for both target and non-target classes. The target label is obtained by smoothing the student's own target logit. The non-target labels use the rank from weak supervision on intermediate features and Zipf's distribution. 

3. The proposed USKD works for both CNNs and ViTs, achieving new state-of-the-art self-KD results. It has negligible overhead compared to normal training.

In summary, the key innovation is the normalized KD loss and customized soft labels generation that unifies KD and self-KD in a simple and effective framework. This achieves excellent performance for both tasks with minimal overhead.
