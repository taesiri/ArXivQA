# [Better Batch for Deep Probabilistic Time Series Forecasting](https://arxiv.org/abs/2305.17028)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem: 
Most existing deep learning models for probabilistic time series forecasting assume errors are independent and identically distributed over time. However, errors often exhibit autocorrelation (also known as serial correlation), where errors are correlated across adjacent time steps. Ignoring such autocorrelation undermines forecasting performance.  

Proposed Solution:
This paper proposes a novel training approach to model error autocorrelation in probabilistic forecasting models. The key ideas are:

(1) Construct mini-batches by grouping consecutive training instances during model training. Each mini-batch contains errors across multiple adjacent time steps. 

(2) Model the joint distribution of errors within a mini-batch using a multivariate Gaussian. The covariance matrix is decomposed into a scale vector and a time-varying correlation matrix. 

(3) Parameterize the correlation matrix as a dynamic weighted sum of base kernel matrices to ensure it is positive definite and can adapt to evolving correlation structure over time.

(4) The time-varying weights are predicted from the hidden state of the base forecasting model and learned jointly. 

(5) Use the learned dynamic covariance matrix during prediction to calibrate and improve distribution estimates at each time step.

Main Contributions:

- Proposes a new training scheme using mini-batches to capture error autocorrelation in probabilistic forecasting models  

- Introduces a time-varying covariance matrix to model dynamic autocorrelation structure  

- Achieves notable improvement in prediction accuracy across multiple benchmark datasets and base models

- Provides a statistical framework to encode error autocorrelation without substantially increasing model parameters

- Enhances flexibility in training and offers better uncertainty quantification

The method is evaluated by applying it on DeepAR and Transformer models using diverse real-world datasets. Results confirm its effectiveness in improving performance of both base models across different datasets.
