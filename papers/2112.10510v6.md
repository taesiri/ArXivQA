# [Transformers Can Do Bayesian Inference](https://arxiv.org/abs/2112.10510v6)

## What is the central research question or hypothesis that this paper addresses?

 The key research question addressed in this paper is how to perform efficient and scalable Bayesian inference. Specifically, the paper focuses on building an approximation to a Bayesian neural network that can capture posterior uncertainty and make predictions on new data points. 

The main hypothesis is that a transformer neural network can be trained to approximate Bayesian inference and capture posterior uncertainty by fitting it to samples drawn from a prior distribution over datasets and neural network parameters. The prior serves as the "world model" that the transformer learns from.

Some key aspects of the hypothesis:

- A transformer can learn to perform approximate Bayesian inference from samples of datasets/models drawn from a defined prior distribution.

- The trained transformer can make predictions on new datasets and capture posterior uncertainty in those predictions.

- This approach can scale Bayesian neural networks to large datasets and models by avoiding expensive sampling procedures like MCMC during inference.

- The transformer trained this way performs comparably or better than strong baselines at representing uncertainty while being much faster.

So in summary, the central hypothesis is that a transformer trained on prior samples can efficiently learn to represent Bayesian posteriors for fast and scalable approximate inference. The paper aims to demonstrate this approach and compare it to other Bayesian neural network methods.


## What is the main contribution of this paper?

 The main contribution of the paper is presenting a novel approach for approximate Bayesian deep learning. The key ideas are:

- Proposing to use Transformers to learn flexible priors over neural network architectures and weights. This allows capturing complex correlations and dependencies between different architectural choices and weight values. 

- Showing that by training the Transformer to minimize cross-entropy on samples from the prior, it can accurately approximate posterior predictive distributions. This avoids expensive Monte Carlo sampling or variational inference methods.

- Demonstrating strong performance across a range of tasks including regression, classification and few-shot learning. The method competes with or exceeds Bayesian baselines in terms of accuracy and calibration.

- Introducing the Riemann distribution for regression, which is more flexible than Gaussian likelihoods typically used. This improves performance on regression tasks.

- Analyzing the theoretical properties of the cross-entropy training objective, proving that minimizing cross-entropy recovers the exact posterior given infinite data and capacity.

In summary, the key innovation is using Transformers in a novel way for scalable and accurate Bayesian deep learning by fitting flexible priors. This is shown to work well empirically across different tasks while having attractive theoretical properties. The method solves major challenges in Bayesian deep learning around flexibility, scalability and accuracy.
