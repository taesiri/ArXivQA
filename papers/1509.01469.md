# [Quantization based Fast Inner Product Search](https://arxiv.org/abs/1509.01469)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading, the key points of this paper are:

- It proposes a quantization-based approach called QUIP for fast approximate maximum inner product search (MIPS). 

- The key idea is to quantize each database vector in multiple subspaces using learned codebooks. The inner product between a query vector and a database vector is then approximated by the sum of inner products between the query subvectors and the corresponding quantized subvectors of the database point.

- This avoids having to map the vectors to a higher dimensional space as done in some prior works like ALSH.

- The paper shows the inner product approximation is an unbiased estimator if the quantizers are the centroids of their Voronoi cells.

- It presents two codebook learning procedures:
    - A k-means style algorithm that minimizes quantization error in inner product.
    - A constrained optimization method that also tries to preserve ranking when some example queries are available.

- Theoretical analysis is provided including concentration bounds on the probability of large deviation between exact and approximate inner product.

- Experiments on recommendation and classification datasets demonstrate superior accuracy compared to state-of-the-art hashing methods for MIPS.

So in summary, the key hypothesis is that learning subspace quantizers directly optimized for inner product search can lead to more accurate and efficient maximum inner product search compared to approaches based on mapping to higher dimensions like LSH. The paper provides supporting empirical evidence as well as some theoretical justification.


## What is the main contribution of this paper?

 Based on my reading, the main contributions of this paper are:

1. It proposes a quantization-based approach called QUIP for fast approximate maximum inner product search (MIPS). 

2. It formulates the quantization as a codebook learning problem that directly minimizes the quantization error in inner products. Each database vector is quantized in multiple subspaces via separate codebooks. 

3. It provides theoretical analysis consisting of concentration inequalities for the proposed quantization method.

4. It introduces additional constraints in the codebook learning when example queries are available, to further improve accuracy.

5. It evaluates the proposed methods extensively on recommendation and image classification datasets, showing significant improvements over prior state-of-the-art hashing techniques for MIPS.

6. It proposes a tree-quantization hybrid approach to further speed up search for very large databases.

In summary, the key novelty seems to be a principled quantization framework tailored for MIPS that learns asymmetric codebooks matched to the database and queries. This is shown to outperform previous approaches based on locality sensitive hashing through extensive experiments.
