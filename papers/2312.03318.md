# [Complementary Benefits of Contrastive Learning and Self-Training Under   Distribution Shift](https://arxiv.org/abs/2312.03318)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a summary paragraph of the key points from the paper:

This paper investigates the complementary benefits of self-training and contrastive pretraining under distribution shift. Through experiments on 8 distribution shift datasets, the authors find that combining self-training (using FixMatch) after contrastive pretraining (using SwAV) yields significant improvements in accuracy (3-8%) on target out-of-distribution data compared to using either method alone. However, in semi-supervised learning settings without distribution shift, the benefits of combining the two methods are negligible. To understand this, the authors analyze both techniques theoretically in a simplified model of distribution shift. Their analysis shows that contrastive pretraining performs "feature amplification", increasing reliance on invariant features over spurious ones. However, some dependence on spurious features remains, hurting out-of-distribution generalization. Self-training after contrastive pretraining initialization can then improve "linear transferability", reducing reliance on spurious features by fitting a linear probe on target unlabeled data. Thus, the two techniques provide complementary benefits under distribution shift. The theoretical intuitions are supported by experiments probing learnt representations. Overall, the paper demonstrates that self-training and contrastive learning should be combined to maximize accuracy on out-of-distribution data.
