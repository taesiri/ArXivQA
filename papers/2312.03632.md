# [Multimodal Data and Resource Efficient Device-Directed Speech Detection   with Large Foundation Models](https://arxiv.org/abs/2312.03632)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a summary paragraph of the key points from the paper:

This paper explores a multimodal model for device-directed speech detection that determines whether a user is addressing a virtual assistant. The model combines acoustic features from an audio encoder with decoder signals and transcriptions from an ASR system as input to a large language model (LLM). Specifically, audio features and decoder signals are mapped to learned prefixes that are concatenated with the ASR transcript embeddings and fed into the LLM, which is finetuned to make device-directed decisions. A low-rank adaptation method allows finetuning without changing the base LLM weights. The approach is designed to work in low-data scenarios with only a single frozen LLM, making it suitable for on-device deployment. Experiments show the multimodal model outperforms unimodal baselines, achieving lower error rates with only a fraction of the 80k training utterances. Specialized audio representations from an on-device model also prove more effective than general representations from a large foundation model when data is extremely limited. The method provides an accurate and efficient way to combine modalities for device-directed speech detection using a frozen LLM.
