# [Do LLMs Find Human Answers To Fact-Driven Questions Perplexing? A Case   Study on Reddit](https://arxiv.org/abs/2404.01147)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem Statement: 
The paper investigates how well large language models (LLMs) can model human answers to fact-driven questions posed on Reddit's r/Ask{Topic} communities. Specifically, it examines whether an LLM's perplexity of an answer correlates with the answer's peer-assigned score (as a proxy for quality/helpfulness). 

Methodology:
- Collect 409 fact-driven questions and 7,534 answers from 15 diverse r/Ask{Topic} subreddits 
- Use the LLaMA-2-70B model to filter and only keep fact-driven questions 
- Compare two LLMs: (1) Sheared LLaMA 1.3B (SL) and (2) Fine-tuned Sheared LLaMA 1.3B (SLFT)
- Calculate each LLM's perplexity for every answer, indicating how well the LLM models each answer
- Compare perplexities to the human-assigned score for each answer  

Key Findings:
- There is generally negative correlation between LLM perplexity and human score, especially for SLFT
- Lower LLM perplexity aligns with higher human score, suggesting LLMs capture linguistic properties that humans value
- Fine-tuning significantly improves correlation over pre-trained LLM

Main Contributions:
- Curates dataset of fact-driven Reddit question-answers
- Provides methodology to compare LLM perplexity to human scores  
- Demonstrates LLMs, especially fine-tuned ones, achieve relatively strong alignment with qualities humans value in answers to fact-driven questions
- Opens up further research into understanding what linguistic properties contribute to quality answers

In summary, the paper shows fine-tuned LLMs can effectively model helpful answers to fact-driven questions from Reddit, paving way for using LLMs to generate or evaluate such answers.


## Summarize the paper in one sentence.

 This paper analyzes how well large language models can model human answers to fact-driven questions posted on Reddit's r/Ask{Topic} communities, using perplexity and peer-assigned scores as evaluation metrics.


## What is the main contribution of this paper?

 Based on the content provided, the main contribution of this paper seems to be:

An analysis of how well large language models (LLMs) can model human answers to fact-driven questions posed on Reddit's r/Ask{Topic} communities. Specifically:

- The paper collects a dataset of 409 fact-driven questions and 7,534 human answers from 15 different r/Ask{Topic} subreddits. The questions are filtered to be fact-driven using the LLaMA-2-70B model.

- The ability of two LLM variants (an out-of-the-box Sheared LLaMA model and a fine-tuned version) to model the human answers is analyzed using perplexity. 

- The perplexity of the LLM-generated answers is compared to the human peer-assigned score for each answer to see if LLMs assign low perplexity (i.e. model well) to highly rated human answers.

So in summary, the main contribution is an analysis of how well current LLMs can capture the characteristics of high quality, fact-driven answers posed by humans on topic-specific Reddit communities, using perplexity as the main evaluation metric.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and concepts include:

- Fact-driven questions - The paper focuses specifically on modeling human answers to factually-grounded questions posed on Reddit's r/Ask{Topic} communities.

- Answer score - The peer-assigned score given to each answer comment on Reddit, used as a proxy for human perception and helpfulness. 

- Perplexity - A metric measuring how "surprised" a language model is by a sequence, used to evaluate how well LLMs can model human answers.

- Sheared LLaMA - A light-weight 1.3 billion parameter version of the LLaMA2-7B model used as the main LLM.

- Fine-tuning - Additional training of the Sheared LLaMA model on in-domain Reddit comment data to compare against the base model.

- Social media analysis - The paper situates the work in the context of analyzing complex linguistic phenomena on social media platforms.

Some other potentially relevant terms: autoregressive models, conditional probabilities, inter-annotator agreement, peer engagement, subjective vs fact-driven questions.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in the paper:

1. The paper proposes using perplexity as a metric to evaluate how well a language model can model human answers. However, perplexity may not fully capture semantic meaning and factual correctness. Are there other metrics that could complement perplexity to provide a more comprehensive evaluation?

2. The inter-annotator agreement scores for determining whether a question is fact-driven versus subjective are only moderate. How could the annotation process be improved to achieve higher agreement? Could a more nuanced labeling system (e.g. a scale rather than binary labels) help capture the spectrum of subjectivity?  

3. The paper uses peer-assigned scores from Reddit as a proxy for answer quality. However, Reddit scores could be biased. Was any analysis done to determine if certain types of answers tend to be upvoted more frequently, irrespective of quality? 

4. The dataset filters out questions with low engagement (less than 3 comments). Could this bias the types of questions selected? Could questions with few comments still be useful to analyze?

5. The paper utilizes Sheared Llama to conduct experiments due to computational constraints. Would results be meaningfully different with larger LLMs? Could an ablation study help determine the impact of model scale?

6. Only a subset of the data collected from Reddit was utilized in experiments. Was statistical power analysis conducted to determine appropriate sample sizes? Were assumptions made about effect sizes?

7. The fine-tuning dataset for SLFT consists of 100,000 comments from a related subreddit. Was any testing done to determine if fine-tuning on more in-domain data would improve results? Would transfer learning to the target task help?

8. What other subreddits were considered for inclusion beyond the 15 selected? Was inclusion based solely on subreddit size and activity? How was diversity and representation assessed?

9. Answer lengths on social media can vary greatly. Was max token length tuned as a hyperparameter? Are different optimal lengths for in-domain fine-tuning versus evaluation?

10. The paper focuses exclusively on Reddit as a dataset source. Would findings generalize to other social media sites or conversational datasets? What differences could impact results?
