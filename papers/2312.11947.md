# [Emotion Rendering for Conversational Speech Synthesis with Heterogeneous   Graph-Based Context Modeling](https://arxiv.org/abs/2312.11947)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Conversational speech synthesis (CSS) aims to generate speech utterances with appropriate prosody and emotions within a conversational context. However, prior CSS works have not thoroughly addressed challenges in modeling emotions due to lack of emotional conversational data and difficulty in stateful emotion modeling.

- Humans express emotions with varying intensities (e.g. weak, medium, strong). Modeling emotion intensity is important for speech expressiveness but has been missing from CSS research.

Proposed Solution:
- The authors propose a novel emotional CSS model called ECSS which includes:

1) Heterogeneous graph-based emotional context modeling: Constructs an Emotional Conversational Graph (ECG) with nodes representing text, audio, speaker, emotion categories and intensities of the dialogue history. Models complex emotional dependencies among nodes using a Heterogeneous Graph Transformer.

2) Emotion rendering module: Predicts emotion, intensity and prosody of the current utterance from ECG node representations. Uses contrastive losses to distinguish between emotion categories and intensities. 

- Additional emotional annotations of categories and intensities were created for the DailyTalk dataset to enable training.

Main Contributions:

- First in-depth study on modeling emotions in conversational speech synthesis
- Heterogeneous graph and contrastive learning mechanisms effectively model and render emotions 
- Comprehensive emotional labeling of public dataset
- Superior emotional expressiveness demonstrated through subjective and objective evaluations

In summary, the paper proposes novel context modeling and emotion rendering solutions to advance the capability of conversational speech synthesis in modeling emotions, backed by meticulous emotion annotation of an existing dataset. Evaluations demonstrate clear improvements in emotional expressiveness.
