# Learning video embedding space with Natural Language Supervision

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is:How can we map video embedding space to natural language in order to enable text-based video retrieval?The key hypotheses are:- By first extracting visual features from video frames using a pre-trained CNN, and then encoding those features through the CLIP model, videos can be projected into the same joint embedding space as text. - Adding temporal modeling with LSTM or Transformer architectures on top of the frame-level CLIP encodings can capture temporal relationships and improve video classification performance.- Mapping videos into the joint text-image embedding space will enable text-based video retrieval, where videos semantically related to a textual query can be efficiently retrieved.So in summary, the main research direction is developing an effective approach to project videos into a joint embedding space with text to enable text-based video retrieval. The key hypotheses are that using CLIP plus temporal modeling can achieve this, and that this joint embedding will improve retrieval performance compared to relying purely on metadata.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contributions appear to be:- Proposing a novel approach to map video embedding space to natural language by first extracting visual features from video frames using a CNN, and then encoding the features with CLIP to get video embeddings that lie in the joint embedding space with text.- Evaluating this approach on UCF101 and HMDB51 benchmark datasets and showing state-of-the-art performance for video retrieval based on natural language queries. - Extending the image-based CLIP model to video by adding LSTM/Transformer modules to model temporal relationships between frames and fuse them into a single embedding. This gave much better performance on action recognition compared to baseline approaches.- Demonstrating the capability of the model for text-based video retrieval on Kinetics and VIRAT datasets, where it can retrieve relevant videos for complex textual queries.So in summary, the main contribution is developing a video embedding space aligned with natural language supervision using CLIP, and showing its effectiveness for tasks like action recognition and text-based video retrieval. The temporal fusion modules allow better video understanding compared to simple extensions of image CLIP.
