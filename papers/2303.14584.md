# Learning video embedding space with Natural Language Supervision

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is:How can we map video embedding space to natural language in order to enable text-based video retrieval?The key hypotheses are:- By first extracting visual features from video frames using a pre-trained CNN, and then encoding those features through the CLIP model, videos can be projected into the same joint embedding space as text. - Adding temporal modeling with LSTM or Transformer architectures on top of the frame-level CLIP encodings can capture temporal relationships and improve video classification performance.- Mapping videos into the joint text-image embedding space will enable text-based video retrieval, where videos semantically related to a textual query can be efficiently retrieved.So in summary, the main research direction is developing an effective approach to project videos into a joint embedding space with text to enable text-based video retrieval. The key hypotheses are that using CLIP plus temporal modeling can achieve this, and that this joint embedding will improve retrieval performance compared to relying purely on metadata.
