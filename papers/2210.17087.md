# [DanZero: Mastering GuanDan Game with Reinforcement Learning](https://arxiv.org/abs/2210.17087)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
The paper tackles the problem of developing an AI system for the complex card game GuanDan. GuanDan has large state and action spaces, long episode lengths, and a variable number of players, posing significant challenges for AI development. Prior rule-based agents perform poorly, while existing RL algorithms like DQN and A3C struggle with the large action space.

Proposed Solution:
The paper proposes an AI system called DanZero that uses deep Monte Carlo reinforcement learning along with the following techniques:

1) Distributed self-play framework with actor processes for simulation and a learner process for neural network training. This facilitates efficient training.

2) Carefully designed state and action feature representations using 54-dim vectors to encode card combinations. This keeps feature sizes reasonable.

3) Deep neural network that takes state and legal action features as input and outputs state-action values. This handles the large state/action spaces.

4) Îµ-greedy action selection and episodic reward assignment during self-play to generate training data.

5) Preprocessing of actor Q-values on the learner before loss computation to account for delays. 

Main Contributions:

1) First AI system to achieve strong performance in the complex game of GuanDan using deep reinforcement learning.

2) Careful feature engineering and neural network design to handle challenges of large state/action spaces and long episodes. 

3) Demonstration that distributed self-play with deep Monte Carlo RL can work well even with variable number of players within a game.

4) Extensive evaluation showing DanZero outperforming state-of-the-art rule-based agents as well as human players after training for 30 days.

In summary, the paper makes significant contributions in using deep RL to solve a complex imperfect information game, designing an end-to-end pipeline tailored to the game's challenges, and systematically evaluating the strong performance of DanZero against competitive baselines.
