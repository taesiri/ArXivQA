# [KnowIT VQA: Answering Knowledge-Based Questions about Videos](https://arxiv.org/abs/1910.10706)

## What is the central research question or hypothesis that this paper addresses?

Based on my review, the central research question of this paper is:How can we develop an effective video understanding model that incorporates both visual information from the video content as well as background knowledge needed to answer complex natural language questions about the video?The key hypothesis appears to be that fusing multi-modal video representations (visual, textual, temporal) with external knowledge will allow for answering more complex video questions that require reasoning beyond just the visual content. Specifically, the paper introduces a new video question answering dataset called KnowIT VQA that contains knowledge-based questions on video clips that require knowledge of the broader context/plot to answer correctly. The paper then proposes a model called ROCK that retrieves relevant knowledge from a knowledge base and fuses it with visual and textual features from the video to answer the complex questions. The central research direction is developing and evaluating this knowledge-enabled video QA model on their new benchmark dataset.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contributions are:1. Introducing KnowIT VQA, a new video question answering dataset that combines visual, textual, temporal, and knowledge-based reasoning. The dataset contains over 24,000 human-generated question-answer pairs about the TV show The Big Bang Theory. 2. Proposing a video understanding model called ROCK that retrieves relevant knowledge from a knowledge base and fuses it with visual and textual features from the video to predict answers. 3. Demonstrating through experiments that incorporating external knowledge provides significant improvements in video QA performance on KnowIT VQA compared to methods that just use the visual and textual content.4. Showing that there is still a large gap between the performance of the proposed ROCK model and human accuracy on KnowIT VQA, indicating it is a challenging benchmark that can encourage further progress in video and knowledge-based reasoning.In summary, the main contribution is the introduction of a new knowledge-based video QA dataset, along with a proposed model that retrieves and reasons over external knowledge to answer questions that require real-world understanding beyond just the visual content. The work highlights the importance of knowledge in video QA and provides a challenging benchmark for future research in this area.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Based on my reading of the paper, here is a one sentence summary:The paper introduces KnowIT VQA, a new video question answering dataset requiring knowledge about the show to answer real-world natural language questions, along with a proposed model that retrieves relevant knowledge and fuses it with video and text representations to reason about the questions.


## How does this paper compare to other research in the same field?

Here are a few key points comparing this paper to other research in knowledge-based visual question answering:- It introduces a new video dataset (KnowIT VQA) for studying knowledge-based VQA, which is the first such dataset based on videos rather than static images. This allows probing questions that require temporal reasoning in addition to external knowledge.- Most prior work in knowledge-based VQA relies on structured knowledge bases. This paper takes a different approach of using natural language descriptions as the knowledge source, which better fits the open-domain nature of the questions.- The proposed model incorporates a knowledge retrieval module to find relevant knowledge from a pool of unstructured text. This is more flexible than methods that query knowledge bases, but may require more training data to learn effective retrieval.- Evaluation shows substantial gains from incorporating retrieved knowledge, demonstrating the value of knowledge-based reasoning. But performance still lags well behind human accuracy, suggesting room for improvement in knowledge modeling.- Compared to concurrent work like FVQA and OK-VQA, KnowIT VQA appears more challenging due to its video domain and less constrained question types.In summary, this paper makes a solid contribution in pushing VQA capabilities by combining external knowledge with complex video data. The introduced dataset and models open up new research directions in knowledge-based video understanding. More work is still needed to achieve human-level proficiency on this challenging task.
