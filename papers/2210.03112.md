# [A New Path: Scaling Vision-and-Language Navigation with Synthetic   Instructions and Imitation Learning](https://arxiv.org/abs/2210.03112)

## What is the central research question or hypothesis that this paper addresses?

 The central research question this paper addresses is: Can large-scale augmentation with high-quality synthetic instructions enable simpler imitation learning agents to achieve state-of-the-art performance on the challenging task of Vision-and-Language Navigation (VLN)?

The key points are:

1) VLN agents struggle with complex language grounding and spatial language understanding due to the scarcity of human annotated training data and limited diversity in training environments. 

2) Pretraining on large web image-text datasets has been explored but improvements are limited, likely because these datasets lack the action-oriented and spatially grounded language needed for VLN.

3) This paper investigates using a large-scale synthetic VLN dataset for augmentation. They use the high-quality Marky instruction generator to create 4.2M instruction-trajectory pairs across 500+ environments.

4) They train a simple transformer agent with imitation learning on this augmented dataset. Without any human annotations, this approach outperforms prior RL-based agents on the RxR benchmark, demonstrating the effectiveness of augmentation with near-human quality synthetic instructions.

In summary, the central hypothesis is that large-scale augmentation with high-quality synthetic instructions can enable simpler imitation learning agents to achieve new state-of-the-art results on challenging VLN tasks, despite the lack of diverse human annotated data. The experiments confirm this hypothesis.
