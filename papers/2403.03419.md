# [Negating Negatives: Alignment without Human Positive Samples via   Distributional Dispreference Optimization](https://arxiv.org/abs/2403.03419)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Large language models (LLMs) can propagate harmful content, requiring alignment to human values. 
- Existing alignment methods rely on noisy human-labeled pairs of preferred/dispreferred responses. Marginal difference in scores between pairs makes training challenging.  
- Given LLMs' capabilities, new focus is achieving alignment using solely human-labeled negative responses, maintaining helpfulness while reducing harmfulness.

Method - Distributional Dispreference Optimization (D2O):
- Maximizes discrepancy between self-generated responses and human-provided negative responses.  
- Learns a distribution-level preference model reflecting human dispreference against negative distribution.
- Converges to loss calculated with both positive and negatives, reducing noise.  
- Integrates implicit Jeffrey divergence regularization for better constraint on policy deviation.
- Forgets negative responses while retaining capabilities, minimizing harmfulness and catastrophic forgetting.

Contributions:
- Novel task formulation and method for LLM alignment using only human-labeled negatives.
- Theoretical analysis showing D2O learns distributional preference model and has training advantages.
- Experiments show superiority over strong baselines in harmfulness reduction with comparable helpfulness and better stability.

In summary, this paper introduced a new direction for LLM alignment focused on using readily available human negative feedback. The proposed D2O method achieves strong empirical performance in mitigating harmfulness while maintaining helpfulness. Key innovations are the distributional preference learning and implicit training regularization.
