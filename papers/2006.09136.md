# [When Does Self-Supervision Help Graph Convolutional Networks?](https://arxiv.org/abs/2006.09136)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the main research questions seem to be:1. Can self-supervised learning improve the performance of graph convolutional networks (GCNs) on node classification and other downstream tasks? 2. What are effective mechanisms to incorporate self-supervision into GCN training? The paper compares pretraining+finetuning, self-training, and multi-task learning.3. What are good self-supervised pretext tasks that provide useful inductive biases and regularization for GCNs? The paper proposes and evaluates node clustering, graph partitioning, and graph completion tasks. 4. Can self-supervision improve the robustness and uncertainty estimation of GCNs? The paper examines using self-supervision in adversarial training for GCNs.In summary, the central hypothesis is that properly designed self-supervised tasks and training mechanisms can enhance the generalizability, robustness, and uncertainty estimation of GCNs for node classification and other graph-based tasks. The paper aims to provide a systematic study to validate this hypothesis.
