# [DeepArchitect: Automatically Designing and Training Deep Architectures](https://arxiv.org/abs/1704.08792)

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper addresses is how to automatically design and train deep neural network architectures. The key hypothesis is that it is possible to develop a modular and composable language to specify expressive search spaces over architectures, and use this language to effectively search for well-performing architectures.The main goals of the paper are:- To develop a language that allows compactly representing complex search spaces over architectures and their hyperparameters.- To show this language enables systematically searching over these spaces to find good architectures, using algorithms like random search, Monte Carlo tree search, and sequential model-based optimization.- To demonstrate through experiments that the framework can discover competitive architectures on CIFAR-10 without much manual effort.So in summary, the paper aims to tackle the challenge of automating architecture design by proposing a new modeling language and search algorithms built on top of it. The key idea is the language provides a way to easily define and traverse spaces of architectures to uncover performant models.


## What is the main contribution of this paper?

This paper proposes a framework for automatically designing and training deep neural network architectures. The key contributions are:1. A modular, composable, and extensible language for compactly representing expressive search spaces over deep learning architectures. This allows:   - Expert control over the architecture variations to consider.   - Easy search over the space to find high-performing architectures.    - Automatic compilation of chosen architectures to computational graphs.2. Model search algorithms that leverage the tree-structured search spaces to efficiently search for good architectures, including:   - Random search, which is effective due to the expressiveness of the language.   - MCTS and SMBO, which outperform random search by generalizing across models based on structure. 3. Experiments demonstrating the framework's effectiveness for automatic model discovery. The language allows setting up expressive search spaces easily, and the search algorithms can discover competitive architectures without much human effort.In summary, the key innovation is an integrated framework that combines deep model specification, structured architecture search spaces, and automatic compilation. This provides an easy way for experts to search over and discover well-performing architectures.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper proposes a modular framework for automatically designing and training deep neural network architectures, featuring a composable language to easily define complex search spaces over models, algorithms to efficiently search these spaces, and the ability to automatically compile chosen architectures into executable computational graphs.


## How does this paper compare to other research in the same field?

Here are my thoughts on how this paper compares to other research in the field of automatic architecture search for deep learning:- The main contribution of this paper is the development of a modular, compositional language for specifying expressive search spaces over neural network architectures. This allows expert users to easily define complex spaces over architectures to explore. - This is different from some other approaches like evolutionary algorithms or reinforcement learning methods for architecture search, where the search space is hardcoded and less flexible. The language proposed here gives more control to the human expert.- Compared to general hyperparameter optimization tools like Hyperopt, this language is specialized for searching over architectures, allowing automatic compilation to computational graphs once models are fully specified. It is more tailored for architecture search than generic hyperparameter tuning.- The paper shows experimentally that random search over architectures defined with this language can work well, but more informed MCTS and SMBO search algorithms utilizing the structure of the space outperform random search.- The code and experiments are fairly simple proof-of-concept tests on CIFAR-10. The scale is much smaller than some other recent work like NASNet searching on ImageNet with far greater compute resources. But the goal here seems to be introducing the overall framework.In summary, the key differentiation of this work seems to be the introduction of a modular language for expressive architecture search spaces, which gives more control to experts than hardcoded evolutionary or RL methods. The experimental validation is fairly preliminary but shows promise. Overall it proposes an interesting new automation framework for architecture search.


## What future research directions do the authors suggest?

Based on my reading of the paper, some of the main future research directions suggested by the authors include:- Exploring more complex and expressive search spaces over architectures using their framework. The paper focuses on single-input single-output modules without output sharing for simplicity, but the ideas could extend to more complex architectures.- Developing more sophisticated model search algorithms that leverage the structure of the search space. The authors experiment with random search, MCTS, and SMBO, but other algorithms could be developed.- Improving the surrogate modeling and features used in SMBO. The authors use a simple ridge regressor with n-gram features, but more complex surrogate models and expressive features capturing architectural patterns could improve performance.- Extending the framework to support multi-objective optimization, like optimizing for accuracy and efficiency/computation time simultaneously. The current framework focuses on a single performance metric.- Developing better modular building blocks, like different types of merge operations in residual modules or attention modules. The compositionality of the language allows easily integrating new modules.- Supporting search spaces with shared modules and parameters. The current framework assumes no sharing for simplicity.- Implementing support for additional deep learning frameworks, like PyTorch, beyond the Tensorflow support.- Creating better visualization tools for understanding and analyzing the search spaces.- Using DeepArchitect as a platform for research in architecture search and hyperparameter optimization for deep learning.In summary, the authors propose their framework as a foundation that can be extended in many fruitful ways for architecture search and deep learning research. The modular components suggest avenues for improvements within each one independently.


## Summarize the paper in one paragraph.

Here is a one paragraph summary of the key points from the paper:The paper proposes a framework for automatically designing and training deep neural network architectures. The framework consists of three components: 1) a modular, composable language for specifying expressive search spaces over architectures and their hyperparameters; 2) model search algorithms like random search, Monte Carlo tree search, and sequential model-based optimization that leverage the tree structure of the search space to efficiently find good models; 3) a model evaluation algorithm defined by the user that computes a score indicating how good a fully specified model is. Experiments on CIFAR-10 compare different model search algorithms and show that techniques like Monte Carlo tree search and sequential model-based optimization outperform random search in terms of finding models with higher validation accuracy. Overall, the framework enables automatically searching over architectures in a flexible way and can serve as an effective tool for model discovery.


## Summarize the paper in two paragraphs.

Here is a 2 paragraph summary of the paper:The paper proposes a framework for automatically designing and training deep neural network architectures. The framework has three main components: a model search space specification language, a model search algorithm, and a model evaluation algorithm. The model search space specification language allows representing complex search spaces over architectures in a modular and extensible way. The language makes it easy to define new architectures by composing basic building blocks. The model search algorithms leverage the structure of the search space to efficiently explore promising architectures. Algorithms like Monte Carlo tree search and sequential model-based optimization are shown to outperform random search. The model evaluation algorithm trains and evaluates candidate architectures. Experiments on CIFAR-10 compare different search algorithms and show the framework can effectively explore architecture spaces, discovering competitive models without much manual effort. Overall, the framework provides an integrated approach to neural architecture search that gives more control to experts than prior work.


## Summarize the main method used in the paper in one paragraph.

Here is a one paragraph summary of the main method used in the paper:The paper proposes a framework for automatically designing and training deep neural network architectures. The key components of the framework are: (1) A modular, compositional language for specifying expressive search spaces over neural network architectures and their hyperparameters. The language allows complex architectures to be built up compositionally from simpler modules. (2) Model search algorithms like random search, Monte Carlo tree search, and sequential model-based optimization that leverage the tree structure of the search spaces to efficiently explore promising architectures. (3) Automatic compilation of fully specified models from the search space into executable computational graphs. The modular design allows each component to be improved independently. Experiments on CIFAR-10 demonstrate that the framework can effectively explore architecture spaces and discover competitive models. The sophisticated search algorithms like MCTS and SMBO outperform random search by focusing more on high-performing regions of the search space.
