# [Memory-Efficient Reversible Spiking Neural Networks](https://arxiv.org/abs/2312.07922)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a high-quality paragraph summarizing the key points of the paper:

This paper proposes reversible spiking neural networks (RevSNNs) to reduce the high memory cost of training deeper spiking neural networks (SNNs). The authors first extend the reversible architecture along the temporal dimension and propose a spiking reversible block that can reconstruct the computational graph and recompute intermediate activations in the forward pass through a reverse process. Building on this, they develop RevSResNet and RevSFormer, which are reversible variants of state-of-the-art SNN models MS-ResNet and SpikingFormer. Experiments on CIFAR10/100 and neuromorphic datasets demonstrate that the proposed RevSNNs achieve similar accuracy as their non-reversible counterparts, while significantly reducing memory consumption by not caching intermediate activations. For example, RevSResNet37 consumes 3.79x lower GPU memory per image than MS-ResNet34 with comparable parameters and FLOPs. Overall, RevSNNs break the linear dependency between network depth and memory cost during SNN training, unleashing memory constraints and paving the way for developing deeper SNN models.
