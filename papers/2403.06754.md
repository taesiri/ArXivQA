# [ALaRM: Align Language Models via Hierarchical Rewards Modeling](https://arxiv.org/abs/2403.06754)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem: 
- Current approaches for aligning large language models (LLMs) with human preferences struggle with inconsistent and sparse human supervision signals. This makes it difficult to provide accurate and consistent guidance, especially for complex open-ended text generation tasks.

Proposed Solution:
- The paper proposes a new framework called ALaRM that models rewards hierarchically to address the limitations of existing alignment approaches. 

- ALaRM integrates holistic rewards that measure overall quality with aspect-specific rewards that focus on particular dimensions. This enables more precise and consistent guidance towards desired outcomes.

- It employs a methodology to filter and combine multiple rewards based on their consistency with the holistic reward. This provides a reliable mechanism for improving alignment.

Key Contributions:
- ALaRM is the first framework to hierarchically model both holistic and aspect-specific rewards for aligning LLMs via reinforcement learning from human feedback.

- The paper investigates reward selection strategies to mitigate conflicting signals from different rewards.

- Comprehensive experiments on question answering and machine translation tasks demonstrate ALaRM's effectiveness in providing more accurate and consistent supervision. This sheds light on its potential for scalable oversight of complex LLM behaviors.

- Detailed ablation studies and analyses validate that ALaRM's hierarchical approach strengthens training signals and outperforms existing baselines.

In summary, the key innovation is a hierarchical rewards framework that handles inconsistent human signals more effectively to improve alignment of LLMs with human preferences, especially for open-ended generation tasks.
