# [DeePoint: Visual Pointing Recognition and Direction Estimation](https://arxiv.org/abs/2304.06977)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question is how to perform automatic visual recognition and 3D direction estimation of pointing gestures from monocular RGB video captured by a fixed-view camera. The key hypothesis is that pointing recognition and direction estimation can be achieved by modeling the spatio-temporal coordination of the whole body, not just the hands, using a deep neural network model.In particular, the paper proposes:1) A new dataset called DP Dataset containing over 2 million annotated frames of people pointing in a room environment.2) A deep network model called DeePoint that uses Transformer encoders to model spatial relationships between body parts in each frame and temporal relationships between frames. The experiments aim to validate that DeePoint can accurately recognize pointing gestures and estimate their 3D directions using only monocular RGB video as input, outperforming past geometry-based and hand-focused methods. The ablation studies also validate the importance of modeling full-body spatial-temporal coordination rather than just hands or individual frames.In summary, the main research question is monocular pointing recognition and direction estimation, addressed through the key ideas of the DP Dataset and modeling full-body spatio-temporal coordination with Transformers in DeePoint. The experiments aim to validate these contributions.
