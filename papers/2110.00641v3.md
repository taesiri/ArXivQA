# [Batch size-invariance for policy optimization](https://arxiv.org/abs/2110.00641v3)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the main research questions/hypotheses appear to be:

1) Can decoupling the proximal policy from the behavior policy in PPO's objectives improve performance and robustness, particularly when using stale data? 

The authors hypothesize that the proximal policy only needs to be a recent policy, not necessarily the behavior policy, to effectively constrain policy updates. This allows stale data to be used more efficiently.

2) Can making PPO batch size-invariant at small batch sizes through using an EWMA for the proximal policy and adjusting hyperparameters improve performance?

The authors hypothesize that by maintaining the age of the proximal policy and making other adjustments when changing the batch size, the original PPO behavior can be recovered, making it batch size-invariant.

3) Is using an EWMA for the proximal policy beneficial on its own, even without batch size adjustments? 

The authors hypothesize the EWMA may reduce variance and improve sample efficiency compared to using the previous policy parameters directly.

In summary, the key hypotheses relate to decoupling the proximal and behavior policies in PPO, using an EWMA proximal policy, and adjusting hyperparameters to achieve batch size-invariance. The authors test these hypotheses through experiments on Procgen environments.


## What is the main contribution of this paper?

 Based on my reading of the paper, the main contributions appear to be:

1. Introducing decoupled policy objectives for PPO that separate the proximal policy used to control policy updates from the behavior policy used for importance sampling. This provides greater robustness to stale data.

2. Showing how to make PPO and PPG batch size invariant by using an EWMA of the policy parameters as the proximal policy. This allows the batch size to be freely adjusted for computational reasons while preserving algorithm behavior. 

3. Providing analysis and experiments that help explain why PPO works and how decoupling the proximal policy can be beneficial. The experiments demonstrate the robustness to stale data with decoupled objectives and achieve a high degree of batch size invariance.

4. Discussing the implications of their findings, including viewing PPO as approximating natural policy gradients rather than trust regions, and providing practical advice for using policy optimization algorithms effectively at small batch sizes.

In summary, the key ideas introduced are decoupling the proximal and behavior policies in PPO, using an EWMA proximal policy to achieve batch size invariance, and providing analysis and empirical demonstrations supporting these ideas. The main contribution is showing how these modifications can improve policy optimization algorithms.
