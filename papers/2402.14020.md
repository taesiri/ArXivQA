# [Coercing LLMs to do and reveal (almost) anything](https://arxiv.org/abs/2402.14020)

## Summarize the paper in one sentence.

 Here is a one sentence summary of the key points from the paper:

The paper provides a broad overview of diverse adversarial attack objectives against large language models beyond jailbreaking that coerce unintended behaviors like misdirection, control, denial-of-service, or data extraction, analyzes attack mechanisms and success rates, and concludes that the security of language models must be addressed through comprehensive understanding of their capabilities and limitations.


## What is the main contribution of this paper?

 According to the paper, the main contribution is providing a broad overview of the spectrum of possible adversarial attacks against large language models (LLMs) beyond just jailbreaking attacks. Specifically, it discusses, categorizes and systematizes various types of attacks that can coerce unintended behaviors from LLMs, such as misdirection, model control, denial-of-service, or data extraction. 

The paper analyzes these different attacks through concrete examples and controlled experiments. It finds that many stem from training LLMs with coding capabilities and from the existence of "glitch" tokens in vocabularies. The authors conclude that the space of adversarial attacks against LLMs is much broader than previously thought, and security measures should comprehensively address the capabilities and limitations of these models.

In summary, the key contribution is broadening the understanding of adversarial threats to LLMs beyond alignment bypassing, providing a structured overview of attacks, analyzing commonalities and mechanisms exploited, and arguing for the need to re-assess model security given the ease of attack coercion.
