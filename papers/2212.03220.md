# [Visual Query Tuning: Towards Effective Usage of Intermediate   Representations for Parameter and Memory Efficient Transfer Learning](https://arxiv.org/abs/2212.03220)

## What is the central research question or hypothesis that this paper addresses?

 The central research question this paper addresses is how to effectively utilize intermediate features of pre-trained Vision Transformer models for parameter and memory efficient transfer learning to downstream tasks. Specifically, the key hypothesis is that introducing additional learnable "query" tokens to each Transformer layer allows summarizing the rich intermediate features in a way that keeps them intact, avoids expensive backpropagation, and enables effective transfer via linear probing.

The paper proposes Visual Query Tuning (VQT) as a simple yet effective approach to leverage frozen intermediate features of Transformers for transfer learning. The key ideas are:

1. Intermediate features of pre-trained models are informative for transfer but directly using them is challenging due to their huge number. 

2. Introducing learnable "query" tokens to each Transformer layer allows leveraging the self-attention mechanism to summarize intermediate features across tokens.

3. This keeps intermediate features intact, avoids expensive backpropagation, and enables memory-efficient training.

4. The summarized features can then be used for effective transfer learning via linear probing.

5. VQT is complementary to methods that adapt intermediate features, and combining them can further improve transfer learning performance.

So in summary, the central hypothesis is that VQT provides an effective and lightweight way to leverage rich intermediate features for parameter and memory efficient transfer learning. Experiments validate this approach and show advantages over prior methods.
