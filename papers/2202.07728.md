# [Don't Lie to Me! Robust and Efficient Explainability with Verified   Perturbation Analysis](https://arxiv.org/abs/2202.07728)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the main research question addressed is: How can we develop an explainability method for deep neural networks that provides exhaustive and unbiased explanations while remaining efficient?The key points are:- Current explanation methods like saliency maps, occlusion, or sampling-based approaches can be biased or fail to reliably reflect the true importance of inputs. - They also do not exhaustively explore the space of possible perturbations around an input, making explanations potentially unreliable.- The authors propose a new method called EVA that leverages verified perturbation analysis to exhaustively probe the perturbation space.- This allows for unbiased explanations that are guaranteed to cover all possible perturbations, unlike existing methods. - EVA is also designed to remain efficient by propagating bounds through the network in one pass rather than requiring extensive sampling.- They introduce techniques to scale EVA to large vision models like VGG-16.So in summary, the central research question is how to create a neural network explanation method that is exhaustive, unbiased, and efficient - which EVA aims to achieve through verified perturbation analysis.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contribution is the introduction of EVA (Explaining using Verified Perturbation Analysis), a new explainability method that leverages recent progress in verified perturbation analysis to generate explanations for deep neural networks. Some key points:- EVA is the first explainability method that comes with guarantees that an entire set of possible perturbations has been exhaustively searched. It does this by propagating bounds through the network using verified perturbation analysis. - This allows EVA to identify important pixels that drive a model's decision without relying on sampling, which can introduce biases.- EVA introduces a new estimator called "adversarial overlap" to quantify the importance of input variables by measuring the overlap between class confidence bounds when variables are fixed vs free to be perturbed.- The paper shows systematically that EVA yields state-of-the-art results on multiple benchmark explanation datasets and metrics.- The paper also introduces a scaling strategy to apply EVA to large vision models by estimating bounds empirically on an intermediate layer before propagating bounds through the rest of the network.- Overall, EVA leverages properties of verified perturbation analysis (efficiency, guaranteed coverage) to generate explanations that are reliable, efficient, and achieve excellent results based on standard evaluation metrics.In summary, the main contribution is presenting a novel way to generate visual explanations for deep neural networks that exploits formal verification methods to provide guarantees on exhaustive perturbation analysis. EVA pushes the explainability field forward through its reliability and efficiency.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper introduces EVA, a new explainability method that uses verified perturbation analysis to efficiently and exhaustively probe the space of possible perturbations around an input image in order to identify the most influential pixels driving a model's decision.


## How does this paper compare to other research in the same field?

This paper introduces a new explainability method called EVA that uses verified perturbation analysis to generate explanations for deep neural network predictions. Here are some key ways this paper compares to other research on explainability methods:- Leverages verified perturbation analysis: EVA is the first explainability method that uses verified perturbation analysis techniques like LiRPA to explore the space of possible perturbations around an input example. This allows it to provide guarantees that the full space has been searched, unlike other methods that rely on sampling or gradient estimation.- Focuses on robustness-based explanations: The EVA importance score is based on measuring how easily perturbations to input variables can change the network's decision. This connects to an emerging focus on robustness and stability for more reliable explanations. - Computationally efficient: Despite exhaustive perturbation analysis, EVA remains efficient by reusing computations. It is much faster than prior methods like Greedy-AS that use many adversarial attack iterations. The proposed scaling approach also allows application to large vision models.- Evaluated on multiple metrics: The paper tests EVA across standard deletion/insertion fidelity metrics as well as the newer robustness (SR) metric. EVA performs very competitively, achieving state-of-the-art or near SOTA across all metrics and datasets tested.- Enables targeted explanations: EVA can efficiently generate explanations focused on a specific class of interest, even if not originally predicted. This provides more insight into model behavior.Overall, EVA represents an innovative approach to explainability by connecting robustness analysis and formal verification. The exhaustive perturbation analysis is a unique advantage and could become even more powerful as verification techniques progress. The strong empirical results validate EVA's competitiveness with other latest methods while also offering guarantees and efficiency.
