# [Don't Lie to Me! Robust and Efficient Explainability with Verified   Perturbation Analysis](https://arxiv.org/abs/2202.07728)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the main research question addressed is: How can we develop an explainability method for deep neural networks that provides exhaustive and unbiased explanations while remaining efficient?The key points are:- Current explanation methods like saliency maps, occlusion, or sampling-based approaches can be biased or fail to reliably reflect the true importance of inputs. - They also do not exhaustively explore the space of possible perturbations around an input, making explanations potentially unreliable.- The authors propose a new method called EVA that leverages verified perturbation analysis to exhaustively probe the perturbation space.- This allows for unbiased explanations that are guaranteed to cover all possible perturbations, unlike existing methods. - EVA is also designed to remain efficient by propagating bounds through the network in one pass rather than requiring extensive sampling.- They introduce techniques to scale EVA to large vision models like VGG-16.So in summary, the central research question is how to create a neural network explanation method that is exhaustive, unbiased, and efficient - which EVA aims to achieve through verified perturbation analysis.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contribution is the introduction of EVA (Explaining using Verified Perturbation Analysis), a new explainability method that leverages recent progress in verified perturbation analysis to generate explanations for deep neural networks. Some key points:- EVA is the first explainability method that comes with guarantees that an entire set of possible perturbations has been exhaustively searched. It does this by propagating bounds through the network using verified perturbation analysis. - This allows EVA to identify important pixels that drive a model's decision without relying on sampling, which can introduce biases.- EVA introduces a new estimator called "adversarial overlap" to quantify the importance of input variables by measuring the overlap between class confidence bounds when variables are fixed vs free to be perturbed.- The paper shows systematically that EVA yields state-of-the-art results on multiple benchmark explanation datasets and metrics.- The paper also introduces a scaling strategy to apply EVA to large vision models by estimating bounds empirically on an intermediate layer before propagating bounds through the rest of the network.- Overall, EVA leverages properties of verified perturbation analysis (efficiency, guaranteed coverage) to generate explanations that are reliable, efficient, and achieve excellent results based on standard evaluation metrics.In summary, the main contribution is presenting a novel way to generate visual explanations for deep neural networks that exploits formal verification methods to provide guarantees on exhaustive perturbation analysis. EVA pushes the explainability field forward through its reliability and efficiency.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper introduces EVA, a new explainability method that uses verified perturbation analysis to efficiently and exhaustively probe the space of possible perturbations around an input image in order to identify the most influential pixels driving a model's decision.
