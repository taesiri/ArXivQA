# [The False Promise of Imitating Proprietary LLMs](https://arxiv.org/abs/2305.15717)

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper addresses is: can cheaply imitating proprietary large language models (LLMs) like ChatGPT enable open-source models to match their capabilities? The authors critically analyze the efficacy of "model imitation", where weaker open-source LMs are finetuned on outputs from stronger proprietary LLMs like ChatGPT. Their hypothesis is that model imitation may provide an easy shortcut for open-source models to match proprietary ones, without needing as much compute or data.However, through experiments training a variety of imitation models and evaluating them with humans and benchmarks, the authors find that model imitation has limited efficacy. Imitation models largely just mimic the style of proprietary models, not their capabilities. The authors conclude there still exists a substantial gap between open and closed LLMs that can only be closed by developing better base models, not by imitation shortcuts.In summary, the central hypothesis is that model imitation can cheaply close the gap between open and closed LLMs. But through experiments, the authors find this is a "false promise" and that improving base model quality is more important.


## What is the main contribution of this paper?

The main contribution of this paper is a critical analysis of the efficacy of "model imitation", which is the practice of training an open-source language model (LM) to mimic a proprietary LM like ChatGPT. The key findings and contributions are:- The authors train a variety of open-source LMs (ranging from 1.5B to 13B parameters) to imitate ChatGPT using different amounts of imitation data. They evaluate using both human evaluations and canonical NLP benchmarks.- Initially, imitation models appear promising - they are better at following instructions and human evaluators rate their quality as similar to ChatGPT. However, targeted automatic evaluations reveal that the imitation models do not actually improve performance on most NLP benchmarks.- The imitation models are good at mimicking ChatGPT's style and confidence but do not match its capabilities or factuality. The discrepancy arises because human evaluations can be fooled by stylistic fluency, while benchmarks specifically test capabilities.- Simply training on more imitation data provides diminishing returns and does not significantly close the gap to ChatGPT. Increasing the scale of the base LM is far more impactful.- The authors argue that model imitation does not provide a simple shortcut to match proprietary models. There exists a capabilities gap arising from differences in model scale, data, and algorithms. Overall, the key contribution is a rigorous analysis showing that model imitation has limitations and is not an easy shortcut to match proprietary models. The highest leverage path forward is developing better base models rather than imitating existing ones.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper critically analyzes the efficacy of training open-source language models to imitate proprietary models like ChatGPT, finding that while imitation can improve style and persona, it falls short in improving capabilities like factuality and problem solving, indicating that developing stronger base models is a higher leverage path forward.


## How does this paper compare to other research in the same field?

Here are my thoughts on how this paper compares to other research in the field of imitating large language models:- This paper provides one of the most comprehensive analyses to date of using model imitation to try to match proprietary models like ChatGPT. Many prior works have claimed near parity with little rigorous evaluation, whereas this paper conducts extensive human and automatic evaluations.- The paper's conclusion that model imitation has limitations in matching proprietary models echoes some prior skepticism. However, this paper provides perhaps the clearest experimental evidence validating this view through systematic experiments and analysis. - The paper introduces useful new analysis techniques for evaluating imitation models, like the targeted automatic evaluations to probe specific capabilities. This reveals failure modes that complement standard human evaluations.- The findings on the discrepancy between human and automatic evaluations highlight an important challenge for the field. Many prior works have relied predominantly on human evaluations, but this paper shows they can be misleading.- The experiments on scaling up base model size versus imitation data size provide new insights into the most promising directions for improving open source models. Most prior work has focused on scaling imitation data.- The paper studies imitation in a regime with a large capabilities gap between the base and target model. Most prior work considers smaller gaps or classification tasks. This setting is more representative of the true challenges today.Overall, this paper significantly advances our understanding of model imitation. It provides comprehensive evidence that imitation has limitations, especially when the base model is far weaker than the target model. This contrasts some recent optimism and suggests improving base models is more important than scaling imitation data. The analysis techniques and findings will inform future research on benchmarking, evaluating and improving large language models.


## What future research directions do the authors suggest?

Based on my reading of the paper, here are some of the key future research directions suggested by the authors:- Developing better methods for human evaluation of large language models. The authors point out inconsistencies between crowdworker evaluations and targeted NLP benchmark evaluations of imitation models. They suggest it is increasingly necessary to engage domain experts and curate highly difficult prompts to rigorously evaluate model capabilities.- Improving open-source language models by increasing model scale, using better pre-training data, and developing new pre-training methods. The authors argue this is a higher leverage way to improve models compared to imitation. - Studying the implications of using proprietary model outputs in various indirect ways, like for data annotation and augmentation. The authors suggest their findings may apply to these techniques as well.- Developing better methods for mitigating and detecting imitation models. The authors discuss the need for improved techniques with less severe tradeoffs.- Exploring legal and ethical questions around model imitation and intellectual property of closed AI systems. The authors suggest future work could delve into these issues.- Investigating cognitive biases in large language models through phenomena like GPT-4 replicating some human evaluation biases.Overall, the authors emphasize the need for future work to tackle the challenges around developing more capable open-source models, rigorous evaluation, and the responsible development and deployment of large language models.


## Summarize the paper in one paragraph.

Here is a one paragraph summary of the paper:The paper critically analyzes the efficacy of imitating proprietary large language models (LLMs) like ChatGPT using weaker open-source models. The authors train imitation models using varying amounts of data from ChatGPT and base models of different sizes. Although human evaluations show promise, targeted automatic evaluations reveal that imitation models fail to close the performance gap from the base model to ChatGPT, except on tasks heavily supported by the imitation data. This is because imitation models mainly learn ChatGPT's style rather than its capabilities. The key limitation is the weaker base model, not the lack of imitation data. Thus, the highest leverage path forward is improving base model capabilities, not taking shortcuts by imitating proprietary models. Overall, the paper concludes that imitation is a false promise for matching proprietary LLMs' capabilities with today's open-source models.


## Summarize the paper in two paragraphs.

Here is a two paragraph summary of the paper:The paper explores the efficacy of imitating proprietary language models like ChatGPT by training open-source models on datasets of API outputs from the proprietary models. The authors train a variety of imitation models by finetuning base models like GPT-2 and LLaMA on datasets collected from sources like ShareGPT. They evaluate the imitation models using both human evaluations and performance on NLP benchmarks. Initially, the imitation models appear promising based on human evaluations, with outputs rated competitively to ChatGPT. However, targeted automatic evaluations reveal that imitation provides little to no improvement on capabilities like factuality and knowledge. The authors conclude that imitation fails to close the substantial capabilities gap between open and closed models. They argue the highest leverage approach is developing better base models, not taking shortcuts by imitating proprietary systems. Overall, the paper provides an insightful critical analysis of model imitation, an approach that has gained popularity for trying to match proprietary LLMs.


## Summarize the main method used in the paper in one paragraph.

Here is a one paragraph summary of the main method used in the paper:The paper critically analyzes the efficacy of model imitation for improving open-source language models (LMs) by imitating proprietary models like ChatGPT. The authors train imitation LMs using datasets of ChatGPT inputs and outputs collected from public sources. They experiment with different base model sizes (1.5B to 13B parameters), different amounts of imitation data (0.3M to 150M tokens), and both task-specific and broad-coverage imitation data. The imitation models are evaluated using human evaluations as well as targeted automatic evaluations on NLP benchmarks. The key findings are that model imitation improves subjective quality according to crowdworkers but fails to close the capabilities gap with proprietary models. In particular, imitation models improve mainly on tasks that are heavily supported in the training data but do not generalize well to other tasks. The results suggest that model imitation has limitations for broadly improving open-source LMs, and it is better to focus efforts on developing stronger base models rather than imitating proprietary systems.
