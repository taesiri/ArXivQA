# [The False Promise of Imitating Proprietary LLMs](https://arxiv.org/abs/2305.15717)

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper addresses is: can cheaply imitating proprietary large language models (LLMs) like ChatGPT enable open-source models to match their capabilities? The authors critically analyze the efficacy of "model imitation", where weaker open-source LMs are finetuned on outputs from stronger proprietary LLMs like ChatGPT. Their hypothesis is that model imitation may provide an easy shortcut for open-source models to match proprietary ones, without needing as much compute or data.However, through experiments training a variety of imitation models and evaluating them with humans and benchmarks, the authors find that model imitation has limited efficacy. Imitation models largely just mimic the style of proprietary models, not their capabilities. The authors conclude there still exists a substantial gap between open and closed LLMs that can only be closed by developing better base models, not by imitation shortcuts.In summary, the central hypothesis is that model imitation can cheaply close the gap between open and closed LLMs. But through experiments, the authors find this is a "false promise" and that improving base model quality is more important.


## What is the main contribution of this paper?

The main contribution of this paper is a critical analysis of the efficacy of "model imitation", which is the practice of training an open-source language model (LM) to mimic a proprietary LM like ChatGPT. The key findings and contributions are:- The authors train a variety of open-source LMs (ranging from 1.5B to 13B parameters) to imitate ChatGPT using different amounts of imitation data. They evaluate using both human evaluations and canonical NLP benchmarks.- Initially, imitation models appear promising - they are better at following instructions and human evaluators rate their quality as similar to ChatGPT. However, targeted automatic evaluations reveal that the imitation models do not actually improve performance on most NLP benchmarks.- The imitation models are good at mimicking ChatGPT's style and confidence but do not match its capabilities or factuality. The discrepancy arises because human evaluations can be fooled by stylistic fluency, while benchmarks specifically test capabilities.- Simply training on more imitation data provides diminishing returns and does not significantly close the gap to ChatGPT. Increasing the scale of the base LM is far more impactful.- The authors argue that model imitation does not provide a simple shortcut to match proprietary models. There exists a capabilities gap arising from differences in model scale, data, and algorithms. Overall, the key contribution is a rigorous analysis showing that model imitation has limitations and is not an easy shortcut to match proprietary models. The highest leverage path forward is developing better base models rather than imitating existing ones.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper critically analyzes the efficacy of training open-source language models to imitate proprietary models like ChatGPT, finding that while imitation can improve style and persona, it falls short in improving capabilities like factuality and problem solving, indicating that developing stronger base models is a higher leverage path forward.
