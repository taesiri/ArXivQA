# [Denotationally Correct, Purely Functional, Efficient Reverse-mode   Automatic Differentiation](https://arxiv.org/abs/2212.09801)

## What is the central research question or hypothesis that this paper addresses?

This paper presents a new approach for reverse-mode automatic differentiation in a purely functional programming language. The central hypothesis is that it is possible to design a reverse-mode AD transformation that is:- Denotationally correct - it is proven semantically correct using categorical semantics.- Purely functional - it avoids using mutable state or other impure constructs. - Provably efficient - it satisfies the "cheap gradient principle" meaning the cost of computing gradients is no more than a small constant factor times computing the original function.- Modular - it decomposes the overall transformation into separate compilation steps with cleaner interfaces.The key ideas involve:- Introducing a novel "Unary Normal Form" intermediate representation inspired by PROPs and compilation to categories. This simplifies reasoning about reverse-mode AD.- Using partial evaluation and program transformations to optimize the naively constructed gradients into efficient code.- Leveraging array primitives like map, reduce, scan to express the gradients, enabling parallelism and optimizations.- Providing denotational semantics using multicategories and proving the correctness of each compilation step.The overall approach demonstrates it is possible to achieve efficient, provably correct reverse-mode AD in a pure functional language through a combination of novel intermediate representations, semantic models, and optimization techniques. The modularity also makes the approach easier to understand, extend and optimize further.


## What is the main contribution of this paper?

Here is a summary of the key contributions of this paper:- It presents a purely functional (no side effects or mutation) reverse-mode automatic differentiation transformation for a functional language with arrays. This transformation is provably efficient and correct.- It introduces a novel intermediate representation called Unary Normal Form (UNF) to decompose the complex reverse-mode transformation into simpler steps. UNF is inspired by PROPs and compilation to categories. - It proves complexity guarantees showing the transformed program satisfies a version of the cheap gradient principle, meaning the cost of computing gradients is no more than a small constant factor times the cost of the original program.- It gives a denotational semantics for the language and transformations using multicategories and concategories. This semantics is used to prove correctness of the overall transformation by verifying correctness of each compilation step.- The transformation produces efficient code by using partial evaluation and optimizations on the pure functional output. For simple programs the output resembles static single assignment (SSA) code.- It shows the approach is modular - the language and transformation can be easily extended, for example with more array operations or conditionals.In summary, the key contribution is a provably efficient and correct reverse-mode automatic differentiation technique for a functional array language, factored into simpler steps via a novel intermediate representation. The functional purity allows optimizations and a formal semantics.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper presents a provably efficient and purely functional reverse-mode automatic differentiation transformation on a functional array language, using a novel intermediate representation called Unary Normal Form to decompose the complex transformation into simpler compilation steps and leverage categorical semantics to prove correctness.


## How does this paper compare to other research in the same field?

Here are my thoughts on how this paper compares to other research in differentiable programming:- This paper presents a purely functional approach to reverse-mode automatic differentiation that provides efficient gradients without sacrificing purity or correctness. Many other works in this area use impure techniques like mutable state or only focus on correctness without efficiency guarantees. The functional purity here is a notable advantage.- The complexity analysis showing that the gradient computation satisfies a form of the cheap gradient principle is a useful theoretical result. Other works in functional AD often lack formal complexity guarantees. The cheap gradient principle is important for efficiency.- Introducing the Unary Normal Form (UNF) as an intermediate representation is a novel approach not seen in other work. UNF helps decompose the overall transformation into simpler steps and aids the proofs. - Using denotational semantics in categories/multicategories to prove correctness gives a strong semantic foundation. Some other papers use operational semantics or informal reasoning. The categorical semantics connects to broader foundations.- Support for array operations like map, reduce, scan puts this on firmer ground for modern ML than papers dealing only with scalars. Arrays are crucial for efficiency.- The overall architecture using compilation through UNF seems more modular and extensible than end-to-end source transformations in other works. Adding extensions like new array ops or conditionals is simplified.- There is less emphasis here on higher-order functions compared to some recent papers. Higher-order AD is still an open challenge. But the techniques here could integrate into compilers for higher-order languages.So in summary, I'd say this paper advances the state of the art in functional AD by providing an efficient, provably correct technique with formal complexity guarantees, while also introducing new ideas like the UNF representation and compilation-based architecture. The connections to broader theory via categories are also valuable.
