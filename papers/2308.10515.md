# [QD-BEV : Quantization-aware View-guided Distillation for Multi-view 3D   Object Detection](https://arxiv.org/abs/2308.10515)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central research question this paper tries to address is how to enable efficient deployment of state-of-the-art multi-view 3D object detection models based on bird's eye view (BEV) representations while maintaining high accuracy. Specifically, the paper points out that existing BEV models like BEVFormer have high computational and memory requirements, making deployment difficult. At the same time, directly applying standard model compression techniques like quantization can lead to significant accuracy drops. To address this, the paper proposes a new method called QD-BEV that applies progressive quantization-aware training and a novel view-guided distillation approach to obtain compact yet accurate BEV models. The key research question is whether their proposed techniques can enable efficient deployment of BEV models without sacrificing accuracy.The paper shows through experiments that their QD-BEV models can match or even exceed the accuracy of the original BEVFormer models while requiring 8x less memory. This demonstrates the effectiveness of their method in obtaining efficient BEV models. The view-guided distillation technique is a key contribution that helps boost quantized model accuracy by leveraging complementary image and BEV knowledge.In summary, the central research question is how to deploy resource-hungry BEV models efficiently without hurting accuracy, which the paper addresses through quantization-aware training and view-guided distillation. The experiments validate that their approach can produce highly compact yet accurate BEV models.


## What is the main contribution of this paper?

Based on my reading, the main contributions of this paper are:1. The authors propose a new method called QD-BEV for efficient multi-view 3D object detection based on bird's eye view (BEV). 2. They conduct systematic experiments to analyze the issues with directly quantizing BEV models, showing that standard post-training quantization (PTQ) and quantization-aware training (QAT) methods face challenges like training instability and significant performance degradation.3. To address these challenges, they design a novel view-guided distillation (VGD) technique specifically for BEV models. VGD leverages complementary information from both image features and BEV features to boost performance and stabilize training. 4. Through extensive experiments on the nuScenes dataset, they demonstrate that their QD-BEV models can match or even outperform the floating-point counterparts under 4-bit weight and 6-bit activation quantization. For example, their QD-BEV-Tiny achieves 1.8% higher NDS than the baseline BevFormer-Tiny, while having 8x smaller model size.5. Compared to standard PTQ and QAT baselines, QD-BEV shows superior accuracy under ultra-low bit quantization. The results highlight the effectiveness of their proposed techniques in obtaining highly efficient yet accurate BEV models.In summary, the key innovation is the design of view-guided distillation to enable effective deployment of compact BEV models via quantization, with empirical results validating their approach over baselines. The method helps bridge the gap between accuracy and efficiency for practical multi-view 3D detection.
