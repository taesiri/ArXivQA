# [Leveraging Non-Decimated Wavelet Packet Features and Transformer Models   for Time Series Forecasting](https://arxiv.org/abs/2403.08630)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem Statement
The paper focuses on improving time series forecasting performance by using wavelet analysis techniques to generate additional features, which are then fed into various machine learning models. Time series data is ubiquitous across many fields, but accurately forecasting future values is challenging, especially for non-stationary data. The authors argue that wavelets can capture useful multi-scale and temporal properties of time series that may improve forecasting accuracy when used alongside machine learning models.  

Key Contributions
The main contributions of this work are:

1) Evaluate Daubechies wavelets with different numbers of vanishing moments as features for both non-temporal (e.g. regression) and temporal forecasting models (e.g. RNNs). The wavelet is selected via cross-validation.

2) Compare using the non-decimated wavelet transform (NDWT) versus the non-decimated wavelet packet transform (NWPT) for generating wavelet features. The latter provides more coefficient vectors that may better capture signals.

3) Propose an online algorithm to compute NDWT and NWPT features that avoids leakage of future info into the features.

4) Evaluate wavelet features on a much wider range of forecasting methods than prior works, including latest deep learning models like Transformers.

5) Demonstrate improved 1-step and multi-step forecast accuracy on real and synthetic datasets when using wavelet features compared to just lags for most models.

Proposed Solution
The authors leverage NDWT and NWPT to generate multiple wavelet coefficient vectors from time series, using an online algorithm that prevents future data leakage. Dimensionality reduction via ridge regression or PCA is used to select the most relevant vectors as additional features for forecasting models like RNNs, Transformers etc. Models are trained in a rolling manner and tuned via cross-validation.

Key Results
- For 1-step forecasting, wavelet features outperform lag features for all non-temporal models, with over 30% SMAPE reduction for some datasets/models.
- For multi-step forecasting with deep models, wavelet features show modest but consistent gains over univariate models for most datasets and architectures.
- Cross-validation indicates that non-Haar wavelets often perform best, justifying selection of wavelet complexity.

In summary, the paper presents a systematic framework and empirical analysis that strongly advocates for using wavelet transforms to extract useful features from time series when building forecasting models. The results demonstrate broader applicability and superior performance compared to prior related works.


## Summarize the paper in one sentence.

 This paper combines wavelet analysis techniques with machine learning methods for univariate time series forecasting, demonstrating performance improvements from using wavelet features across a wide range of statistical and deep learning forecasting models.


## What is the main contribution of this paper?

 Based on my reading of the paper, the main contributions are:

1) Considering the use of Daubechies wavelets with different numbers of vanishing moments as input features to both non-temporal and temporal forecasting methods, by selecting these numbers during the cross-validation phase. 

2) Comparing the use of both the non-decimated wavelet transform (NDWT) and the non-decimated wavelet packet transform (NWPT) for computing wavelet features. The NWPT provides a much larger set of potentially useful coefficient vectors. The wavelet coefficients are computed using a shifted algorithm to avoid leakage of future information.

3) Evaluating the use of wavelet features on a significantly wider set of forecasting methods than previous studies, including both temporal and non-temporal models, statistical and deep learning-based methods. The latter include state-of-the-art transformer-based neural network architectures.

In summary, the main contribution is a comprehensive evaluation of wavelet features across a diverse range of forecasting methods, using proper procedures to avoid data leakage and select optimal wavelet parameters.


## What are the keywords or key terms associated with this paper?

 Based on scanning the paper, some of the key terms and keywords associated with this paper include:

- Time series forecasting
- Wavelets
- Wavelet transforms
- Non-decimated wavelet transforms
- Wavelet packet transforms 
- Machine learning
- Deep learning
- Neural networks
- Transformers
- Temporal Fusion Transformer
- Informer
- Autoformer
- Patch Time Series Transformer

The paper examines using wavelet analysis techniques, including non-decimated wavelet transforms and non-decimated wavelet packet transforms, to generate features for machine learning and deep learning models to improve time series forecasting performance. It evaluates these wavelet features on a variety of models, including ridge regression, random forests, LSTM networks, and several recent transformer-based architectures. So the key focus is on leveraging multiscale wavelet features to enhance time series forecasting across different types of models.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in this paper:

1) The paper proposes using both the non-decimated wavelet transform (NDWT) and non-decimated wavelet packet transform (NWPT) to generate features for time series forecasting. What are the key differences between these two transforms and what motivates the use of both in this framework?

2) The paper suggests using a shifted version of the typical pyramidal algorithm to compute the NDWT and NWPT coefficients. What is the benefit of this shifted approach and how does it allow the features to be computed in an online fashion without leakage of future information?

3) The paper examines the use of wavelet features with both non-temporal and temporal machine learning methods. Why might wavelet features be particularly beneficial when used with temporal models compared to just using lagged values as inputs?

4) The paper demonstrates performance gains from using wavelet features compared to just lags across most models and datasets. However, the gains are much larger on average for the non-temporal methods. What explanations are provided in the paper for why wavelets seem more impactful for one-step ahead forecasting?

5) For the temporal deep learning models, what motivates the inclusion of an additional cross-validation step to select the wavelet number compared to just using the Haar wavelet transform? What trend is observed regarding the choice of optimal wavelet numbers?

6) Why does the use of additional wavelet coefficient vectors as inputs to the temporal models not seem to cause overfitting, despite incorporating no extra information beyond the original time series? 

7) The paper examines the use of wavelet features for both short- and long-horizon forecasting tasks. What is the rationale behind evaluating performance over a large range of forecast horizons in the experiments with temporal models?

8) The results demonstrate particular benefit to using wavelet features for forecasting wind electricity supply and humidity time series. What properties of wavelets make them well suited to capturing patterns in these types of time series?

9) The paper proposes future work in comparing selection approaches over the coefficient vectors from different wavelet numbers. What are some possible selection methods that could be examined? What challenges might arise?

10) Could the benefits demonstrated from using wavelet features as additional inputs be attributed to an ensemble effect? How could the paper better isolate the improvements stemming from the multiscale decomposition itself?
