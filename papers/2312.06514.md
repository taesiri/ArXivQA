# [Where exactly does contextualization in a PLM happen?](https://arxiv.org/abs/2312.06514)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a summary paragraph of the key points from the paper:

This paper investigates where exactly contextualization of word meanings happens within the architecture of pretrained language models like BERT. Rather than just analyzing the final output layers, the authors examine the internal "sub-layers" within each BERT encoder layer, specifically the Self-Attention, Activation, and Output sub-layers. Using contextuality measures like sub-layer similarity, word embedding similarity, and PCA visualized distances, they analyze the vector representations of polysemous words in different sentential contexts across these sub-layers. They find that the Self-Attention and Activation sub-layers exhibit stronger and earlier signs of contextualization compared to the Output sub-layers. The Output sub-layer representations retain more similarity to the word's static embedding, indicating less contextualization. Over the layers, they see contextualization increase and then decrease, with peaks between layers 5-9, similar to previous works. A key finding is that word meaning transformation seems to occur most strongly in the Self-Attention and Activation sub-layers rather than the Output. This localized analysis of contextualization within BERT's encoder sheds new light on how these models capture word meanings.


## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Pre-trained language models (PLMs) like BERT learn contextualized representations of words, encoding word meaning via textual context rather than static word embeddings. 
- Prior work has explored contextualization in PLMs by analyzing the output layer(s), but this paper aims to localize where exactly in the BERT architecture contextualization of polysemous words happens.

Methodology:
- Use the Contextualised Polysemy Word Sense dataset containing polysemous words in sentential contexts. 
- Extract vector representations from 3 sub-layers in each BERT encoder layer: Self-Attention (SA), Activation (Acts), Output. 
- Also extract static BERT word embeddings (layer 0) for comparison.
- Define contextuality measures: 
    - Sub-Layer Similarity (SLSim): Cosine similarity between vector reps of the same polysemous word. Lower = more contextualized.
    - Word Embedding Similarity (WESim): Cosine similarity to static embedding. Lower = more contextualized. 
    - PCA L2 Distance: Euclidean distance between PCA projections. Higher = more contextualized.

Results:
- Output sub-layer is less contextualized than SA and Acts sub-layers.
- Contextualization happens more strongly between layers 5-9. 
- SA and Acts sub-layers exhibit earlier and stronger contextualization signals than Output.
- WESim shows influence of residual connections on Output but not on SA.
- PCA visually confirms stronger contextuality in SA and Acts than Output.

Conclusion:
- Localizes contextualization of polysemous words to happen more strongly in SA and Acts sub-layers compared to Output. 
- Provides evidence that word meaning transformation occurs in these sub-layers rather than just the output layer.

Main Contribution:
- First study to analyze contextuality in BERT sub-layers instead of just output layer(s)
- Evidence that word contextualization happens strongly in SA and Acts sub-layers of BERT encoders


## Summarize the paper in one sentence.

 This paper investigates where in the architecture of BERT word sense disambiguation happens by analyzing the representations of polysemous words in the self-attention, activation, and output sub-layers of each BERT encoder layer using contextualization measures and dimensionality reduction.


## What is the main contribution of this paper?

 Based on the content of the paper, the main contribution is:

The authors investigate representations of polysemous words not just in the output layers of BERT, but also in different sub-layers within each BERT encoder layer. Specifically, they look at the Self-Attention sub-layer, Activation sub-layer, and Output sub-layer in each encoder layer. They apply dimensionality reduction techniques on these latent sub-layers to better visualize their qualitative findings. Their key contributions are:

1) Confirming previous findings that higher layers of BERT exhibit higher degrees of contextualization. 

2) Finding for the first time that representations in the Self-Attention sub-layers exhibit stronger and earlier signs of contextualization compared to the Activation and Output sub-layers.

So in summary, the main contribution is localizing where in the BERT architecture word contextualization happens, by looking granularly at the different sub-layers rather than just the output layers. The key finding is that contextualization happens most strongly in the Self-Attention sub-layers.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and keywords associated with it are:

- Pre-trained Language Models (PLMs)
- Contextualized representations
- BERT
- Encoder layers and sub-layers (Self-Attention, Activation, Output)  
- Contextuality measures (Sub-Layer Similarity, Static Word Embedding Similarity, Principal Components Analysis)
- Polysemous words
- Word Sense Disambiguation (WSD)
- Dimensionality Reduction (DR)
- Representational Similarity Analysis (RSA)

The paper investigates where exactly contextualization of word meanings happens within the BERT model by analyzing the vector representations of polysemous words in different sub-layers of the BERT encoders. It applies contextuality measures and dimensionality reduction techniques to quantify and visualize the extent of contextualization across these sub-layers. The key goal is to localize where word meaning transformation occurs in the self-attention, activation and output sub-layers of BERT.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in the paper:

1. The paper mentions using the Contextualised Polysemy Word Sense v2 Dataset. What are some of the key properties and statistics of this dataset that make it suitable for the experiments in this work?

2. The SubLayerSim measure is used to quantify the degree of contextualization in the different sub-layers. What are some potential limitations or drawbacks of using cosine similarity in this way to measure contextuality? 

3. The paper extracts representations from three different sub-layers - Self-Attention, Activations, and Output. What is the motivation behind choosing these particular sub-layers to analyze? Are there any other sub-layers that could also be relevant?

4. Dimensionality reduction using PCA is utilized in the paper. What are some alternative dimensionality reduction or representation learning methods that could be used instead of PCA in this context and what would be their relative advantages/disadvantages?

5. The static word embedding similarities (WESim) behave differently for the Self-Attention and Output sub-layers. What does this indicate about the flow and usage of information between these sub-layers?

6. What other quantitative contextualization measures beyond SubLayerSim and WESim could be defined and used to analyze the degree of contextuality captured in the different sub-layers?

7. The paper indicates higher layers exhibit more contextuality. Is there a theoretical motivation or explanation for why deeper layers in BERT can capture more contextual aspects? 

8. How sensitive are the conclusions to the specifics of the BERT architecture used? Would the same observations hold for other transformer-based PLMs?

9. The paper analyzes polysemous words. Would the conclusions generalize to other categories of words and linguistic phenomena beyond just polysemy?

10. The paper indicates potential "contextualization redundancies" across sub-layers. What could be the causes of such redundancies and how could representational similarity analysis help uncover them?
