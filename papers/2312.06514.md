# [Where exactly does contextualization in a PLM happen?](https://arxiv.org/abs/2312.06514)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a summary paragraph of the key points from the paper:

This paper investigates where exactly contextualization of word meanings happens within the architecture of pretrained language models like BERT. Rather than just analyzing the final output layers, the authors examine the internal "sub-layers" within each BERT encoder layer, specifically the Self-Attention, Activation, and Output sub-layers. Using contextuality measures like sub-layer similarity, word embedding similarity, and PCA visualized distances, they analyze the vector representations of polysemous words in different sentential contexts across these sub-layers. They find that the Self-Attention and Activation sub-layers exhibit stronger and earlier signs of contextualization compared to the Output sub-layers. The Output sub-layer representations retain more similarity to the word's static embedding, indicating less contextualization. Over the layers, they see contextualization increase and then decrease, with peaks between layers 5-9, similar to previous works. A key finding is that word meaning transformation seems to occur most strongly in the Self-Attention and Activation sub-layers rather than the Output. This localized analysis of contextualization within BERT's encoder sheds new light on how these models capture word meanings.
