# [Where exactly does contextualization in a PLM happen?](https://arxiv.org/abs/2312.06514)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a summary paragraph of the key points from the paper:

This paper investigates where exactly contextualization of word meanings happens within the architecture of pretrained language models like BERT. Rather than just analyzing the final output layers, the authors examine the internal "sub-layers" within each BERT encoder layer, specifically the Self-Attention, Activation, and Output sub-layers. Using contextuality measures like sub-layer similarity, word embedding similarity, and PCA visualized distances, they analyze the vector representations of polysemous words in different sentential contexts across these sub-layers. They find that the Self-Attention and Activation sub-layers exhibit stronger and earlier signs of contextualization compared to the Output sub-layers. The Output sub-layer representations retain more similarity to the word's static embedding, indicating less contextualization. Over the layers, they see contextualization increase and then decrease, with peaks between layers 5-9, similar to previous works. A key finding is that word meaning transformation seems to occur most strongly in the Self-Attention and Activation sub-layers rather than the Output. This localized analysis of contextualization within BERT's encoder sheds new light on how these models capture word meanings.


## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Pre-trained language models (PLMs) like BERT learn contextualized representations of words, encoding word meaning via textual context rather than static word embeddings. 
- Prior work has explored contextualization in PLMs by analyzing the output layer(s), but this paper aims to localize where exactly in the BERT architecture contextualization of polysemous words happens.

Methodology:
- Use the Contextualised Polysemy Word Sense dataset containing polysemous words in sentential contexts. 
- Extract vector representations from 3 sub-layers in each BERT encoder layer: Self-Attention (SA), Activation (Acts), Output. 
- Also extract static BERT word embeddings (layer 0) for comparison.
- Define contextuality measures: 
    - Sub-Layer Similarity (SLSim): Cosine similarity between vector reps of the same polysemous word. Lower = more contextualized.
    - Word Embedding Similarity (WESim): Cosine similarity to static embedding. Lower = more contextualized. 
    - PCA L2 Distance: Euclidean distance between PCA projections. Higher = more contextualized.

Results:
- Output sub-layer is less contextualized than SA and Acts sub-layers.
- Contextualization happens more strongly between layers 5-9. 
- SA and Acts sub-layers exhibit earlier and stronger contextualization signals than Output.
- WESim shows influence of residual connections on Output but not on SA.
- PCA visually confirms stronger contextuality in SA and Acts than Output.

Conclusion:
- Localizes contextualization of polysemous words to happen more strongly in SA and Acts sub-layers compared to Output. 
- Provides evidence that word meaning transformation occurs in these sub-layers rather than just the output layer.

Main Contribution:
- First study to analyze contextuality in BERT sub-layers instead of just output layer(s)
- Evidence that word contextualization happens strongly in SA and Acts sub-layers of BERT encoders
