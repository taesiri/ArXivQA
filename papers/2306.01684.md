# [Harnessing large-language models to generate private synthetic text](https://arxiv.org/abs/2306.01684)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is:How can we harness large language models to generate private synthetic text data?The authors aim to show that large language models can be used to generate high-quality synthetic text data that preserves the privacy of sensitive training data through differential privacy. Specifically, they investigate two main questions:1) Can synthetic text data generated by a differentially private language model match the utility of real private data for training downstream models?2) Can prompt tuning, which tunes only a small portion of the large language model, outperform full model fine-tuning for generating private synthetic text data?The central hypothesis is that with the proper training methodology, large language models can generate synthetic text data that is differentially private yet performs comparably to real private data on downstream tasks. The authors argue that prompt tuning is better suited than full fine-tuning for this task.In summary, the paper aims to demonstrate that high-utility differentially private synthetic text can be generated from large pre-trained language models, with prompt tuning being the preferred training approach over full fine-tuning. Evaluating the utility of this synthetic data on downstream tasks is the main way the authors test their hypothesis.
