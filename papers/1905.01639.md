# Deep Video Inpainting

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper addresses is how to develop an effective deep learning method for general video inpainting. Specifically, the paper investigates whether a feed-forward deep neural network can be adapted to the video inpainting task while maintaining both spatial coherence and temporal consistency in the inpainted results. The key hypotheses are:1) Casting video inpainting as a sequential multi-to-single frame inpainting problem where information from neighboring frames is aggregated can produce better inpainting results than per-frame image inpainting.2) Using recurrent feedback and memory (convolutional LSTM) in the network can help enforce temporal consistency in the inpainted video.3) The proposed network design and training strategy with flow and warping losses can enable a single feed-forward model to generate spatio-temporally coherent video inpainting results, without needing complex optimization.The experiments aim to validate these hypotheses by analyzing the design choices, comparing to optimization-based and per-frame inpainting methods, and demonstrating applications like object removal and video retargeting. Overall, this paper explores how to effectively adapt deep image inpainting techniques to the video domain using ideas like multi-frame aggregation, recurrence, and losses for temporal consistency.
