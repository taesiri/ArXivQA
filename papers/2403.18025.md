# [Improving Pre-trained Language Model Sensitivity via Mask Specific   losses: A case study on Biomedical NER](https://arxiv.org/abs/2403.18025)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Fine-tuning pre-trained language models (PLMs) on target domain data is the common approach for domain adaptation. However, fine-tuning can be insensitive to disparities between the source and target domains, e.g. differences in word meanings.
- For example, words like "pressure" and "attack" are treated lightly in social conversations but are clinically concerning. So fine-tuning needs to appropriately acquire domain knowledge and weigh the importance of domain-specific terms.

Proposed Solution:
- The paper proposes Mask-Specific Language Modeling (MSLM) to efficiently acquire target domain knowledge during fine-tuning. 
- MSLM jointly masks generic words (base level masking) and domain-specific terms (entity level masking).
- It learns mask-specific losses by imposing larger penalties for inaccurately predicting masked domain-specific terms compared to generic words. This increases model sensitivity towards domain terms.

Key Contributions:
- Shows MSLM improves language model sensitivity and exact match detection of domain terms by 3.2 percentage points on average.
- Demonstrates the optimal masking rate depends on the model, dataset and sequence lengths.
- Finds that distributing the masking budget between domain terms and generic words boosts performance.
- Shows the proposed joint ELM-BLM masking strategy outperforms advanced masking strategies like PMI- and span-based masking.

In summary, the paper addresses insensitive fine-tuning and proposes an efficient domain adaptation approach via mask-specific losses to appropriately elevate model awareness of domain-specific terms without hurting downstream performance.
