# [Defining Expertise: Applications to Treatment Effect Estimation](https://arxiv.org/abs/2403.00694)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
In treatment effect estimation, the standard assumption made about a decision-maker's policy is that of overlap - that all treatments have a non-zero probability of being assigned to any individual. However, decision-makers are often experts of their domain who take informed actions based on factors that influence outcomes. This results in confounding biases when estimating treatment effects from observational data. Surprisingly, the expertise of decision-makers is seldom formalized as an assumption or leveraged methodologically beyond just assuming overlap. 

Proposed Solution:
This paper argues that explicitly encoding the intuition that decision-makers tend to be domain experts can act as an inductive bias when estimating treatment effects. The authors formally define two types of expertise - prognostic and predictive. Prognostic expertise refers to actions being based on potential outcomes in general, while predictive expertise refers to actions being specifically based on the treatment effect. The paper shows theoretically and empirically that:

1. High expertise leads to greater covariate shift and poorer overlap. This makes encoding expertise as an inductive bias even more critical as estimating treatment effects becomes more challenging.

2. The type and amount of expertise in a dataset significantly influences the performance of different treatment effect estimation methods. Methods that learn balancing representations perform better under purely prognostic expertise, while methods that learn action-predictive representations perform better under predictive expertise.

3. It is possible to estimate the type and amount of expertise likely present in a dataset. This allows selecting the most suitable method for that dataset in a data-driven manner.

Main Contributions:
- Formal definitions of prognostic and predictive expertise
- Theoretical analysis showing high expertise implies poor overlap 
- Empirical analysis demonstrating the performance impact of expertise
- Proposed method to estimate expertise in data and inform model selection

Overall, this paper introduces expertise as a novel and relevant inductive bias for treatment effect estimation, and provides both theory and experiments supporting its utility. Defining, estimating and encoding expertise can lead to better performing and more reliable treatment effect estimates.


## Summarize the paper in one sentence.

 This paper introduces formal definitions of prognostic and predictive expertise for decision-making policies, and demonstrates empirically that explicitly modeling a policy's expertise type as an inductive bias can improve treatment effect estimation performance.


## What is the main contribution of this paper?

 According to the paper, the main contributions are:

1. Conceptually, it introduces the idea that a decision-maker's policy should not always be considered a nuisance that causes covariate shift, but rather the fact that it often may have high expertise could be leveraged as an inductive bias.

2. Technically, it provides a definition of what it means for a policy to have expertise (in Section 3): Expertise is the extent to which variations in a policy's actions coincide with variations in treatment effects (for predictive expertise) or potential outcomes (for prognostic expertise). 

3. Empirically, it demonstrates that: (i) the type and the amount of expertise present in a dataset significantly influences the performance of different methods for treatment effect estimation, and (ii) it may be possible to classify datasets according to what type of expertise they reflect and thereby identify what methods might be more or less suitable for a given dataset.

In summary, the main contribution is introducing expertise as a potentially useful inductive bias for treatment effect estimation, formally defining expertise, and providing empirical evidence that encoding assumptions about expertise can inform the selection of methods for estimating treatment effects.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and concepts are:

- Treatment effect estimation
- Expertise (prognostic, predictive)
- Inductive bias
- Confounding/covariate shift
- Overlap assumption
- Balancing representations
- Action-predictive representations 
- Performance metrics (PEHE)
- Synthetic/simulation environments

The paper focuses on leveraging the expertise of decision-makers as an inductive bias to inform the selection of methods for estimating heterogeneous treatment effects. It introduces prognostic and predictive expertise, analyzes the performance of different methods under varying expertise scenarios, and shows expertise can be quantified to match datasets with suitable algorithms. Key terms reflect this focus on expertise, treatment effect estimation, evaluating performance, and the methodological setup.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 potential in-depth questions about the method proposed in this paper:

1. The paper argues that expertise should be leveraged as an inductive bias in treatment effect estimation. What are some challenges with formally encoding expertise as an assumption when designing new methods?

2. The definitions of prognostic and predictive expertise rely on using entropy to quantify uncertainty in actions. What are some limitations of using entropy for this purpose? Could any alternative statistical measures have been used instead and what would be the trade-offs?  

3. Propositions 1 and 2 establish theoretically that higher expertise leads to lower overlap. What are the practical implications of this result in being able to actually estimate treatment effects reliably?

4. In the simulation experiments, what factors determine the relative performance improvements of the different methods as expertise increases? Can we characterize formally when each method might be more or less suitable? 

5. The expertise estimation pipeline relies on first using the Action-Predictive method to identify the type of expertise. What assumptions does this approach make and when might it fail to select the right downstream method?

6. How sensitive is the performance of Expertise-Informed to the threshold used for determining whether to use Action-Predictive versus Balancing for treatment effect estimation? 

7. The paper argues expertise can act as an inductive bias. Can we construct some counter-examples or scenarios where making assumptions about expertise could actually hurt performance compared to remaining agnostic?

8. What types of real-world datasets might have predominantly prognostic versus predictive expertise? Can we move beyond healthcare versus education as prototypical examples?

9. What theoretical guarantees can we provide about the sample complexity or generalization performance of methods that leverage expertise versus those that do not?

10. The current expertise definitions only apply to binary actions. How should prognostic and predictive expertise be extended to settings with more than two treatment options?
