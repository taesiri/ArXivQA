# [Hypergraph Node Representation Learning with One-Stage Message Passing](https://arxiv.org/abs/2312.00336)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a high-quality one-paragraph summary of the key points from the paper:

This paper proposes a novel hypergraph representation learning framework called HGraphormer that combines hypergraph structure with Transformer architecture in an innovative way. The key ideas are: (1) introduce a new one-stage message passing paradigm that models both global (semantic correlations between nodes) and local (structure-based connections) interactions, overcoming limitations of prior two-stage approaches; (2) inject hypergraph structure into Transformer attention by combining the semantic attention matrix with the hypergraph Laplacian matrix, enabling effective propagation of both local and global signals; (3) extensive experiments on 5 benchmark datasets demonstrate state-of-the-art performance, outperforming strong baselines by 2.52-6.70% accuracy, validating the advantages of the proposed techniques. Overall, through theoretical analysis and empirical evaluations, the paper makes significant contributions regarding effective integration of Transformer and graphs for representation learning on complex hypergraph-structured data.
