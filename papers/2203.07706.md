# [ActFormer: A GAN-based Transformer towards General Action-Conditioned 3D   Human Motion Generation](https://arxiv.org/abs/2203.07706)

## What is the central research question or hypothesis that this paper addresses?

 The central research question of this paper is how to generate realistic and diverse 3D human motions conditioned on semantic action labels, for both single-person actions and multi-person interactive actions. 

The key hypotheses are:

1) A GAN-based Transformer framework (ActFormer) can achieve superior performance in generating realistic and diverse action-conditioned human motions, by combining the representation power of Transformers and generative modeling capability of GANs.

2) Equipping ActFormer with a Gaussian Process latent prior can help enforce temporal correlations in the generated motions.

3) ActFormer can be extended to generate multi-person interactive motions by modeling temporal correlations and human interactions alternately with Transformer encoders.

4) ActFormer demonstrates strong adaptability to various human motion representations (e.g. joint coordinates, SMPL parameters) and different datasets of single-person and multi-person motions.

In summary, the central goal is to develop a powerful framework ActFormer for general action-conditioned human motion generation, and validate its effectiveness on both single-person and multi-person scenarios. The key hypothesis is that the proposed design can achieve superior generation performance across diverse motion data.


## What is the main contribution of this paper?

 The main contribution of this paper is presenting ActFormer, a GAN-based Transformer framework for general action-conditioned 3D human motion generation. Specifically, the key contributions are:

1. ActFormer leverages the strong representation capability of Transformer and combines it with a Gaussian Process latent prior to generate high-quality single-person human motions for various action categories.

2. ActFormer is extended to handle multi-person interactive motions by alternatingly modeling temporal correlations and human interactions with Transformer encoders. This allows ActFormer to generate synchronized and natural-looking multi-person motions. 

3. A new synthetic dataset called GTA Combat is introduced to provide complex multi-person combat motion data, facilitating research on multi-person motion generation.

4. Extensive experiments show ActFormer achieves state-of-the-art performance on both single-person and multi-person motion generation across various datasets. The results demonstrate ActFormer's effectiveness for general human motion generation.

In summary, the main contribution is proposing ActFormer as a powerful and flexible framework for generating diverse, realistic 3D human motions conditioned on action labels, for both single-person and multi-person scenarios. The introduction of the GTA Combat dataset also facilitates future research on complex multi-person motion modeling.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the key points in the paper:

The paper proposes ActFormer, a GAN-based Transformer framework for general action-conditioned 3D human motion generation, including single-person actions and multi-person interactions; it achieves state-of-the-art results on various benchmarks and motion representations and introduces a new synthetic multi-person combat dataset to facilitate further research.


## How does this paper compare to other research in the same field?

 Here are my thoughts on how this paper compares to other research in the field of action-conditioned 3D human motion generation:

- This paper presents a significant advancement in generating diverse and high-quality motions for both single-person actions and multi-person interactions. Previous works have been limited to simpler datasets with fewer action categories and focused primarily on single-person motion generation.

- The ActFormer framework combines several key innovations to achieve strong performance:
  - Leverages a Gaussian Process latent prior to model inherent temporal correlations in motion sequences. This helps generate more realistic motions compared to autoregressive or independent latent priors used in previous works.
  - Uses a Transformer architecture that excels at modeling long-range dependencies in sequences. This captures the non-local correlations in human motions better than RNN/CNN-based generators used before.
  - Extends naturally to multi-person motions through alternating interaction and temporal modeling modules. Most prior works focus only on single-person motions.

- The results demonstrate superior performance over prior state-of-the-art methods like Action2Motion, ACTOR, and CSGN on large-scale benchmarks with complex data distributions (NTU RGB+D, BABEL). The ActFormer also shows better generalizability across motion representations.

- The paper makes a unique contribution through the new GTA Combat dataset for multi-person interactions. There is a lack of diverse MoCap data for complex multi-person motions, which this synthetic dataset helps mitigate.

Overall, this paper pushes forward the state-of-the-art in conditioned motion generation through technical innovations and evaluation on more challenging and comprehensive benchmarks. The results highlight the potential of the ActFormer framework to move towards a general human motion generation model applicable to a wide range of actions and motion representations. Multi-person motion generation is also an exciting new direction made possible by extending the ActFormer approach.
