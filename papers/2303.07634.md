# [I$^2$-SDF: Intrinsic Indoor Scene Reconstruction and Editing via   Raytracing in Neural SDFs](https://arxiv.org/abs/2303.07634)

## What is the central research question or hypothesis that this paper addresses?

 The central research question that the paper addresses is how to reconstruct and represent indoor scenes from multi-view images to enable photorealistic scene editing and relighting applications. 

Specifically, the paper proposes a new method called "I2-SDF" that can jointly decompose an indoor scene into its underlying shape, material, and lighting components using implicit neural representations. The key ideas and contributions include:

- Using implicit neural signed distance fields (SDFs) to represent the scene geometry, radiance, materials, and lighting in a continuous and differentiable manner suitable for gradient-based optimization. 

- A novel "bubble loss" and adaptive sampling strategy to effectively reconstruct small, thin objects like lamps and chandeliers that are challenging for implicit representations.

- Introducing Monte Carlo raytracing techniques to decompose the radiance field into material and emission fields in a physically based manner, enabling photorealistic relighting and material editing.

- A two-stage training scheme that first reconstructs geometry and radiance, and then optimizes for materials and lighting to avoid ambiguities.

- Experiments on synthetic and real datasets demonstrating state-of-the-art performance in indoor scene reconstruction, novel view synthesis, and editing compared to previous methods.

In summary, the key hypothesis is that by combining implicit neural scene representations with differentiable raytracing and an intrinsic decomposition approach, they can overcome limitations of prior work and enable high-quality reconstruction and editing of complex indoor scenes from multi-view images.


## What is the main contribution of this paper?

 The main contributions of this paper are:

- It proposes I^2-SDF, a holistic neural SDF-based framework to jointly recover the underlying shape, radiance, and material fields from multi-view images of indoor scenes. 

- It introduces a novel bubble loss and error-guided adaptive sampling strategy to effectively reconstruct small objects and fine details in complex indoor scenes, outperforming previous neural rendering methods.

- It is the first to introduce Monte Carlo raytracing in scene-level neural SDFs to enable photorealistic indoor scene relighting and editing by decomposing the radiance field into material and emission fields.

- It provides a high-quality synthetic indoor scene dataset with ground truth camera poses and geometry annotations for benchmarking. 

In summary, the key novelty is using neural SDFs and differentiable raytracing for intrinsic decomposition and reconstruction of indoor scenes, enabling high-quality novel view synthesis and realistic editing applications. The proposed bubble loss and adaptive sampling also improve reconstruction of small objects compared to prior arts.
