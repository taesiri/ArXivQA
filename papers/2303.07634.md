# [I$^2$-SDF: Intrinsic Indoor Scene Reconstruction and Editing via   Raytracing in Neural SDFs](https://arxiv.org/abs/2303.07634)

## What is the central research question or hypothesis that this paper addresses?

 The central research question that the paper addresses is how to reconstruct and represent indoor scenes from multi-view images to enable photorealistic scene editing and relighting applications. 

Specifically, the paper proposes a new method called "I2-SDF" that can jointly decompose an indoor scene into its underlying shape, material, and lighting components using implicit neural representations. The key ideas and contributions include:

- Using implicit neural signed distance fields (SDFs) to represent the scene geometry, radiance, materials, and lighting in a continuous and differentiable manner suitable for gradient-based optimization. 

- A novel "bubble loss" and adaptive sampling strategy to effectively reconstruct small, thin objects like lamps and chandeliers that are challenging for implicit representations.

- Introducing Monte Carlo raytracing techniques to decompose the radiance field into material and emission fields in a physically based manner, enabling photorealistic relighting and material editing.

- A two-stage training scheme that first reconstructs geometry and radiance, and then optimizes for materials and lighting to avoid ambiguities.

- Experiments on synthetic and real datasets demonstrating state-of-the-art performance in indoor scene reconstruction, novel view synthesis, and editing compared to previous methods.

In summary, the key hypothesis is that by combining implicit neural scene representations with differentiable raytracing and an intrinsic decomposition approach, they can overcome limitations of prior work and enable high-quality reconstruction and editing of complex indoor scenes from multi-view images.
