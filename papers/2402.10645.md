# [Can Separators Improve Chain-of-Thought Prompting?](https://arxiv.org/abs/2402.10645)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem: 
- Chain-of-Thought (CoT) prompting is an effective method for improving reasoning capabilities of large language models (LLMs). 
- However, the densely structured prompt exemplars in CoT may cause cognitive overload for LLMs, limiting their reasoning ability.

Proposed Solution:  
- The paper proposes CoT-Sep, a novel method that strategically employs separators at the end of each exemplar in CoT prompting to help LLMs better comprehend and reason through the prompts.

Key Contributions:
- Introduces the use of separators in CoT prompting to reduce cognitive load and improve LLM reasoning. Tests 5 separators: newline, hash, star, HTML line break tags.
- Shows CoT-Sep significantly improves LLM accuracy on complex reasoning datasets over vanilla CoT across multiple models including GPT-3.5-Turbo, GPT-4, LLaMA-2 7B.  
- Discovers proper placement of separators impacts LLM reasoning capabilities. Putting separators after full exemplars is better than within sentences of exemplars.
- Number of separators also matters. Three separators works best. Too many separators can hurt accuracy.
- When uncertain which separator will work best, using a heterogeneous mix of separators still improves over vanilla CoT.
- Overall, demonstrates strategic use of separators in CoT prompting can optimize LLM comprehension and reasoning in an easily adoptable way across models and datasets. Sets new standard in prompt design.

In summary, the key innovation is optimizing CoT prompting through separators to reduce cognitive load for better LLM reasoning, with thorough experiments showing significant gains. The concept is simple but highly effective.
