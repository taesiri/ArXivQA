# [Logits of API-Protected LLMs Leak Proprietary Information](https://arxiv.org/abs/2403.09539)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Many large language models (LLMs) are being commercialized and only made available via APIs, allowing access but hiding architectural details. 
- This may give a false sense of security to LLM providers, who may not realize how much information can still be extracted from the API.

Key Idea:
- Most LLMs have a "softmax bottleneck" where the output vocabulary size is much larger than the embedding dimension. This constrains outputs to a low-dimensional subspace.
- By collecting outputs through the API, we can find a basis for this subspace (the "image" of the LLM). The image enables extracting non-public info.

Methods & Contributions:
- Propose efficient algorithms to extract the full output distributions from restricted APIs by exploiting the low rank structure. This is up to 100x faster.
- Show how to discover the embedding dimension of API-protected LLMs from outputs. Use this to estimate GPT-3.5 Turbo's dimension.
- The LLM's image acts as a unique "signature" that can identify which LLM produced an output. This enables detecting updates.
- Additional applications like finding tokenization issues, reconstructing softmax matrices, improving inversion attacks, and better decoding.
- Analyze potential mitigations like removing logits/bias access from the API. Argue the attacks may be more feature than bug.

In summary, the paper shows how the inherent low-rank structure of LLMs allows extracting non-public info from restricted APIs. This challenges assumptions about privacy but also enables accountability. A range of novel applications are demonstrated and analyzed.


## Summarize the paper in one sentence.

 The paper shows how the low-rank output layer in large language models enables extracting non-public information and capabilities from API-protected models using only a small number of affordable API queries.


## What is the main contribution of this paper?

 The main contribution of this paper is showing how the low-rank output layer common to most LLM architectures leads to model outputs being constrained to a low-dimensional subspace (called the "image") of the full output space. By collecting a small number of outputs, the authors show how one can obtain a basis for this subspace and use it to efficiently extract non-public information about API-protected LLMs, including:

- Efficiently obtaining full vocabulary outputs
- Discovering the embedding size and guessing parameter counts
- Detecting granular model updates like LoRA fine-tuning
- Identifying which LLM produced a given output 
- Approximately reconstructing the softmax matrix
- Recovering "hidden prompts" used with the LLM

The authors argue that rather than being an exploit that needs patched, this capability can provide transparency and accountability benefits between LLM providers and users. However, providers may still wish to be aware of the information leaked from the common softmax bottleneck when providing API access.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and concepts include:

- Language models (LLMs)
- API-protected models
- Model image/signature
- Softmax bottleneck
- Embedding size
- Logit bias
- Model updates
- Parameter estimation
- Model inversion
- Accountability and transparency

The main focus of the paper is using the low-dimensional "image" or output space of language models, which results from the softmax bottleneck, to glean non-public information about proprietary API-only models. This includes estimating parameters like embedding size, detecting model updates, approximating the softmax matrix, and recovering hidden prompts. The authors also discuss applications for accountability and transparency between API providers and users.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in the paper:

1. The paper proposes using the singular values of the logit/probability matrix to estimate the embedding dimension $d$. What are some potential issues with numerical precision or conditioning that might make this estimate inaccurate? How could the method be made more robust?

2. Section 3 describes discovering the embedding size by collecting outputs until linear dependence kicks in. What statistical tests could be used to rigorously quantify when linear dependence occurs, rather than just visually inspecting singular values? 

3. The proof for the fast logprobs algorithm relies on the assumption that logit differences are bounded. What could be done to adapt the method if this assumption fails to hold?

4. How much faster is the proposed $O(d)$ algorithm for getting full outputs, compared to prior work, for a very large model like GPT-3? What are the practical limitations on how much it could speed things up?

5. Could the proposed method for detecting LoRA updates be fooled? For example, could other types of updates exhibit similar low-rank patterns in the image change?

6. The similarity between model images is used to identify which LLM produced an output. What are limitations of this approach? When would it fail to disambiguate models?  

7. What types of defenses not discussed in the paper could be used to prevent obtaining the LLM image? For example, could different sampling methods like nucleus sampling mitigate this?

8. How accurately could the softmax matrix be reconstructed using the proposed method of constraining row norms? What ambiguities would remain compared to the true matrix?

9. How does the proposed modified conditioning method for prompt inversion compare empirically to prior work? What are limitations of still needing to project down to the inversion model dimension?

10. Could the proposed basis-aware decoding method be implemented without needing the full LLM image basis, but instead using an approximate basis fitted from outputs? How would performance compare?
