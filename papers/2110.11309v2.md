# [Fast Model Editing at Scale](https://arxiv.org/abs/2110.11309v2)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question seems to be: 

How can we enable fast, targeted editing of large pre-trained models using only a single input-output edit example, without significantly affecting the model's performance on other unrelated inputs?

The key ideas and contributions appear to be:

- Proposing a method called MEND (Model Editor Networks with Gradient Decomposition) that learns to transform the standard fine-tuning gradient for a given edit example into a more targeted parameter update. 

- Leveraging the low-rank structure of gradients w.r.t. fully connected layers to make this gradient transformation computationally tractable, even for models with billions of parameters.

- Showing that MEND can be trained efficiently on a single GPU and then used to rapidly apply edits to large pre-trained models like BERT, GPT, T5 etc. without modifying the original model architecture.

- Demonstrating through experiments that MEND produces local, reliable edits that generalize to related inputs, while minimizing negative impact on unrelated inputs. 

- Comparing MEND to other editing approaches like fine-tuning and prior methods like ENN and KE, and showing it uniquely scales to editing 10B+ parameter models with better edit success.

In summary, the key hypothesis seems to be that exploiting gradient structure will enable efficient and effective editing of very large models using only single example edits. The paper aims to propose and evaluate a method along these lines.


## What is the main contribution of this paper?

 The main contribution of this paper is proposing a model editing algorithm called MEND (Model Editor Networks with Gradient Decomposition). MEND learns to transform the fine-tuning gradient for a model into a more targeted update that can successfully edit the model's behavior using only a single input-output pair. It does this by factorizing the gradient into a low-rank form, which makes learning to transform high-dimensional gradients tractable. The key benefits highlighted are:

- MEND can successfully edit very large pre-trained models with over 10 billion parameters, which prior editing algorithms struggled with. 

- It is efficient to train, requiring less than a day on a single GPU even for huge models.

- Once trained, MEND enables rapid application of edits to a pre-trained model using just a single example of the desired change in behavior.

- It is a general approach not tied to any specific model architecture, and can edit transformer models like BERT/GPT as well as sequence-to-sequence models.

- Ablations show design choices like parameter sharing and identity initialization are important for effective and scalable editing.

In summary, the main contribution is proposing MEND, a scalable and general algorithm for editing very large pre-trained models using only single input-output pairs to correct model behavior.
