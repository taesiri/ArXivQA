# [Improving Commonsense in Vision-Language Models via Knowledge Graph   Riddles](https://arxiv.org/abs/2211.16504)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question being addressed is: How can we improve the commonsense ability of recent popular vision-language (VL) models?

The authors observe that despite the great success of VL models like CLIP, they still lack commonsense knowledge and reasoning ability. For example, when shown an image of a lemon and given the text "It tastes sour", VL models fail to make the connection that the text is referring to the lemon. 

To address this limitation, the authors propose a new training strategy called "Data Augmentation with kNowledge graph linearization for CommonsensE capability" (DANCE). The key idea is to leverage knowledge graphs like ConceptNet to generate new training examples that inject commonsense knowledge into existing VL datasets. This helps teach the models about relationships and connections between entities during training.

Additionally, the authors create a new diagnostic benchmark to evaluate commonsense ability in a retrieval task setting, allowing comparison across different VL model architectures.

Through experiments, they show DANCE significantly improves performance on commonsense tasks while maintaining accuracy on normal retrieval benchmarks. This demonstrates it enhances commonsense capabilities without sacrificing general visual-linguistic understanding.

In summary, the central hypothesis is that augmenting training data with commonsense knowledge graphs can improve the commonsense reasoning abilities of VL models. The DANCE strategy and diagnostic benchmark are proposed to test this hypothesis.
