# [The Curse of Recursion: Training on Generated Data Makes Models Forget](https://arxiv.org/abs/2305.17493)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central research question this paper aims to address is:What happens to large language models (LLMs) like GPT when they are trained on increasing amounts of text generated by previous versions of the model itself, as opposed to text written by humans?The key hypothesis is that using model-generated text for training causes a degenerative effect called "model collapse", where the models start to forget or misrepresent the true underlying distribution of human language. Specifically, the paper investigates what happens when text produced by an early version of GPT forms the training data for later versions of the model. The hypothesis is that over successive generations, the models will start losing information about the original human language distribution, first forgetting improbable events and then even converging to a point estimate with little variance. The paper aims to demonstrate the existence of this "model collapse" phenomenon across different model types like Gaussian Mixture Models, Variational Autoencoders, and Large Language Models. The goal is to show that continued access to genuine human-written text is crucial to avoid this degenerative effect when training future generations of large language models.


## What is the main contribution of this paper?

Based on my reading, the main contributions of this paper are:1. It demonstrates a degenerative process in machine learning models called "model collapse", where models trained on data generated by previous model generations start to diverge from the true underlying data distribution. 2. It provides examples showing model collapse occurring in various model types and datasets, including Gaussian Mixture Models, Variational Autoencoders, and Large Language Models.3. It analytically derives lower bounds on the risk/distance of later model generations from the original data distribution, arguing that without superlinear increases in sampling, the distance will grow unbounded. 4. It highlights the importance of continued access to genuine human-generated data to avoid model collapse, as model-produced data can recursively pollute training sets over generations.In summary, the paper identifies and analyzes an important phenomenon termed "model collapse" that can arise when models are trained on synthetic data from previous models. It demonstrates this across models and datasets, provides theoretical analysis, and discusses implications for needing ongoing access to real human-generated data.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence TL;DR summary of the paper:The paper demonstrates that when generative machine learning models are trained on data produced by previous generations of models, over time they lose information about the true underlying data distribution and converge to a distorted representation, an effect referred to as model collapse.


## How does this paper compare to other research in the same field?

Here are a few thoughts on how this paper compares to other research in the field:- This paper focuses specifically on the phenomenon of "model collapse", which the authors argue is a universal issue affecting generations of learned generative models when trained on their own outputs. Much prior work has studied related issues like catastrophic forgetting in continual learning, but the setting here seems quite different and focused on the compounding effects over generations of models.- The analysis of model collapse through theoretical models (discrete distributions and Gaussian models) provides intuition about how statistical and functional approximation errors can lead to divergence from the original data distribution over iterations. This theoretical angle seems novel compared to prior empirical studies of issues like mode collapse in GANs. - Demonstrating model collapse across various model families (GMMs, VAEs, LLMs) makes a strong case that this is a general issue, not just something affecting one model type. The LLM experiments in particular stand out as more realistic and practical than just toy examples.- Overall the framing of how access to genuine human-generated data will be increasingly valuable to avoid model collapse implications seems an important insight. The emphasis on needing the true distribution, especially the tails, connects well to related ideas about distribution shift and out-of-distribution generalization challenges.- Situating model collapse in relation to other threads like data poisoning, catastrophic forgetting, etc provides useful context, though the dynamics studied seem meaningfully different than prior work.In summary, the paper makes a compelling case this is a distinct and important phenomenon, with thorough investigation across models and both theory and experiments. The implications seem far-reaching in terms of strategies needed to sustain reliable LLM training over generations.


## What future research directions do the authors suggest?

Based on my reading of the paper, some of the main future research directions suggested by the authors include:- Further exploring the phenomenon of model collapse/model dementia in other types of generative models besides the ones examined in the paper (LLMs, VAEs, GMMs). The authors demonstrate it is a universal issue but there is more work to be done looking at other model families.- Better understanding the progression of model collapse over generations of models. The authors show some initial results but more research could quantify the rate of degradation and how different factors affect it.- Developing methods to detect or mitigate model collapse. The paper demonstrates the problem exists but does not propose specific solutions. More research could aim to detect early signs of model collapse or develop training procedures to avoid it.- Examining model collapse in other modalities like images, audio, etc. beyond just text. The issue likely applies there too but focused study is needed. - Considering the societal impacts of model collapse and how it may affect the reliability and fairness of models over time if left uncontrolled.- Studying whether access to the original human-generated training data can help avoid model collapse, as the authors suggest. More analysis is needed around data requirements.- Exploring the connections between model collapse and related issues like catastrophic forgetting, distribution shift, etc. There seem to be useful links to be made to existing research.In general, the authors lay out model collapse as an important open problem for generative models. They empirically demonstrate its existence but further research is needed to really understand and address it.


## Summarize the paper in one paragraph.

Here is a one paragraph summary of the paper:The paper investigates what happens when large language models (LLMs) like GPT are trained on data that includes text generated by previous versions of the model, as opposed to only human-written text. The authors find that this causes a degenerative effect they call "model collapse", where over successive generations the models start to forget improbable events from the original data distribution. They demonstrate model collapse in Gaussian mixture models, variational autoencoders, and LLMs, showing how statistical sampling errors compound over generations to make models converge to a simplified distribution. The authors argue model collapse has broad implications, as access to genuine human-written data will become increasingly valuable to sustain learning in LLMs deployed at scale. They recommend preserving access to original human-curated data sources to avoid models misperceiving reality based on errors introduced by their generative ancestors.


## Summarize the paper in two paragraphs.

Here is a two paragraph summary of the paper:The paper discusses how training machine learning models on data generated by previous versions of the model leads to a degenerative process called model collapse. Model collapse refers to how over successive generations, models start to forget aspects of the true underlying data distribution. The paper demonstrates this through theoretical analysis and experiments on Gaussian Mixture Models, Variational Autoencoders, and Large Language Models. The authors show that model collapse arises from two key sources of error that compound over generations - statistical sampling errors and functional approximation errors. As models are trained on finite imperfect samples of data produced by earlier generations, the errors accumulate and the models start to perceive a distorted version of reality. The paper argues that avoiding model collapse requires continued access to genuine human-generated data, rather than relying solely on artificial data recursively produced by machine learning models. The authors warn that large language models trained only on web content may lose track of true human language as they generate more content that gets incorporated into training data.


## Summarize the main method used in the paper in one paragraph.

Here is a one paragraph summary of the main method used in the paper:The paper proposes a new method to detect a phenomenon they term "model collapse" in generations of learned generative models. They hypothesize that when generative models like large language models are recursively trained on data produced by previous model generations, the models will gradually "forget" the true underlying data distribution and misperceive reality. To demonstrate this, they use simple theoretical models like a Gaussian distribution and Markov chains to show analytically how statistical sampling errors inevitably compound over generations of models. They then empirically evaluate more complex models like Gaussian mixture models, variational autoencoders, and large language models, showing how they demonstrate early signs of model collapse when trained iteratively on synthetic data from previous model generations. The key finding is that access to genuine human-generated data is essential to avoid the compounding errors of model collapse over many generations.
