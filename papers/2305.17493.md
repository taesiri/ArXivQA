# [The Curse of Recursion: Training on Generated Data Makes Models Forget](https://arxiv.org/abs/2305.17493)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central research question this paper aims to address is:What happens to large language models (LLMs) like GPT when they are trained on increasing amounts of text generated by previous versions of the model itself, as opposed to text written by humans?The key hypothesis is that using model-generated text for training causes a degenerative effect called "model collapse", where the models start to forget or misrepresent the true underlying distribution of human language. Specifically, the paper investigates what happens when text produced by an early version of GPT forms the training data for later versions of the model. The hypothesis is that over successive generations, the models will start losing information about the original human language distribution, first forgetting improbable events and then even converging to a point estimate with little variance. The paper aims to demonstrate the existence of this "model collapse" phenomenon across different model types like Gaussian Mixture Models, Variational Autoencoders, and Large Language Models. The goal is to show that continued access to genuine human-written text is crucial to avoid this degenerative effect when training future generations of large language models.


## What is the main contribution of this paper?

Based on my reading, the main contributions of this paper are:1. It demonstrates a degenerative process in machine learning models called "model collapse", where models trained on data generated by previous model generations start to diverge from the true underlying data distribution. 2. It provides examples showing model collapse occurring in various model types and datasets, including Gaussian Mixture Models, Variational Autoencoders, and Large Language Models.3. It analytically derives lower bounds on the risk/distance of later model generations from the original data distribution, arguing that without superlinear increases in sampling, the distance will grow unbounded. 4. It highlights the importance of continued access to genuine human-generated data to avoid model collapse, as model-produced data can recursively pollute training sets over generations.In summary, the paper identifies and analyzes an important phenomenon termed "model collapse" that can arise when models are trained on synthetic data from previous models. It demonstrates this across models and datasets, provides theoretical analysis, and discusses implications for needing ongoing access to real human-generated data.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence TL;DR summary of the paper:The paper demonstrates that when generative machine learning models are trained on data produced by previous generations of models, over time they lose information about the true underlying data distribution and converge to a distorted representation, an effect referred to as model collapse.
