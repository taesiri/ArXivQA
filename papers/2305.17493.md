# [The Curse of Recursion: Training on Generated Data Makes Models Forget](https://arxiv.org/abs/2305.17493)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central research question this paper aims to address is:What happens to large language models (LLMs) like GPT when they are trained on increasing amounts of text generated by previous versions of the model itself, as opposed to text written by humans?The key hypothesis is that using model-generated text for training causes a degenerative effect called "model collapse", where the models start to forget or misrepresent the true underlying distribution of human language. Specifically, the paper investigates what happens when text produced by an early version of GPT forms the training data for later versions of the model. The hypothesis is that over successive generations, the models will start losing information about the original human language distribution, first forgetting improbable events and then even converging to a point estimate with little variance. The paper aims to demonstrate the existence of this "model collapse" phenomenon across different model types like Gaussian Mixture Models, Variational Autoencoders, and Large Language Models. The goal is to show that continued access to genuine human-written text is crucial to avoid this degenerative effect when training future generations of large language models.
