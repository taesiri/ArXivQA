# Chameleon: Plug-and-Play Compositional Reasoning with Large Language
  Models

## What is the central research question or hypothesis that this paper addresses?

Based on my review, the central research question this paper seeks to address is: How can we augment large language models (LLMs) with the capability to automatically compose external tools in a flexible, plug-and-play manner in order to tackle complex reasoning tasks? The key hypothesis appears to be: By prompting an LLM-based natural language planner with descriptions and examples of various tools, the LLM will be able to generate programs that select and sequence appropriate tools to effectively solve complex reasoning problems across different domains and modalities.Specifically, the paper introduces a framework called Chameleon that integrates tools like additional LLM models, vision models, search engines, Python functions, etc. into a modular inventory. An LLM then acts as a natural language planner to generate tool sequences/programs based on the task instructions and examples. The central premise is that this approach will enhance the reasoning and problem-solving abilities of LLMs like GPT-3 and GPT-4 in a generalizable way.The paper tests this hypothesis by evaluating Chameleon on two diverse reasoning tasks - ScienceQA (multimodal question answering) and TabMWP (mathematical reasoning with tables). The results demonstrate that Chameleon with GPT-4 significantly outperforms prior published approaches on both benchmarks, supporting the potential of their method for augmenting LLMs via flexible tool composition.In summary, the key research question is how to effectively augment LLMs by prompting them to integrate tools in a plug-and-play and automatic way, with the hypothesis that this will enhance complex reasoning across tasks. The paper introduces Chameleon to test this hypothesis, with promising results on two benchmarks.
