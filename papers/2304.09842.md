# Chameleon: Plug-and-Play Compositional Reasoning with Large Language   Models

## What is the central research question or hypothesis that this paper addresses?

Based on my review, the central research question this paper seeks to address is: How can we augment large language models (LLMs) with the capability to automatically compose external tools in a flexible, plug-and-play manner in order to tackle complex reasoning tasks? The key hypothesis appears to be: By prompting an LLM-based natural language planner with descriptions and examples of various tools, the LLM will be able to generate programs that select and sequence appropriate tools to effectively solve complex reasoning problems across different domains and modalities.Specifically, the paper introduces a framework called Chameleon that integrates tools like additional LLM models, vision models, search engines, Python functions, etc. into a modular inventory. An LLM then acts as a natural language planner to generate tool sequences/programs based on the task instructions and examples. The central premise is that this approach will enhance the reasoning and problem-solving abilities of LLMs like GPT-3 and GPT-4 in a generalizable way.The paper tests this hypothesis by evaluating Chameleon on two diverse reasoning tasks - ScienceQA (multimodal question answering) and TabMWP (mathematical reasoning with tables). The results demonstrate that Chameleon with GPT-4 significantly outperforms prior published approaches on both benchmarks, supporting the potential of their method for augmenting LLMs via flexible tool composition.In summary, the key research question is how to effectively augment LLMs by prompting them to integrate tools in a plug-and-play and automatic way, with the hypothesis that this will enhance complex reasoning across tasks. The paper introduces Chameleon to test this hypothesis, with promising results on two benchmarks.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contribution appears to be proposing a new framework called \model~that augments large language models (LLMs) with the ability to perform compositional reasoning by integrating external tools in a plug-and-play manner. Specifically, the key aspects of the contribution are:- Developing a plug-and-play compositional reasoning framework called \model~that addresses limitations of current LLMs like the inability to access real-time information or perform mathematical reasoning. - \model~synthesizes programs to compose various tools including other LLMs, vision models, web search, Python functions, and rule-based modules. This allows it to tackle a diverse range of reasoning tasks.- At the core of \model~is an LLM-based natural language planner that determines the appropriate sequence of tools to execute to generate the final response to a given query.- Showcasing the effectiveness of \model~by significantly improving performance over prior methods on two challenging multi-modal QA datasets - ScienceQA and TabMWP. For example, \model~with GPT-4 achieves 86.54\% accuracy on ScienceQA, surpassing prior best few-shot model by 11.37\%.- Demonstrating the adaptability of \model~by composing different sets of tools tailored to the ScienceQA and TabMWP tasks, such as using knowledge retrieval and search engines for ScienceQA vs using Python programs for TabMWP.In summary, the main contribution is proposing the \model~framework that augments LLMs with compositional reasoning capabilities by integrating diverse tools in a flexible, plug-and-play manner and showing its effectiveness on complex reasoning tasks. The framework addresses limitations of current LLMs and demonstrates how they can be enhanced via compositional tool use.
