# [Treatment of Epistemic Uncertainty in Conjunction Analysis with   Dempster-Shafer Theory](https://arxiv.org/abs/2402.00060)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- There is inherent uncertainty in the data used for collision probability (PoC) calculation between two space objects, known as conjunction data messages (CDMs). This uncertainty stems from lack of knowledge about the measurement, propagation and distribution models used to generate the CDMs.  
- Current approaches either try to improve realism of covariance in CDMs or increase covariance to account for uncertainty. They do not directly address epistemic uncertainty or model evolution of uncertainty over a sequence of CDMs.

Proposed Solution:
- Use Dempster-Shafer theory (DSt) to model epistemic uncertainty in a sequence of CDMs based on the assumption that CDM parameters are drawn from unknown probability distributions.  
- Use Dvoretzky–Kiefer–Wolfowitz (DKW) inequality to construct bounds on the unknown distribution based on CDM time series and build DSt structures capturing uncertainty in CDMs.
- Compute Belief and Plausibility of collision probability exceeding a threshold to quantify confidence and ambiguity in calculated PoC.  
- Propose an evidence-based classification system with six risk classes using Plausibility, Belief-Plausibility difference and time to closest approach.

Contributions:
- Novel way to model evolution of epistemic uncertainty over a sequence of CDMs based on DKW inequality and DSt theory.
- Quantification of confidence and ambiguity in collision probability calculation.
- Evidence-based conjunction risk classification system tied to operational decision making.  
- Analysis using real CDM data shows the approach is more conservative than ESA's probabilistic approach and provides more nuanced uncertainty information compared to CNES's scaled probability approach.

In summary, the paper introduces an innovative way to address epistemic uncertainty in collision assessment using belief functions, with promising results on real spacecraft data. The evidence-based classification system in particular allows more informed operational decision making.


## Summarize the paper in one sentence.

 Here is a one-sentence summary of the paper:

The paper presents a methodology based on Dempster-Shafer theory to model the epistemic uncertainty in sequences of conjunction data messages and make robust decisions about collision risk assessment and collision avoidance for spacecraft conjunction events.


## What is the main contribution of this paper?

 The main contribution of this paper is proposing a methodology to quantify and model the epistemic uncertainty in sequences of Conjunction Data Messages (CDMs). Specifically:

- It uses the Dvoretzky–Kiefer–Wolfowitz (DKW) inequality to construct bounds on the unknown distribution that CDM data is sampled from. This allows building a Dempster-Shafer structure to represent the uncertainty. 

- It introduces two metrics - Plausibility (Pl) and Belief (Bel) - computed from the Dempster-Shafer structure to quantify the uncertainty in the Probability of Collision (PoC). The difference between Pl and Bel measures the epistemic uncertainty.

- It proposes an evidence-based classification system using Pl, Bel and time to closest approach to make robust decisions on collision avoidance maneuvers. The classes differentiate between certain high-risk, certain low-risk and uncertain events.

- It tests the methodology on real conjunction events faced by ESA and CNES operators and shows it can provide consistent decisions while better quantifying uncertainty.

In summary, the key contribution is a new way to model uncertainty in CDMs using evidence theory and make robust decisions by explicitly accounting for the uncertainty.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and keywords associated with it are:

- Conjunction data messages (CDMs)
- Epistemic uncertainty
- Dempster-Shafer theory (DSt) 
- Belief and plausibility
- Probability of collision (PoC)
- Scaled probability of collision (sPoC)
- Collision avoidance maneuver (CAM)
- Conjunction assessment risk analysis (CARA)
- Dvoretzky–Kiefer–Wolfowitz (DKW) inequality
- Focal elements
- Probability boxes (p-boxes)
- Classification system for conjunction risk

The paper focuses on modeling and quantifying the epistemic uncertainty in sequences of CDMs for space conjunction events. It leverages concepts from DSt like belief and plausibility along with the DKW inequality and p-boxes. A classification system is also proposed to make robust decisions on collision risk and the need for CAMs based on the analysis of uncertainty. So these are some of the central ideas and terms highlighted in the paper.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in this paper:

1. The paper assumes that the mean and covariance in each CDM are affected by epistemic uncertainty. What are some potential sources of this epistemic uncertainty and how might they impact the computed probability of collision (PoC)?

2. The Dvoretzky–Kiefer–Wolfowitz (DKW) inequality is used to construct robust bounds on the family of unknown distributions that generate the CDMs. Explain the process of using the DKW inequality and empirical CDF to construct the probability boxes (p-boxes). 

3. The paper builds a Dempster-Shafer structure to model the epistemic uncertainty in the CDMs. Walk through the full process of constructing the focal elements and assigning basic probability assignments based on the p-boxes.  

4. The paper proposes a classification system with six different classes to categorize the risk level of conjunction events. Explain each of the six classes in detail, including how they are defined based on the plausibility, belief, time to closest approach, and area between plausibility and belief curves.

5. The conjunction events in the paper are classified using thresholds on plausibility ($Pl_0$), area between curves ($A_{Pl,Bel}$), and time to closest approach ($T_1$, $T_2$). Discuss how these thresholds are set and what impact changing the thresholds may have on the performance of the classification system.  

6. The CDMs are weighted over time using an exponential fitting law in the paper. Explain the motivation behind using a weighting scheme and discuss how the choice of weighting impacts the construction of p-boxes and focal elements.  

7. Compare and contrast the evidence-based approach proposed in the paper with the approaches currently used operationally by ESOC and CNES for conjunction risk assessment. What are the key differences in methodology and outcomes?

8. In the results, the paper finds more "uncertain" cases than ESOC's escalated events. Speculate on some potential reasons why the evidence-based method detects more uncertainty and discuss how an operator might interpret these uncertain cases.  

9. What assumptions does the methodology make about independence of variables when constructing focal elements? How might dependencies impact the outcomes and is there room for improvement here?

10. The paper focuses solely on modeling epistemic uncertainty present in the CDMs. What are other potential sources of uncertainty in conjunction analysis and how might the methodology be extended to handle other uncertainties?
