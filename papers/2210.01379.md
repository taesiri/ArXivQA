# [Extraneousness-Aware Imitation Learning](https://arxiv.org/abs/2210.01379)

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper addresses is how an agent can learn visuomotor policies from video demonstrations that contain extraneous, task-irrelevant subsequences. The key hypothesis is that by learning action-conditioned embeddings that capture temporal correspondences across demonstrations, and using these embeddings to align the task-relevant parts, the agent will be able to effectively imitate from the useful portions of noisy visual demonstrations.Specifically, the paper proposes an approach called Extraneousness-Aware Imitation Learning (EIL) to tackle this problem. The core ideas are:1) Learn an encoder through temporal cycle-consistency loss to obtain action-conditioned embeddings that capture progress similarities across demonstrations. 2) Use an unsupervised voting-based algorithm to align and retrieve the task-relevant observations across demonstrations while excluding extraneous ones.3) Perform standard visual imitation learning on the filtered state-action pairs from step 2.The central hypothesis is that by learning to identify and exclude extraneous, irrelevant segments in this self-supervised manner, the agent can successfully imitate the useful parts of noisy demonstrations and acquire the desired skills. The experiments aim to validate this hypothesis.
