# [Towards Equipping Transformer with the Ability of Systematic   Compositionality](https://arxiv.org/abs/2312.07280)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Recent studies show that language models struggle with generalizing through composition, even large pretrained models. This highlights issues with their systematic compositionality - the ability to understand composed unseen examples from seen primitives.
- Systematic compositionality is considered essential for language productivity and human cognition, so improving it in models is important.

Proposed Solution:  
- The authors propose a Compositionality-Aware Transformer (CAT) which has two modules added to the standard Transformer encoder:
   1) Multi-Primitive Composition module: Decomposes word representations into multiple discrete primitive vectors that are composed into a new representation.
   2) Representation Enhancement module: Integrates the standard contextual word representation and new compositional representation to enhance the overall representation.

- They also propose two new pretraining tasks: 
   1) Guided Decomposition: Uses sememe knowledge base to supervise decomposition into primitives.  
   2) Semantics Composition: Ensures composed representations retain key semantics.

- These modules and pretraining tasks aim to improve the model's systematic compositionality.

Contributions:
- First model aiming to directly improve systematic compositionality of Transformers.  
- Novel modules and pretraining tasks to achieve this.
- Empirical evaluation shows CAT outperforms baselines on compositionality-aware tasks, with minimal impact on standardized language tasks.
- Shows superior performance in identifying semantic changes in compositionality and improvements in compositional generalization.
- Proof-of-concept for enhancing compositionality capabilities of large language models.
