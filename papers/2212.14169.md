# [Discriminator-Cooperated Feature Map Distillation for GAN Compression](https://arxiv.org/abs/2212.14169)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be: How can knowledge distillation be improved to enable better compression and performance of GAN models?Specifically, the paper investigates incorporating the discriminator more effectively into the knowledge distillation process for GAN compression. The key ideas proposed are:1) Using the teacher discriminator as a learned transformation to match intermediate feature maps from the student and teacher generators. This aims to encourage perceptual similarity rather than pixel-level matching. 2) Collaborative adversarial training where the teacher discriminator co-trains with the student generator. This helps mitigate mode collapse during GAN compression.3) Combining these two ideas - discriminator-cooperated distillation (DCD) and collaborative adversarial training - to develop a GAN compression framework that outperforms prior work.The central hypothesis appears to be that utilizing the discriminator more effectively in distillation can lead to better performing and more compressed GANs compared to prior distillation techniques like pixel-level feature matching. The paper presents experiments on compressing CycleGAN and Pix2Pix models to validate this hypothesis.


## What is the main contribution of this paper?

The main contributions of this paper appear to be:1. Proposing a novel discriminator-cooperated distillation (DCD) method to transfer feature map knowledge from a teacher GAN generator to a student generator. This aims to optimize the compressed student generator to generate perceptually better images. 2. Using the teacher discriminator as a "transformation" to pursue better visual perception in the student generator's outputs, rather than just pixel-to-pixel matching between student and teacher feature maps.3. Introducing a collaborative adversarial training paradigm that allows the teacher discriminator to co-train with the student generator, avoiding issues like mode collapse.4. Demonstrating significant performance improvements and complexity reduction on benchmark datasets like horse2zebra, summer2winter, and edges2shoes using compressed CycleGAN and Pix2Pix models.In summary, the key novelty seems to be using the teacher discriminator in a novel way during distillation to improve the student generator's outputs, as well as the collaborative adversarial training approach. Both aim to boost the performance of a lightweight compressed GAN generator.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper proposes a novel discriminator-cooperated distillation method to transfer feature map knowledge from a teacher GAN to a compressed student GAN for better image generation, using the teacher discriminator as a transformation and collaborative adversarial training to optimize the student generator.
