# [CLIP as RNN: Segment Countless Visual Concepts without Training Endeavor](https://arxiv.org/abs/2312.07661)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Existing open-vocabulary image segmentation methods require fine-tuning on mask annotations or image-text datasets, which limits the number of categories they can handle.
- Methods without fine-tuning struggle to generate accurate masks, especially when text queries refer to non-existing objects.

Proposed Solution: 
- A novel recurrent framework called "CLIP as Recurrent Neural Network" (CaR) that progressively refines masks and filters out irrelevant texts without any training.

- The recurrent unit is a two-stage segmenter built on frozen CLIP weights:
  1) Mask proposal generator using CAM. 
  2) Mask classifier to assess alignment of each mask-text pair.

- Operates recurrently to align visual and textual spaces, removing low-confidence text queries in each pass.

- Shared weights make it analogous to a RNN.

Main Contributions:

1) A training-free open-vocabulary segmentation method that preserves CLIP's vocabulary capacity.

2) Significantly outperforms prior arts on zero-shot semantic/referring segmentation:
   - 28.8 mIoU higher on Pascal VOC
   - 16.0 mIoU higher on COCO Object 
   - 6.9 mIoU higher on Pascal Context

3) Surpasses state-of-the-arts fine-tuned on millions of data samples:
   - 12.6 mIoU higher on Pascal VOC
   - 4.6 mIoU higher on COCO Object

4) Qualitative results showcase success across diverse concepts like landmarks, brands, fictional characters etc.

In summary, this paper introduces an effective training-free technique for open-vocabulary segmentation that retains the vocabulary capacity of CLIP through a recurrent refinement approach. Both quantitative and qualitative results demonstrate the superiority of this method.
