# [Masked Audio Text Encoders are Effective Multi-Modal Rescorers](https://arxiv.org/abs/2305.07677)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is: How can multi-modal masked language models effectively integrate acoustic information to improve automatic speech recognition systems, especially for generalizing to unseen domains?Specifically, the key hypotheses tested in this work are:- Incorporating acoustic representations as additional input can help masked language models generalize better and be more robust for ASR rescoring, compared to using only text input. - Applying contrastive learning between audio and text modalities can further improve the multi-modal masked language model by better aligning the representations across modalities.- The proposed multi-modal masked language model can rapidly adapt to new domains in few-shot and zero-shot settings by leveraging both text and acoustic information, outperforming text-only models.The paper introduces a multi-modal masked language model called MATE that incorporates both text and acoustic features from speech. It investigates whether this approach can improve ASR performance, especially on out-of-domain test sets where the model has not seen examples from those domains during training. The central hypothesis is that the additional acoustic input will make the model more robust and help it generalize better to new domains in low resource settings. The contrastive learning component further helps align the representations across modalities. The empirical results generally validate these hypotheses, showing improved performance over text-only models in both in-domain and out-of-domain evaluations.
