# [GIST: Improving Parameter Efficient Fine Tuning via Knowledge   Interaction](https://arxiv.org/abs/2312.07255)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a high-quality paragraph summarizing the key points of the paper:

This paper proposes a novel fine-tuning framework called GIST to enhance the performance of parameter-efficient fine-tuning (PEFT) methods for pre-trained models. GIST introduces a trainable "Gist token" that serves as an aggregator to learn downstream task-specific knowledge. It also utilizes a bidirectional Kullback-Leibler divergence loss for explicit interaction between the task-agnostic knowledge from pre-training and task-specific knowledge for fine-tuning. This knowledge interaction better adapts the model to downstream tasks. Experiments across image classification, few-shot learning, and language understanding datasets demonstrate that integrating existing PEFT methods into the GIST framework boosts performance without significantly increasing parameters. For example, GIST improves adapter performance by 2.25% on the VTAB benchmark with only 0.01â€° additional parameters for ViT-B/16. The framework exhibits universality across models and tasks as well as scalability to integrate more advanced interaction losses in the future. Overall, GIST provides an efficient way to enhance PEFT methods by establishing explicit connections to downstream targets and facilitating comprehensive knowledge interaction.
