# [Green Hierarchical Vision Transformer for Masked Image Modeling](https://arxiv.org/abs/2205.13515)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question is: 

How can hierarchical vision transformers like Swin Transformer and Twins Transformer be adapted to work efficiently with masked image modeling (MIM) by operating only on visible image patches?

The key challenge is that the locality bias induced by window attention and convolution/pooling in these models is incompatible with random masking, which creates unevenly-sized local windows. 

To address this, the paper proposes an approach with three main components:

1) A Group Window Attention scheme that partitions uneven local windows into equal-sized groups and applies masked attention within each group.

2) An optimal grouping strategy based on dynamic programming that minimizes the computation cost of attention. 

3) Replacing convolutions with sparse convolutions that can handle the sparse, masked inputs.

Together, these modifications allow hierarchical vision transformers to efficiently discard masked patches and only process visible patches during MIM pre-training, providing significant speed and memory improvements while maintaining accuracy.

In summary, the central hypothesis is that with the proposed approach, hierarchical vision transformers can be effectively adapted to leverage the computational benefits of operating only on visible patches for MIM. The experiments aim to validate this hypothesis.


## What is the main contribution of this paper?

 This paper presents a new method for masked image modeling (MIM) using hierarchical vision transformers. The key contributions are:

1. A Group Window Attention scheme that partitions uneven local windows into equal-sized groups and performs masked self-attention within each group. This allows hierarchical transformers to operate only on visible patches during MIM.

2. An optimal grouping algorithm based on dynamic programming that finds the best group partition to minimize computational cost. 

3. Use of sparse convolution to replace standard convolution layers, enabling them to work with sparse masked inputs.

4. Experiments showing the method achieves similar accuracy to baseline MIM methods but with substantially improved efficiency - up to 2.7x faster training and 70% less GPU memory usage.

5. The approach is general and can work with different hierarchical vision transformer architectures like Swin and Twins.

In summary, the main contribution is a more efficient and greener approach to enable masked image modeling with hierarchical vision transformers, without sacrificing accuracy. This is achieved through novel group attention, optimal grouping, and sparse convolution techniques. The efficiency gains open up MIM to a wider range of researchers and applications.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Based on my reading, here is a one sentence summary of the key points of this paper:

The paper presents an efficient approach for Masked Image Modeling with hierarchical Vision Transformers that allows operating only on visible image patches through Group Window Attention, Optimal Grouping with Dynamic Programming, and Sparse Convolutions.


## How does this paper compare to other research in the same field?

 Here are some key points comparing this paper to other research in masked image modeling:

- The main contribution is adapting hierarchical vision transformers like Swin Transformer to operate efficiently on masked images, whereas previous works like MAE focused on isotropic ViTs. This allows combining the benefits of locality/multi-scale processing with computation on only visible patches.

- The proposed techniques like group window attention and optimal dynamic grouping are novel methods aimed at handling the uneven windows created by random masking in hierarchical models. Other MIM papers did not address this issue before.

- The approach achieves similar accuracy to state-of-the-art like SimMIM but reduces computations by up to 2.7x during pre-training. This aligns well with the goals and trends in "Green AI".

- The method is evaluated on ImageNet classification and COCO detection/segmentation. Showing strong performance on dense tasks highlights the advantages of hierarchical representations compared to previous MIM works focused on classification.

- Unlike some other methods that modify model architectures, this approach does not alter the backbone networks like Swin/Twins. This allows fair comparison to supervised baselines using the same architectures.

- The idea of using sparse convolutions to handle masked patches is novel in the context of MIM and eliminates the need to retain masked patches as in some prior works.

Overall, the paper makes nice contributions in adapting masked image modeling to hierarchical vision transformers with much greater efficiency. The compute savings enable scaling up self-supervised pre-training practically. The techniques are fairly simple and generic too.
