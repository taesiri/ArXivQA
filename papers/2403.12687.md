# [Audio-Visual Compound Expression Recognition Method based on Late   Modality Fusion and Rule-based Decision](https://arxiv.org/abs/2403.12687)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Compound expression recognition (CER) to identify combinations of multiple basic emotions is an important task for affective computing and human-computer interaction. 
- Existing CER methods rely predominantly on visual modality and lack publicly available annotated data containing compound expressions.

Proposed Solution:
- The paper proposes a novel audio-visual CER method that does not require CER-specific training data. 
- Three models are used - static visual, dynamic visual using LSTM, and acoustic using Wav2vec2. 
- Models are first trained on basic emotions using multiple corpora like AffectNet, RAMAS, RAVDESS, etc.
- During CER, modality fusion is done using Dirichlet-based and hierarchical weighting of emotion probabilities predicted by the three models.
- Finally, rule-based decision making predicts compound expressions based on analysis of weighted probabilities.

Main Contributions:
- Introduction of an audio-visual CER method relying on basic emotion recognition models and late modality fusion.
- Use of multi-corpus and cross-corpus training for enhancing model generalizability.  
- Proposal of a rule-based CER decision making technique that assigns responsibility to different models for predicting specific compound expressions.
- Experimental results demonstrating the potential of using the method as an annotation assistance tool for basic and compound emotions.

In summary, the key novelty of the paper is in presenting an audio-visual compound expression recognition technique that does not need CER-specific data and relies on rule-based decision making through late probability-level multimodal fusion.
