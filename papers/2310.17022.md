# [Controlled Decoding from Language Models](https://arxiv.org/abs/2310.17022)

## Summarize the paper in one sentence.

 The paper proposes controlled decoding, a novel off-policy reinforcement learning method to steer the generation from language models towards higher reward outcomes at inference time.


## Summarize the paper in one paragraphs.

 Here is a one paragraph summary of the paper:

The paper proposes a novel off-policy reinforcement learning method called controlled decoding (CD) to steer the generation from language models towards high reward outcomes at inference time. CD trains a prefix scorer to predict the expected reward when decoding is continued from a partial response. At inference time, the prefix scorer is used to bias the sampling from the language model to increase the likelihood of high reward outcomes. The prefix scorer can be trained in an off-policy manner using the Bellman equation. The paper shows CD is effective at improving dialogue safety and length on a Reddit dataset. It also demonstrates the flexibility of CD, like the ability to combine multiple prefix scorers for multi-objective control and to exert blockwise instead of tokenwise control. Overall, CD provides a promising approach for aligning language models at inference time while limiting drift from the original model.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the paper:

The paper proposes controlled decoding, a novel reinforcement learning method to steer the generation from language models towards high reward outcomes at inference time by training a prefix scorer to predict expected future reward.


## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question seems to be: 

How can we guide or control the generation from language models towards higher reward or desired outcomes at inference time, without having to retrain or fine-tune the models?

The key ideas and contributions of the paper related to this question appear to be:

- Proposing a reinforcement learning framework with a KL regularization term to trade off between increasing rewards vs staying close to the original language model distribution.

- Showing that this KL-regularized RL objective can be optimized by learning a "prefix scorer" that estimates the value/reward of partial decoded sequences. 

- The prefix scorer can be trained separately in an off-policy way and used at inference time with the frozen language model to steer generation towards higher rewards.

- Introducing "controlled decoding" methods that use the prefix scorer in different ways to control generation, including token-wise and block-wise variants.

- Demonstrating controlled decoding enables inference-time control of language models for various rewards like safety and length, without model retraining.

- Showing the modularity of controlled decoding to handle multiple rewards and tradeoffs between them at inference time.

So in summary, the main hypothesis is that a separately trained prefix scorer can be used to control language model generation at inference time, enabling flexible alignment without retraining the base models. The paper provides both theory and experiments to validate this hypothesis.


## What is the main contribution of this paper?

 The main contribution of this paper seems to be proposing a new method called "controlled decoding" (CD) for aligning language model generations towards desired objectives. The key ideas are:

- Formulating a KL-regularized reinforcement learning objective for language model decoding. The optimal policy involves combining the base LM probabilities with a "prefix scorer" that estimates future reward. 

- Showing how this prefix scorer can be trained in an off-policy manner using a Bellman equation, similar to Q-learning in DQN. This allows using offline datasets.

- Two ways to leverage the prefix scorer at inference time: (1) token-wise sampling (2) blockwise sample and rerank. The latter bridges best-of-K and token-level control.

- Demonstrating improved performance over baselines like FUDGE, PPO, and best-of-K on tasks like improving dialog safety and length, using the Reddit conversations dataset.

- Showing the modularity of CD by combining multiple prefix scorers for multi-objective control without retraining.

In summary, the main contribution seems to be the controlled decoding framework and showing how it can effectively control LM generations in an off-policy, modular way. The connections to RL and ability to do blockwise control seem notable.


## How does this paper compare to other research in the same field?

 Here are some thoughts on how this paper compares to other research in controlled language generation:

- The main contribution is the proposed "controlled decoding" (CD) method, which uses a prefix scorer to steer autoregressive generation towards higher reward outcomes at inference time. This is an inference-time add-on solution rather than a generator improvement method that modifies the base model.

- Using a prefix scorer to control generation has been explored before in works like FUDGE and GeDi, but the key difference is that this paper trains the prefix scorer using an off-policy RL objective based on the Bellman equation. This theoretically grounds the prefix scorer as a value function.

- The off-policy nature is a difference from prior RL methods like PPO which require on-policy rollouts. The off-policy training allows using any scored dataset. However, the results show PPO can still be competitive.

- The proposed blockwise CD method seems quite novel as a middle ground between best-of-K reranking and token-level control. This bridges a gap between those approaches in a natural way.

- Multi-objective control combining multiple prefix scorers is shown as an advantage of the modular CD approach. This is more flexible than single-reward PPO at inference time.

- Overall CD appears competitive with or better than prior work in the empirical results. But there still seems to be a gap compared to best-of-K in the safety experiment. Closing this gap further could be an area for future work.

- The framework seems generic and extensible. But more analysis may be needed on factors like noisy/imperfect rewards and potential overoptimization.

So in summary, the off-policy nature, modular design, and new blockwise extension seem like the main differentiating aspects compared to related controlled generation literature. But continued analysis on challenges like reward generalization remains open.
