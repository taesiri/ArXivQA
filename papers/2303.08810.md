# [BiFormer: Vision Transformer with Bi-Level Routing Attention](https://arxiv.org/abs/2303.08810)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central research question this paper addresses is: How can we design an attention mechanism for vision transformers that achieves dynamic, query-adaptive sparsity to improve computational efficiency?The key ideas and contributions to address this question are:- Propose a novel bi-level routing attention (BRA) mechanism that filters out irrelevant key-value pairs at a coarse region level before applying fine-grained token-level attention. This allows dynamic, query-aware sparsity.- Provide an efficient implementation of BRA using region-level routing and token gathering that only involves GPU-friendly dense matrix multiplications. - Analyze the computational complexity of BRA and show it can achieve O(n^(4/3)) with proper region partition size.- Introduce BiFormer, a new vision transformer architecture using BRA modules, which achieves state-of-the-art efficiency-performance trade-off on image classification, detection, segmentation.So in summary, the central hypothesis is that routing attention hierarchically to filter irrelevant regions first can enable more efficient yet accurate query-adaptive sparse attention patterns compared to prior works. The key contribution is the proposed BRA mechanism and BiFormer architecture demonstrating this.


## What is the main contribution of this paper?

The main contributions of this paper are:1. It proposes a novel bi-level routing attention mechanism for vision transformers, which enables dynamic and query-adaptive sparsity. The key idea is to first filter out irrelevant key-value pairs at a coarse region level, and then apply fine-grained token-to-token attention in the union of remaining candidate regions. This provides a more flexible allocation of computations compared to prior sparse attention mechanisms like dilated windows or axial stripes. 2. It provides an efficient implementation of the proposed bi-level routing attention using key/value token gathering and dense matrix multiplications. This implementation saves computation and memory while using GPU-friendly operations.3. It presents a new general vision transformer backbone called BiFormer built with the bi-level routing attention. Experiments on image classification, object detection, and semantic segmentation demonstrate that BiFormer achieves better performance under similar model sizes compared to baselines.4. It analyzes the computational complexity of the bi-level routing attention and shows it can achieve O((HW)^{4/3}) complexity with proper region partition size, which is lower than the O(HW^2) of standard attention.5. It visualizes the attention maps to demonstrate that bi-level routing attention can locate semantically related regions and capture long-range inter-object relationships in a query-adaptive manner.In summary, the main contribution is the proposal of the bi-level routing attention and BiFormer architecture to enable efficient and flexible allocation of computations in vision transformers with improved performance. The theoretical analysis and experiments validate the advantages of this approach.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper proposes a novel bi-level routing attention mechanism for vision transformers, which dynamically selects a small subset of the most relevant tokens for each query to attend in a coarse-to-fine manner, enabling efficient sparse attention with long-range dependency modeling; this is used to build a new vision transformer called BiFormer which achieves better performance and efficiency trade-offs compared to prior work, as demonstrated through extensive experiments on image classification, object detection, instance segmentation and semantic segmentation tasks.
