# [BiFormer: Vision Transformer with Bi-Level Routing Attention](https://arxiv.org/abs/2303.08810)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central research question this paper addresses is: How can we design an attention mechanism for vision transformers that achieves dynamic, query-adaptive sparsity to improve computational efficiency?The key ideas and contributions to address this question are:- Propose a novel bi-level routing attention (BRA) mechanism that filters out irrelevant key-value pairs at a coarse region level before applying fine-grained token-level attention. This allows dynamic, query-aware sparsity.- Provide an efficient implementation of BRA using region-level routing and token gathering that only involves GPU-friendly dense matrix multiplications. - Analyze the computational complexity of BRA and show it can achieve O(n^(4/3)) with proper region partition size.- Introduce BiFormer, a new vision transformer architecture using BRA modules, which achieves state-of-the-art efficiency-performance trade-off on image classification, detection, segmentation.So in summary, the central hypothesis is that routing attention hierarchically to filter irrelevant regions first can enable more efficient yet accurate query-adaptive sparse attention patterns compared to prior works. The key contribution is the proposed BRA mechanism and BiFormer architecture demonstrating this.


## What is the main contribution of this paper?

The main contributions of this paper are:1. It proposes a novel bi-level routing attention mechanism for vision transformers, which enables dynamic and query-adaptive sparsity. The key idea is to first filter out irrelevant key-value pairs at a coarse region level, and then apply fine-grained token-to-token attention in the union of remaining candidate regions. This provides a more flexible allocation of computations compared to prior sparse attention mechanisms like dilated windows or axial stripes. 2. It provides an efficient implementation of the proposed bi-level routing attention using key/value token gathering and dense matrix multiplications. This implementation saves computation and memory while using GPU-friendly operations.3. It presents a new general vision transformer backbone called BiFormer built with the bi-level routing attention. Experiments on image classification, object detection, and semantic segmentation demonstrate that BiFormer achieves better performance under similar model sizes compared to baselines.4. It analyzes the computational complexity of the bi-level routing attention and shows it can achieve O((HW)^{4/3}) complexity with proper region partition size, which is lower than the O(HW^2) of standard attention.5. It visualizes the attention maps to demonstrate that bi-level routing attention can locate semantically related regions and capture long-range inter-object relationships in a query-adaptive manner.In summary, the main contribution is the proposal of the bi-level routing attention and BiFormer architecture to enable efficient and flexible allocation of computations in vision transformers with improved performance. The theoretical analysis and experiments validate the advantages of this approach.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper proposes a novel bi-level routing attention mechanism for vision transformers, which dynamically selects a small subset of the most relevant tokens for each query to attend in a coarse-to-fine manner, enabling efficient sparse attention with long-range dependency modeling; this is used to build a new vision transformer called BiFormer which achieves better performance and efficiency trade-offs compared to prior work, as demonstrated through extensive experiments on image classification, object detection, instance segmentation and semantic segmentation tasks.


## How does this paper compare to other research in the same field?

Here are a few key ways this paper compares to other research in vision transformers:- It proposes a new sparse attention mechanism called bi-level routing attention (BRA) that enables dynamic, query-aware sparsity. This is different from prior works that use handcrafted static patterns or share the sampled key/value tokens across all queries. - BRA first filters out irrelevant regions at a coarse level before fine-grained token-to-token attention. This allows modeling long-range dependencies efficiently. In contrast, some prior works rely only on local contexts when making key/value selections.- The paper shows strong performance of BRA and the proposed BiFormer architecture across multiple vision tasks like classification, detection, and segmentation. Many prior sparse attention works focus more narrowly on image classification.- The complexity analysis shows BRA can achieve O((HW)^(4/3)) complexity, lower than standard attention's O(HW^2) and competitive with other sparse attention schemes.- The paper provides useful ablation studies and visualizations to analyze how the bi-level routing works and impacts performance compared to baseline models like Swin Transformers.Overall, the introduction of bi-level routing attention is a nice contribution over prior works and seems to provide a better computation-performance trade-off. The comprehensive experimental validation on multiple tasks and analysis is also a strength. If I had to critique, I'd say more comparison to very recent works like MaxViT and Wave-ViT would make the relative merits clearer. But it's a solid paper advancing research in efficient vision transformers.
