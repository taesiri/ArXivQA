# [BiFormer: Vision Transformer with Bi-Level Routing Attention](https://arxiv.org/abs/2303.08810)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central research question this paper addresses is: How can we design an attention mechanism for vision transformers that achieves dynamic, query-adaptive sparsity to improve computational efficiency?The key ideas and contributions to address this question are:- Propose a novel bi-level routing attention (BRA) mechanism that filters out irrelevant key-value pairs at a coarse region level before applying fine-grained token-level attention. This allows dynamic, query-aware sparsity.- Provide an efficient implementation of BRA using region-level routing and token gathering that only involves GPU-friendly dense matrix multiplications. - Analyze the computational complexity of BRA and show it can achieve O(n^(4/3)) with proper region partition size.- Introduce BiFormer, a new vision transformer architecture using BRA modules, which achieves state-of-the-art efficiency-performance trade-off on image classification, detection, segmentation.So in summary, the central hypothesis is that routing attention hierarchically to filter irrelevant regions first can enable more efficient yet accurate query-adaptive sparse attention patterns compared to prior works. The key contribution is the proposed BRA mechanism and BiFormer architecture demonstrating this.
