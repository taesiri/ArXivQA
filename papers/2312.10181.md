# [Coupling Fairness and Pruning in a Single Run: a Bi-level Optimization   Perspective](https://arxiv.org/abs/2312.10181)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Neural network pruning aims to remove redundant parameters to improve efficiency, but can introduce or amplify algorithmic bias issues. 
- Existing methods tackle fairness and pruning separately, which can degrade performance or fairness. Integrating them is challenging as pruning focuses on the mask while fairness focuses on the weights.

Proposed Solution: 
- The paper proposes a novel framework called Bi-level Fair Pruning (BiFP) to jointly optimize the pruning mask and weight update processes with fairness constraints.  
- It formulates fair pruning as a constrained bi-level optimization problem, with the upper level optimizing the mask and the lower level optimizing the weights under fairness constraints.
- To facilitate optimization, it relaxes the discrete masks into continuous scores and reformulates the non-differentiable fairness metric into a differentiable constraint.

Main Contributions:
- Defines new fairness notions tailored for model compression - performance fairness and performance degradation fairness.
- Pioneers a joint optimization framework that simultaneously couples fair mask learning and fair weight learning. This unified approach ensures efficiency and fairness in one shot.
- Outperforms baselines in fairness, accuracy and efficiency. Ablation studies validate the vital role of both mask and weight constraints in achieving fair pruning.

In summary, the paper proposes an innovative bi-level optimization framework to simultaneously optimize masks and weights with fairness constraints. By harmonizing these typically disjoint processes, it achieves the delicate balance of performance, efficiency and fairness during neural network pruning.
