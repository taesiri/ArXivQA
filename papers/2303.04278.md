# [CUDA: Convolution-based Unlearnable Datasets](https://arxiv.org/abs/2303.04278)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central research question/hypothesis of this paper is: Can we generate unlearnable datasets that are robust to adversarial training by using controlled class-wise convolutions instead of additive noises?The key points are:- Existing methods for generating unlearnable datasets use small additive noises, making them vulnerable to adversarial training which is designed to be robust against such noises. - This paper proposes a new method called Convolution-based Unlearnable Dataset (CUDA) that instead uses controlled class-wise convolutions to generate the unlearnable data.- The convolutions are performed using randomly generated filters per class based on a private key. This encourages models to learn the relation between filters and labels rather than useful features.- CUDA is designed to be robust against adversarial training since it introduces multiplicative noise in the Fourier domain rather than small additive noises.- Experiments show CUDA is effective across datasets and architectures, and robust to various training techniques like adversarial training, augmentations, regularization etc.So in summary, the central hypothesis is that using class-wise controlled convolutions can generate unlearnable datasets that are more robust compared to prior additive noise-based techniques. The effectiveness of CUDA is demonstrated empirically.


## What is the main contribution of this paper?

The main contribution of this paper is proposing a new method called Convolution-based Unlearnable DAtaset (CUDA) for generating unlearnable datasets that are robust against adversarial training. Specifically:- They propose CUDA, a novel technique to generate unlearnable datasets by performing controlled class-wise convolutions on the training data using randomly generated filters based on a private key. - They show theoretically that CUDA can successfully poison Gaussian mixture data by reducing the accuracy of the optimal Bayes classifier on clean data.- They demonstrate empirically that models trained on CUDA exhibit poor generalization on clean test data under various training settings like empirical risk minimization, adversarial training, randomized smoothing, transfer learning etc. CUDA is also robust to adaptive defenses designed specifically to break it.- Compared to prior unlearnability techniques like error-minimizing noise, targeted adversarial poisoning etc., CUDA generation is much faster as it does not require iterative optimization. It is also more robust to adversarial training and data augmentations.- Overall, CUDA provides an efficient and robust way to generate unlearnable datasets that prevent unauthorized usage of data for training deep learning models. The multiplicative noise added by CUDA in Fourier domain makes it resilient to adversarial training with small additive noise budgets.In summary, the main contribution is proposing CUDA as a novel unlearnability technique and demonstrating its effectiveness theoretically and empirically.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper proposes a new method called Convolution-based Unlearnable DAtaset (CUDA) to generate poisoned datasets that make deep learning models fail to learn useful features, instead just memorizing spurious correlations between random convolutional filters and labels; this allows protecting private data from unauthorized use.
