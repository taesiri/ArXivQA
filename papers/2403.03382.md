# [Adaptive Discovering and Merging for Incremental Novel Class Discovery](https://arxiv.org/abs/2403.03382)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem: 
The paper focuses on the task of class-incremental novel class discovery (class-iNCD), where the goal is to continuously identify new classes from unlabeled data streams, while preserving performance on previously learned classes, without accessing old data. Two main challenges exist - discovering novel classes from unlabeled data, and preventing catastrophic forgetting of old classes when learning new concepts.

Proposed Solution - Adaptive Discovering and Merging (ADM):
1) Adaptive novel class discovery: Separate representation learning from classifier learning. Representations are learned via contrastive self-supervised learning on new unlabeled data. Classifiers are trained using proposed techniques - Triple Comparison (TC) minimizes intra-class distance and maximizes inter-class distance for better pseudo-labels, and Probability Regularization (PR) promotes probability diversity across novel classes.

2) Adaptive Model Merging (AMM): A base-novel hybrid model, where the novel branch is gated to minimize interference with base branch. Gating is based on BN gamma weights from base branch, allowing lossless merging of branches after discovery.

Main Contributions:
- TC and PR for more accurate pseudo-labeling and adaptive discovery of novel classes from unlabeled data
- AMM architecture that limits forgetting of old classes, while allowing discovery and integration of new concepts without growth in parameters or computations
- Significantly outperforms prior class-iNCD methods, also benefits regular class-incremental learning by reducing catastrophic forgetting


## Summarize the paper in one sentence.

 This paper proposes Adaptive Discovering and Merging (ADM), a new paradigm for incremental novel class discovery that adaptively discovers novel categories and integrates novel knowledge into the model without affecting original knowledge.


## What is the main contribution of this paper?

 The main contributions of this paper are:

1. It proposes two techniques - Triple Comparison (TC) and Probability Regularization (PR) - for adaptive novel class discovery. Specifically, TC constrains the probability differences and PR promotes probability diversity to avoid overfitting to a single class when generating pseudo-labels. This enables more robust discovery of novel classes from unlabeled data.

2. It introduces a hybrid model architecture with base and novel branches called Adaptive Model Merging (AMM). AMM reduces the interference of the novel branch on old classes in the base model to mitigate catastrophic forgetting. It also allows merging the novel branch into the base model without increasing parameters or computations.

3. Extensive experiments show state-of-the-art performance on several class-incremental novel class discovery datasets. The proposed AMM module also benefits class-incremental learning methods by alleviating catastrophic forgetting.

In summary, the key contribution is a new paradigm called Adaptive Discovering and Merging (ADM) that enables adaptive discovery of novel classes and non-destructive integration of new knowledge into the model.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and keywords associated with this paper include:

- Incremental learning
- Novel class discovery
- Class-incremental learning
- Catastrophic forgetting
- Pseudo-labeling
- Knowledge distillation
- Contrastive learning 
- Representation learning
- Neural network architecture
- Multi-branch models
- Model merging
- Adaptive discovering and merging (ADM)
- Triple comparison
- Probability regularization

The paper proposes a new paradigm called "Adaptive Discovering and Merging" (ADM) to address the challenges of discovering novel classes from unlabeled data and preventing catastrophic forgetting in incremental learning settings. The key components include adaptive pseudo-labeling techniques like triple comparison and probability regularization for novel class discovery, as well as architectural innovations like multi-branch base/novel models and adaptive model merging to mitigate forgetting. The experimental results demonstrate state-of-the-art performance on class-incremental novel class discovery and the ability to plug ADM into other incremental learning methods as well.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in the paper:

1) The paper proposes to separate representation learning and category discovery. What is the intuition behind this? How does it help with novel category discovery compared to having a joint objective?

2) The paper introduces Triplet Comparison (TC) to enhance the quality of pseudo-labels. How does the triplet loss in TC help minimize intra-class variance and maximize inter-class variance? What impact does this have on the classifier learning?

3) What is the motivation behind using Probability Regularization (PR)? How does maximizing entropy of class probability distribution help avoid trivial solutions during early training stages?

4) Explain the working of Adaptive Feature Fusion (AFF) module. How does using the base branch outputs to gate the novel branch features help reduce interference on old class discrimination?

5) What is the key limitation of AFF that prevents mergeability of the novel branch into the base model? How does Adaptive Model Merging (AMM) address this? 

6) Explain how the BatchNorm gamma weights can act as an approximation to the dynamic gating function in AMM. Why is this an effective way to achieve linear model merging?

7) Analyze the improvements obtained by applying AMM to existing continual learning approaches like iCaRL, PODNet etc. What specific issues does it help alleviate in those methods?

8) How robust is the proposed ADM approach to a large number of incremental steps? Analyze the trend in performance gains compared to baselines as number of steps increase.

9) What changes would be required to apply ADM to convolutional architectures beyond ResNet? Would the AMM scheme work seamlessly or need modifications?

10) A core advantage highlighted is no increase in computations for inference after merging. Does the training process incur high overhead due to the two branch architecture? Analyze the training time complexity.
