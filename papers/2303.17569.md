# [Iterative Prompt Learning for Unsupervised Backlit Image Enhancement](https://arxiv.org/abs/2303.17569)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question appears to be: 

How can we develop an unsupervised method for enhancing backlit images that leverages the visual-language prior knowledge encapsulated in Contrastive Language-Image Pre-Training (CLIP) models?

The key hypothesis seems to be:

The rich visual-language priors encoded in CLIP models can be exploited to distinguish between backlit and well-lit images and guide the training of a enhancement network, even without paired training data. However, directly applying CLIP for low-level enhancement tasks is non-trivial due to issues like varying optimal prompts and interference from high-level semantics. These challenges can be addressed through an iterative prompt learning framework.

In summary, the paper explores using CLIP and prompt learning to unlock the potential of large vision-language models for low-level vision tasks like backlit image enhancement in an unsupervised manner. The core hypothesis is that the CLIP prior is useful for this task if the prompt learning can be done effectively.
