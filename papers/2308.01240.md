# [Evaluating Instruction-Tuned Large Language Models on Code Comprehension   and Generation](https://arxiv.org/abs/2308.01240)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central research question this paper aims to address is:How do recent open-source instruction-tuned large language models (LLMs) perform on diverse code comprehension and generation tasks?The key points are:- The paper evaluates open-source instruction-tuned LLMs on code-related tasks. Instruction tuning is a technique to enhance LLMs' generalization capability on new downstream tasks. - It focuses on code comprehension and generation tasks which are important in software engineering. The tasks evaluated include defect detection, clone detection, assertion generation, and code summarization.- The aim is to understand the capabilities of these open-source instructed LLMs on the code tasks, as most existing work studies proprietary models like ChatGPT or focuses only on the NL-to-code generation task. - The paper performs a comprehensive study across multiple dimensions - comparing instructed vs non-instructed models, general vs code-specific instruction tuning, model sizes, diverse task settings (zero-shot, few-shot, fine-tuning).So in summary, the key research question is assessing how the capabilities of recent open-source instructed LLMs vary on different code comprehension and generation tasks across different settings. The study aims to provide useful insights into applying instructed LLMs for software engineering.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contributions are:1. This paper presents the first study evaluating open-source instruction-tuned large language models (LLMs) on code comprehension and generation tasks. Specifically, it evaluates 10 recent open-source instructed LLMs along with 5 baseline models on 4 representative code-related tasks (defect detection, clone detection, assertion generation, and code summarization).2. It performs a large-scale experiment, evaluating the models in diverse settings - zero-shot, few-shot, and fine-tuning. The extensive experiments take around 1000 GPU-hours. 3. The paper provides several findings and practical implications regarding using instructed LLMs for code comprehension and generation:- Instructed LLMs are competitive on code tasks even without fine-tuning, and sometimes outperform smaller specialized models. General-domain instructed LLMs can perform well on code.- Adding demonstration examples helps, but can sometimes worsen performance. Performance tends to decrease with longer input context.- Fine-tuning consistently improves performance over zero/few-shot learning. After fine-tuning, instructed LLMs outperform both small specialized models and uninstructed LLMs.- BM25 retrieval is the best shot selection strategy for code generation tasks. No strategy is clearly better for code classification.- There are trade-offs between model performance and computational costs. Smaller models are more efficient but instructed LLMs achieve better performance.In summary, this paper provides a comprehensive empirical study and useful insights on leveraging instructed LLMs for code intelligence tasks. The large-scale evaluation and findings can inform future research and practice in this emerging area.
