# [Evaluating Instruction-Tuned Large Language Models on Code Comprehension   and Generation](https://arxiv.org/abs/2308.01240)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central research question this paper aims to address is:How do recent open-source instruction-tuned large language models (LLMs) perform on diverse code comprehension and generation tasks?The key points are:- The paper evaluates open-source instruction-tuned LLMs on code-related tasks. Instruction tuning is a technique to enhance LLMs' generalization capability on new downstream tasks. - It focuses on code comprehension and generation tasks which are important in software engineering. The tasks evaluated include defect detection, clone detection, assertion generation, and code summarization.- The aim is to understand the capabilities of these open-source instructed LLMs on the code tasks, as most existing work studies proprietary models like ChatGPT or focuses only on the NL-to-code generation task. - The paper performs a comprehensive study across multiple dimensions - comparing instructed vs non-instructed models, general vs code-specific instruction tuning, model sizes, diverse task settings (zero-shot, few-shot, fine-tuning).So in summary, the key research question is assessing how the capabilities of recent open-source instructed LLMs vary on different code comprehension and generation tasks across different settings. The study aims to provide useful insights into applying instructed LLMs for software engineering.
