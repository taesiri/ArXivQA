# [Probing and Mitigating Intersectional Social Biases in Vision-Language   Models with Counterfactual Examples](https://arxiv.org/abs/2312.00825)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Prior studies on probing social biases in vision-language models (VLMs) have focused on single attributes like gender or race, ignoring intersectional biases (combinations of attributes). This is likely due to the difficulty of collecting real image-text pairs covering different combinations of social attributes.

Proposed Solution: 
- The authors propose using text-to-image diffusion models to automatically generate counterfactual image-text pairs depicting occupations with different combinations of social attributes related to gender, race and physical characteristics. 

- They use Stable Diffusion with cross attention control to produce sets of images that differ only in the social attributes, keeping other details constant. This allows precisely probing the impact of attribute changes on model biases.

- The authors apply a 3-stage filtering process, including CLIP similarity checks, NSFW filtering and CLIP-based attribute detectability filtering, to ensure only high-quality counterfactuals are retained.

Contributions:
- The authors construct SocialCounterfactuals, a dataset with over 171k synthetic counterfactual image-text pairs covering intersectional biases related to gender, race and physical characteristics across 158 occupations.

- Extensive experiments demonstrate the dataset can effectively uncover intersectional biases in 6 VLMs. All models exhibit skewed retrieval results, with substantial variation across different race-gender combinations.

- Additional training experiments show the dataset can also mitigate intersectional biases in VLMs. Models fine-tuned on it have up to 42.6% reduction in retrieval skew on separate test sets.

In summary, the paper introduces an automated approach to generate multimodal counterfactual data for probing intersectional biases in VLMs. It also releases the first large-scale resource explicitly focused on intersectional social biases in vision-language tasks.
