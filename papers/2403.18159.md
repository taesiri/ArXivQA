# [Oh! We Freeze: Improving Quantized Knowledge Distillation via Signal   Propagation Analysis for Large Language Models](https://arxiv.org/abs/2403.18159)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Large language models (LLMs) like LLaMA have revolutionized NLP but are difficult to deploy on edge devices due to high compute/memory requirements. 
- Quantization can optimize models for edge devices but suffers performance loss at very low bitwidths like 4-bits. 
- The paper focuses on 4-bit quantization of LLaMAv2-Chat model for on-device chat applications.

Proposed Solution:
- The authors propose a knowledge distillation based quantization-aware training (KD-QAT) method to finetune a 4-bit quantized LLaMAv2-Chat model.
- They analyze forward/backward passes in multi-head self-attention and find o- and v- projections are more susceptible to quantization errors.
- Based on this analysis, they propose "ov-freeze" - freezing o- and v- weights to stabilize training.

Main Contributions:
- Create a KD-QAT pipeline for 4-bit LLaMAv2-Chat using public datasets in <1 day on 8 GPUs.
- Empirically analyze forward/backward passes and identify o- and v- projections as most vulnerable to quantization errors. 
- Propose ov-freeze to stabilize training which achieves near floating point accuracy (within 0.7% on benchmarks).

In summary, the paper provides insights into vulnerabilities of LLaMA to low-bit quantization and proposes ov-freeze to stabilize KD-QAT training achieving high 4-bit model accuracy, enabling on-device deployment.
