# [Make-Your-Video: Customized Video Generation Using Textual and   Structural Guidance](https://arxiv.org/abs/2306.00943)

## What is the central research question or hypothesis that this paper addresses?

The central research question addressed in this paper is: How can we achieve customized video generation using both textual and structural guidance for improved controllability? Specifically, the authors propose using text prompts as abstract context descriptions and frame-wise depth maps as concrete motion structure guidance to enable more precise control over synthesized video content. Their method, called Make-Your-Video, involves joint-conditional video generation using a latent diffusion model pre-trained on images and adapted for video with the addition of temporal modules.The key ideas and hypotheses tested in this work are:- Text alone provides global context but may be insufficient for precise control over video content. Combining text prompts with structural guidance like depth maps can improve controllability.- Pre-training a latent diffusion model on images allows transferring rich visual concepts to video generation while keeping training costs manageable. Freezing the pre-trained image model and adding small trainable temporal modules adapts it well for coherent video synthesis.- A causal attention mask enables their model to synthesize longer videos while maintaining quality, by preventing confusion from long-range temporal attention.In summary, the central hypothesis is that leveraging both textual and structural guidance will enable higher-quality, more controllable text-to-video generation, and their method and experiments aim to demonstrate this.
