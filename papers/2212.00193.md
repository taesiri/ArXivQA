# Distilling Reasoning Capabilities into Smaller Language Models

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper addresses is how to distill reasoning capabilities from large language models into smaller models. Specifically, the authors propose and investigate methods to leverage the step-by-step reasoning abilities of large language models to improve the reasoning capabilities of smaller models.The key hypothesis is that by using the chain-of-thought reasoning generated by large language models as additional supervision when fine-tuning smaller models, the smaller models can gain improved reasoning skills. The authors also propose an enhancement called "Socratic chain-of-thought" that incorporates subquestioning into the reasoning steps to provide further improvements.In summary, the main research question is how to transfer reasoning abilities from large teacher language models to smaller student models, with the hypothesis that using the teacher's chain-of-thought reasoning and subquestioning as extra supervision signals during training can enable significant gains in the reasoning skills of the smaller models.


## What is the main contribution of this paper?

The main contribution of this paper is proposing a knowledge distillation approach to transfer the reasoning capabilities of large language models (LLMs) into smaller models. Specifically:- They propose using LLMs to generate step-by-step reasoning chains (chain of thought) to train smaller models. This allows distilling the reasoning skills of large models without needing access to their parameters.- They propose an enhanced chain of thought approach called Socratic chain of thought, where the LLM also generates subquestions at each reasoning step. This further improves the reasoning skills transferred to the smaller model.- They demonstrate their approaches on three multi-step reasoning datasets - GSM8K, StrategyQA, and SVAMP. Using the LLM-generated reasoning chains as supervision during training boosts the smaller models' performance significantly, with gains of up to 40%.- They show cases where a smaller 774M parameter model trained with their approach matches or exceeds the performance of a 10x larger 6B parameter model.In summary, the key contribution is a knowledge distillation framework to transfer reasoning capabilities from large to small models, using only the solutions generated by the large model as a supervisory signal. The proposed Socratic chain of thought technique further improves this transfer.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper proposes a knowledge distillation approach to transfer reasoning capabilities from large language models to smaller models by using the larger models to generate step-by-step reasoning examples as training data for the smaller models.


## How does this paper compare to other research in the same field?

Here are a few key points comparing this paper to other related work on distilling reasoning capabilities into smaller language models:- The main contribution of this paper is proposing a framework to leverage large language models (LLMs) with strong reasoning abilities to help train smaller models, transferring reasoning skills via generated annotations. This is similar in spirit to knowledge distillation, but doesn't require direct access to the teacher model parameters. - The paper explores different types of annotation structures for supervision - gold step-by-step solutions, LLM-generated chain of thought, and their proposed Socratic chain of thought which decomposes problems into subquestion-solution pairs. The benefits of intermediate subquestioning is a novel idea compared to prior work.- The paper evaluates the approach on multiple reasoning datasets (GSM8K, StrategyQA, SVAMP) covering different reasoning skills like mathematical word problems, factual reasoning, etc. Showing broad applicability across reasoning tasks is a nice contribution.- In contrast, related works like Chain of Thought prompting or Least-to-Most prompting operate only during inference time and rely solely on very large LLMs. This paper explores more practical training-time distillation and shows it can work with smaller student models.- The idea of distilling reasoning skills itself relates to prior works on knowledge distillation for improving model rationalization. But the focus on multi-step reasoning tasks and using subquestion decomposition is unique.- Compared to prompting strategies, training the student models end-to-end allows more flexibility in model architecture choices. The iterative question generator + question answerer approach is novel.Overall, the paper makes excellent progress on an important challenge of transferring reasoning abilities to smaller models in a practical and generalizable manner. The analysis provides insights into when the proposed approaches are most effective.


## What future research directions do the authors suggest?

The authors suggest several future research directions in the paper:- Using multiple subquestion-solution pairs sampled from the LLM instead of just one, and selecting the most frequent final answer using majority voting. This could potentially lead to better results compared to using just a single sample. - Using more prompts when comparing CoT and Socratic CoT, instead of just a single prompt. Using more prompts could lead to a fairer comparison and potentially better performance.- Investigating other methods to eliminate the need for an explicit subquestion module at test time. The authors tried one approach of training the model to generate the full reasoning chain implicitly when prompted with subquestions, but this led to degraded performance. More research could be done to find better ways to avoid needing subquestions at test time.- Applying the proposed distillation framework to other domains beyond math word problems and factual reasoning, to see if it generalizes. - Exploring the use of different student model architectures besides the Transformer-based models used in this work.- Conducting a more thorough investigation into when Socratic CoT is most effective compared to standard CoT prompting. The authors demonstrated some cases where it helps, but more analysis could shed light on the tradeoffs.- Evaluating the robustness and generalization abilities of the distilled student models, compared to the teacher LLMs.So in summary, the main future directions are around exploring modifications to the distillation framework itself, applying it to new domains/tasks, using different model architectures, and better understanding when each approach is most suitable. The goal is to further improve the capability to transfer reasoning skills from large to small models.


## Summarize the paper in one paragraph.

Here is a one paragraph summary of the paper:This paper proposes a knowledge distillation approach to transfer the reasoning capabilities of large language models (LLMs) into smaller models. The key idea is to leverage the step-by-step reasoning that LLMs can perform via chain of thought prompting to provide additional supervision when training smaller student models. The authors study different types of annotation structures for providing this supervision, including gold step-by-step solutions, LLM-generated chain of thought, and a proposed Socratic chain of thought approach which decomposes problems into subquestion-solution pairs. Experiments on multi-step reasoning datasets show performance gains of 40% can be achieved by distilling LLM-generated annotations into small models. The Socratic chain of thought approach works best when gold step-by-step annotation is available for generating subquestions. The authors demonstrate cases where a smaller 774M parameter model trained with this approach can outperform a 10x larger 6B parameter LLM fine-tuned on human annotations. Overall, the work provides an effective framework for transferring reasoning skills from large to small models.
