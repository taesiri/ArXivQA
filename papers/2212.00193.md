# Distilling Reasoning Capabilities into Smaller Language Models

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper addresses is how to distill reasoning capabilities from large language models into smaller models. Specifically, the authors propose and investigate methods to leverage the step-by-step reasoning abilities of large language models to improve the reasoning capabilities of smaller models.The key hypothesis is that by using the chain-of-thought reasoning generated by large language models as additional supervision when fine-tuning smaller models, the smaller models can gain improved reasoning skills. The authors also propose an enhancement called "Socratic chain-of-thought" that incorporates subquestioning into the reasoning steps to provide further improvements.In summary, the main research question is how to transfer reasoning abilities from large teacher language models to smaller student models, with the hypothesis that using the teacher's chain-of-thought reasoning and subquestioning as extra supervision signals during training can enable significant gains in the reasoning skills of the smaller models.


## What is the main contribution of this paper?

The main contribution of this paper is proposing a knowledge distillation approach to transfer the reasoning capabilities of large language models (LLMs) into smaller models. Specifically:- They propose using LLMs to generate step-by-step reasoning chains (chain of thought) to train smaller models. This allows distilling the reasoning skills of large models without needing access to their parameters.- They propose an enhanced chain of thought approach called Socratic chain of thought, where the LLM also generates subquestions at each reasoning step. This further improves the reasoning skills transferred to the smaller model.- They demonstrate their approaches on three multi-step reasoning datasets - GSM8K, StrategyQA, and SVAMP. Using the LLM-generated reasoning chains as supervision during training boosts the smaller models' performance significantly, with gains of up to 40%.- They show cases where a smaller 774M parameter model trained with their approach matches or exceeds the performance of a 10x larger 6B parameter model.In summary, the key contribution is a knowledge distillation framework to transfer reasoning capabilities from large to small models, using only the solutions generated by the large model as a supervisory signal. The proposed Socratic chain of thought technique further improves this transfer.
