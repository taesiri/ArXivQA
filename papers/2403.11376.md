# [ShapeFormer: Shape Prior Visible-to-Amodal Transformer-based Amodal   Instance Segmentation](https://arxiv.org/abs/2403.11376)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper "ShapeFormer: Shape Prior Visible-to-Amodal Transformer-based Amodal Instance Segmentation":

Problem: 
Existing amodal instance segmentation (AIS) methods rely on a bidirectional approach for feature learning, involving transitions between amodal features (which include occluded regions) and visible features. However, using amodal features to enhance visible features compromises the quality of visible predictions. This subsequently impacts amodal predictions that depend on visible features.

Proposed Solution:
The paper proposes ShapeFormer, a transformer-based AIS method with three key components:

1) Visible-Occluding (Vis-Occ) Mask Head: Predicts precise visible masks while considering occlusions.

2) Category-Specific Shape Prior (Cat-SP) Retriever: Retrieves shape priors conditioned on the visible mask and object category using a category-specific vector quantized variational autoencoder.

3) Shape-Prior Amodal (SPA) Mask Head: Predicts amodal and occluded masks by utilizing retrieved shape priors through a novel shape-prior masked attention mechanism within a transformer decoder. This allows focusing on relevant regions when predicting the amodal mask.

Overall, ShapeFormer models the explicit relationship from visible features to amodal features and incorporates shape prior knowledge to enhance amodal predictions, avoiding compromising visible features.

Main Contributions:

- Proposes a decoupled transformer-based architecture for AIS that focuses solely on visible-to-amodal transition and omits amodal-to-visible transition.

- Develops a category-specific vector quantized variational autoencoder to effectively retrieve shape priors based on visible masks and object categories.

- Introduces shape-prior masked attention to leverage retrieved shape priors when predicting amodal masks within the transformer decoder.

- Achieves new state-of-the-art performance on four AIS datasets, demonstrating the efficacy of ShapeFormer's design.
