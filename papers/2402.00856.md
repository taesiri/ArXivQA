# [Towards Efficient and Exact Optimization of Language Model Alignment](https://arxiv.org/abs/2402.00856)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Aligning language models with human preferences is vital for real-world applications, but challenging due to lack of high-quality preference data and potential reward model misspecification.  
- Reinforcement learning (RL) methods suffer from high variance and instability in policy updates, impeding efficient optimization.
- Recently proposed direct preference optimization (DPO) directly optimizes from preferences, but relies on attaining the optimal policy, which is unrealistic. 

Proposed Solution - Efficient Exact Optimization (EXO):
- Reveals equivalence between the alignment objective and probability matching measured by reverse KL divergence.  
- Guarantees EXO optimizes the alignment objective asymptotically, irrespective of policy parametrization.
- Enables efficient supervised optimization by matching empirical distributions.
- Shows DPO corresponds to minimizing forward KL divergence, leading to a mean-seeking policy that poorly captures modes. 
- EXO learns a mode-seeking policy that concentrates on principal modes.

Contributions:
- Establishes equivalence between alignment objective and probability matching perspective.
- Proposes EXO for efficient and exact optimization of the alignment objective.  
- Compares EXO and DPO theoretically and empirically - EXO is more effective in capturing essential characteristics of the optimal policy.
- Demonstrates effectiveness of EXO over DPO and RL methods on realistic human preference data across summarization, dialogue and instruction following.
