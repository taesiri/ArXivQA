# [Multitrack Music Transcription with a Time-Frequency Perceiver](https://arxiv.org/abs/2306.10785)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the main research focus is on developing a new neural network architecture called Perceiver TF for the task of multitrack automatic music transcription (AMT). The key research goals appear to be:- To develop a model architecture that has better scalability to handle transcribing many instruments simultaneously (addressing the issue of "model scalability"). - To improve the model's ability to discriminate between different instruments in the input mixture (addressing "instrument discrimination").Specifically, the Perceiver TF architecture incorporates a Perceiver module along with additional components to model time-frequency representations for AMT. It is designed to be more efficient and scalable than prior work like SpecTNT. The authors also propose using a random mixing data augmentation technique during training to expose the model to diverse combinations of instruments, aiming to improve instrument discrimination.So in summary, the main research focus is on designing a new neural architecture and training procedure to advance state-of-the-art in multitrack AMT, with a particular emphasis on enhancing model scalability and instrument discrimination capabilities. The proposed Perceiver TF model is evaluated on several datasets and shown to outperform previous methods.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contributions seem to be:- Proposing a new neural network architecture called Perceiver TF for multitrack automatic music transcription (AMT). This architecture augments the Perceiver architecture with additional components to better model the time-frequency representation of audio for AMT.- Using a random-mixing data augmentation technique adapted from music source separation to train the model. This technique generates augmented training samples by randomly mixing together stems of different instruments from various datasets. It helps the model learn to better discriminate between instruments. - Combining vocal and multi-instrument AMT into a unified framework trained in a multi-task manner. Many prior works focused only on either vocal or multi-instrument AMT.- Demonstrating state-of-the-art performance on several public datasets for both vocal transcription and multi-instrument transcription compared to existing methods.In summary, the main contribution seems to be proposing the Perceiver TF architecture and the random-mixing augmentation technique to address the problems of model scalability and instrument discrimination in multitrack AMT. The authors show these techniques allow building a unified model that achieves new state-of-the-art results on multiple public datasets.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from this paper:The paper proposes a novel neural network architecture called Perceiver TF that combines aspects of the Perceiver model with hierarchical Transformers to achieve state-of-the-art results on multitrack automatic music transcription, which aims to transcribe multiple instruments and vocals from a music audio input into musical notes.
