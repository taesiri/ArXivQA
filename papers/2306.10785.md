# [Multitrack Music Transcription with a Time-Frequency Perceiver](https://arxiv.org/abs/2306.10785)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the main research focus is on developing a new neural network architecture called Perceiver TF for the task of multitrack automatic music transcription (AMT). The key research goals appear to be:- To develop a model architecture that has better scalability to handle transcribing many instruments simultaneously (addressing the issue of "model scalability"). - To improve the model's ability to discriminate between different instruments in the input mixture (addressing "instrument discrimination").Specifically, the Perceiver TF architecture incorporates a Perceiver module along with additional components to model time-frequency representations for AMT. It is designed to be more efficient and scalable than prior work like SpecTNT. The authors also propose using a random mixing data augmentation technique during training to expose the model to diverse combinations of instruments, aiming to improve instrument discrimination.So in summary, the main research focus is on designing a new neural architecture and training procedure to advance state-of-the-art in multitrack AMT, with a particular emphasis on enhancing model scalability and instrument discrimination capabilities. The proposed Perceiver TF model is evaluated on several datasets and shown to outperform previous methods.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contributions seem to be:- Proposing a new neural network architecture called Perceiver TF for multitrack automatic music transcription (AMT). This architecture augments the Perceiver architecture with additional components to better model the time-frequency representation of audio for AMT.- Using a random-mixing data augmentation technique adapted from music source separation to train the model. This technique generates augmented training samples by randomly mixing together stems of different instruments from various datasets. It helps the model learn to better discriminate between instruments. - Combining vocal and multi-instrument AMT into a unified framework trained in a multi-task manner. Many prior works focused only on either vocal or multi-instrument AMT.- Demonstrating state-of-the-art performance on several public datasets for both vocal transcription and multi-instrument transcription compared to existing methods.In summary, the main contribution seems to be proposing the Perceiver TF architecture and the random-mixing augmentation technique to address the problems of model scalability and instrument discrimination in multitrack AMT. The authors show these techniques allow building a unified model that achieves new state-of-the-art results on multiple public datasets.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from this paper:The paper proposes a novel neural network architecture called Perceiver TF that combines aspects of the Perceiver model with hierarchical Transformers to achieve state-of-the-art results on multitrack automatic music transcription, which aims to transcribe multiple instruments and vocals from a music audio input into musical notes.


## How does this paper compare to other research in the same field?

Here are a few key points about how this paper compares to other research on multitrack music transcription:- It proposes a new model architecture called Perceiver TF that combines aspects of Perceiver and SpectTNT models. This is a novel approach compared to prior work like MT3 that uses a standard Transformer encoder-decoder. The Perceiver TF model aims to improve scalability for handling many instruments simultaneously.- The paper introduces using random mixing augmentation for training, which is commonly used in music source separation but not as prevalent in prior automatic music transcription (AMT) research. Random mixing is shown to improve instrument discrimination in a multi-instrument setting.- The model is evaluated on multiple datasets including one with vocal melodies (MIR-ST500), whereas most prior AMT research focuses on transcribing regular instruments only. The proposed model handles vocals and instruments in a unified framework.- Results show state-of-the-art performance compared to prior work like MT3 and SpectTNT. The gains are especially notable on less common instruments and in the vocal transcription task.- The focus is on piano-roll based transcription rather than sequence-to-sequence models like MT3. The authors argue piano-roll models are easier to train with partial labels and have faster inference.Overall, this paper pushes forward the state-of-the-art in multitrack AMT research through architectural innovations and training strategies. The experiments demonstrate improvements in model scalability, handling diverse instruments including voice, and robustness to timbral variations.
