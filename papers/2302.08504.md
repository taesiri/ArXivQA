# [PersonNeRF: Personalized Reconstruction from Photo Collections](https://arxiv.org/abs/2302.08504)

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper addresses is how to take an unstructured photo collection of a person with varying poses, viewpoints, and appearances over time and build a personalized 3D model that can render novel combinations of these attributes not directly observed in the input images. The key hypotheses are:1) Modeling a canonical neural volumetric representation of the person in a standard pose allows using a shared pose-dependent motion field to map different observed poses back to this canonical space, enabling generalization across sparse pose observations. 2) Embedding the diverse appearances into a latent space and conditioning the canonical volume on appearance will enable the model to synthesize realistic textures for novel viewpoints by sharing information across the appearances through the latent space.3) Regularizing the geometry to be locally smooth and enforcing an opacity loss will improve the canonical volume's shape and appearance consistency across viewpoints despite having only sparse viewpoint observations per appearance.In summary, the paper aims to show that sharing information across sparse observations in pose, viewpoint, and appearance through a canonical-pose volumetric representation, appearance embeddings, and appropriate regularization losses can enable high-quality free-viewpoint rendering from extremely challenging unstructured photo collections.


## What is the main contribution of this paper?

The main contribution of this paper is developing PersonNeRF, a method that takes an unstructured photo collection of a person with varying poses and appearances, and enables rendering the person under novel combinations of viewpoint, body pose, and appearance. The key ideas are:- Representing the person as a canonical neural radiance field (NeRF) volume in a T-pose, and using a shared motion field to map it to different observed poses. This allows modeling diverse appearances that all rely on the same underlying motion model. - Regularizing the canonical volume's geometry using losses applied to renderings of the volume from unseen viewpoints. This results in higher quality geometry from sparse view observations.- Modeling appearance using a latent appearance code, which enables rendering high-quality textures for unobserved regions by sharing information across the person's appearances. - Building a personalized space for the subject by mapping coordinates of viewpoint, pose, and appearance to corresponding renderings. This supports intuitive exploration of novel combinations of these attributes.So in summary, the main contribution is developing a personalized neural volumetric representation and optimization approach that enables high-quality free-viewpoint rendering of a person from an unstructured photo collection with varying poses and appearances. The method combines ideas like canonical volumes, motion fields, appearance embedding, and novel view regularization to address this very challenging sparse data scenario.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper presents PersonNeRF, a method that builds a personalized 3D model of a person from an unstructured photo collection spanning different times, enabling rendering of the person from novel combinations of viewpoint, body pose and appearance.
