# [Reflect-RL: Two-Player Online RL Fine-Tuning for LMs](https://arxiv.org/abs/2402.12621)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
Large language models (LLMs) have shown promising capabilities in problem solving and reasoning. However, their ability to make effective multi-step decisions through interaction with environments is still limited. Most prior works use offline supervised learning, which is insufficient for learning complex behaviors. There is a need for methods that can allow LMs to dynamically adapt online using reinforcement learning (RL).  

Proposed Solution - RL Fine-Tuning (RLFT):
The paper proposes RLFT, a novel approach to improve LMs using online RL with Markov decision processes for multi-step decision making. The key ideas are:

1) Reflection-aided decision making: A frozen pretrained "reflection" model generates textual reflections on states to help guide the policy model. This speeds up training. 

2) Negative example generation: Perturbing optimal trajectories creates more robust reflections by improving error-correction capabilities.

3) Single-prompt action enumeration: Valid actions are enumerated, allowing the model to choose by generating only a single token, ensuring valid actions and reducing complexity.  

4) Task-specific curriculum learning: Additional reward signals at milestones enable efficient learning for long-horizon sparse reward tasks.

The method has a 2-stage training process - supervised pretraining, followed by online RL fine-tuning.

Contributions:
1) Proposes RLFT - a new approach to dynamically fine-tune LMs for multi-step decision making using ideas like reflection and curriculum learning.

2) Introduces AutoExplore, a new benchmark motivated by applications like software repositories and databases.

3) Empirical results show RLFT outperforms both offline supervised learning and online RL without reflection. Fine-tuned GPT-2 models also outperform untuned models like Mistral 7B on the benchmarks.

The method opens opportunities for effectively training LMs locally to adapt to interactive environments beyond static supervised datasets.
