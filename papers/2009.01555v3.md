# [Sample-Efficient Automated Deep Reinforcement Learning](https://arxiv.org/abs/2009.01555v3)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be: How can we develop an automated reinforcement learning framework that is sample-efficient and capable of dynamically optimizing hyperparameters as well as the neural architecture?The key hypotheses appear to be:1) A population-based approach with shared experience replay can enable more sample-efficient hyperparameter optimization compared to standard methods like random search. 2) Simultaneously optimizing the neural architecture together with hyperparameters can improve performance compared to using a fixed architecture.3) Dynamic configuration adaptation through an evolutionary approach can help deal with the non-stationarity of the RL problem compared to static configuration.The authors propose the Sample-Efficient Automated Reinforcement Learning (SEARL) framework to address these hypotheses. SEARL uses a population of agents with a shared replay buffer to increase sample efficiency. It also evolves the neural architecture and dynamically adapts hyperparameters through mutations. The case study on optimizing TD3 shows SEARL can find well-performing configurations using 10x fewer samples than alternative approaches.In summary, the key research question is how to do efficient automated hyperparameter and architecture search for RL, with the main hypotheses relating to using a population, shared experience, and dynamic configuration. SEARL is proposed and evaluated as a method for tackling this problem.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contribution seems to be a framework called Sample-Efficient Automated RL (SEARL) for automating and optimizing deep reinforcement learning algorithms. The key aspects of SEARL include:- It performs automated hyperparameter and neural architecture optimization for off-policy RL algorithms using an evolutionary approach with a population of agents.- It shares experiences between the population via a shared replay memory. This allows for more sample-efficient optimization, reducing the number of environment interactions needed. - It enables dynamic optimization of hyperparameters over time to account for the non-stationarity of the RL problem.- It modifies the neural architecture online during training by adding/removing layers and nodes in an incremental way that preserves trained weights.- It is simple to apply to any off-policy deep RL algorithm without much modification.The authors demonstrate SEARL on optimizing the TD3 RL algorithm on MuJoCo benchmarks, showing substantial improvements in sample efficiency (10x fewer environment interactions) compared to baselines like population-based training and random search. They also demonstrate it on optimizing DQN for Atari games.In summary, the main contribution is a way to do automated, sample-efficient, and dynamic hyperparameter and architecture optimization for deep RL algorithms using principles like experience sharing and incremental architecture search. This improves applicability of deep RL methods by removing the need for manual tuning.
