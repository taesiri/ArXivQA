# [Complex Reasoning over Logical Queries on Commonsense Knowledge Graphs](https://arxiv.org/abs/2403.07398)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Event commonsense reasoning requires understanding relationships between events and inferring implicit context, but data scarcity makes it hard for language models to learn to generate commonsense inferences for contexts and questions with complex event interactions.
- Existing commonsense reasoning benchmarks have limitations like simpler reasoning structures, distant supervision, small scale, or expensive human annotations.

Proposed Solution: 
- The paper presents COMCOM, a new dataset created by sampling multi-hop logical queries from the ATOMIC commonsense knowledge graph and verbalizing them into natural language. 
- The queries involve conjunction and existential quantification to capture event intersections and projections for common causes/effects and abductive reasoning.
- The verbalized narratives are turned into multiple choice and text generation questions with answer options.

Key Contributions:
- Pipeline to sample and verbalize complex logical queries from commonsense KGs with minimal human effort. 
- New COMCOM benchmark for complex, multi-hop commonsense reasoning with 790K questions, including 1.3K manually verified.
- Experiments showing state-of-the-art models still struggle on COMCOM's complexity.
- Models trained on COMCOM substantially improve on 8 commonsense QA and inference tasks, demonstrating efficacy of the framework.

In summary, the paper introduces a valuable new benchmark to advance complex commonsense reasoning, with a data generation framework that produces high-quality training data without expensive annotations. Experiments validate the difficulty of COMCOM and the benefits of training on it.


## Summarize the paper in one sentence.

 Here is a one-sentence summary of the paper:

The paper presents a new complex commonsense reasoning dataset called Com2, constructed by sampling and verbalizing multi-hop logical queries from the ATOMIC commonsense knowledge graph into question-answer pairs, without needing additional human annotations.


## What is the main contribution of this paper?

 The main contributions of this paper are three-fold:

1. It presents a pipeline for sampling and verbalizing complex logical queries from commonsense knowledge graphs to construct a complex commonsense reasoning dataset, \dataname{}, with minimal human effort. 

2. It benchmarks the complex reasoning ability of various state-of-the-art language models and question answering models on \dataname{}.

3. It validates the benefit of fine-tuning on \dataname{} for improving performance on eight diverse downstream commonsense reasoning tasks.

In summary, the paper introduces a method to create a complex reasoning dataset efficiently, shows that even advanced models struggle on this dataset, and demonstrates that fine-tuning on the dataset helps improve commonsense reasoning on other tasks. The key innovation is using complex logical queries on commonsense KGs to get reasoning examples without extra human annotations.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and keywords associated with this paper include:

- Complex commonsense reasoning
- Multi-hop reasoning
- Logical queries
- Knowledge graphs
- Conjunctive logical queries 
- Projection queries
- Intersection queries
- Commonsense reasoning datasets
- Question answering 
- Generative commonsense reasoning
- Zero-shot evaluation
- Event reasoning
- Narrative reasoning

The paper introduces a new dataset called COM2 for complex commonsense reasoning, created by sampling multi-hop logical queries from a commonsense knowledge graph. It studies different types of queries like projections and intersections and verbalizes them into natural language for question answering and commonsense inference. The paper shows improvements in complex reasoning abilities by fine-tuning on this dataset and evaluates the transfer learning performance on other commonsense QA and inference tasks. Some key aspects explored are multi-hop, zero-shot and generative commonsense reasoning over events and narratives.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in this paper:

1. How does the paper address the issue of sparsity in commonsense knowledge graphs when sampling complex queries? What specific techniques are used for node normalization and merging?

2. What is the motivation behind using a plausibility filter based on Vera for filtering low-quality triples before sampling complex queries? How is the threshold for filtering determined? 

3. What are the key differences between the rule-based verbalizer and the ChatGPT-based verbalizer for complex queries? What are the tradeoffs between them?

4. How are the negative examples and distractors sampled for each query during data construction? What strategies are used to make them challenging?

5. What qualifications did the authors use for selecting annotators on Amazon Mechanical Turk? How was inter-annotator agreement ensured during human annotation?

6. Why is the performance on pi queries significantly higher than other complex query types like ip? What explanations are provided for this phenomenon?

7. How does the dataset construction methodology used for ParaCOMET and COMET-M differ from the approach proposed for the complex commonsense reasoning dataset?

8. What types of common reasoning scenarios can two-way intersection (2i) queries represent? How do they relate to defeasible reasoning?

9. What were some common errors observed in the human evaluation of generated inferences by the Llama model? How did fine-tuning on complex queries alleviate them?

10. What are some limitations of the proposed dataset construction methodology? How can future work expand on this approach for creating even more complex and diverse commonsense reasoning benchmarks?
