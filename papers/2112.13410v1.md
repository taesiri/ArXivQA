# [Generative Kernel Continual learning](https://arxiv.org/abs/2112.13410v1)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading, the key research question this paper seeks to address is: 

How can we develop an effective continual learning system that avoids catastrophic forgetting, but does not rely on maintaining an explicit episodic memory of previous tasks?

The paper proposes a "generative kernel continual learning" approach that aims to get the benefits of kernel continual learning in terms of avoiding catastrophic forgetting, while replacing the need for an episodic memory with a generative model. 

Specifically, the paper investigates using a variational autoencoder as the generative model, which allows generating pseudo-samples from previous tasks to construct the kernels for kernel continual learning. This removes the dependence on an episodic memory while still enabling the model to tackle catastrophic forgetting and task interference.

The central hypothesis seems to be that by synergizing kernels and generative models in this way, it will be possible to get a highly effective continual learning system that avoids the scalability issues around maintaining an episodic memory, while still leveraging the strengths of kernels for avoiding catastrophic forgetting. The experiments aim to validate whether this hybrid approach can surpass previous state-of-the-art continual learning techniques.

In summary, the key research question is whether replacing the episodic memory in kernel continual learning with a generative model can lead to better scalability and performance in continual learning across multiple non-stationary tasks. The proposed generative kernel continual learning approach aims to test this hypothesis.
