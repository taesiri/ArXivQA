# [Sparse Spiking Neural Network: Exploiting Heterogeneity in Timescales   for Pruning Recurrent SNN](https://arxiv.org/abs/2403.03409)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Recurrent Spiking Neural Networks (RSNNs) are computationally complex models that are hard to optimize and deploy on hardware with limited resources. 
- Traditional methods to get sparse RSNNs have drawbacks:
    - They first train a dense, complex RSNN, then prune neurons/synapses based on activity to reduce computations while minimizing performance drop. 
    - Adapted from DNN pruning methods so don't leverage unique spiking neuron dynamics.
    - Task-dependent, so complexity of pruned models varies across tasks/datasets.
    - Pruned models tend to overfit to the task they are trained on.

Proposed Solution:
- Presents a \textbf{task-agnostic method called Lyapunov Noise Pruning (LNP)} to design sparse Heterogeneous RSNNs (HRSNNs) 
    - Starts with a randomly initialized, dense HRSNN 
    - Uses graph sparsification and Lyapunov exponents to prune synapses/neurons while keeping network stable
    - Retains delocalized eigenvectors for stability 
    - Optimizes neuronal timescales using Lyapunov spectrum
    - Does not consider task performance during pruning
- Can train resulting sparse HRSNN on different tasks using supervised or unsupervised methods

Main Contributions:
- Novel LNP algorithm that leverages Lyapunov spectrum and spectral graph methods to prune dense HRSNNs while ensuring stability
- Utilizes diversity of neuronal timescales in HRSNNs to assist pruning
- Task-agnostic method optimizes model structure/parameters during pruning without targeting performance on specific dataset
- Shows pruned model can be trained for different tasks like image classification and time-series prediction
- Outperforms traditional activity-based pruning in computational efficiency and performance across tasks
- Provides stable, efficient, adaptable sparse models without extensive task-specific tuning

In summary, the paper introduces an innovative task-agnostic pruning technique for recurrent spiking neural networks to develop sparse yet efficient models that can generalize well across tasks. The proposed LNP algorithm specifically caters to the unique dynamics of heterogeneous spiking networks.
