# [Speculative Streaming: Fast LLM Inference without Auxiliary Models](https://arxiv.org/abs/2402.11131)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Inference of large autoregressive language models like LLMs is slow due to their sequential nature, posing challenges for latency-sensitive applications. 
- Existing speculative decoding methods use a separate small "draft" model for speculation and a large "target" model for verification. This has drawbacks like training complexity, hosting two models, and misalignments. 

Proposed Solution:  
- The paper proposes "Speculative Streaming", a single model approach that unifies speculation and verification within the target model itself. 
- It incorporates multi-stream attention to enable the model to concurrently verify past tokens and speculate future tokens in one forward pass.  
- It initializes speculation streams using main stream states and stream embeddings. 
- It forms speculative token trees to increase candidates. Prunes less probable tokens using early exit predictions.
- The method is trained end-to-end by changing the objective from next token to n-gram prediction.

Main Contributions:
- Achieves 1.8-3x speedups across tasks without quality loss or separate draft models.
- Shows comparable or better performance than two-model speculation and Medusa method with 10000x fewer parameters. 
- Simplifies training and deployment by removing the need for separate draft models.
- Analyzes tradeoffs compared to two-model speculation. Shows higher kernel utilization and efficiency.
- Demonstrates consistent speedups across model sizes and diverse tasks like summarization, SQL generation etc.

In summary, the paper presents Speculative Streaming, a simple yet effective single model approach for fast inference of large language models that unifies speculation and verification without needing auxiliary models.
