# [A Broad Dataset is All You Need for One-Shot Object Detection](https://arxiv.org/abs/2011.04267)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be:Can training object detection models on datasets with a large number of object categories help close the "generalization gap" between performance on known vs novel categories in few-shot/one-shot detection settings?The key hypothesis appears to be that increasing the number of object categories used for training will improve the generalization capabilities of the learned feature representations, allowing the models to better detect and recognize new objects from just one or a few examples. The authors operationalize this question by training object detection models on datasets with varying numbers of categories - from 20 classes in Pascal VOC up to over 1000 classes in LVIS and Objects365. They compare performance on known training classes versus held out test classes not seen during training.The central finding is that the relative performance on novel classes improves substantially as the number of training classes increases - indicating that broad datasets help "close" the generalization gap. This supports their hypothesis that training with more object categories leads to more generalizable representations.In summary, the main research question is whether broader training sets with more categories can improve generalization in one-shot object detection, which the authors address through experiments manipulating the training set size and measuring generalization performance.


## What is the main contribution of this paper?

The main contribution of this paper is showing that the generalization gap in one-shot object detection (i.e. the difference in performance between known vs novel categories) can be significantly reduced by training on datasets with more object categories. Specifically, the key findings are:- There is a large generalization gap when training on COCO (80 categories), with performance on novel categories being less than half of that on known categories.- Increasing the number of training categories to 365 (Objects365 dataset) improves relative performance from 45% to 76%. Further increasing to 1200 categories (LVIS dataset) pushes relative performance to 89%, nearly closing the gap.- This effect is shown to be caused by the number of categories, not just the amount of data. More categories help even when controlling for number of instances.- Once the generalization gap is reduced, techniques like stronger backbones now improve performance on both known and novel categories. This allows reaching new state-of-the-art on COCO by training on LVIS.- The results hold for different model architectures and configurations, suggesting the findings apply broadly.In summary, the key insight is that training on more diverse datasets with more categories is crucial for learning representations that generalize better to novel objects in few-shot detection. This implies creating broader datasets should be a priority over collecting more data per category.
