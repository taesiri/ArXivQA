# [Scan and Snap: Understanding Training Dynamics and Token Composition in   1-layer Transformer](https://arxiv.org/abs/2305.16380)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central hypothesis of this paper is that the self-attention mechanism in Transformers acts as a discriminative scanning algorithm during training, progressively focusing on key tokens that are most relevant for predicting the next token. Specifically, the paper analyzes the training dynamics of a 1-layer Transformer on a next token prediction task. Under certain assumptions, including no positional encoding, a long input sequence, and a faster learning decoder layer, the paper mathematically proves that self-attention gravitates towards tokens that:1) Are "distinct", meaning they only occur in the context of a specific next token, rather than "common" tokens that occur across contexts. 2) Have high co-occurrence frequency with the next token being predicted, compared to distinct tokens with lower co-occurrence.3) Were less attended to initially, following a low-to-high attention order based on the above co-occurrence frequency. So in essence, the self-attention mechanism demonstrates an inductive bias towards scanning for and focusing on the most relevant distinct tokens for prediction, acting like a discriminative feature selector. This provides a rigorous explanation for how self-attention learns to focus on key parts of the input during training.


## What is the main contribution of this paper?

The paper presents a theoretical analysis of the training dynamics of a 1-layer transformer model on a next token prediction task. The key contributions are:1. It proves that the self-attention layer acts as a "discriminative scanning algorithm" during training. It learns to pay more attention to key tokens that are distinct (uniquely occur) for a specific next token, and less attention to common tokens across next tokens.2. Among the distinct tokens, it progressively drops attention weights following the order of increasing co-occurrence frequency between the key and query tokens. This provides a frequency bias. 3. However, the attention weights do not collapse to a winner-takes-all sparse pattern. There is a phase transition controlled by the learning rates that causes the attention weights to become (almost) frozen, leaving a fixed token composition. 4. This overall "scan and snap" training dynamics of the self-attention layer is formalized mathematically and demonstrated through experiments on synthetic and real text data.5. The analysis reveals interesting properties of the self-attention layer's inductive bias when trained on data using gradient descent. It sheds light on how the layer learns to focus on and combine relevant tokens in the input.In summary, this paper provides a theoretical characterization and understanding of the training dynamics of self-attention in transformers, revealing its inherent discriminative, frequency-biased, and snap-stabilizing behaviors. The analysis offers insights into how the model learns representations from data.
