# [Score-based Causal Representation Learning with Interventions](https://arxiv.org/abs/2301.08230)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- The paper studies causal representation learning when the latent causal variables are related to the observed variables through an unknown linear transformation. 
- The objectives are to: (1) recover the unknown linear transformation up to scaling, and (2) determine the directed acyclic graph (DAG) structure among the latent variables.

Proposed Solution: 
- Leverages the changes in the score function (gradient of log-likelihood) across different interventional environments. 
- Shows that under certain assumptions, the effect of an intervention can be detected from the changes in the score function.
- Key insight: Any valid transformation renders the score functions of latent variables to have minimal variations across interventional environments. This property is used to recover the transformation and DAG structure.

Contributions:

1. For linear transformation and stochastic hard interventions covering all latent variables:
   - Identifies transformation up to coordinate-wise scaling and permutation consistent with valid causal ordering.
   - Perfectly recovers latent DAG structure.

2. For linear transformation and stochastic soft interventions on every node: 
   - Identifies transformation up to a mixing-consistency equivalence class. 
   - Recovers latent DAG up to a permutation consistent with topological ordering.
   - Estimated latent variables are Markov to recovered DAG.

- Soft interventions suffice for recovering latent DAG which existing works require hard interventions. 

- Applicable to non-linear relationships in latent causal variables unlike existing works focused on linear models.

In summary, the paper provides theoretical guarantees for recovering interpretable latent representations and the underlying causal structure from interventional data, under mild assumptions.
