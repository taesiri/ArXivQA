# [Bolstering Stochastic Gradient Descent with Model Building](https://arxiv.org/abs/2111.07058v2)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question seems to be:

How can second-order information be incorporated into stochastic gradient descent to make it more efficient, adaptive and robust? 

Specifically, the authors propose a new algorithm called Stochastic Model Building (SMB) that uses quadratic model building to adjust both the step size and direction of stochastic gradient steps. The key ideas are:

- Build local quadratic models using stochastic function/gradient evaluations to capture curvature information around the current iterate. 

- Use these models to compute adaptive step sizes and directions for each group of parameters, making the steps more robust.

- The models are built using the latest stochastic gradients, so they capture the most recent curvature information unlike quasi-Newton approaches.

- Convergence analysis is provided for a variant SMBi where the models are built with independent gradients.

- Experiments on neural network models demonstrate faster convergence and more robustness of SMB compared to SGD, Adam and other baselines.

So in summary, the central hypothesis is that stochastic model building can make SGD more efficient and robust by incorporating local second-order information in an online manner. The results provide evidence for this hypothesis and demonstrate the potential of SMB.


## What is the main contribution of this paper?

 Based on my reading, the main contributions of this paper are:

- It proposes a new stochastic optimization algorithm called Stochastic Model Building (SMB). The key idea is to use quadratic model building to adaptively adjust the step size and direction of the stochastic gradient steps. This is different from standard stochastic line search methods that only adjust the step size. 

- The model building is done in a "diagonal" way, where separate quadratic models are built for each group of parameters (e.g. layers in a neural network). This allows the step lengths to be adaptive for each group.

- The paper provides a convergence analysis for a variant of SMB called SMBi, where the model is built using an independent sample batch. This allows the analysis to follow the standard framework for stochastic quasi-Newton methods.

- Extensive numerical experiments are provided comparing SMB against SGD, Adam, and other baselines on various deep learning models and datasets. The results show SMB achieves faster convergence and better generalization performance in many cases.

- The paper argues that SMB requires less hyperparameter tuning than standard SGD methods and shows comparable or better performance than adaptive methods like Adam. The experiments also suggest SMB is more robust to choice of learning rate.

Overall, the main contribution is proposing the SMB algorithm and its diagonal model building approach, along with supporting convergence theory and strong experimental results showing its promise as an optimization method for deep learning and other stochastic problems. The adaptive nature of SMB and its performance improvements over SGD are the key aspects emphasized.
