# [Cappy: Outperforming and Boosting Large Multi-Task LMs with a Small   Scorer](https://arxiv.org/abs/2311.06720)

## Summarize the paper in one sentence.

 The paper introduces Cappy, a small pretrained scorer that enhances the performance and efficiency of large multi-task language models.


## Summarize the paper in one paragraphs.

 Here is a one paragraph summary of the paper:

This paper introduces Cappy, a small pretrained scoring model designed to enhance the performance and efficiency of large multi-task language models (LLMs). Cappy is pretrained via weakly supervised regression to estimate the correctness of a response to a given instruction. With just 360 million parameters, Cappy can function independently on classification tasks or serve as an auxiliary component for LLMs on generation tasks. Experiments show that Cappy outperforms LLMs orders of magnitude larger on 11 language understanding tasks. On 45 complex BIG-Bench tasks, Cappy substantially improves the performance of the advanced multi-task LLM FLAN-T5 across various sizes, without needing to fine-tune the LLM. Cappy enables efficiently adapting LLMs to downstream tasks through finetuning Cappy alone on downstream data, circumventing LLM tuning. Furthermore, Cappy is flexible to collaborate with other LLM adaptations like finetuning and in-context learning, providing additional gains. Overall, Cappy offers an effective and lightweight approach to enhance multi-task LLMs.


## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a high-quality paragraph summarizing the key points of the paper:

The paper introduces Cappy, a small pretrained scorer designed to enhance the performance and efficiency of large multi-task language models (LLMs). Cappy has only 360 million parameters but can outperform LLMs orders of magnitude larger on a variety of language understanding tasks. Cappy takes an instruction and candidate response as input and outputs a score from 0 to 1 judging the response's correctness. It is pretrained via weakly supervised regression using constructed training data and model generations. Cappy functions in two modes - independently for classification via answer selection, or cooperatively as an auxiliary component for LLMs to select their best generation. Experiments demonstrate Cappy matches massive LLMs like OPT-175B on 11 PromptSource tasks and boosts FLAN-T5 substantially on 45 complex BIG-Bench tasks. Cappy enables efficiently adapting LLMs on downstream tasks without finetuning or accessing their parameters. It is also flexible to work jointly with LLM adaptations like finetuning and in-context learning for further gains. The paper ablates key components like Cappy's pretraining and data augmentation. Limitations include reliance on Rouge-L for weak supervision and performance gaps on mathematical/logical reasoning. Future work may refine Cappy's answer aggregation and explore human feedback.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 The paper introduces Cappy, a small pretrained scorer that can enhance the performance and efficiency of large multi-task language models. Cappy scores the correctness of candidate responses to instructions, allowing it to select the best response from a large LM or work independently on classification tasks.


## What is the central research question or hypothesis that this paper addresses?

 This paper introduces Cappy, a small pretrained scorer designed to enhance the performance and efficiency of large language models (LLMs) for multi-task learning. The central hypothesis is that Cappy can improve multi-task LLMs in two key ways:

1. When used independently, Cappy can outperform much larger multi-task LLMs on a variety of language understanding tasks, demonstrating superior performance and parameter efficiency. 

2. When used in conjunction with multi-task LLMs, Cappy can boost their performance on complex downstream tasks through a flexible adaptation approach that does not require fine-tuning the LLM or access to its parameters.

The paper tests these two hypotheses through experiments on language understanding tasks from PromptSource and complex generation tasks from BIG-Bench. The results support both hypotheses, showing Cappy matching or exceeding far larger models when used alone, and consistently improving state-of-the-art multi-task LLMs when used as an auxiliary scorer. Overall, Cappy demonstrates the ability of a small pretrained model to enhance large language models for multi-task learning in terms of both efficiency and effectiveness.


## What is the main contribution of this paper?

 The main contribution of this paper is proposing Cappy, a small pretrained scorer that can enhance the performance and efficiency of large multi-task language models (LLMs). Specifically:

- Cappy is a lightweight model (360M parameters) that scores the correctness of a response to an instruction. It is pretrained via weakly supervised regression using constructed training data.

- Cappy can work independently to solve tasks in a candidate selection style, outperforming much larger LLMs on 11 classification tasks. 

- When paired with a multi-task LLM, Cappy serves as an auxiliary scorer to boost the LLM's performance on 45 complex generation tasks from BIG-Bench, without needing to fine-tune the LLM.

- Cappy enables efficient LLM adaptation by incorporating downstream training data into its scoring, circumventing the need for LLM fine-tuning. It is also compatible with other LLM adaptations like finetuning and in-context learning.

- The proposed pretraining strategy and data augmentation method are shown to be important for Cappy's effectiveness through ablation studies.

In summary, the key contribution is using the lightweight Cappy scorer to enhance large multi-task LLMs in terms of performance, efficiency, and adaptability to downstream tasks.
