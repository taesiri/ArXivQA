# [Compact Speech Translation Models via Discrete Speech Units Pretraining](https://arxiv.org/abs/2402.19333)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Using self-supervised learning (SSL) models like wav2vec2 and HuBERT for speech translation (ST) gives state-of-the-art results, but they have a large memory footprint which hinders on-device deployment. 
- Knowledge distillation to create smaller models requires expensive high-quality pseudo labels for training.
- Using discrete speech units (DSUs) extracted from SSL models is cheaper and are intermediate representations between speech and text, but using them as inputs has downsides: requires slow SSL+clustering inference pipeline, sensitive to tokenization.

Proposed Solution:
- Leverage SSL models by pretraining smaller models on DSUs instead of using DSUs as inputs.
- Pretrain encoder-decoder models on: 
  1) Filterbank-to-DSU (Fbk-to-DSU) 
  2) DSU-to-Translation (DSU-to-Trl)
- Take encoder from 1) and decoder from 2) to initialize a new compact model.
- Finetune this model on limited speech-translation data.

Benefits:
- Compact model with no need for SSL model at inference time
- Avoids issues with using DSUs as inputs
- Works for low-resource languages without transcripts

Key Contributions:  
- Method to distill SSL models into compact ST models via DSU pretraining
- Avoid lengthy pipeline needed when using DSUs as inputs
- Analysis showing method is robust to DSU tokenization unlike using DSUs as inputs
- Experiments on CoVoST-2 showing gains over finetuning SSL model directly and on par with ASR pretraining
- Analysis on using CTC in pretraining and finetuning to mitigate modality gap

In summary, the paper proposes a way to effectively transfer knowledge from large SSL speech models into compact and performant speech translation models via pretraining on discrete speech units, demonstrating advantages over alternative transfer learning approaches.
