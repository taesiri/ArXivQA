# [Contractive error feedback for gradient compression](https://arxiv.org/abs/2312.08538)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Distributed training suffers from limited bandwidth (slow communication) and limited memory. Existing communication-efficient methods like error feedback SGD (EFSGD) are challenged by memory constraints.
- EFSGD relies on an additional error vector to ensure convergence, consuming extra memory the size of the model. This hinders adoption especially with large models.

Proposed Solution: 
- The paper proposes Contractive Error Feedback (ConEF) to make EFSGD memory efficient. 
- The key idea is to compress the error vector using a lightweight compressor like count sketch. This saves memory with little overhead.

Main Contributions:
- ConEF saves 80-90% of EFSGD's extra memory on tasks like image classification and language modeling, with similar runtime.
- ConEF achieves 1.3-5x speedup over SGD, making it communication efficient. 
- ConEF establishes tradeoff between memory efficiency and faster convergence. It finds the sweet spot between the two extremes of EFSGD (fast but memory intensive) and QSGD (memory efficient but slower convergence).
- Theoretical convergence is proved for ConEF. Generalization ability is also analyzed.
- Guidance provided on choosing hyperparameters like sketch size, number of rows/columns for count sketch based on experiments.

In summary, the paper makes EFSGD memory efficient via contractive error feedback, while retaining communication efficiency. This facilitates adoption of EF methods for distributed training of large models under memory constraints.
