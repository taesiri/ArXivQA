# [On Task Performance and Model Calibration with Supervised and   Self-Ensembled In-Context Learning](https://arxiv.org/abs/2312.13772)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Recent advances in large language models (LLMs) have led to promising performance with supervised fine-tuning (SFT) and in-context learning (ICL). However, both paradigms suffer from the critical problem of overconfidence (miscalibration), especially in low-resource scenarios.
- It remains unclear which learning paradigm performs better in terms of task performance and calibration. Also, it is an open question whether simultaneous gains in both metrics can be achieved.

Methodology:
- The authors conduct a comprehensive analysis comparing SFT, ICL and variants like supervised ICL (SICL) across 7 classification tasks in low-resource setups.
- They analyze both task performance metrics and calibration error (ECE - expected calibration error).
- To address miscalibration, they propose using self-ensembling by creating variations in in-context examples or prompts and aggregating predictions.

Key Findings:
- Task performance and calibration are correlated in a task-dependent way. ICL can match SFT on some 'seen' tasks but not on 'unseen' tasks.
- All methods suffer from miscalibration, especially in low-resource scenarios. 
- Self-ensembling boosts calibration without compromising task performance. Combining variations in both prompts and examples works best.

Main Contributions:
- In-depth analysis of the interplay between performance and calibration of different LLM learning paradigms.
- Actionable guidelines on choosing suitable methods based on task properties.
- Demonstrating the efficacy of tailored self-ensembling strategies in enhancing both metrics simultaneously.
