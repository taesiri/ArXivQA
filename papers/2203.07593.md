# [Distraction is All You Need for Fairness](https://arxiv.org/abs/2203.07593)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, it seems the central research question is how to train deep learning models to make fair and accurate predictions, even when the training data contains biases. The paper proposes a new approach called the "Distraction module" to control for bias during model training. The key hypotheses appear to be:1) The proposed Distraction module can theoretically be proven to help learn classifiers that are invariant to protected attributes like gender or race. 2) Adversarially training the Distraction module against the classifier network enables optimizing for both accuracy and fairness simultaneously.3) This approach can work with different data types like tabular data, graphs, and images. 4) The method can outperform current state-of-the-art techniques for bias mitigation across different datasets.5) The self-attention mechanism enables explainability and identification of proxy variables contributing to bias.So in summary, the central research question seems to be how to create an accurate and fair classifier using a novel adversarial training procedure with the proposed Distraction module. The key hypotheses are that this approach can achieve state-of-the-art performance on bias mitigation across data types while retaining accuracy and interpretability.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contributions appear to be:1. Proposing a new model called the Distraction module to mitigate data bias effects on neural networks for classification tasks. 2. Providing an adversarial training procedure with theoretical analysis to train networks with attention modules for fairness.3. Demonstrating that the proposed model with the new adversarial training procedure significantly improves state-of-the-art accuracy and fairness metrics over the accuracy-fairness tradeoff curve and statistical parity on various datasets - tabular (UCI Adult, Heritage Health), graph (Pokec-Z, Pokec-N, NBA), and vision (CelebA).4. Showing that the proposed method with the self-attention mechanism can increase explainability and help identify proxy attributes that contribute to bias.The key ideas seem to be using a Distraction module with isolated weights to minimize demographic parity loss while the main network focuses on maximizing classification accuracy, formulating it as a maximin game, and proving theoretically that the proposed approach can converge to a classifier that is both accurate and invariant to protected attributes. The method is demonstrated to achieve superior performance over existing state-of-the-art bias mitigation techniques on a diverse set of datasets. The use of self-attention also enables model explainability.In summary, the main contribution appears to be proposing a new and effective bias mitigation technique with strong theoretical grounding that can flexibly improve fairness across different data types while maintaining accuracy and interpretability.
