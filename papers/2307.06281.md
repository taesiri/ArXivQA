# MMBench: Is Your Multi-modal Model an All-around Player?

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, it seems the main research question is: How can we develop a systematic benchmark to robustly evaluate the various abilities of large vision-language models? The key points related to this question are:- Existing benchmarks like VQA and COCO have limitations in providing fine-grained ability assessment and use non-robust evaluation metrics. - Recent subjective benchmarks with human evaluation are biased and not scalable. - This paper proposes MMBench, a novel objective benchmark with the following goals:1. Systematically evaluate vision-language models across diversified abilities using a hierarchical taxonomy.2. Enable robust evaluation by using a novel CircularEval strategy and leveraging ChatGPT for comparing free-form predictions to choices.So in summary, the central focus is on creating a rigorous benchmark that can thoroughly and robustly assess the multifaceted capabilities of vision-language models, overcoming limitations of prior evaluation methods. The key contribution is the systematic design of the benchmark itself.


## What is the main contribution of this paper?

This paper introduces MMBench, a new benchmark for evaluating the capabilities of vision-language models. The main contributions are:1. Systematically-constructed dataset: MMBench contains thousands of multiple-choice questions covering 20 fine-grained abilities across perception and reasoning. The dataset is meticulously designed to provide a comprehensive and balanced evaluation. 2. Robust evaluation: The paper proposes a novel CircularEval strategy that evaluates models multiple times on rotated options, making the evaluation more robust. It also leverages ChatGPT for extracting choices from free-form predictions, enabling fair comparisons between models.3. Analysis and observations: The paper benchmarks 14 existing models on MMBench and analyzes their capabilities across different skills. The results provide valuable insights into the strengths and weaknesses of current models, guiding future research directions.In summary, the main contribution is the proposal of MMBench, which is the first systematic and robust benchmark for evaluating the diverse capabilities of vision-language models through thousands of carefully designed questions and a reliable evaluation protocol. MMBench can facilitate more meaningful comparisons and drive progress in this field.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key point from this paper:The paper proposes MMBench, a new benchmark for evaluating the perception and reasoning abilities of vision-language models, consisting of a diverse set of over 1,000 multiple choice questions and utilizing ChatGPT for robust evaluation of free-form model predictions.
