# [Influencing Bandits: Arm Selection for Preference Shaping](https://arxiv.org/abs/2403.00036)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper "Influencing Bandits: Arm Selection for Preference Shaping":

Problem:
The paper considers stochastic multi-armed bandits (MAB) used in recommendation systems. Typically, MAB algorithms assume user preferences are unaffected by the sequence of recommended items. However, in reality user preferences evolve over time based on the recommendations shown and rewards received. The paper studies this setting where user population preferences are reinforced positively or negatively by the observed rewards over time. The goal is to actively shape the population preferences, unlike typical MAB objectives of maximizing cumulative reward.

Proposed Solution:
The paper models the user population using an urn model that keeps track of preferences. It considers two models - decreasing influence dynamics (DID) where the population becomes more rigid over time, and constant influence dynamics (CID) where influence remains constant. For the DID model with 2 arms and types, it derives the optimal policy when the reward statistics are known. For unknown rewards, it proposes and analyzes two algorithms:

1) Explore-then-Commit: Explores uniformly initially to estimate rewards, then commits to optimal policy using the estimates. Provides logarithmic regret for a symmetric case. 

2) Thompson Sampling: Maintains Bayesian priors on rewards and samples estimated rewards in each round. Shows this also attains logarithmic regret, without needing time horizon.

It further shows the CID model leads to the same optimal policy and algorithm performance. Extensions consider an N arm/N type setting with optimal policy derivation and Thompson sampling algorithm. It also models competing recommendation systems and studies system popularity vs. preference shaping tradeoffs.

Main Contributions:
- Formalizes preference shaping in non-stationary contextual bandits by modeling evolving user population through urn model
- Derives optimal policy for known system, and analyzes two practical algorithms for unknown system
- Guarantees logarithmic regret in cumulative type 1 population for the algorithms
- Extends modeling and algorithms to more complex scenarios of N arms/types and competing systems  

In summary, the paper provides a rigorous treatment of preference shaping in stochastic bandits, with practical algorithms and theoretical performance guarantees. The notion of shaping user preferences over time by a recommendation system is an important modern consideration.
