# [Symmetry Breaking and Equivariant Neural Networks](https://arxiv.org/abs/2312.09016)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a summary paragraph of the key points from the paper:

This paper analyzes an important limitation of equivariant neural networks: their inability to break symmetry at the level of individual data samples. The authors argue that while exploiting symmetry as an inductive bias is useful, many learning tasks like modeling phase transitions in physics require symmetry breaking. They formally characterize this limitation by showing equivariant functions must preserve the symmetry of inputs. To address this, they introduce a relaxation called "relaxed equivariance" which allows mapping symmetric inputs to asymmetric outputs. They further demonstrate how to build relaxed equivariant multilayer perceptrons that can break symmetry through constraints on the weight matrices. Potential applications where symmetry breaking is relevant include physics, graph representation learning, combinatorial optimization, and decoding from invariant representations. Overall, this paper identifies an under-explored aspect of exploiting symmetry in deep learning and proposes both theory and methods for symmetry breaking when learning equivariant models.
