# [Generalizing Few-Shot NAS with Gradient Matching](https://arxiv.org/abs/2203.15207)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be: How can we improve the accuracy of neural architecture search methods that rely on weight sharing and one-shot models, especially for ranking top-performing architectures?The key hypothesis appears to be:By reducing the amount of weight sharing among child models in the one-shot supernet, specifically by splitting child models that have more dissimilar gradients/training dynamics into separate sub-networks, we can improve the accuracy of the supernet's performance estimates and get better neural architecture search results.In more detail:- Neural architecture search methods like ENAS, DARTS, etc rely on a one-shot model that shares weights between child models to reduce search costs. - But weight sharing leads to coupled training dynamics and inaccurate performance estimation, especially among top architectures.- The paper proposes a more general splitting approach to reduce weight sharing, using gradient similarity to decide which child models should share weights vs be separated.- The hypothesis is that splitting child models with more dissimilar gradients will improve the ranking of top architectures and the overall search results, while still being efficient.So in summary, the key question is how to improve weight sharing NAS methods, and the core hypothesis is that using gradient information to selectively reduce weight sharing will help address the limitations of full weight sharing approaches.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contributions appear to be:- Proposes a new method called Gradient Matching NAS (GM-NAS) to improve neural architecture search with weight sharing. - Utilizes gradient matching scores between child models as a criterion to determine which models should share weights vs be separated into different subnets. Models with more dissimilar gradients are separated.- Formulates the subnet partitioning as a graph clustering problem based on the gradient matching scores. This allows more flexible branching factors compared to prior work like Few-Shot NAS.- Demonstrates improved architecture search performance across a variety of search spaces (NASBench-201, DARTS, MobileNet), datasets (CIFAR-10, CIFAR-100, ImageNet), and base search methods compared to Few-Shot NAS and one-shot NAS baselines.- Achieves state-of-the-art results on NASBench-201 and competitive results on DARTS and MobileNet search spaces.In summary, the main contribution is proposing a novel method to partition the subnets during architecture search in a more informed way using gradient matching, leading to improved search performance over prior work. The gradient matching based partitioning is the key idea that enables more effective architecture search.
