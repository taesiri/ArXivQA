# [Generalizing Few-Shot NAS with Gradient Matching](https://arxiv.org/abs/2203.15207)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be: How can we improve the accuracy of neural architecture search methods that rely on weight sharing and one-shot models, especially for ranking top-performing architectures?The key hypothesis appears to be:By reducing the amount of weight sharing among child models in the one-shot supernet, specifically by splitting child models that have more dissimilar gradients/training dynamics into separate sub-networks, we can improve the accuracy of the supernet's performance estimates and get better neural architecture search results.In more detail:- Neural architecture search methods like ENAS, DARTS, etc rely on a one-shot model that shares weights between child models to reduce search costs. - But weight sharing leads to coupled training dynamics and inaccurate performance estimation, especially among top architectures.- The paper proposes a more general splitting approach to reduce weight sharing, using gradient similarity to decide which child models should share weights vs be separated.- The hypothesis is that splitting child models with more dissimilar gradients will improve the ranking of top architectures and the overall search results, while still being efficient.So in summary, the key question is how to improve weight sharing NAS methods, and the core hypothesis is that using gradient information to selectively reduce weight sharing will help address the limitations of full weight sharing approaches.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contributions appear to be:- Proposes a new method called Gradient Matching NAS (GM-NAS) to improve neural architecture search with weight sharing. - Utilizes gradient matching scores between child models as a criterion to determine which models should share weights vs be separated into different subnets. Models with more dissimilar gradients are separated.- Formulates the subnet partitioning as a graph clustering problem based on the gradient matching scores. This allows more flexible branching factors compared to prior work like Few-Shot NAS.- Demonstrates improved architecture search performance across a variety of search spaces (NASBench-201, DARTS, MobileNet), datasets (CIFAR-10, CIFAR-100, ImageNet), and base search methods compared to Few-Shot NAS and one-shot NAS baselines.- Achieves state-of-the-art results on NASBench-201 and competitive results on DARTS and MobileNet search spaces.In summary, the main contribution is proposing a novel method to partition the subnets during architecture search in a more informed way using gradient matching, leading to improved search performance over prior work. The gradient matching based partitioning is the key idea that enables more effective architecture search.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper proposes a new neural architecture search method called GM-NAS that improves upon prior Few-Shot NAS approaches by using gradient matching scores to more effectively partition the search space and reduce harmful weight sharing among child models.


## How does this paper compare to other research in the same field?

Here are a few thoughts on how this paper compares to other research in the field of neural architecture search:- This paper focuses on improving the accuracy of one-shot NAS methods like DARTS by reducing the degree of weight sharing between child models in the search space. Other recent papers like EvaluateNAS and Few-Shot NAS have identified inaccuracies in weight-sharing NAS methods, so this work is tackling a known and relevant problem.- The proposed technique of using gradient matching to determine which child models should share weights is novel. Previous work like Few-Shot NAS used exhaustive splitting strategies that are less targeted. Using gradients to inform splitting decisions in a principled way is an interesting idea.- The paper demonstrates strong empirical performance across multiple search spaces, datasets, and base NAS algorithms. Achieving state-of-the-art or comparable results to prior work shows the effectiveness and general applicability of the proposed technique.- The method appears relatively simple to implement on top of existing differentiable NAS methods. This contrasts with some other recent improvements that require more significant modifications to the overall NAS pipeline. Greater simplicity could aid adoption.- The authors have released code for their method, which will support reproducibility and extensions by other researchers. Releasing code is an important contribution.Overall, this paper makes solid contributions to an active research area by proposing a novel technique to address weight sharing inaccuracies. The strong empirical results across different contexts highlight the promise of the gradient matching approach. It offers useful ideas that move the field forward while building nicely on prior art.


## What future research directions do the authors suggest?

Based on my reading of the paper, some of the main future research directions suggested by the authors are:- Developing methods to better estimate the performance of neural architectures in weight-sharing NAS without coupled optimization between child models. The authors suggest this could further improve the accuracy of the search process.- Exploring how to effectively combine the proposed gradient matching approach with other techniques like search space pruning and distribution learning. The authors mention these are orthogonal techniques that could potentially be combined with gradient matching.- Applying the proposed method to additional NAS algorithms and benchmark tasks to further demonstrate its general applicability. The authors show results on several algorithms and tasks but suggest more comprehensive benchmarks could be useful.- Developing theoretical understandings of why and how gradient matching helps improve weight-sharing NAS. The empirical results demonstrate its effectiveness but formal analysis could provide additional insights.- Tuning the hyperparameters and implementation details of the gradient matching approach, such as the graph clustering algorithms, to further optimize performance. The authors use simple/intuitive choices but suggest refinements could help.- Extending the method to other weight-sharing scenarios like multi-task learning. The authors focus on NAS but suggest the idea could generalize.In summary, the main future directions are developing improved weight-sharing NAS methods, combining gradient matching with other techniques, more comprehensive empirical testing, theoretical analysis, hyperparameter tuning, and extending the approach to new domains. The authors provide promising initial results but suggest much more work can still be done to advance this research direction.


## Summarize the paper in one paragraph.

Here is a one paragraph summary of the paper:The paper proposes a method called Gradient Matching NAS (GM-NAS) to improve neural architecture search with weight-sharing. Existing methods like one-shot NAS use weight-sharing in a supernet to enable efficient search, but suffer from inaccurate performance estimation of child models due to coupled optimization. Few-Shot NAS reduces weight-sharing by splitting the supernet, but uses exhaustive partitioning which limits the number of splits. GM-NAS generalizes few-shot NAS by using gradient matching score to decide which child models should share weights. This allows flexible partitioning to reduce weight-sharing more effectively. GM-NAS models the partitioning as a graph clustering problem based on gradient similarity between child models. Experiments across multiple search spaces, datasets and base search algorithms demonstrate GM-NAS consistently improves one-shot and few-shot NAS in terms of performance of discovered architectures. The method achieves state-of-the-art results on CIFAR-10 and ImageNet datasets using the DARTS and MobileNet search spaces.


## Summarize the paper in two paragraphs.

Here is a two paragraph summary of the paper:The paper proposes a novel method called Gradient Matching NAS (GM-NAS) to improve neural architecture search with weight-sharing supernetworks. Weight-sharing allows training one supernetwork to approximate the performance of all architectures in the search space, drastically reducing search cost. However, due to coupled optimization between child models, the supernetwork suffers from inaccurate performance estimation, especially for top architectures. To address this, the paper generalizes Few-Shot NAS, which splits the supernetwork into separate sub-networks. Rather than naively partitioning each operation, GM-NAS leverages gradient matching scores to identify operations with dissimilar training dynamics that should not share weights. GM-NAS formulates splitting as a graph clustering problem, allowing flexible branching factors to split more layers given a budget. Extensive experiments demonstrate GM-NAS consistently outperforms Few-Shot NAS across various search spaces, datasets, and base methods. On CIFAR-10, GM-NAS achieves 2.34% test error on DARTS space, surpassing prior work. On ImageNet with ProxylessNAS, it reaches 23.4% top-1 error, beating Few-Shot NAS by 0.7%. Overall, GM-NAS effectively reduces harmful weight-sharing to improve neural architecture search performance. Its simple yet principled approach of using gradient matching for informed partitioning provides a general method for addressing inaccuracies caused by weight-sharing during architecture search.


## Summarize the main method used in the paper in one paragraph.

Here is a one paragraph summary of the main method used in the paper:The paper proposes a new neural architecture search method called Gradient Matching NAS (GM-NAS) that improves upon prior Few-Shot NAS methods. Few-Shot NAS reduces the amount of weight sharing between child models by splitting the one-shot supernet into multiple independent sub-supernets. However, it uses an exhaustive splitting schema that divides operations on each edge into separate sub-supernets, resulting in high branching factors that limit the number of edges it can split. GM-NAS generalizes the supernet splitting to allow arbitrary branching factors. The key idea is to use a gradient matching score between child models as the splitting criterion. Child models that produce more mismatched gradients at the shared modules are more harmful to share weights, and are assigned to different sub-supernets. This allows GM-NAS to use a lower branching factor and afford splitting more edges under a fixed budget. The splitting decision is made by formulating supernet partitioning as a graph clustering problem, where graph links are weighted by the gradient matching scores. Experiments across various search spaces, datasets and base NAS methods demonstrate GM-NAS consistently outperforms prior Few-Shot NAS approaches.
