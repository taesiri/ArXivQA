# [Generalizing Few-Shot NAS with Gradient Matching](https://arxiv.org/abs/2203.15207)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be: How can we improve the accuracy of neural architecture search methods that rely on weight sharing and one-shot models, especially for ranking top-performing architectures?The key hypothesis appears to be:By reducing the amount of weight sharing among child models in the one-shot supernet, specifically by splitting child models that have more dissimilar gradients/training dynamics into separate sub-networks, we can improve the accuracy of the supernet's performance estimates and get better neural architecture search results.In more detail:- Neural architecture search methods like ENAS, DARTS, etc rely on a one-shot model that shares weights between child models to reduce search costs. - But weight sharing leads to coupled training dynamics and inaccurate performance estimation, especially among top architectures.- The paper proposes a more general splitting approach to reduce weight sharing, using gradient similarity to decide which child models should share weights vs be separated.- The hypothesis is that splitting child models with more dissimilar gradients will improve the ranking of top architectures and the overall search results, while still being efficient.So in summary, the key question is how to improve weight sharing NAS methods, and the core hypothesis is that using gradient information to selectively reduce weight sharing will help address the limitations of full weight sharing approaches.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contributions appear to be:- Proposes a new method called Gradient Matching NAS (GM-NAS) to improve neural architecture search with weight sharing. - Utilizes gradient matching scores between child models as a criterion to determine which models should share weights vs be separated into different subnets. Models with more dissimilar gradients are separated.- Formulates the subnet partitioning as a graph clustering problem based on the gradient matching scores. This allows more flexible branching factors compared to prior work like Few-Shot NAS.- Demonstrates improved architecture search performance across a variety of search spaces (NASBench-201, DARTS, MobileNet), datasets (CIFAR-10, CIFAR-100, ImageNet), and base search methods compared to Few-Shot NAS and one-shot NAS baselines.- Achieves state-of-the-art results on NASBench-201 and competitive results on DARTS and MobileNet search spaces.In summary, the main contribution is proposing a novel method to partition the subnets during architecture search in a more informed way using gradient matching, leading to improved search performance over prior work. The gradient matching based partitioning is the key idea that enables more effective architecture search.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper proposes a new neural architecture search method called GM-NAS that improves upon prior Few-Shot NAS approaches by using gradient matching scores to more effectively partition the search space and reduce harmful weight sharing among child models.


## How does this paper compare to other research in the same field?

Here are a few thoughts on how this paper compares to other research in the field of neural architecture search:- This paper focuses on improving the accuracy of one-shot NAS methods like DARTS by reducing the degree of weight sharing between child models in the search space. Other recent papers like EvaluateNAS and Few-Shot NAS have identified inaccuracies in weight-sharing NAS methods, so this work is tackling a known and relevant problem.- The proposed technique of using gradient matching to determine which child models should share weights is novel. Previous work like Few-Shot NAS used exhaustive splitting strategies that are less targeted. Using gradients to inform splitting decisions in a principled way is an interesting idea.- The paper demonstrates strong empirical performance across multiple search spaces, datasets, and base NAS algorithms. Achieving state-of-the-art or comparable results to prior work shows the effectiveness and general applicability of the proposed technique.- The method appears relatively simple to implement on top of existing differentiable NAS methods. This contrasts with some other recent improvements that require more significant modifications to the overall NAS pipeline. Greater simplicity could aid adoption.- The authors have released code for their method, which will support reproducibility and extensions by other researchers. Releasing code is an important contribution.Overall, this paper makes solid contributions to an active research area by proposing a novel technique to address weight sharing inaccuracies. The strong empirical results across different contexts highlight the promise of the gradient matching approach. It offers useful ideas that move the field forward while building nicely on prior art.
