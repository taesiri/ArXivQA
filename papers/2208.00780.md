# [Visual correspondence-based explanations improve AI robustness and   human-AI team accuracy](https://arxiv.org/abs/2208.00780)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research questions/hypotheses appear to be:1) Can correspondence-based image classifiers that first explain, and then predict, improve robustness on out-of-distribution (OOD) image datasets compared to standard classifiers like ResNet-50?2) Can the correspondence-based explanations generated by these interpretable models improve human accuracy and decision making in image classification tasks compared to other explanation methods? 3) Is it possible for correspondence-based classifiers and their explanations to yield complementary human-AI team performance that is better than either the AI or humans alone on image classification benchmarks like ImageNet and CUB?The key ideas seem to be:- Using correspondence between image patches, rather than global features, may improve robustness on OOD data- Showing visual correspondence between image patches can act as explanations for classifier decisions - These correspondence-based explanations may help improve human accuracy when assisting AI image classification- Combining correspondence-based classifier decisions on high-confidence inputs with human decisions on the rest may achieve better accuracy than AI or humans aloneThe central hypotheses appear to be that correspondence-based interpretable models can: 1) improve robustness, 2) generate useful explanations for humans, and 3) enable complementary human-AI teams. The paper seems focused on empirically evaluating these hypotheses on ImageNet and CUB datasets with both AI and human evaluations.


## What is the main contribution of this paper?

Based on my review of the paper, the main contributions seem to be:1. Proposing two new interpretable image classifiers, EMD-Corr and CHM-Corr, that generate visual correspondence-based explanations. These models consistently improve classification accuracy on out-of-distribution datasets compared to ResNet-50 and kNN baselines.2. Conducting large-scale human studies on ImageNet and CUB datasets to evaluate the usefulness of different explanation methods. The correspondence-based explanations from EMD-Corr and CHM-Corr are found to help users more accurately reject incorrect AI predictions compared to other methods.3. Demonstrating for the first time the possibility of achieving complementary human-AI team accuracy (i.e. higher than either AI alone or human alone) on ImageNet and CUB image classification tasks through a practical interaction model.In summary, the key innovations appear to be in proposing new interpretable classifiers that improve robustness, evaluating explanation methods via large-scale human studies, and showing the benefits of human-AI teaming for image classification. The correspondence-based explanations are shown to be more useful for rejecting incorrect AI decisions while also enabling complementary human-AI performance.


## How does this paper compare to other research in the same field?

Here are a few key ways this paper compares to other related works in interpretable and robust image classification:1. Improving robustness on out-of-distribution datasets:- The paper shows that using patch-wise similarity with a kNN classifier can improve robustness on ImageNet OOD datasets like ImageNet-R, Sketch, etc. This is a novel finding, as prior works like DeepEMD and FaceEMD focused on face identification and few-shot classification tasks, not ImageNet. 2. Human evaluation of correspondence-based explanations:- The paper conducts large-scale human studies to evaluate the utility of visual correspondence explanations on both ImageNet and CUB classification. Prior prototype-based methods like ProtoPNet did not assess the real-world usefulness of explanations through human studies.3. Complementary human-AI team performance:- The paper demonstrates a human-AI team can outperform either alone on ImageNet and CUB classification. Other XAI papers found explanations useless for human-AI teaming. This is the first such result in image classification tasks.4. Non-parametric classifiers:- The interpretable classifiers are non-parametric, allowing them to work with any training set. Parametric ProtoPNet-like classifiers may fail on out-of-domain data.5. Only uses 5 correspondence pairs:- The classifiers only rely on 5 patch pairs unlike prior works that use all pairs. This likely improves interpretability for humans.So in summary, key novelties are showing correspondence-based explanations can improve robustness on ImageNet, be useful for humans even for fine-grained tasks like CUB, and enable complementary human-AI teams. The non-parametric design also provides flexibility.


## What future research directions do the authors suggest?

Based on my reading of the paper, here are some of the main future research directions suggested by the authors:- Evaluate the proposed correspondence-based classifiers on a wider range of out-of-distribution (OOD) datasets (e.g. adversarial poses, more fine-grained datasets) to further assess their robustness. The authors tested their methods on only a limited set of ImageNet OOD datasets due to computational constraints.- Explore different architectures and training procedures for the correspondence-based classifiers to improve their runtime and scalability while maintaining accuracy. The authors note that their methods are slower than standard ResNet classifiers.- Conduct further human studies on different tasks and with more controlled conditions (e.g. in a lab setting) to further understand the utility of correspondence-based explanations. The authors note some limitations of using online crowdworkers. - Test the human-AI complementary team framework proposed on a broader range of datasets and tasks beyond image classification. The authors only demonstrated this framework on ImageNet and CUB datasets.- Explore other ways humans and AI systems can collaborate effectively beyond the confidence thresholding approach proposed. The authors only test one simple human-AI interaction model.- Develop new evaluation metrics and procedures to better assess the utility of explanations for improving human-AI team performance. The authors note current metrics have limitations.In summary, the main directions are: evaluating on more diverse tasks/datasets, developing better classifier architectures, conducting more controlled human studies, testing different human-AI interaction frameworks, and creating new evaluation methodologies to fully understand the benefits of correspondence-based explanations.


## Summarize the paper in one paragraph.

Here is a one paragraph summary of the paper:The paper proposes two novel interpretable image classification architectures called EMD-Corr and CHM-Corr that generate correspondence-based explanations to support their predictions. The classifiers first retrieve similar training images using global features, then re-rank them by comparing local patches between the query and training images. The top patch correspondences serve as visual explanations for the model's decisions. Experiments on ImageNet and CUB datasets show that the correspondence-based classifiers improve out-of-distribution robustness over ResNet-50 and kNN baselines by 1-4\%. A large-scale human study finds the correspondence explanations more useful than kNN for rejecting incorrect predictions, especially on fine-grained CUB classification. The study also demonstrates for the first time complementary human-AI team performance on ImageNet and CUB image classification, where team accuracy exceeds both human-alone and AI-alone accuracy. Overall, the correspondence-based explanations and human-AI collaboration results are novel contributions towards building more accurate and transparent vision systems.


## Summarize the paper in two paragraphs.

Here is a two paragraph summary of the paper:The paper proposes two novel interpretable image classification architectures called EMD-Corr and CHM-Corr that generate visual correspondence-based explanations. The classifiers first retrieve the top k nearest neighbors to a query image based on global features. Then, they re-rank the neighbors using patch-level correspondences between the query and training images. For example, EMD-Corr computes an optimal transport cost between image patches while CHM-Corr leverages a pretrained convolutional hough matching network to find semantic correspondences. The top predicted label is based on the dominant class among the re-ranked neighbors. The classifiers are evaluated on ImageNet and CUB fine-grained image classification. They consistently improve robustness over ResNet-50 and kNN baselines on out-of-distribution datasets by 1-4\% while performing comparably on in-distribution data. The correspondence explanations are also shown to be more useful than kNN explanations in assisting humans, especially for rejecting incorrect predictions. On CUB, the explanations help achieve complementary human-AI team accuracy. Overall, the visual correspondence-based interpretable classifiers enhance robustness and generate explanations that are more useful for humans compared to non-interpretable baselines.


## Summarize the main method used in the paper in one paragraph.

Here is a one paragraph summary of the main method used in the paper:The paper proposes two novel architectures of self-interpretable image classifiers that first explain, and then predict (as opposed to post-hoc explanations) by harnessing the visual correspondences between a query image and exemplars. The two models, EMD-Corr and CHM-Corr, operate in similar ways. First, they use a kNN classifier to retrieve the top 50 nearest neighbors to a query image based on global image features. Then, they re-rank these 50 images based on patch-wise visual correspondences, by either using Earth Mover's Distance (EMD-Corr) or a Convolutional Hough Matching network (CHM-Corr) to find the top 5 most similar patches between the query and each candidate image. The predicted label is the dominant class among the top 20 re-ranked images. The top 5 patch correspondences serve as visual explanations to support the prediction. Experiments on ImageNet and CUB datasets show that EMD-Corr and CHM-Corr improve accuracy on out-of-distribution data compared to ResNet-50 and kNN baselines, while slightly underperforming on in-distribution data. Human studies also demonstrate the usefulness of the correspondence-based explanations in helping users accurately reject incorrect AI predictions compared to other explanation methods.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper proposes two visual correspondence-based image classifiers, EMD-Corr and CHM-Corr, that first generate prototype-based explanations and then make predictions; these models improve robustness on out-of-distribution datasets compared to ResNet-50 and kNN baselines while providing useful explanations that help lay users accurately reject incorrect AI decisions in image classification tasks.
