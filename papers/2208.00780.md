# [Visual correspondence-based explanations improve AI robustness and
  human-AI team accuracy](https://arxiv.org/abs/2208.00780)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research questions/hypotheses appear to be:

1) Can correspondence-based image classifiers that first explain, and then predict, improve robustness on out-of-distribution (OOD) image datasets compared to standard classifiers like ResNet-50?

2) Can the correspondence-based explanations generated by these interpretable models improve human accuracy and decision making in image classification tasks compared to other explanation methods? 

3) Is it possible for correspondence-based classifiers and their explanations to yield complementary human-AI team performance that is better than either the AI or humans alone on image classification benchmarks like ImageNet and CUB?

The key ideas seem to be:

- Using correspondence between image patches, rather than global features, may improve robustness on OOD data

- Showing visual correspondence between image patches can act as explanations for classifier decisions 

- These correspondence-based explanations may help improve human accuracy when assisting AI image classification

- Combining correspondence-based classifier decisions on high-confidence inputs with human decisions on the rest may achieve better accuracy than AI or humans alone

The central hypotheses appear to be that correspondence-based interpretable models can: 1) improve robustness, 2) generate useful explanations for humans, and 3) enable complementary human-AI teams. The paper seems focused on empirically evaluating these hypotheses on ImageNet and CUB datasets with both AI and human evaluations.


## What is the main contribution of this paper?

 Based on my review of the paper, the main contributions seem to be:

1. Proposing two new interpretable image classifiers, EMD-Corr and CHM-Corr, that generate visual correspondence-based explanations. These models consistently improve classification accuracy on out-of-distribution datasets compared to ResNet-50 and kNN baselines.

2. Conducting large-scale human studies on ImageNet and CUB datasets to evaluate the usefulness of different explanation methods. The correspondence-based explanations from EMD-Corr and CHM-Corr are found to help users more accurately reject incorrect AI predictions compared to other methods.

3. Demonstrating for the first time the possibility of achieving complementary human-AI team accuracy (i.e. higher than either AI alone or human alone) on ImageNet and CUB image classification tasks through a practical interaction model.

In summary, the key innovations appear to be in proposing new interpretable classifiers that improve robustness, evaluating explanation methods via large-scale human studies, and showing the benefits of human-AI teaming for image classification. The correspondence-based explanations are shown to be more useful for rejecting incorrect AI decisions while also enabling complementary human-AI performance.


## How does this paper compare to other research in the same field?

 Here are a few key ways this paper compares to other related works in interpretable and robust image classification:

1. Improving robustness on out-of-distribution datasets:
- The paper shows that using patch-wise similarity with a kNN classifier can improve robustness on ImageNet OOD datasets like ImageNet-R, Sketch, etc. This is a novel finding, as prior works like DeepEMD and FaceEMD focused on face identification and few-shot classification tasks, not ImageNet. 

2. Human evaluation of correspondence-based explanations:
- The paper conducts large-scale human studies to evaluate the utility of visual correspondence explanations on both ImageNet and CUB classification. Prior prototype-based methods like ProtoPNet did not assess the real-world usefulness of explanations through human studies.

3. Complementary human-AI team performance:
- The paper demonstrates a human-AI team can outperform either alone on ImageNet and CUB classification. Other XAI papers found explanations useless for human-AI teaming. This is the first such result in image classification tasks.

4. Non-parametric classifiers:
- The interpretable classifiers are non-parametric, allowing them to work with any training set. Parametric ProtoPNet-like classifiers may fail on out-of-domain data.

5. Only uses 5 correspondence pairs:
- The classifiers only rely on 5 patch pairs unlike prior works that use all pairs. This likely improves interpretability for humans.

So in summary, key novelties are showing correspondence-based explanations can improve robustness on ImageNet, be useful for humans even for fine-grained tasks like CUB, and enable complementary human-AI teams. The non-parametric design also provides flexibility.


## What future research directions do the authors suggest?

 Based on my reading of the paper, here are some of the main future research directions suggested by the authors:

- Evaluate the proposed correspondence-based classifiers on a wider range of out-of-distribution (OOD) datasets (e.g. adversarial poses, more fine-grained datasets) to further assess their robustness. The authors tested their methods on only a limited set of ImageNet OOD datasets due to computational constraints.

- Explore different architectures and training procedures for the correspondence-based classifiers to improve their runtime and scalability while maintaining accuracy. The authors note that their methods are slower than standard ResNet classifiers.

- Conduct further human studies on different tasks and with more controlled conditions (e.g. in a lab setting) to further understand the utility of correspondence-based explanations. The authors note some limitations of using online crowdworkers. 

- Test the human-AI complementary team framework proposed on a broader range of datasets and tasks beyond image classification. The authors only demonstrated this framework on ImageNet and CUB datasets.

- Explore other ways humans and AI systems can collaborate effectively beyond the confidence thresholding approach proposed. The authors only test one simple human-AI interaction model.

- Develop new evaluation metrics and procedures to better assess the utility of explanations for improving human-AI team performance. The authors note current metrics have limitations.

In summary, the main directions are: evaluating on more diverse tasks/datasets, developing better classifier architectures, conducting more controlled human studies, testing different human-AI interaction frameworks, and creating new evaluation methodologies to fully understand the benefits of correspondence-based explanations.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the paper:

The paper proposes two novel interpretable image classification architectures called EMD-Corr and CHM-Corr that generate correspondence-based explanations to support their predictions. The classifiers first retrieve similar training images using global features, then re-rank them by comparing local patches between the query and training images. The top patch correspondences serve as visual explanations for the model's decisions. Experiments on ImageNet and CUB datasets show that the correspondence-based classifiers improve out-of-distribution robustness over ResNet-50 and kNN baselines by 1-4\%. A large-scale human study finds the correspondence explanations more useful than kNN for rejecting incorrect predictions, especially on fine-grained CUB classification. The study also demonstrates for the first time complementary human-AI team performance on ImageNet and CUB image classification, where team accuracy exceeds both human-alone and AI-alone accuracy. Overall, the correspondence-based explanations and human-AI collaboration results are novel contributions towards building more accurate and transparent vision systems.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the paper:

The paper proposes two novel interpretable image classification architectures called EMD-Corr and CHM-Corr that generate visual correspondence-based explanations. The classifiers first retrieve the top k nearest neighbors to a query image based on global features. Then, they re-rank the neighbors using patch-level correspondences between the query and training images. For example, EMD-Corr computes an optimal transport cost between image patches while CHM-Corr leverages a pretrained convolutional hough matching network to find semantic correspondences. The top predicted label is based on the dominant class among the re-ranked neighbors. 

The classifiers are evaluated on ImageNet and CUB fine-grained image classification. They consistently improve robustness over ResNet-50 and kNN baselines on out-of-distribution datasets by 1-4\% while performing comparably on in-distribution data. The correspondence explanations are also shown to be more useful than kNN explanations in assisting humans, especially for rejecting incorrect predictions. On CUB, the explanations help achieve complementary human-AI team accuracy. Overall, the visual correspondence-based interpretable classifiers enhance robustness and generate explanations that are more useful for humans compared to non-interpretable baselines.


## Summarize the main method used in the paper in one paragraph.

 Here is a one paragraph summary of the main method used in the paper:

The paper proposes two novel architectures of self-interpretable image classifiers that first explain, and then predict (as opposed to post-hoc explanations) by harnessing the visual correspondences between a query image and exemplars. The two models, EMD-Corr and CHM-Corr, operate in similar ways. First, they use a kNN classifier to retrieve the top 50 nearest neighbors to a query image based on global image features. Then, they re-rank these 50 images based on patch-wise visual correspondences, by either using Earth Mover's Distance (EMD-Corr) or a Convolutional Hough Matching network (CHM-Corr) to find the top 5 most similar patches between the query and each candidate image. The predicted label is the dominant class among the top 20 re-ranked images. The top 5 patch correspondences serve as visual explanations to support the prediction. Experiments on ImageNet and CUB datasets show that EMD-Corr and CHM-Corr improve accuracy on out-of-distribution data compared to ResNet-50 and kNN baselines, while slightly underperforming on in-distribution data. Human studies also demonstrate the usefulness of the correspondence-based explanations in helping users accurately reject incorrect AI predictions compared to other explanation methods.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the paper:

The paper proposes two visual correspondence-based image classifiers, EMD-Corr and CHM-Corr, that first generate prototype-based explanations and then make predictions; these models improve robustness on out-of-distribution datasets compared to ResNet-50 and kNN baselines while providing useful explanations that help lay users accurately reject incorrect AI decisions in image classification tasks.


## What problem or question is the paper addressing?

 Based on my reading of the paper, here are the main problems/questions it is addressing:

1. How to design image classifiers that are robust to out-of-distribution (OOD) data while also being interpretable and able to provide explanations for their predictions. Many convolutional neural networks perform well on in-distribution data but struggle to generalize to OOD inputs. The authors propose correspondence-based classifiers that leverage patch-wise image similarity to improve robustness.

2. Whether visual correspondence-based explanations can help improve human accuracy and calibration when using AI assistance for image classification tasks. Most prior work has focused on using saliency maps or nearest neighbors as explanations, but it is unclear if showing fine-grained correspondences between image patches can better inform human decision making. The authors test this through large-scale human studies on ImageNet and CUB datasets.

3. Achieving complementary human-AI team performance in image classification, where the joint accuracy of human+AI decisions is higher than either alone. Prior work has shown this for NLP and some other domains but it has been lacking for computer vision tasks.

In summary, the key goals are improving out-of-distribution robustness for classifiers through patch correspondences, demonstrating the utility of correspondence-based explanations for human decision making, and exploring complementary human-AI teams for image classification. The large-scale human studies are a key contribution toward addressing these questions.


## What are the keywords or key terms associated with this paper?

 Based on my review, some of the key terms and keywords associated with this paper include:

- Visual correspondence-based explanations: The paper proposes two novel image classifier architectures (EMD-Corr and CHM-Corr) that generate visual correspondence-based explanations to support their predictions. These explanations highlight corresponding patches between the input image and training images to provide an interpretable rationale for the model's decisions.

- Improving AI robustness: The correspondence-based classifiers are shown to consistently improve out-of-distribution accuracy over baselines like ResNet-50 and kNN on ImageNet datasets. The explanations allow focusing on key discriminative patches and ignoring irrelevant perturbations. 

- Human evaluation: The paper conducts large-scale human studies on ImageNet and CUB datasets to evaluate the utility of different explanation methods. Correspondence-based explanations are found to help users accurately reject incorrect AI predictions.

- Complementary human-AI teams: For the first time, the paper shows it is possible to achieve higher accuracy with human-AI teams over individual AI or human performance on image classification tasks. Humans can effectively handle images the AI is less confident about.

- Interpretable AI: The proposed correspondence-based classifiers and explanations represent an interpretable-by-design approach to generate inherently interpretable models instead of post-hoc explanations.

- Prototype-based explanations: The paper provides a novel combination of heatmap and prototype-based explanations by surfacing discriminative patches and correspondences.

- Trust calibration: Analysis shows correspondence explanations help calibrate human trust in AI by improving rejection of incorrect predictions compared to kNN explanations.

So in summary, the key focus is on visual correspondence explanations for improving robustness, human-AI collaboration, and trust calibration in image classification.


## Based on the paper, what questions should we ask to create a comprehensive summary of it? Please list at least 10 questions.

 Here are 10 potential questions to ask to summarize the key information in this paper:

1. What is the main goal or objective of this paper?

2. What datasets were used in the experiments? 

3. What methods or models were proposed and compared?

4. What were the main evaluation metrics and results on those metrics?

5. What were the key findings or conclusions of the paper?

6. What were the limitations or drawbacks identified by the authors? 

7. How does this work compare to prior related work in the field? What are the key differences?

8. What theoretical analysis or proofs were provided to support the technical approach?

9. Were ablation studies conducted to analyze different components of the proposed method? If so, what were the key insights?

10. Did the authors perform any human subject studies or evaluations? If so, what were the major takeaways?

Asking these types of targeted questions can help extract the core goals, techniques, results, and implications from the paper in a systematic way. Additional questions could probe deeper into the methodological details, theoretical justifications, experimental setup, limitations and societal impact. The goal is to understand both the technical novelty and practical significance of the work.


## Design 10 in-depth questions about method proposed in this paper and avoid simple question.

 Here are 10 in-depth questions about the method proposed in the paper:

1. The paper proposes two novel visual correspondence-based classifiers, EMD-Corr and CHM-Corr. How do these classifiers differ in their approach to computing patch-wise similarity between images? What are the tradeoffs between these two approaches?

2. The paper finds that using just 5 patch correspondences for computing image similarity performs well compared to prior work that used all patch pairs. Why might using fewer, more "important" patches be beneficial? Does reducing the number of patch comparisons introduce any limitations?

3. The EMD-Corr and CHM-Corr classifiers are evaluated on several out-of-distribution (OOD) datasets. Why might correspondence-based classifiers be better suited for OOD generalization compared to global feature classifiers like ResNet-50? What properties of the OOD datasets might they still struggle with?

4. The paper shows CHM-Corr explanations help users correctly reject incorrect AI predictions more than kNN explanations. What factors contribute to this? How could the diversity and relevance of retrieved neighbors be further improved? 

5. For CUB classification, the paper introduces a CHM-Corr+ baseline using human-defined bird keypoints. Why does this perform worse than regular CHM-Corr? What disadvantages arise from relying solely on predefined keypoints?

6. The human studies evaluate two interaction models - where users make all decisions, or only hard decisions deferred by the AI. What are the tradeoffs between these models? When might one be preferred over the other?

7. The paper finds human-AI team accuracy exceeds AI or human alone. Does this hold across different domains and task difficulties? How could we identify optimal tasks for human-AI collaboration?

8. The classifiers are evaluated on binary classification based on the AI's top-1 prediction. How might performance differ if using top-k predictions or multi-class decisions? What changes would be needed?

9. The EMD-Corr and CHM-Corr classifiers rely on pretrained features from a ResNet-50 backbone. How might using different pretrained features impact their performance? What benefits or limitations might arise from using different backbone architectures? 

10. The paper performs extensive human evaluation studies. What are the challenges and ethical considerations in conducting large-scale human studies? How could the study design be improved to better capture real-world performance?
