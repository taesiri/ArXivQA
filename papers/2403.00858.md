# [Direct Alignment of Draft Model for Speculative Decoding with   Chat-Fine-Tuned LLMs](https://arxiv.org/abs/2403.00858)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Large language models (LLMs) are slow for inference on edge devices due to their autoregressive nature and memory bandwidth limitations. 
- Speculative decoding can accelerate LLM inference by using a smaller "draft" model to predict the output of a larger "target" model. 
- However, high-quality draft models aligned to target models are often unavailable, especially for recent large LLMs.

Proposed Solution:
- A framework to train a draft model directly aligned to a target LLM for chat capabilities. 
- Pipeline has 3 steps: 
  1) Pretrain draft model on large text corpus for language modeling basics
  2) Generate distillation dataset by having target LLM generate responses to prompts 
  3) Finetune draft model on distillation dataset using target LLM in loop
- Propose a new "TVD++" distillation loss inspired by policy gradient in RL to better align distributions

Main Contributions:
- Simple 3-step framework to train draft model aligned to chat-tuned target LLM 
- TVD++ loss outperforms KL divergence and TVD for draft model distillation
- Train 115M parameter Llama2 draft model for 7B Llama2 chat model (1.64% size)
- Achieves up to 2.4x speedup on tasks like open-ended text generation without degrade in quality
- Framework is effective despite not having access to target model's original training data

In summary, the paper presents an effective knowledge distillation framework to train a very small draft model that can effectively accelerate inference of much larger chat-tuned LLMs by over 2x, enabling their deployment on edge devices. The TVD++ loss helps better align distributions without needing alignment datasets.


## Summarize the paper in one sentence.

 This paper proposes a simple framework for training a small draft model that can accelerate inference of a large chat-capable language model through speculative decoding, using pretraining, distillation dataset generation with the target model, and finetuning with a novel distillation loss inspired by policy gradient reinforcement learning.


## What is the main contribution of this paper?

 The main contribution of this paper is proposing a simple draft model training framework for direct alignment to chat-capable target models. Specifically:

- They propose a 3-step training pipeline consisting of (1) pretraining, (2) distillation dataset generation, and (3) finetuning with knowledge distillation to train a draft model aligned with a target chat model. 

- They generate a distillation dataset by having the target model generate responses to seed instructions from existing datasets. This creates data in a plausible distribution for the target model.

- They propose a novel distillation loss called Total Variation Distance++ (TVD++) inspired by policy gradient in RL. TVD++ incorporates variance reduction techniques for better optimization.

- They use this framework to train Llama 2 Chat Drafter, a 115M parameter draft model aligned to the 7B parameter Llama 2 Chat model. Their draft model achieves up to 2.3x block efficiency and 2.4x speedup over autoregressive decoding on various chat tasks.

So in summary, the main contribution is the overall draft model training framework and the new TVD++ loss for improved alignment to chat-capable target models.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper abstract and introduction, here are some of the key terms and keywords associated with this paper:

- Large language models (LLMs)
- Speculative decoding
- Inference acceleration
- Draft models
- Knowledge distillation
- Total variation distance (TVD)
- Policy gradient
- Reinforcement learning
- Variance reduction
- Llama 2 Chat
- Open-ended text generation
- Text summarization
- Auto-regressive decoding
- Block efficiency 
- Memory-bound speedup

The paper proposes a framework to train a small "draft" model to accelerate inference of a large "target" chatbot LLM using speculative decoding. Key ideas include using knowledge distillation with a novel TVD++ loss inspired by policy gradient methods from RL, and evaluating performance in terms of block efficiency and memory-bound speedup on tasks like open-ended generation and summarization.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in this paper:

1) The paper proposes a simple draft model training framework for direct alignment to chat-capable target models. Can you explain in more detail the 3 phases of this training framework - pretraining, distillation dataset generation, and finetuning with knowledge distillation? 

2) How does generating a distillation dataset by having the target model generate responses help in simplifying the training procedure? What are some of the practical challenges it helps mitigate?

3) The paper proposes a new loss function called Total Variation Distance++ (TVD++). Can you explain in detail how this loss builds on top of the Total Variation Distance (TVD) loss by making connections to policy gradient methods in reinforcement learning? 

4) What is the key idea behind using techniques like advantage normalization from reinforcement learning for the TVD++ loss? How does this lead to better learning signals during draft model fine-tuning?

5) The draft model in the paper is only 1.64% the size of the target Llama 2 Chat 7B model. What were some of the architectural choices made to design such a small draft model? How does this size help achieve negligible inference overheads?

6) The paper evaluates draft models on metrics like block efficiency and memory-bound speed up. Can you explain what these metrics capture and how they relate to the goal of acceleration via speculative decoding?

7) What results did the paper show regarding the efficacy of fine-tuning with the proposed TVD++ loss? How did it compare against other losses like KLD and TVD on metrics like block efficiency? 

8) One interesting result is the improvement in block efficiency during fine-tuning over the base draft model. What does this show about the importance of the fine-tuning stage? 

9) For out-of-distribution tasks like WMT translation, draft models performed worse than the base model. What may be the reason for this? How can this issue potentially be resolved?

10) The target model used in the paper is Llama 2 Chat 7B. Do you think the proposed training framework can work for other large chat-capable language models as well? What may need to change to adapt it?
