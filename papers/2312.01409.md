# [Generative Rendering: Controllable 4D-Guided Video Generation with 2D   Diffusion Models](https://arxiv.org/abs/2312.01409)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Traditional 3D content creation tools are laborious, time-consuming and require expertise. Emerging AI generative models like text-to-image (T2I) and text-to-video (T2V) can automate parts of this process but lack control over scene layout and motion in a temporally consistent manner. 

Proposed Solution:
- The paper presents a novel framework called "Generative Rendering" that combines the controllability of animated 3D meshes with the expressivity of diffusion models to render high-quality, stylized animations. 

- It takes an animated, low-fidelity 3D mesh as input and injects the ground truth correspondence information from the mesh into various stages of a pre-trained T2I diffusion model to output temporally consistent stylized frames.

- Specifically, it enriches the self-attention layers of the diffusion model with feature blending over input and output features to enforce consistent appearance synthesis. It also initializes noise in the UV space of each object and projects it to each frame for better consistency.  

- Depth cues from the 3D scene are used with control models to provide structural guidance.

Main Contributions:
- A novel framework for 4D-guided animation synthesis that utilizes pre-trained T2I diffusion models as multi-frame renderers.

- A correspondence-aware blending mechanism for the self-attention layers of the image diffusion model to enforce consistent appearance synthesis across frames.

- A UV-space noise initialization strategy that significantly improves temporal coherence of generated outputs.

- Demonstrates the framework on various animated 3D scenes and shows better quantitative consistency scores and qualitative results compared to relevant baselines.

In summary, the key idea is to leverage the controllability of 3D content creation workflows and combine it with the expressiveness of diffusion models in a principled way to generate high-quality, controllable and temporally consistent stylized animations.
