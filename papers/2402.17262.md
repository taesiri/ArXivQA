# [Speak Out of Turn: Safety Vulnerability of Large Language Models in   Multi-turn Dialogue](https://arxiv.org/abs/2402.17262)

## What is the main contribution of this paper?

 This paper highlights an important vulnerability in the safety mechanisms of large language models (LLMs) when used in multi-turn dialogues. The key contributions are:

1. It exposes how malicious users can decompose harmful queries into series of seemingly harmless sub-queries and pose them to LLMs over multiple dialogue turns. This allows the models to incrementally build up context and knowledge to ultimately generate harmful responses in the final turns. 

2. It systematically evaluates this attack vector across several state-of-the-art LLMs such as ChatGPT, Claude, and Gemini. The results demonstrate that current safety assurances are inadequate when LLMs are interacted with over multi-turn dialogues.

3. It provides an analysis into why aligned LLMs remain susceptible to such attacks, attributing it to issues like competing objectives, mismatched generalization between training and test scenarios, and limitations in understanding dialog context.

4. It proposes mitigation strategies to enhance LLM safety in multi-turn dialogues, arguing for reinforcement learning from human feedback using multi-turn dialog data, and improvements to dialog context modeling.

In summary, the key insight this paper provides is that existing alignment strategies overlook the unique challenges introduced by multi-turn dialogues, which malicious actors can exploit to ultimately induce harmful content from most state-of-the-art LLMs. Addressing this previously ignored attack surface is critical for the safe deployment of conversational AI systems.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the methods proposed in this paper:

1. The paper proposes a Malicious Query Decomposition paradigm with several key strategies like purpose inversion and keyword replacement to transform harmful questions into seemingly harmless sub-questions for jailbreaking. How robust is this decomposition methodology, and what are some ways it could be improved or adapted to other contexts?

2. Figure 1 illustrates the baseline paradigm for jailbreaking LLMs via multi-turn dialogue. Could this approach be extended by incorporating complementary jailbreaking techniques designed for single-turn interactions to enhance jailbreak capacity? 

3. Table 1 shows the evaluation results of harmfulness across models. Is there any correlation evident between model scale/architecture and vulnerability in multi-turn dialogue? Could the relative performance provide any directionality regarding mitigation?

4. The role-playing technique in the final turn increased average harmfulness. Does this indicate potential synergies between single-turn and multi-turn jailbreaking methodologies? What theories support the enhanced jailbreaking capacity observed?

5. Figure 5 shows the relationship between number of turns and harmfulness indicators. If the number of turns was increased substantially, what do you hypothesize would happen? Would the generation quality continue improving, plateau, or deteriorate?

6. The results demonstrate deficiencies in the contextual understanding of language models during multi-turn interactions. Would a context clarification prompt between topic changes reduce vulnerability, or could it be circumvented?  

7. The paper argues that language models require better semantic understanding of unrelated queries to mitigate risks in multi-turn dialogue. What current techniques show promise for enhancing semantic comprehension of language models?

8. The role-playing results showcase the technique's applicability in multi-turn dialogue scenarios. Should role-playing be incorporated into safety data augmentation and red team testing for multi-turn interactions? What risks exist?

9. Could the automatic query decomposition technique enable large-scale attacks? What risk mitigations regarding the public availability of decomposition systems should be considered?

10. A core insight is that language models struggle determining hidden malicious intent across multi-turn interactions. Would a separate intent classification model or self-supervised objectives help address this contextual deficiency during pre-training or fine-tuning?
