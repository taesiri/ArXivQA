# [The Benefits of Reusing Batches for Gradient Descent in Two-Layer   Networks: Breaking the Curse of Information and Leap Exponents](https://arxiv.org/abs/2402.03220)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem Statement:
- Recent work has shown limitations of single-pass stochastic gradient descent (SGD) and gradient flow for training two-layer neural networks, in terms of the number of steps needed to learn target functions. These limitations are characterized by the information exponent and leap exponent of the target function. 

- This paper investigates whether reusing mini-batches multiple times in multi-pass SGD can overcome these limitations and expand the class of functions that can be efficiently learned.

Key Contributions:

- Shows that multi-pass SGD with only two repetitions of the mini-batch is able to achieve positive correlation with a broader class of target functions compared to single-pass methods, breaking the curse of the information and leap exponents.

- Proves that multi-pass SGD with a fixed, finite number of steps can efficiently learn non-symmetric multi-index target functions, while single-pass methods require a diverging number of steps for such functions.

- Identifies symmetric target functions as exceptions that cannot be learned with a finite number of steps even with multi-pass SGD, due to symmetry breaking times.

- Provides a theoretical framework based on Dynamical Mean Field Theory (DMFT) to rigorously analyze the training dynamics of multi-pass SGD and give an analytical characterization.

- Conceptually explains the "hidden progress" in the first SGD step that enables learning new directions in the second step, based on pre-activation dependencies.

- Demonstrates through theory and experiments that multi-pass SGD can surpass single-pass SGD even when the latter uses more data points, highlighting the value of reusing batches.

The key insight is that reusing mini-batches creates useful correlations in the pre-activations across examples, which allow multi-pass SGD to learn new directions not possible with independent samples. This challenges prevailing notions of information and leap exponents as inherent limitations.
