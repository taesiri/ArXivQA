# Explanations from Large Language Models Make Small Reasoners Better

## What is the central research question or hypothesis that this paper addresses?

This paper appears to address the following central research question:Can explanations generated by large language models improve the reasoning capability and explainability of smaller language models?The key hypothesis seems to be that leveraging explanations generated by large language models, through approaches like chain of thought prompting and rationalization prompting, can help improve the performance of smaller language models on reasoning tasks when integrated via multi-task learning. The paper systematically explores different explanation generation strategies and multi-task learning frameworks to test this hypothesis. Additionally, the paper evaluates whether the resulting small models can generate high-quality explanations, moving towards more explainable AI systems.In summary, the central research question is whether large model-generated explanations can enhance small model reasoning and explainability, which is tested through empirical experiments on reasoning datasets using different explanation integration strategies.


## What is the main contribution of this paper?

The main contributions of this paper are:1. Showing that explanations generated from large language models (LLMs) can be used to improve the reasoning capability of small language models (SLMs) in a multi-task learning framework. 2. Systematically exploring three approaches (COTE, RP, CROP) for generating explanations from LLMs, and proposing a hybrid approach (CROP) that combines chain of thought and rationalization prompting.3. Proposing a new multi-task learning method called MT-CoT that trains the model to first generate an explanation and then generate the answer, allowing the model to learn to derive the answer from the explanation. 4. Demonstrating consistent and significant improvements over strong single-task finetuning baselines across different explanation generation methods, multi-task learning setups, training sample sizes, and SLM sizes.5. Showing the method can outperform finetuning/prompting of even very large LLMs like GPT-3 175B on CommonsenseQA, while using a much smaller model like T5-3B.6. Demonstrating the generated explanations are high quality through human evaluation, moving towards more explainable AI systems.In summary, the key contribution is showing that leveraging automatically generated explanations from LLMs can substantially improve the reasoning capabilities of SLMs, even outperforming very large LLMs, across a variety of settings. The hybrid explanation generation method and new multi-task learning approach proposed are also notable contributions.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper proposes leveraging explanations generated by large language models to improve the reasoning capability of smaller models through multi-task learning, demonstrating consistent performance gains across tasks and models.
