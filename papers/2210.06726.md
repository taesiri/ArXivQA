# Explanations from Large Language Models Make Small Reasoners Better

## What is the central research question or hypothesis that this paper addresses?

This paper appears to address the following central research question:Can explanations generated by large language models improve the reasoning capability and explainability of smaller language models?The key hypothesis seems to be that leveraging explanations generated by large language models, through approaches like chain of thought prompting and rationalization prompting, can help improve the performance of smaller language models on reasoning tasks when integrated via multi-task learning. The paper systematically explores different explanation generation strategies and multi-task learning frameworks to test this hypothesis. Additionally, the paper evaluates whether the resulting small models can generate high-quality explanations, moving towards more explainable AI systems.In summary, the central research question is whether large model-generated explanations can enhance small model reasoning and explainability, which is tested through empirical experiments on reasoning datasets using different explanation integration strategies.


## What is the main contribution of this paper?

The main contributions of this paper are:1. Showing that explanations generated from large language models (LLMs) can be used to improve the reasoning capability of small language models (SLMs) in a multi-task learning framework. 2. Systematically exploring three approaches (COTE, RP, CROP) for generating explanations from LLMs, and proposing a hybrid approach (CROP) that combines chain of thought and rationalization prompting.3. Proposing a new multi-task learning method called MT-CoT that trains the model to first generate an explanation and then generate the answer, allowing the model to learn to derive the answer from the explanation. 4. Demonstrating consistent and significant improvements over strong single-task finetuning baselines across different explanation generation methods, multi-task learning setups, training sample sizes, and SLM sizes.5. Showing the method can outperform finetuning/prompting of even very large LLMs like GPT-3 175B on CommonsenseQA, while using a much smaller model like T5-3B.6. Demonstrating the generated explanations are high quality through human evaluation, moving towards more explainable AI systems.In summary, the key contribution is showing that leveraging automatically generated explanations from LLMs can substantially improve the reasoning capabilities of SLMs, even outperforming very large LLMs, across a variety of settings. The hybrid explanation generation method and new multi-task learning approach proposed are also notable contributions.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper proposes leveraging explanations generated by large language models to improve the reasoning capability of smaller models through multi-task learning, demonstrating consistent performance gains across tasks and models.


## How does this paper compare to other research in the same field?

Here are my thoughts on how this paper compares to other research in the same field:- This paper explores using explanations generated by large language models (LLMs) like GPT-3 to improve the training of smaller models. This is a novel approach as most prior work has focused on using human-generated explanations or prompting the LLMs themselves. - The idea of using LLM-generated explanations to train smaller models is similar in spirit to prior work like STaR which also uses a large model to generate explanations to train a smaller model. However, this paper systematically explores different explanation generation methods and multi-task learning frameworks which is a more thorough investigation.- The overall goal of improving reasoning capabilities for smaller models is shared by other works, but this paper shows consistent and significant gains over standard finetuning baselines which some prior works did not achieve.- Evaluating the quality of generated explanations via human evaluation is unique to this paper compared to most prior work which focused only on end-task accuracy. This aligns more closely with the goals of explainable AI.- The technique of generating explanations via chain of thought and rationalization prompting builds directly on recent advances in prompting LLMs for reasoning tasks. However, the application to training smaller models is novel.- The proposed MT-CoT technique appears unique to this paper for multi-task learning by chaining explanation to answer. Most prior work used simpler multi-task formulations.In summary, this paper differs from prior work by systematically exploring prompt-based explanation generation methods from LLMs and multi-task learning frameworks to successfully improve reasoning capabilities for smaller models. The human evaluation of explanation quality also distinguishes this paper from most prior work.


## What future research directions do the authors suggest?

The authors propose several key future research directions:1. Investigate how explanations generated by LLMs can be utilized in other modalities like vision and robotics. The paper focuses on leveraging textual explanations for improving natural language reasoning, but the authors suggest exploring if similar ideas could be applied in other domains as well.2. Study how explanations can improve few-shot learning of small models in additional reasoning tasks beyond the textual tasks explored in the paper. Examples mentioned include symbolic reasoning and arithmetic reasoning tasks.3. Explore different methods for generating explanations from LLMs beyond the chain of thought, rationalization prompting, and hybrid approaches studied in the paper. Other prompting styles or methods of eliciting explanations could be investigated.4. Analyze the impacts of different training objectives, architectures, and hyperparameters when training small models with LLM-generated explanations. The paper studies a multi-task learning approach, but other frameworks could be explored.5. Further analyze the quality and biases of LLM-generated explanations, especially if they are to be used to improve other models. The paper acknowledges concerns around potential biases being propagated.In summary, the main future directions are studying this approach in other modalities and tasks, developing new explanation generation techniques, analyzing different training frameworks, and further evaluating the quality of LLM-generated explanations. The key goal is extending this line of research to improve reasoning across modalities, tasks, and models.


## Summarize the paper in one paragraph.

Here is a one paragraph summary of the paper:The paper proposes a method to leverage explanations generated by large language models (LLMs) like GPT-3 to improve the reasoning capability of smaller language models. It systematically explores three approaches for generating explanations from LLMs - chain of thought prompting with incorrect answer rejection, rationalization prompting, and a hybrid method. These explanations are then integrated into the training of the small model using multi-task learning frameworks like MT-Re, MT-Ra, and a proposed MT-CoT. Experiments on CommonsenseQA, StrategyQA, and OpenbookQA show the method consistently and significantly outperforms standard fine-tuning baselines. Benefits are demonstrated across different explanation generations, multi-task setups, training sizes, and model sizes. The method even outperforms finetuning/prompting the much larger GPT-3 on some tasks. Human evaluation also indicates the method generates high quality explanations towards more explainable AI. Overall, the paper shows explanations elicited from LLMs can effectively improve reasoning capability of smaller models.


## Summarize the paper in two paragraphs.

Here is a two paragraph summary of the paper:The paper explores utilizing explanations generated from large language models (LLMs) like GPT-3 to improve the training of small language models on reasoning tasks. The authors systematically investigate three approaches for generating explanations from LLMs: chain of thought prompting with incorrect answer rejection (COTE), rationalization prompting (RP), and a hybrid approach combining COTE and RP (CROP). They then use these generated explanations in a multi-task learning framework with three setups - MT-Re, MT-Ra, and a proposed MT-CoT - to train the small models. Experiments on CommonsenseQA, StrategyQA, and OpenbookQA show that using LLM-generated explanations consistently and significantly improves the reasoning capabilities of the small models across different explanation methods, multi-task setups, and training sizes. The proposed CROP explanation method and MT-CoT multi-task approach achieve the best results on two of the three datasets. The method even outperforms finetuning/prompting the much larger 175B parameter GPT-3 model on CommonsenseQA by up to 9.5% accuracy. As a side benefit, human evaluation indicates the trained small models can generate high-quality explanations approaching those of GPT-3.In summary, the key contributions are: 1) Demonstrating that multi-task learning with LLM-generated explanations can substantially improve reasoning performance of small language models across settings. 2) Proposing CROP for explanation generation and MT-CoT for multi-task learning to further enhance the improvements. 3) Showing the method surpasses finetuning/prompting 175B GPT-3 on a reasoning task and produces high-quality explanations, moving towards explainable AI. The results highlight the potential of leveraging knowledge from large LLMs to significantly boost the capabilities of small models.


## Summarize the main method used in the paper in one paragraph.

Here is a one paragraph summary of the main method used in the paper:The paper proposes a framework to leverage explanations generated by large language models (LLMs) like GPT-3 to improve the reasoning capability of smaller language models (SLMs). First, they use several examples with human-written explanations to demonstrate reasoning steps for the LLM. The LLM then generates explanations for the training set using chain of thought prompting, rationalization prompting, or a hybrid approach. These explanations are then integrated into the SLM using a multi-task learning framework, where the SLM is trained on both predicting answers directly and generating explanations. Specifically, they explore three multi-task setups: MT-Re where the SLM generates explanations without answers, MT-Ra where the SLM generates rationalizations conditioned on answers, and MT-CoT where the SLM generates chain of thought explanations leading to answers. Experiments on reasoning datasets show this framework with LLM-generated explanations consistently improves SLMs across different explanation methods, multi-task learning setups, and model sizes. The method also outperforms larger LM baselines, while generating high-quality explanations.
