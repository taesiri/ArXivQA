# [Explanations from Large Language Models Make Small Reasoners Better](https://arxiv.org/abs/2210.06726)

## What is the central research question or hypothesis that this paper addresses?

 This paper appears to address the following central research question:

Can explanations generated by large language models improve the reasoning capability and explainability of smaller language models?

The key hypothesis seems to be that leveraging explanations generated by large language models, through approaches like chain of thought prompting and rationalization prompting, can help improve the performance of smaller language models on reasoning tasks when integrated via multi-task learning. The paper systematically explores different explanation generation strategies and multi-task learning frameworks to test this hypothesis. Additionally, the paper evaluates whether the resulting small models can generate high-quality explanations, moving towards more explainable AI systems.

In summary, the central research question is whether large model-generated explanations can enhance small model reasoning and explainability, which is tested through empirical experiments on reasoning datasets using different explanation integration strategies.


## What is the main contribution of this paper?

 The main contributions of this paper are:

1. Showing that explanations generated from large language models (LLMs) can be used to improve the reasoning capability of small language models (SLMs) in a multi-task learning framework. 

2. Systematically exploring three approaches (COTE, RP, CROP) for generating explanations from LLMs, and proposing a hybrid approach (CROP) that combines chain of thought and rationalization prompting.

3. Proposing a new multi-task learning method called MT-CoT that trains the model to first generate an explanation and then generate the answer, allowing the model to learn to derive the answer from the explanation. 

4. Demonstrating consistent and significant improvements over strong single-task finetuning baselines across different explanation generation methods, multi-task learning setups, training sample sizes, and SLM sizes.

5. Showing the method can outperform finetuning/prompting of even very large LLMs like GPT-3 175B on CommonsenseQA, while using a much smaller model like T5-3B.

6. Demonstrating the generated explanations are high quality through human evaluation, moving towards more explainable AI systems.

In summary, the key contribution is showing that leveraging automatically generated explanations from LLMs can substantially improve the reasoning capabilities of SLMs, even outperforming very large LLMs, across a variety of settings. The hybrid explanation generation method and new multi-task learning approach proposed are also notable contributions.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the paper:

The paper proposes leveraging explanations generated by large language models to improve the reasoning capability of smaller models through multi-task learning, demonstrating consistent performance gains across tasks and models.


## How does this paper compare to other research in the same field?

 Here are my thoughts on how this paper compares to other research in the same field:

- This paper explores using explanations generated by large language models (LLMs) like GPT-3 to improve the training of smaller models. This is a novel approach as most prior work has focused on using human-generated explanations or prompting the LLMs themselves. 

- The idea of using LLM-generated explanations to train smaller models is similar in spirit to prior work like STaR which also uses a large model to generate explanations to train a smaller model. However, this paper systematically explores different explanation generation methods and multi-task learning frameworks which is a more thorough investigation.

- The overall goal of improving reasoning capabilities for smaller models is shared by other works, but this paper shows consistent and significant gains over standard finetuning baselines which some prior works did not achieve.

- Evaluating the quality of generated explanations via human evaluation is unique to this paper compared to most prior work which focused only on end-task accuracy. This aligns more closely with the goals of explainable AI.

- The technique of generating explanations via chain of thought and rationalization prompting builds directly on recent advances in prompting LLMs for reasoning tasks. However, the application to training smaller models is novel.

- The proposed MT-CoT technique appears unique to this paper for multi-task learning by chaining explanation to answer. Most prior work used simpler multi-task formulations.

In summary, this paper differs from prior work by systematically exploring prompt-based explanation generation methods from LLMs and multi-task learning frameworks to successfully improve reasoning capabilities for smaller models. The human evaluation of explanation quality also distinguishes this paper from most prior work.


## What future research directions do the authors suggest?

 The authors propose several key future research directions:

1. Investigate how explanations generated by LLMs can be utilized in other modalities like vision and robotics. The paper focuses on leveraging textual explanations for improving natural language reasoning, but the authors suggest exploring if similar ideas could be applied in other domains as well.

2. Study how explanations can improve few-shot learning of small models in additional reasoning tasks beyond the textual tasks explored in the paper. Examples mentioned include symbolic reasoning and arithmetic reasoning tasks.

3. Explore different methods for generating explanations from LLMs beyond the chain of thought, rationalization prompting, and hybrid approaches studied in the paper. Other prompting styles or methods of eliciting explanations could be investigated.

4. Analyze the impacts of different training objectives, architectures, and hyperparameters when training small models with LLM-generated explanations. The paper studies a multi-task learning approach, but other frameworks could be explored.

5. Further analyze the quality and biases of LLM-generated explanations, especially if they are to be used to improve other models. The paper acknowledges concerns around potential biases being propagated.

In summary, the main future directions are studying this approach in other modalities and tasks, developing new explanation generation techniques, analyzing different training frameworks, and further evaluating the quality of LLM-generated explanations. The key goal is extending this line of research to improve reasoning across modalities, tasks, and models.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the paper:

The paper proposes a method to leverage explanations generated by large language models (LLMs) like GPT-3 to improve the reasoning capability of smaller language models. It systematically explores three approaches for generating explanations from LLMs - chain of thought prompting with incorrect answer rejection, rationalization prompting, and a hybrid method. These explanations are then integrated into the training of the small model using multi-task learning frameworks like MT-Re, MT-Ra, and a proposed MT-CoT. Experiments on CommonsenseQA, StrategyQA, and OpenbookQA show the method consistently and significantly outperforms standard fine-tuning baselines. Benefits are demonstrated across different explanation generations, multi-task setups, training sizes, and model sizes. The method even outperforms finetuning/prompting the much larger GPT-3 on some tasks. Human evaluation also indicates the method generates high quality explanations towards more explainable AI. Overall, the paper shows explanations elicited from LLMs can effectively improve reasoning capability of smaller models.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the paper:

The paper explores utilizing explanations generated from large language models (LLMs) like GPT-3 to improve the training of small language models on reasoning tasks. The authors systematically investigate three approaches for generating explanations from LLMs: chain of thought prompting with incorrect answer rejection (COTE), rationalization prompting (RP), and a hybrid approach combining COTE and RP (CROP). They then use these generated explanations in a multi-task learning framework with three setups - MT-Re, MT-Ra, and a proposed MT-CoT - to train the small models. Experiments on CommonsenseQA, StrategyQA, and OpenbookQA show that using LLM-generated explanations consistently and significantly improves the reasoning capabilities of the small models across different explanation methods, multi-task setups, and training sizes. The proposed CROP explanation method and MT-CoT multi-task approach achieve the best results on two of the three datasets. The method even outperforms finetuning/prompting the much larger 175B parameter GPT-3 model on CommonsenseQA by up to 9.5% accuracy. As a side benefit, human evaluation indicates the trained small models can generate high-quality explanations approaching those of GPT-3.

In summary, the key contributions are: 1) Demonstrating that multi-task learning with LLM-generated explanations can substantially improve reasoning performance of small language models across settings. 2) Proposing CROP for explanation generation and MT-CoT for multi-task learning to further enhance the improvements. 3) Showing the method surpasses finetuning/prompting 175B GPT-3 on a reasoning task and produces high-quality explanations, moving towards explainable AI. The results highlight the potential of leveraging knowledge from large LLMs to significantly boost the capabilities of small models.


## Summarize the main method used in the paper in one paragraph.

 Here is a one paragraph summary of the main method used in the paper:

The paper proposes a framework to leverage explanations generated by large language models (LLMs) like GPT-3 to improve the reasoning capability of smaller language models (SLMs). First, they use several examples with human-written explanations to demonstrate reasoning steps for the LLM. The LLM then generates explanations for the training set using chain of thought prompting, rationalization prompting, or a hybrid approach. These explanations are then integrated into the SLM using a multi-task learning framework, where the SLM is trained on both predicting answers directly and generating explanations. Specifically, they explore three multi-task setups: MT-Re where the SLM generates explanations without answers, MT-Ra where the SLM generates rationalizations conditioned on answers, and MT-CoT where the SLM generates chain of thought explanations leading to answers. Experiments on reasoning datasets show this framework with LLM-generated explanations consistently improves SLMs across different explanation methods, multi-task learning setups, and model sizes. The method also outperforms larger LM baselines, while generating high-quality explanations.


## What problem or question is the paper addressing?

 The paper is addressing the problem of how to leverage explanations generated by large language models (LLMs) to improve the training and reasoning capabilities of small language models (SLMs). 

Specifically, the authors are exploring whether explanations generated by LLMs, without additional human filtering or annotation, can be effectively used to improve the reasoning skills of smaller models through multi-task learning. This is an important question because SLMs are more practical for real-world deployment due to their lower computational and memory requirements compared to massive LLMs. However, SLMs tend to lag behind LLMs in reasoning ability, especially in few-shot settings. 

The key research question is: can explanations generated by LLMs, even though they may be noisy or imperfect, help boost the reasoning skills of SLMs when integrated through multi-task learning? The authors systematically investigate different explanation generation methods from LLMs and multi-task learning frameworks to incorporate those explanations into SLM training.

In summary, the main problem is how to leverage explanatory knowledge elicited from large models to improve reasoning in more practically-sized small models, with a focus on using automatically generated explanations rather than human-curated ones.


## What are the keywords or key terms associated with this paper?

 Based on reviewing the paper, some key terms and keywords are:

- Explanations - The paper focuses on using explanations generated from large language models (LLMs) to improve the training of small reasoners. Explanations are a core concept.

- Reasoning - The paper aims to improve reasoning capabilities in small models by using explanations from LLMs. Reasoning is another key focus.

- Multi-task learning - The paper utilizes a multi-task learning framework to integrate explanations from LLMs into small models. This is an important technique.

- Chain of thought - One method explored is generating explanations through chain of thought prompting in LLMs. This is a key prompting approach.  

- Rationalization prompting - Another explanation generation method is rationalization prompting in LLMs. This is another prompting style.

- In-context learning - The paper uses in-context learning with LLMs to generate explanations. This paradigm is important.

- Commonsense reasoning - One of the evaluation tasks is commonsense reasoning using the CommonsenseQA dataset. This is a key application area.

- Symbolic reasoning - Improving symbolic reasoning is a goal and motivation of the work.

- Numerical reasoning - Numerical reasoning is another evaluation task area using the StrategyQA dataset.

So in summary, the key terms cover explanations, reasoning, multi-task learning, prompting approaches, in-context learning, and reasoning tasks like commonsense and symbolic reasoning. These capture the core techniques and domains involved.


## Based on the paper, what questions should we ask to create a comprehensive summary of it? Please list at least 10 questions.

 Here are 10 potential questions to ask to create a comprehensive summary of the paper:

1. What is the main objective or purpose of this paper? 

2. What problem is the paper trying to solve? What gap in knowledge is it trying to fill?

3. What is the key hypothesis or claim made in the paper? 

4. What methodology did the authors use to test their hypothesis - what experiments, data sources, analytical techniques, etc?

5. What were the main findings or results of the study? 

6. Did the results support or reject the original hypothesis? 

7. What are the key conclusions drawn from the results? How do the authors interpret the findings?

8. What are the limitations of the study as acknowledged by the authors?

9. What are the broader implications of this research according to the authors? How could it impact the field?

10. What future work do the authors suggest could follow on from this study? What open questions remain?


## Design 10 in-depth questions about method proposed in this paper and avoid simple question.

 Here are 10 potential in-depth questions about the method proposed in this paper:

1. The paper proposes using explanations generated by large language models (LLMs) to improve the training of small reasoners. What are the potential advantages and disadvantages of using LLM-generated explanations compared to human-annotated explanations?

2. The paper explores three different approaches for generating explanations from LLMs - chain of thought prompting (COTE), rationalization prompting (RP), and a hybrid approach (CROP). How do these methods differ in terms of the quality and coverage of the explanations generated? What are the trade-offs between them?

3. The paper adopts a multi-task learning framework to incorporate the LLM-generated explanations into training the small reasoners. How does this framework allow the model to leverage explanations that are only partially generated for the training set? What are the benefits of multi-task learning compared to other approaches like pre-training or fine-tuning?

4. The paper proposes a new multi-task learning approach called MT-CoT that trains the model to generate an explanation followed by the answer. How does this differ from prior work like MT-Re and MT-Ra? Why might training the model in this sequence promote better reasoning capabilities?

5. The three datasets used for evaluation - CommonsenseQA, StrategyQA, and OpenbookQA - involve different types of reasoning skills. How do the gains from using LLM-generated explanations vary across the datasets? What does this suggest about the benefits for different reasoning tasks?

6. The paper shows gains from using explanations even when training large models like T5-3B. Why might explanations still be beneficial despite the greater reasoning capacity of larger models? What are the limitations?

7. The paper demonstrates that the approach works well in low-data regimes. What factors contribute to the greater improvements observed when less training data is available? How does the multi-task learning framework adapt in the low-data setting?

8. The human evaluation results show that the model can generate high quality explanations compared to GPT-3. What metrics or analyses could be done to further assess the quality of the generated explanations? How could the explanation abilities be improved?

9. The paper focuses on textual reasoning tasks. What considerations would be important for applying this approach to other domains like symbolic or arithmetic reasoning? What challenges might arise?

10. The paper identifies potential biases that could be propagated from biased LLM-generated explanations. How might the multi-task learning framework be adapted to mitigate absorbing or amplifying these biases? What safeguards could be implemented?


## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a high-quality paragraph summarizing the key points of the paper:

This paper explores using explanations generated by large language models (LLMs) like GPT-3 to improve the reasoning capabilities of smaller language models through multi-task learning. The authors systematically compare three approaches for generating explanations from LLMs: chain of thought prompting with incorrect answer rejection (COTE), rationalization prompting (RP), and a hybrid approach (CROP). They then train the small models with these LLM-generated explanations using three multi-task setups: generating explanations then answers (MT-Re), rationalizations conditioned on answers (MT-Ra), and chain of thought before answers (MT-CoT). Experiments on CommonsenseQA, StrategyQA, and OpenbookQA show that multi-task learning with LLM explanations consistently and significantly improves reasoning over standard finetuning, with the hybrid CROP approach and proposed MT-CoT setup achieving the best results. The method even outperforms finetuning or prompting much larger LLMs like GPT-3 in some cases. Human evaluation also indicates the small models trained this way generate high-quality explanations. Overall, this work demonstrates that explanations from LLMs can meaningfully improve small model reasoning and explainability through multi-task learning.


## Summarize the paper in one sentence.

 This paper proposes using explanations generated by large language models to improve reasoning capabilities of small language models through multi-task learning.


## Summarize the paper in one paragraphs.

 Here is a one paragraph summary of the paper:

This paper explores using explanations generated by large language models (LLMs) like GPT-3 to improve the reasoning capability and explainability of smaller language models through multi-task learning. The authors systematically compare three methods of generating explanations from LLMs - chain of thought prompting, rationalization prompting, and a hybrid approach. They then train smaller models like T5 using these explanations in three multi-task setups and evaluate on reasoning datasets. Experiments show their method consistently and significantly outperforms standard fine-tuning, even surpassing finetuning a 60x larger GPT-3 model on CommonsenseQA by 9.5% accuracy. The method also produces high quality explanations according to human evaluation, demonstrating its potential for more explainable AI. Overall, the paper shows that explanations from LLMs can be effectively leveraged through multi-task learning to make small models better reasoners and explain their predictions.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in this paper:

1. The paper proposes using explanations generated by large language models (LLMs) to improve the training of small reasoners. Can you elaborate on why explanations from LLMs are more useful for this task compared to human-annotated explanations? What are the key advantages and disadvantages?

2. The paper explores three approaches for generating explanations from LLMs - COTE, RP, and CROP. Can you explain the key differences between these three methods? When would you expect each approach to perform better or worse?

3. The paper adopts a multi-task learning framework to utilize the LLM-generated explanations. Can you discuss the benefits of using multi-task learning compared to other techniques like finetuning or two-stage training? Why is multi-task learning well-suited for this problem?

4. The paper proposes a new multi-task learning method called MT-CoT. How does MT-CoT differ from existing methods like MT-Re and MT-Ra? Why does positioning the answer after the explanation help the model learn better?

5. The results show that explanations help even very large T5 models. Why do you think explanations are still useful despite the model already being so large? When would you expect the benefits to diminish?

6. The paper demonstrates strong performance compared to prompting large GPT-3 models. What factors allow the method to outperform even much larger language models? When would you expect GPT-3 to perform better?

7. Can you discuss any potential drawbacks or limitations of using LLM-generated explanations? For example, issues around bias or factual correctness. How might these be addressed?

8. The paper focuses on textual reasoning tasks. How do you think the approach would need to be adapted for other reasoning tasks like arithmetic or symbolic reasoning? What new challenges might arise?

9. The paper uses a fixed set of human demonstrations when generating explanations. How important do you think the demonstration examples are? What if no human demonstrations were provided?

10. The paper argues that explanations improve reasoning capability without sacrificing accuracy. Based on the results, do you think there is any accuracy vs. explanation quality trade-off occurring? Why or why not?
