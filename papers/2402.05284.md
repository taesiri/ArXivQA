# [Analyzing Adversarial Inputs in Deep Reinforcement Learning](https://arxiv.org/abs/2402.05284)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Deep reinforcement learning (DRL) agents are susceptible to adversarial inputs - small perturbations to inputs that cause unpredictable/dangerous behavior. This limits deployment of DRL in safety-critical applications.
- Prior methods to improve reliability have limitations:
    - Training-based methods only provide safety in expectation, don't analyze underlying problem.
    - Formal verification methods identify adversarial inputs but don't provide in-depth characterization and analysis.
- There is a need for methods that formally guarantee DRL robustness while also characterizing adversarial inputs over time and space to understand their impact.

Solution:
- Introduce a new metric called "Adversarial Rate" to quantify DRL susceptibility to adversarial inputs and characterize them over input domain.
- Present tools like ProVe and CountingProVe to compute the Adversarial Rate and approximately identify number of adversarial inputs.
- Conduct comprehensive analysis on thousands of DRL agents over two benchmarks to understand spatial/temporal properties and architecture factors influencing adversarial inputs.

Key Findings:
- Adversarial inputs can be concentrated in small regions of input space, unlikely to be found by just random testing.
- Separate models trained identically can have completely different unsafe regions vulnerable to adversarial inputs. Locations of unsafe regions also shift during training.  
- Larger DNN size correlates with more adversarial inputs, activation function type does not.

Contributions:
- New metric and tools to quantify and characterize adversarial inputs for DRL agents
- First analysis showing adversarial inputs are inherent property of neural networks themselves, not training
- Analysis providing spatial, temporal and architecture-specific understanding of impact of adversarial inputs
- Guidelines and insights on detecting and potentially mitigating effect of adversarial inputs on DRL safety

The paper provides the first formalization and in-depth analysis of adversarial inputs for DRL agents. It gives useful practical guidelines while opening up new research directions into ensuring safe and reliable DRL.
