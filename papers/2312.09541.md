# [Picking the Underused Heads: A Network Pruning Perspective of Attention   Head Selection for Fusing Dialogue Coreference Information](https://arxiv.org/abs/2312.09541)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Transformer models like BERT have shown state-of-the-art performance on NLP tasks, but still struggle on language generation tasks requiring high-level reasoning, like dialogue summarization.  
- Explicitly incorporating linguistic features like coreference information can improve summarization, but often requires extra parameters.

Method:  
- Propose selecting "underused" attention heads using importance scoring, based on sensitivity of heads to being masked. 
- Manipulate these heads by replacing attention weights with structure-aware matrices encoding coreference links between mentions.  
- Compare full-link versus adjacent-link matrices and importance-based versus probing-based head selection.

Experiments:
- Test on SAMSum dialogue summarization dataset.  
- Show lower importance heads can be pruned with minimal impact on performance.
- Injecting coreference information via head manipulation improves summarization over strong baselines.  
- Importance-based selection outperforms probing-based selection.

Main Contributions:
- Novel network pruning view of incorporating linguistic knowledge by manipulating underused heads. 
- Demonstrate effectiveness of method for improving dialogue summarization via coreference injection.
- Analysis of head importance validates that manipulated heads are better utilized.
- Computationally cheaper than adding extra components.

In summary, the paper proposes a parameter-efficient method to inject beneficial linguistic knowledge into Transformer models by replacing underused attention heads with structure-aware information. Experiments on dialogue summarization demonstrate this is an effective way to improve context modeling.
