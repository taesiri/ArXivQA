# [Information structures and their cohomology](https://arxiv.org/abs/1709.07807)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be:How can entropy be characterized from a cohomological perspective, and what new insights does this topological viewpoint provide about the nature and properties of entropy?More specifically, the paper introduces a framework of "information structures" and associated "information cohomology" to give an algebraic characterization of entropy. The key ideas are:- Defining information structures as categories of observables, abstracting the relations between their outputs. This provides a unified setting to treat classical and quantum measurements.- Introducing information cohomology via derived functors, as a topological invariant associated to an information structure. - Showing that Shannon and Tsallis entropies arise naturally as 1-cocycles in this cohomology theory, with the 1-cocycle condition encoding their chain rule / additive property.- Proving that under suitable nondegeneracy conditions, these entropies are essentially the unique nontrivial 1-cocycles, providing an alternative characterization.Overall, the central hypothesis seems to be that taking a cohomological viewpoint can provide new conceptual insights and unify the understanding of entropy across different settings like classical probability, quantum mechanics, etc. The paper aims to develop the mathematical framework to support this claim.


## What is the main contribution of this paper?

This paper introduces the category of information structures and defines information cohomology on them. The key contributions are:- It provides a general definition of an information structure as a conditional meet semilattice equipped with a functor to measurable spaces. This captures both classical probability models and quantum ones. - It defines information cohomology using derived functors, extending the work of Baudot and Bennequin. The coefficients are modules over the structure ring generated by the semilattice. - It shows the bar resolution gives an explicit projective resolution that can be used to compute information cohomology. This recovers the previous cochain complexes used for classical/quantum information cohomology.- For probabilities as coefficients, it proves the only 1-cocycles are Shannon or Tsallis entropy, depending on the module parameter Î±. This generalizes a previous characterization result.- It studies how the 1-cohomology behaves under different conditions on the information structure, relating it to connectivity properties. Entropy behaves like a "fundamental class" in nicely connected cases.In summary, the main contribution is providing a unified framework to define and study information cohomology across classical and quantum settings, using categorical and homological techniques. The computations characterize entropy through its cocycle property.


## How does this paper compare to other research in the same field?

Here are a few thoughts on how this paper compares to other research in the field of information theory and entropy:- The paper takes a novel, categorical approach to studying entropy by introducing "information structures" and associating entropy with cohomology classes. This is quite different from most information theory research that studies entropy from an axiomatic, probabilistic, or algorithmic perspective. The categorical framework seems innovative.- Relating entropy to cohomology classes provides an algebraic perspective on entropy. Some other algebraic approaches have been taken before, like studying entropy in terms of solutions to functional equations. But formulating entropy in terms of cocycles seems new. - The paper connects entropy to the combinatorics encoded in the information structures. This differs from most information theory that focuses just on the probabilistic or statistical properties of entropy. Looking at the combinatorial foundations seems interesting.- The generality of information structures means the results could apply across classical probability, quantum mechanics, continuous variables, etc. This is more broad than most entropy research that sticks to one domain like discrete random variables. The unified approach is noteworthy.- Compared to abstract approaches that axiomatize entropy from scratch, this work builds on existing definitions of entropy like Shannon and Tsallis. So it leverages established knowledge while providing new perspective.Overall, the categorical viewpoint and links to cohomology seem to distinguish this paper from prior art. The work appears novel while building on foundations of information theory. Of course, fully assessing the impact would require examining follow-up research that applies and extends these ideas. But at first glance, the approach looks innovative and promising.
