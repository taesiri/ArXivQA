# [Indiscriminate Data Poisoning Attacks on Pre-trained Feature Extractors](https://arxiv.org/abs/2402.12626)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
The paper studies the threat of data poisoning attacks on downstream tasks that utilize pre-trained feature extractors. Specifically, it considers a scenario where a feature extractor is first trained using self-supervised learning (e.g. contrastive learning) on unlabeled data. This feature extractor is then used in a downstream task, like image classification, where only a linear classifier head is trained on top of the frozen feature extractor. The paper examines if an attacker can construct a small set of poisoned samples that when added to the downstream training set, reduces the accuracy of the model on clean test data.

Proposed Solution:
The paper proposes two types of attacks:

1. Input space attacks: Existing attacks like TGDA and GC are adapted to generate poisoned images by optimizing directly in the input space while keeping the feature extractor fixed. However, adding constraints to make the poisoned images similar to real images makes these attacks much weaker. 

2. Feature targeted attacks: A 3-stage attack is proposed to alleviate the difficulties of input space attacks: (i) Acquire target parameters for the linear head using gradient corruption;  (ii) Treat learned feature representations on clean data as the dataset and construct poisoned features using GC attack to make the linear evaluation fail after retraining; (iii) Invert poisoned features back to input space using either a trained decoder network or an optimization method called feature matching.

The attacks are evaluated on two downstream tasks - fine-tuning where the pre-training and downstream dataset are the same, and transfer learning across different datasets.

Main Contributions:

- Sets baselines for data poisoning attacks on fixed feature extractors and shows transfer learning is more vulnerable.

- Shows input space attacks are effective without constraints but weaker with visually similarity constraints.

- Proposes feature targeted attacks which break the problem into multiple stages and achieve better attack performance.

- Examines unlearnable examples attack and shows it is less effective for fixed feature extractors compared to end-to-end training.

In summary, the paper exposes the threat of data poisoning attacks on pre-trained feature extractors, specially for transfer learning scenarios, and proposes feature targeted attacks that currently achieve the best attack performance with constraints.


## Summarize the paper in one sentence.

 This paper studies data poisoning attacks against downstream tasks with pre-trained feature extractors, proposing input space attacks by adapting existing methods and new feature targeted attacks that acquire poisoned features and invert them back to input space.


## What is the main contribution of this paper?

 The main contributions of this paper are:

1) It exposes the threat of indiscriminate data poisoning attacks on pre-trained feature extractors, and sets baselines on fine-tuning and transfer learning downstream tasks. It shows that transfer learning is generally more vulnerable to the attacks considered. 

2) It tailors existing attacks like TGDA, Gradient Canceling, and Unlearnable Examples to the setting of poisoning fixed future extractors, and empirically identifies when they succeed (e.g. without constraints) and fail (with constraints). 

3) It proposes new attacks called feature targeted attacks which involve acquiring target features with gradient canceling, and then inverting them back to the input space using either a trained decoder or feature matching. These attacks are shown to outperform input space attacks.

4) It examines unlearnable examples in poisoning fixed feature extractors, and shows that they become much less effective compared to end-to-end training. Overall, the work explores the vulnerability of downstream tasks with pre-trained feature extractors against different data poisoning attacks.


## What are the keywords or key terms associated with this paper?

 Based on my review of the paper, some of the key terms and keywords associated with this paper include:

- Data poisoning attacks
- Indiscriminate attacks 
- Pre-trained feature extractors
- Self-supervised learning (SSL)
- Contrastive learning 
- Fine-tuning
- Transfer learning
- Input space attacks
- Gradient Canceling (GC) attack
- Total Gradient Descent Ascent (TGDA) attack  
- Feature targeted attacks
- Decoder inversion
- Feature matching
- Unlearnable Examples (UE)

The paper focuses on studying indiscriminate data poisoning attacks, where the goal is to decrease the overall model performance, on pre-trained feature extractors used in self-supervised learning methods like contrastive learning. It looks at attacks in the context of fine-tuning the feature extractors on the same dataset used for pre-training, as well as transfer learning to adapt the feature extractors to new datasets. Different input space attacks like GC and TGDA are analyzed, in addition to proposed feature targeted attacks involving decoder inversion and feature matching. Comparisons are also made to Unlearnable Examples. So these key ideas and terminology summarize the main technical content and contributions.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 potential in-depth questions about the method proposed in this paper:

1. The paper proposes two types of attacks: input space attacks and feature targeted attacks. What are the key differences between these two attack types and what are the relative advantages/disadvantages of each?

2. For the feature targeted attacks, the paper breaks the attack down into three stages - acquiring target parameters, obtaining poisoned features, and inverting features back to input space. Why is this staged approach beneficial compared to directly optimizing in input space? 

3. When inverting poisoned features back to input space, the paper explores both decoder inversion and feature matching. What factors contribute to the relative effectiveness of these two approaches? When would you choose one over the other?

4. The constraints posed on the attacks seem to significantly impact their effectiveness. What is the intuition behind why constraining the attacks makes optimization more difficult? Are there ways this challenge could be mitigated?

5. Transfer learning tasks seem more vulnerable to the attacks than fine-tuning tasks. What factors contribute to this increased vulnerability in transfer learning scenarios?  

6. For the feature matching attack, the choice of $\beta$ controls the strength of input space constraints. How does tuning $\beta$ impact attack effectiveness and stealthiness? What are the tradeoffs?

7. The feature matching attack behaves differently for fine-tuning vs transfer learning, with some anomalous poisoned points emerging in transfer learning. What causes these anomalies and how do they impact attack analysis?

8. Error minimizing noise (EMN) attacks are much less effective compared to end-to-end training. What factors contribute to increased robustness when the feature extractor is fixed? Could this motivate new defense strategies?

9. The attacks proposed target the linear evaluation head only. Could these attacks be extended to more complex evaluation schemes like fine-tuning part/all of the feature extractor? What new challenges might emerge?

10. Feature inversion proves difficult with the proposed attacks - small feature space perturbations do not easily invert back to input space given constraints. Is this finding surprising given sensitivity results in adversarial examples literature? How might we better understand this?
