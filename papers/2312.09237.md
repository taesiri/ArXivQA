# [Pixel Aligned Language Models](https://arxiv.org/abs/2312.09237)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a detailed paragraph summarizing the key points of the paper:

This paper proposes PixelLLM, a vision-language model that can take an image and optionally a location prompt (like a box) or text prompt as input, and output a descriptive caption along with pixel locations aligned to each predicted word. The key idea is to add a small MLP regression head in parallel to the language model's vocabulary output layer, which predicts a 2D location per word token. The model is pretrained on the Localized Narratives dataset containing images, captions, and mouse cursor trajectories tracing the caption words. This provides full supervision for aligning each word to an image location. PixelLLM achieves state-of-the-art results on referring expression localization, dense object captioning, and location-conditioned image captioning. Ablations demonstrate the benefits of the dense regression supervision and increased language model capacity. The paper makes notable contributions in enabling language models to perform fine-grained localization and spatial reasoning, which are important abilities for general intelligence. The model is flexible and extensible to various vision-language tasks involving locations.
