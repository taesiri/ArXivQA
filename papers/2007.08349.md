# [Natural Graph Networks](https://arxiv.org/abs/2007.08349)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is: How can we design graph neural networks that are equivariant to global node permutations while using only local computations?The key points are:- Traditional graph convolutional networks (GCNs) that aggregate messages from neighbors are only invariant to local node permutations, limiting their expressiveness. - Global equivariant graph networks (EGNs) treat the entire graph as a whole, making them computationally expensive. - This paper proposes "natural graph networks" that are equivariant to global node permutations but implemented via efficient local computations.- The key idea is that a global node permutation when restricted to a local neighborhood induces a local isomorphism. So a graph network can be made globally equivariant by constraining the message passing kernels to be equivariant to these local isomorphisms.- They introduce global and local natural graph networks to capture this idea, with the latter being scalable like GCNs.- The local natural networks share weights between locally isomorphic neighborhoods and satisfy kernel constraints based on local symmetries. - The kernels can be parameterized with a message network, where the kernel constraint arises from the message network being equivariant.So in summary, the central hypothesis is that global equivariance can be achieved via constraints on local message passing, yielding expressive and scalable graph networks. The natural graph network framework formalizes this idea.


## What is the main contribution of this paper?

The main contribution of this paper is proposing a new framework for building graph neural networks called natural graph networks (NGNs). The key ideas are:- Using the concept of naturality from category theory to construct graph networks that are equivariant to global node permutations but use only local computations. - Defining global NGNs where layers satisfy a naturality condition so that isomorphic graphs are processed equivalently.- Defining local NGNs where message passing kernels satisfy naturality so global equivariance emerges.- Showing local NGNs generalize previous approaches like graph convolutional networks and covariant compositional networks.- Giving a parameterization of local NGNs using an equivariant message network, which allows constructing universal and practical NGNs.- Providing experiments on icosahedral MNIST and graph classification benchmarks showing competitive performance of a NGN model using a simple GCN-based message network.In summary, the main contribution is introducing the NGN framework to get powerful and scalable graph networks based on the concept of naturality from category theory. NGNs open up a broader space of graph networks compared to previous approaches.


## How does this paper compare to other research in the same field?

Based on the abstract and overview, this paper introduces the concept of natural graph networks, which is a new framework for building neural networks on graph-structured data. Here are some key ways I see this paper relating to other research on graph neural networks:- It generalizes the idea of equivariant graph networks, which require processing isomorphic graphs in the same way, to allow more flexibility by using "naturality" from category theory. This opens up a bigger class of graph networks.- It proposes global and local natural graph networks. The local ones seem more practical as they have lower computational complexity like conventional message passing networks, while still being more expressive.- The idea of using category theory and "natural" transformations to develop graph networks seems novel. Other work like equivariant or convolutional graph networks is based more on symmetries and invariance. The naturality condition in this paper is a less restrictive way to get a well-defined graph network.- For practical networks, it suggests parameterizing the kernel using a message passing neural network, which allows weight sharing and makes the model more feasible for irregular graphs. This is similar to other work using message passing networks.- When applied to regular graphs, the natural networks reduce to conventional CNNs or gauge CNNs on manifolds. So it generalizes these as well.- It shows competitive performance on several graph classification benchmarks, suggesting the natural networks work well in practice while being more flexible.Overall, the paper seems to propose a new perspective on building graph networks using category theory and naturality. The resulting models are scalable like message passing networks but more expressive. The ideas seem generally applicable, though more extensive evaluation on real-world problems would be helpful. The connections to CNNs are also interesting.


## What future research directions do the authors suggest?

Based on my reading of the paper, some of the main future research directions suggested by the authors include:- Developing more efficient algorithms and implementations for natural graph networks. The authors note that their method currently has around 2x higher computational cost compared to standard message passing networks. Reducing this cost through more optimized implementations could help improve scalability.- Exploring different choices for the node/edge neighborhoods and representations in natural graph networks. The authors suggest the neighborhoods and representations used are important design choices that impact expressivity vs efficiency trade-offs. More work could be done to understand good heuristics for choosing these.- Applying the natural network framework to other domains beyond graphs, such as sets or sequences. The authors suggest the mathematical framework of naturality may have broader applicability for building equivariant networks in other domains.- Combining ideas from natural networks with other advanced graph network architectures like Graph Attention Networks or GraphSAGE. The natural network approach provides a different notion of equivariance than these methods, and combining strengths could lead to improved models.- Developing theoretical understandings of the representational power and limitations of natural graph networks compared to other graph network architectures. The authors provide some initial expressivity experiments but more theoretical analysis could be done.- Exploring different choices for parameterizing the message passing kernels, beyond using a message network. The message network gives one way to parameterize the kernels while satisfying naturality, but there may be other more efficient or expressive approaches.In summary, the main directions are improving efficiency/scalability, better understanding design choices like neighborhoods and representations, applying the natural network framework more broadly, combining with other graph network methods, and developing more theoretical understandings of the approach.


## Summarize the paper in two paragraphs.

Here is a two paragraph summary of the paper:The paper presents a new framework called natural graph networks (NGNs) for building neural networks on graph-structured data. NGNs are designed to process graphs irrespective of how they are encoded, an important property since graph encodings are highly non-unique. The key idea is that NGNs should satisfy a "naturality" constraint, meaning they give the same output for isomorphic graphs. This is more general than requiring equivariance to node permutations like in many existing graph networks. The authors define global NGNs which apply the naturality constraint across the whole graph, and local NGNs which only apply it locally through message passing. Local NGNs are more scalable but still capture global properties through the relationship between local and global isomorphisms. The authors give a specific instantiation using an equivariant message network that performs well on several graph classification benchmarks. Overall, the NGN framework allows maximally flexible graph networks while still ensuring they are well-defined on graph-structured data.


## Summarize the main method used in the paper in one paragraph.

The paper proposes a new framework called natural graph networks for building graph neural networks. The key idea is to use the concept of naturality from category theory to ensure the network processes graphs in a way that does not depend on how the graph is represented. The authors define global and local natural graph networks. Global natural graph networks process the entire graph in an equivariant way to graph isomorphisms. Local natural graph networks are more scalable and consist of message passing between nodes where the message functions satisfy a local naturality constraint - they commute with isomorphisms of the local node neighbourhoods.The authors also propose a practical instantiation of a local natural graph network called GCN2 which uses a Graph Convolutional Network (GCN) to parameterize the message functions. This allows sharing weights between locally isomorphic neighbourhoods while preserving expressiveness. The GCN2 model achieves competitive performance on several graph classification benchmarks.In summary, the main contribution is introducing the naturality concept from category theory to graph neural networks to ensure representations do not depend on graph encodings. This enables more flexible graph networks that can leverage local symmetries.
