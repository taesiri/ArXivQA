# [Natural Graph Networks](https://arxiv.org/abs/2007.08349)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is: How can we design graph neural networks that are equivariant to global node permutations while using only local computations?The key points are:- Traditional graph convolutional networks (GCNs) that aggregate messages from neighbors are only invariant to local node permutations, limiting their expressiveness. - Global equivariant graph networks (EGNs) treat the entire graph as a whole, making them computationally expensive. - This paper proposes "natural graph networks" that are equivariant to global node permutations but implemented via efficient local computations.- The key idea is that a global node permutation when restricted to a local neighborhood induces a local isomorphism. So a graph network can be made globally equivariant by constraining the message passing kernels to be equivariant to these local isomorphisms.- They introduce global and local natural graph networks to capture this idea, with the latter being scalable like GCNs.- The local natural networks share weights between locally isomorphic neighborhoods and satisfy kernel constraints based on local symmetries. - The kernels can be parameterized with a message network, where the kernel constraint arises from the message network being equivariant.So in summary, the central hypothesis is that global equivariance can be achieved via constraints on local message passing, yielding expressive and scalable graph networks. The natural graph network framework formalizes this idea.


## What is the main contribution of this paper?

The main contribution of this paper is proposing a new framework for building graph neural networks called natural graph networks (NGNs). The key ideas are:- Using the concept of naturality from category theory to construct graph networks that are equivariant to global node permutations but use only local computations. - Defining global NGNs where layers satisfy a naturality condition so that isomorphic graphs are processed equivalently.- Defining local NGNs where message passing kernels satisfy naturality so global equivariance emerges.- Showing local NGNs generalize previous approaches like graph convolutional networks and covariant compositional networks.- Giving a parameterization of local NGNs using an equivariant message network, which allows constructing universal and practical NGNs.- Providing experiments on icosahedral MNIST and graph classification benchmarks showing competitive performance of a NGN model using a simple GCN-based message network.In summary, the main contribution is introducing the NGN framework to get powerful and scalable graph networks based on the concept of naturality from category theory. NGNs open up a broader space of graph networks compared to previous approaches.


## How does this paper compare to other research in the same field?

Based on the abstract and overview, this paper introduces the concept of natural graph networks, which is a new framework for building neural networks on graph-structured data. Here are some key ways I see this paper relating to other research on graph neural networks:- It generalizes the idea of equivariant graph networks, which require processing isomorphic graphs in the same way, to allow more flexibility by using "naturality" from category theory. This opens up a bigger class of graph networks.- It proposes global and local natural graph networks. The local ones seem more practical as they have lower computational complexity like conventional message passing networks, while still being more expressive.- The idea of using category theory and "natural" transformations to develop graph networks seems novel. Other work like equivariant or convolutional graph networks is based more on symmetries and invariance. The naturality condition in this paper is a less restrictive way to get a well-defined graph network.- For practical networks, it suggests parameterizing the kernel using a message passing neural network, which allows weight sharing and makes the model more feasible for irregular graphs. This is similar to other work using message passing networks.- When applied to regular graphs, the natural networks reduce to conventional CNNs or gauge CNNs on manifolds. So it generalizes these as well.- It shows competitive performance on several graph classification benchmarks, suggesting the natural networks work well in practice while being more flexible.Overall, the paper seems to propose a new perspective on building graph networks using category theory and naturality. The resulting models are scalable like message passing networks but more expressive. The ideas seem generally applicable, though more extensive evaluation on real-world problems would be helpful. The connections to CNNs are also interesting.
