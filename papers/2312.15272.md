# [Detecting anxiety from short clips of free-form speech](https://arxiv.org/abs/2312.15272)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Mental health issues like anxiety are prevalent but often go undiagnosed due to associated stigma and lack of proper assessments. Automated methods for diagnosis using speech analysis can help address this.

- There is some prior work showing correlations between certain speech features (e.g. pitch, jitter, emotion etc.) and anxiety disorders. However, most models are simple and use only audio or only text. Complex models leveraging both modalities are lacking.

Proposed Solution:
- The authors experiment with multiple machine learning models of varying complexity for anxiety detection from audio journals, using a novel dataset from Kintsugi.

- They implement models utilizing (1) handcrafted audio features, (2) sentence embeddings of transcripts (3) speech features from Wav2Vec (4) multi-modal features combining Wav2Vec speech embeddings and RoBERTa sentence embeddings.

- The multi-modal model quantizes speech features, embeds them using Speech-RoBERTa. It also embeds transcripts using standard RoBERTa. These embeddings are concatenated and fed to an SVM classifier.

Main Contributions:
- Analysis of a new anxiety speech dataset with 2263 audio samples and different feature distributions. 

- Benchmarking of various audio, text and multi-modal machine learning approaches for anxiety detection, with the top model achieving AUC ROC of 0.69.

- Demonstration that both audio and text modalities provide complementary signal for the task, though their fusion does not show significant gains over individual models.

In summary, the paper presents a systematic study of anxiety classification models using speech, evaluates multiple model architectures, and analyzes a novel mental health speech dataset. The multi-modal approach is promising but needs further improvement in the future.
