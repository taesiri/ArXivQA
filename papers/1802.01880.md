# [Learning Image Representations by Completing Damaged Jigsaw Puzzles](https://arxiv.org/abs/1802.01880)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central hypothesis of this paper is that making self-supervised learning tasks more challenging can help learn more robust and generalizable visual representations. Specifically, the authors propose complicating existing self-supervised learning tasks like jigsaw puzzle solving, inpainting, and colorization by adding additional constraints or corruptions to the data, making the tasks harder. Their hypothesis is that by solving these more complicated self-supervised problems, neural networks will be forced to learn more robust visual features that capture higher-level semantics and generalize better to downstream tasks.To test this, they design complicated versions of the individual tasks, as well as a joint task called "Completing Damaged Jigsaw Puzzles" which requires solving all three complicated self-supervised problems together. They then evaluate the learned representations on transfer learning for classification, detection, and segmentation on PASCAL VOC, and find their approach outperforms the original self-supervised tasks as well as simple combinations of them. This supports their central hypothesis that complicating self-supervised tasks is an effective technique for representation learning.In summary, the key hypothesis is that making self-supervised learning harder via data corruptions forces models to learn more robust and transferable visual features. The experiments on complicated self-supervised learning tasks support this idea.


## What is the main contribution of this paper?

The main contribution of this paper is proposing a new self-supervised learning approach to learn robust and generalizable image representations by complicating existing self-supervised tasks. Specifically:- They complicate existing self-supervised tasks of jigsaw puzzle solving, image inpainting, and colorization by adding additional challenges like removing color channels or image regions. This forces the model to learn more robust representations to handle the more difficult tasks. - They propose a novel self-supervised task called "Completing Damaged Jigsaw Puzzles" which combines aspects of all three tasks - shuffling image patches, removing one patch, and removing color from the patches. This jointly optimizes and combines the different self-supervised objectives.- They show that their approach of complicating self-supervised tasks outperforms the original tasks, as well as simple combinations of tasks, on transfer learning benchmarks like PASCAL VOC classification and segmentation. The features learned by their model achieve state-of-the-art performance among self-supervised methods when transferred to an AlexNet classifier.In summary, the key contribution is a new training methodology of intentionally complicating self-supervised tasks to push models to learn more robust and generalizable representations, instead of simply combining existing self-supervised tasks. The damaged jigsaw puzzle task provides an effective instantiation of this approach.
