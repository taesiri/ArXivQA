# [Learning Image Representations by Completing Damaged Jigsaw Puzzles](https://arxiv.org/abs/1802.01880)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central hypothesis of this paper is that making self-supervised learning tasks more challenging can help learn more robust and generalizable visual representations. Specifically, the authors propose complicating existing self-supervised learning tasks like jigsaw puzzle solving, inpainting, and colorization by adding additional constraints or corruptions to the data, making the tasks harder. Their hypothesis is that by solving these more complicated self-supervised problems, neural networks will be forced to learn more robust visual features that capture higher-level semantics and generalize better to downstream tasks.To test this, they design complicated versions of the individual tasks, as well as a joint task called "Completing Damaged Jigsaw Puzzles" which requires solving all three complicated self-supervised problems together. They then evaluate the learned representations on transfer learning for classification, detection, and segmentation on PASCAL VOC, and find their approach outperforms the original self-supervised tasks as well as simple combinations of them. This supports their central hypothesis that complicating self-supervised tasks is an effective technique for representation learning.In summary, the key hypothesis is that making self-supervised learning harder via data corruptions forces models to learn more robust and transferable visual features. The experiments on complicated self-supervised learning tasks support this idea.
