# [Learning Image Representations by Completing Damaged Jigsaw Puzzles](https://arxiv.org/abs/1802.01880)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central hypothesis of this paper is that making self-supervised learning tasks more challenging can help learn more robust and generalizable visual representations. Specifically, the authors propose complicating existing self-supervised learning tasks like jigsaw puzzle solving, inpainting, and colorization by adding additional constraints or corruptions to the data, making the tasks harder. Their hypothesis is that by solving these more complicated self-supervised problems, neural networks will be forced to learn more robust visual features that capture higher-level semantics and generalize better to downstream tasks.To test this, they design complicated versions of the individual tasks, as well as a joint task called "Completing Damaged Jigsaw Puzzles" which requires solving all three complicated self-supervised problems together. They then evaluate the learned representations on transfer learning for classification, detection, and segmentation on PASCAL VOC, and find their approach outperforms the original self-supervised tasks as well as simple combinations of them. This supports their central hypothesis that complicating self-supervised tasks is an effective technique for representation learning.In summary, the key hypothesis is that making self-supervised learning harder via data corruptions forces models to learn more robust and transferable visual features. The experiments on complicated self-supervised learning tasks support this idea.


## What is the main contribution of this paper?

The main contribution of this paper is proposing a new self-supervised learning approach to learn robust and generalizable image representations by complicating existing self-supervised tasks. Specifically:- They complicate existing self-supervised tasks of jigsaw puzzle solving, image inpainting, and colorization by adding additional challenges like removing color channels or image regions. This forces the model to learn more robust representations to handle the more difficult tasks. - They propose a novel self-supervised task called "Completing Damaged Jigsaw Puzzles" which combines aspects of all three tasks - shuffling image patches, removing one patch, and removing color from the patches. This jointly optimizes and combines the different self-supervised objectives.- They show that their approach of complicating self-supervised tasks outperforms the original tasks, as well as simple combinations of tasks, on transfer learning benchmarks like PASCAL VOC classification and segmentation. The features learned by their model achieve state-of-the-art performance among self-supervised methods when transferred to an AlexNet classifier.In summary, the key contribution is a new training methodology of intentionally complicating self-supervised tasks to push models to learn more robust and generalizable representations, instead of simply combining existing self-supervised tasks. The damaged jigsaw puzzle task provides an effective instantiation of this approach.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence TL;DR summary of the paper:The paper proposes a new self-supervised learning method called "Completing Damaged Jigsaw Puzzles" that complicates existing self-supervision tasks like jigsaw puzzles, inpainting, and colorization by adding damage and shows this approach learns more robust and transferable visual representations that achieve state-of-the-art performance on PASCAL classification and segmentation.


## How does this paper compare to other research in the same field?

This paper presents a novel approach for self-supervised representation learning by complicating existing self-supervised tasks and combining them into a joint problem of "Completing damaged jigsaw puzzles." Here is how it relates to other recent work in self-supervised representation learning:- It builds on top of popular self-supervised tasks like jigsaw puzzles, inpainting, and colorization by making them more challenging. This is a unique strategy compared to most prior work that uses the standard formulations of these tasks. - It combines multiple self-supervised tasks into one joint problem. Some other recent papers have also explored multi-task self-supervised learning, like Doersch and Zisserman (2017) and Wang et al. (2017). However, this paper's approach of complicating the tasks and combining them into a joint problem is novel.- The joint "damaged jigsaw puzzle" task requires the model to integrate different types of visual reasoning (spatial, contextual, cross-channel) in order to succeed. This encourages the learning of rich, multi-purpose representations compared to methods relying on a single supervisory signal.- It demonstrates state-of-the-art transfer learning performance on PASCAL classification and segmentation compared to prior self-supervised approaches. This shows the effectiveness of the proposed strategy.- It focuses on learning representations using only grayscale images during pre-training. Most prior work uses color images. This makes the model learn more robust features less dependent on color cues.In summary, this paper introduces a new perspective to self-supervised learning by intentionally complicating tasks and combining them in an integrated manner. The joint task requires sophisticated visual reasoning leading to beneficial representations as evidenced by strong transfer learning performance. The approach is novel compared to prior self-supervised learning papers.
