# [WDiscOOD: Out-of-Distribution Detection via Whitened Linear Discriminant   Analysis](https://arxiv.org/abs/2303.07543)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the abstract, this paper does not seem to pose a specific research question or hypothesis. Rather, it provides a review and overview of the current state of knowledge regarding bias in machine learning systems. The key points I gathered are:

- Machine learning systems can inherit and amplify bias present in data and society. This is a concern as these systems are increasingly used in high-stakes domains like criminal justice, healthcare, and hiring.

- Sources of bias include historical biases encoded in data, representation bias in datasets, biases in human labeling, technical bias due to optimization objectives, and biases in real-world deployment. 

- Mitigation strategies involve techniques like data augmentation, reweighting, and changes to model architecture/training. But there are challenges around detecting bias, defining fairness, and making tradeoffs.

- Understanding how bias manifests in ML systems requires interdisciplinary collaboration drawing on social sciences, ethics, law, and human-computer interaction.

So in summary, this paper seems aimed at providing a broad synthesis of the current knowledge and open challenges related to bias in machine learning, rather than testing a specific hypothesis. The review spans different types of biases, sources, measurement techniques, mitigations, and need for interdisciplinary perspectives.


## What is the main contribution of this paper?

 Based on my reading of the paper, the main contributions appear to be:

- Proposing a new out-of-distribution (OOD) detection method called WDiscOOD that is based on Whitened Linear Discriminant Analysis (WLDA). The key idea is to use WLDA to project image features into two subspaces - a discriminative subspace and a residual subspace. 

- Demonstrating that the proposed residual subspace, referred to as the Whitened Discriminative Residual Space (WDRS), is highly effective for OOD detection. This provides new insights into how OOD samples may trigger different activations compared to in-distribution samples.

- Achieving state-of-the-art OOD detection performance on ImageNet-1k benchmark using various classifier models (CNNs and Vision Transformers) as well as contrastive visual encoders (SupCon and CLIP models). The method is shown to be robust across different model architectures and training objectives.

- Providing ablation studies and analysis to understand the contributions of different components of the proposed method, such as feature whitening, the discriminative vs residual subspaces, and method hyperparameters.

In summary, the main contribution appears to be proposing a new OOD detection method that leverages WLDA to reason about class-specific and class-agnostic information purely in the feature space, without needing classifier outputs. The efficacy of the residual subspace for OOD detection is also an important discovery of this work.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence TL;DR summary of the paper:

The paper proposes a new out-of-distribution detection method called WDiscOOD that projects image features into discriminative and residual subspaces using whitened linear discriminant analysis and combines the distance to class centroids and overall dataset centroid in each subspace as an anomaly score.


## How does this paper compare to other research in the same field?

 Here are a few key points comparing this paper to other related work in out-of-distribution detection for deep neural networks:

- The main contribution is proposing a new OOD detection method based on Whitened Linear Discriminant Analysis (WLDA). This differs from most prior work that focuses on analyzing the output softmax probabilities or logits. 

- The key novelty is using WLDA to project features into a discriminative subspace and a residual subspace, then combining distances in both spaces for the final OOD score. This allows jointly leveraging class-specific and class-agnostic information without relying on a classifier head.

- Most prior feature space methods like Mahalanobis distance only look at distance to the overall training data distribution. Analyzing the discriminative vs residual subspaces provides more fine-grained signal.

- The proposed method achieves state-of-the-art results on ImageNet compared to prior feature space approaches like Mahalanobis distance and KNN. It also outperforms many classifier-based methods without requiring network modifications.

- The method demonstrates strong performance across different network architectures (CNNs and Vision Transformers) and also generalizes well to contrastive representation learning models like SupCon and CLIP.

- Analysis shows the residual subspace is highly informative for OOD, which differs from prior subspace techniques like principle components that assume minimal OOD information.

In summary, this paper introduces a novel perspective based on WLDA for OOD detection that achieves superior performance compared to prior arts. The insight on the informativeness of the residual subspace is also an interesting finding compared to related subspace methods.
