# [Set-level Guidance Attack: Boosting Adversarial Transferability of   Vision-Language Pre-training Models](https://arxiv.org/abs/2307.14061)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central research question this paper tries to address is: How can we generate more transferable adversarial examples to attack vision-language pre-training (VLP) models? Specifically, the paper observes that existing attack methods exhibit strong performance in white-box settings but have limited transferability when attacking other black-box VLP models. Thus, the authors aim to develop a more transferable attack that can fool different VLP models in black-box settings. To achieve this, the paper proposes a new attack method called Set-level Guidance Attack (SGA) that enhances adversarial transferability in two main ways:1) By using alignment-preserving augmentations to create diverse multimodal input sets rather than just single examples. This increases variability and makes the attack more generalizable.2) By incorporating cross-modal guidance to disrupt alignments and interactions between modalities when generating adversarial examples. This makes the attack less dependent on specifics of individual models.The central hypothesis is that by using these techniques, SGA can craft adversarial examples that will more effectively transfer to other black-box VLP models compared to prior attack methods. The experiments then aim to validate whether SGA indeed provides stronger transferability across different models and tasks.In summary, the key research question is how to improve transferability of adversarial attacks on VLP models, with the central hypothesis that techniques like alignment-preserving augmentation and cross-modal guidance in SGA will allow for more transferable attacks. The paper aims to demonstrate and validate this hypothesis.


## What is the main contribution of this paper?

This paper presents Set-level Guidance Attack (SGA), a novel adversarial attack method to generate transferable adversarial examples for vision-language pre-training (VLP) models. The key contributions are:1. SGA is the first work to systematically study and improve the transferability of adversarial attacks on VLP models. It provides empirical analysis showing existing attacks exhibit poor transferability across models. 2. SGA proposes using alignment-preserving augmentations with cross-modal guidance to enhance transferability. It leverages diverse multimodal interactions through set-level image and text data.3. Extensive experiments demonstrate SGA consistently improves transferability over state-of-the-art attacks on various VLP models and tasks. On image-text retrieval, SGA boosts attack success rate by over 30% when transferring across models.4. Analysis shows alignment diversity and cross-modal guidance are crucial for transferability. The scale-invariant image set and matching caption set help align perturbations to generalize across models.In summary, the main innovation is using set-level augmentations and cross-modal guidance to enable highly transferable adversarial attacks on multimodal VLP models. SGA significantly outperforms prior work and provides a robust benchmark for evaluating model security.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper: The paper proposes a highly transferable adversarial attack method called Set-level Guidance Attack (SGA) that generates adversarial examples for vision-language pre-trained models by utilizing diverse cross-modal interactions among multiple image-text pairs.
