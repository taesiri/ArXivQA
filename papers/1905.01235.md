# [Scaling and Benchmarking Self-Supervised Visual Representation Learning](https://arxiv.org/abs/1905.01235)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central research question this paper addresses is: How does scaling up self-supervised learning along different axes (data size, model capacity, problem complexity) affect the quality of learned visual representations? The key hypotheses explored are:1. Increasing the pre-training data size for self-supervised methods will lead to better visual representations that transfer better to downstream tasks.2. Using higher capacity models like ResNet for self-supervised pre-training will allow taking better advantage of larger datasets. 3. Increasing the complexity/difficulty of the pretext tasks will lead to learning more transferable representations, especially when using higher capacity models.4. Scaling up self-supervised learning along these axes could allow it to match or exceed the performance of supervised pre-training on various computer vision tasks.The paper conducts a detailed empirical evaluation of these hypotheses by pre-training self-supervised models on up to 100 million images, using AlexNet and ResNet architectures, and modifying the pretext tasks to make them more complex. The quality of representations is evaluated by transfer learning on a diverse set of 9 vision tasks.In summary, this paper focuses on rigorously examining if and how scaling up self-supervised learning can help it achieve better visual representations than supervised pre-training, which is considered the key open question and end goal for this area of research.
