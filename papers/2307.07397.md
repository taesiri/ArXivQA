# [Improving Zero-Shot Generalization for CLIP with Synthesized Prompts](https://arxiv.org/abs/2307.07397)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question seems to be: 

How can we improve the performance of pretrained vision-language models like CLIP on downstream classification tasks, for categories both with and without available labeled data?

The key hypotheses appear to be:

1) Existing fine-tuning methods for CLIP require full labeled data for all classes, which is impractical for real-world scenarios where some classes lack any labeled data due to the long tail. 

2) A generative approach can be used to synthesize features for classes without any labeled data, allowing existing fine-tuning methods to leverage both real and synthesized samples.

3) Variational autoencoders are more suitable for this generative approach compared to GANs, as they can be effectively trained with limited data. 

4) Utilizing the pretrained CLIP text encoder in the generative model results in higher quality synthesized features compared to training a generator from scratch.

5) Augmenting existing CLIP fine-tuning techniques like CoOp, Tip-Adapter etc. with the proposed generative model can improve their performance on classes without labeled data while maintaining accuracy on classes with data.

In summary, the central hypothesis is that a data-efficient generative model can synthesize features for classes lacking labeled data, enabling improved fine-tuning of CLIP on downstream tasks containing both labeled and unlabeled classes.


## What is the main contribution of this paper?

 This paper presents a method called SyntHesIzed Prompts (SHIP) to improve the performance of vision-language models like CLIP on downstream tasks where some classes have no labeled data. The key contributions are:

- Proposes a generative approach called SHIP that can synthesize features for label-only classes by inputting class names into a generator. This allows combining synthesized features with original labeled data to improve fine-tuning.

- Uses a variational autoencoder framework for the generator instead of GANs for better data efficiency and stability. 

- Introduces a CLIP-based generator that reconstructs features by inputting synthesized prompts and class names into the CLIP text encoder. Leverages CLIP's aligned vision-language space.

- Shows state-of-the-art performance on base-to-new generalization, cross-dataset transfer, and generalized zero-shot learning tasks by combining SHIP with existing CLIP fine-tuning methods like CoOp and Tip-Adapter.

- Demonstrates the ability to handle label-only classes during fine-tuning, unlike previous methods that require full labeled data. Maintains data efficiency.

Overall, the key contribution is proposing a generative approach to synthesize features for label-only classes, enabling handling new concepts during fine-tuning of vision-language models like CLIP. SHIP is model-agnostic and can enhance existing methods.
