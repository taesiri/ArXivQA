# [Lossless Compression with Probabilistic Circuits](https://arxiv.org/abs/2111.11632)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central research question addressed in this paper is:How can we develop a neural lossless compression algorithm that achieves both high compression performance and computational efficiency?Specifically, the paper investigates using Probabilistic Circuits (PCs) as the backbone model for neural lossless compression. The key hypothesis is that PCs can achieve a good balance between model expressiveness and tractability, allowing them to learn powerful generative models of complex datasets like images while still supporting efficient encoding and decoding algorithms. The paper develops compression and decompression algorithms for PCs that are proved to have logarithmic time complexity, making them much faster than existing neural compression methods. It also shows how to scale up the training of large PCs on image datasets using customized GPU kernels. Through experiments, the paper demonstrates that PC-based compression can achieve state-of-the-art bitrates on datasets like MNIST and EMNIST, while running significantly faster than competitive neural compression algorithms. It also shows how PCs can be naturally integrated with Flow-based models like IDF to improve performance on more complex image datasets.In summary, the central research question is how to develop neural lossless compression algorithms that are both high-performing in terms of compression rate and fast in terms of encoding/decoding time. The paper proposes using PCs as a model class that balances expressiveness and tractability to effectively address this question.
