# [Lossless Compression with Probabilistic Circuits](https://arxiv.org/abs/2111.11632)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central research question addressed in this paper is:How can we develop a neural lossless compression algorithm that achieves both high compression performance and computational efficiency?Specifically, the paper investigates using Probabilistic Circuits (PCs) as the backbone model for neural lossless compression. The key hypothesis is that PCs can achieve a good balance between model expressiveness and tractability, allowing them to learn powerful generative models of complex datasets like images while still supporting efficient encoding and decoding algorithms. The paper develops compression and decompression algorithms for PCs that are proved to have logarithmic time complexity, making them much faster than existing neural compression methods. It also shows how to scale up the training of large PCs on image datasets using customized GPU kernels. Through experiments, the paper demonstrates that PC-based compression can achieve state-of-the-art bitrates on datasets like MNIST and EMNIST, while running significantly faster than competitive neural compression algorithms. It also shows how PCs can be naturally integrated with Flow-based models like IDF to improve performance on more complex image datasets.In summary, the central research question is how to develop neural lossless compression algorithms that are both high-performing in terms of compression rate and fast in terms of encoding/decoding time. The paper proposes using PCs as a model class that balances expressiveness and tractability to effectively address this question.


## What is the main contribution of this paper?

The main contribution of this paper is proposing to use Probabilistic Circuits (PCs) for lossless data compression. Specifically:- The paper introduces PCs as a new class of tractable entropy models for neural lossless compression. PCs have an expressive structure that also permits efficient computations of marginals needed for coding. - It develops a provably efficient compression algorithm for PCs that leverages their ability to compute marginals in O(log(D) * |p|) time, where D is the data dimensionality and |p| is the PC model size. This enables fast encoding and decoding.- The paper shows how to scale up the training of PCs using customized GPU kernels, achieving competitive densities on image datasets like MNIST. - It demonstrates how PCs can be naturally integrated with existing neural compression models like normalizing flows, significantly improving their compression performance on natural images.- Experiments highlight that PC-based compression can achieve near state-of-the-art bitrates on datasets like MNIST while being much faster than neural baselines.In summary, the key novelty is utilizing PCs, a less commonly used class of generative models, for lossless compression. This allows blending expressiveness and tractability for efficient coding algorithms that realize competitive bitrates across various datasets.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence TL;DR of the paper:The paper introduces Probabilistic Circuits as efficient and scalable models for lossless compression of images, achieving competitive compression performance and faster encoding/decoding speeds compared to existing neural compression methods.


## How does this paper compare to other research in the same field?

This paper introduces probabilistic circuits (PCs) as efficient models for lossless compression. Here are some key ways it compares to other neural compression research:- Most prior work on neural lossless compression uses variational autoencoders (VAEs) or normalizing flows. This paper proposes an alternative architecture based on PCs.- It provides theoretical analysis showing PCs can enable efficient O(log D) encoding/decoding, compared to O(D) for typical autoencoding models.- Experiments demonstrate PCs can achieve competitive or superior compression performance to VAE and flow baselines on image datasets. For example, PC models outperform IDF, BitSwap, and JPEG2000 on MNIST. - When integrated with flows, PCs as priors significantly improve performance over standalone flows on subsampled ImageNet. This shows PCs can complement existing compression techniques.- The PC encoder/decoder is much faster than neural baselines with similar compression rate. For instance, it is 15-40x faster than IDF and BitSwap on MNIST while achieving better compression.So in summary, the key novelty is proposing PCs for compression. Theoretical and empirical results show they are efficient yet compress as well or better than popular techniques like VAEs and flows. The paper also demonstrates how PCs can be integrated with flows to boost performance. Overall, it expands the set of techniques for neural lossless compression with a model architecture that balances expressiveness and tractability.
