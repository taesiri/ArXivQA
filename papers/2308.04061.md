# [Enhancing Adversarial Robustness in Low-Label Regime via Adaptively   Weighted Regularization and Knowledge Distillation](https://arxiv.org/abs/2308.04061)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the main research question seems to be how to improve adversarial robustness of deep neural networks in a semi-supervised setting where labeled data is scarce. 

Specifically, the key questions and goals appear to be:

- How can we effectively utilize unlabeled data to improve adversarial robustness when labeled data is limited?

- Can we develop a theoretically-motivated regularization approach for unlabeled data that improves robustness?

- Can we design a semi-supervised adversarial training algorithm that outperforms existing methods in the low labeled data regime?

The central hypothesis seems to be that by deriving robustness regularization terms for unlabeled data and combining them with knowledge distillation using a semi-supervised teacher, they can develop a semi-supervised adversarial training method that achieves significantly better robustness and generalization compared to prior arts when labeled data is scarce. 

The proposed SRST-AWR algorithm aims to test this hypothesis by incorporating an adaptively weighted robustness regularization on unlabeled data motivated by new robust risk upper bounds, along with soft pseudo-labeling via distillation. The goal is to achieve adversarial robustness comparable to fully supervised methods even with limited labeled data.

In summary, the key research question is how to improve semi-supervised adversarial training, especially in the low labeled data regime, via theoretically-grounded robustness regularization and soft knowledge distillation. The central hypothesis is that the proposed SRST-AWR algorithm will significantly advance state-of-the-art on this problem.


## What is the main contribution of this paper?

 Based on my reading, the main contributions of this paper are:

- It proposes a new semi-supervised adversarial training algorithm called SRST-AWR (Semi-supervised Robust Self-Training with Adaptively Weighted Regularization) for improving adversarial robustness when labeled data is scarce. 

- It derives two new upper bounds on the robust risk that motivate a regularization term for unlabeled data in the proposed SRST-AWR algorithm. These bounds do not require label information.

- It combines the proposed regularization term with knowledge distillation using a semi-supervised teacher to implement soft pseudo-labeling of unlabeled data. This contrasts with prior works that use hard pseudo-labels.

- It demonstrates through experiments that SRST-AWR achieves state-of-the-art performance compared to existing methods on various benchmark datasets. The performance degrades only slightly even when using a very small labeled dataset.

- It shows SRST-AWR can match the performance of supervised adversarial training methods that use the full labeled dataset, by using only around 10% of the labels.

In summary, the main contribution is proposing a new semi-supervised adversarial training method SRST-AWR that leverages novel regularization bounds, soft pseudo-labeling, and a semi-supervised teacher to achieve improved robustness in the low labeled data regime. The efficacy of SRST-AWR is demonstrated through extensive experiments.
