# [Towards End-to-End Generative Modeling of Long Videos with   Memory-Efficient Bidirectional Transformers](https://arxiv.org/abs/2303.11251)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading, the key aspects of this paper are:

- It tackles the problem of modeling long-term dependencies in video generation using transformers. Long videos require capturing complex spatiotemporal structures over long timescales, which is challenging.

- The paper proposes a new model called Memory-efficient Bidirectional Transformer (MeBT) for video generation. The key ideas are:

1) Using a bidirectional transformer instead of autoregressive for fast parallel decoding and avoiding error propagation during generation. 

2) Employing an efficient encoder-decoder architecture with a latent bottleneck to achieve linear complexity instead of quadratic, enabling training on longer videos.

3) A curriculum learning strategy that gradually increases sequence length during training to learn short to long term dependencies.

- The central hypothesis is that combining these ideas - bidirectional modeling, efficient architecture, and curriculum learning - will allow better modeling of long-term dynamics in videos compared to prior transformer approaches.

- Experiments across multiple datasets suggest MeBT can generate higher quality long videos (128 frames) more efficiently than strong baselines like MoCoGAN, DIGAN, TATS etc. Both qualitatively and quantitatively it shows improvements.

In summary, the main contribution is an efficient transformer design that can exploit long-range dependencies during training while enabling fast and robust generation of long videos. The combination of architectural choices is tailored for video modeling.
