# [Towards End-to-End Generative Modeling of Long Videos with   Memory-Efficient Bidirectional Transformers](https://arxiv.org/abs/2303.11251)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading, the key aspects of this paper are:

- It tackles the problem of modeling long-term dependencies in video generation using transformers. Long videos require capturing complex spatiotemporal structures over long timescales, which is challenging.

- The paper proposes a new model called Memory-efficient Bidirectional Transformer (MeBT) for video generation. The key ideas are:

1) Using a bidirectional transformer instead of autoregressive for fast parallel decoding and avoiding error propagation during generation. 

2) Employing an efficient encoder-decoder architecture with a latent bottleneck to achieve linear complexity instead of quadratic, enabling training on longer videos.

3) A curriculum learning strategy that gradually increases sequence length during training to learn short to long term dependencies.

- The central hypothesis is that combining these ideas - bidirectional modeling, efficient architecture, and curriculum learning - will allow better modeling of long-term dynamics in videos compared to prior transformer approaches.

- Experiments across multiple datasets suggest MeBT can generate higher quality long videos (128 frames) more efficiently than strong baselines like MoCoGAN, DIGAN, TATS etc. Both qualitatively and quantitatively it shows improvements.

In summary, the main contribution is an efficient transformer design that can exploit long-range dependencies during training while enabling fast and robust generation of long videos. The combination of architectural choices is tailored for video modeling.


## What is the main contribution of this paper?

 Based on my reading, the main contributions of this paper are:

1. It proposes a memory-efficient bidirectional transformer (MeBT) for generative modeling of videos. Unlike prior methods, MeBT can directly learn long-range dependency from training videos while enjoying fast inference and robustness to error propagation. 

2. It introduces a simple yet effective curriculum learning approach to train MeBT for moderately long videos. This curriculum guides the model to gradually learn from short-term to long-term dependencies over the course of training.

3. It evaluates MeBT on three challenging real-world video datasets. MeBT shows competitive performance to state-of-the-art methods for short 16-frame videos. More importantly, it outperforms all methods for long 128-frame videos while being much more efficient in memory and computation during both training and inference.

In summary, the key contribution is the proposal of MeBT, which combines a memory-efficient bidirectional transformer architecture with curriculum learning to achieve improved modeling of long videos compared to prior work. MeBT demonstrates the potential of transformers for unconditional long-term video generation through more efficient training and inference.
