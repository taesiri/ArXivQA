# [AerialVLN: Vision-and-Language Navigation for UAVs](https://arxiv.org/abs/2308.06735)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the key research question it aims to address is: How can we develop an autonomous aerial vision-and-language navigation agent that can navigate unseen real-world environments by following natural language instructions?Specifically, the paper proposes a new task called AerialVLN along with a dataset and environment simulator to facilitate research in this direction. The key aspects of the AerialVLN task that make it novel and challenging compared to prior ground-based VLN research are:- Larger action space including vertical movements like "ascend", "descend" - Bigger, more complex outdoor city environments with many objects- Longer paths (on average over 600m) with more instructions steps (avg 9.7 objects referred)- First-person viewpoint requiring 3D collision avoidanceThe paper aims to establish this new aerial VLN task as a benchmark for future research by:- Developing a realistic simulator with 25 large city environments - Collecting a diverse dataset of over 8,000 paths with crowd-sourced instructions- Providing analysis of the linguistic complexity - Evaluating standard VLN baselines like Seq2Seq and cross-modal attention models which achieve very low success, indicating it is a challenging problem.In summary, the key research question is how to enable autonomous vision-and-language based aerial navigation in complex real-world environments, for which this paper introduces and analyzes the new task of AerialVLN.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contribution appears to be proposing a new aerial vision-and-language navigation task called AerialVLN, along with a corresponding large-scale dataset. Specifically, the key contributions seem to be:- Proposing the AerialVLN task, which requires an unmanned aerial vehicle (UAV) agent to navigate in outdoor environments by following natural language instructions and first-person view images. This is different from prior ground-based VLN tasks.- Developing a simulator using AirSim and Unreal Engine that supports continuous navigation and near-realistic rendering of city-level environments. The simulator has diverse scenes and interactive elements.- Collecting a large-scale dataset called AerialVLN with over 25,000 crowd-sourced instructions paired with over 8,400 paths demonstrating expert UAV piloting. The dataset has long path lengths, a rich vocabulary, and complex language instructions.- Providing an analysis of the AerialVLN dataset, including statistics on instruction lengths, vocabulary, action distributions, etc. that demonstrate the complexity and diversity of the data.- Evaluating strong existing VLN baselines like Seq2Seq and cross-modal attention models on AerialVLN and analyzing the challenges and failure cases. There is a significant gap between current methods and human performance.In summary, the main contribution appears to be proposing the novel AerialVLN task for UAV navigation based on natural language, along with a large-scale dataset and analysis that demonstrates this is a challenging open problem for future research.
