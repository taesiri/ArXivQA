# GPTScore: Evaluate as You Desire

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the key research questions and hypotheses of this paper are:1) Can generative pre-trained language models like GPT-3 be used to effectively evaluate text quality along multiple dimensions with just natural language instructions? The hypothesis is that the emergent abilities of large language models like GPT-3 for in-context learning and following instructions can allow them to effectively evaluate text quality in a customized, multi-faceted way without needing annotated training data.2) How does the performance of GPT-3 based text evaluation compare to existing automated metrics and human evaluation?The hypothesis is that GPT-3 based evaluation will correlate better with human judgments compared to existing automated metrics across a diverse set of text generation tasks and quality aspects.3) How do different variants of GPT-3 (model size, with/without fine-tuning) compare in their ability to evaluate text quality? The hypothesis is that larger GPT-3 models will perform better at text evaluation compared to smaller models, and human feedback fine-tuning may further improve evaluation capabilities.4) Can providing example demonstrations further improve GPT-3's evaluation capabilities compared to just instructions?The hypothesis is that adding a few example demonstrations will allow GPT-3 models to better learn the evaluation criteria in-context and improve correlation with human judgments.In summary, the key research questions focus on exploring GPT-3 for customizable multi-faceted text evaluation, comparing it to existing methods, and analyzing the effects of model size, fine-tuning, and demonstrations. The overarching hypothesis is that GPT-3 can effectively evaluate text in a human-like way with just instructions and examples.


## What is the main contribution of this paper?

The main contribution of this paper seems to be proposing a novel evaluation framework called GPTScore that utilizes the capabilities of large pre-trained language models like GPT-3 for evaluating generated text. Specifically:- GPTScore leverages the zero-shot instruction and in-context learning abilities of models like GPT-3 to evaluate text on multiple customizable aspects just from natural language descriptions, without needing training data.- The framework is flexible to incorporate task specifications, aspect definitions, and example demonstrations to help define the evaluation protocol. - Experiments cover many common NLP generation tasks (dialogue, summarization, MT, etc.) and aspects, showing GPTScore achieves strong performance across a range of models and datasets.- Analysis provides insights into the benefits of instruction and demonstration, model size, and relationships between aspects.So in summary, the key contribution appears to be introducing and validating a new paradigm for customizable, multifaceted, and zero-shot text evaluation through the capabilities of large pre-trained language models. The variety of experiments and analysis help demonstrate the potential of this approach.
