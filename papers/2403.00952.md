# [MediSwift: Efficient Sparse Pre-trained Biomedical Language Models](https://arxiv.org/abs/2403.00952)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Training large language models (LLMs) is computationally expensive, limiting their widespread adoption and application in specialized domains like biomedicine. 
- Methods to improve efficiency like sparse training face challenges in terms of hardware compatibility and potential accuracy losses.

Proposed Solution:
- Introduce MediSwift, a suite of biomedical LLMs available in 3 sizes - Med, Large, XL, with dense and 50-75% sparse variants.
- Leverage sparse pre-training on biomedical texts to reduce computational costs, while using dense fine-tuning and soft prompting to recover accuracy. 
- Show benefits of sparse pre-training combined with dense fine-tuning, setting new SOTA efficiency-accuracy tradeoffs on tasks like PubMedQA.

Key Contributions:
- First study showing benefits of inducing sparsity into biomedical LM pre-training, cutting FLOPs by 2-2.5x.
- Dense MediSwift-XL (1.2B params) sets new SOTA of 76.8% on PubMedQA, being 5.8x smaller than prior best model. 
- 50-75% sparse MediSwift variants exceed performance of larger/denser models, highlighting efficiency.
- Establish new benchmark for specialized biomedical LMs balancing accuracy and compute constraints.

In summary, this work introduces MediSwift models that use sparse pre-training and dense fine-tuning to improve efficiency-accuracy tradeoffs for biomedical LLMs. Results show accuracy exceeding larger dense models, while slashing FLOPs significantly, and setting new benchmarks on tasks like question answering.
