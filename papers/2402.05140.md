# [Tag-LLM: Repurposing General-Purpose LLMs for Specialized Domains](https://arxiv.org/abs/2402.05140)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
Large language models (LLMs) like GPT-3 have shown impressive proficiency in natural language tasks. However, their performance degrades significantly on specialized domains not well represented in their pretraining corpus, such as protein sequences, SMILES notations for molecules, or scientific data. Developing specialized models from scratch for these domains is expensive and requires abundant in-domain data and compute. Therefore, the authors explore how to effectively repurpose general-purpose LLMs for specialized domains.

Proposed Solution: 
The authors propose Tag-LLM, a novel framework to adapt LLMs using modular meta-linguistic tags. There are two types of tags: (1) domain tags that indicate domain-specific representations in the input and provide relevant context (e.g. \token{Protein}), and (2) function tags that encode instructions for task families (e.g. \token{Binding Affinity}). The tags are parameterized as continuous vector embeddings and appended to the LLM's embedding layer, preserving the original model weights. 

A 3-stage protocol is introduced to hierarchically train the tags from general to specialized, exploiting different levels of available in-domain data:
(1) Domain tags are trained via next-token prediction on unlabeled domain data 
(2) Single-domain function tags are trained on supervised single-domain labeled data
(3) Cross-domain function tags are trained on supervised multi-domain labeled data, learning shared abilities across domains

Additionally, non-textual prediction tasks are handled by pairing function tags with specialized output heads.

Contributions:
- Novel method to inject localized and reusable domain/function knowledge into LLMs via learned tags 
- Generalizable framework that allows combining tags to solve unseen tasks
- Outperforms expert models and baselines on scientific tasks like protein-drug binding 
- Significantly boosts LLM performance on 10 translation tasks
- More parameter-efficient and transferable than existing LLM tuning methods
