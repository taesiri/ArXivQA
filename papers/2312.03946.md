# [A Layer-Wise Tokens-to-Token Transformer Network for Improved Historical   Document Image Enhancement](https://arxiv.org/abs/2312.03946)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem: 
Document image binarization is an important preprocessing step for many downstream document analysis tasks. However, degraded document images with artifacts, noise, bleed-through etc make binarization challenging. Prior methods using thresholding, CNNs, GANs have limitations in capturing both global and local contexts. Recently, Vision Transformers (ViTs) have shown promise for image processing tasks but simple tokenization in ViTs limits modeling local image structure. 

Proposed Solution:
The paper proposes a novel Tokens-to-Token Transformer network called T2T-BinFormer for document image binarization. Instead of simple splitting of image into patches, it uses a progressive tokenization method to model both global and local contexts. The encoder module extracts context from tokens using the Tokens-to-Token ViT. The decoder module then produces enhanced image patches which are combined to get binarized output.

Main Contributions:
- First Tokens-to-Token ViT architecture for document binarization to capture both global and local image context
- Shows state-of-the-art performance across several DIBCO benchmarks compared to prior thresholding, CNN, GAN and ViT methods  
- Comprehensive experiments demonstrating superiority in recovering text and visual quality from degraded document images
- Qualitative and quantitative analyses highlighting improved modeling of local structure for better identification of textual components

In summary, the paper presents a transformer-based architecture for document binarization that effectively models both global and local contexts in images via novel tokenization. Extensive comparative experiments verify state-of-the-art performance.
