# [Perceiving Longer Sequences With Bi-Directional Cross-Attention   Transformers](https://arxiv.org/abs/2402.12138)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
Transformers have shown great success across domains like natural language processing and computer vision thanks to their flexibility and high performance. However, their computational cost and memory usage scale quadratically with the input sequence length, making it difficult to apply them to longer sequences like high-resolution images or point clouds when computational resources are limited. Efforts to improve their efficiency often trade off performance or limit the model's flexibility to specific input types.

Proposed Solution: 
The authors propose a novel Bi-Directional Cross-Attention Transformer (\arch) architecture that combines the performance and flexibility of regular Transformers with linear scaling efficiency inspired by Perceiver models. 

Key ideas:
- Model the 'what' and 'where' aspects via a small set of latent vectors and a longer input-dependent token sequence.
- Discover an approximate symmetry in cross-attention patterns between latents and tokens. 
- Propose bi-directional cross-attention to leverage this symmetry - attending both ways simultaneously using one cross-attention matrix. This is more efficient, requiring fewer parameters and FLOPs.
- Refine latents and tokens simultaneously in a ladder-like structure across layers. Latents self-attend while tokens optionally incorporate task-specific processing.  

Main Contributions:
- Introduces \arch architecture that scales linearly in computational cost and memory with sequence length like Perceivers, but retains the performance and flexibility of regular Transformers.

- Proposes bi-directional cross-attention, which leverages symmetric patterns to enable efficient exchange using one cross-attention matrix, saving computation and parameters. 

- Demonstrates \arch's ability to process longer sequences, showing strong performance on image classification, segmentation and point cloud tasks compared to Transformers and Perceivers, especially in limited compute settings.

- Provides simple and modular design allowing task-specific token processing modules to be conveniently incorporated, trading off some generality for improved performance when needed.

In summary, the proposed \arch architecture combines the best aspects of Transformers and Perceivers in an efficient yet flexible architecture applicable across modalities and tasks involving longer input sequences. The bi-directional cross-attention offers an efficient way to exchange information bidirectionally.
