# Reframing Human-AI Collaboration for Generating Free-Text Explanations

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the main research questions seem to be:1) Are GPT-3-generated explanations preferable to crowdsourced ones in existing datasets? The authors compare GPT-3 explanations to human-written explanations from existing crowdsourced datasets like CoS-E, ECQA, and e-SNLI. The aim is to evaluate the relative quality of GPT-3 explanations compared to what supervised models trained on crowdsourced data could produce.2) Can improving prompt quality improve GPT-3-generated explanations?The authors investigate whether using higher quality prompt examples results in better GPT-3 explanations compared to using lower quality crowdsourced prompts.3) Along what fine-grained dimensions are GPT-3-generated explanations preferred, and do these correlate with overall acceptability?The authors conduct human studies to evaluate qualities like factuality, grammar, novelty, label support, and overall acceptability. They analyze which factors are most correlated with overall preference.So in summary, the main research questions focus on evaluating the quality of few-shot GPT-3 explanations relative to crowdsourced data, whether prompt engineering can improve quality, and analyzing the fine-grained attributes that correlate with human judgments of explanation quality. The overarching goal is assessing the potential of LLMs for generating acceptable explanations with minimal supervision.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contributions are:1) Demonstrating that GPT-3 can generate surprisingly competitive free-text explanations when prompted appropriately, providing an alternative to expensive crowdsourced explanation datasets.2) Showing that the quality of GPT-3's generated explanations can be further improved by using higher quality prompt examples and by filtering the generations using a supervised acceptability model trained on human judgments. 3) Analyzing the characteristics of GPT-3's generated explanations using fine-grained human evaluations, finding strengths in surface features like grammar but weaknesses in areas like providing novel information and justifying the label. The proposed filtering model is able to select explanations that are improved in these areas.4) Proposing a general framework of overgenerating candidate explanations from large language models and then filtering using human acceptability judgments. This is shown to produce explanations deemed unanimously acceptable by humans more consistently than greedy decoding.In summary, the main contribution is demonstrating that large language models prompted appropriately can be a promising path for generating free-text explanations, especially when combined with light human oversight. The analysis also sheds light on the fine-grained attributes that constitute a high quality explanation according to human annotators.
