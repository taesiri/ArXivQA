# Reframing Human-AI Collaboration for Generating Free-Text Explanations

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the main research questions seem to be:1) Are GPT-3-generated explanations preferable to crowdsourced ones in existing datasets? The authors compare GPT-3 explanations to human-written explanations from existing crowdsourced datasets like CoS-E, ECQA, and e-SNLI. The aim is to evaluate the relative quality of GPT-3 explanations compared to what supervised models trained on crowdsourced data could produce.2) Can improving prompt quality improve GPT-3-generated explanations?The authors investigate whether using higher quality prompt examples results in better GPT-3 explanations compared to using lower quality crowdsourced prompts.3) Along what fine-grained dimensions are GPT-3-generated explanations preferred, and do these correlate with overall acceptability?The authors conduct human studies to evaluate qualities like factuality, grammar, novelty, label support, and overall acceptability. They analyze which factors are most correlated with overall preference.So in summary, the main research questions focus on evaluating the quality of few-shot GPT-3 explanations relative to crowdsourced data, whether prompt engineering can improve quality, and analyzing the fine-grained attributes that correlate with human judgments of explanation quality. The overarching goal is assessing the potential of LLMs for generating acceptable explanations with minimal supervision.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contributions are:1) Demonstrating that GPT-3 can generate surprisingly competitive free-text explanations when prompted appropriately, providing an alternative to expensive crowdsourced explanation datasets.2) Showing that the quality of GPT-3's generated explanations can be further improved by using higher quality prompt examples and by filtering the generations using a supervised acceptability model trained on human judgments. 3) Analyzing the characteristics of GPT-3's generated explanations using fine-grained human evaluations, finding strengths in surface features like grammar but weaknesses in areas like providing novel information and justifying the label. The proposed filtering model is able to select explanations that are improved in these areas.4) Proposing a general framework of overgenerating candidate explanations from large language models and then filtering using human acceptability judgments. This is shown to produce explanations deemed unanimously acceptable by humans more consistently than greedy decoding.In summary, the main contribution is demonstrating that large language models prompted appropriately can be a promising path for generating free-text explanations, especially when combined with light human oversight. The analysis also sheds light on the fine-grained attributes that constitute a high quality explanation according to human annotators.


## How does this paper compare to other research in the same field?

Here are a few thoughts on how this paper compares to other research in explainable AI:- The focus on free-text explanation generation is fairly common in recent XAI research. Many papers have looked at training models to generate natural language explanations, often using supervised learning over human-authored explanation datasets. - However, this paper takes a unique approach of using large language models (LLMs like GPT-3) in a few-shot prompting setup, rather than full supervised training. Evaluating how well LLMs can generate explanations when simply prompted with a few examples is a novel contribution.- The authors make a good point that collecting high-quality human-written explanations for full supervision is challenging and expensive. Using few-shot prompting is an interesting alternative that leverages the knowledge already contained in large pretrained models.- The idea of generating multiple candidate explanations via sampling and then filtering for the best ones using human judgments is also novel. This is a clever way to combine the power of LLMs with human input. The supervised acceptability filter is a simple but effective idea.- The detailed human evaluation, analyzing both overall quality and fine-grained attributes like novelty and label support, provides useful insight. This type of analysis is still not very common for free-text XAI methods in my experience. - The competitive results compared to human-written explanations are impressive and suggest few-shot prompting merits further study. The fact that GPT-3 can apparently produce decent explanations with minimal task-specific training is remarkable.In summary, the focus on few-shot prompting and simple human filtering to generate free-text explanations seems unique and promising compared to prior work relying on large supervised datasets. The extensive human evaluation and analysis is also a notable contribution over much previous XAI research.
