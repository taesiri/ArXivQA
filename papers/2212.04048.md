# [Executing your Commands via Motion Diffusion in Latent Space](https://arxiv.org/abs/2212.04048)

## What is the central research question or hypothesis that this paper addresses?

 The central research question this paper addresses is: 

How to enable robust 3D capture of challenging human motions from a single RGB video, which suffer from extreme poses, complex motion patterns, and severe self-occlusion?

The key hypothesis is that embracing multi-modal references from both temporally unsynchronized marker-based systems and lightweight markerless multi-view systems in a data-driven manner can address the limitations of existing monocular 3D human motion capture methods for such challenging motions. 

Specifically, the paper proposes:

1) A hybrid motion inference module to learn challenging motion characteristics from supervised multi-modal motion references. This utilizes a temporal encoder-decoder network to extract motion details from sparse-view image references, and a discriminator network for unpaired marker-based motion references.

2) A robust motion optimization module to further refine the estimated motions using reliable 2D keypoints and silhouettes from the input video.

3) A new dataset containing challenging motions with both unsynchronized marker-based motion capture data and synchronized multi-view image sequences for multi-modal supervision.

Through this proposed approach of combining data-driven multi-modal motion priors and image-based optimization, the paper aims to tackle the key challenges of monocular 3D capture for complex human motions and demonstrate significantly improved performance over state-of-the-art methods. The hybrid learning and optimization framework is the core novelty proposed to address the limitations of existing work.


## What is the main contribution of this paper?

 The main contribution of this paper is a novel approach for 3D human motion capture of challenging motions from a single RGB video. The key ideas are:

1. Proposing a two-stage learning-and-optimization framework that makes use of multi-modal references, including unsynchronized marker-based motion capture data and sparse multi-view videos. 

2. A hybrid motion inference module that combines temporal encoder-decoder and adversarial learning to extract challenging motion details from the multi-modal references in a data-driven manner.

3. A robust motion optimization module that further refines the motions by combining learned 3D priors and 2D/silhouette constraints from the input video.

4. Introduction of a new challenging human motion dataset with various motions, multi-view videos, and marker-based mocap data.

The main novelty is the effective usage of multi-modal references, including both accurate but unsynchronized marker-based mocap and multi-view videos, to enable robust 3D motion capture of complex and challenging motions from monocular RGB. This is achieved via a novel hybrid learning scheme and optimization approach. Experiments demonstrate significant improvements over state-of-the-art monocular mocap methods on challenging motions with severe occlusion and complex poses. The multi-modal dataset with various challenging motions also supports future research in this direction.

In summary, the key contribution is a new learning-based framework and dataset to push the boundary of monocular 3D mocap to handle complex motions by exploiting multi-modal references in a hybrid manner. The results outperform previous monocular methods by a large margin on challenging cases.
