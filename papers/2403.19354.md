# [AIpom at SemEval-2024 Task 8: Detecting AI-produced Outputs in M4](https://arxiv.org/abs/2403.19354)

## Summarize the paper in one sentence.

 AIpom detects AI-produced text in M4 using a pipeline of decoder and encoder models, ranking 2nd out of 33 teams in SemEval-2024 Task 8 Subtask C with a Mean Absolute Error of 15.94.


## What is the main contribution of this paper?

 The main contribution of this paper is proposing a novel two-stage pipeline called AIpom for detecting machine-generated text in human-machine mixed texts (Subtask C of SemEval-2024 Task 8). The key aspects of AIpom are:

- It combines predictions from an instruction-tuned decoder-only model (Mistral-7B-OpenOrca) and encoder-only sequence taggers (DeBERTaV3-Large).

- The decoder model is first fine-tuned to predict the transition point from human-written to machine-generated text. Its predictions are then used to label the data for training the encoder models. 

- Two encoder models are trained - one on the decoder-labeled data, and one on a mixture of original training data and decoder-labeled data. Their predictions are averaged to get the final output.

- AIpom achieves 2nd place on the leaderboard for Subtask C with a Mean Absolute Error of 15.94. Ablation studies confirm the benefits of combining decoder and encoder models in a pipeline.

So in summary, the key contribution is the novel two-stage pipeline with decoder and encoder models for detecting machine-generated text in mixed human-machine texts.


## What are the keywords or key terms associated with this paper?

 Based on my review of the paper, some of the key terms and keywords associated with it include:

- SemEval-2024 Task 8 - The shared task that this paper focuses on, for detecting machine-generated text
- Subtask C - The specific subtask that the proposed AIpom system tackles, for human-machine mixed text detection
- Mean Absolute Error (MAE) - The performance metric used to evaluate models on this task 
- Generative language models (LMs) - The type of models that are used to generate the machine-text in the dataset
- Decoder model - One component of the proposed two-stage AIpom pipeline
- Encoder model - The other component of the AIpom pipeline 
- Fine-tuning - The process used to adapt the decoder and encoder models for this task
- Low-Rank Adaptation (LoRA) - A specific fine-tuning technique used for the decoder model
- Ablation studies - Experiments done to analyze the contribution of different components
- Robustness - A measure of how well the model performs on out-of-domain data, which is highlighted as something to improve on


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in this paper:

1. The paper proposes a two-stage pipeline combining predictions from a decoder-only model and encoder-only sequence taggers. What are the potential benefits and drawbacks of this pipelined approach compared to an end-to-end model? 

2. The decoder model is fine-tuned using Low-Rank Adaptation (LoRA). Why was LoRA chosen over other tuning methods? What are the key hyperparameters used for tuning the decoder?

3. Two separate encoder models are fine-tuned in the pipeline. What is the motivation behind fine-tuning two encoders rather than one? How do the training procedures and datasets used for the two encoders differ?

4. The paper mentions employing special tokens such as <BREAK> during fine-tuning and inference. What is the purpose of using these tokens? How are they incorporated into the pipeline?

5. Ablation studies in the paper highlight the superior performance of the decoder compared to encoder-only models. What factors might explain this? How can the encoder performance be further improved?  

6. The results show a significant performance gap between the development set and test set. What does this suggest about the model's robustness? How can domain shift issues be addressed?

7. The conclusion mentions creating longer pipelines with more models. What benefits could this provide? What challenges need to be overcome in implementing complex pipelines?

8. How suitable is the proposed pipeline approach for real-time detection applications? What throughput/latency tradeoffs need to be considered?

9. The decoder model uses sampling during inference. How does this impact prediction consistency and accuracy? What alternatives could be explored?

10. The paper uses a simple averaging approach to aggregate predictions. What other aggregation strategies could be effective? How can confidence scores be incorporated?


## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

The paper proposes AIpom, a system to detect transitions between human-written and machine-generated text in mixed documents (SemEval 2024 Task 8, Subtask C). The goal is to identify the index of the first machine-generated word.

The approach uses a pipeline with decoder and encoder language models. First, a decoder model (Mistral-7B-OpenOrca) is fine-tuned to output only the machine text part from the input. Its predictions help label the data for training the encoder. Next, two DeBERTaV3 encoder models are fine-tuned - one on the decoder-labeled data, another on a mix of original training data and decoder predictions. At inference, the encoder predictions are aggregated by averaging.

The proposed pipeline outperforms using only a single decoder or encoder model. Combining predictions helps achieve robustness. The best result is a Mean Absolute Error of 15.21 on the official test set. 

The main contributions are:
(1) A novel pipeline with decoder and encoder LMs for text segmentation.
(2) Showing the benefits of combining decoder and encoder models over using them individually.
(3) Ablation studies analyzing different model combinations and training strategies. 
(4) Releasing the source code and models publicly.

The limitation is domain shift issues between the dev and test sets. Future work involves enhancing robustness across text domains and generators.
