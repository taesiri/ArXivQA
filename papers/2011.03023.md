# [Language Model is All You Need: Natural Language Understanding as   Question Answering](https://arxiv.org/abs/2011.03023)

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper addresses is whether natural language understanding (NLU) problems can be effectively mapped to question answering (QA) problems, especially in low data regimes, to leverage transfer learning from QA models. The key hypothesis is that framing NLU as QA will enable significant performance improvements in few-shot and low-resource settings compared to standard approaches like sentence classification and token tagging.The paper proposes an approach called QANLU where NLU data is converted to QA style data with questions and answers created based on the intent and slot annotations. QA models pretrained on QA datasets are then fine-tuned on this converted NLU data. The hypothesis is that this allows transfer learning from the QA task that the model has been pretrained on.The main experiments then evaluate QANLU versus standard classification and tagging approaches in few-shot settings on the ATIS and Restaurants datasets. The results generally validate the hypothesis, showing clear benefits to the QA framing in low data regimes. Additional experiments also demonstrate how QANLU enables effective transfer learning across NLU domains.So in summary, the central research question is whether NLU can be improved, especially in low resource scenarios, by mapping it to QA to leverage transfer learning. The paper provides evidence that this QA framing delivers significant gains over conventional methods when data is limited.
