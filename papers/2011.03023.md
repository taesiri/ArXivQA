# [Language Model is All You Need: Natural Language Understanding as   Question Answering](https://arxiv.org/abs/2011.03023)

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper addresses is whether natural language understanding (NLU) problems can be effectively mapped to question answering (QA) problems, especially in low data regimes, to leverage transfer learning from QA models. The key hypothesis is that framing NLU as QA will enable significant performance improvements in few-shot and low-resource settings compared to standard approaches like sentence classification and token tagging.The paper proposes an approach called QANLU where NLU data is converted to QA style data with questions and answers created based on the intent and slot annotations. QA models pretrained on QA datasets are then fine-tuned on this converted NLU data. The hypothesis is that this allows transfer learning from the QA task that the model has been pretrained on.The main experiments then evaluate QANLU versus standard classification and tagging approaches in few-shot settings on the ATIS and Restaurants datasets. The results generally validate the hypothesis, showing clear benefits to the QA framing in low data regimes. Additional experiments also demonstrate how QANLU enables effective transfer learning across NLU domains.So in summary, the central research question is whether NLU can be improved, especially in low resource scenarios, by mapping it to QA to leverage transfer learning. The paper provides evidence that this QA framing delivers significant gains over conventional methods when data is limited.


## What is the main contribution of this paper?

The main contribution of this paper is developing an approach for natural language understanding (NLU) by mapping NLU tasks like intent and slot detection to question answering (QA). Specifically:- They propose Question Answering for Natural Language Understanding (QANLU), where they construct QA style datasets from intent and slot annotated data. This allows transferring knowledge from QA to intent and slot detection tasks.- They show that QANLU significantly outperforms standard intent and slot classification methods in low-resource and few-shot settings, with improvements up to 60% in some cases.- They demonstrate that QANLU enables effective transfer learning for intent and slot detection across domains. Fine-tuning a QA model on one domain and then another domain improves performance by over 50% compared to training only on the target domain. - They find that augmenting QA training data with QANLU data further improves QA model performance, suggesting bidirectional transfer between QA and NLU is possible.In summary, the key contribution is using QA as a canonical task to transfer knowledge to other NLU problems, especially when training data is scarce. The paper shows this is an effective approach and establishes QANLU as a strong few-shot NLU method.
