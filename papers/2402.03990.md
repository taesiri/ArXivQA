# [Subsampling is not Magic: Why Large Batch Sizes Work for Differentially   Private Stochastic Optimisation](https://arxiv.org/abs/2402.03990)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Differentially private stochastic gradient descent (DP-SGD) is important for training deep learning models privately. Several works have empirically found that large batch sizes improve performance of DP-SGD. 
- However, existing theoretical explanations for this are limited. It is unclear how the effective noise variance injected to ensure privacy scales with the batch size or subsampling rate.

Proposed Solution: 
- The paper studies the relationship between subsampling rate and effective noise variance both asymptotically and for a single iteration.
- They decompose the total variance of DP-SGD gradients into subsampling-induced variance and effective noise variance.
- They prove that asymptotically, as number of iterations goes to infinity, there is a linear relationship between subsampling rate and noise standard deviation. This cancels out the effect of subsampling rate on effective noise variance.
- For a single iteration, they derive a sufficient condition under which larger subsampling rates reduce effective noise variance. This condition appears valid for practical range of hyperparameters.

Main Contributions:
- Asymptotic theoretical result showing invariance of effective noise variance to subsampling rate, implying large batches always help by reducing subsampling variance.
- Empirical demonstration that asymptotic regime is quickly reached even for small privacy budgets.
- Numerical evidence that outside asymptotic regime, effective noise variance reduces even more with large batches.
- Sufficient condition and empirical verification that for a single iteration, large batches also often reduce effective noise. 

Overall, this improves theoretical understanding of role of batch size in DP-SGD and explains why large batches are useful, addressing a major open question.


## Summarize the paper in one sentence.

 Here is a one sentence summary of the key point from the paper:

The paper proves that for differential privacy stochastic gradient descent, in the limit of many iterations the effective noise variance becomes invariant to the batch size, so larger batches reduce overall variance by decreasing subsampling variance while keeping effective noise variance fixed.


## What is the main contribution of this paper?

 This paper studies the relationship between the batch size (subsampling rate) and the effective noise variance in differentially private stochastic gradient descent (DP-SGD). The main contributions are:

1) It proves that in the limit of an infinite number of iterations, there is a linear relationship between the subsampling rate $q$ and the noise standard deviation $\sigma$. This means the two effects $q$ has on the effective noise variance cancel out, leaving the variance invariant to $q$. As a result, a larger $q$ always reduces the total effective gradient variance by decreasing the subsampling-induced variance.

2) For a single iteration, it provides a sufficient condition under which larger $q$ values reduce the effective DP noise variance. This condition is checked numerically and found to hold for a wide range of hyperparameters. 

3) Empirically, it is shown that the asymptotic linear regime between $q$ and $\sigma$ is reached quickly for small privacy parameters $\epsilon$. Moreover, when not in the asymptotic regime, the effective noise variance decreases even more with larger $q$.

In summary, the paper provides theoretical analysis and empirical evidence that explain why large batch sizes are useful in DP-SGD, furthering the understanding of this algorithm. The "magic" of subsampling amplification comes from computational savings, not higher utility.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper's abstract and introduction, some of the key terms and concepts associated with this paper include:

- Differentially private stochastic gradient descent (DP-SGD)
- Subsampling/subsampling amplification 
- Batch size
- Gradient variance 
- Total gradient variance decomposition (into subsampling-induced and noise-induced components)
- Effective noise variance
- Asymptotic analysis (in the limit of infinite iterations)
- Relationship between noise level (sigma) and subsampling rate 
- Tight differential privacy accounting
- Poisson subsampled Gaussian mechanism
- Dominating pairs
- Privacy loss random variables (PLRVs)

The paper studies the effect of batch size on the total gradient variance in DP-SGD, seeking a theoretical explanation for why large batch sizes are useful. It decomposes the total variance into subsampling and noise components, analyzes how the effective noise variance scales with subsampling rate, and proves results about the asymptotic linear relationship between noise level and subsampling. The analysis relies heavily on concepts from tight differential privacy accounting like dominating pairs and PLRVs. So those are some of the key terms and concepts explored in this work.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 potential in-depth questions about the method proposed in the paper:

1. The paper decomposes the total gradient variance in DP-SGD into subsampling-induced and noise-induced components. Can you explain intuitively why larger batch sizes reduce the subsampling-induced variance? What assumptions does this rely on?

2. In Section 3.1, the paper shows that in the limit of infinite iterations, there is a linear relationship between the subsampling rate $q$ and noise standard deviation $\sigma$. Why does this linear relationship cause the effects of $q$ on the effective noise variance to cancel out?

3. The linear relationship $\sigma = cq$ emerges in the infinite iteration limit. Do you expect this relationship to hold well even for a moderate number of iterations like 100 or 1000? What evidence supports your answer?

4. Theorem 1 shows that $\sigma_T^2$ must grow at least linearly with $T$. Does the proof technique generalize to other iterative DP algorithms besides the Poisson subsampled Gaussian mechanism? Why or why not?  

5. The empirical results in Section 3.2 verify that the linear $\sigma = cq$ relationship emerges quickly for small privacy budgets. Can you think of any ways to theoretically analyze the convergence rate to this asymptotic regime?

6. For the single iteration case in Section 3.3, the paper finds a sufficient condition for large $q$ to reduce effective noise variance. Do you think this condition would hold for most practical DP-SGD applications? How could it be checked?

7. Conjecture 1 proposes that the sufficient condition of Theorem 2 holds when $\epsilon, q \geq 4\delta$. Is there an intuitive explanation for why a relationship between $q$ and $\delta$ is involved here?  

8. The empirical evaluation of Conjecture 1 considers $\delta = 10^{-5}$. How sensitive do you expect the results to be to the choice of $\delta$? Would the conjecture likely fail for much larger $\delta$?

9. In the discussion, the paper suggests interpolating between the single iteration and asymptotic results. What challenges do you foresee in developing such an interpolation, both theoretically and empirically?  

10. The conclusion states that less subsampling always improves the privacy-utility tradeoff at the cost of compute. Do you think this relationship holds across the full range of subsampling rates? How could the extremes of $q$ be studied further?
