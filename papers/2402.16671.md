# [StructLM: Towards Building Generalist Models for Structured Knowledge   Grounding](https://arxiv.org/abs/2402.16671)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem: 
- Large language models (LLMs) like ChatGPT still struggle with structured knowledge grounding (SKG) tasks and lag behind state-of-the-art models by 35% on average. 
- There is a lack of generalist models that can ground on diverse types of structured knowledge to interface with humans. Prior work has focused on building task-specific models with limited generalization.

Proposed Solution:
- Constructed a large instruction tuning dataset with 1.1 million examples, comprising 18 SKG tasks and additional general instruction following data. 
- Trained and released 3 models based on the CodeLlama architecture at scales of 7B, 13B and 34B parameters, referred to as StructLM.

Main Contributions:  
- StructLM beats the previous state-of-the-art on 11 out of 18 SKG tasks. It also achieves new state-of-the-art results on 7 tasks, surpassing task-specific models.
- Demonstrated strong zero-shot generalization capability on 6 unseen SKG tasks, which was not shown by prior models. 
- Found that scaling model size from 7B to 34B parameters offered marginal benefits, suggesting structured knowledge grounding remains a challenging task needing more innovative designs.
- Showed the performance benefits of code pretraining and mixing general instruction data for improving SKG capabilities.

In summary, the paper proposed StructLM models to address the limitations of LLMs on structured knowledge grounding. The models were trained on a large instruction tuning dataset and demonstrated improved performance and generalization across diverse SKG tasks. But scaling model size did not lead to significant gains, highlighting it as an open challenge for future work.
