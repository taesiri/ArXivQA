# [Discovering Spatio-Temporal Rationales for Video Question Answering](https://arxiv.org/abs/2307.12058)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be: How to develop an effective video question answering (VideoQA) method that can handle complex videos with long durations and multiple objects/events?The key challenges in complex VideoQA that the paper identifies are:- Long videos contain lots of irrelevant content/background that can overwhelm the key evidence needed to answer the question.- Hard negative answer candidates tailored for each video can establish spurious correlations with the background, misleading the prediction.To address these challenges, the main ideas proposed in the paper are:- Use a Spatio-Temporal Rationalization (STR) module to adaptively select the most relevant frames and objects as rationales to focus on. This removes irrelevant background content.- Use a separate answer decoding stage after video-question encoding to avoid spurious correlations between negative answers and background.The central hypothesis seems to be that using STR to extract rationales and a revised answer decoding scheme will significantly improve performance on complex VideoQA benchmarks compared to prior methods. The experiments aim to validate this hypothesis.In summary, the key research question is how to develop a VideoQA method that can effectively handle long, complex videos through adaptive rationale extraction and improved answer modeling. The paper hypothesizes these ideas will substantially improve complex VideoQA performance.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contributions appear to be:- Analyzing the necessity and challenges of complex video question answering (VideoQA), where videos are long and involve multiple objects interacting over time. The key challenges are handling massive irrelevant visual content and modeling hard negative answer candidates.- Proposing a Spatio-Temporal Rationalization (STR) module to adaptively select critical frames and objects from the video to focus reasoning and filter out irrelevant content.- Developing a novel answer decoding scheme that separates question and answer candidate encoding to avoid spurious correlations between negative candidates and irrelevant video content. - Introducing TranSTR, a Transformer-based model instantiating the proposed STR module and answer decoding scheme.- Demonstrating state-of-the-art performance on four VideoQA benchmarks, especially on complex VideoQA datasets like NExT-QA and Causal-VidQA where TranSTR improves substantially over prior work.- Providing extensive analysis and studies on the importance of the STR module and answer decoding scheme through ablation studies and visualizations.In summary, the main contributions are introducing the STR module and a new answer decoding scheme to handle the key challenges in complex VideoQA, along with design and evaluation of the TranSTR model showing significant improvements on this task. The solutions aim to spark more progress on VideoQA for long videos with multiple objects and events.
