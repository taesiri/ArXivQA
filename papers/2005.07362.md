# Is Your Goal-Oriented Dialog Model Performing Really Well? Empirical   Analysis of System-wise Evaluation

## What is the central research question or hypothesis that this paper addresses?

The central research questions addressed in this paper are:1. Which configurations lead to better goal-oriented dialog systems? 2. Whether the component-wise, single-turn metrics are consistent with system-wise, multi-turn metrics for evaluation?3. How does the performance vary when a system is evaluated using tasks of different complexities, e.g., from single-domain to multi-domain tasks?4. Does simulated evaluation correlate well with human evaluation?The key hypothesis is that system-wise evaluation and analysis is crucial for properly assessing and comparing different goal-oriented dialog systems. The authors conduct experiments with various system configurations and evaluation settings to provide an empirical analysis.In summary, the paper aims to shed light on evaluating and comparing goal-oriented dialog systems by doing a system-wise evaluation and analysis to answer the above research questions. The central hypothesis is that more attention should be paid to system-wise evaluation for developing effective goal-oriented dialog systems.


## What is the main contribution of this paper?

The main contributions of this paper are:1. It provides a system-wise evaluation and empirical analysis of different types of dialog systems on the MultiWOZ corpus. Specifically, it compares pipeline, joint, and end-to-end models across a variety of configurations.2. It investigates the correlation between component-wise, single-turn evaluation metrics and overall system performance in multi-turn dialogs. The results show that the component metrics do not always reflect real system performance.3. It analyzes how system performance varies across tasks of different complexities, from single to multi-domain dialogs. All systems see a significant performance drop as complexity increases.4. It compares simulation-based evaluation to human evaluation. Despite some discrepancies, simulation correlates moderately well with human evaluation, making it a valid alternative especially during early development. 5. The key findings are: (a) pipeline systems outperform joint and end-to-end models, (b) component metrics don't fully reflect system performance, (c) all systems struggle with complex tasks, and (d) simulation can approximate human evaluation.In summary, this is the first in-depth empirical analysis of goal-oriented dialog systems from a system-wise perspective, providing new insights into evaluation and comparison of such systems. The authors advocate for more system-level evaluation in dialog research.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

The paper presents an empirical analysis of the performance of different goal-oriented dialog system architectures and configurations. The key finding is that pipeline systems trained with fine-grained supervision outperform end-to-end systems, and component-wise evaluation metrics do not always correlate with overall system performance.
