# [Is Your Goal-Oriented Dialog Model Performing Really Well? Empirical   Analysis of System-wise Evaluation](https://arxiv.org/abs/2005.07362)

## What is the central research question or hypothesis that this paper addresses?

 The central research questions addressed in this paper are:

1. Which configurations lead to better goal-oriented dialog systems? 

2. Whether the component-wise, single-turn metrics are consistent with system-wise, multi-turn metrics for evaluation?

3. How does the performance vary when a system is evaluated using tasks of different complexities, e.g., from single-domain to multi-domain tasks?

4. Does simulated evaluation correlate well with human evaluation?

The key hypothesis is that system-wise evaluation and analysis is crucial for properly assessing and comparing different goal-oriented dialog systems. The authors conduct experiments with various system configurations and evaluation settings to provide an empirical analysis.

In summary, the paper aims to shed light on evaluating and comparing goal-oriented dialog systems by doing a system-wise evaluation and analysis to answer the above research questions. The central hypothesis is that more attention should be paid to system-wise evaluation for developing effective goal-oriented dialog systems.


## What is the main contribution of this paper?

 The main contributions of this paper are:

1. It provides a system-wise evaluation and empirical analysis of different types of dialog systems on the MultiWOZ corpus. Specifically, it compares pipeline, joint, and end-to-end models across a variety of configurations.

2. It investigates the correlation between component-wise, single-turn evaluation metrics and overall system performance in multi-turn dialogs. The results show that the component metrics do not always reflect real system performance.

3. It analyzes how system performance varies across tasks of different complexities, from single to multi-domain dialogs. All systems see a significant performance drop as complexity increases.

4. It compares simulation-based evaluation to human evaluation. Despite some discrepancies, simulation correlates moderately well with human evaluation, making it a valid alternative especially during early development. 

5. The key findings are: (a) pipeline systems outperform joint and end-to-end models, (b) component metrics don't fully reflect system performance, (c) all systems struggle with complex tasks, and (d) simulation can approximate human evaluation.

In summary, this is the first in-depth empirical analysis of goal-oriented dialog systems from a system-wise perspective, providing new insights into evaluation and comparison of such systems. The authors advocate for more system-level evaluation in dialog research.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 The paper presents an empirical analysis of the performance of different goal-oriented dialog system architectures and configurations. The key finding is that pipeline systems trained with fine-grained supervision outperform end-to-end systems, and component-wise evaluation metrics do not always correlate with overall system performance.


## How does this paper compare to other research in the same field?

 Here are a few key ways this paper compares to other research in goal-oriented dialog systems:

- It focuses on system-wise evaluation rather than just component evaluation. Many prior works evaluate parts of a dialog system like the state tracker or policy model in isolation. This paper emphasizes the need to evaluate the full system with end-to-end dialogs.

- It systematically compares pipeline, joint, and end-to-end architectures. The paper investigates how different modular vs end-to-end architectures impact overall dialog performance. Many prior works focus on a single architecture. 

- It studies how performance varies across different domains and levels of complexity. The paper examines dialog success across single, double, and triple domain tasks. This provides insight into how robust different systems are as complexity increases.

- It contrasts simulated and human evaluation. The paper compares system rankings between a simulated user and real humans. This helps validate the use of simulation for dialog testing and exposes differences between synthetic and human users.

- It advocates the use of a standard platform like ConvLab. Having a common platform facilitates controlled experiments and fair comparisons between different dialog systems, which is lacking in much prior research.

Overall, this paper provides a comprehensive empirical analysis into factors impacting goal-oriented dialog success. The emphasis on end-to-end evaluation and detailed system comparison helps advance understanding of how to build effective dialog agents beyond isolated components. The paper highlights the need for platforms that support reproducible dialog research.


## What future research directions do the authors suggest?

 Based on the empirical analysis and results, the authors suggest some potential future research directions:

1. Training end-to-end dialog systems without intermediate supervision: The authors note that pipeline systems require heavy fine-grained annotations and effort to assemble modules, which can be challenging to deploy in practice. Thus training good end-to-end systems without intermediate supervision signals will be appealing. 

2. Enhancing robustness of dialog systems: The results show current systems are still limited in handling natural human conversations. Enhancing robustness to deal with complex and uncertain situations is vital.

3. Devising better user simulators: There is a discrepancy between simulators and real users. Developing user simulators that can generate more natural, diverse responses and mimic complex user behaviors more closely would help evaluate systems more accurately.

4. Rethinking dialog system construction: The huge performance gap between NLU and word-level DST indicates the user intent predicted by NLU is important for decision making, but is missing from word-level DST. This suggests rethinking system construction to not neglect user intent.

5. Leveraging dialog acts: Dialog acts seem sufficient for goal-oriented dialogs. Annotations with dialog acts may help build better end-to-end systems, user simulators, or semantic parsing.

In summary, key directions are improving end-to-end training, enhancing robustness, developing better user simulators, rethinking system architectures to leverage user intent, and using dialog acts to enable advances. The authors advocate more attention on system-wise evaluation in developing goal-oriented dialog systems.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the paper:

This paper presents an empirical analysis and system-wise evaluation of goal-oriented dialog systems to investigate which system architectures and configurations perform better in practical dialog settings. The authors build dialog systems using different combinations of state-of-the-art natural language understanding, dialog state tracking, dialog policy, and natural language generation modules. They conduct both simulated and human evaluations of the dialog systems on the MultiWOZ corpus. The results show that pipeline systems trained with fine-grained supervision signals at the component level generally achieve higher success rates than end-to-end trained systems. The component-wise evaluation metrics are not always consistent with system-wise metrics. The performance of all systems decreases as the task complexity increases from single domain to multi-domain. Despite discrepancies between simulated and human users, the simulated evaluation correlates moderately well with human evaluation, indicating it is a valid alternative, especially in early development stages. The authors suggest that system-wise evaluation and leveraging fine-grained annotations are crucial for developing effective goal-oriented dialog systems.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the paper:

This paper presents an empirical analysis and system-wise evaluation of goal-oriented dialog systems on the MultiWOZ corpus. The authors compare multiple systems with different architectures including pipeline, joint model, and end-to-end systems. The pipeline systems leverage fine-grained labels and achieve the best performance. The main findings are: (1) Pipeline systems outperform joint model and end-to-end systems likely due to using fine-grained supervision. (2) Component-wise single-turn evaluation metrics do not always correlate with system-wise multi-turn metrics. (3) All systems suffer performance drops on more complex multi-domain tasks, but pipeline systems are most robust. (4) Despite discrepancies, simulated evaluation correlates moderately with costly human evaluation, especially for system development. 

In summary, this paper advocates for more attention on system-wise evaluation of dialog systems, rather than just component-wise. The results show that leveraging fine-grained labels at different levels of abstraction in dialog is essential for overall system performance. The findings also highlight the need for better user simulators and increased dialog system robustness for complex, real human conversations. The empirical analysis offers insights into evaluating and comparing goal-oriented dialog systems.


## Summarize the main method used in the paper in one paragraph.

 Here is a one paragraph summary of the main method used in the paper:

The paper proposes a system-wise evaluation approach to compare different architectures of goal-oriented dialog systems, including pipeline, joint model, and end-to-end systems. The evaluation is conducted on the MultiWOZ benchmark using the ConvLab platform. Both automatic simulated evaluation with an agenda-based user simulator and human evaluation with crowdworkers are performed. The systems are evaluated based on multi-turn dialog metrics such as dialog turns, inform rate, match rate and task success rate. The results show that pipeline systems generally achieve better performance than joint models and end-to-end systems. The paper also analyzes the discrepancy between component-wise single-turn evaluation and system-wise multi-turn evaluation. It further investigates the impact of task complexity on system performance. The correlation between simulated and human evaluations is also examined. Overall, the paper provides an in-depth empirical analysis and comparison of goal-oriented dialog systems from a system-wise perspective.


## What problem or question is the paper addressing?

 The paper is addressing the problem of how to properly evaluate and compare the overall performance of different goal-oriented dialog systems. Specifically, it aims to answer the following key research questions:

1. Which system configurations (e.g. pipeline vs end-to-end) lead to better goal-oriented dialog systems?

2. Are component-wise, single-turn evaluation metrics consistent with system-wise, multi-turn metrics for evaluating dialog systems? 

3. How does a dialog system's performance vary when evaluated on tasks of varying complexity (single-domain vs multi-domain)?

4. Does simulated user evaluation correlate well with human evaluation for assessing dialog system performance?

The paper argues that most prior work evaluates dialog systems in a component-wise, single-turn manner which does not reflect the real performance of the full dialog system. It advocates for more system-wise, multi-turn evaluation and presents an empirical analysis comparing different dialog system architectures and evaluation settings. The goal is to shed light on best practices for evaluating and comparing goal-oriented dialog systems.
