# [Harnessing PubMed User Query Logs for Post Hoc Explanations of   Recommended Similar Articles](https://arxiv.org/abs/2402.03484)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Literature search is crucial for research but challenging due to the vast number of papers published yearly. "Similar articles" features that recommend related papers based on a seed paper are widely used to aid search.
- However, most academic search engines lack explanations for their recommended articles, making it difficult for users to efficiently identify relevant papers.

Proposed Solution:
- The paper formulates the task of explaining recommendations as selecting the most relevant tokens in the title of a recommended "similar article," given the title and abstract of the seed article. 
- A new dataset called PubCLogs is constructed by repurposing 5.6 million pairs of coclicked articles from PubMed user logs as seed/similar article pairs. Queries leading to coclicks are used to label relevant tokens in similar article titles.
- A transformer-based sequence tagging model called HSAT (Highlight Similar Article Title) is trained on PubCLogs to select relevant tokens in similar article titles.

Main Contributions:
- Novel method to repurpose user search logs to build explanation datasets for academic search engines
- Formulation of the recommendation explanation task as selection of relevant tokens in similar article titles
- PubCLogs, a new dataset with 1.5M examples constructed from PubMed user logs using article coclicks 
- HSAT, a model trained on PubCLogs that outperforms baselines like BM25, Word2Vec and GPT-4 on highlighting relevant tokens, demonstrating the utility of the dataset

The main insight is that user search behavior such as article coclicks can be leveraged to provide personalized explanations for literature recommendations. By harnessing this "user intelligence", the paper shows improved performance on the explanation task.


## Summarize the paper in one sentence.

 Here is a one-sentence summary of the paper:

The paper presents a method to explain literature recommendations by selecting the most relevant parts of a recommended article's title in relation to the original search article, using a dataset constructed from PubMed user search logs to train a transformer model for this task.


## What is the main contribution of this paper?

 The main contributions of this paper are three-fold:

1. It formulates the task of explaining similar article recommendations as selecting the most relevant tokens in the title of the recommended article that relate it to the original "seed" article. 

2. It constructs a dataset called PubCLogs by repurposing coclicks in PubMed user query logs. Specifically, coclicked article pairs are treated as seed and similar articles, and the queries associated with those coclicks are used to establish ground truth labels of relevant tokens in the similar article titles.

3. It trains a transformer-based model called Highlight Similar Article Title (HSAT) on the PubCLogs dataset. HSAT is designed to select the most relevant parts of the title of a similar article given the title and abstract of the seed article. Evaluations show HSAT outperforms several baseline models like BM25, Word2Vec, GPT-3.5 and GPT-4.

In summary, the main contribution is using repurposed user query logs to train a model (HSAT) for explaining literature recommendations by highlighting relevant words in the titles of recommended articles.


## What are the keywords or key terms associated with this paper?

 Based on my review of the paper, some of the key keywords and terms associated with it include:

- Literature search 
- Recommendation systems
- Explainability
- Query logs
- Post hoc explainability
- PubMed
- User behavior/intelligence
- Coclicked articles
- Sequence tagging 
- Highlight Similar Article Title (HSAT) model
- Transformer model
- PubCLogs dataset

The paper focuses on explaining literature recommendations in PubMed by selecting the most relevant parts (tokens) of recommended similar article titles. It utilizes a post hoc approach to explainability, where the explanation model is separate from the recommendation model. The key innovation is using PubMed user query logs and coclicked articles to construct a dataset called PubCLogs to train the HSAT transformer model. Overall, the paper demonstrates an effective way to harness implicit user intelligence from search logs to provide explanations for literature recommendations.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in this paper:

1. The paper proposes a novel method to construct the PubCLogs dataset by pairing coclicked articles as seed and similar articles. Could you elaborate more on the intuition behind using coclicks to identify related article pairs? What are some potential limitations of this approach?

2. The paper formulates the task of explaining similar article recommendations as selecting the most relevant tokens in the title of the recommended article. What are some alternative approaches you considered to formulating this task? Why did you settle on token selection in the end? 

3. The Highlight Similar Article Title (HSAT) model is trained on a binary sequence tagging objective using the PubCLogs dataset. Walk me through your process of designing and training this model. What architecture options did you experiment with and why did you end up choosing the final model design?

4. The paper shows that the HSAT model outperforms several baseline models like BM25, Word2Vec, MPNet and GPT-4. Could you analyze the relative strengths and weaknesses of these models on this task? Why do you think HSAT performs the best?

5. One interesting finding is that model performance correlates with the search popularity and semantic similarity of the article pairs. What underlying factors might explain this correlation? How can this insight inform future work? 

6. The paper mentions potential applications for personalized recommendation explanations. What specific approaches do you envision for extending the HSAT model to account for user priors and information needs? What dataset construction and modeling challenges need to be addressed?

7. You utilize the NLTK tokenizer for preprocessing queries andtitles in constructing PubCLogs. What were some other tokenization schemes you experimented with? What were the tradeoffs and how did they impact downstream model performance?

8. The sequence tagging scheme treats the task as binary classification for each token. Did you consider any multi-class classification schemes to account for degrees of relevance? What implementation and evaluation challenges did that introduce?

9. One limitation discussed is the prevalence of uninformative article titles that provide little content insight. How does this impact the quality of explanations that can be provided? What data augmentation or filtering techniques could help address uninformative titles? 

10. The paper focuses on highlighting relevant words in similar article titles, but mentions providing full article recommendation explanations as an end goal. What are your next steps towards generating more complete explanations beyond concise title highlights?
