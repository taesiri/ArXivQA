# [Evolutionary Optimization of Model Merging Recipes](https://arxiv.org/abs/2403.13187)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper "Evolutionary Optimization of Model Merging Recipes":

Problem:
- Model merging is a promising approach for developing powerful AI models by combining multiple pre-trained models, but relies heavily on human intuition and domain knowledge for selecting models and merging recipes. This limits its potential.

Solution:
- Propose an evolutionary approach to automatically discover effective combinations of diverse open-source models to create new capable models, without needing additional data or compute.

- Operate in both parameter space (optimizing weight mixing of layers) and data flow space (optimizing inference path through layers). Allows optimization beyond just model weights.

- Facilitates even cross-domain merging, e.g. Japanese LLM with math reasoning.

Contributions:  
- Introduce Evolutionary Model Merge, a general evolutionary method to automatically find optimal combinations of models to create new foundation models with desired capabilities.

- Show cross-domain merging, discovering ways to merge disparate domain models (non-English + Math/Vision) that may be hard for humans.

- Created state-of-the-art Japanese LLM with math reasoning and Japanese VLM using this method, without explicitly optimizing for downstream tasks.

- Japanese Math LLM achieves SOTA on variety of Japanese LLM benchmarks, even surpassing some 70B parameter models. High efficiency.

- Japanese VLM handles culture-specific content well, outperforming previous Japanese VLMs.

- Overall, introduced new paradigm for automated model composition leveraging collective intelligence of existing models, alternative to expensive training.
