# [Evaluating Data Attribution for Text-to-Image Models](https://arxiv.org/abs/2306.09345)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question seems to be:

How can we evaluate and improve data attribution methods for large-scale text-to-image generative models?

More specifically, the key goals and contributions appear to be:

1. Proposing an efficient method to generate a dataset of text-image pairs where the text exerts a known influence over the synthesized image. This allows for ground truth evaluation of data attribution.

2. Using this dataset to benchmark different feature spaces and retrieval methods for data attribution.

3. Showing the dataset can be used to improve feature spaces for attribution through a contrastive learning objective. 

4. Developing a method to assign soft influence scores indicating the likelihood training images influenced the synthesized output.

5. Demonstrating the approach generalizes beyond small sets of exemplar images to suggest applicability to full data attribution.

So in summary, the main research question seems to center around developing methods to evaluate and improve data attribution for large text-to-image models by creating a dataset with exemplar influence ground truth.


## What is the main contribution of this paper?

 Based on my reading, the main contributions of this paper are:

1. Proposes an efficient method to generate a dataset of synthesized images paired with ground-truth exemplar images that influenced them. This is done by taking a pretrained generative model and tuning it towards new exemplar images/concepts using "customization" techniques. 

2. Uses this dataset to evaluate and compare different candidate image feature spaces (e.g. CLIP, DINO etc) for the task of data attribution. Shows that the dataset can also be used to improve these feature spaces through additional contrastive learning.

3. Demonstrates how to extract soft influence scores over the training dataset by calibrating the similarities between synthesized and training images. This allows assigning probabilities indicating how likely each training image influenced the synthesized image.

4. Finds that while the method is designed and evaluated on small exemplar sets, it generalizes reasonably well even when tuning is done on larger sets of random unrelated images. This suggests it could be applicable to the more general data attribution problem.

In summary, the main contribution is an efficient way to create attribution datasets to evaluate and improve models for attributing training data influence on generative model outputs. The method is analyzed on exemplar-based customization but shows promise for general data attribution.
