# [Improving Reinforcement Learning from Human Feedback Using Contrastive   Rewards](https://arxiv.org/abs/2403.07708)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Existing reinforcement learning from human feedback (RLHF) methods for aligning large language models rely heavily on accurate reward models. 
- However, reward models can be imperfect due to issues like human labeling errors, making the RLHF pipeline fragile.

Proposed Solution: 
- Introduce a concept called "contrastive rewards" which adds a penalty term to the original reward to account for its imperfections.  
- Involves two steps:
   (1) Offline sampling to get baseline responses to prompts. 
   (2) Compute contrastive rewards using the baseline responses and original rewards. Apply them in the PPO stage.

Main Contributions:
- Propose contrastive rewards to improve robustness of RLHF by explicitly handling imperfections of reward models. 
- Show both analytically and empirically that contrastive rewards encourage improvements over baseline, calibrate to task difficulty, reduce variance, and improve overall alignment.
- Achieve substantially better performance over strong baselines on multiple datasets, evaluated by both automatic GPT metrics and human assessments. 
- Consistently outperform baselines by ~20% across different alignment tasks.

In summary, the paper introduces an effective yet simple technique called contrastive rewards to handle imperfections in reward models for RLHF. This enhances robustness and leads to noticeable improvements in aligning language models.
