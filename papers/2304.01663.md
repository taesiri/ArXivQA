# [On the Stability-Plasticity Dilemma of Class-Incremental Learning](https://arxiv.org/abs/2304.01663)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be:How can we effectively balance stability and plasticity in class-incremental learning models, so that they are able to retain knowledge about old classes while continuing to learn about new classes?The key hypotheses/claims explored in the paper are:- Most current class-incremental learning algorithms focus too heavily on stability (avoiding catastrophic forgetting of old classes) at the expense of plasticity (learning new classes). - This leads to feature representations that remain mostly static after initial training and do not accumulate much new knowledge as more data is encountered incrementally.- Analytic tools like centered kernel alignment (CKA) and classifier retraining can be used to evaluate the plasticity of class-incremental learning models.- Using these tools reveals that many recent algorithms have high stability but low plasticity in their learned feature representations.- Methods should aim for a better balance, even if the initial pretrained features seem reasonably good, in order to maximize the potential of continual learning.- Architectures that isolate/expand parameters per task (like DER) may be a promising direction, and efficiency can be improved (like with the proposed pDER).So in summary, the main focus is on analyzing and improving how class-incremental learning algorithms handle the stability-plasticity tradeoff, especially in terms of feature representation learning. Let me know if I have misinterpreted or missed any key aspects!
