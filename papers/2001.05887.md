# [MixPath: A Unified Approach for One-shot Neural Architecture Search](https://arxiv.org/abs/2001.05887)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading, the key points of this paper are:

- It proposes a unified approach called MixPath for efficient neural architecture search that supports multi-path network structures. Current popular two-stage one-shot NAS methods mostly focus on single-path search spaces. 

- It identifies that training a multi-path supernet is very unstable due to the changing statistics when randomly activating paths during training. This disrupts optimization and hurts the ranking ability of the supernet.

- It proposes a lightweight regularization technique called Shadow Batch Normalization (SBN) that uses separate BNs to track the statistics of different path combinations. This stabilizes training and improves ranking.

- By exploiting feature similarity, it reduces the number of required SBNs from exponential to linear in the number of paths. This greatly decreases overhead.

- Experiments show SBN improves ranking correlation on NAS-Bench-101. Searched models on ImageNet and CIFAR obtain state-of-the-art accuracy and outperform hand-crafted networks.

In summary, the key contribution is a unified and efficient approach to enable multi-path NAS by stabilizing supernet training with SBNs, while reducing the overhead through feature similarity. The method obtains strong performance on multiple benchmarks.
