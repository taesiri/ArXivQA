# [MixPath: A Unified Approach for One-shot Neural Architecture Search](https://arxiv.org/abs/2001.05887)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading, the key points of this paper are:

- It proposes a unified approach called MixPath for efficient neural architecture search that supports multi-path network structures. Current popular two-stage one-shot NAS methods mostly focus on single-path search spaces. 

- It identifies that training a multi-path supernet is very unstable due to the changing statistics when randomly activating paths during training. This disrupts optimization and hurts the ranking ability of the supernet.

- It proposes a lightweight regularization technique called Shadow Batch Normalization (SBN) that uses separate BNs to track the statistics of different path combinations. This stabilizes training and improves ranking.

- By exploiting feature similarity, it reduces the number of required SBNs from exponential to linear in the number of paths. This greatly decreases overhead.

- Experiments show SBN improves ranking correlation on NAS-Bench-101. Searched models on ImageNet and CIFAR obtain state-of-the-art accuracy and outperform hand-crafted networks.

In summary, the key contribution is a unified and efficient approach to enable multi-path NAS by stabilizing supernet training with SBNs, while reducing the overhead through feature similarity. The method obtains strong performance on multiple benchmarks.


## What is the main contribution of this paper?

 The main contributions of this paper are:

1. It proposes a unified approach called MixPath for one-shot neural architecture search that supports multi-path search spaces. This extends existing single-path one-shot NAS methods. 

2. It identifies the issues with training a multi-path supernet, including training instability and poor ranking ability. These are caused by the changing statistics of features when summing outputs from multiple paths.

3. It proposes a lightweight mechanism called Shadow Batch Normalization (SBN) to regularize the statistics of multi-path features. By exploiting feature similarity, it reduces the number of SBNs from exponential to linear in the maximum number of paths.

4. It shows through experiments that SBNs stabilize supernet training and significantly improve the ranking correlation. Together with post BN calibration, the proposed method achieves high Kendall tau on NAS-Bench-101.

5. It demonstrates the effectiveness of MixPath by searching for models on CIFAR-10 and ImageNet. The discovered models like MixPath-B outperform reference models like EfficientNet-B0 with fewer parameters and FLOPS.

In summary, the key contribution is a unified one-shot NAS approach that can efficiently search multi-path models, enabled by the proposed SBN technique to train a robust supernet evaluator.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the key points from the paper:

The paper proposes MixPath, a unified one-shot neural architecture search approach that can efficiently search over multi-path network structures by using a novel Shadow Batch Normalization technique to stabilize supernet training and boost model ranking ability.
