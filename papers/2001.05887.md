# [MixPath: A Unified Approach for One-shot Neural Architecture Search](https://arxiv.org/abs/2001.05887)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading, the key points of this paper are:

- It proposes a unified approach called MixPath for efficient neural architecture search that supports multi-path network structures. Current popular two-stage one-shot NAS methods mostly focus on single-path search spaces. 

- It identifies that training a multi-path supernet is very unstable due to the changing statistics when randomly activating paths during training. This disrupts optimization and hurts the ranking ability of the supernet.

- It proposes a lightweight regularization technique called Shadow Batch Normalization (SBN) that uses separate BNs to track the statistics of different path combinations. This stabilizes training and improves ranking.

- By exploiting feature similarity, it reduces the number of required SBNs from exponential to linear in the number of paths. This greatly decreases overhead.

- Experiments show SBN improves ranking correlation on NAS-Bench-101. Searched models on ImageNet and CIFAR obtain state-of-the-art accuracy and outperform hand-crafted networks.

In summary, the key contribution is a unified and efficient approach to enable multi-path NAS by stabilizing supernet training with SBNs, while reducing the overhead through feature similarity. The method obtains strong performance on multiple benchmarks.


## What is the main contribution of this paper?

 The main contributions of this paper are:

1. It proposes a unified approach called MixPath for one-shot neural architecture search that supports multi-path search spaces. This extends existing single-path one-shot NAS methods. 

2. It identifies the issues with training a multi-path supernet, including training instability and poor ranking ability. These are caused by the changing statistics of features when summing outputs from multiple paths.

3. It proposes a lightweight mechanism called Shadow Batch Normalization (SBN) to regularize the statistics of multi-path features. By exploiting feature similarity, it reduces the number of SBNs from exponential to linear in the maximum number of paths.

4. It shows through experiments that SBNs stabilize supernet training and significantly improve the ranking correlation. Together with post BN calibration, the proposed method achieves high Kendall tau on NAS-Bench-101.

5. It demonstrates the effectiveness of MixPath by searching for models on CIFAR-10 and ImageNet. The discovered models like MixPath-B outperform reference models like EfficientNet-B0 with fewer parameters and FLOPS.

In summary, the key contribution is a unified one-shot NAS approach that can efficiently search multi-path models, enabled by the proposed SBN technique to train a robust supernet evaluator.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the key points from the paper:

The paper proposes MixPath, a unified one-shot neural architecture search approach that can efficiently search over multi-path network structures by using a novel Shadow Batch Normalization technique to stabilize supernet training and boost model ranking ability.


## How does this paper compare to other research in the same field?

 This paper introduces MixPath, a unified approach for one-shot neural architecture search that can efficiently search for multi-path network architectures. Here are some key ways it compares to other research in neural architecture search:

- It focuses on enabling efficient search over multi-path architectures, while most prior one-shot NAS methods like SPOS, FairNAS, and Single Path One-Shot have focused only on single-path architectures. 

- It proposes a new training regularization method called Shadow Batch Normalization (SBN) to stabilize training of the one-shot supernet with multiple paths. This is a novel contribution compared to prior work.

- It achieves state-of-the-art results on CIFAR-10 and ImageNet compared to other one-shot NAS methods. Thesearched MixPath models are comparable to or better than models found by methods like MnasNet, FBNet, and MixNet.

- It demonstrates strong transfer learning performance on COCO object detection, outperforming other mobile NAS models like MnasNet-A2 and MobileNetV3.

- It analyzes the model ranking capability using the NAS-Bench-101 benchmark and shows SBN improves correlation compared to training without it. This is a more rigorous analysis than in some other one-shot NAS papers.

- The overall approach of training a weight-sharing supernet and then searching for architectures is similar to other two-stage one-shot NAS methods like SPOS, Single Path One-Shot, and FairNAS. But the focus on multi-path search spaces is novel.

So in summary, the main novel contributions compared to prior work are 1) support for multi-path search spaces in one-shot NAS, and 2) the proposed SBN technique to make this feasible. The results demonstrate state-of-the-art performance and strong transferability.


## What future research directions do the authors suggest?

 Based on my reading of the paper, here are some of the future research directions suggested by the authors:

- Exploring more advanced search spaces with multi-path structures beyond the inverted bottleneck blocks used in this work. The authors propose combining mixed depthwise convolutions and multi-branch aggregations, but there is room to explore other types of blocks and connections.

- Applying the proposed MixPath approach to other tasks beyond image classification, such as object detection, segmentation, etc. The authors demonstrate some transferability to COCO object detection, but more work could be done to customize MixPath for different tasks.

- Developing more advanced training techniques for stabilizing and improving the ranking ability of multi-path supernets. The shadow batch normalization mechanism shows promising results, but there may be other regularization or calibration methods worth exploring. 

- Reducing the search cost further. The authors reduce search costs substantially compared to prior NAS methods through weight sharing, but searching large models on big datasets like ImageNet still requires non-trivial resources. More work on reducing search costs would be valuable.

- Automating the search space design itself, rather than relying on manual creation of multi-path spaces. Using techniques like neural architecture optimization or evolutionary algorithms to evolve the search space could remove the need for human expertise.

- Analyzing the theoretical properties of the shadow batch normalization approach more formally. The authors provide empirical evidence of its benefits, but a more rigorous theoretical analysis could provide better understanding.

- Incorporating architectural innovations outside of NAS, such as self-attention, into the search space. The authors focus on convolutional networks, but search spaces including transformers and other architectures could be beneficial.

In summary, the core suggestions are to explore more advanced multi-path search spaces, apply the approach to new tasks, improve training techniques, reduce costs further, automate search space design, strengthen theoretical analysis, and incorporate new architectural advances. Overall, the paper opens up many exciting avenues for improving automated architecture search.
