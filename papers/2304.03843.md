# Why think step by step? Reasoning emerges from the locality of   experience

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is:Why is chain-of-thought reasoning helpful for language models, and under what conditions does it provide significant benefits over direct conditional probability estimation? More specifically, the authors hypothesize that chain-of-thought reasoning is effective when the training data consists of local clusters of variables that influence each other strongly. In this case, reasoning through intermediate variables allows the model to incrementally chain together inferences between variables that were frequently observed together, in order to estimate relationships between variables that were rarely or never observed together during training.The key hypothesis is that the local structure of the training data, where subsets of highly influential variables are observed together, enables effective chain-of-thought reasoning. The authors test this hypothesis theoretically and empirically by training language models on samples from Bayesian networks with different observation structures.


## What is the main contribution of this paper?

Based on the abstract, the main contribution of this paper seems to be investigating why and how chain-of-thought reasoning is useful in language models. The authors test the hypothesis that reasoning is effective when training data consists of local clusters of variables that influence each other strongly. They prove a "reasoning gap" for the simple case of an autoregressive density estimator trained on local samples from a chain-structured probabilistic model. They also test the hypothesis empirically in more complex models, training an autoregressive language model on samples from Bayes nets with only subsets of variables in each sample. Their results suggest that chain-of-thought reasoning is useful for language models because direct prediction is inaccurate for inferences spanning variables rarely seen together in training, while reasoning can incrementally chain local statistical dependencies that are frequently observed. The combination of locally-structured training data and reasoning leads to greater data efficiency than training on more complete data. Overall, the paper provides insight into why step-by-step reasoning is useful in language models and suggests the usefulness stems from the local statistical structure of the training data.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key findings from the paper:The paper shows theoretically and empirically that when training data has local structure, with variables clustered into related subsets, then autoregressive models like transformers can accurately estimate conditional probabilities between unobserved variable pairs by reasoning through intermediate steps.


## How does this paper compare to other research in the same field?

This paper makes a nice contribution to the literature on prompting and chain-of-thought reasoning in large language models. Here are a few key ways it relates to other recent work:- It directly builds on the findings of Kojima et al. (2022) and Nye et al. (2021), who showed that prompting large LMs to generate intermediate reasoning steps improves performance on certain tasks. This paper digs into the question of why and when chain-of-thought prompting is helpful.- It shares the goal of understanding reasoning in LMs with work like Wei et al. (2022) and Zelikman et al. (2022) on in-context learning from reasoning chains. However, it focuses specifically on zero-shot prompting without giving examples.- The theoretical analysis connects to recent work like Chan et al. (2022) in formalizing notions like burstiness and locality that explain in-context learning. This paper introduces a complementary theoretical perspective centered on local dependency structure. - The experiments manipulating training data structure are novel and help isolate the impact of local vs global correlations. This complements prompt-based analyses.- The framing in terms of recovering the ground-truth conditional probabilities is also unique and links the study to probabilistic modeling.Overall, this paper carves out a distinct perspective centered on local statistical structure in training data as an explanation for when and why chain-of-thought prompting is effective. The theory and experiments provide complementary support for this hypothesis. Situating reasoning prompting in a probabilistic framework and manipulating training data itself are both innovative directions for this area.


## What future research directions do the authors suggest?

Based on my reading of the paper, some of the main future research directions suggested by the authors include:- Testing their hypothesis in more complex models and naturalistic settings. The authors mention exploring whether chain-of-thought reasoning is similarly helpful when training language models on real-world text data as opposed to the synthetic Bayes net data used in their experiments. - Exploring the structure of the observation distribution for human learners. The authors suggest studying the statistical structure of the information humans encounter during development to better understand if it facilitates reasoning in a similar way to the locally-structured training data in their experiments.- Investigating how language models might learn to reason more effectively. The authors mention using techniques like fine-tuning, in-context learning, and reinforcement learning to potentially improve the reasoning abilities of language models.- Exploring the origins of human reasoning more broadly. While focused on chain-of-thought reasoning, the authors suggest their approach of using language models as toy models could be helpful for studying other aspects of human reasoning and problem solving.- Developing better datasets and data curation strategies to support reasoning. The authors propose that constructing datasets with tight correlations between concepts could enhance reasoning and reduce data needs.- Analyzing the inductive biases that lead models to default to marginal probabilities. The authors plan to study why the transformer language models revert to marginal probabilities on unfamiliar variable pairs.In summary, the main future directions focus on extending their approach to more naturalistic settings, connecting it more deeply to human reasoning, and using language models to study the origins and mechanisms of reasoning.


## Summarize the paper in one paragraph.

Here is a one paragraph summary of the key points from the paper:The paper investigates why chain-of-thought reasoning, where language models generate intermediate reasoning steps before answering a question, improves performance on certain tasks. The authors hypothesize that chain-of-thought reasoning is effective when the training data has local structure, meaning variables that influence each other tend to be observed together. They prove theoretically that in a simple case, reasoning through intermediate variables reduces bias compared to direct prediction when training data is locally structured. Experimentally, they train language models on samples from Bayesian networks with different observation structures and find reasoning is only helpful when samples are drawn from local neighborhoods in the network graph. The combination of local training data and reasoning also yields greater data efficiency. Overall, their results suggest chain-of-thought reasoning is useful because local statistical structure in training data enables chaining accurate local inferences to estimate unconditional relationships. The locality of human experience may similarly facilitate human reasoning.


## Summarize the paper in two paragraphs.

Here is a two paragraph summary of the paper:The paper investigates why reasoning step-by-step is useful for language models. The authors hypothesize that chain-of-thought reasoning is effective when training data has local structure, meaning observations tend to occur in overlapping neighborhoods of related concepts. They first prove theoretically that for a simple autoregressive model trained on adjacent pairs from a chain graph, reasoning through intermediate variables reduces bias in estimating conditional probabilities between non-adjacent variables. The authors then train transformer language models on samples from Bayesian networks with different observation structures. They find reasoning improves estimation of held-out conditional probabilities only when training data comes from local neighborhoods in the Bayesian networks. Locally-structured training also enables more data-efficient learning. With the right training data structure, models can accurately reason through chains of inferences between concepts not directly observed together.The key findings are: 1) Chain-of-thought reasoning improves inference in language models when training data has local structure reflecting dependencies between concepts. 2) Reasoning is unhelpful with non-local training data. 3) Locally-structured data with reasoning at test time is substantially more data-efficient for learning conditional probabilities than training on fully observed data. The results suggest chain-of-thought reasoning is effective because local training enables models to learn dependencies between concepts that can be chained together, while reasoning is necessary to span dependencies beyond the local training clusters. The theory and experiments show how the locality of experience determines the effectiveness of reasoning in language models.
