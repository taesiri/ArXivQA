# [TinyMIM: An Empirical Study of Distilling MIM Pre-trained Models](https://arxiv.org/abs/2301.01296)

## What is the central research question or hypothesis that this paper addresses?

 The central research question this paper addresses is how to transfer the success of large masked image modeling (MIM) pre-trained models to smaller vision transformer models. 

The key hypothesis is that distillation techniques can be used to train high-quality smaller models by mimicking the representations learned by larger MIM pre-trained models, avoiding the need to directly pre-train the small models on the difficult MIM task.

In particular, the paper systematically studies different options in the distillation framework when using MIM models as teachers, including:

- Distillation targets (e.g. class token, features, relations)
- Losses 
- Input (masked vs original image)
- Network regularization
- Sequential distillation

The goal is to understand which factors lead to the best performance when distilling MIM models to smaller vision transformers.

In summary, the paper aims to enable small vision transformer models to benefit from masked image modeling pre-training through an empirical exploration of knowledge distillation techniques.


## What is the main contribution of this paper?

 The main contribution of this paper is presenting TinyMIM, a method to transfer knowledge from large masked image modeling (MIM) pre-trained models to smaller vision Transformer models via distillation. The key ideas and contributions are:

- This is the first work to successfully perform masked image modeling (MIM) pre-training for smaller ViT models. Previous MIM methods like MAE hurt performance when applied to small models. 

- TinyMIM avoids directly training small models with MIM. Instead, it uses knowledge distillation to transfer knowledge from a larger MIM pre-trained teacher to a smaller student model.

- The paper systematically studies various factors in the distillation framework for MIM models, including distillation targets, losses, input images, network regularization, sequential distillation, etc. 

- It finds that distilling token relations is more effective than distilling CLS tokens or features, using intermediate layers as targets can be better than the last layer, and weaker augmentation/regularization is preferred.

- TinyMIM achieves significant accuracy gains over MAE pre-training across ViT-Tiny, ViT-Small, and ViT-Base on ImageNet classification (+4.2%/+2.4%/+1.4%). The TinyMIM-Base model also gets +4.1 mIoU boost on ADE20K segmentation.

- TinyMIM sets a new SOTA accuracy record among tiny Transformers on ImageNet while using a plain architecture without specialized inductive biases. This suggests training methods may be more important than architectural innovations for small models.

In summary, the key contribution is successfully applying knowledge distillation to transfer MIM pre-training benefits to smaller vision Transformers, through comprehensive experiments to determine the optimal distillation framework. The strong performance of plain TinyMIM models also provides new insight into developing small vision Transformers.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the key points from the paper:

The paper proposes TinyMIM, a method to effectively transfer knowledge from large masked image modeling pre-trained vision Transformers to smaller models via distillation, achieving significant performance improvements; through comprehensive studies, it identifies relation distillation, using intermediate layers as targets, and weak regularization as optimal choices.


## How does this paper compare to other research in the same field?

 Here are a few key points comparing this paper to other research in masked image modeling and vision transformers:

- The main contribution is using knowledge distillation to transfer masked image modeling (MIM) pre-training benefits to smaller vision transformer (ViT) models. Most prior MIM work has focused on larger models. 

- It provides a systematic study of various design choices for distilling MIM models such as distillation targets, losses, input augmentation, etc. This helps reveal insights like distilling relations being most effective.

- The distilled TinyMIM models achieve significant gains over MIM pre-training directly on smaller models, with +4.2% on ViT-Tiny. This demonstrates an effective way to leverage MIM for small models.

- The 79.6% ImageNet accuracy of TinyMIM-T sets a new SOTA for tiny transformers. This shows competitive performance can be achieved without specialized model architecture inductive biases used in other works.

- The gains on semantic segmentation (+4.1 mIoU on ADE20K) demonstrate transferability to other vision tasks beyond image classification.

- Compared to concurrent works like MobileFormer, MobileViT, etc. that design efficient transformer architectures, this explores the training methodology direction. The strong TinyMIM results suggest training advances may be an alternative to specialized model architectures.

Overall, this paper provides novel insight into knowledge transfer for MIM models to smaller models, achieving strong results. The systematic study methodology also helps advance understanding in this area compared to other works. The competitive tiny model performance also demonstrates the viability of this training-based approach relative to architecture modification approaches.


## What future research directions do the authors suggest?

 The authors suggest a few potential future research directions based on their work:

- Investigate distilling other pre-trained models besides MAE, such as models pre-trained with contrastive learning or other self-supervised approaches. They note their distillation framework could potentially improve other pre-trained models as well.

- Explore distillation for even smaller models, like MobileNet-scale networks, to enable deployment on devices with very limited compute.

- Study combining TinyMIM with architectural improvements like MobileViT to further optimize efficiency. 

- Research distillation methods to transfer inductive biases like translation equivariance from CNNs to vision Transformers.

- Develop methods to distill knowledge not just within the same model family (e.g. ViT to ViT) but also across model families (e.g. CNN to ViT).

- Explore TinyMIM for additional domains beyond image classification and segmentation, such as object detection, video, etc.

- Analyze what visual knowledge is captured by different layers of MIM pre-trained models to better understand what is being transferred via TinyMIM distillation.

In summary, they suggest further exploring TinyMIM distillation for other models, tasks, and model families, as well as analyzing what knowledge is encoded in MIM models and how that transfers through distillation. The goal is to continue improving efficiency and performance of vision Transformers, especially smaller models, through better training techniques.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the key points:

This paper explores distillation techniques to transfer the success of large masked image modeling (MIM) pre-trained vision transformer models to smaller models. Through a systematic study of different options in the distillation framework like distillation targets, losses, input, network regularization, and sequential distillation, the authors find that distilling token relations works better than distilling the CLS token or features, using an intermediate layer of the teacher as target is better when student depth mismatches teacher depth, and weak regularization is preferred. Using these findings, the authors achieve significant fine-tuning accuracy improvements on ImageNet classification over direct MIM pre-training for ViT-Tiny, ViT-Small, and ViT-Base models. The ViT-Tiny model also achieves state-of-the-art accuracy among small vision Transformers on ImageNet classification and ADE20K segmentation. This suggests an alternative way to develop small vision Transformers by exploring better training methods rather than introducing inductive biases into architectures.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the paper:

This paper explores distillation techniques to transfer the success of large masked image modeling (MIM) pre-trained models to smaller vision transformer models. The authors systematically study different options in the distillation framework including distillation targets, losses, input, network regularization, sequential distillation, etc. Key findings include: 1) Distilling token relations is more effective than distilling the CLS token and feature maps; 2) Using intermediate layers as the target can perform better than the last layer when student and teacher depth differs; 3) Weak regularization is preferred; 4) Sequential distillation from larger to smaller models improves performance. 

The authors use these findings to achieve significant fine-tuning accuracy improvements over direct MIM pre-training on ImageNet classification across ViT-Tiny, ViT-Small, and ViT-Base models. The TinyMIM models also achieve strong performance on ADE20K semantic segmentation. The ViT-Tiny model sets a new record accuracy for small vision models of similar size/computation. This suggests exploring better training methods, rather than introducing inductive biases into architectures, may be a promising direction for developing high-performing small vision transformers.


## Summarize the main method used in the paper in one paragraph.

 The paper proposes TinyMIM, a method to transfer knowledge from large masked image modeling (MIM) pre-trained vision Transformers to smaller ones via distillation. Instead of directly training small ViT models with MIM, which is difficult, TinyMIM mimics the relations of a larger MIM pre-trained teacher model to distill its knowledge into a smaller student model. The paper systematically studies various factors in the distillation framework for MIM models, including distillation targets (relations vs tokens/features), losses, input images (masked vs original), network regularization, auxiliary losses, and macro distillation strategies. Key findings include: 1) Distilling token relations works better than distilling class tokens or features; 2) Using intermediate layers of the teacher as targets can be better when student/teacher depth differs; 3) Weak regularization is preferred; 4) An auxiliary MIM loss does not help; 5) Sequential distillation from larger to smaller models is beneficial. TinyMIM achieves significant gains over MIM pre-training on ImageNet classification across different model sizes, especially for very small models where MIM struggles. The transferred knowledge also leads to better performance on segmentation.


## What problem or question is the paper addressing?

 This paper is addressing the problem that masked image modeling (MIM) pre-training, which has proven very effective for large vision transformer models, does not work well or can even hurt performance when applied to small vision transformer models. The key question is how to make small vision transformer models benefit from MIM pre-training.

The main contributions and findings of the paper are:

- It proposes TinyMIM, which uses knowledge distillation to transfer MIM pre-training from larger models to smaller ones rather than directly pre-training the small models with MIM.

- It systematically studies different factors in the knowledge distillation framework for MIM models, including distillation targets, losses, input, network regularization, sequential distillation, etc. 

- Key findings from the experiments include: 

1) Distilling token relations is more effective than distilling CLS token and features. 

2) Using intermediate layers as target can be better than using the last layer when student/teacher depth differs.

3) Weak regularization is preferred.

4) Sequential distillation helps.

5) An auxiliary MIM loss does not improve accuracy.

- TinyMIM achieves significant gains over direct MIM pre-training for ViT-Tiny, ViT-Small, and ViT-Base on ImageNet classification (+4.2%, +2.4%, +1.4% respectively).

- It also achieves strong performance on other tasks like semantic segmentation. 

- The TinyMIM-T model sets a new SOTA for tiny Transformer models, demonstrating the power of pre-training rather than specialized architectures.

In summary, the key innovation is using distillation to enable small vision Transformers to benefit from MIM pre-training, with extensive experiments providing insights into optimal distillation strategies. The strong performance shows pre-training can be more impactful than architectural modifications for small models.
