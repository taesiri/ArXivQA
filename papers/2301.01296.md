# [TinyMIM: An Empirical Study of Distilling MIM Pre-trained Models](https://arxiv.org/abs/2301.01296)

## What is the central research question or hypothesis that this paper addresses?

 The central research question this paper addresses is how to transfer the success of large masked image modeling (MIM) pre-trained models to smaller vision transformer models. 

The key hypothesis is that distillation techniques can be used to train high-quality smaller models by mimicking the representations learned by larger MIM pre-trained models, avoiding the need to directly pre-train the small models on the difficult MIM task.

In particular, the paper systematically studies different options in the distillation framework when using MIM models as teachers, including:

- Distillation targets (e.g. class token, features, relations)
- Losses 
- Input (masked vs original image)
- Network regularization
- Sequential distillation

The goal is to understand which factors lead to the best performance when distilling MIM models to smaller vision transformers.

In summary, the paper aims to enable small vision transformer models to benefit from masked image modeling pre-training through an empirical exploration of knowledge distillation techniques.


## What is the main contribution of this paper?

 The main contribution of this paper is presenting TinyMIM, a method to transfer knowledge from large masked image modeling (MIM) pre-trained models to smaller vision Transformer models via distillation. The key ideas and contributions are:

- This is the first work to successfully perform masked image modeling (MIM) pre-training for smaller ViT models. Previous MIM methods like MAE hurt performance when applied to small models. 

- TinyMIM avoids directly training small models with MIM. Instead, it uses knowledge distillation to transfer knowledge from a larger MIM pre-trained teacher to a smaller student model.

- The paper systematically studies various factors in the distillation framework for MIM models, including distillation targets, losses, input images, network regularization, sequential distillation, etc. 

- It finds that distilling token relations is more effective than distilling CLS tokens or features, using intermediate layers as targets can be better than the last layer, and weaker augmentation/regularization is preferred.

- TinyMIM achieves significant accuracy gains over MAE pre-training across ViT-Tiny, ViT-Small, and ViT-Base on ImageNet classification (+4.2%/+2.4%/+1.4%). The TinyMIM-Base model also gets +4.1 mIoU boost on ADE20K segmentation.

- TinyMIM sets a new SOTA accuracy record among tiny Transformers on ImageNet while using a plain architecture without specialized inductive biases. This suggests training methods may be more important than architectural innovations for small models.

In summary, the key contribution is successfully applying knowledge distillation to transfer MIM pre-training benefits to smaller vision Transformers, through comprehensive experiments to determine the optimal distillation framework. The strong performance of plain TinyMIM models also provides new insight into developing small vision Transformers.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the key points from the paper:

The paper proposes TinyMIM, a method to effectively transfer knowledge from large masked image modeling pre-trained vision Transformers to smaller models via distillation, achieving significant performance improvements; through comprehensive studies, it identifies relation distillation, using intermediate layers as targets, and weak regularization as optimal choices.
