# [Japanese-English Sentence Translation Exercises Dataset for Automatic   Grading](https://arxiv.org/abs/2403.03396)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Sentence translation exercises (STEs) are useful tools in early stages of L2 language learning, but manually grading them places a heavy burden on teachers. 
- Automating the grading of STEs can transform language learning, but it is a challenging task even for state-of-the-art models.

Proposed Solution:
- Formulate STE grading as classifying student responses on each predefined analytic criterion in the grading rubric.
- Created a dataset of 21 STE questions between Japanese and English, with detailed rubrics and 3,498 student responses annotated with scores on each criterion.
- Evaluated performance of finetuned BERT model and GPT models with few-shot learning on this dataset.

Key Contributions:  
- First formulation of automatic STE grading task grounded in educational objectives.
- First STE dataset spanning 21 questions with detailed analytic rubrics and over 3,000 annotated responses.  
- Strong baseline from finetuned BERT, achieving ~90% F1 on classifying correct responses, but weaker performance (<80% F1) on incorrect ones.
- GPT models struggled despite few-shot learning, indicating challenging open problems in this space.

In summary, this paper introduced and established the novel task of automating analytic grading for sentence translation exercises to assist language learning. The formulation, dataset creation, and model evaluations provide a strong foundation while indicating opportunities for advancement.
