# [Large Language Model Programs](https://arxiv.org/abs/2305.05364)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the key points of this paper are:- The paper proposes the idea of "Large Language Model Programs" (LLM programs) as a way to enhance the capabilities of large pre-trained language models (LLMs). - The key limitation of standard LLMs is that they have a fixed context size, which restricts their ability to process long sequences or large amounts of information. LLM programs aim to overcome this by embedding the LLM within an external program that controls the input/output to the LLM.- LLM programs break down complex tasks into simpler subtasks or steps. Each step uses a focused prompt and provides only the necessary context to the LLM. This allows simplifying the specifications for each subtask.- The paper demonstrates LLM programs through an example of evidence-based question answering. The program has a filtering stage to select relevant paragraphs, followed by a tree search to iteratively expand reasoning chains.- Without any finetuning, the LLM program achieves improved performance compared to standard prompting techniques like chain of thought reasoning.- The paper argues LLM programs can expand capabilities, improve interpretability, incorporate algorithmic knowledge, and provide generalization guarantees compared to end-to-end training.- Recent related work is highlighted showing the emerging use of multi-step programs and modules with LLMs.In summary, the key hypothesis is that structured LLM programs can greatly expand the capabilities of large pre-trained language models in a more interpretable and generalizable way compared to standard finetuning approaches. The question answering example supports this hypothesis.
