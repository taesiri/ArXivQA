# [Selective Ensembles for Consistent Predictions](https://arxiv.org/abs/2111.08230v1)

## What is the central research question or hypothesis that this paper addresses?

 This paper addresses the problem of inconsistencies between deep machine learning models trained under similar conditions, such as with different random seeds or small changes to the training data. The central hypothesis is that selective ensembling, which involves only making predictions when there is high agreement among an ensemble of models trained under different random conditions, can bound the probability of disagreements between models. 

The key research questions addressed are:

1) To what extent do deep models exhibit inconsistencies in their predictions and explanations due to small changes in training conditions?

2) Can selective ensembling provably bound the probability of disagreements between models trained under different random conditions?

3) How effectively does selective ensembling reduce inconsistencies in practice, in terms of prediction disagreement rates and differences in feature attributions?

4) What are the tradeoffs between tighter bounds on disagreements versus higher abstention rates when using selective ensembling?

Overall, the paper aims to both quantify the extent of inconsistencies in deep models and propose selective ensembling as a method to mitigate these issues with theoretical guarantees and empirical demonstrations. The main hypothesis is that selective ensembling can reliably reduce inconsistencies compared to individual models.


## What is the main contribution of this paper?

 This paper proposes selective ensembles, a new ensemble modeling approach that aims to provide bounded inconsistency in predictions and explanations across inconsequential changes during model training, such as random initialization or small changes to the training data. 

The key contributions are:

1. The paper first demonstrates that deep neural networks can produce inconsistent predictions and explanations not just across different models, but even between very similar models that differ only in small random ways like initialization or one-point training set differences. This suggests the decision process in deep models is very sensitive to minor perturbations.

2. The paper introduces selective ensembles, which are ensemble models that can abstain from prediction on points where the constituent models disagree, based on a statistical test. This allows selective ensembles to guarantee bounded disagreement rates with the "true" majority prediction.

3. Empirically, the paper shows selective ensembles achieve zero disagreement on 7 datasets, even with as few as 10 constituent models. They also produce more consistent explanations compared to individual models.

In summary, selective ensembles mitigate the prediction and explanation instability of deep models across minor random differences in training, providing a way to get consistent behavior from neural networks in sensitive applications. The key innovation is allowing the ensemble model to abstain when constituent models disagree significantly.
