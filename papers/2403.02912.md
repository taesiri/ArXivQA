# [Mirror Descent Algorithms with Nearly Dimension-Independent Rates for   Differentially-Private Stochastic Saddle-Point Problems](https://arxiv.org/abs/2403.02912)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem Statement:
- The paper studies the problem of solving stochastic saddle-point (convex-concave) problems under differential privacy constraints. Specifically, it considers minimizing the expected duality gap for objectives that are Lipschitz and smooth with respect to the $\ell_1$ norm over polyhedral constraint sets. 

- Previous work has shown that for bilinear objectives (a special case), it is possible to achieve convergence rates that only scale poly-logarithmically with the dimension. However, for general convex-concave objectives, all existing algorithms have rates that scale polynomially with the dimension. 

- The goal is to develop new algorithms that can achieve nearly dimension-independent rates for broader classes of convex-concave stochastic saddle-point problems.

Proposed Solution:
- The paper proposes two stochastic mirror descent algorithms based on private iterations obtained via Maurey sparsification (random vertex sampling).

- The first algorithm controls bias by using a large number of samples per iteration. The second algorithm reduces bias via debiasing techniques from the simulation literature.

- The analysis relies on several new extensions of the classical Maurey sparsification lemma to control errors in function values, gradient approximations, and duality gaps under vertex sampling.

Main Contributions:
- First mirror descent style algorithms for DP stochastic saddle-point problems and DP stochastic convex optimization in $\ell_1$ setups with nearly dimension-independent rates.

- New analysis based on Maurey sparsification guarantees for function approximation rather than norm approximation. Includes high-probability and second moment bounds.

- For quadratic objectives, the algorithms match the optimal dimension-independent rates, providing new optimal methods e.g. for private synthetic data generation.

- With debiasing, the algorithms attain the optimal rates for second-order smooth objectives with constant probability. This provides evidence that optimal rates should be achievable.

In summary, the paper provides an important step towards optimal and practical methods for an important class of differentially private optimization problems. The analysis introduces useful tools based on Maurey sparsification.
