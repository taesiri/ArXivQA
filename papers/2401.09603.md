# [Rethinking FID: Towards a Better Evaluation Metric for Image Generation](https://arxiv.org/abs/2401.09603)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem Statement
- FID (Frèchet Inception Distance) is currently the most popular metric used to evaluate image generation models, but the authors identify important limitations:
  - Inception embeddings are weak, only trained on 1M ImageNet images with 1000 classes
  - FID assumes Inception embeddings are multivariate normal, but this assumption is wrong
  - FID has poor sample efficiency and is a biased estimator
- The authors show FID can contradict human ratings, does not reflect iterative model improvements, and fails to capture complex distortions

Proposed Solution  
- Introduce CMMD, a new metric using CLIP embeddings and Maximum Mean Discrepancy (MMD)
- CLIP is trained on much more data (400M images) so represents image content better
- MMD is a distribution-free distance metric that does not assume normality
- MMD is sample efficient and an unbiased estimator

Key Contributions
- Call into question use of FID as the primary image generation evaluation metric
  - Show examples where FID fails through experiments and statistical tests 
- Propose CMMD as a more suitable metric for modern image generation
  - Leverages CLIP embeddings and properties of MMD
  - Empirically demonstrate CMMD does not have FID's shortcomings
- Analyze drawbacks of FID more deeply than prior work
  - Show FID's normality assumption is wrong and consequences
  - Reveal FID fails to capture complex distortions 
  - Demonstrate inconsistent FID values under varying sample sizes

In summary, the authors make a case that the widely used FID metric has substantial flaws, and propose an alternative CMMD metric to enable more robust evaluation of modern image generation models.


## Summarize the paper in one sentence.

 This paper calls into question the use of Fréchet Inception Distance (FID) for evaluating modern text-to-image models, proposes an alternative metric called CMMD using CLIP embeddings and maximum mean discrepancy, and shows CMMD correlates better with human raters and captures image distortions more accurately than FID.


## What is the main contribution of this paper?

 The main contributions of this paper are:

1) It calls for a reevaluation of FID as the primary evaluation metric for image generation models, especially modern text-to-image models. The authors show that FID can contradict human raters, does not reflect gradual improvements in iterative models, and fails to capture complex image distortions. 

2) It identifies and analyzes some key limitations of using the Fréchet distance with Inception features for evaluating image generation models, including incorrect normality assumptions and poor sample efficiency.

3) It proposes an alternative metric called CMMD, which uses CLIP embeddings with the maximum mean discrepancy (MMD) distance. CMMD does not make assumptions about the distributions, is an unbiased estimator, and is more sample efficient compared to FID.

4) Through extensive experiments and analysis, the paper demonstrates the issues with relying solely on FID for text-to-image evaluation, and shows that the proposed CMMD metric offers a more reliable assessment of image quality.

In summary, the key contribution is calling into question the widespread use of FID, analyzing its limitations, and proposing a more suitable alternative in CMMD for evaluating modern text-to-image models.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and concepts are:

- Fréchet Inception Distance (FID): A popular metric to evaluate image generation models by estimating the distance between distributions of Inception features. The paper argues FID has several limitations.

- Maximum Mean Discrepancy (MMD): A proposed alternative distance metric that uses CLIP image features. Does not assume normality and is more sample efficient compared to FID.  

- CLIP features: Image embeddings from the CLIP model, which is trained on a much larger and diverse dataset compared to Inception. Better suited to represent complex generated image content.

- Iterative image generation: Models like diffusion and Muse that progressively refine images over multiple steps. FID often fails to reflect quality improvements over iterations.

- Image distortions: FID cannot reliably detect complex distortions like replacing VQGAN tokens. The proposed CMMD metric correctly ranks based on distortion severity.  

- Sample efficiency: FID requires large sample sizes while CMMD remains consistent even for small samples. Important for fast evaluation.

- Human evaluation: Considered the gold standard for evaluating text-to-image models. Examples in paper show FID can contradict human raters, while CMMD aligns better.

In summary, key concepts are limitations of FID, the proposed CMMD metric to address those, use of CLIP features, comparison to human evaluation, and model iteration/distortion tests showing differences between the metrics.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in the paper:

1) The paper proposes using CLIP embeddings with MMD as an improved image quality metric over FID. What are some potential downsides or limitations of using CLIP embeddings compared to Inception embeddings? For example, could CLIP overfit to artificial patterns it saw during pre-training?

2) The paper shows sample efficiency advantages for MMD over FID. However, does MMD lose discriminative power with very large sample sizes compared to FID? At what sample size does this crossover happen? 

3) The paper advocates for a "distribution-free" metric with MMD. However, what assumptions does MMD still make about the distributions? Could these assumptions be violated in practice?

4) How sensitive is the performance of CMMD to the choice of kernel and its hyperparameters? Is an RBF kernel with fixed sigma parameter optimal across all scenarios? 

5) The paper shows improved correlation of CMMD with human raters on a particular text-to-image model. However, could there be other models or datasets where CMMD correlates worse with human judgement compared to FID?

6) Can the computational and statistical advantages of CMMD be combined with classifier-based metrics to get the best of both worlds? For example, using classifier confidence scores with MMD.

7) What is the power of the CMMD metric to detect overfitting of image generation models to the train dataset? How does it compare with FID in detecting memorization and generalization?

8) The paper focuses on evaluating image quality. Can CMMD be effectively adapted to also measure prompt relevance without human annotations? How would the metric formulation need to change?

9) What theoretical guarantees exist for MMD as a metric for comparing distributions, that do not hold for Fréchet distance? For example, can we derive sample complexity bounds? 

10) How straightforward is it to apply the proposed method to modalities other than images, such as audio or video generation tasks? What changes would need to be made?
