# [MULAN-WC: Multi-Robot Localization Uncertainty-aware Active NeRF with   Wireless Coordination](https://arxiv.org/abs/2403.13348)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Multi-robot 3D reconstruction faces challenges in inter-robot pose estimation, localization uncertainty quantification, and active best-next-view selection. 
- Traditional methods rely on appearance-based loop closure for aligning maps across robots, but this has complications. 
- Need methods for relative pose estimation, quantifying uncertainty, and intelligent view planning to improve reconstruction.

Proposed Solution - MULAN-WC:
- Leverages wireless signals (AOA, ranging) between robots for inter-robot pose estimation instead of just visual features.
- Develops method to quantify AOA uncertainty and incorporate into NeRF training loss to mitigate inaccurate poses. 
- Proposes active view selection approach that samples novel views and evaluates expected rendering variance reduction, considering pose uncertainties.
- Directs robots to capture views that maximize info gain for NeRF.

Main Contributions:
- Framework integrating wireless coordination into multi-robot localization uncertainty-aware NeRF
- Method for collaborative active image acquisition based on variance reduction 
- Extensive experiments on synthetic and real robot hardware demonstrating:
  - Achieves high quality 3D reconstruction close to ground truth poses
  - Uncertainty-aware training improves accuracy
  - Active view finding leads to consistent rendering quality improvement

In summary, the paper presents a full pipeline leveraging wireless signals, uncertainty quantification, and active view planning to enable accurate and efficient multi-robot 3D reconstruction with NeRF. Key innovations are around inter-robot coordination, uncertainty-based training supervision, and intelligent next best view direction to optimize scene coverage.
