# [Regret Minimization in Stackelberg Games with Side Information](https://arxiv.org/abs/2402.08576)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- The paper studies Stackelberg games, where a leader commits to a strategy first and then a follower chooses a best response. These games model many real-world security applications.  
- However, existing models do not capture the side information (context) that players often have, which can significantly affect strategies and payoffs. 
- The paper formalizes "Stackelberg games with side information", where on each round the leader and follower observe a context and then play. The context affects the utilities.

Proposed Solution:
- The paper first shows no-regret learning is impossible if both context and follower sequences are adversarial. 
- So the paper studies two relaxations: (1) stochastic follower, adversarial context, and (2) stochastic context, adversarial follower.

Main Results:
- With stochastic followers, a greedy algorithm gets O(sqrt(T)) regret by estimating follower type frequencies.  
- With stochastic contexts, hedge algorithm over policy class gets O(sqrt(T)) regret. Discretization error is carefully bounded.
- The paper further extends both algorithms to bandit (partial) feedback, getting O(T^(2/3)) regret. Barycentric spanners are used for variance reduction.

Key Contributions:
- Formalizes and motivates the study of Stackelberg games with side information.
- Provides impossibility result for fully adversarial setting. 
- Gives algorithms with regret guarantees for two natural relaxations, as well as bandit feedback extensions.
- The techniques and analyses open possibilities for other variants and applications of these contextual Stackelberg games.
