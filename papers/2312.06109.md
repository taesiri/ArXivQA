# [Vary: Scaling up the Vision Vocabulary for Large Vision-Language Models](https://arxiv.org/abs/2312.06109)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a high-quality paragraph summarizing the key points of this paper:

This paper proposes Vary, an effective and efficient method for scaling up the visual vocabulary of large vision-language models (LVLMs). Existing LVLMs rely on a fixed CLIP-based vision vocabulary which has limitations in handling some specialized visual tasks requiring dense perception, like document OCR and chart understanding. Vary addresses this through a two-step approach - first generating a new vision vocabulary tailored for such tasks via a vocabulary network and tiny transformer trained on synthetic document and chart image-text pairs, and then integrating this with the original CLIP vocabulary into the LVLM, with both vocabularies frozen during downstream tuning. Experiments demonstrate Vary's superior fine-grained visual perception on tasks like Chinese/English dense OCR and document conversion to LaTeX/markdown while also performing on par or better on downstream datasets like DocVQA, ChartQA and MMVet. The simple yet effective strategy of expanding visual vocabulary opens up avenues for strengthening LVLMs on a wider range of visual tasks.
