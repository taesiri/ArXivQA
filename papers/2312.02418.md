# [Decoding Data Quality via Synthetic Corruptions: Embedding-guided   Pruning of Code Data](https://arxiv.org/abs/2312.02418)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Code datasets collected from diverse sources like GitHub can contain quality issues that negatively impact the performance and training efficiency of large language models (LLMs) for code generation. 
- Existing embedding-based pruning methods mainly focus on removing duplicates or improving variety, not directly identifying "low-quality" code.

Proposed Solution: 
- Introduce a new pruning method called "synthetic corruption informed pruning (SCIP)" that uses insights from synthetically corrupted code to identify low-quality data.
- First, systematically corrupt code by introducing syntax errors (removing brackets, renaming variables) and content errors (altering conditionals, offsetting array indices).
- Observe that corrupted code tends to shift embeddings to smaller clusters farther from centroids compared to original code.  
- Use these insights to design pruning metrics based on cluster size and distance to centroids that can effectively remove low-quality entries.

Main Contributions:
- Demonstrate what constitutes "low-quality" code data by analyzing effects of synthetic corruptions on embeddings.
- Propose novel, accessible pruning metrics operating in embedding space to identify low-quality code informed by behavior of intentionally corrupted code.
- Show that pruning using these metrics improves performance on HumanEval and MBPP benchmarks compared to no pruning and other embedding-based methods.
- Establish promise of insights from synthetic corruptions to guide development of better data pruning techniques, not just for code but potentially for language data as well.
