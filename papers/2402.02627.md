# [Stability Analysis of Various Symbolic Rule Extraction Methods from   Recurrent Neural Network](https://arxiv.org/abs/2402.02627)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Recurrent neural networks (RNNs) like LSTM and GRU can effectively model complex temporal patterns but it is difficult to interpret what they have learned internally. 
- Recent works extract symbolic rules like DFAs from RNNs to understand their internal representations. But the stability of these rule extraction methods is not well studied.
- Instability in rule extraction leads to failure in explaining RNN's behavior.

Proposed Solution:
- The paper analyzes stability of two rule extraction methods - quantization-based and equivalence query-based ($L^*$) across RNNs like LSTM, GRU, O2RNN and MIRNN.
- Extensive experiments are done on 7 Tomita and 4 Dyck grammars, which are standard benchmarks for regular and context-free languages.
- 3600 RNNs are trained and 18000 DFAs extracted using quantization, along with 3600 DFAs by $L^*$, across 10 random seeds.

Key Results:
- Quantization methods extract more stable DFAs than $L^*$ in terms of number of states and accuracy.
- $L^*$ shows instability in number of states for complex grammars, producing >100 states DFA while ground truth has <10 states. 
- For partially trained RNNs below 100% accuracy, quantization outperforms $L^*$. $L^*$ often fails to extract valid DFAs.
- Among RNN cells, 2nd order O2RNN encapsulates most stable rules and its DFAs have highest accuracy over others.
- Even when GRU has higher accuracy on Dyck languages, O2RNN DFAs achieve better performance and stability.

Main Contributions:
- First extensive stability analysis of rule extraction methods from RNNs.
- Empirical validation that 2nd order RNNs (O2RNN) have more stable internal representations aligning with theory.
- Quantization methods extract significantly more stable DFAs than $L^*$, especially for partially trained RNNs.
- O2RNN coupled with quantization seems most promising direction for neuro-symbolic systems.

In summary, the paper clearly demonstrates the superiority of quantization-based rule extraction and O2RNN towards building stable and interpretable RNN models for modeling formal languages.
