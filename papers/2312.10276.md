# [Asymmetric Norms to Approximate the Minimum Action Distance](https://arxiv.org/abs/2312.10276)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem: 
The paper addresses the challenge of learning goal-conditioned policies in reward-free reinforcement learning. Specifically, it focuses on learning a representation of states such that the distance between state embeddings corresponds to the minimum number of actions needed to transition between states (Minimum Action Distance or MAD). Learning such a representation allows goal-conditioned planning by minimizing the distance between the current state and goal state in the embedding space. However, prior methods that use symmetric norms to embed states fail to capture asymmetry in transition dynamics, hurting planning performance.

Proposed Solution:
The key idea is to learn an asymmetric embedding of states using Wide Norms, which can model both symmetric and asymmetric distances. This allows accurately approximating the asymmetric MAD even in environments with inherent asymmetry. In addition, the paper proposes learning a transition model in the embedding space to enable planning by optimizing distances to goal states.  

Key Contributions:

- Formalizes the notion of Minimum Action Distance (MAD) which measures the minimum actions needed to transition between pairs of states

- Shows how to learn an approximation of MAD from state trajectories using a constrained optimization objective 

- Introduces the use of asymmetric Wide Norms to embed MAD, unlike prior work that relied on symmetric norms

- Proposes an approach to learn action conditioned transition models in the embedding space  

- Empirically demonstrates that asymmetric Wide Norm embeddings can accurately capture MAD in both symmetric and asymmetric MDPs, outperforming methods based on symmetric norms

- Provides useful state representations and transition models to enable goal conditioned policy learning in reward-free RL

In summary, the key innovation is an asymmetric state representation learning approach that can provably approximate minimum action distances even in asymmetric MDPs. This representation enables planning to satisfy goals specified as target states.
