# [FormulaQA: A Question Answering Dataset for Formula-Based Numerical   Reasoning](https://arxiv.org/abs/2402.12692)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Existing numerical reasoning datasets rely on implicit commonsense knowledge and don't require explicit formulas to guide reasoning. This makes them limited for evaluating complex real-world problems.
- Recently introduced formula-based datasets only have formulas for 33-38% of questions and use simple commonsense formulas.

Proposed Solution:
- Construct a new formula-based question answering dataset called FormulaQA from Chinese junior high school physics exams.
- Annotate each question with reasoning steps, formulas (structure, symbols, values, units), and final answer using both manual work and assistance from large language models (LLMs).
- Build a formula database by merging semantically similar formulas that can serve as an external knowledge base.
- Evaluate performance of LLMs ranging from 7B to over 100B parameters using zero-shot and few-shot chain of thoughts prompting.
- Also evaluate fine-tuned smaller models and retrieval-augmented LLMs.

Key Contributions:
- Introduce FormulaQA, a complex formula-driven numerical reasoning dataset requiring multi-step reasoning.
- Comprehensive annotations include reasoning chains and formula database.
- Establish strong baselines with state-of-the-art models, showing significant room for improvement.
- FormulaQA pushes progress on incorporating domain-specific formulas into numerical reasoning.

In summary, the paper presents FormulaQA, a new challenging dataset aiming to advance numerical reasoning research by requiring reasoning guided by formal domain-specific formulas instead of just commonsense knowledge. Thorough experiments underscore poor performance of current methods, motivating future efforts.


## Summarize the paper in one sentence.

 This paper introduces FormulaQA, a question answering dataset for formula-based numerical reasoning collected from Chinese junior high school physics exams, with comprehensive annotations including reasoning steps, formulas, final answers, and a formula database.


## What is the main contribution of this paper?

 Based on my reading of the paper, I would summarize the main contributions as:

1. The authors constructed a new question answering dataset called FormulaQA for formula-based numerical reasoning, consisting of 5,420 physics questions from Chinese junior high school exams. Unlike previous datasets, all questions in FormulaQA require formulas to solve and have annotated formulas and reasoning steps.

2. The authors created a formula database for FormulaQA by merging formulas with similar meanings, to serve as an external knowledge base for retrieval-based methods. 

3. The authors evaluated a range of models on FormulaQA, including large language models (LLMs) with up to over 100B parameters, fine-tuned smaller models, and retrieval-augmented LLMs. The results establish baseline performances and demonstrate room for improvement on formula-based reasoning.

In summary, the key contributions are the new FormulaQA dataset to spur research into formula-based reasoning, the formula database as an external knowledge source, and extensive experiments assessing the capabilities of current models, which point to challenges that remain in this complex numerical reasoning task.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and keywords associated with it are:

- FormulaQA - The name of the question answering dataset introduced in the paper for formula-based numerical reasoning.

- Numerical reasoning - The paper focuses on improving numerical reasoning capabilities of language models, which is a key aspect of natural language understanding. 

- Formulas - A central theme of the paper is the use of formulas to guide the numerical reasoning process, as formulas represent important domain knowledge.

- Annotation - The dataset was constructed through manual annotation of reasoning steps and formulas, along with assistance from large language models.

- Formula database - The authors compiled a database of formulas by merging equivalent formulas, which serves as an external knowledge base.

- Evaluation - Various sizes of language models, fine-tuned smaller models, and retrieval-augmented models were evaluated on the FormulaQA dataset to establish baselines.

- Generalization - The test set is divided into in-distribution and out-of-distribution sections to assess model generalization on unseen formulas.

- Error analysis - Analysis of common error types highlights challenges models face with formula application and numerical calculation.

In summary, key terms cover the proposed dataset, task formulation, dataset construction, model evaluation, and findings regarding strengths and weaknesses of existing approaches.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 potential in-depth questions about the method proposed in this paper:

1. The paper mentions using both manual annotation and assistance from large language models (LLMs) to annotate the reasoning steps and formulas for each question. Can you elaborate more on the interplay between the manual annotation and LLM assistance? What specific steps were done manually versus with LLM help? 

2. The process of normalizing and standardizing formulas is a key contribution for constructing the dataset. Can you walk through this process in more detail? What were some of the major challenges faced and how were they overcome?

3. The paper constructs a formula database by merging formulas with similar meanings. What were some of the approaches explored for judging semantic similarity between formulas? How was the LLM leverage to assist in this process?

4. The paper introduces an out-of-distribution (OOD) test set with formulas not seen during training. What motivated this decision and what insights were gained by having both an in-distribution and OOD test set?

5. For the experiments involving the formula retriever, how was the retriever trained? What data was used and what was the process to create training examples from the formula database?  

6. The error analysis revealed issues with both incorrect formulas and calculation errors. Can you expand more on the types and sources of these errors? Are there plans to address these in future work?

7. The best LLM models achieved approximately 65-70% accuracy. Why do you think performance is not yet higher and what are some areas to focus on for improvement?

8. How suitable do you believe this dataset and problem formulation is for evaluating reasoning versus mathematical/calculation capabilities? Could the framework be extended to better disentangle these factors?  

9. The paper focuses specifically on physics questions. Do you foresee value in constructing similar formula annotated datasets for other scientific domains and applications?

10. What other analysis would have been informative to do in evaluating model performance on this dataset? Are there other metrics, visualizations, or experiments you would suggest for future work?
