# [Sqrt(d) Dimension Dependence of Langevin Monte Carlo](https://arxiv.org/abs/2109.03839)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central research question is: What is the optimal dependence on dimension d and accuracy tolerance ε for the mixing time of Langevin Monte Carlo (LMC) algorithm to sample from log-smooth and log-strongly-convex distributions? The main contributions and findings are:- The authors prove an improved upper bound of Õ(sqrt(d)/ε) on the mixing time of LMC in 2-Wasserstein distance under standard assumptions on the target distribution (log-smoothness, log-strong-convexity) plus an additional mild condition. This improves upon previous best known bounds that had worse dependence on d and/or ε.- They show this Õ(sqrt(d)/ε) upper bound is tight (optimal) through an example distribution where LMC requires at least Ω(sqrt(d)/ε) iterations to converge. - The analysis framework they use is based on extending classical mean-square analysis of numerical SDEs to infinite time horizon and sampling error. Some refinements to previous work in this direction are made.- Overall, the paper establishes the optimal sqrt(d)/ε scaling of mixing time for LMC under those assumptions on the target distribution, resolving an open problem on the dependence on dimension. This was previously thought to require momentum as in kinetic Langevin but is shown here to hold for unadjusted LMC too.


## What is the main contribution of this paper?

Based on my reading, the main contributions of this paper are:1. It provides an improved mixing time bound of $\tilde{O}(\sqrt{d}/\epsilon)$ for the Langevin Monte Carlo (LMC) algorithm to sample from log-smooth and log-strongly-convex distributions. This improves upon previous bounds that had a worse dependence on the dimension $d$. 2. It shows this $\tilde{O}(\sqrt{d}/\epsilon)$ bound is tight (optimal) among the class of log-smooth and log-strongly-convex distributions. 3. It establishes the mixing time bound using a refined mean-square analysis framework that connects the integration error of discretizing a stochastic differential equation (SDE) to the sampling error of the resulting algorithm. This provides a general tool for analyzing sampling algorithms obtained from discretizing contractive SDEs.4. Compared to previous mean-square analysis, the refined version here allows for non-uniform bounds on the local errors, making it more widely applicable. It also carefully tracks constants' dependence on parameters like dimension to obtain the tightened mixing time bound.In summary, the main contribution is using a refined mean-square analysis technique to prove an optimal dimension dependence in the mixing time of LMC, improving upon previous results. The refined analysis framework is general and can likely be applied to other discretized SDE sampling algorithms too.
