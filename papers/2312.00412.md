# [SCHEME: Scalable Channer Mixer for Vision Transformers](https://arxiv.org/abs/2312.00412)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a high-quality paragraph summarizing the key points of the paper:

This paper proposes a new Scalable CHannEl MixEr (SCHEME) module to improve the channel mixer block in vision transformers (ViTs). The SCHEME module combines: (1) a Block Diagonal MLP (BD-MLP) that uses sparse feature mixing to enable larger expansion ratios and (2) a Channel Covariance Attention (CCA) branch that helps cluster features during training. Experiments show BD-MLP alone can achieve comparable accuracy to standard MLPs and CCA further improves class separability of learned features without increasing inference cost. CCA uses channel correlations to reweight features and the authors find its contribution decays over training once good feature clusters form. Thus, CCA can be removed at test time. Replacing MLP blocks in ViTs with the SCHEME module enables flexible scaling and improved accuracy-efficiency tradeoffs. The authors demonstrate this via a new family of SCHEMEformer models that outperform baselines, especially for smaller models. Gains generalize across classification, detection and segmentation. Overall, SCHEME enables sparse feature mixing to improve vision transformers with minimal overhead.
