# [The Queen of England is not England's Queen: On the Lack of Factual   Coherency in PLMs](https://arxiv.org/abs/2402.01453)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Pre-trained language models (PLMs) are known to contain factual knowledge captured during pre-training, which enriches their representations. 
- Prior work has focused on evaluating how much factual knowledge is contained in PLMs by measuring their ability to predict object entities given a subject and relation. 
- This paper argues that the internal coherence of factual knowledge in PLMs is an important complementary metric that has not been studied before. 

Proposed Solution:
- The authors introduce the notion of "coherency" to evaluate how often PLMs can predict the original subject entity when given the relation and the PLM's own (potentially incorrect) prediction for the object entity.
- This tests whether PLMs can inference bidirectional relationships, not just predict objects from subjects.
- Coherency is evaluated on the standard LAMA benchmark across various PLMs, prompt optimization methods, evidence paragraphs, etc.

Key Findings:
- PLMs show very poor internal coherency of factual knowledge. Their own predictions are often incoherent.
- Providing evidence paragraphs substantially improves coherency by giving context.
- Scaling model size and using entity-focused pre-training help improve coherency.
- Optimized prompts and paraphrasing do not help coherency, showing brittleness.

Main Contributions:
- Formalizes the novel metric of factual knowledge coherency in PLMs to study internal model consistency.
- Extensive experiments over factors like model scale, training objectives, prompts. 
- Key insight that existing PLMs lack coherent internal factual knowledge, limiting their use as knowledge bases.
- New direction for improving PLMs - enhancing internal coherence.

In summary, the paper identifies fundamental inconsistencies in how factual knowledge is stored in PLMs, and provides insights into potential areas of improvement.


## Summarize the paper in one sentence.

 Here is a one-sentence summary of the paper:

The paper introduces the concept of factual knowledge coherency in pre-trained language models, shows that current models have poor coherency, and provides evidence that including contextual paragraphs substantially improves coherency while optimized prompts do not.


## What is the main contribution of this paper?

 According to the paper, the main contribution is:

1) Introducing coherency to investigate the internal state of factual knowledge in pre-trained language models (PLMs). Coherency refers to the ability of a PLM to infer the subject entity given its initial prediction for the object entity and vice versa, based on a relation between them.

2) Evaluating different PLMs on the metric of coherency and showing that they generally have low coherency scores.

3) Demonstrating that optimized and paraphrased prompts do not substantially improve coherency, but using evidence paragraphs as additional context does lead to significant improvements.

4) Arguing that further architectural and data improvements are needed for PLMs to develop a more coherent internal knowledge state before they can be reliably used as knowledge bases.

In summary, the key contribution is introducing and evaluating coherency as an intrinsic way to analyze the factual knowledge contained within PLMs, beyond just assessing how much knowledge they have. The results highlight issues for using PLMs as knowledge bases currently.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and concepts associated with it are:

- Coherency - The main concept examined, referring to the ability of language models to make consistent predictions when queried from different directions. 

- Factual knowledge - The paper probes how coherent the factual knowledge encoded in pre-trained language models (PLMs) is.

- Pre-trained language models (PLMs) - Models like BERT, T5, GPT that are pre-trained on large corpora and fine-tuned for downstream tasks. The paper evaluates coherency in several PLMs.

- Knowledge bases - The potential for using PLMs as knowledge bases is analyzed in relation to their coherency.

- Prompts - The prompts used to query PLMs, including manually written, optimized, paraphrased prompts and prompts with evidence paragraphs. Their effect on coherency is tested.

- Relations - Different types of relations in the evaluation data (1-1, N-1, N-M) which have varying difficulties.

- Evaluation methodology - The process of evaluating coherency by querying PLMs from both directions using entities and relations is a key aspect.

Does this summary cover the major keywords and concepts associated with this paper? Let me know if you need any clarification or have additional questions!


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in this paper:

1. The paper introduces the concept of "coherency" to evaluate the internal state of factual knowledge in PLMs. How is coherency defined and how does it differ from simply evaluating the correctness of PLMs' factual knowledge? 

2. The authors evaluate coherency by first querying a PLM to predict an object given a subject and relation, then querying it again to predict the original subject given the predicted object and relation. What is the intuition behind this methodology? How does it assess the coherence of a PLM's internal knowledge?

3. The paper finds that PLMs have poor coherency scores, even when using optimized or paraphrased prompts. Why might this be the case? What deficiencies in PLMs does this highlight? 

4. Providing an evidence paragraph along with the prompt is found to substantially improve coherency. Why might this additional context help PLMs answer more coherently? What does this suggest about the nature of factual knowledge stored in PLMs?

5. The authors categorize relations in the evaluation dataset into 1-1, N-1, and N-M relations. Which type of relation poses the biggest challenge for PLMs in terms of coherency? Why?

6. The paper shows lower coherency scores when predicting the subject entity compared to the object entity. What factors might explain why it's more difficult for PLMs to coherently predict subject vs object entities?  

7. How suitable is the evaluation methodology in this paper for assessing the coherency of factual knowledge in autoregressive language models? What adjustments need to be made?

8. Could the coherency evaluation approach proposed in this paper be adapted to assess coherence of commonsense or procedural knowledge stored in LMs? What changes would be required?

9. The authors suggest architectural improvements to PLMs could help address the lack of coherency found. What specific architectural changes might help PLMs develop a more coherent internal knowledge state?

10. If PLMs had higher levels of factual coherency, to what extent could they viably serve as knowledge bases? What other capabilities would be required alongside coherency?
