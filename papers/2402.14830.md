# [Orca-Math: Unlocking the potential of SLMs in Grade School Math](https://arxiv.org/abs/2402.14830)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Mathematical word problem solving is challenging for small language models (SLMs). Recent work has hypothesized that model sizes of at least 34B parameters are needed to achieve over 80% accuracy on the GSM8K math word problem benchmark. 
- To reach high accuracy levels, current approaches often use ensembling (combining multiple model outputs), which significantly increases computational costs. Additionally, existing methods leverage very large training sets in the millions of problems.

Proposed Solution:  
- The authors propose an iterative learning approach combined with a high-quality synthetic dataset of only 200K problems to train an SLM to reach 86.81% on GSM8K without needing ensembling or other supplementary techniques.

- A multi-agent data generation method called Agent-Instruct is used to create diverse and challenging math problems. This leverages a seed set of existing problems and several agents that paraphrase, suggest modifications, and edit problems.

- The method trains an initial 7B parameter model (M1) using supervised finetuning on the 200K dataset. M1 is then used to generate additional solutions and explanations for each problem. These are labeled as positive or negative using GPT-4's solutions. 

- Two iterative preference learning algorithms are explored using the positive/negative pairs to further improve the SLM: Direct Preference Optimization (DPO) and Kahneman-Tversky Optimization (KTO).

Key Results:
- With supervised finetuning alone, the method achieves 81.5% on GSM8K. With additional iterative preference learning, accuracy is boosted to 86.81%, surpassing more complex state-of-the-art approaches.

- Analysis shows the value of using both model-generated positives and synthetically created negatives during iterative training.

- The approach reaches high accuracy with an order of magnitude less training data than prior work, demonstrating the promise of agent-based data generation and iterative learning for advancing SLMs.
