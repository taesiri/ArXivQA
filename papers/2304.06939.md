# Multimodal C4: An Open, Billion-scale Corpus of Images Interleaved with
  Text

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, it does not appear to have a clearly stated central research question or hypothesis. The paper introduces Multimodal C4 (MMC4), a new large-scale corpus of documents with images interleaved in the text. The key contributions seem to be:1) The creation and release of MMC4, a large public multimodal dataset for pretraining vision-language models. 2) Analysis of the dataset contents, including topic modeling, image relevance assessments, etc.3) A method for aligning images to sentences within documents using a bipartite linear assignment algorithm and CLIP image-text similarity. This is evaluated on existing benchmarks.4) Preliminary experiments showing that pretraining a model on MMC4 can improve few-shot in-context learning for image captioning compared to pretraining on image-caption pairs only.So in summary, the paper focuses on introducing a new resource and benchmark rather than testing a specific hypothesis. The evaluations are mainly centered around analyzing the dataset itself and showing it can be used to improve multimodal pretraining. There is no single central research question posed at the outset. The main contribution is the dataset creation and release.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contribution is introducing Multimodal C4 (MMC4), a large-scale public corpus for multimodal pretraining. MMC4 augments the text-only C4 corpus with over 570 million images interleaved in the text. The key aspects of MMC4 are:- It contains over 100 million documents with images and text interleaved, which is useful for training models that can handle prompts with both modalities. This format enables few-shot learning by interleaving independent supervised examples. - The images are aligned to the text using a bipartite linear assignment algorithm based on CLIP image-text similarity. This is shown to outperform prior methods on document-level image-text alignment benchmarks.- Analysis shows the corpus covers diverse everyday topics and the images are often relevant to the associated text, with good sentence-level alignment.- Subsets are provided with different levels of filtering, like removing images with detected faces. A smaller "core" subset is available for initial experimentation.- An open source model called OpenFlamingo is presented as an early application trained on MMC4 interleaved sequences. It shows improved few-shot adaptation to image captioning compared to training on just image-caption pairs.In summary, the main contribution is releasing this large-scale corpus of documents with interleaved images and text to support multimodal pretraining and prompting. The analysis helps validate the quality and utility of the corpus.
