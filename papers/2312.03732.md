# [A Rank Stabilization Scaling Factor for Fine-Tuning with LoRA](https://arxiv.org/abs/2312.03732)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Large language models (LLMs) require substantial compute resources and memory to fine-tune, limiting their widespread adoption and use. 
- Parameter-efficient fine-tuning (PEFT) methods like Low-Rank Adapters (LoRA) have been proposed to reduce training costs. LoRA adds low-rank "adapter" matrices to existing model weights.
- However, LoRA uses an aggressive rank-dependent scaling factor that causes gradient collapse and stalled learning with higher-rank adapters, limiting usefulness.

Proposed Solution:
- The paper analytically studies the impact of the LoRA scaling factor on the learning process.
- It proves the scaling factor should be the square root of the adapter rank for rank-stabilized learning. 
- The proposed rank-stabilized LoRA (rsLoRA) method uses this corrected scaling factor.

Contributions:
- Formal analysis revealing the appropriate scaling factor for stable LoRA learning.  
- Introduction of rsLoRA method that allows higher-rank adapters to improve fine-tuning performance.
- Experiments showing rsLoRA unlocks better performance with larger adapter ranks, while the LoRA scaling factor causes degraded learning.
- Demonstration that rsLoRA provides a flexible performance vs compute trade-off during fine-tuning by varying adapter rank.

In summary, the paper formally analyzes the LoRA scaling factor and finds it overly aggressive, causing degraded learning with higher rank adapters. The proposed rsLoRA method uses the corrected scaling factor to enable high-quality fine-tuning across a range of adapter ranks.
