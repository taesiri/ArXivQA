# [Discounted Adaptive Online Prediction](https://arxiv.org/abs/2402.02720)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
The paper studies the problem of online learning under nonstationarity. Specifically, it revisits the classical notion of "discounted regret", which allows an online learning algorithm to forget past mistakes and adapt to distribution shifts over time. Compared to popular nonstationary metrics like dynamic regret and strongly adaptive regret, discounted regret offers simplicity and computational benefits. 

The paper argues that prevailing algorithms for nonstationary environments (based on dynamic/adaptive regret) often target an excessively wide range of nonstationarity levels. This causes them to maintain conflicting beliefs simultaneously and degrade over time. In contrast, discounted algorithms only target one level of nonstationarity.

Proposed Solution: 
The paper proposes a new discounted online learning algorithm that adapts to both the loss sequence complexity and the comparator. This is achieved via:

1) A simple rescaling trick that converts scale-free undiscounted regret bounds to discounted analogues. This allows exploiting advances in stationary online learning for the nonstationary setting.  

2) Using a recent undiscounted algorithm (Zhang et al. 2023) as the base learner in the rescaling trick. This base algorithm is parameter-free and does not need an a priori bound on the gradients.

3) Modifying the base algorithm to enable rapid forgetting of past observations while retaining useful offline knowledge. Specifically, the algorithm employs exponential moving average-type updates.

The proposed discounted algorithm achieves an instance-dependent $\tilde{O}(\|u\|\sqrt{V})$ regret bound, where $V$ measures the discounted gradient variance. This matches a lower bound and strictly improves upon constant step-size gradient descent.

Additionally, the algorithm is applied to online conformal prediction. Leveraging the stability of FTRL updates, coverage guarantees are derived that adapt to the targeted miscoverage rate. Experiments demonstrate the practicality of the approach.

Main Contributions:

1) New discounted online learning algorithm that adapts to both the loss sequence and comparator, with strong robustness and consistency over time.

2) Demonstrating a simple rescaling trick to transfer undiscounted guarantees to the nonstationary discounted setting.

3) Establishing coverage guarantees for FTRL-based online conformal prediction. The analysis relies on the prediction stability.

4) Empirical evaluation showing the adaptive algorithms match or outperform constant step-size methods in online conformal prediction.

In summary, the paper provides an efficient and adaptive approach to tackle nonstationarity in sequential decision making problems. The techniques offer both theoretical and practical benefits over prevailing methods.
