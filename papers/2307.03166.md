# [VideoGLUE: Video General Understanding Evaluation of Foundation Models](https://arxiv.org/abs/2307.03166)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central research question this paper tries to address is: How do existing foundation models (FMs) perform on video understanding tasks compared to specialized video models? More specifically, the authors evaluate six representative FMs on three video tasks - action recognition, temporal localization, and spatiotemporal localization, using eight datasets and four adaptation methods. The key hypotheses they test are:1) Existing FMs still underperform specialized video models on these tasks, unlike the superiority FMs have shown on language and image tasks. This suggests the need for better video-focused FMs.2) Video-native FMs outperform image-native FMs on motion-rich videos and temporal/spatiotemporal reasoning, likely because of video data in pretraining.3) Light adaptation (e.g. frozen backbones) benefit video-native FMs more than full finetuning, while image-native FMs excel at full finetuning. This highlights the importance of both tasks and adaptation methods in FM evaluation.In summary, the central research question is assessing how current generalist FMs perform on specialized video understanding tasks compared to state-of-the-art specialized models. The key hypotheses examine if video-native pretraining and choice of adaptation impact FM performance on motion-rich video tasks.


## What is the main contribution of this paper?

The main contributions of this paper are:1. It proposes a new benchmark called VideoGLUE for evaluating foundation models (FMs) on video understanding tasks. VideoGLUE consists of 3 hallmark video tasks (action recognition, temporal localization, spatiotemporal localization), 8 datasets, and 4 adaptation methods to tailor FMs to the tasks. 2. It evaluates 6 major visual and multimodal FMs (CoCa, CLIP, FLAVA, VideoMAE, VATT, InternVideo) using VideoGLUE. The key findings are:- Task-specialized models still significantly outperform the FMs, showing the need for video-focused FMs. - Video-native FMs are generally better than image-native FMs at classifying motion-rich videos, temporally localizing actions, and understanding videos with multiple actions.- Video-native FMs perform well under light adaptation (e.g. frozen backbones), while image-native FMs excel at full end-to-end finetuning.3. It proposes a VideoGLUE score (VGS) to measure an FM's efficacy and efficiency at adapting to general video tasks.4. The results show the importance of considering both tasks and adaptation methods when evaluating FMs. The work promotes video understanding in FM evaluation.In summary, the main contribution is proposing the VideoGLUE benchmark and protocol for standardized evaluation of foundation models on diverse video understanding tasks. The results provide insights into current FMs and highlight opportunities for future video-focused FMs.
