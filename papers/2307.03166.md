# [VideoGLUE: Video General Understanding Evaluation of Foundation Models](https://arxiv.org/abs/2307.03166)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central research question this paper tries to address is: How do existing foundation models (FMs) perform on video understanding tasks compared to specialized video models? More specifically, the authors evaluate six representative FMs on three video tasks - action recognition, temporal localization, and spatiotemporal localization, using eight datasets and four adaptation methods. The key hypotheses they test are:1) Existing FMs still underperform specialized video models on these tasks, unlike the superiority FMs have shown on language and image tasks. This suggests the need for better video-focused FMs.2) Video-native FMs outperform image-native FMs on motion-rich videos and temporal/spatiotemporal reasoning, likely because of video data in pretraining.3) Light adaptation (e.g. frozen backbones) benefit video-native FMs more than full finetuning, while image-native FMs excel at full finetuning. This highlights the importance of both tasks and adaptation methods in FM evaluation.In summary, the central research question is assessing how current generalist FMs perform on specialized video understanding tasks compared to state-of-the-art specialized models. The key hypotheses examine if video-native pretraining and choice of adaptation impact FM performance on motion-rich video tasks.


## What is the main contribution of this paper?

The main contributions of this paper are:1. It proposes a new benchmark called VideoGLUE for evaluating foundation models (FMs) on video understanding tasks. VideoGLUE consists of 3 hallmark video tasks (action recognition, temporal localization, spatiotemporal localization), 8 datasets, and 4 adaptation methods to tailor FMs to the tasks. 2. It evaluates 6 major visual and multimodal FMs (CoCa, CLIP, FLAVA, VideoMAE, VATT, InternVideo) using VideoGLUE. The key findings are:- Task-specialized models still significantly outperform the FMs, showing the need for video-focused FMs. - Video-native FMs are generally better than image-native FMs at classifying motion-rich videos, temporally localizing actions, and understanding videos with multiple actions.- Video-native FMs perform well under light adaptation (e.g. frozen backbones), while image-native FMs excel at full end-to-end finetuning.3. It proposes a VideoGLUE score (VGS) to measure an FM's efficacy and efficiency at adapting to general video tasks.4. The results show the importance of considering both tasks and adaptation methods when evaluating FMs. The work promotes video understanding in FM evaluation.In summary, the main contribution is proposing the VideoGLUE benchmark and protocol for standardized evaluation of foundation models on diverse video understanding tasks. The results provide insights into current FMs and highlight opportunities for future video-focused FMs.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper proposes VideoGLUE, a benchmark for evaluating foundation models on video understanding tasks using a set of datasets and adaptation methods, finding that task-specialized models outperform current foundation models on video and that both tasks and adaptation methods impact performance.


## How does this paper compare to other research in the same field?

Here are a few key ways this paper compares to other related research:- It focuses specifically on evaluating foundation models (FMs) for video understanding. Many prior works have evaluated FMs for language or images, but this paper uniquely targets video tasks.- It proposes a comprehensive benchmark called VideoGLUE for evaluating video FMs. This includes diverse tasks, datasets, and adaptation methods tailored for probing video capabilities. Other benchmarks like Elevator and Perception Tests are not focused on video. - It studies a broad set of 6 image-based and video-based FMs from different research groups. Most prior video FM papers promote their own new model, but this work aims to impartially evaluate and compare existing FMs.- It examines both task design and adaptation methods as critical to FM evaluation. Many papers use inconsistent/limited tasks and adaptations, but this work argues both facets are key to thoroughly probing FMs.- It finds video-specialized models still significantly outperform current FMs on video tasks, unlike the FM dominance in language/images. This highlights the need for future video-focused FM research.- It reveals insights like video-native FMs outperforming image-native ones on motion/temporal reasoning tasks. And that FM rankings can flip under different adaptations.Overall, this paper provides one of the most extensive and methodical evaluations of existing foundation models for video understanding. The VideoGLUE benchmark and analysis offer a solid foundation to track progress and guide future research in this space.
