# [VideoGLUE: Video General Understanding Evaluation of Foundation Models](https://arxiv.org/abs/2307.03166)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central research question this paper tries to address is: How do existing foundation models (FMs) perform on video understanding tasks compared to specialized video models? More specifically, the authors evaluate six representative FMs on three video tasks - action recognition, temporal localization, and spatiotemporal localization, using eight datasets and four adaptation methods. The key hypotheses they test are:1) Existing FMs still underperform specialized video models on these tasks, unlike the superiority FMs have shown on language and image tasks. This suggests the need for better video-focused FMs.2) Video-native FMs outperform image-native FMs on motion-rich videos and temporal/spatiotemporal reasoning, likely because of video data in pretraining.3) Light adaptation (e.g. frozen backbones) benefit video-native FMs more than full finetuning, while image-native FMs excel at full finetuning. This highlights the importance of both tasks and adaptation methods in FM evaluation.In summary, the central research question is assessing how current generalist FMs perform on specialized video understanding tasks compared to state-of-the-art specialized models. The key hypotheses examine if video-native pretraining and choice of adaptation impact FM performance on motion-rich video tasks.


## What is the main contribution of this paper?

The main contributions of this paper are:1. It proposes a new benchmark called VideoGLUE for evaluating foundation models (FMs) on video understanding tasks. VideoGLUE consists of 3 hallmark video tasks (action recognition, temporal localization, spatiotemporal localization), 8 datasets, and 4 adaptation methods to tailor FMs to the tasks. 2. It evaluates 6 major visual and multimodal FMs (CoCa, CLIP, FLAVA, VideoMAE, VATT, InternVideo) using VideoGLUE. The key findings are:- Task-specialized models still significantly outperform the FMs, showing the need for video-focused FMs. - Video-native FMs are generally better than image-native FMs at classifying motion-rich videos, temporally localizing actions, and understanding videos with multiple actions.- Video-native FMs perform well under light adaptation (e.g. frozen backbones), while image-native FMs excel at full end-to-end finetuning.3. It proposes a VideoGLUE score (VGS) to measure an FM's efficacy and efficiency at adapting to general video tasks.4. The results show the importance of considering both tasks and adaptation methods when evaluating FMs. The work promotes video understanding in FM evaluation.In summary, the main contribution is proposing the VideoGLUE benchmark and protocol for standardized evaluation of foundation models on diverse video understanding tasks. The results provide insights into current FMs and highlight opportunities for future video-focused FMs.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper proposes VideoGLUE, a benchmark for evaluating foundation models on video understanding tasks using a set of datasets and adaptation methods, finding that task-specialized models outperform current foundation models on video and that both tasks and adaptation methods impact performance.


## How does this paper compare to other research in the same field?

Here are a few key ways this paper compares to other related research:- It focuses specifically on evaluating foundation models (FMs) for video understanding. Many prior works have evaluated FMs for language or images, but this paper uniquely targets video tasks.- It proposes a comprehensive benchmark called VideoGLUE for evaluating video FMs. This includes diverse tasks, datasets, and adaptation methods tailored for probing video capabilities. Other benchmarks like Elevator and Perception Tests are not focused on video. - It studies a broad set of 6 image-based and video-based FMs from different research groups. Most prior video FM papers promote their own new model, but this work aims to impartially evaluate and compare existing FMs.- It examines both task design and adaptation methods as critical to FM evaluation. Many papers use inconsistent/limited tasks and adaptations, but this work argues both facets are key to thoroughly probing FMs.- It finds video-specialized models still significantly outperform current FMs on video tasks, unlike the FM dominance in language/images. This highlights the need for future video-focused FM research.- It reveals insights like video-native FMs outperforming image-native ones on motion/temporal reasoning tasks. And that FM rankings can flip under different adaptations.Overall, this paper provides one of the most extensive and methodical evaluations of existing foundation models for video understanding. The VideoGLUE benchmark and analysis offer a solid foundation to track progress and guide future research in this space.


## What future research directions do the authors suggest?

Based on the results and analyses in the paper, the authors suggest the following future research directions:- Develop powerful video-focused foundation models (FMs). The experiments show that existing FMs, even the video-native ones, still significantly underperform specialized models on video tasks. This reveals the need and great opportunities for dedicated research on video-focused FMs.- Pay more attention to model adaptations in FM evaluation. The paper finds that both tasks and adaptation methods profoundly impact FMs' measured capabilities on downstream tasks. Hence, the authors suggest the community reach a consensus on standard adaptation protocols and include them as an inherent part of FM evaluations. - Improve video-native FM pretraining. While video-native FMs are generally better on motion-related tasks than image-native FMs, they underperform image-native FMs in full finetuning and adapter-based tuning. This indicates the need to improve video pretraining data diversity and model capacity.- Study when and why different adaptations work. The results show image-native FMs favor full finetuning while video-native FMs prefer lightweight tuning on frozen features/backbones. Analyzing the causes behind such phenomenon could inform the design of versatile, efficient FMs.- Explore efficient FM tuning methods. The proposed VideoGLUE metric considers both efficacy and efficiency of adaptations. Continued research on efficient tuning methods like adapter and prompt-tuning is encouraged.- Expand VideoGLUE's coverage. While VideoGLUE focuses on three core video tasks, expanding it to cover a wider range of video-centric tasks could provide a more comprehensive view of FMs' video understanding abilities.


## Summarize the paper in one paragraph.

Here is a one paragraph summary of the key points in the paper:The paper proposes VideoGLUE, a benchmark for evaluating foundation models (FMs) on video understanding tasks. It consists of three core tasks - action recognition, temporal localization, and spatiotemporal localization - across eight diverse datasets, using four different adaptation methods to tailor the FMs to each task. The key findings are: 1) Specialized models still significantly outperform current FMs on these video tasks, unlike in language and images, revealing opportunities for video-focused FMs. 2) Video-native FMs generally surpass image-native FMs on motion-rich videos, temporal localization, and multi-action videos. 3) Video-native FMs excel under light adaptation of frozen backbones, while image-native FMs win at full end-to-end finetuning. Overall, both tasks and adaptation methods impact FM evaluation, and video-focused FMs have ample room for progress on temporal/motion reasoning. The work promotes video understanding in FM evaluation through its video-centric tasks and modular adaptation protocols.


## Summarize the paper in two paragraphs.

Here is a two paragraph summary of the paper:The paper proposes VideoGLUE, a benchmark for evaluating foundation models (FMs) on video understanding tasks. VideoGLUE consists of three main components: (1) Three hallmark video understanding tasks: action recognition, temporal action localization, and spatiotemporal action localization. (2) Eight popular video datasets that cover a diverse range of properties such as data source, action complexity, etc. (3) Four adaptation methods to tailor the FMs to the downstream tasks: end-to-end finetuning, frozen backbone evaluation, multi-layer attention pooling over frozen features, and low-rank adapters. The authors evaluate six visual and multimodal FMs using VideoGLUE: CoCa, CLIP, FLAVA, VideoMAE, VATT, and InternVideo. The main findings are: (1) Task-specialized models still significantly outperform current FMs on video tasks, indicating the need for video-focused FMs. (2) Video-native FMs generally perform better on motion-rich videos and temporal reasoning tasks compared to image-native FMs. (3) Video-native FMs excel with light adaptation methods like frozen backbones, while image-native FMs are better with full finetuning. This highlights the importance of considering both tasks and adaptation methods when evaluating FMs. The work proposes VideoGLUE as a benchmark to systematically evaluate and analyze video capabilities of existing and future FMs.


## Summarize the main method used in the paper in one paragraph.

Here is a one paragraph summary of the main method used in the paper:The paper proposes a video general understanding evaluation (VideoGLUE) benchmark for evaluating foundation models' (FMs) capabilities on video understanding tasks. The benchmark consists of three main components: (1) Eight video datasets spanning three tasks - action recognition, temporal localization, and spatiotemporal localization. The datasets are selected to examine different aspects of video understanding such as motion, temporal reasoning, etc. (2) Four adaptation methods to tailor the FMs to the downstream tasks - end-to-end finetuning, frozen backbone evaluation, frozen features with multi-layer attention pooling, and low-rank adapter. This allows probing the FMs' capabilities under different settings. (3) A scalar VideoGLUE score (VGS) to measure an FM's efficacy and efficiency by normalizing its average score on the tasks by the trainable FLOPs under each adaptation method. Using this benchmark, six major visual and multimodal FMs are evaluated. The key findings are - task specialized models outperform FMs on these datasets, showing gaps for video-focused FMs; video-native FMs are better on motion and temporal reasoning tasks compared to image-native FMs; ranking of FMs varies based on adaptation method, confirming both tasks and adaptations matter for FM evaluation.
