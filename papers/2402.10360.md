# [Learnability is a Compact Property](https://arxiv.org/abs/2402.10360)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Recent work has shown that the learnability of some machine learning problems can be undecidable or independent of the standard axioms of set theory. For example, a hypothesis class can have simple, easily learnable finite projections, yet the learnability of the entire infinite class can depend on abstract set-theoretic properties. 

- However, learning theory also contains many dimensionality notions (like VC dimension) that characterize learnability through finite projections of a problem. These seem to contradict the possibility of undecidable learnability. 

- So which classes of learning problems can have their learnability and sample complexity detected by examining finite projections? And when can logical undecidability occur?

Proposed Solution:
- The paper shows that for supervised learning with metric losses, the sample complexity is a compact property - it can be detected by examining finite restrictions.

- For realizable and agnostic learning with proper losses (like norms on R^n), a class is learnable with sample complexity m if and only if all its finite projections are learnable with complexity m.  

- For realizable learning with improper losses, compactness can fail, but sample complexity gaps between a class and its projections are bounded between 1 and 2.

- The core technical contribution is a generalization of Hall's marriage theorem to assignments of variables maintaining functions below a threshold. This implies the compactness of sample complexity.

Main Contributions:
- First compactness result for sample complexity of supervised learning problems with metric losses

- Shows that undecidability requires going beyond proper losses to settings like EMX learning

- Provides matching upper and lower bounds on how much sample complexity can differ between a full class and its projections

- Generalizes Hall's theorem to capture supervised learning in a novel way that may be of independent mathematical interest

In summary, the paper delineates when logical obstacles like undecidability can and cannot occur in supervised learning, and shows that for proper losses, sample complexity admits a tight, finite characterization. The compactness result for sample complexity is the main breakthrough enabling this understanding.
