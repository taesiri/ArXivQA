# [Watch or Listen: Robust Audio-Visual Speech Recognition with Visual   Corruption Modeling and Reliability Scoring](https://arxiv.org/abs/2303.08536)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading, the central research question this paper addresses is: How can we develop a robust audio-visual speech recognition (AVSR) system that performs well even when both the audio and visual inputs are corrupted? 

The key points are:

- Previous AVSR systems assume clean visual inputs and mainly handle corrupted audio inputs. But in real situations, visual inputs can also be corrupted (e.g. occluded faces, blurry/noisy video).

- The authors analyze that current AVSR systems actually perform worse than audio-only ASR systems under joint audio-visual corruption. 

- They propose two main contributions to handle this:
  - Audio-visual corruption modeling during training to make the model robust.
  - A novel Audio-Visual Reliability Scoring (AV-RelScore) module that scores the reliability of each modality's features and emphasizes the more reliable ones.

- Experiments show the proposed method outperforms previous AVSR systems under various audio-visual corruption scenarios, demonstrating its robustness.

In summary, the central hypothesis is that by explicitly modeling audio-visual corruption and scoring reliability during training/inference, they can develop an AVSR system that is robust to real-world audio-visual input degradations.
