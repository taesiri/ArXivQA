# [Watch or Listen: Robust Audio-Visual Speech Recognition with Visual   Corruption Modeling and Reliability Scoring](https://arxiv.org/abs/2303.08536)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading, the central research question this paper addresses is: How can we develop a robust audio-visual speech recognition (AVSR) system that performs well even when both the audio and visual inputs are corrupted? 

The key points are:

- Previous AVSR systems assume clean visual inputs and mainly handle corrupted audio inputs. But in real situations, visual inputs can also be corrupted (e.g. occluded faces, blurry/noisy video).

- The authors analyze that current AVSR systems actually perform worse than audio-only ASR systems under joint audio-visual corruption. 

- They propose two main contributions to handle this:
  - Audio-visual corruption modeling during training to make the model robust.
  - A novel Audio-Visual Reliability Scoring (AV-RelScore) module that scores the reliability of each modality's features and emphasizes the more reliable ones.

- Experiments show the proposed method outperforms previous AVSR systems under various audio-visual corruption scenarios, demonstrating its robustness.

In summary, the central hypothesis is that by explicitly modeling audio-visual corruption and scoring reliability during training/inference, they can develop an AVSR system that is robust to real-world audio-visual input degradations.


## What is the main contribution of this paper?

 The main contribution of this paper is proposing a novel audio-visual speech recognition (AVSR) framework that is robust to corrupted multimodal inputs, including both audio and visual corruption. Specifically, the key contributions are:

- Analyzing the robustness of previous AVSR models under different types of input corruption, including audio-only, visual-only, and audio-visual corruption. The analysis shows that previous AVSR models are not robust to multimodal corruption.

- Proposing audio-visual corruption modeling with occlusion patches and noises to simulate real-world corruption during training. This is shown to be important for developing robust AVSR models. 

- Proposing a new AVSR framework called Audio-Visual Reliability Scoring (AV-RelScore) that can evaluate the reliability of each modality's features and emphasize more reliable representations. This allows focusing on less corrupted modalities.

- Conducting comprehensive experiments on LRS2 and LRS3 datasets validating the effectiveness of the proposed audio-visual corruption modeling and AV-RelScore. The method achieves state-of-the-art performance.

In summary, the key contribution is developing a novel AVSR framework that is robust to simultaneous corruption in both audio and visual modalities, through corruption modeling and reliability scoring. This advances AVSR performance in noisy real-world conditions.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the key points from the paper:

The paper proposes audio-visual corruption modeling and an Audio-Visual Reliability Scoring module for robust audio-visual speech recognition that can determine which input modality stream is more reliable when both audio and visual inputs are corrupted.


## How does this paper compare to other research in the same field?

 Here are a few thoughts on how this paper compares to other research in audio-visual speech recognition:

- This paper focuses on the problem of robustness to corrupted audio and visual inputs, which has been less explored compared to handling audio-only noise in prior AVSR work. The analysis of different AVSR models under audio, visual, and audio-visual corruption provides new insights into their limitations. 

- The proposed audio-visual corruption modeling through occlusions and noise seems novel as a way to make AVSR systems more robust. Most prior work has focused only on audio noise modeling. Explicitly modeling visual noise/occlusions is an important contribution.

- The proposed Audio-Visual Reliability Scoring module for weighting reliable vs corrupted modalities is also a new technique not seen in other AVSR systems. This allows the model to dynamically rely more on clearer modalities.

- The comprehensive experiments on LRS2 and LRS3 benchmark datasets demonstrate clear improvements in accuracy over prior AVSR models under corrupted conditions. This verifies the benefits of the proposed techniques.

- More broadly, this work pushes AVSR robustness to multimodal noise/corruption closer to real-world conditions. The techniques could likely transfer to other multimodal tasks as well. The ideas align well with the growing interest in robustness for deep learning systems.

In summary, the analysis of AVSR model limitations, the proposed audio-visual corruption modeling, and the novel AV-RelScore module for reliability scoring seem like unique contributions that advance the state-of-the-art in making AVSR systems more robust to real-world multimodal noise and occlusions. The rigorous experiments confirm these benefits.


## What future research directions do the authors suggest?

 Based on my reading of the paper, some of the main future research directions suggested by the authors are:

- Developing more robust AVSR models that can handle even more challenging multimodal corruption situations, beyond the audio-visual corruption cases explored in this work. The authors suggest exploring other types of visual and audio corruptions.

- Exploring different architectures for the reliability scoring module, beyond the temporal convolution model used in this work, to see if other designs can further improve the robustness.

- Applying the ideas of multimodal corruption modeling and reliability scoring to other multimodal tasks beyond AVSR, such as audio-visual action recognition. The authors suggest the ideas could be broadly useful.

- Collecting and generating more diverse multimodal datasets with real-world corruption, to enable more robust training and evaluation.

- Considering longer input sequences, as the experiments in this work were limited to sentences of up to 600 frames. Evaluating on longer sequences could reveal new challenges.

- Exploring the use of other modalities beyond audio and visual, such as text, to provide additional complementary information in cases where both audio and visual are highly corrupted.

- Developing online adaptation techniques to adjust the model during inference as the corruption conditions change dynamically in real-world applications.

In summary, the key theme in the suggested future work is continuing to enhance the robustness of multimodal systems like AVSR in even more challenging real-world conditions with complex corruption across modalities.


## Summarize the paper in one paragraph.

 The paper presents an audio-visual speech recognition method that is robust to corruption in both the audio and visual modalities. The key ideas are:

1) They analyze existing AVSR models and find they perform worse than audio-only models when both audio and visual inputs are corrupted. 

2) They propose to model corruption in both audio and visual streams during training. For visual, they use occlusion patches and noise to simulate real degradation. 

3) They propose an Audio-Visual Reliability Scoring module that scores the reliability of each modality's features at each timestep. This allows emphasizing the more reliable modality.

4) The emphasized multi-modal features are fused using a conformer encoder-decoder model. 

5) Comprehensive experiments on LRS2 and LRS3 datasets demonstrate the effectiveness of modeling audio-visual corruption and the proposed AV-RelScore module in improving robustness to both modalities being corrupted, outperforming audio-only and visual-only models.
