# [Unveiling Vulnerability of Self-Attention](https://arxiv.org/abs/2402.16470)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Pre-trained language models (PLMs) like BERT are vulnerable to minor input perturbations, posing challenges for deploying robust NLP systems. 
- Prior work has focused on manipulating input words to generate adversarial samples, but is limited in generalization. 
- This paper examines the vulnerability of the self-attention (SA) mechanism inside PLMs as it plays a fundamental role.

Methodology:
- Propose "HackAttend", a novel perturbation strategy to attack the SA matrices by perturbing attention scores using meticulously crafted attention masks. 
- Identify important layers, heads and SA units to perturb using gradient-based ranking and greedily mask top units.
- Constraints are imposed to ensure minimal perturbations while exploring search space effectively.

Results:
- Empirical evaluation shows state-of-the-art PLMs are heavily vulnerable to HackAttend, achieving 98% attack success rate with only 1% SA perturbations.
- HackAttend is very effective in complex linguistic tasks like reading comprehension while less so in simple tasks like sentiment analysis. 
- Analysis provides insights into model's reliance on SA, with higher layers more sensitive.

Key Contributions:
- First work to systematically discuss perturbations and defense methods targeting SA units. 
- Proposed HackAttend perturbs SA matrices to reveal vulnerabilities of PLMs.
- Introduced S-Attend, an efficient smoothing technique to train robust models via SA perturbations. 
- Empirical analysis offers insights into role of SA layers in capturing signals.

In summary, this paper made significant contributions by proposing novel attack and defense methods involving perturbations of the SA mechanism in PLMs, evaluated their effectiveness extensively, and provided valuable analysis.
