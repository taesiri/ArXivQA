# [Improving Fairness via Federated Learning](https://arxiv.org/abs/2110.15545v3)

## What is the central research question or hypothesis that this paper addresses?

 The central research questions this paper addresses are:

1. Is federated learning necessary for fair learning on decentralized data, i.e. can we simply train locally fair classifiers and aggregate them to achieve good performance? 

2. How does the performance of existing federated learning algorithms for fair learning compare to training on centralized data? Can we design an algorithm that matches or exceeds this centralized performance?

The key hypotheses tested in this work are:

1. Federated learning can strictly improve fairness over non-federated approaches like training local fair classifiers and ensembling (LFT+Ensemble). This demonstrates the necessity of federated learning.

2. Existing algorithms like LFT+FedAvg achieve worse accuracy-fairness tradeoffs than centralized training. The authors hypothesize this is due to limitations of the FedAvg aggregation scheme.

3. A novel federated learning algorithm called FedFB that is designed to mimic centralized training can match the performance of centralized fair learning on decentralized data.

The experiments and analysis aim to validate these hypotheses, demonstrating the limitations of non-federated and basic federated approaches, and showing the benefits of the proposed FedFB algorithm in approaching centralized performance on fair decentralized learning tasks.
