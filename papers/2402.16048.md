# [LLMs with Chain-of-Thought Are Non-Causal Reasoners](https://arxiv.org/abs/2402.16048)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Chain-of-thought (CoT) prompting is used in large language models (LLMs) to elicit step-by-step reasoning and improve performance, but empirically there is a mismatch between CoT correctness and answer correctness. 

- The paper finds that across models and tasks, there is a surprising frequency of correct answers despite incorrect CoTs, and vice versa. This indicates potential issues with reasoning in LLMs.

Methodology:
- The paper frames CoT reasoning in LLMs using causal analysis, modeling it as a structural causal model (SCM) with three key variables: Instruction, CoT, and Answer.  

- Four types of SCMs are proposed to characterize how CoT and answers are connected in LLMs: Causal Chain (ideal), Full Connection (default LLM), Common Cause (disconnected), and Isolated Answer (extreme disconnection).

- Interventions are used to uncover the implied SCM in LLMs across tasks, revealing the strength of causal connections between variables. The inferred SCM is compared to human reasoning.

Key Findings:
- LLMs show varying SCMs across different tasks - larger models tend to have more optimal SCM types, but this is not guaranteed. Suboptimal SCMs likely lead to reasoning errors.  

- Factors like in-context learning, supervised fine-tuning and reinforcement learning impact the causal links, especially between Instruction and Answer. This highlights issues in common LLM techniques.

Implications:
- There are noticeable discrepancies between LLM and human reasoning processes in terms of causal structures. The analysis provides a framework to anticipate potential LLM mistakes.

- More research is needed to develop faithful reasoning in LLMs. Causal analysis marks a preliminary step towards reliable and transparent LLMs.
