# [Meta-Personalizing Vision-Language Models to Find Named Instances in   Video](https://arxiv.org/abs/2306.10169)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is: How can we personalize vision-language models to enable language-guided search for specific named object instances in videos, even when trained from only a few examples per instance?  The key points are:- The paper aims to enable search for user-specific instances like "my dog Biscuit" in videos through natural language queries. This is challenging for current VLMs which can only handle generic categories. - The paper proposes a method to "meta-personalize" a VLM to learn representations of named instances from few examples by modeling instance tokens as combinations of shared category-specific features.- They also introduce an automatic mining procedure to collect training data from narrated videos without manual annotation.- The meta-personalization on a large automatically mined dataset improves generalization when adapting the model to new personal instances.- They demonstrate their approach on a new challenging personal video instance retrieval benchmark they collected called "This-is-My", where it outperforms baselines.In summary, the key research question is how to effectively personalize a VLM for few-shot instance search in videos by meta-learning an instance representation model from narrated video data.


## What is the main contribution of this paper?

The main contributions of this paper are:1. A method to automatically identify and mine moments depicting named personal instances in video by searching for possessive patterns in transcripts and using vision-language relevance. This allows collecting training data without needing manual annotations.2. A model that represents personal instance tokens as a linear combination of shared category-specific features. This improves generalization from few examples by preventing the features from capturing nuisance information like background. The category features are pre-trained through a meta-personalization procedure on a large corpus of automatically mined instances.3. Introduction of a new benchmark dataset called \datasetname for evaluating personalized video instance retrieval. The dataset contains subsets for meta-personalization, test-time personalization, and query-time evaluation.4. Experimental results demonstrating that the proposed approach outperforms baselines and prior work for personalized retrieval of named instances on the \datasetname and DeepFashion2 datasets.In summary, the main contribution is a novel meta-personalization approach to learn personalized representations of named visual instances from few examples by leveraging automatically mined training data. This enables retrieving the learned personal instances through natural language queries.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a 1-sentence TL;DR of the paper:The paper proposes a method to meta-personalize vision-language models to represent and retrieve user-specific visual instances through natural language queries, without requiring manual annotation to collect training data.


## How does this paper compare to other research in the same field?

Here are a few key points comparing this paper to other related work in personalized vision-language models:- The paper introduces a new method for "meta-personalizing" vision-language models (VLMs) to search video for user-specific objects/people without requiring manual supervision. This is a novel contribution compared to prior work like PALAVRA that relies on manually annotated training data.- The proposed approach represents personal instance tokens as a combination of shared category-level features and instance-specific weights. This is different from PALAVRA which models instances independently. The authors show this improves generalization and few-shot learning.- The paper collects a new benchmark dataset called "This-Is-My" for personal video instance retrieval. This is a more challenging dataset than existing ones like YT-VOS since queries and retrievals are from different videos showing the instance in different contexts.- For training, the paper proposes a contrastive learning loss rather than requiring additional networks like PALAVRA. The training leverages auto-mined data unlike prior works needing manual supervision. - The proposed "meta-personalization" pre-trains the model on a large corpus of auto-mined instance examples before adapting to user data. This differs from prior test-time tuning methods that adapt models independently per instance.- Experiments show the approach outperforms baselines and prior methods like PALAVRA on the new "This-Is-My" dataset and DeepFashion retrieval. The gains are especially significant for few-shot personalization.Overall, the key novelties are the meta-personalization training strategy, automated data collection, and improvements for few-shot instance learning compared to prior arts like PALAVRA. The new challenging dataset for personalized video retrieval is also an important contribution.
