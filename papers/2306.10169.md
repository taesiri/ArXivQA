# [Meta-Personalizing Vision-Language Models to Find Named Instances in   Video](https://arxiv.org/abs/2306.10169)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is: How can we personalize vision-language models to enable language-guided search for specific named object instances in videos, even when trained from only a few examples per instance?  The key points are:- The paper aims to enable search for user-specific instances like "my dog Biscuit" in videos through natural language queries. This is challenging for current VLMs which can only handle generic categories. - The paper proposes a method to "meta-personalize" a VLM to learn representations of named instances from few examples by modeling instance tokens as combinations of shared category-specific features.- They also introduce an automatic mining procedure to collect training data from narrated videos without manual annotation.- The meta-personalization on a large automatically mined dataset improves generalization when adapting the model to new personal instances.- They demonstrate their approach on a new challenging personal video instance retrieval benchmark they collected called "This-is-My", where it outperforms baselines.In summary, the key research question is how to effectively personalize a VLM for few-shot instance search in videos by meta-learning an instance representation model from narrated video data.


## What is the main contribution of this paper?

The main contributions of this paper are:1. A method to automatically identify and mine moments depicting named personal instances in video by searching for possessive patterns in transcripts and using vision-language relevance. This allows collecting training data without needing manual annotations.2. A model that represents personal instance tokens as a linear combination of shared category-specific features. This improves generalization from few examples by preventing the features from capturing nuisance information like background. The category features are pre-trained through a meta-personalization procedure on a large corpus of automatically mined instances.3. Introduction of a new benchmark dataset called \datasetname for evaluating personalized video instance retrieval. The dataset contains subsets for meta-personalization, test-time personalization, and query-time evaluation.4. Experimental results demonstrating that the proposed approach outperforms baselines and prior work for personalized retrieval of named instances on the \datasetname and DeepFashion2 datasets.In summary, the main contribution is a novel meta-personalization approach to learn personalized representations of named visual instances from few examples by leveraging automatically mined training data. This enables retrieving the learned personal instances through natural language queries.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a 1-sentence TL;DR of the paper:The paper proposes a method to meta-personalize vision-language models to represent and retrieve user-specific visual instances through natural language queries, without requiring manual annotation to collect training data.
