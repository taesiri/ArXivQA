# [Meta-Personalizing Vision-Language Models to Find Named Instances in   Video](https://arxiv.org/abs/2306.10169)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is: How can we personalize vision-language models to enable language-guided search for specific named object instances in videos, even when trained from only a few examples per instance?  The key points are:- The paper aims to enable search for user-specific instances like "my dog Biscuit" in videos through natural language queries. This is challenging for current VLMs which can only handle generic categories. - The paper proposes a method to "meta-personalize" a VLM to learn representations of named instances from few examples by modeling instance tokens as combinations of shared category-specific features.- They also introduce an automatic mining procedure to collect training data from narrated videos without manual annotation.- The meta-personalization on a large automatically mined dataset improves generalization when adapting the model to new personal instances.- They demonstrate their approach on a new challenging personal video instance retrieval benchmark they collected called "This-is-My", where it outperforms baselines.In summary, the key research question is how to effectively personalize a VLM for few-shot instance search in videos by meta-learning an instance representation model from narrated video data.
