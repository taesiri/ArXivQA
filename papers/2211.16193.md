# [In-Hand 3D Object Scanning from an RGB Sequence](https://arxiv.org/abs/2211.16193)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central research question this paper aims to address is how to reconstruct the 3D shape and appearance of an unknown object from an RGB video sequence captured while the object is being manipulated by hands. Specifically, the paper focuses on the challenging problem of simultaneous 3D reconstruction and pose estimation, where estimating both the shape of the object and the camera pose trajectory is difficult without proper initialization. The key hypotheses/claims of the paper are:- Standard Structure-from-Motion methods like COLMAP fail on in-hand scanning of objects due to lack of distinct textures and features in all regions of the object. They can only provide reliable camera poses for highly textured objects.- Simultaneous 3D reconstruction and pose estimation by neural rendering methods like NeRF are prone to failure/divergence without proper pose initialization.- Splitting the video into segments and reconstructing object parts within each segment can provide better pose initialization for a global reconstruction.- A regularization loss can help avoid degenerate shapes and stabilize the optimization when shape is underconstrained due to fewer views.- Rendered depths from previous estimates can help constrain the shape from undergoing large deviations.The paper aims to demonstrate a robust reconstruction and pose estimation method for in-hand scanning that does not rely on textures, works for arbitrary objects, and does not need pose supervision or prior training data.


## What is the main contribution of this paper?

Based on my understanding, the main contribution of this paper is proposing a method for in-hand 3D object scanning from an RGB image sequence. The key ideas and contributions are:- Proposing a method that can reconstruct both textured and poorly textured objects from an RGB video of the object being manipulated, without requiring any prior object knowledge or ground truth poses.- Handling the challenges of simultaneous tracking and reconstruction, which is prone to drift, by splitting the input sequence into overlapping segments. Tracking and reconstruction is done independently on each segment, allowing incremental reconstruction of different object surfaces. The segments are then merged for full reconstruction.- Introducing a shape regularization loss to avoid degenerate shapes during early optimization when constraints are limited. Also using a synthetic depth loss to stabilize tracking against drift.- Achieving camera pose estimation without reliance on texture features like COLMAP by combining both geometric and texture information during tracking. This enables reconstruction of textureless objects.- Demonstrating results on real datasets including handheld captures and achieving better reconstructions than COLMAP, and comparable results to methods that use ground truth poses.In summary, the key contribution is a practical RGB-based reconstruction method for in-hand scanning that can handle both textured and textureless objects through a tracking-based approach and suitable regularization strategies. The method does not require any pose initialization or object priors.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper proposes a method for reconstructing the 3D shape and appearance of an unknown object being manipulated by hands from an RGB video sequence, by representing the object with implicit neural fields, splitting the sequence into segments to enable incremental tracking and reconstruction, and aligning the per-segment reconstructions into a complete model.The key ideas are using implicit neural fields to represent object shape and appearance, splitting the sequence to enable incremental tracking, and aligning per-segment reconstructions. The method does not require any prior object knowledge or ground truth pose information. Experiments on in-hand object datasets demonstrate that the approach can reconstruct both textured and textureless objects.
