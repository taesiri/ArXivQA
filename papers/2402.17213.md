# [VCD: Knowledge Base Guided Visual Commonsense Discovery in Images](https://arxiv.org/abs/2402.17213)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Current visual commonsense discovery is coarse-grained and focuses mainly on salient objects, ignoring minor objects and not extracting different types of commonsense from different objects
- Existing methods only extract explicit commonsense directly observable in images, neglecting implicit commonsense that requires reasoning based on experiences

Proposed Solution:
- Systematically define 5 types of explicit commonsense (property, spatial, action) and 5 types of implicit commonsense (property, spatial, action) 
- Introduce the Visual Commonsense Discovery (VCD) task to extract fine-grained commonsense of different types related to various objects in images
- Construct a large-scale VCD dataset with over 100K images and 14M object-commonsense pairs based on Visual Genome and ConceptNet
- Propose the VCD model (VCDM) combining a vision-language model with instruction tuning to generate commonsense

Main Contributions:
- First to discover implicit visual commonsense and extract systematic fine-grained explicit and implicit commonsense
- Construct large-scale VCD dataset with strict quality control 
- Propose VCDM model that outperforms GPT-4V on implicit commonsense discovery
- Demonstrate VCD's utility in improving commonsense evaluation and VQA performance

In summary, this paper makes significant contributions towards advancing fine-grained and implicit visual commonsense discovery through the introduction of the VCD task and dataset, and the proposal of the effective VCDM model.
