# [VCD: Knowledge Base Guided Visual Commonsense Discovery in Images](https://arxiv.org/abs/2402.17213)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Current visual commonsense discovery is coarse-grained and focuses mainly on salient objects, ignoring minor objects and not extracting different types of commonsense from different objects
- Existing methods only extract explicit commonsense directly observable in images, neglecting implicit commonsense that requires reasoning based on experiences

Proposed Solution:
- Systematically define 5 types of explicit commonsense (property, spatial, action) and 5 types of implicit commonsense (property, spatial, action) 
- Introduce the Visual Commonsense Discovery (VCD) task to extract fine-grained commonsense of different types related to various objects in images
- Construct a large-scale VCD dataset with over 100K images and 14M object-commonsense pairs based on Visual Genome and ConceptNet
- Propose the VCD model (VCDM) combining a vision-language model with instruction tuning to generate commonsense

Main Contributions:
- First to discover implicit visual commonsense and extract systematic fine-grained explicit and implicit commonsense
- Construct large-scale VCD dataset with strict quality control 
- Propose VCDM model that outperforms GPT-4V on implicit commonsense discovery
- Demonstrate VCD's utility in improving commonsense evaluation and VQA performance

In summary, this paper makes significant contributions towards advancing fine-grained and implicit visual commonsense discovery through the introduction of the VCD task and dataset, and the proposal of the effective VCDM model.


## Summarize the paper in one sentence.

 Here is a one-sentence summary of the paper:

The paper introduces a new task called Visual Commonsense Discovery that aims to extract fine-grained, multidimensional commonsense knowledge related to different objects in images, and constructs a dataset and proposes a model based on object-text alignment and instruction tuning to tackle this task.


## What is the main contribution of this paper?

 Based on the paper, the main contribution is:

1) Introducing a new task called Visual Commonsense Discovery (VCD), which aims to extract fine-grained commonsense of different types related to various objects in an image. This includes both explicit commonsense that can be directly observed from the image, as well as implicit commonsense that requires inferring based on life experiences.  

2) Constructing a large-scale dataset called VCDD for the VCD task, which contains over 100,000 images and 14 million object-commonsense pairs. The dataset covers commonsense across property, spatial, and action dimensions.

3) Proposing a generative model called VCDM that combines a vision-language model with instruction tuning to tackle the VCD task. The model can handle complex inputs including images, object names, bounding boxes, and commonsense types.

4) Demonstrating VCDM's capabilities in discovering both explicit and implicit visual commonsense through automatic metrics and human evaluation. The model outperforms powerful multimodal models like GPT-4V in implicit commonsense discovery.

5) Validating the value of the VCD task by showing performance improvements achieved by VCDM on downstream tasks like visual commonsense evaluation and visual question answering.

In summary, the key contribution is introducing the novel VCD task along with a dataset and model tailored for fine-grained, systematic visual commonsense discovery encompassing both explicit and implicit dimensions. The utility of VCD is further evidenced through downstream task evaluations.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and keywords associated with this paper include:

- Visual commonsense discovery (VCD) - The main task introduced in the paper, aiming to extract fine-grained commonsense of different types for different objects in images.

- Types of visual commonsense - The paper systematically defines different types of visual commonsense, categorized into explicit and implicit, as well as property, space, and action. 

- Dataset construction - The paper constructs a large-scale dataset VCDD for the VCD task by leveraging Visual Genome and ConceptNet.

- Model (VCDM) - The paper proposes a generative model that combines a vision-language model with instruction tuning to tackle the VCD task.

- Downstream tasks - The value of VCD is demonstrated through downstream tasks like visual commonsense evaluation on ImageNetVc and visual question answering on VQAv2.

- Performance evaluation - Both automatic metrics and human evaluation are used to assess the performance of the proposed VCDM model on the VCD task.

In summary, the key terms cover the proposed task, dataset, model, performance evaluation, and applications in downstream tasks. Let me know if you need any clarification or have additional questions!


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in the paper:

1. The paper proposes a new task called Visual Commonsense Discovery (VCD). Can you explain in more detail what the goal and formulation of this task is? What are some key differences compared to prior work on visual commonsense reasoning?

2. The paper constructs a new dataset called VCDD for the VCD task. Can you describe the process for constructing this dataset? What are the sources of data and how are they combined? What quality control steps are taken?

3. The paper proposes a new model called VCDM for the VCD task. At a high level, can you explain the architecture and key components of VCDM? What is the motivation behind using instruction tuning? 

4. When generating the output for VCDM, the paper employs different strategies for explicit and implicit commonsense triples. Can you summarize what these strategies are and why they differ? What is the intuition behind the strategies?

5. The paper compares VCDM against multimodal large language models like GPT-4V. What prompts and evaluation process is used for this comparison? What are some key findings from the human evaluation?

6. Ablation studies are performed by removing different input components to VCDM such as images, bounding boxes, and object names. Can you summarize the impact observed by removing each of these components? What does this reveal about the model?

7. Beyond VCD, the value of discovering visual commonsense is also evaluated on the downstream tasks of ImageNetVc and VQA v2. Can you briefly summarize the experimental setup, methodology, and findings from these experiments? 

8. The related work section discusses various areas such as knowledge graphs, language models, vision-language models, and instruction tuning. Can you compare and contrast how the method in this paper builds upon or differs from relevant prior work in these areas?

9. The conclusion mentions that implicit commonsense discovery necessitates further research. In your opinion, what are some limitations of the current work in discovering implicit commonsense and how might these be addressed in future work?

10. What component of the overall pipeline - the task formulation, dataset, model architecture, training methodology, or evaluation - do you think provides the most value? What are some potential directions or enhancements for improving this component even further?
