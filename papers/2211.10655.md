# [Solving 3D Inverse Problems using Pre-trained 2D Diffusion Models](https://arxiv.org/abs/2211.10655)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central research question this paper addresses is: How can we leverage pre-trained 2D diffusion models to efficiently solve 3D inverse problems in medical imaging?Specifically, the authors aim to develop a method that can scale diffusion models to 3D while avoiding the prohibitively high memory/computation cost of modeling the full 3D space. Their key ideas are:1) Apply 2D diffusion models slice-by-slice in parallel along the z-axis. This avoids directly modeling the full 3D space.2) Augment the 2D diffusion prior with a model-based 3D prior (TV along z-axis) to impose consistency between slices. 3) Develop an efficient ADMM optimization strategy to integrate the diffusion model sampling with the 3D prior in a memory-efficient way.Overall, the central goal is to develop an approach that can achieve state-of-the-art 3D medical image reconstruction using only pre-trained 2D diffusion models, avoiding the need for costly 3D modeling or training directly on 3D volumes. The proposed DiffusionMBIR method aims to achieve this.


## What is the main contribution of this paper?

The main contribution of this paper is proposing a method called DiffusionMBIR for 3D medical image reconstruction using pre-trained 2D diffusion models. The key ideas are:- Combining model-based iterative reconstruction (MBIR) with diffusion model sampling to perform coherent 3D reconstructions from 2D models. - Augmenting the 2D diffusion model prior with a 3D total variation (TV) prior in the z-direction to impose coherence between slices.- An efficient implementation using "variable sharing" to minimize costly ADMM and conjugate gradient iterations.Specifically, the paper shows how to leverage powerful 2D diffusion models that can be trained with small datasets (<10 volumes) for 3D tasks like sparse-view CT, limited-angle CT, and compressed sensing MRI. By augmenting the 2D prior with 3D TV regularization, the method achieves state-of-the-art performance even with very sparse measurements (e.g. 2-view CT). A key result is the high generalization capacity to out-of-distribution data.In summary, the main contribution is an effective way to scale 2D diffusion models to 3D medical imaging problems through a combination of data-driven and model-based priors. This establishes strong performance across applications with high fidelity, accuracy and surprising generalization ability.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:This paper proposes combining 2D diffusion models with 3D model-based iterative reconstruction to enable memory-efficient and accurate 3D medical image reconstruction from sparse/limited measurements, achieving state-of-the-art results on tasks like sparse-view CT, limited-angle CT, and compressed sensing MRI.


## How does this paper compare to other research in the same field?

Here are a few key thoughts on how this paper compares to other research in 3D medical image reconstruction:- Using pre-trained 2D diffusion models for 3D reconstruction is a novel approach. Most prior work has focused on training 3D generative models directly, which is more computationally expensive. Leveraging 2D models is an efficient way to incorporate strong image priors into the 3D setting.- Augmenting the diffusion model with a model-based (TV) prior specifically in the z-direction is clever. It allows enforcing inter-slice consistency while still relying primarily on the learned prior within each slice. Other papers tend to use generic 3D regularizers like isotropic TV rather than this directional approach.- The method performs very well across multiple 3D modalities - sparse-view CT, limited angle CT, and compressed sensing MRI. Many recent papers focus on a single problem, whereas this demonstrates broader applicability. The results significantly outperform previous state-of-the-art in some cases, like 2-view CT reconstruction.- The approach seems quite robust to out-of-distribution data, reconstructing very different phantom images well from limited data. This is an important consideration for real-world applicability. Some learning-based methods are highly data dependent.- A limitation is that diffusion models are still computationally expensive compared to simpler learning priors based on CNNs. So the method may be slower than some alternatives, though the parallelization techniques proposed help mitigate this.Overall, the paper introduces a conceptually simple but effective way to bring modern generative priors into 3D imaging. The experiments convincingly demonstrate state-of-the-art results across multiple important modalities while being robust. The approach helps overcome limitations of applying diffusion models directly in 3D.


## What future research directions do the authors suggest?

Based on my reading of the paper, some of the main future research directions suggested by the authors are:- Extending the method to other 3D inverse problems beyond CT and MRI, such as positron emission tomography (PET) or ultrasound imaging. The proposed framework of augmenting a 2D diffusion prior with a model-based 3D prior could potentially be applied to other modalities.- Exploring different choices for the augmented prior beyond total variation (TV). The authors mainly focused on TV as the 3D prior, but suggest trying other regularization functions based on domain knowledge of each imaging modality.- Training 3D diffusion models with efficient representations like point clouds or latent vectors. The authors discuss current limitations in scaling up the diffusion process to 3D due to memory constraints. Developing more efficient 3D generative models could help address this.- Applying the framework to solve video and 4D reconstruction problems. The authors propose applying their method along the temporal dimension in addition to the spatial dimensions.- Evaluating the method on a wider range of datasets, especially highly out-of-distribution examples. The authors showed promising generalization ability but suggest more rigorous evaluation across diverse data.- Developing theoretical analysis to better understand the properties of combining data-driven diffusion models with model-based priors. For example, analyzing how the priors interact and affect reconstruction accuracy.In summary, the main suggestions are around extending the approach to new modalities and data types, improving computational and memory efficiency for 3D, evaluating generalization more thoroughly, and developing theoretical grounding. The framework shows promising performance but there are many opportunities to build on it further.
