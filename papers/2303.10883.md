# [Explicit Visual Prompting for Low-Level Structure Segmentations](https://arxiv.org/abs/2303.10883)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading, the central research question this paper seeks to address is:

How can we develop a unified approach that performs well across various low-level structure segmentation tasks, including forgery detection, shadow detection, defocus blur detection, and camouflaged object detection? 

The key hypothesis is that by taking inspiration from pre-training and prompt tuning approaches in NLP, they can develop a new visual prompting model called Explicit Visual Prompting (EVP) that leverages explicit visual features from individual images to efficiently adapt a single model to diverse segmentation tasks. Their hypothesis is that EVP can significantly outperform other parameter-efficient tuning methods and achieve state-of-the-art performance compared to task-specific solutions.

In summary, the central research question is how to develop a unified segmentation model for diverse low-level tasks. The key hypothesis is that explicit visual prompting based on image features can enable efficient tuning and strong performance across tasks compared to specialized models.


## What is the main contribution of this paper?

 Based on my reading of the paper, the main contributions are:

1. The authors propose a unified approach called Explicit Visual Prompting (EVP) that achieves state-of-the-art performance on multiple low-level vision tasks including forgery detection, shadow detection, defocus blur detection, and camouflaged object detection. 

2. EVP takes inspiration from prompting methods in NLP and adapts a frozen vision transformer backbone to new tasks by prompting with explicit features from the input image itself. Specifically, it tunes the frozen patch embeddings and learns an extra embedding for the high-frequency components of each image.

3. EVP significantly outperforms other parameter-efficient tuning methods like VPT and AdaptFormer with the same number of extra trainable parameters. It also matches or exceeds the performance of task-specific state-of-the-art models on various benchmarks while using a simple and unified network.

4. The results demonstrate that prompting with explicit image features enables efficient adaptation and knowledge transfer from pre-trained vision models to diverse low-level structure segmentation tasks. This simplifies model design while achieving strong performance across different domains.

In summary, the key novelty is explicitly prompting vision transformers with image features like frozen embeddings and high-freq components to enable simple yet effective transfer to multiple low-level vision tasks. The unified EVP model matches or beats task-specific state-of-the-art approaches.
