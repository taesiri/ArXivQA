# [Keyword-Guided Neural Conversational Model](https://arxiv.org/abs/2012.08383)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is how to impose conversational goals/keywords on open-domain conversational agents, where the agent is required to lead the conversation to a target keyword smoothly and fast. Specifically, the paper aims to address two key limitations in prior work on this problem:1) The training and evaluation datasets for next-turn keyword prediction are noisy and have low correlation with human judgements. 2) During keyword transitions, agents rely solely on similarities between word embeddings, which may not reflect how humans converse.To address these limitations, the central hypothesis of the paper is that human conversations are grounded on commonsense knowledge. Therefore, the paper proposes a model that can leverage external commonsense knowledge graphs for both keyword transition and response retrieval in order to achieve smoother and faster keyword transitions.In summary, the central research question is how to enable conversational agents to smoothly and efficiently guide conversations towards target keywords by grounding the model in commonsense knowledge. The key hypothesis is that incorporating commonsense knowledge will lead to more human-like keyword transitions compared to prior approaches.


## What is the main contribution of this paper?

The main contribution of this paper is proposing a keyword-guided neural conversational model that can leverage external commonsense knowledge graphs (CKG) for both keyword transition and response retrieval in open-domain conversations. Specifically, the key contributions are:- Identifying two limitations of existing approaches for next-turn keyword selection: 1) noisy training and evaluation datasets, and 2) unreliable keyword transition based on word embedding similarities. - Proposing two graph neural network (GNN) based models to incorporate commonsense knowledge from CKG for improving next-turn keyword prediction and keyword-augmented response retrieval. - Collecting a large-scale open-domain Reddit conversation dataset that has more diverse linguistic patterns compared to existing datasets.- Conducting extensive experiments showing that grounding keyword transitions on CKG improves conversation smoothness and allows reaching the target keyword faster. Leveraging commonsense triplets also substantially improves the performance of next-turn keyword prediction and response retrieval.- Human evaluations and model analysis validating that the proposed model produces smoother responses and achieves higher success rates in reaching target keywords compared to competitive baselines.In summary, the key contribution is using commonsense knowledge graphs to improve keyword transition and response retrieval in goal-oriented open-domain conversational agents. The proposed techniques and the Reddit dataset enable building more human-like conversational agents.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper proposes a keyword-guided neural conversational model that incorporates commonsense knowledge graphs to improve the smoothness and efficiency of leading conversations to target keywords.


## How does this paper compare to other research in the same field?

Here are a few thoughts on how this paper compares to other research in conversational AI:- This paper focuses specifically on imposing conversational goals/keywords on open-domain chatbots, with the aim of smoothly and quickly guiding the conversation towards a target topic. This is a fairly novel task compared to much existing work that focuses on more passive, open-ended conversation.- The idea of breaking down guided conversation into next-turn keyword prediction and keyword-augmented response retrieval follows previous work like Tang et al. 2019. However, this paper proposes improvements like using commonsense knowledge graphs to ground the keyword transitions. - Using external commonsense knowledge graphs to improve conversational models has been explored in other recent work, but this paper utilizes it in a new way for keyword transitions and response retrieval. The graph neural network models for incorporating the knowledge graph are also novel.- The large-scale Reddit dataset created for this paper provides a more diverse training source compared to existing dialogue datasets like ConvAI2 that come from a small set of crowdworkers.- Overall, the paper introduces useful innovations in conversational goal-driven dialogue agents, especially the knowledge graph grounding and models. The evaluations demonstrate clear improvements over competitive baselines.- One limitation is that existing approaches, including this one, still struggle to accurately retrieve keyword-related responses. The authors acknowledge this issue and propose it as an area for future work.In summary, this paper advances guided conversational agents through novel use of external knowledge and neural models tailored for this task. It compares favorably to related work, while still facing some challenges common to this research area. The innovations and analyses overall provide a useful contribution.


## What future research directions do the authors suggest?

Based on my reading of the paper, some of the main future research directions suggested by the authors include:- Improving the accuracy of retrieving keyword-related responses. The authors note this is a current limitation of their approach and other existing approaches, and is a bottleneck for improving overall target keyword success rate. They suggest training the response retrieval model on datasets where keywords and responses are well-correlated.- Exploring different graph neural network architectures and training objectives for incorporating commonsense knowledge graphs. The authors show benefits from using GNNs but there is room to explore other graph modeling techniques.- Studying how to better balance tradeoffs between conversation smoothness and efficiency in reaching the target keyword. The authors note this is a key challenge in keyword-guided conversation modeling. New techniques could be developed to optimize this tradeoff.- Evaluating the approach on other conversation datasets and domains beyond the Reddit and ConvAI2 datasets used in the paper. The authors note their approach is designed for open-domain conversations but it would be useful to test it in other domains.- Incorporating other types of external knowledge beyond commonsense knowledge graphs, such as topic models, multimedia context, etc. The authors only use commonsense KGs but other knowledge sources could also be beneficial.- Developing enhanced user simulations for training and evaluation. The authors use rather simple user simulations, better user modeling could improve learning.- Exploring the approach for other conversational agent applications like recommendation and psychotherapy. The authors suggest the keyword guidance capability could benefit many real-world applications.In summary, the main future directions focus on improving response retrieval, knowledge incorporation, conversation modeling/evaluation, and exploring new applications of the keyword-guided conversation framework.


## Summarize the paper in one paragraph.

Here is a one paragraph summary of the paper:The paper studies the problem of imposing conversational goals/keywords on open-domain conversational agents, where the agent is required to lead the conversation to a target keyword smoothly and fast. The paper identifies two limitations in existing approaches: 1) the training and evaluation datasets for next-turn keyword prediction are noisy and have low correlation with human judgements, and 2) the keyword transition relies solely on similarities between word embeddings which may not reflect how humans converse. To address these limitations, the paper proposes a keyword-guided neural conversational model that leverages external commonsense knowledge graphs (CKG) for both keyword transition and response retrieval. Specifically, the model uses graph neural networks to incorporate CKG triplets for next-turn keyword prediction and keyword-augmented response retrieval. Evaluations show that grounding keyword transitions on CKG improves conversation smoothness and allows reaching the target keyword faster. Incorporating CKG triplets also substantially improves the performance of both subtasks over competitive baselines. Overall, the proposed model produces responses with smoother keyword transitions and achieves higher success rates based on human evaluations.


## Summarize the paper in two paragraphs.

Here is a two paragraph summary of the paper:The paper proposes a keyword-guided neural conversational model that can leverage external commonsense knowledge graphs (CKG) to smoothly transition a conversation from a starting keyword to a target keyword. The key ideas are: 1) Using a CKG to select the next keyword that is related to the previous keywords and closer to the target keyword on the graph. This allows for more reasonable keyword transitions compared to just using word embeddings. 2) Incorporating CKG triplets into neural models for next-turn keyword prediction and keyword-augmented response retrieval. This improves the accuracy of both tasks. 3) Proposing a new conversational dataset collected from Reddit which has more diverse linguistic patterns compared to existing datasets.The proposed model is evaluated on next-turn keyword prediction, response retrieval, and end-to-end dialogues. Results show that using the CKG improves performance across all tasks compared to competitive baselines. In end-to-end dialogues, the model achieves higher success rates in reaching target keywords, faster transition to targets, and smoother conversations judged by humans. Overall, the paper demonstrates that grounding keyword transitions on commonsense knowledge leads to more human-like conversational agents that can smoothly lead conversations towards conversational goals.


## Summarize the main method used in the paper in one paragraph.

Here is a one paragraph summary of the main method used in the paper:The paper proposes a keyword-guided neural conversational model called CKC that can leverage external commonsense knowledge graphs (CKG) to smoothly transition a conversation from a starting keyword to a target keyword. The model breaks down the task into two components: next-turn keyword prediction and keyword-augmented response retrieval. For next-turn keyword prediction, the model uses a gated graph neural network (GGNN) to incorporate commonsense triplets from the CKG into predicting the next keyword to talk about that is closer to the target. For response retrieval, the model encodes the conversation context and candidate responses using a hierarchical GRU and matches them based on both utterance representations and keyword representations from the GGNN. The keyword selection strategy relies on weighted shortest paths between keywords on the CKG graph to pick keywords that are highly related to the target. By grounding keyword transitions on commonsense knowledge, the model is able to produce smoother topic transitions and reach the target keyword faster compared to previous approaches.


## What problem or question is the paper addressing?

Based on my reading of the paper, the key problem the authors are trying to address is how to impose conversational goals or keywords on open-domain conversational agents, so that the agent can actively lead the conversation towards a target keyword in a smooth and efficient manner. Specifically, the paper identifies two main limitations in prior work on this problem:1) The training and evaluation datasets for next-turn keyword prediction are noisy, as they are extracted directly from conversations without human annotations. Many of the keyword transitions in these datasets are not actually relevant or natural. 2) The keyword selection strategies used during conversation rely solely on cosine similarities between word embeddings, which may not reflect how humans relate words and transition between topics during conversations.To address these issues, the authors propose a new commonsense-aware keyword-guided conversational model that can leverage external commonsense knowledge graphs to improve both keyword prediction and transition, as well as response retrieval. The key ideas are:- Use the commonsense knowledge graph to filter noisy keyword transitions in the datasets.- Propose graph neural network models to incorporate commonsense triplets for more accurate next-turn keyword prediction and response retrieval. - Use the knowledge graph to guide keyword selection during conversation in a more human-like way, by traversing reasonable paths between keywords.So in summary, the paper aims to address the limitations of prior work by leveraging external commonsense knowledge to enable smoother and more efficient keyword-guided conversations. The main goals are improving keyword prediction, response retrieval, and keyword transition strategies.
