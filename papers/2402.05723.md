# [In-Context Learning Can Re-learn Forbidden Tasks](https://arxiv.org/abs/2402.05723)

## Summarize the paper in one sentence.

 This paper investigates whether in-context learning can undo safety fine-tuning of large language models by teaching them to perform forbidden tasks again.


## What is the main contribution of this paper?

 The main contributions of this paper are:

1. Demonstrating on two tasks (sentiment classification and link hallucination) that LLMs can re-learn forbidden tasks via in-context learning after being fine-tuned to refuse to perform those tasks. There is a correlation between the attack success rate and the number of tokens in the ICL examples.

2. Introducing In-Chat ICL attacks and showing that the safety behavior of Vicuna-7B and Starling-7B is vulnerable to ICL and In-Chat ICL attacks, while LLama-7B is more resilient. Specifically, the attack works out-of-the-box on Vicuna-7B and Starling-7B but fails on LLama-7B, though the log-probability of an informative response does increase on LLama-7B. The In-Chat ICL attack performs much better at breaking safety alignment.

So in summary, the main contributions are using ICL and In-Chat ICL to attack models fine-tuned for safety across various tasks, and showing that while defense is possible, vigilance against this attack vector is still necessary.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key keywords and terms associated with it include:

- In-context learning (ICL)
- Forbidden tasks
- Safety training/fine-tuning
- Alignment
- Helpfulness, honesty, harmlessness 
- Adversarial attacks
- Sentiment classification
- Link hallucination
- Confabulation/hallucination
- Prompting mechanisms (e.g. chain-of-thought, scratchpads)
- Instruction fine-tuning 
- Reinforcement learning (RLHF, RLAIF, PPO, reward modeling)  
- Direct preference optimization (DPO)
- AdvBench dataset
- Toxic/harmful content generation
- Threat models
- Gradient-based attacks
- In-Chat ICL attacks
- Chat template tokens (e.g. {User}, {Assistant})
- Log probability analysis
- Semantic similarity analysis

The key goals and contributions relate to systematically evaluating whether in-context learning can "un-learn" or break fine-tuning intended to make models refuse certain queries, studying this across different forbidden tasks, and proposing more powerful "In-Chat ICL" attack prompts. The safety analysis on models like LaMDA, Vicuna, and Starling is also a core contribution, assessing model vulnerabilities.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in this paper:

1. The paper mentions using direct preference optimization (DPO) for fine-tuning. How does DPO compare to other reinforcement learning and fine-tuning methods like RLHF or supervised fine-tuning in the context of training models to refuse tasks? What are the relative advantages and disadvantages?

2. When attacking the model's safety using in-context learning examples, what is the explanation for why model performance peaks around 350-500 tokens before decreasing? Is there an optimal number of examples and prompt length? 

3. For the link hallucination task, what causes the model to sometimes refuse answering not because the input contains a weblink but due to a sensitive keyword in the title? Does this indicate a limitation in how well the model has learned to refuse summarizing weblinks specifically?

4. The in-chat ICL attack performs much better than the normal ICL attack especially on Vicuna-7B. What specifically about framing the examples as a previous chat enables better attack performance? Does this indicate models may have a capability to learn better in-context in conversational settings?

5. Although the ICL safety attack succeeds on Vicuna-7B and Starling-7B, it fails on LLaMA-7B. However, the log probability of an affirmative response still increases slightly on LLaMA-7B. Could the attack potentially succeed given better attack examples and methodology? How might the attack be further improved?

6. For the semantic similarity analysis between attack queries and ICL examples, could there be better metrics beyond cosine similarity using SentenceBERT embeddings to capture semantic closeness? Are there limitations in relying on semantic similarity to predict attack success?

7. The paper studies attacking models to behave in harmful ways as a means to evaluate safety. However, safety also entails dimensions of honesty and helpfulness. How well does the attack methodology explore those facets of safety compared to harmlessness?

8. How does directly training on ICL attack examples during fine-tuning, as explored in Table 2, perform compared to other potential defenses against ICL safety attacks? What other defense strategies merit further exploration? 

9. The paper uses binary sentiment classification as a simple proxy task to study forbidden tasks. How well do the conclusions generalize to more complex, open-ended forbidden tasks? What experiments could be beneficial to conduct using different forbidden tasks?

10. For real-world deployment of safe LLMs, are in-context learning attacks a credible threat vector that merits significant additional research to defend against? Or are other attack vectors like gradient-based attacks more concerning? What evidence supports your view?
