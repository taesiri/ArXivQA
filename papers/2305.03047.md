# Principle-Driven Self-Alignment of Language Models from Scratch with
  Minimal Human Supervision

## What is the central research question or hypothesis that this paper addresses?

Based on my review, the central research question/hypothesis of this paper appears to be: How can large language models be aligned to human values and intentions effectively and efficiently, with minimal dependence on extensive human annotations?The key points are:- Recent AI assistants like ChatGPT rely heavily on human supervision via supervised fine-tuning and reinforcement learning from feedback. This dependence has limitations in terms of cost, quality, reliability, diversity, self-consistency, and potential biases of human annotations. - The paper proposes a new approach called "Principle-Driven Self-Alignment" that combines human-defined principles and the generative power of LLMs to align LLMs with minimal human supervision.- The approach has 4 main stages: Topic-Guided Self-Instruct, Principle-Driven Self-Alignment, Principle Engraving, and Verbose Cloning. Together these aim to develop helpful, ethical and reliable AI assistants.- When applied to LLaMA-65B, the approach results in an AI assistant called Dromedary that surpasses state-of-the-art models on alignment benchmarks, using less than 300 lines of human annotations.So in summary, the central hypothesis is around developing a more efficient alignment approach for LLMs that depends minimally on human supervision, with the goal of creating helpful, ethical and reliable AI assistants. The Principle-Driven Self-Alignment method is proposed and evaluated as a way to achieve this.
