# [Self-Supervised Visual Terrain Classification from Unsupervised Acoustic   Feature Learning](https://arxiv.org/abs/1912.03227)

## What is the central research question or hypothesis that this paper addresses?

Based on my review, the central research question this paper aims to address is: How can proprioceptive and exteroceptive sensor modalities be leveraged in a self-supervised manner for robust terrain classification without requiring manual labeling of training data?Specifically, the key aspects are:- Developing an unsupervised proprioceptive terrain classifier using vehicle-terrain interaction sounds that does not require manually labeled data.- Using the unsupervised proprioceptive classifier to automatically label visual terrain data in a self-supervised manner. - Training an exteroceptive visual terrain classifier such as a semantic segmentation network on these automatic weakly labeled samples without human annotation.- Demonstrating that this allows the exteroceptive classifier to adapt to changing environments and terrain appearances in a lifelong learning approach.So in summary, the main research contribution is a self-supervised cross-modal learning framework for terrain classification that does not rely on manual labeling and enables adaptation to new environments. The key hypothesis is that by exploiting proprioceptive and exteroceptive modalities in this manner, robust terrain classification can be achieved.


## What is the main contribution of this paper?

Based on my reading, the main contributions of this paper are:1. Proposing a self-supervised terrain classification framework that uses an unsupervised proprioceptive classifier based on vehicle-terrain interaction sounds to generate training labels for an exteroceptive classifier that performs semantic segmentation of camera images.2. Introducing a novel heuristic for triplet sampling in metric learning that leverages visual features from images rather than requiring ground truth labels. This is used to learn an embedding space for the vehicle-terrain interaction sounds in an unsupervised manner.3. Developing a self-supervised semantic segmentation model that learns from weakly labeled birds-eye view terrain images generated using the unsupervised audio embeddings.4. Creating a new challenging dataset called Freiburg Terrains containing over 4 hours of synchronized audio, video, and robot pose data captured in diverse environments.5. Presenting extensive quantitative and qualitative results demonstrating state-of-the-art performance of the unsupervised audio classifier and comparable performance of the self-supervised semantic segmentation model to fully supervised learning.6. Showing the ability of the framework to generalize to new environments and lighting conditions by leveraging the audio classifier to automatically adapt the visual classifier without requiring additional human labeling.In summary, the core contribution is a multimodal self-supervised learning pipeline for terrain classification that can automatically label training data for a visual classifier by exploiting an unsupervised audio classifier, taking a step towards lifelong learning for robots operating in changing environments.
