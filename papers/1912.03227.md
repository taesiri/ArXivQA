# [Self-Supervised Visual Terrain Classification from Unsupervised Acoustic   Feature Learning](https://arxiv.org/abs/1912.03227)

## What is the central research question or hypothesis that this paper addresses?

Based on my review, the central research question this paper aims to address is: How can proprioceptive and exteroceptive sensor modalities be leveraged in a self-supervised manner for robust terrain classification without requiring manual labeling of training data?Specifically, the key aspects are:- Developing an unsupervised proprioceptive terrain classifier using vehicle-terrain interaction sounds that does not require manually labeled data.- Using the unsupervised proprioceptive classifier to automatically label visual terrain data in a self-supervised manner. - Training an exteroceptive visual terrain classifier such as a semantic segmentation network on these automatic weakly labeled samples without human annotation.- Demonstrating that this allows the exteroceptive classifier to adapt to changing environments and terrain appearances in a lifelong learning approach.So in summary, the main research contribution is a self-supervised cross-modal learning framework for terrain classification that does not rely on manual labeling and enables adaptation to new environments. The key hypothesis is that by exploiting proprioceptive and exteroceptive modalities in this manner, robust terrain classification can be achieved.


## What is the main contribution of this paper?

Based on my reading, the main contributions of this paper are:1. Proposing a self-supervised terrain classification framework that uses an unsupervised proprioceptive classifier based on vehicle-terrain interaction sounds to generate training labels for an exteroceptive classifier that performs semantic segmentation of camera images.2. Introducing a novel heuristic for triplet sampling in metric learning that leverages visual features from images rather than requiring ground truth labels. This is used to learn an embedding space for the vehicle-terrain interaction sounds in an unsupervised manner.3. Developing a self-supervised semantic segmentation model that learns from weakly labeled birds-eye view terrain images generated using the unsupervised audio embeddings.4. Creating a new challenging dataset called Freiburg Terrains containing over 4 hours of synchronized audio, video, and robot pose data captured in diverse environments.5. Presenting extensive quantitative and qualitative results demonstrating state-of-the-art performance of the unsupervised audio classifier and comparable performance of the self-supervised semantic segmentation model to fully supervised learning.6. Showing the ability of the framework to generalize to new environments and lighting conditions by leveraging the audio classifier to automatically adapt the visual classifier without requiring additional human labeling.In summary, the core contribution is a multimodal self-supervised learning pipeline for terrain classification that can automatically label training data for a visual classifier by exploiting an unsupervised audio classifier, taking a step towards lifelong learning for robots operating in changing environments.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points in the paper:The paper proposes a self-supervised framework for terrain classification where an unsupervised proprioceptive classifier using vehicle-terrain sounds automatically labels visual terrain patches enabling a semantic segmentation network to be trained without manual annotation; experiments show the proprioceptive classifier exceeds state-of-the-art unsupervised methods and the self-supervised semantic segmentation achieves comparable performance to supervised learning.


## How does this paper compare to other research in the same field?

Based on my reading, this paper makes several contributions to the field of self-supervised learning for terrain classification compared to other related works:- It proposes a novel framework that uses an unsupervised proprioceptive terrain classifier on vehicle-terrain sounds to self-supervise an exteroceptive classifier for semantic segmentation of visual images. This allows exploiting the complementary nature of multiple sensory modalities.- It introduces a new heuristic for sampling triplets to form an embedding space for metric learning without requiring manual labels. Instead, it leverages visual features from a pretrained model to approximate ground truth labels. This is a unique approach not explored before. - It demonstrates self-supervised semantic segmentation results comparable to fully supervised learning on a new challenging dataset. Most prior works have focused only on few terrain classes.- It shows the ability to adapt and improve the model in new environments in a self-supervised manner. This takes a step towards lifelong learning as manual re-labeling is not needed.- It introduces the new Freiburg Terrains dataset with diverse conditions and five terrain classes. Many prior datasets are limited in diversity and size.Overall, the key novelty is in exploiting proprioception for self-supervision and the new triplet sampling heuristic. The results demonstrate applicability to semantic segmentation and generalizability. This moves beyond prior works that have focused only on utilizing proprioception or learning with few terrain classes. The multimodal self-supervision and lifelong learning capabilities are a notable contribution.
