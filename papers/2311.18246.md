# [Combined Scheduling, Memory Allocation and Tensor Replacement for   Minimizing Off-Chip Data Accesses of DNN Accelerators](https://arxiv.org/abs/2311.18246)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a summary paragraph of the paper:

This paper proposes COSMA, a framework that optimizes three key dimensions - operator scheduling, memory allocation, and tensor replacement - to minimize off-chip data accesses when mapping deep neural networks (DNNs) onto hardware accelerators. An integer linear programming (ILP) formulation is used to encode the problem and find optimal solutions. Experiments on 10 popular human-designed DNNs show COSMA reduces non-compulsory data accesses by 84% on average compared to prior approaches. For complex DNNs found by neural architecture search (NAS), a divide-and-conquer heuristic is proposed that scales up COSMA. This heuristic reduces data accesses by 85% for 4 state-of-the-art NAS DNNs. On average, COSMA generates optimal solutions in under a second for the human-designed DNNs. The NAS DNN heuristic solutions require around a minute. Overall, by co-optimizing the three key dimensions impacting off-chip accesses, COSMA significantly advances the state-of-the-art in minimizing expensive data movement when mapping DNNs onto hardware accelerators with tight memory budgets.


## Summarize the paper in one sentence.

 Here is a one-sentence summary of the paper:

The paper proposes an optimization framework, named COSMA, that combines operator scheduling, memory allocation, and tensor replacement to minimize off-chip data accesses when mapping deep neural networks onto hardware accelerators.


## What is the main contribution of this paper?

 The main contribution of this paper is proposing a framework called COSMA that collectively optimizes three key dimensions - operator scheduling, memory allocation, and tensor replacement - to minimize off-chip data accesses when mapping deep neural networks (DNNs) to hardware accelerators. Specifically:

- COSMA provides an integer linear programming (ILP) formulation to find the optimal solution for mapping a DNN to an accelerator by combining scheduling, allocation and replacement decisions.

- It demonstrates that COSMA can generate optimal solutions within seconds for a wide range of state-of-the-art DNNs using an off-the-shelf ILP solver, outperforming prior approaches.

- For complex DNNs generated by neural architecture search, COSMA proposes a divide-and-conquer heuristic to scale up the solution techniques. This heuristic also significantly reduces off-chip accesses compared to other methods.

- The evaluation shows COSMA reduces on average 84% of non-compulsory data accesses for human-designed DNNs and 85% for NAS-based DNNs compared to existing techniques.

In summary, the key contribution is proposing and evaluating COSMA, the first framework to optimally combine scheduling, allocation and replacement to minimize off-chip data movement when mapping DNNs to hardware accelerators.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, here are some of the key terms and keywords associated with it:

- Deep neural networks (DNNs)
- Hardware accelerators
- Operator scheduling
- Memory allocation  
- Tensor replacement
- Off-chip data accesses
- Integer linear programming (ILP)
- Divide-and-conquer heuristic
- Neural architecture search (NAS)

The paper proposes an optimization framework called COSMA that combines scheduling, memory allocation and tensor replacement to minimize off-chip data accesses when mapping DNNs to hardware accelerators. It formulates the problem as an ILP and also proposes a divide-and-conquer based heuristic to scale up to complex DNNs designed by NAS. The key focus is on reducing non-compulsory off-chip memory accesses.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 potential in-depth questions about the method proposed in the paper:

1. The paper proposes an Integer Linear Programming (ILP) formulation to minimize off-chip data accesses by combining operator scheduling, memory allocation, and tensor replacement. What are the key variables and constraints used in this ILP formulation? How do they capture the interactions between these three decisions?

2. The paper demonstrates the versatility of the ILP formulation by modifying the objective function for different goals like minimizing peak memory footprint. What changes need to be made to the formulation to optimize for these other objectives?

3. The complexity analysis shows that the size of the ILP encoding grows quadratically with the number of tensors. What techniques are used in the paper to reduce the number of variables and constraints? How do these techniques exploit the structure of common DNN graphs?

4. For complex DNNs generated by Neural Architecture Search (NAS), the paper uses a divide-and-conquer heuristic. How does this heuristic work? What is the tradeoff compared to finding the global optimal solution?

5. The evaluation compares the method against 4 schemes that combine different operator scheduling, memory allocation, and tensor replacement techniques. What are these schemes and what are their limitations compared to the proposed method? 

6. The results show that minimizing peak memory footprint does not necessarily minimize off-chip accesses. What is the intuition behind this? When can minimizing footprint help reduce accesses?

7. The proposed method cannot change tensor locations once allocated. What modification would allow movement and could it help further reduce accesses? What challenges would this introduce?

8. The target accelerator model assumes software control over address and access. What changes would be needed to support accelerators with hardware-managed scratchpads?

9. Operator schedules are fixed per DNN graph. Can the formulation be extended to support dynamic schedules that change based on runtime conditions? What additional complexities would this introduce?

10. Thepaper focuses on DNN inference. How would the optimization objectives and constraints need to be modified to support training workloads?
