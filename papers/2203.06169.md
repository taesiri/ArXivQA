# [LaPraDoR: Unsupervised Pretrained Dense Retriever for Zero-Shot Text   Retrieval](https://arxiv.org/abs/2203.06169)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is:How can we develop an unsupervised pretrained dense retriever that can achieve strong performance on diverse downstream tasks without requiring any labeled data? The authors propose Laprador (LaPraDoR), a pretrained dual-tower dense retriever, to address this question. The two key components of their approach are:1) Iterative Contrastive Learning (ICoL): An efficient pretraining method that iteratively trains the query and document encoders using contrastive learning objectives and a caching mechanism. This allows using a large number of negative samples to improve representation quality without running into GPU memory limitations.2) Lexicon-Enhanced Dense Retrieval (LEDR): A simple yet effective technique to incorporate lexical matching signals from BM25 into the dense retriever, making it more robust for zero-shot transfer.The central hypothesis is that by pretraining LaPraDoR in an unsupervised manner on a large diverse corpus using ICoL and then enhancing it with LEDR, the model can achieve state-of-the-art performance on downstream zero-shot text retrieval tasks across different domains. The experiments on the BEIR benchmark verify this hypothesis, showing the effectiveness of the proposed techniques.In summary, the paper focuses on unsupervised pretraining for zero-shot dense retrieval, with LaPraDoR being proposed as a solution. ICoL and LEDR are presented as key technical innovations to make this viable.


## What is the main contribution of this paper?

The main contribution of this paper is proposing LaPraDoR, an unsupervised pretrained dense retriever for zero-shot text retrieval. Specifically, the key contributions are:1. Proposing LaPraDoR, a fully unsupervised pretrained retriever that achieves state-of-the-art performance on the BEIR benchmark for zero-shot text retrieval, outperforming supervised dense retrieval models. 2. Introducing Iterative Contrastive Learning (ICoL) for efficiently training the dense retriever at scale by iteratively training the query and document encoders with a cache mechanism.3. Proposing Lexicon-Enhanced Dense Retrieval (LEDR) as an efficient way to enhance dense retrieval with lexical matching from BM25, compared to the widely used re-ranking paradigm.In summary, the main contribution is presenting LaPraDoR, an unsupervised pretrained dense retriever that obtains superior zero-shot retrieval performance through the proposed training strategy ICoL and matching approach LEDR. The model achieves new state-of-the-art results on the BEIR benchmark without using any supervised data.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper proposes LaPraDoR, an unsupervised pretrained dense retriever for zero-shot text retrieval that achieves state-of-the-art performance by using iterative contrastive learning and combining lexical and semantic matching without requiring any labeled data.


## How does this paper compare to other research in the same field?

Here are a few thoughts on how this paper compares to other research in unsupervised text retrieval:- This is the first paper I'm aware of proposing a fully unsupervised pretrained dense retriever. Most other work in dense retrieval relies on some supervised data, either for pretraining (e.g. DPR) or fine-tuning (e.g. ANCE). The unsupervised pretraining approach allows the model to generalize better to new domains.- The proposed Iterative Contrastive Learning (ICoL) training approach is novel for dense retrieval, though it builds on prior work in contrastive representation learning like MoCo. ICoL seems more suitable for training retrieval encoders compared to vanilla MoCo.- Applying lexical matching as a way to enhance unsupervised dense retrievers is a simple but impactful idea. Most prior work formulates lexical matching as reranking on top of dense retrieval. The proposed Lexicon-Enhanced Dense Retrieval (LEDR) gives better efficiency.- Evaluation on the BEIR benchmark allows comprehensive comparison to state-of-the-art supervised models across diverse domains. The strong zero-shot performance of LaPraDoR highlights the effectiveness of unsupervised pretraining for retrieval.- After fine-tuning on MS MARCO, LaPraDoR outperforms all previous systems on BEIR, including reranking approaches, while being much faster. This demonstrates the potential of unsupervised pretraining as a new paradigm.Overall, the unsupervised pretraining approach, simple yet effective training method, and fast lexical matching integration make this work stand out compared to prior research. The strong empirical results verify the viability of unsupervised dense retrievers for generalized zero-shot retrieval across domains.
