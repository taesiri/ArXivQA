# GlyphDiffusion: Text Generation as Image Generation

## What is the central research question or hypothesis that this paper addresses?

This paper proposes a new approach for conditional text generation using diffusion models, called GlyphDiffusion. The key idea is to render the target text as an image containing visual language content (glyph image) so that text generation can be cast as image generation. In this way, continuous diffusion models can be naturally leveraged for discrete text generation without needing to convert tokens to embeddings (which can cause training instability). The central hypothesis is that rendering text as images is an effective way to apply and adapt continuous diffusion models to conditional text generation tasks. The experiments aim to validate whether GlyphDiffusion can achieve strong performance for text generation compared to autoregressive, non-autoregressive and other diffusion models.In summary, the central research question is: Can rendering target text as images allow continuous diffusion models to be effectively adapted for conditional text generation? The key hypothesis is that the proposed GlyphDiffusion approach of generating glyph images guided by text conditions can enable high-quality and diverse text generation.


## What is the main contribution of this paper?

Based on my understanding, the main contribution of this paper is proposing a novel approach to apply continuous diffusion models to text generation by rendering target texts as glyph images. Specifically, the key ideas and contributions are:- The paper proposes to render a target text as a glyph image containing visual language content. In this way, conditional text generation can be naturally formulated as a glyph image generation task. This allows leveraging continuous diffusion models for discrete text generation without modifying the models.- The paper introduces a cascaded diffusion architecture with a base and a super-resolution model to generate high-fidelity glyph images conditioned on the input text semantics. It utilizes classifier-free guidance to enhance the conditioning.- A text grounding model is designed to transform the visual content in generated glyph images into natural language text while considering the overall semantics. - Through experiments on four text generation tasks, the proposed GlyphDiffusion model demonstrates strong performance and outperforms previous autoregressive, non-autoregressive, and diffusion models in terms of both quality and diversity metrics.- The idea and approach of rendering target text as images to enable continuous diffusion models is novel for text generation. It explores a new direction to apply diffusion models to discrete data like text.In summary, the key contribution is proposing and validating a conceptually simple but effective approach to adapt continuous diffusion models for conditional text generation by establishing a semantic mapping from text to visual language content. The overall model design and the empirical results demonstrate the efficacy of this idea.
