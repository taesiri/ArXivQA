# [Image Captioners Are Scalable Vision Learners Too](https://arxiv.org/abs/2306.07915)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be: How does image captioning as a pretraining task compare to contrastive image-text pretraining for learning general vision representations, when carefully controlling for training data, compute, and model capacity?The key hypotheses tested in the paper are:1) Image captioning alone can produce competitive vision representations compared to contrastive pretraining, despite prior work showing contrastive pretraining to be superior. 2) Image captioning exhibits favorable scaling behavior as model size and data scale increase, being competitive or surpassing contrastive pretraining.3) Models pretrained with captioning are better suited for some downstream vision-language tasks like VQA compared to contrastively pretrained models.So in summary, the main research question is re-evaluating image captioning as a pretraining strategy compared to contrastive pretraining under controlled setups, testing if conclusions from prior work still hold. The key hypotheses are that captioning can be competitive or better, especially as scale increases, and is well-suited for certain downstream tasks.


## What is the main contribution of this paper?

Based on my reading, the main contributions of this paper seem to be:- Performing a systematic comparison between image captioning and contrastive learning for pre-training vision representations, carefully matching factors like model capacity, compute, and training data. - Showing that a simple encoder-decoder Transformer trained on image captioning can produce vision encoders competitive with or even superior to contrastively trained ones like CLIP when transferred to various downstream vision and vision-language tasks.- Introducing a mixed captioning training procedure called CapPa that interleaves standard auto-regressive prediction with parallel prediction of the full caption. This further improves the quality of the vision encoder compared to standard captioning.- Analyzing the effect of model capacity, training data size, encoder architecture etc. on the relative performance of contrastive and captioning-based pre-training.- Demonstrating that captioning-based encoders have favorable properties for fine-grained classification, combining well with frozen language models, and handling detailed image descriptions.So in summary, the main contribution seems to be showing that image captioning alone, when implemented properly, can be a surprisingly effective pre-training task for learning general visual representations, despite some previous beliefs. The paper provides extensive experimental evidence across various model settings and tasks.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper: The paper presents an empirical comparison of image captioning and contrastive vision-text pretraining strategies for learning general visual representations, finding that captioning alone can produce competitive vision encoders.
