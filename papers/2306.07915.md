# [Image Captioners Are Scalable Vision Learners Too](https://arxiv.org/abs/2306.07915)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be: How does image captioning as a pretraining task compare to contrastive image-text pretraining for learning general vision representations, when carefully controlling for training data, compute, and model capacity?The key hypotheses tested in the paper are:1) Image captioning alone can produce competitive vision representations compared to contrastive pretraining, despite prior work showing contrastive pretraining to be superior. 2) Image captioning exhibits favorable scaling behavior as model size and data scale increase, being competitive or surpassing contrastive pretraining.3) Models pretrained with captioning are better suited for some downstream vision-language tasks like VQA compared to contrastively pretrained models.So in summary, the main research question is re-evaluating image captioning as a pretraining strategy compared to contrastive pretraining under controlled setups, testing if conclusions from prior work still hold. The key hypotheses are that captioning can be competitive or better, especially as scale increases, and is well-suited for certain downstream tasks.
