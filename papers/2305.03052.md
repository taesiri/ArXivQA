# [Tracking through Containers and Occluders in the Wild](https://arxiv.org/abs/2305.03052)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the main research question this paper seeks to address is: how capable are current computer vision systems at tracking objects through heavy occlusion and containment, and what progress needs to be made for them to attain a robust sense of object permanence like humans? Specifically, the paper introduces a new benchmark dataset and approach for evaluating and training neural network models on their ability to track objects that become occluded or contained inside other objects in videos. It compares two recent video transformer models on this challenging task and finds they can be surprisingly capable under certain conditions, but there still remains a considerable performance gap compared to human abilities for spatial reasoning about object permanence in complex realistic settings.The key research contributions appear to be:- Proposing a new comprehensive benchmark video dataset with dense annotations to support studying occlusion and containment events during object tracking.- Introducing a structured task and output representation for models to predict the target object, occluder, and container masks over time. - Evaluating recent state-of-the-art video transformers on this dataset and analyzing where they succeed and fail.- Identifying remaining challenges like handling nested or moving containers that future work should focus on to reach human-level visual reasoning for object permanence.In summary, the central research question is assessing and pushing towards closing the gap between current computer vision systems and humans in terms of reasoning about object persistence under occlusion or containment events in videos. The paper introduces a new benchmark and approach to support this investigation.
