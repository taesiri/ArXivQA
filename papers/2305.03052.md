# [Tracking through Containers and Occluders in the Wild](https://arxiv.org/abs/2305.03052)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the main research question this paper seeks to address is: how capable are current computer vision systems at tracking objects through heavy occlusion and containment, and what progress needs to be made for them to attain a robust sense of object permanence like humans? Specifically, the paper introduces a new benchmark dataset and approach for evaluating and training neural network models on their ability to track objects that become occluded or contained inside other objects in videos. It compares two recent video transformer models on this challenging task and finds they can be surprisingly capable under certain conditions, but there still remains a considerable performance gap compared to human abilities for spatial reasoning about object permanence in complex realistic settings.The key research contributions appear to be:- Proposing a new comprehensive benchmark video dataset with dense annotations to support studying occlusion and containment events during object tracking.- Introducing a structured task and output representation for models to predict the target object, occluder, and container masks over time. - Evaluating recent state-of-the-art video transformers on this dataset and analyzing where they succeed and fail.- Identifying remaining challenges like handling nested or moving containers that future work should focus on to reach human-level visual reasoning for object permanence.In summary, the central research question is assessing and pushing towards closing the gap between current computer vision systems and humans in terms of reasoning about object persistence under occlusion or containment events in videos. The paper introduces a new benchmark and approach to support this investigation.


## What is the main contribution of this paper?

Based on my reading, the main contributions of this paper are:1. Proposing a new task and dataset for evaluating object permanence capabilities of neural networks. The paper introduces the TCOW benchmark, which consists of both synthetic (Kubric) and real-world (Rubric) videos depicting objects undergoing occlusion and containment events. 2. Defining the task of not just tracking a target object, but also explicitly segmenting the occluder or container when the target becomes occluded or contained. This provides a more complete understanding of the target's situation.3. Evaluating two recent video transformer models (AOT and TimeSFormer) on this new benchmark. Through analysis, the paper shows these models can track objects reasonably well in certain settings, but there is still a considerable performance gap compared to human abilities in terms of robustly reasoning about object permanence.4. Providing thorough ablation studies to understand where the performance and generalization of the models come from. This suggests focusing on occluders/containers rather than precisely localizing invisible targets is often more effective.5. Releasing the novel benchmark to the community to facilitate further research on this challenging problem central to attaining strong spatial reasoning in computer vision.In summary, the main contribution is proposing a new task formulation, dataset, and benchmark to push towards visual reasoning systems that have a proper sense of object permanence, which remains an open challenge for current methods.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper introduces TCOW, a new benchmark and model for visual tracking of objects through heavy occlusion and containment, using both synthetic and real datasets to evaluate transformer-based video models on their ability to reason about object permanence.
