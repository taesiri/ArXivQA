# [Metric-aware LLM inference](https://arxiv.org/abs/2403.04182)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Large language models (LLMs) show strong performance on NLP tasks when evaluated on the exact match (EM) metric. 
- However, many tasks use different evaluation metrics like squared error, F1 score etc.
- Current LLM inference via sampling the most likely target works well for EM but is suboptimal for other metrics.

Proposed Solution: 
- Propose metric-aware LLM inference (MALI) to optimize a custom metric at inference time.  
- Formulate the inference as a decision theoretic problem to maximize expected score.
- Derive optimal rules for common metrics like squared error, F1 etc.
- Approximate the expectation over LLM predictions via sampling. 

Main Contributions:
- Show across datasets that sampling just the top prediction is suboptimal for metrics like RMSE. Taking the mean/median of samples works better.
- For text metrics like F1, optimize over candidates derived from the samples.
- Analyze tradeoffs wrt number of samples, temperature scaling etc.
- Show gains from MALI over strong baselines like greedy decoding across models, metrics and datasets in a zero-shot setting.
- Point out that calibration of LLM distributions is crucial for effective application of MALI.

In summary, the paper proposes a principled technique to adapt LLM inference to the evaluation metric of interest that gives noticeable gains over current approaches. The decision theoretic view provides a valuable framework for metric-aware decoding.
