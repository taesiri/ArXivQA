# Language Models of Code are Few-Shot Commonsense Learners

## What is the central research question or hypothesis that this paper addresses?

The central hypothesis of this paper is that large language models of code (code-LLMs) can be effective for structured commonsense reasoning tasks, even though they are trained on code rather than natural language text. The key claims are:1) Code-LLMs like Codex are better at structured commonsense reasoning compared to natural language LMs like GPT-3 when the output structure is represented as code rather than text.2) Converting structured commonsense reasoning tasks into code generation allows leveraging the strengths of code-LLMs for both programming constructs and natural language.3) Code-LLMs show strong few-shot learning for structured commonsense reasoning when provided just a few examples formatted as code snippets.4) Code-LLMs outperform fine-tuned natural language models on structured commonsense tasks when controlling for the amount of in-domain training data.5) Factors like model size, prompt design, and dynamically selecting examples impact the efficacy of code-LLMs on these tasks.In summary, the central hypothesis is that code-LLMs are well-suited for structured commonsense reasoning when the task is framed as generating code rather than text. The experiments on three diverse tasks seem to validate this claim in a few-shot learning setup.


## What is the main contribution of this paper?

The main contribution of this paper is demonstrating that large language models of code (LLMCs) are better at structured commonsense reasoning tasks than natural language LMs. The key ideas are:- Existing methods for structured commonsense reasoning tasks like script generation and graph generation convert the structured output into text, which is unnatural for LMs pre-trained on free-form text. - Instead, the authors propose representing the structured output as code, which is more natural for LLMCs like Codex that are pre-trained on large amounts of code.- They show on three diverse structured commonsense reasoning tasks (ProScript, ProPara, ExplaGraphs) that using Codex with code representations of the output vastly outperforms LMs like GPT-3 and T5 fine-tuned on the tasks. - With only a few examples, Codex reaches or exceeds the performance of models trained on orders of magnitude more data.- The gains come from both using a LLMC and representing the output as structured code. Using GPT-3 with code also improves over text.- Analysis shows the approach benefits from larger models, and is less sensitive to prompt format details.In summary, the key insight is that LLMCs are better structured reasoners if provided examples in a code format close to their pre-training data, instead of trying to force natural language LMs to generate structured outputs. This demonstrates the promise of leveraging LLMCs for symbolic reasoning.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence TL;DR of the paper:The paper shows that framing structured commonsense reasoning tasks as code generation allows large language models of code, like Codex, to outperform natural language models fine-tuned on the tasks, demonstrating code models' effectiveness for structured reasoning.
