# [Language Models of Code are Few-Shot Commonsense Learners](https://arxiv.org/abs/2210.07128)

## What is the central research question or hypothesis that this paper addresses?

 The central hypothesis of this paper is that large language models of code (code-LLMs) can be effective for structured commonsense reasoning tasks, even though they are trained on code rather than natural language text. 

The key claims are:

1) Code-LLMs like Codex are better at structured commonsense reasoning compared to natural language LMs like GPT-3 when the output structure is represented as code rather than text.

2) Converting structured commonsense reasoning tasks into code generation allows leveraging the strengths of code-LLMs for both programming constructs and natural language.

3) Code-LLMs show strong few-shot learning for structured commonsense reasoning when provided just a few examples formatted as code snippets.

4) Code-LLMs outperform fine-tuned natural language models on structured commonsense tasks when controlling for the amount of in-domain training data.

5) Factors like model size, prompt design, and dynamically selecting examples impact the efficacy of code-LLMs on these tasks.

In summary, the central hypothesis is that code-LLMs are well-suited for structured commonsense reasoning when the task is framed as generating code rather than text. The experiments on three diverse tasks seem to validate this claim in a few-shot learning setup.


## What is the main contribution of this paper?

 The main contribution of this paper is demonstrating that large language models of code (LLMCs) are better at structured commonsense reasoning tasks than natural language LMs. The key ideas are:

- Existing methods for structured commonsense reasoning tasks like script generation and graph generation convert the structured output into text, which is unnatural for LMs pre-trained on free-form text. 

- Instead, the authors propose representing the structured output as code, which is more natural for LLMCs like Codex that are pre-trained on large amounts of code.

- They show on three diverse structured commonsense reasoning tasks (ProScript, ProPara, ExplaGraphs) that using Codex with code representations of the output vastly outperforms LMs like GPT-3 and T5 fine-tuned on the tasks. 

- With only a few examples, Codex reaches or exceeds the performance of models trained on orders of magnitude more data.

- The gains come from both using a LLMC and representing the output as structured code. Using GPT-3 with code also improves over text.

- Analysis shows the approach benefits from larger models, and is less sensitive to prompt format details.

In summary, the key insight is that LLMCs are better structured reasoners if provided examples in a code format close to their pre-training data, instead of trying to force natural language LMs to generate structured outputs. This demonstrates the promise of leveraging LLMCs for symbolic reasoning.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence TL;DR of the paper:

The paper shows that framing structured commonsense reasoning tasks as code generation allows large language models of code, like Codex, to outperform natural language models fine-tuned on the tasks, demonstrating code models' effectiveness for structured reasoning.


## How does this paper compare to other research in the same field?

 This paper presents a novel method for using language models to perform structured commonsense reasoning tasks. Here are the key differences compared to prior work:

- Most prior work has used natural language models like GPT-3 and fine-tuned them on structured reasoning datasets. However, the paper shows that code generation models like Codex actually perform better on these tasks when the output is formatted as code, without any task-specific fine-tuning.

- The key insight is that the syntax trees and control flows inherent in code provide an inductive bias that is very useful for structured and logical reasoning. Code generation models leverage this, whereas natural language models struggle with the "unnatural" formatting of serialized graphs.

- By representing outputs as Python classes/functions rather than serialized node-edge lists, the paper shows it is possible to get strong results from Codex in a pure few-shot prompting setup. Prior work has typically required full fine-tuning.

- The approach is demonstrated to work well across three diverse graph generation tasks - script generation, entity tracking, and argument explanation graphs. The consistent gains show the approach generalizes.

- Compared to prior work on leveraging Codex for reasoning (e.g. math formalization), this paper's novelty is in using code generation for more free-form commonsense tasks with less rigid outputs.

- The analysis reveals that both code prompting and code models are important, and that larger models are less sensitive to prompt engineering. This provides useful insights.

In summary, the key novelty is using code generation models out-of-the-box via careful prompt design, rather than fine-tuning natural language models. The paper shows this is a simple but surprisingly effective approach for structured commonsense reasoning across diverse tasks.


## What future research directions do the authors suggest?

 The authors suggest the following future research directions:

- Exploring the efficacy of structured generation methods in cross-lingual settings. The authors note that they only experimented with English language datasets in this work. Applying the methods to other languages would be an interesting direction.

- Investigating whether fine-tuning a code language model (Code-LLM) on large amounts of natural language text data can preserve its structured generation capabilities. The authors point out that their analysis shows prompt design becomes less critical for very large models, so it's worth exploring if fine-tuning can strike a balance between language and structured reasoning skills.

- Addressing the limitations around using proprietary models like Codex, GPT-3, etc. The authors note issues around model transparency and potential lack of future access. Using more open models or releasing insights into what coding schemes work best would help advance research in this area.

- Applying the principles and methods to other NLP tasks requiring language understanding and structured prediction. The authors believe their approach of framing tasks as code generation could be useful for a broader set of applications.

In summary, the main future work suggested is exploring how their approach could be extended to other languages, tasks, and models, as well as investigating how fine-tuning affects the structured reasoning abilities of Code-LLMs. Overcoming issues around model opacity is also highlighted.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the paper:

The paper proposes a method called CoCoGen for leveraging large language models of code (Code-LLMs) for structured commonsense reasoning tasks. The key idea is to represent the structured outputs (e.g. graphs) of these tasks as Python code, which is more similar to the pre-training data of Code-LLMs compared to standard textual representations. The paper demonstrates CoCoGen on three diverse structured reasoning tasks - script generation, entity state tracking, and explanation graph generation. Across all tasks, Code-LLMs like Codex prompted with a few examples in Python code vastly outperform natural language LMs (e.g. GPT-3) prompted similarly and even fine-tuned transformers, while using orders of magnitude less training data. The proposed code representations allow Code-LLMs to effectively leverage their reasoning and code generation abilities for graph prediction. Overall, the paper highlights that Code-LLMs are highly effective few-shot learners for structured commonsense reasoning when the problem is framed as code generation.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the paper:

This paper presents a method called CoCoGen for using code generation language models (Code-LLMs) for structured commonsense reasoning tasks. The key idea is to represent the structured outputs, such as graphs, as Python code rather than natural language text. This allows leveraging Code-LLMs like Codex which are pretrained on large amounts of code. The paper shows through experiments on three diverse structured reasoning tasks - script generation, entity state tracking, and explanation graph generation - that Code-LLMs significantly outperform natural language LMs fine-tuned on the target tasks. The Code-LLMs are used in a few-shot prompting setup with only around 15 examples, yet achieve better performance than LMs trained on thousands of examples. Analyses reveal that both using a Code-LLM and representing the output as structured code contribute to the improved performance. The results demonstrate that Code-LLMs can effectively perform complex structured reasoning, despite being trained only on code.

In summary, this paper presents a novel method CoCoGen that uses Code-LLMs for structured commonsense reasoning by representing the graph outputs as code. Experiments on three structured reasoning tasks show Code-LLMs like Codex outperform even fine-tuned natural language LMs when given just 15 examples. The key insight is that Code-LLMs are better structured reasoners when the outputs are formatted as code similar to their pretraining data. The results open up new possibilities for effectively applying Code-LLMs to diverse reasoning tasks.


## Summarize the main method used in the paper in one paragraph.

 The paper presents a method to leverage large language models of code (Code-LLMs) for structured commonsense reasoning tasks. The key idea is to convert the structured outputs (e.g. graphs) into equivalent Python code, instead of serializing them into a flat text format. This allows taking advantage of the pre-training of Code-LLMs like Codex on large amounts of Python code. Specifically, the structured outputs are converted into Python classes or functions, with the nodes and edges represented using standard programming constructs like objects, attributes, lists, and function calls. 

For a given input text, a few training examples are converted into equivalent Python code and used as prompts for Codex in few-shot setting. The test input is also converted into partial Python code and appended to the prompt for Codex to complete. The completed Python code is then converted back into the desired structured output. Experiments on three structured reasoning tasks show Codex with Python prompts significantly outperforms LMs like GPT-3 with text prompts in few-shot setting. The paper demonstrates the effectiveness of leveraging programming knowledge in Code-LLMs for commonsense reasoning.


## What problem or question is the paper addressing?

 The key question this paper is addressing is: can large language models of code be effectively leveraged for structured commonsense reasoning tasks? 

The paper highlights that existing approaches try to convert structured outputs like graphs into text in order to use standard natural language models. However, the paper argues that the serialized graph outputs are very different from the natural language corpora these models are trained on. As a result, language models struggle to generate good outputs for structured commonsense tasks.

To address this issue, the paper proposes using large language models that are pre-trained on code, instead of natural language text. The key idea is to convert the desired structured output into code, which is more similar to what code models have seen during pre-training. The paper shows through experiments on three structured commonsense tasks that code language models significantly outperform standard language models when evaluated in a few-shot setting.

In summary, the key question is whether code language models can be more effective for structured commonsense reasoning compared to standard natural language models. The paper provides evidence that this is indeed the case when the output is appropriately encoded as code.
