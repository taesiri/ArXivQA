# [Just Ask: Learning to Answer Questions from Millions of Narrated Videos](https://arxiv.org/abs/2012.00451)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question it aims to address is: How can we develop an approach to train video question answering (VideoQA) models without requiring a large amount of manually annotated visual data?The key points are:- Current VideoQA models rely on training datasets of video-question-answer triplets that are costly and time-consuming to collect at scale. This limits their performance.- The authors propose a method to automatically generate a large-scale VideoQA training dataset by leveraging cross-modal supervision and transformer models trained on text-only QA data. - Their approach applies the text QA models to narrated videos to generate video-question-answer triplets, creating a dataset of 69M examples.- They show this automatic generation approach allows training a VideoQA model that achieves state-of-the-art results by pretraining on their generated dataset and finetuning on existing benchmarks.- Their method removes the need for manually annotated visual data during training, enabling large-scale VideoQA training.So in summary, the core research question is how to train VideoQA models without relying on large manually annotated visual datasets, which they address through automatic generation of training data.


## What is the main contribution of this paper?

The main contribution of this paper is the introduction of an approach to automatically generate a large-scale video question answering (VideoQA) dataset, called HowToVQA69M. The key points are:- They propose a method to generate video-question-answer triplets from narrated videos using cross-modal supervision. Specifically, they leverage transformer models trained on text-only question-answering data to generate questions and answers from the speech transcripts of narrated instructional videos. - Applying this approach to 1.2M videos from HowTo100M dataset, they create HowToVQA69M, which contains 69M video-question-answer triplets. This is two orders of magnitude larger than existing VideoQA datasets.- To handle the diverse open-ended answers, they introduce a training procedure using contrastive learning between a video-question transformer and an answer transformer. This allows scaling to the large vocabulary of answers.- They show strong zero-shot VideoQA results without using any manually annotated visual data during training, demonstrating the generalization of their approach.- Finetuning their model pretrained on HowToVQA69M leads to new state-of-the-art results on several downstream VideoQA datasets including MSRVTT-QA, MSVD-QA, ActivityNet-QA and How2QA.- They also introduce a new VideoQA benchmark dataset called iVQA with manually verified questions and multiple annotated answers per question, for detailed evaluation.In summary, the main contribution is an automatic approach to generate large-scale training data for open-ended VideoQA and models that leverage this data to advance the state of the art on multiple VideoQA benchmarks.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper introduces an approach to automatically generate large-scale training data for video question answering by leveraging language models and cross-modal supervision, and shows strong results on multiple datasets including a proposed zero-shot evaluation.
