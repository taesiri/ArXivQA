# [Robust CLIP: Unsupervised Adversarial Fine-Tuning of Vision Embeddings   for Robust Large Vision-Language Models](https://arxiv.org/abs/2402.12336)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
Recent large vision-language models (VLMs) like LLaVA and OpenFlamingo show great promise on various vision-and-language tasks. However, they rely on a frozen CLIP vision encoder which makes them highly vulnerable to adversarial attacks on images. Prior work has shown that imperceptible image perturbations can be used for targeted attacks to make the VLM output anything the attacker wants. This is concerning as it allows spreading fake information or defrauding users at scale. Thus making VLMs robust against such attacks is an important problem.

Proposed Solution:
The authors propose an unsupervised adversarial fine-tuning method called FARE to make the CLIP vision encoder robust. The key idea is to minimize the $\ell_2$ distance between original and fine-tuned CLIP embeddings to preserve the original features. This allows plugging in the robust CLIP encoder into downstream VLMs without needing to retrain them. Extensive experiments show that FARE leads to adversarially robust CLIP encoders while maintaining high clean performance on various tasks compared to prior robust CLIP models like TeCoA.

Key Contributions:
- Propose FARE, an unsupervised adversarial fine-tuning scheme to make CLIP robust that preserves original features well
- Show ROS-CLIP encoders can be directly substituted in VLMs like LLaVA and OpenFlamingo making them robust without any VLM retraining
- Demonstrate high clean & robust performance on various vision-language tasks compared to prior robust CLIP (TeCoA) 
- Show that targeted imperceptible attacks no longer succeed against LLaVA when using robust CLIP from FARE
- Enable easy way to make VLMs robust against image attacks, facilitating their safe deployment

In summary, the paper makes VLMs robust to adversarial image perturbations via robustifying CLIP in an unsupervised way without hurting downstream performance. This is an important step towards safely deploying powerful but vulnerable VLMs.
