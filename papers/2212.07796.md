# [CREPE: Can Vision-Language Foundation Models Reason Compositionally?](https://arxiv.org/abs/2212.07796)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the main research question being addressed is: 

Can existing large-scale vision-language models demonstrate compositional reasoning abilities? Specifically, do they exhibit systematicity (the ability to recombine known components to understand novel combinations) and productivity (the ability to understand increasingly complex inputs)?

The key hypotheses seem to be:

1) Current vision-language models struggle with systematic generalization, and will perform worse when tested on novel visual concept compositions compared to seen compositions.

2) Vision-language models' ability to correctly comprehend scenes will degrade as the complexity of the textual description increases.

To evaluate these hypotheses, the authors introduce a new benchmark called CREPE that tests systematicity via seen/unseen splits of compounds, and productivity by testing on captions of increasing complexity. The experiments assess whether current models exhibit the expected deficiencies in these two aspects of compositional reasoning.

So in summary, the paper sets out to probe whether existing vision-language models actually demonstrate robust compositional abilities, or whether their strong performance on many downstream tasks may be relying more on simply recognizing familiar patterns. The introduction of CREPE provides a way to directly test models for these hypothesized limitations.


## What is the main contribution of this paper?

 This paper introduces a new benchmark called CREPE (Compositional REPresentation Evaluation) to evaluate the compositional reasoning abilities of vision-language foundation models. The key contributions are:

- It proposes evaluating two important aspects of compositionality from cognitive science - systematicity and productivity. 

- It introduces large-scale datasets to test systematicity and productivity via image-text retrieval. For systematicity, it provides three splits to test generalization to seen compounds, unseen compounds, and unseen atoms. For productivity, it generates examples across 9 levels of complexity.

- It generates hard negatives for the datasets corresponding to different failure modes like ignoring incorrect atoms or improper binding of atoms. 

- It evaluates various CLIP-style models on these datasets and finds that they struggle at compositional reasoning, with performance dropping on unseen compounds and higher complexity.

- The benchmark and datasets are released publicly to facilitate future research on improving compositionality of vision-language models.

In summary, the key contribution is introducing a rigorous benchmark to evaluate compositionality of vision-language models, demonstrating current models lack strong compositional abilities. The datasets and analysis provide a means to make progress on this important capability.
