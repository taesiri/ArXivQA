# [Dynamic Memory Based Adaptive Optimization](https://arxiv.org/abs/2402.15262)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key aspects of the paper:

Problem:
The paper investigates how optimizers for machine learning models can utilize additional memory units to store useful information during training. Specifically, it examines how more than the typical 0-2 memory units used by SGD and Adam can improve optimization performance. The key research questions are: How can optimizers make use of more memory units? What information should be stored in them? How to use the memory units effectively for the learning steps?

Proposed Solution: 
The paper introduces a general framework called "Retrospective Learning Law Correction" (RLLC) for calculating a linear combination of arbitrary memory units to make a learning step. The core idea is to use the latest gradient to correct this "learning law" by projecting the gradient onto the memory units. This allows the contribution of each unit to be adjusted in a smooth, adaptive way during training.

The paper focuses on memory units with simple linear update rules, governed by matrices B (transition) and vector a (gradient influence). Building on theory around Jordan normal forms, the notion of "propagators" is introduced - these are bundles of linearly updated memory units, including momentum vectors, complex momentum vectors, and higher-order analogues. 

By combining propagators and applying RLLC, versatile new optimizers are created. A key insight is that mixing SGD and momentum SGD propagators allows continuous, adaptive interpolation between SGD, momentum SGD, and Nesterov accelerated gradient descent.

Main Contributions:

1) The RLLC framework for correcting learning steps via projection of gradients onto memory units

2) Introduction of Jordan propagator bundles as modular memory units with linear update rules

3) Construction of new optimizers by mixing propagators and applying RLLC 

4) Empirical evaluation showing versions with >2 memory units can outperform Adam and momentum SGD

5) Conceptual bridge between memory and adaptivity in optimization, opening rich possibilities for future work

The paper provides both theoretical grounding and promising experimental results to demonstrate the potential of using more sophisticated memory mechanisms during optimization.
