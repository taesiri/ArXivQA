# [Next3D: Generative Neural Texture Rasterization for 3D-Aware Head   Avatars](https://arxiv.org/abs/2211.11208)

## What is the central research question or hypothesis that this paper addresses?

 The central research question this paper addresses is how to develop an editable 3D-aware generator that can synthesize high-fidelity, view-consistent portraits while also supporting local editing of facial attributes.  

The key hypotheses are:

1. Disentangling shape and texture representations enables independent control over geometry and appearance during image generation.

2. Aligning facial semantics, geometry, and texture in a spatial 3D volume allows using semantic guidance to edit the portrait images. 

3. Training on widely available monocular images paired with semantic masks, rather than scarce multi-view data, provides sufficient supervision for learning the 3D generative model.

4. Jointly learning to predict semantics and color in the 3D volume improves geometry quality compared to learning them separately.

So in summary, the main goal is developing a generative 3D portrait model that combines the realism and editing flexibility of 2D GANs with the view consistency of 3D-aware image synthesis. The key ideas are disentangled shape/texture, aligned 3D semantics/geometry/texture, and self-supervised training with monocular data.
