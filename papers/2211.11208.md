# [Next3D: Generative Neural Texture Rasterization for 3D-Aware Head   Avatars](https://arxiv.org/abs/2211.11208)

## What is the central research question or hypothesis that this paper addresses?

 The central research question this paper addresses is how to develop an editable 3D-aware generator that can synthesize high-fidelity, view-consistent portraits while also supporting local editing of facial attributes.  

The key hypotheses are:

1. Disentangling shape and texture representations enables independent control over geometry and appearance during image generation.

2. Aligning facial semantics, geometry, and texture in a spatial 3D volume allows using semantic guidance to edit the portrait images. 

3. Training on widely available monocular images paired with semantic masks, rather than scarce multi-view data, provides sufficient supervision for learning the 3D generative model.

4. Jointly learning to predict semantics and color in the 3D volume improves geometry quality compared to learning them separately.

So in summary, the main goal is developing a generative 3D portrait model that combines the realism and editing flexibility of 2D GANs with the view consistency of 3D-aware image synthesis. The key ideas are disentangled shape/texture, aligned 3D semantics/geometry/texture, and self-supervised training with monocular data.


## What is the main contribution of this paper?

 The main contributions of this paper are:

1. It proposes FENeRF, the first portrait image generator that is locally editable and strictly view-consistent. This is achieved by learning a 3D representation where the semantics, geometry and texture are spatially-aligned.

2. It trains the generator using widely available monocular images with paired semantic masks, instead of requiring multi-view images or 3D data. This ensures data diversity and enhances the representation ability. 

3. It shows that joint learning of the semantic and texture volumes helps generate finer 3D geometry. 

4. Experiments demonstrate that FENeRF outperforms state-of-the-art methods on various face editing tasks while maintaining strict 3D consistency.

In summary, this paper presents a novel 3D-aware GAN that can synthesize high-quality, locally editable, and view-consistent portrait images. The key idea is the proposed spatially-aligned 3D representation that aligns semantics, geometry and texture. This enables semantic-guided editing while ensuring view consistency. The generator is trained with abundant monocular image data instead of multi-view supervision. Experiments show state-of-the-art performance on tasks like inversion, editing, and style transfer.
