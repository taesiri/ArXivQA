# [Logarithmic Pruning is All You Need](https://arxiv.org/abs/2006.12156)

## What is the central research question or hypothesis that this paper addresses?

The central research question addressed in this paper is:How large must a randomly initialized neural network be in order to contain subnetworks that can approximate any given target network to high accuracy, without any training?Specifically, the paper aims to provide tighter theoretical bounds on the size of the randomly initialized "large" network compared to the target network. This is formalizing the idea of the "lottery ticket hypothesis", which states that large neural networks contain small subnetworks that can match the accuracy of the full network. The key results of the paper are:- They provide a general lemma that shows how the approximation error propagates through the layers of a network based on the accuracy of approximating the individual weights. This gives insight into how the different variables like depth, width, weight norms, etc. impact the required accuracy.- They show that by using a hyperbolic distribution to sample weights (instead of a uniform distribution as in prior work), the overparameterized network only needs O(log n) neurons per weight of the target network. This is much tighter than previous polynomial bounds.- They remove the assumption on bounding weight norms from prior work and show the bound becomes O(L log n) where L is the depth. - Overall, the paper gives an improved theoretical understanding of how overparameterization allows neural networks to contain highly accurate subnetworks, formalizing the lottery ticket hypothesis. The key insight is using a hyperbolic weight distribution for tighter logarithmic bounds.
