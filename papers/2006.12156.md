# [Logarithmic Pruning is All You Need](https://arxiv.org/abs/2006.12156)

## What is the central research question or hypothesis that this paper addresses?

The central research question addressed in this paper is:How large must a randomly initialized neural network be in order to contain subnetworks that can approximate any given target network to high accuracy, without any training?Specifically, the paper aims to provide tighter theoretical bounds on the size of the randomly initialized "large" network compared to the target network. This is formalizing the idea of the "lottery ticket hypothesis", which states that large neural networks contain small subnetworks that can match the accuracy of the full network. The key results of the paper are:- They provide a general lemma that shows how the approximation error propagates through the layers of a network based on the accuracy of approximating the individual weights. This gives insight into how the different variables like depth, width, weight norms, etc. impact the required accuracy.- They show that by using a hyperbolic distribution to sample weights (instead of a uniform distribution as in prior work), the overparameterized network only needs O(log n) neurons per weight of the target network. This is much tighter than previous polynomial bounds.- They remove the assumption on bounding weight norms from prior work and show the bound becomes O(L log n) where L is the depth. - Overall, the paper gives an improved theoretical understanding of how overparameterization allows neural networks to contain highly accurate subnetworks, formalizing the lottery ticket hypothesis. The key insight is using a hyperbolic weight distribution for tighter logarithmic bounds.


## What is the main contribution of this paper?

The main contribution of this paper is proving strong theoretical guarantees for the lottery ticket hypothesis. Specifically, the paper shows that a large randomly initialized neural network only needs a logarithmic factor more neurons per weight compared to a target subnetwork in order to contain good subnetworks that can approximate the target to high accuracy after pruning. Key contributions:- Provides significantly tighter bounds on the overparameterization needed compared to prior work, improving from polynomial to logarithmic in the number of layers, number of neurons per layer, and error tolerance. - Removes limiting assumptions made in prior work, such as bounds on the spectral norms of the weight matrices.- Shows the logarithmic factor above while keeping all other dependencies only logarithmic, compared to prior polynomial dependencies.- Achieves this by using novel proof techniques exploiting the composability of neural nets, including binary decomposition of weights, product weight distributions, and batch sampling.In summary, the paper makes significant theoretical progress on an important open problem regarding neural network overparameterization, proving that randomness and pruning together can extract high-accuracy subnetworks using far fewer parameters than previously thought necessary. This provides valuable insight into the power of overparameterization and the role of pruning.
