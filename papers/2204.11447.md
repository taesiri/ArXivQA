# [Evaluating Interpolation and Extrapolation Performance of Neural   Retrieval Models](https://arxiv.org/abs/2204.11447)

## What is the central research question or hypothesis that this paper addresses?

Based on my understanding, the central research question this paper tries to address is: how to properly evaluate the interpolation and extrapolation performance of neural ranking models? The key points are:1. The paper first defines interpolation and extrapolation for ranking models based on query similarity - whether the test queries are similar to or distinct from the training queries. 2. It then investigates popular benchmarks like MS MARCO and TREC DL and finds they are biased towards evaluating interpolation due to considerable overlap between training and test queries.3. To address this issue, the paper proposes two resampling methods to construct new training-test splits that can evaluate interpolation and extrapolation performance separately.4. Experiments are conducted to validate the proposed methods. Results show extrapolation performance aligns better with out-of-distribution generalization ability, demonstrating the efficacy of the proposed evaluation protocol.  5. The methods are used to re-evaluate various ranking models. Comparisons reveal models behave differently in interpolation and extrapolation regimes, highlighting the importance of separated evaluation.In summary, the central hypothesis is that interpolation and extrapolation are two distinct capacities of ranking models, and existing benchmarks fail to accurately evaluate extrapolation. The paper proposes query similarity based resampling methods to address this issue and demonstrates their efficacy. The overall goal is to properly evaluate interpolation and extrapolation performance separately.


## What is the main contribution of this paper?

The main contribution of this paper is proposing a new evaluation protocol to separately assess the interpolation and extrapolation performance of neural ranking models. The key ideas are:- The paper first defines interpolation and extrapolation in information retrieval based on query similarity between training and test data. Interpolation occurs when test queries are similar to training queries, while extrapolation occurs when test queries are novel compared to training. - It then investigates popular IR benchmarks like MS MARCO and TREC DL and finds they are biased towards interpolation evaluation due to considerable query overlap between training and test sets.- To enable separate evaluation of interpolation and extrapolation, the paper proposes two resampling strategies. They construct new training-test splits by sampling similar or dissimilar queries. Models are trained and evaluated on the resampled data.- The proposed evaluation protocol is shown to align well with out-of-distribution generalization ability, despite being limited to the original data distribution.- By re-evaluating various neural ranking models, the paper shows models behave differently in interpolation and extrapolation regimes. For example, representation-based models excel at interpolation but struggle at extrapolation.In summary, the key contribution is identifying the need for separate interpolation/extrapolation evaluation in IR, proposing an effective resampling-based evaluation protocol, and providing new insights into model behaviors through re-evaluation. The work provides a new perspective for analyzing neural ranking models.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper proposes new methods to evaluate neural ranking models by resampling training and test data to separately measure interpolation and extrapolation performance, finding that models behave differently in the two regimes so it is important to evaluate both capacities.


## How does this paper compare to other research in the same field?

This paper makes several key contributions to the field of evaluating neural ranking models:1. It highlights the issue of test-train similarity in popular IR benchmarks like MS MARCO and TREC DL, showing significant overlap in query entities, intents, and relevance labels between the training and test data. This implies the benchmarks are more suited to evaluating interpolation performance rather than extrapolation. 2. It proposes two novel resampling methods (ReSTrain and ReSTTest) to construct new training/test splits that enable separate evaluation of interpolation and extrapolation capacities on existing benchmarks. This is a simple yet effective approach that does not require any new annotations.3. It provides an analysis of how various neural ranking models (interaction-based vs representation-based) perform differently in interpolation and extrapolation regimes. Key findings are that representation-based models rely more on memorization and struggle to extrapolate compared to interaction-based models.4. It studies the impact of different training techniques like hard negatives, distillation, and pretraining specifically on extrapolation performance. Results suggest pretraining is more beneficial for extrapolation compared to finetuning techniques.5. It validates the proposed evaluation protocol by showing in-domain extrapolation performance correlates strongly with out-of-domain generalization ability.Overall, this work provides novel perspectives and tools to analyze model capabilities in interpolation versus extrapolation. It demonstrates the importance of considering both regimes when evaluating and comparing neural ranking models, rather than just overall metrics. The analysis of model architectures and training techniques also offers useful insights into designing more robust models.Compared to related work like the BEIR benchmark, this paper offers more reliable evaluation free of annotation bias issues. The resampling strategies are also simpler and easier to apply than collecting new out-of-distribution test sets. More broadly, this work follows the trend in ML of studying interpolation vs extrapolation for model evaluation, but is one of the first to focus specifically on information retrieval tasks. The findings could significantly influence how neural ranking models are analyzed in future IR research.
