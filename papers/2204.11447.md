# [Evaluating Interpolation and Extrapolation Performance of Neural   Retrieval Models](https://arxiv.org/abs/2204.11447)

## What is the central research question or hypothesis that this paper addresses?

 Based on my understanding, the central research question this paper tries to address is: how to properly evaluate the interpolation and extrapolation performance of neural ranking models? 

The key points are:

1. The paper first defines interpolation and extrapolation for ranking models based on query similarity - whether the test queries are similar to or distinct from the training queries. 

2. It then investigates popular benchmarks like MS MARCO and TREC DL and finds they are biased towards evaluating interpolation due to considerable overlap between training and test queries.

3. To address this issue, the paper proposes two resampling methods to construct new training-test splits that can evaluate interpolation and extrapolation performance separately.

4. Experiments are conducted to validate the proposed methods. Results show extrapolation performance aligns better with out-of-distribution generalization ability, demonstrating the efficacy of the proposed evaluation protocol.  

5. The methods are used to re-evaluate various ranking models. Comparisons reveal models behave differently in interpolation and extrapolation regimes, highlighting the importance of separated evaluation.

In summary, the central hypothesis is that interpolation and extrapolation are two distinct capacities of ranking models, and existing benchmarks fail to accurately evaluate extrapolation. The paper proposes query similarity based resampling methods to address this issue and demonstrates their efficacy. The overall goal is to properly evaluate interpolation and extrapolation performance separately.


## What is the main contribution of this paper?

 The main contribution of this paper is proposing a new evaluation protocol to separately assess the interpolation and extrapolation performance of neural ranking models. The key ideas are:

- The paper first defines interpolation and extrapolation in information retrieval based on query similarity between training and test data. Interpolation occurs when test queries are similar to training queries, while extrapolation occurs when test queries are novel compared to training. 

- It then investigates popular IR benchmarks like MS MARCO and TREC DL and finds they are biased towards interpolation evaluation due to considerable query overlap between training and test sets.

- To enable separate evaluation of interpolation and extrapolation, the paper proposes two resampling strategies. They construct new training-test splits by sampling similar or dissimilar queries. Models are trained and evaluated on the resampled data.

- The proposed evaluation protocol is shown to align well with out-of-distribution generalization ability, despite being limited to the original data distribution.

- By re-evaluating various neural ranking models, the paper shows models behave differently in interpolation and extrapolation regimes. For example, representation-based models excel at interpolation but struggle at extrapolation.

In summary, the key contribution is identifying the need for separate interpolation/extrapolation evaluation in IR, proposing an effective resampling-based evaluation protocol, and providing new insights into model behaviors through re-evaluation. The work provides a new perspective for analyzing neural ranking models.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the paper:

The paper proposes new methods to evaluate neural ranking models by resampling training and test data to separately measure interpolation and extrapolation performance, finding that models behave differently in the two regimes so it is important to evaluate both capacities.


## How does this paper compare to other research in the same field?

 This paper makes several key contributions to the field of evaluating neural ranking models:

1. It highlights the issue of test-train similarity in popular IR benchmarks like MS MARCO and TREC DL, showing significant overlap in query entities, intents, and relevance labels between the training and test data. This implies the benchmarks are more suited to evaluating interpolation performance rather than extrapolation. 

2. It proposes two novel resampling methods (ReSTrain and ReSTTest) to construct new training/test splits that enable separate evaluation of interpolation and extrapolation capacities on existing benchmarks. This is a simple yet effective approach that does not require any new annotations.

3. It provides an analysis of how various neural ranking models (interaction-based vs representation-based) perform differently in interpolation and extrapolation regimes. Key findings are that representation-based models rely more on memorization and struggle to extrapolate compared to interaction-based models.

4. It studies the impact of different training techniques like hard negatives, distillation, and pretraining specifically on extrapolation performance. Results suggest pretraining is more beneficial for extrapolation compared to finetuning techniques.

5. It validates the proposed evaluation protocol by showing in-domain extrapolation performance correlates strongly with out-of-domain generalization ability.

Overall, this work provides novel perspectives and tools to analyze model capabilities in interpolation versus extrapolation. It demonstrates the importance of considering both regimes when evaluating and comparing neural ranking models, rather than just overall metrics. The analysis of model architectures and training techniques also offers useful insights into designing more robust models.

Compared to related work like the BEIR benchmark, this paper offers more reliable evaluation free of annotation bias issues. The resampling strategies are also simpler and easier to apply than collecting new out-of-distribution test sets. More broadly, this work follows the trend in ML of studying interpolation vs extrapolation for model evaluation, but is one of the first to focus specifically on information retrieval tasks. The findings could significantly influence how neural ranking models are analyzed in future IR research.


## What future research directions do the authors suggest?

 Based on my reading of the paper, some of the main future research directions suggested by the authors include:

- Incorporating dynamics of documents into definitions of interpolation and extrapolation. The current work only considers query dynamics, but documents in search engines are also constantly changing. Further incorporating document dynamics could better characterize generalization performance in practice.

- Exploring custom definitions of interpolation and extrapolation for different ranking models. The current work uses general definitions applicable to any model. Customized definitions tailored to model architectures could better align with generalization. 

- Designing ranking models robust to extrapolation in a min-max game formulation. The paper suggests optimizing models while evaluating extrapolation could be cast as a min-max game. Investigating how to design models robust to extrapolation through this lens is suggested.

- Exploring different implementations of computing query similarity for defining interpolation/extrapolation. The current work uses embedding similarity but other options like intent/entity annotation or lexical overlap could also be explored.

- Evaluating interpolation/extrapolation capacities for other neural ranking models besides the ones studied. The methodology could be applied to other models like cross-encoders.

- Considering both query and document dynamics. The current work only looks at query dynamics but new/changing documents also affect generalization in practice.

In summary, the main future directions are: incorporating document dynamics, customized definitions for models, robust extrapolation optimization, alternate query similarity implementations, broader model evaluation, and joint query-document dynamics.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the key points from the paper:

The paper investigates interpolation and extrapolation capabilities of neural ranking models on popular ad-hoc search benchmarks like MS MARCO and TREC Deep Learning Tracks. Interpolation refers to the ability of models to handle test queries similar to training data, while extrapolation refers to handling novel test queries different from training data. The authors examine the training and test data distribution in these benchmarks and find considerable overlap, implying the benchmarks are biased towards interpolation. To address this, they propose evaluation methods to resample training queries similar/dissimilar to test queries and re-evaluate models in interpolation/extrapolation regimes. Experiments show models like dense retrieval which interpolate well actually substantially underperform in extrapolation compared to interaction-based models like BERT ranker. Overall, the paper argues evaluating both interpolation and extrapolation is necessary for robust ranking models that can generalize, and the proposed resampling provides an effective method to do so on existing benchmarks.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the paper:

The paper investigates the problem of evaluating interpolation and extrapolation of neural ranking models on existing benchmarks like MS MARCO and TREC Deep Learning Tracks. The authors first examine the distribution of training and test queries in these benchmarks and find substantial overlap in entities, intents, and relevance labels between training and test queries. This implies the benchmarks are biased towards evaluating interpolation capacity but fail to reflect extrapolation ability. To address this issue, the authors propose two resampling methods to construct new training-test splits that evaluate interpolation and extrapolation separately. In one method called ReSTrain, they resample similar or dissimilar training queries to construct interpolation or extrapolation training sets. In the other called ReSTTest, they cluster both training and test queries into buckets and sample different combinations for interpolation or extrapolation evaluation. Without requiring new datasets, the proposed methods reliably evaluate extrapolation performance which correlates to generalization ability evaluated on out-of-distribution datasets. Leveraging the methods, the authors revisit various neural ranking models and find they perform very differently in terms of interpolation and extrapolation. The results highlight the necessity to evaluate both capacities, which was overlooked by previous benchmark evaluations.

In summary, the paper proposes a novel perspective to evaluate neural ranking models by separately measuring interpolation and extrapolation performance. The authors design resampling strategies to construct new training-test splits from existing benchmarks. Without costly new annotation, the proposed evaluation protocol reliably reflects model generalization ability and provides insights unattainable by traditional benchmark evaluations. The paper sheds light on analyzing model capacity on benchmarks and provides convenient tools to revisit existing models.


## Summarize the main method used in the paper in one paragraph.

 Here is a one paragraph summary of the main method used in the paper:

The paper proposes a new evaluation protocol to separately assess the interpolation and extrapolation performance of neural ranking models. Interpolation occurs when the test query is similar to training queries, while extrapolation happens when the test query is novel compared to training data. To enable this evaluation, the authors first examine popular benchmarks like MS MARCO and TREC DL and find considerable overlap between training and test queries, indicating a bias towards interpolation. Then, they propose two strategies to resample the training and/or test queries based on their similarity in an embedding space. For small test sets, they sample similar or dissimilar training queries to construct new training sets for interpolation or extrapolation evaluation, respectively. For large test sets, they cluster all queries into buckets and use a cross-validation style evaluation. Models are trained on the resampled data and evaluated in the two regimes. The proposed evaluation protocol does not require new datasets or annotation. Experiments show extrapolation performance correlates well with out-of-distribution generalization ability, while interpolation performance does not. Revisiting existing models reveals different behaviors in the two evaluation scenarios. The resampling strategy provides a simple yet effective way to separately assess interpolation and extrapolation capacities on existing benchmarks.


## What problem or question is the paper addressing?

 From my understanding, the key points of this paper are:

1. It highlights the importance of evaluating the interpolation and extrapolation performance of neural ranking models. Interpolation refers to how well a model handles test queries that are similar to the training queries, while extrapolation refers to performance on novel test queries. Extrapolation is argued to be more important for real-world search engines where queries are continuously changing.

2. It investigates popular ad-hoc search benchmarks like MS MARCO and TREC DL, and finds they have a considerable overlap between training and test queries in terms of entities, intents, and relevance labels. This indicates the benchmarks are biased towards evaluating interpolation rather than extrapolation. 

3. It proposes two resampling methods (ReSTrain and ReSTTest) to construct new training-test splits from existing benchmarks to separately evaluate interpolation and extrapolation performance. The extrapolation evaluation results are shown to correlate well with out-of-distribution generalization performance.

4. Using the proposed evaluation protocol, the paper revisits various neural ranking models and finds their effectiveness differs substantially between interpolation and extrapolation regimes. For example, representation-based models like dense retrieval tend to rely more on memorization and struggle to extrapolate compared to interaction-based models.

In summary, the key contribution is highlighting the importance of extrapolation evaluation for neural ranking models through analysis of existing benchmarks, proposing resampling methods for separate interpolation/extrapolation evaluation, and providing new insights into model behaviors using this evaluation approach. The paper argues extrapolation evaluation is crucial for understanding model generalization in real-world search systems.


## What are the keywords or key terms associated with this paper?

 Based on reviewing the paper, some key keywords and terms include:

- Interpolation and extrapolation: The main concepts explored in the paper in terms of evaluating ranking models. The paper defines and investigates interpolation performance (how models handle test queries similar to training queries) and extrapolation performance (how models handle test queries different from training queries).

- Neural retrieval models: The type of ranking models evaluated, including representation-based models like dense retrieval and interaction-based models like BERT reranker. 

- Model generalization: Evaluating model generalization is a focus, including investigating interpolation/extrapolation and alignment with out-of-distribution performance.

- Query similarity: Defining interpolation and extrapolation is based on query similarity between training and test queries. Computing query similarity is important to the proposed evaluation methods.

- Resampling methods: The paper proposes resampling training/test queries by similarity to construct datasets for separately evaluating interpolation and extrapolation.

- Benchmark investigation: Existing benchmarks like MS MARCO and TREC DL are analyzed and found to have substantial training-test overlap, indicating a bias towards interpolation.

- Model comparisons: Revisiting different model architectures and training techniques shows they perform differently on interpolation versus extrapolation tasks.

In summary, the key focus is on evaluating and analyzing interpolation versus extrapolation capabilities of neural ranking models, using concepts of query similarity and resampling to construct specialized evaluation datasets. Investigating model generalization is a main motivation.


## Based on the paper, what questions should we ask to create a comprehensive summary of it? Please list at least 10 questions.

 Here are 10 potential questions to create a comprehensive summary of the paper:

1. What is the motivation or purpose of the paper? What problem does it aim to address?

2. How does the paper define interpolation and extrapolation in information retrieval? What are the specific definitions provided? 

3. What methods does the paper propose for evaluating interpolation and extrapolation performance? How do they work?

4. What datasets and models were used to evaluate the proposed methods? What were the key results?

5. How does the paper investigate the relationship between interpolation/extrapolation performance and generalization ability? What correlation analysis was performed?

6. What are the key findings from revisiting existing retrieval models using the proposed evaluation methods? How do models compare in the two regimes?

7. What implications or conclusions does the paper draw about the importance of separately evaluating interpolation and extrapolation? 

8. What limitations does the paper acknowledge about the proposed methods or analysis? What future work is suggested?

9. How does this paper relate to or build upon prior work in information retrieval evaluation and neural ranking models? 

10. What are the key takeaways or contributions of this paper to the fields of information retrieval and neural ranking?


## Design 10 in-depth questions about method proposed in this paper and avoid simple question.

 Here are 10 potential in-depth questions about the method proposed in the paper:

1. The paper defines interpolation and extrapolation based on query similarity. How would you extend the definitions to also incorporate document similarity? What challenges might this present?

2. The paper focuses on query dynamics and keeps the document corpus static. How could the method be adapted to account for a dynamic corpus where new documents are constantly added? 

3. The paper uses embedding similarity to determine query similarity. What are some other options you could explore to compute query similarity and what are their tradeoffs?

4. How does the choice of embedding model impact the query similarity computation? What characteristics should the ideal embedding model have for this task?

5. The paper proposes two resampling strategies - ReSTrain and ReSTTest. When is one preferred over the other and what are the limitations of each? 

6. How does the choice of k for the k-means clustering in ReSTTest impact the interpolation vs extrapolation evaluation? How would you determine the optimal k?

7. The paper shows the proposed evaluation method correlates with out-of-distribution performance. What other approaches could you use to validate that the method properly evaluates interpolation vs extrapolation?

8. How does the annotation methodology for determining query entity/intent overlap impact the findings around benchmark bias? What are some ways to improve the robustness?

9. The paper focuses on ranking models, but could this evaluation approach be applied to other ML models? What challenges might arise?

10. The paper demonstrates discrepancies between interpolation and extrapolation performance. How could you design a model that is robust to both regimes?


## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a summary of the key points from the paper:

This paper proposes evaluating neural ranking models by separately examining their interpolation and extrapolation performance. Interpolation refers to effectiveness on test queries that are similar to training queries, while extrapolation refers to performance on novel test queries. The authors first investigate popular IR benchmarks like MS MARCO and TREC DL and find substantial overlap between training and test queries, meaning the benchmarks largely evaluate interpolation ability. To address this, they develop two resampling methods (ReSTrain and ReSTTest) to construct new training/test splits that evaluate interpolation or extrapolation separately. Using these methods, they re-evaluate various neural ranking models and find representation-based models like dense retrieval suffer large effectiveness drops from interpolation to extrapolation, while interaction-based models like ColBERT extrapolate much better. They also find pretraining is more effective than hard negative mining/distillation for improving extrapolation. The work provides a new perspective on evaluating neural ranking models and shows the importance of considering both interpolation and extrapolation performance. The proposed resampling methods offer a simple yet effective way to evaluate both capacities without needing new labeled data.


## Summarize the paper in one sentence.

 The paper proposes to separately evaluate the interpolation and extrapolation capabilities of neural retrieval models by resampling the training and test data based on query similarity. Results show different model architectures and training techniques perform differently on interpolation versus extrapolation.


## Summarize the paper in one paragraphs.

 Here is a one paragraph summary of the paper:

This paper points out that existing benchmarks for evaluating neural ranking models are biased towards evaluating interpolation performance, meaning how well models rank queries that are similar to queries seen during training. The paper proposes new evaluation methods to separately measure interpolation performance and extrapolation performance, which refers to how well models generalize to novel queries not seen during training. They find that model architectures and training techniques which perform well in interpolation may underperform in extrapolation. For example, representation-based models like dense retrieval do much worse in extrapolation while interaction-based models are more robust. The paper argues that evaluating extrapolation is important to understand how models will perform when deployed in real search engines where queries constantly change. They propose a resampling method to construct datasets for separately measuring interpolation and extrapolation without needing new annotations. Using this method reveals important differences between models that standard benchmark evaluations would miss. Overall, this paper demonstrates the need to evaluate ranking models along the dimensions of interpolation and extrapolation to better understand their generalization abilities.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in the paper:

1. The paper defines interpolation and extrapolation based on query similarity. How would you define interpolation and extrapolation if we also consider the dynamics of documents in a search corpus? 

2. The paper uses query embedding distance to determine query similarity. What are some other potential ways to measure query similarity that could be explored? How might that impact the interpolation vs extrapolation evaluation?

3. The paper proposes two resampling methods - ReSTrain and ReSTTest. What are the relative strengths and weaknesses of each method? When would you choose one over the other?

4. The paper evaluates the alignment of interpolation/extrapolation performance with out-of-distribution generalization performance. What other validation approaches could be used to evaluate whether the resampling methods provide meaningful results?

5. The paper finds representation-based models like dense retrieval struggle with extrapolation compared to interaction-based models like ColBERT. Why do you think that is the case? How could representation-based models be improved to better handle extrapolation?

6. Hard negative mining and distillation help interpolation but not extrapolation for dense retrieval. Why do you think they fail to improve extrapolation? How could these techniques be adapted? 

7. Pretraining is found to help extrapolation, especially pretraining on the target corpus. Why does target corpus pretraining help? Does it raise any concerns about real-world generalization?

8. The paper studies ranking models. Could the interpolation vs extrapolation evaluation also be beneficial for other neural IR tasks like query understanding and document understanding? How would you adapt it?

9. The paper focuses on query dynamics. How would you modify the interpolation vs extrapolation evaluation to also account for shifts in the document collection over time?

10. The resampling evaluation methods rely on existing datasets. How could we reduce this dependence and make the evaluation more robust to differences in dataset construction?
