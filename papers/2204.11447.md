# [Evaluating Interpolation and Extrapolation Performance of Neural   Retrieval Models](https://arxiv.org/abs/2204.11447)

## What is the central research question or hypothesis that this paper addresses?

Based on my understanding, the central research question this paper tries to address is: how to properly evaluate the interpolation and extrapolation performance of neural ranking models? The key points are:1. The paper first defines interpolation and extrapolation for ranking models based on query similarity - whether the test queries are similar to or distinct from the training queries. 2. It then investigates popular benchmarks like MS MARCO and TREC DL and finds they are biased towards evaluating interpolation due to considerable overlap between training and test queries.3. To address this issue, the paper proposes two resampling methods to construct new training-test splits that can evaluate interpolation and extrapolation performance separately.4. Experiments are conducted to validate the proposed methods. Results show extrapolation performance aligns better with out-of-distribution generalization ability, demonstrating the efficacy of the proposed evaluation protocol.  5. The methods are used to re-evaluate various ranking models. Comparisons reveal models behave differently in interpolation and extrapolation regimes, highlighting the importance of separated evaluation.In summary, the central hypothesis is that interpolation and extrapolation are two distinct capacities of ranking models, and existing benchmarks fail to accurately evaluate extrapolation. The paper proposes query similarity based resampling methods to address this issue and demonstrates their efficacy. The overall goal is to properly evaluate interpolation and extrapolation performance separately.


## What is the main contribution of this paper?

The main contribution of this paper is proposing a new evaluation protocol to separately assess the interpolation and extrapolation performance of neural ranking models. The key ideas are:- The paper first defines interpolation and extrapolation in information retrieval based on query similarity between training and test data. Interpolation occurs when test queries are similar to training queries, while extrapolation occurs when test queries are novel compared to training. - It then investigates popular IR benchmarks like MS MARCO and TREC DL and finds they are biased towards interpolation evaluation due to considerable query overlap between training and test sets.- To enable separate evaluation of interpolation and extrapolation, the paper proposes two resampling strategies. They construct new training-test splits by sampling similar or dissimilar queries. Models are trained and evaluated on the resampled data.- The proposed evaluation protocol is shown to align well with out-of-distribution generalization ability, despite being limited to the original data distribution.- By re-evaluating various neural ranking models, the paper shows models behave differently in interpolation and extrapolation regimes. For example, representation-based models excel at interpolation but struggle at extrapolation.In summary, the key contribution is identifying the need for separate interpolation/extrapolation evaluation in IR, proposing an effective resampling-based evaluation protocol, and providing new insights into model behaviors through re-evaluation. The work provides a new perspective for analyzing neural ranking models.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper proposes new methods to evaluate neural ranking models by resampling training and test data to separately measure interpolation and extrapolation performance, finding that models behave differently in the two regimes so it is important to evaluate both capacities.


## How does this paper compare to other research in the same field?

This paper makes several key contributions to the field of evaluating neural ranking models:1. It highlights the issue of test-train similarity in popular IR benchmarks like MS MARCO and TREC DL, showing significant overlap in query entities, intents, and relevance labels between the training and test data. This implies the benchmarks are more suited to evaluating interpolation performance rather than extrapolation. 2. It proposes two novel resampling methods (ReSTrain and ReSTTest) to construct new training/test splits that enable separate evaluation of interpolation and extrapolation capacities on existing benchmarks. This is a simple yet effective approach that does not require any new annotations.3. It provides an analysis of how various neural ranking models (interaction-based vs representation-based) perform differently in interpolation and extrapolation regimes. Key findings are that representation-based models rely more on memorization and struggle to extrapolate compared to interaction-based models.4. It studies the impact of different training techniques like hard negatives, distillation, and pretraining specifically on extrapolation performance. Results suggest pretraining is more beneficial for extrapolation compared to finetuning techniques.5. It validates the proposed evaluation protocol by showing in-domain extrapolation performance correlates strongly with out-of-domain generalization ability.Overall, this work provides novel perspectives and tools to analyze model capabilities in interpolation versus extrapolation. It demonstrates the importance of considering both regimes when evaluating and comparing neural ranking models, rather than just overall metrics. The analysis of model architectures and training techniques also offers useful insights into designing more robust models.Compared to related work like the BEIR benchmark, this paper offers more reliable evaluation free of annotation bias issues. The resampling strategies are also simpler and easier to apply than collecting new out-of-distribution test sets. More broadly, this work follows the trend in ML of studying interpolation vs extrapolation for model evaluation, but is one of the first to focus specifically on information retrieval tasks. The findings could significantly influence how neural ranking models are analyzed in future IR research.


## What future research directions do the authors suggest?

Based on my reading of the paper, some of the main future research directions suggested by the authors include:- Incorporating dynamics of documents into definitions of interpolation and extrapolation. The current work only considers query dynamics, but documents in search engines are also constantly changing. Further incorporating document dynamics could better characterize generalization performance in practice.- Exploring custom definitions of interpolation and extrapolation for different ranking models. The current work uses general definitions applicable to any model. Customized definitions tailored to model architectures could better align with generalization. - Designing ranking models robust to extrapolation in a min-max game formulation. The paper suggests optimizing models while evaluating extrapolation could be cast as a min-max game. Investigating how to design models robust to extrapolation through this lens is suggested.- Exploring different implementations of computing query similarity for defining interpolation/extrapolation. The current work uses embedding similarity but other options like intent/entity annotation or lexical overlap could also be explored.- Evaluating interpolation/extrapolation capacities for other neural ranking models besides the ones studied. The methodology could be applied to other models like cross-encoders.- Considering both query and document dynamics. The current work only looks at query dynamics but new/changing documents also affect generalization in practice.In summary, the main future directions are: incorporating document dynamics, customized definitions for models, robust extrapolation optimization, alternate query similarity implementations, broader model evaluation, and joint query-document dynamics.


## Summarize the paper in one paragraph.

Here is a one paragraph summary of the key points from the paper:The paper investigates interpolation and extrapolation capabilities of neural ranking models on popular ad-hoc search benchmarks like MS MARCO and TREC Deep Learning Tracks. Interpolation refers to the ability of models to handle test queries similar to training data, while extrapolation refers to handling novel test queries different from training data. The authors examine the training and test data distribution in these benchmarks and find considerable overlap, implying the benchmarks are biased towards interpolation. To address this, they propose evaluation methods to resample training queries similar/dissimilar to test queries and re-evaluate models in interpolation/extrapolation regimes. Experiments show models like dense retrieval which interpolate well actually substantially underperform in extrapolation compared to interaction-based models like BERT ranker. Overall, the paper argues evaluating both interpolation and extrapolation is necessary for robust ranking models that can generalize, and the proposed resampling provides an effective method to do so on existing benchmarks.


## Summarize the paper in two paragraphs.

Here is a two paragraph summary of the paper:The paper investigates the problem of evaluating interpolation and extrapolation of neural ranking models on existing benchmarks like MS MARCO and TREC Deep Learning Tracks. The authors first examine the distribution of training and test queries in these benchmarks and find substantial overlap in entities, intents, and relevance labels between training and test queries. This implies the benchmarks are biased towards evaluating interpolation capacity but fail to reflect extrapolation ability. To address this issue, the authors propose two resampling methods to construct new training-test splits that evaluate interpolation and extrapolation separately. In one method called ReSTrain, they resample similar or dissimilar training queries to construct interpolation or extrapolation training sets. In the other called ReSTTest, they cluster both training and test queries into buckets and sample different combinations for interpolation or extrapolation evaluation. Without requiring new datasets, the proposed methods reliably evaluate extrapolation performance which correlates to generalization ability evaluated on out-of-distribution datasets. Leveraging the methods, the authors revisit various neural ranking models and find they perform very differently in terms of interpolation and extrapolation. The results highlight the necessity to evaluate both capacities, which was overlooked by previous benchmark evaluations.In summary, the paper proposes a novel perspective to evaluate neural ranking models by separately measuring interpolation and extrapolation performance. The authors design resampling strategies to construct new training-test splits from existing benchmarks. Without costly new annotation, the proposed evaluation protocol reliably reflects model generalization ability and provides insights unattainable by traditional benchmark evaluations. The paper sheds light on analyzing model capacity on benchmarks and provides convenient tools to revisit existing models.


## Summarize the main method used in the paper in one paragraph.

Here is a one paragraph summary of the main method used in the paper:The paper proposes a new evaluation protocol to separately assess the interpolation and extrapolation performance of neural ranking models. Interpolation occurs when the test query is similar to training queries, while extrapolation happens when the test query is novel compared to training data. To enable this evaluation, the authors first examine popular benchmarks like MS MARCO and TREC DL and find considerable overlap between training and test queries, indicating a bias towards interpolation. Then, they propose two strategies to resample the training and/or test queries based on their similarity in an embedding space. For small test sets, they sample similar or dissimilar training queries to construct new training sets for interpolation or extrapolation evaluation, respectively. For large test sets, they cluster all queries into buckets and use a cross-validation style evaluation. Models are trained on the resampled data and evaluated in the two regimes. The proposed evaluation protocol does not require new datasets or annotation. Experiments show extrapolation performance correlates well with out-of-distribution generalization ability, while interpolation performance does not. Revisiting existing models reveals different behaviors in the two evaluation scenarios. The resampling strategy provides a simple yet effective way to separately assess interpolation and extrapolation capacities on existing benchmarks.


## What problem or question is the paper addressing?

From my understanding, the key points of this paper are:1. It highlights the importance of evaluating the interpolation and extrapolation performance of neural ranking models. Interpolation refers to how well a model handles test queries that are similar to the training queries, while extrapolation refers to performance on novel test queries. Extrapolation is argued to be more important for real-world search engines where queries are continuously changing.2. It investigates popular ad-hoc search benchmarks like MS MARCO and TREC DL, and finds they have a considerable overlap between training and test queries in terms of entities, intents, and relevance labels. This indicates the benchmarks are biased towards evaluating interpolation rather than extrapolation. 3. It proposes two resampling methods (ReSTrain and ReSTTest) to construct new training-test splits from existing benchmarks to separately evaluate interpolation and extrapolation performance. The extrapolation evaluation results are shown to correlate well with out-of-distribution generalization performance.4. Using the proposed evaluation protocol, the paper revisits various neural ranking models and finds their effectiveness differs substantially between interpolation and extrapolation regimes. For example, representation-based models like dense retrieval tend to rely more on memorization and struggle to extrapolate compared to interaction-based models.In summary, the key contribution is highlighting the importance of extrapolation evaluation for neural ranking models through analysis of existing benchmarks, proposing resampling methods for separate interpolation/extrapolation evaluation, and providing new insights into model behaviors using this evaluation approach. The paper argues extrapolation evaluation is crucial for understanding model generalization in real-world search systems.
