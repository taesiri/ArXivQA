# [TCIG: Two-Stage Controlled Image Generation with Quality Enhancement   through Diffusion](https://arxiv.org/abs/2403.01212)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
Current state-of-the-art text-to-image diffusion models can generate a wide range of images from text prompts. However, they lack full controllability over the image generation process. Users cannot precisely specify the location and regions of objects in the generated image. Existing methods to address controllability either require costly training procedures, fine-tuning, or are constrained by model architectures. 

Proposed Solution:
This paper proposes a two-stage controlled image generation (TCIG) method that combines controllability and high image quality. The first stage uses a pre-trained segmentation model and CLIP network to generate a controlled image based on input text prompt and segmentation masks. This stage focuses on control rather than quality. The second stage feeds the output of the first stage into a diffusion model to enhance image quality and resolution while preserving controllability.

Key Contributions:
- A flexible two-stage approach that decouples controllability and quality for controlled image generation, without costly training or fine-tuning.
- Leverages pre-trained segmentation models rather than mask encoders to provide control signals. Supports using multiple specialized segmentation models.
- Compatible with any latent or image space diffusion model, providing state-of-the-art quality in second stage.
- Achieves better quantitative IoU metric and qualitative fitting of objects to input masks compared to current methods.
- Opens possibilities for new research directions in separating image control from quality enhancement.

The method represents a significant advancement in controllable text-to-image generation, matching or exceeding state-of-the-art approaches without being constrained by model architectures or training procedures.
