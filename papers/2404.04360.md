# [Prompt Public Large Language Models to Synthesize Data for Private   On-device Applications](https://arxiv.org/abs/2404.04360)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem: 
- Small on-device language models (LMs) are useful for mobile applications like Gboard for next word prediction. Such models need to be trained on user data for good performance. 
- However, user data is private so models cannot access it directly. Federated learning (FL) and differential privacy (DP) allow privacy-preserving training but work better with pre-training.
- The current pre-training data (public Common Crawl data) has a different distribution from private mobile user data. This gap reduces efficiency of private fine-tuning.

Proposed Solution:
- Use large language models (LLMs) to synthesize pre-training data similar to private mobile user data distribution without accessing private data.
- Carefully design prompts to guide LLM to filter, transform or generate synthetic pre-training data.
- Evaluate quality by pre-training small LMs and measuring metrics over real private data via federated evaluation.

Contributions:
- Novel method to exploit generative abilities of LLMs to create better pre-training data for on-device LMs.
- Extensive experiments over millions of real mobile devices to validate effectiveness.
- Models pre-trained on LLM data get 19-23% better accuracy than baseline and achieve better final performance after private fine-tuning.  
- Live A/B testing shows improvements in real Gboard application.
- Analysis provides insights about distribution gaps and future work directions.

In summary, the paper presents a novel and practical way to use publicly available LLMs to create better pre-training data for privacy-preserving learning of on-device models, with very promising results over real-world mobile data.


## Summarize the paper in one sentence.

 Here is a one-sentence summary of the paper:

The paper proposes using large language models to synthesize data that resembles real user data in order to improve pre-training for privacy-preserving federated learning of on-device keyboard prediction models.


## What is the main contribution of this paper?

 The main contribution of this paper is proposing a simple yet effective method to improve the quality of public pre-training data for privately training on-device language models. Specifically, the authors carefully design prompts to guide large language models (LLMs) to filter, transform, and generate new data that resembles the distribution of real user data. They show that pre-training a small on-device LM on this LLM-synthesized data leads to significant accuracy improvements (19-23\% relative) compared to using the original public data, when evaluated on real user mobile typing data. The effectiveness is further validated by extensive federated learning experiments over millions of mobile devices. Their method provides evidence that LLMs can help synthesize useful proxy data for private learning tasks, without accessing the private data.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and keywords associated with it are:

- Large language models (LLMs)
- Prompt design
- Synthetic data generation
- Data filtering 
- Data transformation
- Mobile keyboard
- Gboard
- On-device language models
- Federated learning (FL)
- Differential privacy (DP) 
- Pre-training
- Fine-tuning
- Proxy data
- Distribution gap
- Privacy protection
- Utility

The paper focuses on using large language models to synthesize data that can be used to pre-train small on-device language models for Gboard. The goal is to generate data that is closer to the true private data distribution from users' mobile keyboard inputs. Various prompting strategies are used to filter, transform, and directly generate synthetic conversations. The quality of the data is evaluated by pre-training on-device models and measuring metrics through federated evaluation on real user data. Differential privacy is used during the fine-tuning phase. Reducing the gap between public and private data distribution is a key challenge explored. The keywords cover the main techniques, models, goals, and evaluation metrics associated with this research.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 potential in-depth questions about the method proposed in this paper:

1. The paper uses an instruction-tuned PaLM 2-S model as the LLM for data generation. How might using a different LLM architecture impact the quality and diversity of the generated data? What modifications could be made to the prompting strategy to better leverage different LLM capabilities?

2. When filtering the C4 dataset, what additional prompt designs could help the LLM better determine if an example is likely from mobile device typing? For example, could providing sample mobile device sentences help calibrate the LLM? 

3. For the synthetic chat data generation, are there other prompting strategies besides chain-of-thought that could improve diversity? How sensitive is the data quality to different top-k sampling hyperparameters?

4. The converted C4 to chat data integrates the vocabulary breadth of C4 with chat-style sequences. Are there other public dataset and conversion approaches that could complement this technique?

5. The paper combines filtered C4, generated chat, and converted C4 into the best pretraining data. What analysis could determine the optimal combination ratio? Could an iterative data generation process improve results?

6. For using a fine-tuned on-device LM to filter LLM data, what modifications to the filtering criteria could better select valuable examples for pretraining? What risks exist with overfitting to the private data distribution?

7. How do the techniques explored in this paper complement existing strategies like knowledge distillation and distribution matching from prior work? What combinations merit further analysis? 

8. What additional evaluation metrics beyond vocabulary coverage and next word prediction could more directly measure distribution gap? How can privacy concerns be addressed to enable such analysis?

9. How well do the techniques extend to other languages and keyboard applications? What adjustments help adapt the methods?

10. Beyond pretraining for federated learning, what other potential use cases exist for the synthesized data? Could the data improve server-side models or simulations as well?
