# [Hebbian Learning based Orthogonal Projection for Continual Learning of   Spiking Neural Networks](https://arxiv.org/abs/2402.11984)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
Spiking neural networks (SNNs) are promising for energy-efficient neuromorphic computing. However, unlike humans who can continually learn, SNN models suffer from catastrophic forgetting of old tasks when learning new ones. Overcoming this issue is important for developing human-like artificial intelligence with SNNs.

Proposed Solution: 
The paper proposes a Hebbian learning based orthogonal projection (HLOP) method to enable continual learning for SNNs. The key idea is to leverage lateral neural circuits and Hebbian learning rules to extract principal subspaces of neural activities. These principal subspaces are then used to project presynaptic activity traces that are used to update synaptic weights. By projecting the traces into an orthogonal subspace, weight updates for new tasks will not interfere with existing knowledge about old tasks, preventing catastrophic forgetting.  

Specifically, lateral connections with skew-symmetric weights are introduced to connect feedforward neurons with "subspace neurons". Hebbian learning rules are applied on these lateral connections so they can extract principal subspaces of neural activities over time. The feedforward activity traces are then projected by propagating them through these lateral connections. This projection ensures weight updates are constrained to not influence existing knowledge. The method works for various SNN training methods by modifying eligibility traces or spike representations. It also allows flexible combination with other techniques like episodic replay.

Main Contributions:
- Proposes a neuronal computation based method for continual learning of SNNs using lateral connections and Hebbian learning
- Provides insights into how neural circuits and biological learning rules can systematically enable advanced abilities like continual learning
- Demonstrates how the concept of orthogonal projection can be realized with pure spike-based neural operations
- Achieves superior continual learning performance over previous methods under various SNN training algorithms, datasets and network architectures
- Enables a flexible plug-and-play module to equip arbitrary SNN training methods with strong continual learning ability using neuronal computation

The proposed HLOP method significantly advances continual learning for SNNs through bio-inspired algorithm design. It is hardware-friendly and can pave the way for building high-performance and continual neuromorphic computing systems.


## Summarize the paper in one sentence.

 This paper proposes a continual learning method for spiking neural networks based on lateral connections and Hebbian learning to extract principal subspaces of neural activities for orthogonal projection of activity traces.


## What is the main contribution of this paper?

 This paper proposes a new method called Hebbian Learning based Orthogonal Projection (HLOP) for continual learning of spiking neural networks. The key ideas and contributions are:

1) It leverages lateral neural circuits and Hebbian learning to extract principal subspaces of neural activities from streaming data. This enables the circuits to project activity traces into an orthogonal subspace so that weight updates do not interfere with previous knowledge.

2) It provides a way to realize the concept of orthogonal projection, which has strong theoretical grounding for overcoming catastrophic forgetting, using purely neuronal operations like neural circuits and Hebbian learning. This makes it suitable for neuromorphic hardware implementation. 

3) It demonstrates how neural circuits and Hebbian learning can systematically support continual learning abilities, shedding light on the neural mechanisms behind advanced learning.

4) Comprehensive experiments show that HLOP outperforms previous continual learning methods like replay, regularization, and gradient projection methods in various settings. It achieves near-zero forgetting and high accuracy.

In summary, the key contribution is a neuromorphic-friendly continual learning method for SNNs based on neural circuits and Hebbian learning, which has both strong empirical performance and biological insights. It paves a path for building high-performance continual neuromorphic computing systems.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and concepts associated with this paper include:

- Spiking neural networks (SNNs): The paper focuses on developing continual learning methods for neuromorphic computing systems built with spiking neural networks.

- Continual learning: The paper aims to tackle the catastrophic forgetting problem in neural networks when learning sequential tasks, which is an important challenge for continual learning. 

- Lateral/recurrent connections: The proposed method utilizes additional lateral/recurrent connections between neurons to enable orthogonal projection and Hebbian learning.

- Hebbian learning: Hebbian learning rules are leveraged in the lateral circuits to extract principal subspaces of neural activities to support orthogonal projection. 

- Orthogonal projection: The core idea is to project activity traces into an orthogonal subspace so that weight updates do not interfere with previously learned knowledge.

- Neuromorphic computing: The motivation is to enable continual learning abilities for energy-efficient neuromorphic computing systems inspired by the brain.

- Task incremental: One continual learning setting considered is task-incremental with different classifiers for different tasks.

- Domain incremental: Another continual learning setting is domain-incremental with a shared classifier for different domains.

In summary, the key terms cover spiking neural networks, continual learning, lateral connections, Hebbian learning, orthogonal projection, neuromorphic computing, task incremental, and domain incremental learning.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in this paper:

1. The paper proposes a Hebbian learning based orthogonal projection (HLOP) method for continual learning of spiking neural networks. Can you explain in more detail how the Hebbian learning enables the extraction of principle subspaces of neural activities? 

2. The paper claims that HLOP is more systematic to constrain weight updates compared to previous replay and metaplasticity methods. Can you elaborate on why considering projections in high-dimensional spaces leads to more systematic constraints?

3. HLOP requires additional lateral connections and computations. Can you analyze the time and space complexity compared to baseline methods and discuss whether the benefits outweigh the costs?

4. The paper mainly focuses on supervised learning. Can this method be extended to unsupervised continual learning settings? What changes would need to be made?

5. The number of subspace neurons is currently a hyperparameter. Can you propose methods to automatically determine the number of subspace neurons needed?

6. Can you extend the analysis to rigorously prove that the Hebbian learning algorithm will converge to the principal subspace under certain assumptions? 

7. The paper claims compatibility with neuromorphic hardware. Can you analyze the feasibility and challenges of implementing the proposed HLOP method on existing neuromorphic hardware platforms?

8. The paper focuses on task-incremental and domain-incremental continual learning. How suitable would this method be for class-incremental learning? What improvements could make it more amenable to class-incremental settings?

9. The paper mainly considers static image datasets. How would you adapt HLOP for continual learning on temporal/sequential data?

10. The paper combines HLOP with common SNN training methods like eligibility traces and backpropagation through time. Can you propose other training methods that could potentially benefit from HLOP and analyze the compatibility?
