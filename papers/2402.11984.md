# [Hebbian Learning based Orthogonal Projection for Continual Learning of   Spiking Neural Networks](https://arxiv.org/abs/2402.11984)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
Spiking neural networks (SNNs) are promising for energy-efficient neuromorphic computing. However, unlike humans who can continually learn, SNN models suffer from catastrophic forgetting of old tasks when learning new ones. Overcoming this issue is important for developing human-like artificial intelligence with SNNs.

Proposed Solution: 
The paper proposes a Hebbian learning based orthogonal projection (HLOP) method to enable continual learning for SNNs. The key idea is to leverage lateral neural circuits and Hebbian learning rules to extract principal subspaces of neural activities. These principal subspaces are then used to project presynaptic activity traces that are used to update synaptic weights. By projecting the traces into an orthogonal subspace, weight updates for new tasks will not interfere with existing knowledge about old tasks, preventing catastrophic forgetting.  

Specifically, lateral connections with skew-symmetric weights are introduced to connect feedforward neurons with "subspace neurons". Hebbian learning rules are applied on these lateral connections so they can extract principal subspaces of neural activities over time. The feedforward activity traces are then projected by propagating them through these lateral connections. This projection ensures weight updates are constrained to not influence existing knowledge. The method works for various SNN training methods by modifying eligibility traces or spike representations. It also allows flexible combination with other techniques like episodic replay.

Main Contributions:
- Proposes a neuronal computation based method for continual learning of SNNs using lateral connections and Hebbian learning
- Provides insights into how neural circuits and biological learning rules can systematically enable advanced abilities like continual learning
- Demonstrates how the concept of orthogonal projection can be realized with pure spike-based neural operations
- Achieves superior continual learning performance over previous methods under various SNN training algorithms, datasets and network architectures
- Enables a flexible plug-and-play module to equip arbitrary SNN training methods with strong continual learning ability using neuronal computation

The proposed HLOP method significantly advances continual learning for SNNs through bio-inspired algorithm design. It is hardware-friendly and can pave the way for building high-performance and continual neuromorphic computing systems.
