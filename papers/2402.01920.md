# [Preference Poisoning Attacks on Reward Model Learning](https://arxiv.org/abs/2402.01920)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
The paper studies the vulnerability of reward model learning from pairwise preference data to poisoning attacks. Reward models are used in many applications like autonomous control, conversational agents, and recommendations to align system decisions with user preferences. Since preference data is collected from anonymous users, it creates opportunities for malicious actors to manipulate the data and influence the learned models. Specifically, the paper considers threat models where attackers can flip the preference labels of a subset of data points to promote/demote certain candidates.

Proposed Solution:
The paper systematically analyzes such preference poisoning attacks using two classes of algorithms - gradient-based attacks inspired by data poisoning literature and greedy rank-by-distance (RBD) heuristics. For gradient attacks, it relaxes the discrete preference labels, takes implicit gradients to handle the bi-level optimization, and uses techniques like PCA and embeddings to improve scalability. For RBD attacks, it ranks data points by distance to target candidates under different metrics like l2 norm, model rewards, embeddings etc.  

Experiments and Key Contributions:
1) Comprehensive analysis of poisoning attacks involving both gradient and RBD algorithms across MuJoCo control, Atari, recommendations using Amazon data and safety alignment for LLMs.

2) Shows attacks can be highly effective, achieving near 100% success with only 0.3-10% poisoned data. Attack efficacy varies significantly across domains - no single method works best everywhere.

3) Simple RBD attacks are very effective in many domains like Atari, recommendations and competitive with gradient attacks which have scalability challenges.

4) Targeted attacks cause minimal degradation in overall accuracy, making them stealthy.

5) State-of-the-art defenses provide limited protection against preference poisoning. For instance, attacks remain nearly 100% successful even after applying defenses in the LLM safety alignment setting.

Overall the paper provides a rigorous analysis of an important but less studied vulnerability in reward learning, proposes algorithms spanning gradient and heuristic based methods, highlights challenges in defending against such attacks and sets benchmark across diverse application domains.
