# [LongHeads: Multi-Head Attention is Secretly a Long Context Processor](https://arxiv.org/abs/2402.10685)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Attention mechanisms in large language models (LLMs) have quadratic memory and computational complexity with sequence length, making it difficult to handle long sequences. 

Proposed Solution: 
- The paper proposes Efficient Long Context Encoders (ELCE), a parameter-free method to enable LLMs to handle long sequences without additional training. 

- The key idea is to break the long sequence into fixed-size chunks, compute chunk representations, and selectively attend to a small number of relevant chunks based on similarity between the query and chunks.

- This reduces the complexity from O(N^2) to O(N), while still allowing the model to leverage long context information.

Key Contributions:

- Introduces a training-free chunk selection strategy to pick relevant chunks based on query-chunk dot product similarities.

- Describes methods to create chunk representations that summarize the semantic information within each chunk.

- Enables restructuring and position remapping of selected chunks to fit within pretrained length.

- Achieves O(N) time and memory complexity for encoding and O(N+w^2) decoding complexity.

- Shows strong empirical performance on various tasks with sequences up to 32k tokens without any training.

In summary, the paper proposes an efficient attention mechanism to handle long sequences in LLMs, reducing quadratic complexity to linear, while preserving strong performance. The key innovation is selective attention over sequence chunks combined with position remapping.
