# [Latency-Aware Differentiable Neural Architecture Search](https://arxiv.org/abs/2001.06392)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is:How can we incorporate latency/speed constraints into differentiable neural architecture search in order to find architectures that are both accurate and fast? The key ideas and contributions are:- Proposing a differentiable latency prediction module (LPM) to predict the latency of a given architecture. The LPM is a neural network trained on latency measurements from sampling architectures.- Incorporating the LPM into the loss function for architecture search, allowing joint optimization of accuracy and latency.- Evaluating the approach on CIFAR and ImageNet, showing architectures with similar accuracy but lower latency compared to prior NAS methods.- Demonstrating the portability of the approach by training LPMs for both GPU and CPU and finding efficient architectures for each.So in summary, the main hypothesis is that by incorporating a learned, differentiable latency prediction model into the architecture search process, they can find architectures that are tailored for efficiency on the target hardware platform. The experiments then validate this hypothesis.


## What is the main contribution of this paper?

The main contribution of this paper is proposing a latency-aware differentiable neural architecture search (LA-DNAS) method to optimize both accuracy and latency during architecture search. The key ideas are:1. Designing a differentiable latency prediction module (LPM) to predict the latency of a sampled architecture. LPM is a multi-layer neural network trained by sampling and evaluating many architectures on the target hardware.2. Incorporating the predicted latency from LPM into the loss function for architecture search, allowing joint optimization of accuracy and latency. The balancing coefficient controls the tradeoff.3. Evaluating the method on CIFAR and ImageNet. The searched architectures achieve similar accuracy as baseline DARTS but with 15-20% lower latency.4. Demonstrating the approach can be easily transferred to different hardware (CPU/GPU) by retraining the LPM, without modifying the search algorithm.In summary, the key contribution is developing a latency prediction module to enable latency-aware neural architecture search in a complicated differentiable search space like DARTS. This provides an effective way to optimize for speed without sacrificing accuracy.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper proposes a differentiable neural architecture search method that incorporates a latency prediction module to enable trading off between accuracy and inference speed when searching for efficient network architectures.


## How does this paper compare to other research in the same field?

This paper presents a method for latency-aware differentiable neural architecture search. Here are some key points in comparing it to other related work:- Overall goal: Enabling neural architecture search methods to optimize for latency/speed in addition to accuracy. Many prior NAS methods focus only on accuracy. - Approach: Proposes a differentiable latency loss module that can predict the latency of a given architecture. This allows optimizing for a tradeoff between accuracy and latency during architecture search.- Search space: Applies the method in the DARTS search space, which is more complex than chain-style spaces tackled by some prior work. DARTS has more interlayer connections, making latency harder to model.- Comparison to heuristic NAS methods: These can optimize for latency by measuring it for each sampled architecture, but are computationally expensive. This work aims for a faster differentiable approach.- Comparison to other differentiable NAS: Some add FLOPs-based losses which don't directly capture latency. Others handle simpler chain-like spaces. This tackles latency in a complex space.- Hardware generalizability: Shows the approach can be adapted to search architectures specialized for GPUs or CPUs, highlighting that different hardware prefers different architectures.Overall, a key contribution seems to be introducing an effective differentiable latency optimization method for neural architecture search in complex spaces like DARTS. This is a challenging problem not addressed by prior differentiable NAS research. The results also highlight the importance of hardware-specific architecture search.


## What future research directions do the authors suggest?

Based on my reading of the paper, some future research directions the authors suggest are:- Exploring larger search spaces for NAS. The authors mention that with larger search spaces, it will become even more difficult for non-differentiable search methods to converge efficiently. They suggest more efforts on differentiable search methods like theirs could help deal with larger spaces.- Optimizing for other non-differentiable factors besides latency, such as power consumption. The authors mention their latency prediction module approach could potentially be extended to predicting and optimizing for other non-differentiable objectives.- Improving the accuracy and efficiency of the latency prediction module. The authors suggest this could involve collecting larger datasets of architecture-latency pairs, and researching better network architectures for the latency predictor.- Applying the method to more hardware platforms. The authors show their approach can be adapted to GPUs and CPUs with little effort, and suggest expanding it to more device types.- Searching wider spaces with this approach. The authors believe that with larger search spaces, their method will have more room for optimizing the latency/efficiency of the found architectures.- Exploring automated ways to set the latency loss balancing coefficient. The authors manually select this coefficient, suggesting automating this could be useful future work.In summary, the main future directions are around expanding this approach to larger search spaces, additional hardware platforms, other efficiency objectives, and making parts of the method like loss coefficient balancing more automated. The core idea of differentiable latency prediction seems promising for future NAS research.


## Summarize the paper in one paragraph.

Here is a one paragraph summary of the paper:This paper presents a latency-aware differentiable neural architecture search (LA-DNAS) method. The key idea is to add a differentiable latency loss term to the architecture search optimization so that it can trade off between accuracy and latency. The core of the approach is a latency prediction module (LPM) which predicts the latency of a given architecture. The LPM is a multi-layer neural network trained on latency data collected by sampling and evaluating many architectures. The predicted latency from LPM is incorporated into the loss to guide the search. Experiments show the method can find architectures that are 15-20% faster than baseline DARTS while maintaining accuracy. A key benefit is the approach can be easily adapted to different hardware by training a new LPM. Experiments transferring from GPU to CPU show the value of hardware-specific architecture search.


## Summarize the paper in two paragraphs.

Here is a two paragraph summary of the paper:The paper presents a differentiable method for neural architecture search that takes into account latency constraints. The key idea is to train a latency prediction module (LPM) to predict the latency of a given architecture. The LPM is a multi-layer neural network that takes as input an encoding of the architecture parameters and outputs the predicted latency. The LPM is trained on a dataset of architecture-latency pairs, which are obtained by sampling architectures from the search space and measuring their latency. The LPM allows incorporating latency into the loss function used for architecture search. By tuning a balancing coefficient, the tradeoff between accuracy and latency can be controlled. Experiments on CIFAR-10 and ImageNet show that the method can find architectures with lower latency and similar accuracy compared to prior work. The approach also allows easy transplanting to different hardware platforms like GPUs and CPUs by simply retraining the LPM. Overall, the paper demonstrates an effective way to enable latency-aware neural architecture search in complex search spaces like DARTS.


## Summarize the main method used in the paper in one paragraph.

The paper presents a latency-aware differentiable neural architecture search method (LA-DNAS). The key idea is to design a differentiable loss function that can predict the latency of a neural network architecture. This is done by training a latency prediction module (LPM) to predict the latency of an architecture given an encoded representation of the architecture. The LPM is a multi-layer neural network regressor trained on a dataset of architecture encodings and corresponding latency values. The LPM is incorporated into the architecture search process by adding its predicted latency as a loss term, allowing the search to trade off between accuracy and latency. Architectures are sampled from the current architecture parameters, fed into the LPM to get a latency prediction, and the average latency is added to the loss to guide the search towards low-latency architectures.The method is evaluated on image classification tasks CIFAR-10 and ImageNet. It allows trading off accuracy for lower latency, and discovers architectures that are 15-20% faster than baseline methods with similar accuracy. The LPM approach is also shown to be portable across hardware platforms. Overall, the paper introduces a way to optimize neural architecture search for latency in a differentiable manner.
