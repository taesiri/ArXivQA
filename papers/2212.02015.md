# [Learning Imbalanced Data with Vision Transformers](https://arxiv.org/abs/2212.02015)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading, the central research question this paper addresses is: How can we effectively train Vision Transformers (ViTs) from scratch on long-tailed datasets?

The key points are:

- ViTs require a lot of data to train well, but perform poorly when trained on long-tailed datasets directly with label supervision. 

- The authors propose a two-stage approach called LiVT:
  - Stage 1 uses Masked Generative Pretraining (MGP) to learn generalized features in a self-supervised manner, which is robust to data imbalance.
  - Stage 2 uses a novel Balanced Binary Cross-Entropy (Bal-BCE) loss to fine-tune the classifier, which corrects the inherent bias in standard BCE.

- MGP avoids the toxic influence of imbalanced labels on feature learning. Experiments show MGP extracts similar quality features when trained on balanced vs imbalanced datasets.

- Bal-BCE adds calibrated logit margins to compensate for the imbalance. This allows ViTs to converge quickly and focus on minority classes.

- Without extra data, LiVT achieves state-of-the-art results training ViTs from scratch on various long-tailed benchmarks.

In summary, the key hypothesis is that decoupling feature learning (via MGP) from classifier training (via Bal-BCE) enables effective end-to-end training of ViTs on long-tailed data. The two-stage LiVT approach is shown to outperform prior methods.
