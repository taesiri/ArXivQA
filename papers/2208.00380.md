# [Less is More: Consistent Video Depth Estimation with Masked Frames   Modeling](https://arxiv.org/abs/2208.00380)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question seems to be:

Can temporal consistency for video depth estimation be achieved without relying on additional information like optical flow or camera poses? 

The key hypothesis is that by randomly masking some input frames and training a model to reconstruct the depth of those masked frames, inter-frame correlations can be learned to produce more temporally consistent depth estimations.

In summary, the paper is exploring whether temporal consistency in video depth estimation can be achieved through a frame-masking and reconstruction approach, without needing optical flow or camera pose data as additional input. The hypothesis is that masking frames will force the model to learn useful inter-frame correlations and temporal structure that leads to more consistent depth estimations over time.


## What is the main contribution of this paper?

 Based on my reading of the paper, the main contribution is proposing a frame masking network (FMNet) for consistent video depth estimation. The key ideas are:

- Introducing a masked frames predicting strategy to model temporal consistency without relying on optical flow or camera poses. By randomly masking some input frames and forcing the model to predict the depth of masked frames, it can learn intrinsic inter-frame correlations for consistency.

- Adopting a ConvTransformer architecture to encode temporal correlations in parallel while maintaining spatial structures, unlike prior works that process frames serially.  

- Demonstrating that with a high masking ratio (83% frames masked), the proposed FMNet can achieve comparable or better depth accuracy than prior arts while significantly improving temporal consistency on NYU Depth V2 and KITTI datasets.

In summary, the main contribution is presenting a new perspective for video depth consistency - learning it through masked frames reconstruction rather than explicit modeling of inter-frame correlations. This is shown to be effective without optical flow or poses. The ConvTransformer allows parallel processing unlike prior serial models. Experiments validate improved consistency and comparable accuracy.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Based on my reading of the paper, here is a one-sentence summary: 

The paper proposes a frame masking network (FMNet) that achieves consistent video depth estimation by randomly masking input frames during training and forcing the model to predict the depth of masked frames based on the unmasked neighboring frames.
