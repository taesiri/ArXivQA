# [Less is More: Consistent Video Depth Estimation with Masked Frames   Modeling](https://arxiv.org/abs/2208.00380)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question seems to be:

Can temporal consistency for video depth estimation be achieved without relying on additional information like optical flow or camera poses? 

The key hypothesis is that by randomly masking some input frames and training a model to reconstruct the depth of those masked frames, inter-frame correlations can be learned to produce more temporally consistent depth estimations.

In summary, the paper is exploring whether temporal consistency in video depth estimation can be achieved through a frame-masking and reconstruction approach, without needing optical flow or camera pose data as additional input. The hypothesis is that masking frames will force the model to learn useful inter-frame correlations and temporal structure that leads to more consistent depth estimations over time.


## What is the main contribution of this paper?

 Based on my reading of the paper, the main contribution is proposing a frame masking network (FMNet) for consistent video depth estimation. The key ideas are:

- Introducing a masked frames predicting strategy to model temporal consistency without relying on optical flow or camera poses. By randomly masking some input frames and forcing the model to predict the depth of masked frames, it can learn intrinsic inter-frame correlations for consistency.

- Adopting a ConvTransformer architecture to encode temporal correlations in parallel while maintaining spatial structures, unlike prior works that process frames serially.  

- Demonstrating that with a high masking ratio (83% frames masked), the proposed FMNet can achieve comparable or better depth accuracy than prior arts while significantly improving temporal consistency on NYU Depth V2 and KITTI datasets.

In summary, the main contribution is presenting a new perspective for video depth consistency - learning it through masked frames reconstruction rather than explicit modeling of inter-frame correlations. This is shown to be effective without optical flow or poses. The ConvTransformer allows parallel processing unlike prior serial models. Experiments validate improved consistency and comparable accuracy.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Based on my reading of the paper, here is a one-sentence summary: 

The paper proposes a frame masking network (FMNet) that achieves consistent video depth estimation by randomly masking input frames during training and forcing the model to predict the depth of masked frames based on the unmasked neighboring frames.


## How does this paper compare to other research in the same field?

 Here are a few key points comparing this paper to other research on consistent video depth estimation:

- The main novelty is using a masked frames predicting strategy to achieve temporal consistency without relying on optical flow or camera poses. Most prior works use optical flow or pose estimation to model inter-frame correlations. This work shows temporal consistency can be learned from just the original frames.

- It leverages the idea of masked modeling from NLP (BERT) and vision (MAE), applying it in a new way to video for the task of depth estimation. The masking ratio analysis is interesting, showing very high masking works best.

- It introduces a ConvTransformer architecture tailored for video processing to model global temporal correlations in parallel. This is more effective than RNN/LSTM based approaches. 

- The method achieves strong performance on standard datasets, comparing favorably or on par with recent state-of-the-art methods in terms of depth accuracy and consistency.

- A limitation is that it still uses synthetic indoor and driving datasets. Testing on more complex real-world videos would be interesting future work.  

- Overall, it provides a new perspective on achieving temporal consistency without relying on optical flow or poses, demonstrating the power of masked modeling applied to video data. The simple and effective idea could inspire other video understanding tasks.

In summary, the key novelty is showing masked frames prediction can itself enforce video consistency, without needing additional flow or pose supervision. The results are comparable to recent state-of-the-art while being simpler. It demonstrates a promising new direction for consistent video analysis.
