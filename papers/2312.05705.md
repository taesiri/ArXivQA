# [Structured Inverse-Free Natural Gradient: Memory-Efficient &amp;   Numerically-Stable KFAC for Large Neural Nets](https://arxiv.org/abs/2312.05705)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Second-order optimization methods like KFAC can be useful for neural network training, but have two main issues: high memory consumption and numerical instability in low-precision training.
- This prevents their widespread adoption for training large modern neural networks like transformers.

Proposed Solution - Structured Inverse-Free Natural Gradient (SINGD):

1. Formulate an inverse-free update of KFAC called IKFAC that uses matrix subtraction rather than inversion. Show that IKFAC approximates KFAC updates in a numerically stable way.

2. Impose sparse structures on the Kronecker factors in IKFAC to reduce memory usage. This includes diagonal, block-diagonal, hierarchical structures. The update equations are modified to preserve these sparse structures. 

3. Evaluate SINGD on CNNs, transformers, GNNs. Show it achieves comparable or better performance than AdamW, while using less memory. Also demonstrate stability in low-precision training.

Main Contributions:

- Bridged the gap between existing INGD method and original KFAC method. Made KFAC inverse-free and amenable to low-precision training.

- Proposed various sparse structures for the Kronecker factors to reduce memory consumption, while maintaining performance.

- Showed that SINGD supports stable low-precision training on modern architectures like Transformers, outperforming AdamW. Helps make 2nd order methods more widely applicable.

In summary, the paper proposes an inverse-free, memory-efficient natural gradient algorithm called SINGD that works well for low-precision training of large modern NNs like Transformers. It helps expand the scope of 2nd order methods in deep learning.
