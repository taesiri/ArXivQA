# [Beyond 512 Tokens: Siamese Multi-depth Transformer-based Hierarchical   Encoder for Long-Form Document Matching](https://arxiv.org/abs/2004.12297)

## What is the central research question or hypothesis that this paper addresses?

Based on the introduction, it seems the key research question addressed in this paper is: How to effectively adapt self-attention models like Transformers and BERT for semantic matching between long documents?The paper proposes a new model called SMITH (Siamese Multi-depth Transformer-based Hierarchical Encoder) to address the limitations of existing methods for long document matching. The key aspects of SMITH include:- A hierarchical encoder with sentence-level and document-level Transformers to capture both local semantic relations within sentences and global relations between sentences. This reduces the quadratic complexity of full self-attention.- A greedy sentence filling method to split documents into compact blocks and reduce padded tokens.- Pre-training objectives of masked word prediction and masked sentence block prediction to learn useful representations of words, sentences, and document structure. - A siamese network architecture that allows encoding documents independently for efficient retrieval and matching.The experiments show SMITH achieves new state-of-the-art results on benchmark datasets for long document matching, outperforming previous models like HAN, SMASH RNN, and BERT. A key result is SMITH can handle documents up to 2048 tokens, much longer than the 512 token limit of BERT.In summary, the paper proposes an innovative architecture and pre-training approach to address the key challenges of adapting transformers for long document understanding and matching. The SIAMSE hierarchical encoder and dual pre-training objectives are the main novel contributions.


## What is the main contribution of this paper?

Based on my reading, the main contributions of this paper are:1. Proposes a Siamese Multi-depth Transformer-based Hierarchical (SMITH) Encoder for long-form document matching. The encoder contains novel designs to adapt transformers for long text inputs, including a two-level hierarchical architecture with sentence-level and document-level transformers, and a greedy sentence filling method to reduce padding.2. Introduces a masked sentence block prediction task for model pre-training, in addition to masked word prediction. This helps capture sentence-level semantic relations within a document. 3. Evaluates SMITH on benchmark datasets for long document matching. Results show SMITH outperforms previous state-of-the-art models like HAN, SMASH RNN, and BERT. SMITH also increases maximum input length from 512 to 2048 tokens compared to BERT.4. Will release a Wikipedia benchmark dataset, model code, and pre-trained checkpoints to facilitate research on long document understanding and matching.In summary, the key contribution is proposing a transformer-based hierarchical encoder (SMITH) that adapts transformers for long document matching through architectural designs like two-level hierarchy and pre-training objectives like masked sentence block prediction. Experiments demonstrate improved performance over prior methods on long document matching benchmarks.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper proposes a Siamese Multi-depth Transformer-based Hierarchical (SMITH) Encoder to improve semantic matching between long documents by using a hierarchical encoder with sentence and document level Transformers, a greedy sentence filling approach to reduce padding, and masked sentence block prediction during pre-training to better capture relations between sentences.


## How does this paper compare to other research in the same field?

Here are a few key points comparing this paper to other related work:- It focuses on semantic matching between long document pairs, which has been relatively less explored compared to short text matching. Long document matching has important applications like news/article recommendation. - Existing work like MASH RNN uses RNNs for document matching, which may suffer from vanishing/exploding gradients and difficulty modeling long-range dependencies. This paper proposes using Transformers in a hierarchical encoder (SMITH) to better capture interactions between all tokens and long-range dependencies.- To handle the quadratic complexity of standard Transformers, SMITH uses a hierarchical design with local self-attention over sentence blocks and global attention between blocks. This reduces the complexity while still allowing some global interactions.- Inspired by BERT, SMITH uses masked language model pre-training on words and sentence blocks. The masked sentence block prediction task helps learn relationships between sentences.- Experiments show SMITH outperforms prior methods like HAN, MASH RNN, and BERT on long document matching. It increases max input length from 512 to 2048 tokens compared to BERT.- The paper will release a Wikipedia benchmark dataset, code, and pretrained models to facilitate future research on long document understanding.In summary, the key novelties are using Transformers in a hierarchical encoder for long documents, pre-training on masked words and sentences, and showing strong results on long document matching. The released models/data should be a useful resource for the community.


## What future research directions do the authors suggest?

Based on my reading, some key future research directions suggested by the authors include:- Developing more efficient self-attention mechanisms for long text modeling. The paper mentions reducing the quadratic complexity of standard self-attention as an area for future work, such as using techniques like the revnet trick from the Reformer model.- Exploring different methods for document segmentation. The paper splits documents into sentences using a simple greedy filling approach. The authors suggest experimenting with other segmentation methods as future work.- Applying the learned document representations to other document-level language tasks. The authors mention document classification, clustering, and ranking as potential applications. They suggest studying how to pre-train one model for multiple downstream document tasks.- Releasing benchmark datasets to advance research. As part of this work, the authors plan to release a Wikipedia document matching dataset, code, and pretrained models to facilitate reproducibility and future research.- Further improving long text understanding. There are opportunities to build on this work to advance the state of the art in long document modeling and representation learning using self-attention.In summary, the main future directions are developing more efficient Transformers for long text, exploring alternative document segmentation strategies, applying models to other document tasks, releasing benchmarks to enable research, and pushing the boundaries of long text understanding. The authors lay the groundwork for Transformer-based hierarchical modeling of long documents and suggest exciting next steps to build on this line of research.


## Summarize the paper in one paragraph.

Here is a one paragraph summary of the paper:The paper proposes a Siamese Multi-depth Transformer-based Hierarchical (SMITH) Encoder for semantic matching between long documents. The model contains a hierarchical encoder with sentence-level and document-level Transformers to capture both local context within sentences and global context between sentences. To handle long documents, the paper introduces a greedy sentence filling approach to reduce padding and increase input capacity. For pretraining, masked sentence block prediction is proposed in addition to masked word prediction to learn relations between sentences. Experiments on Wikipedia and paper citation datasets show SMITH outperforms previous methods like RNN-based models and BERT, increasing max input length from 512 to 2048 tokens. Key contributions are the greedy sentence filling, masked sentence block pretraining, and combination of Transformers at multiple depths for long document representation learning.
