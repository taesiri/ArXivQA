# [Beyond 512 Tokens: Siamese Multi-depth Transformer-based Hierarchical   Encoder for Long-Form Document Matching](https://arxiv.org/abs/2004.12297)

## What is the central research question or hypothesis that this paper addresses?

Based on the introduction, it seems the key research question addressed in this paper is: How to effectively adapt self-attention models like Transformers and BERT for semantic matching between long documents?The paper proposes a new model called SMITH (Siamese Multi-depth Transformer-based Hierarchical Encoder) to address the limitations of existing methods for long document matching. The key aspects of SMITH include:- A hierarchical encoder with sentence-level and document-level Transformers to capture both local semantic relations within sentences and global relations between sentences. This reduces the quadratic complexity of full self-attention.- A greedy sentence filling method to split documents into compact blocks and reduce padded tokens.- Pre-training objectives of masked word prediction and masked sentence block prediction to learn useful representations of words, sentences, and document structure. - A siamese network architecture that allows encoding documents independently for efficient retrieval and matching.The experiments show SMITH achieves new state-of-the-art results on benchmark datasets for long document matching, outperforming previous models like HAN, SMASH RNN, and BERT. A key result is SMITH can handle documents up to 2048 tokens, much longer than the 512 token limit of BERT.In summary, the paper proposes an innovative architecture and pre-training approach to address the key challenges of adapting transformers for long document understanding and matching. The SIAMSE hierarchical encoder and dual pre-training objectives are the main novel contributions.


## What is the main contribution of this paper?

Based on my reading, the main contributions of this paper are:1. Proposes a Siamese Multi-depth Transformer-based Hierarchical (SMITH) Encoder for long-form document matching. The encoder contains novel designs to adapt transformers for long text inputs, including a two-level hierarchical architecture with sentence-level and document-level transformers, and a greedy sentence filling method to reduce padding.2. Introduces a masked sentence block prediction task for model pre-training, in addition to masked word prediction. This helps capture sentence-level semantic relations within a document. 3. Evaluates SMITH on benchmark datasets for long document matching. Results show SMITH outperforms previous state-of-the-art models like HAN, SMASH RNN, and BERT. SMITH also increases maximum input length from 512 to 2048 tokens compared to BERT.4. Will release a Wikipedia benchmark dataset, model code, and pre-trained checkpoints to facilitate research on long document understanding and matching.In summary, the key contribution is proposing a transformer-based hierarchical encoder (SMITH) that adapts transformers for long document matching through architectural designs like two-level hierarchy and pre-training objectives like masked sentence block prediction. Experiments demonstrate improved performance over prior methods on long document matching benchmarks.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper proposes a Siamese Multi-depth Transformer-based Hierarchical (SMITH) Encoder to improve semantic matching between long documents by using a hierarchical encoder with sentence and document level Transformers, a greedy sentence filling approach to reduce padding, and masked sentence block prediction during pre-training to better capture relations between sentences.


## How does this paper compare to other research in the same field?

Here are a few key points comparing this paper to other related work:- It focuses on semantic matching between long document pairs, which has been relatively less explored compared to short text matching. Long document matching has important applications like news/article recommendation. - Existing work like MASH RNN uses RNNs for document matching, which may suffer from vanishing/exploding gradients and difficulty modeling long-range dependencies. This paper proposes using Transformers in a hierarchical encoder (SMITH) to better capture interactions between all tokens and long-range dependencies.- To handle the quadratic complexity of standard Transformers, SMITH uses a hierarchical design with local self-attention over sentence blocks and global attention between blocks. This reduces the complexity while still allowing some global interactions.- Inspired by BERT, SMITH uses masked language model pre-training on words and sentence blocks. The masked sentence block prediction task helps learn relationships between sentences.- Experiments show SMITH outperforms prior methods like HAN, MASH RNN, and BERT on long document matching. It increases max input length from 512 to 2048 tokens compared to BERT.- The paper will release a Wikipedia benchmark dataset, code, and pretrained models to facilitate future research on long document understanding.In summary, the key novelties are using Transformers in a hierarchical encoder for long documents, pre-training on masked words and sentences, and showing strong results on long document matching. The released models/data should be a useful resource for the community.
