# [Cross-Domain Image Captioning with Discriminative Finetuning](https://arxiv.org/abs/2304.01662)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the key research questions and hypotheses addressed in this paper are:- Can finetuning an image captioning model with a self-supervised discriminative objective help it generate more informative and useful captions compared to just training on human reference captions? - The authors hypothesize that finetuning a pretrained captioner to play a discrimination game with an image retriever will help it move away from mimicking potentially uninformative or idiosyncratic human captions. This will allow it to produce plainer, more descriptive captions that are better for practical applications like cross-domain image retrieval.- They also hypothesize that the discriminatively finetuned captions will not only be good for retrieval by neural systems, but could also be more useful for human image discrimination compared to human-written or non-finetuned captions.So in summary, the main goals are to explore whether discriminative finetuning can improve caption informativeness and usefulness compared to just training on human references, and testing this both via neural retrieval and human evaluation. The key hypothesis is that the discriminative objective will undo abstraction learned from human captions and recover more plainly descriptive language.


## What is the main contribution of this paper?

The main contribution of this paper is presenting a method for finetuning neural image captioners to generate more discriminative and communicative captions. The key ideas are:- Taking a pretrained image captioning model and finetuning it with a self-supervised discriminative objective using reinforcement learning. Specifically, the model plays a "discrimination game" with an image retriever, trying to generate captions that allow the retriever to identify the target image among distractors.- Showing that this discriminative finetuning improves the generalizability and communicative ability of the captions, even without using any human annotations during finetuning. The new captions enable better cross-domain image retrieval and are more useful for humans trying to identify images compared to the original model. - Analyzing the changes in the caption language and style induced by the discriminative finetuning. It steers the model away from overfitting the idiosyncrasies of the original caption dataset and towards a more plainly descriptive style.- Demonstrating the benefits on two popular captioners - ClipCap and BLIP. The method is captioner-agnostic and only needs unannotated images.Overall, the discriminative finetuning approach provides a simple but effective way to improve caption generalizability and communication abilities without extra human annotation. The analysis offers insights into how the finetuning changes caption language and style.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper proposes a simple method to fine-tune image captioning models using a self-supervised discriminative objective, where the captioner plays an image discrimination game with a frozen image retriever, leading to improved generalization and more descriptive captions compared to non-fine-tuned models.
