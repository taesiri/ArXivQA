# [Cross-Domain Image Captioning with Discriminative Finetuning](https://arxiv.org/abs/2304.01662)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the key research questions and hypotheses addressed in this paper are:- Can finetuning an image captioning model with a self-supervised discriminative objective help it generate more informative and useful captions compared to just training on human reference captions? - The authors hypothesize that finetuning a pretrained captioner to play a discrimination game with an image retriever will help it move away from mimicking potentially uninformative or idiosyncratic human captions. This will allow it to produce plainer, more descriptive captions that are better for practical applications like cross-domain image retrieval.- They also hypothesize that the discriminatively finetuned captions will not only be good for retrieval by neural systems, but could also be more useful for human image discrimination compared to human-written or non-finetuned captions.So in summary, the main goals are to explore whether discriminative finetuning can improve caption informativeness and usefulness compared to just training on human references, and testing this both via neural retrieval and human evaluation. The key hypothesis is that the discriminative objective will undo abstraction learned from human captions and recover more plainly descriptive language.


## What is the main contribution of this paper?

The main contribution of this paper is presenting a method for finetuning neural image captioners to generate more discriminative and communicative captions. The key ideas are:- Taking a pretrained image captioning model and finetuning it with a self-supervised discriminative objective using reinforcement learning. Specifically, the model plays a "discrimination game" with an image retriever, trying to generate captions that allow the retriever to identify the target image among distractors.- Showing that this discriminative finetuning improves the generalizability and communicative ability of the captions, even without using any human annotations during finetuning. The new captions enable better cross-domain image retrieval and are more useful for humans trying to identify images compared to the original model. - Analyzing the changes in the caption language and style induced by the discriminative finetuning. It steers the model away from overfitting the idiosyncrasies of the original caption dataset and towards a more plainly descriptive style.- Demonstrating the benefits on two popular captioners - ClipCap and BLIP. The method is captioner-agnostic and only needs unannotated images.Overall, the discriminative finetuning approach provides a simple but effective way to improve caption generalizability and communication abilities without extra human annotation. The analysis offers insights into how the finetuning changes caption language and style.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper proposes a simple method to fine-tune image captioning models using a self-supervised discriminative objective, where the captioner plays an image discrimination game with a frozen image retriever, leading to improved generalization and more descriptive captions compared to non-fine-tuned models.


## How does this paper compare to other research in the same field?

This paper introduces a new method for improving image captioning models by finetuning them with a self-supervised discriminative objective. Here are some key ways it compares to other related work:- Most prior work on image captioning focuses on training models to maximize similarity to human-generated reference captions, either via cross-entropy loss or rewards based on NLG metrics. This paper explores an alternative objective that does not rely on human references.- A few prior papers have explored discriminative or retrieval-based training objectives for image captioning, but this work proposes a particularly simple and effective approach using an off-the-shelf image retriever. The method is model-agnostic and only needs unlabeled images.- The paper shows strong improvements in generalization to out-of-domain datasets after discriminative finetuning. This demonstrates the method's ability to improve caption quality in a zero-shot cross-domain setup, which has been an open challenge. - The analysis shows finetuning can recover more descriptive and visually-grounded language compared to models that overfit to a particular captioning style. This is a novel finding.- Using human evaluations, the paper demonstrates the finetuned captions can be more useful than human references for identifying images, which questions the assumption that human captions are always the best target.In summary, this paper makes several notable contributions compared to prior work by introducing a simple and effective discriminative finetuning approach for image captioning, and providing extensive analysis of its impacts on generalization, language quality, and human usefulness. The results open interesting future work on balancing human-mimicry with more goal-oriented objectives.
