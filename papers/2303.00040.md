# [Towards Generalisable Video Moment Retrieval: Visual-Dynamic Injection   to Image-Text Pre-Training](https://arxiv.org/abs/2303.00040)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question/hypothesis seems to be:How can we exploit large-scale image-text pre-training models to benefit video moment retrieval (VMR) by learning universal visual-textual correlations, despite the limitations of image-text models in capturing video dynamics?The key hypotheses appear to be:1) Large-scale image-text pre-training can provide useful universal visual-textual correlations that could aid VMR, if the limitations in capturing video dynamics can be addressed.2) Injecting visual context and spatial dynamic information into the text embeddings can make image-text models more sensitive to video changes and dynamics. 3) By injecting visual and dynamic information with a focus on phrases describing video changes (e.g. verbs), the text encoder can be adapted to better align with dynamic video content.4) This visual-dynamic injection method can be integrated into existing VMR models to improve their generalizability to novel scenes and vocabulary.So in summary, the central goal is to take advantage of image-text pre-training for VMR, while overcoming the limitations in modeling video dynamics, by injecting relevant visual and dynamic information into the text encoder. The hypothesis is that this will enable more accurate video-text alignment and superior generalization ability.


## What is the main contribution of this paper?

The main contribution of this paper is proposing a method called Visual-Dynamic Injection (VDI) to enable image-text pre-training models to better understand videos for the task of video moment retrieval (VMR). The key ideas are:1. Extract visual context and spatial dynamic information from video frames. 2. Inject this information into the text embeddings during training, with a focus on words that describe video changes (e.g. verbs). This makes the text embeddings sensitive to video dynamics.3. Integrate VDI into existing VMR models to take advantage of the correlations learned from large-scale image-text pre-training, while adapting the text encoder to understand video changes.4. Achieve state-of-the-art performance on VMR benchmarks Charades-STA and ActivityNet Captions. Show notable improvements on out-of-distribution test sets with novel scenes and vocabulary.In summary, the main contribution is proposing VDI to address the limitations of image-text pre-training models in capturing video dynamics, and enable better video-text alignment for more generalizable video moment retrieval. The key novelty is injecting visual context and dynamics into text embeddings to make them video-change sensitive.
