# [Towards Generalisable Video Moment Retrieval: Visual-Dynamic Injection   to Image-Text Pre-Training](https://arxiv.org/abs/2303.00040)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question/hypothesis seems to be:How can we exploit large-scale image-text pre-training models to benefit video moment retrieval (VMR) by learning universal visual-textual correlations, despite the limitations of image-text models in capturing video dynamics?The key hypotheses appear to be:1) Large-scale image-text pre-training can provide useful universal visual-textual correlations that could aid VMR, if the limitations in capturing video dynamics can be addressed.2) Injecting visual context and spatial dynamic information into the text embeddings can make image-text models more sensitive to video changes and dynamics. 3) By injecting visual and dynamic information with a focus on phrases describing video changes (e.g. verbs), the text encoder can be adapted to better align with dynamic video content.4) This visual-dynamic injection method can be integrated into existing VMR models to improve their generalizability to novel scenes and vocabulary.So in summary, the central goal is to take advantage of image-text pre-training for VMR, while overcoming the limitations in modeling video dynamics, by injecting relevant visual and dynamic information into the text encoder. The hypothesis is that this will enable more accurate video-text alignment and superior generalization ability.
