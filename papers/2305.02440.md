# [Cheaply Evaluating Inference Efficiency Metrics for Autoregressive   Transformer APIs](https://arxiv.org/abs/2305.02440)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the main research questions and hypotheses of this paper appear to be:- Can we develop an inference efficiency metric that allows for apples-to-apples comparisons of language models across different providers and implementations? The authors hypothesize that the proposed "idealized runtime" metric can achieve this by estimating the runtime as though models were run in a uniform software and hardware environment without performance contention.- Is it possible to efficiently estimate the idealized runtime for autoregressive Transformer models without exhaustively profiling every possible prompt and output length? The authors hypothesize that the runtime can be parameterized as a linear function of output length and piecewise linear function of prompt length. - Does analyzing models using the proposed idealized runtime metric reveal different insights about efficiency vs capability tradeoffs compared to traditional metrics like raw runtimes or model size? The authors hypothesize that models may appear more efficient using raw runtimes due to orthogonal optimizations in the serving systems, and that the idealized runtime can uncover the true tradeoffs.In summary, the key research questions involve developing an interpretable and comparable efficiency metric for language models, efficiently estimating this metric, and using it to uncover new insights about efficiency-capability tradeoffs across models and providers. The idealized runtime metric is proposed as a solution.


## What is the main contribution of this paper?

The main contribution of this paper is proposing new metrics to quantify the inference efficiency of large language models available through blackbox APIs. The key ideas are:- Proposing an "idealized runtime" metric that estimates how long inference would take on standardized hardware/software, allowing for apples-to-apples comparison across models. This accounts for optimizations like caching that may speed up some models' APIs. - Showing the runtime can be modeled as a piecewise linear function of prompt size and a linear function of number of output tokens. This allows estimating the idealized runtime efficiently with just a small amount of profiling.- Proposing a "denoised runtime" metric that removes noise from runtime measurements to estimate the true runtime devoid of performance variability.- Demonstrating how these metrics enable analyzing efficiency-capability tradeoffs across models, surfacing insights like optimizations in certain APIs and relative costs across hardware.In summary, the main contribution is proposing novel metrics to quantify and compare the inference efficiency of blackbox language models in a fair and consistent way. This facilitates better understanding of efficiency-capability tradeoffs.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

The paper proposes a new metric called "idealized runtime" to compare the inference efficiency of large language models accessed through black-box APIs. The key idea is to estimate how long inference would take on standardized hardware/software by fitting a model to runtimes from a small set of profiler queries. This allows efficient comparison of models on an "apples-to-apples" basis.
