# [Cheaply Evaluating Inference Efficiency Metrics for Autoregressive   Transformer APIs](https://arxiv.org/abs/2305.02440)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the main research questions and hypotheses of this paper appear to be:- Can we develop an inference efficiency metric that allows for apples-to-apples comparisons of language models across different providers and implementations? The authors hypothesize that the proposed "idealized runtime" metric can achieve this by estimating the runtime as though models were run in a uniform software and hardware environment without performance contention.- Is it possible to efficiently estimate the idealized runtime for autoregressive Transformer models without exhaustively profiling every possible prompt and output length? The authors hypothesize that the runtime can be parameterized as a linear function of output length and piecewise linear function of prompt length. - Does analyzing models using the proposed idealized runtime metric reveal different insights about efficiency vs capability tradeoffs compared to traditional metrics like raw runtimes or model size? The authors hypothesize that models may appear more efficient using raw runtimes due to orthogonal optimizations in the serving systems, and that the idealized runtime can uncover the true tradeoffs.In summary, the key research questions involve developing an interpretable and comparable efficiency metric for language models, efficiently estimating this metric, and using it to uncover new insights about efficiency-capability tradeoffs across models and providers. The idealized runtime metric is proposed as a solution.
