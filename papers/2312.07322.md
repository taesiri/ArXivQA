# [GenHowTo: Learning to Generate Actions and State Transformations from   Instructional Videos](https://arxiv.org/abs/2312.07322)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a detailed summary paragraph of the key points from the paper:

This paper introduces GenHowTo, a novel generative model for transforming input images to showcase actions or object state changes according to given text prompts. The key idea is to leverage a large corpus of instructional videos, from which triplets of frames are automatically extracted to obtain training examples of (1) initial object states, (2) actions, and (3) final object states after transformation. Equipped with around 200k such triplets and corresponding text prompts automatically obtained via captioning, GenHowTo is realized as an image-conditioned diffusion model adapted from Stable Diffusion. During inference, GenHowTo takes an input image and text prompt and generates a new image showcasing the action or object's final state while preserving irrelevant parts of the input scene. The method is evaluated on held-out object transformations, outperforming baselines like InstructPix2Pix and Edit-Friendly Diffusion models in a classification benchmark and FID metric. Qualitative results demonstrate GenHowTo's ability to faithfully modify objects, introduce new interacting elements like hands, and maintain consistent backgrounds when transforming input images per text prompts. The work represents an advance in semantic image editing and conditional image generation grounded in real visual data of state changes and actions extracted automatically from narrated instructional video footage.
