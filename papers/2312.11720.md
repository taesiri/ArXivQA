# [Assessing Logical Reasoning Capabilities of Encoder-Only Transformer   Models](https://arxiv.org/abs/2312.11720)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
The paper investigates whether encoder-only transformer language models (LMs) can perform logical reasoning, defined as the ability to deduce theorems from premises expressed in propositional calculus or first-order logic. Specifically, it examines three research questions: (i) Can LMs perform logical reasoning tasks? (ii) How generalizable is this ability? (iii) Which layers of LMs contribute the most to logical reasoning?

Proposed Solution and Contributions:

1. The paper fine-tunes various encoder-only LMs on four logical reasoning datasets to classify hypotheses as true or false given premises. It finds that LMs can be trained to reasonably successfully perform deductive logic tasks.

2. To examine the generalizability of logical reasoning skills, the paper probes a RoBERTa-large model fine-tuned on each dataset with the other datasets. It finds limited transfer learning, suggesting learned statistical regularities rather than general logical competence. 

3. Probing the layers of fine-tuned RoBERTa reveals that higher layers solve the tasks better than lower layers. As higher layers capture dataset-specific features, this further indicates reliance on statistical rather than logical knowledge.

In summary, the paper shows that while encoder-only LMs can be trained to functionally perform logical deductions, they likely rely on statistical artifacts of datasets rather than robust logic-based representations. The limited transfer learning implies they do not acquire general logical reasoning competence. The paper contributes systematic evidence that dominant LMs still lack human-like logical reasoning.
