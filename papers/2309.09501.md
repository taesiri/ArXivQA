# [Discovering Sounding Objects by Audio Queries for Audio Visual   Segmentation](https://arxiv.org/abs/2309.09501)

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper addresses is how to accurately identify and segment sounding objects in videos by establishing correspondence between audio and visual modalities. Specifically, the paper focuses on two main challenges:1. Establishing semantic correspondence between audio and visual features to distinguish objects based on their acoustic characteristics. 2. Modeling the temporal pattern of sounding objects as they may change over time in a video.To address these challenges, the paper proposes a new Audio-Queried Transformer (AQFormer) architecture with two main components:1. Audio queries that gather visual information to build explicit object-level audio-visual correspondence. 2. An Audio-Bridged Temporal Interaction (ABTI) module that exchanges sounding object information between frames.By establishing finer-grained semantic audio-visual alignment and exploiting temporal context, the proposed AQFormer aims to achieve more accurate audio visual segmentation compared to prior methods. The main hypothesis is that explicit object-level modeling and temporal interaction will lead to performance gains on this task.


## What is the main contribution of this paper?

The main contributions of this paper are:1. It proposes a new architecture called Audio-Queried Transformer (AQFormer) for audio visual segmentation (AVS). 2. It introduces audio queries conditioned on audio features to establish explicit object-level correspondence between audio and visual modalities. The audio queries gather visual information of associated sounding objects.3. It designs an Audio-Bridged Temporal Interaction (ABTI) module to enable interaction between frames bridged by audio features, providing temporal context. 4. Experiments on AVS benchmarks show state-of-the-art performance, with significant gains over previous methods. For example, on the MS3 benchmark, AQFormer achieves 7.1% and 7.6% higher F-score and Jaccard index compared to prior art.5. Ablation studies demonstrate the effectiveness of the proposed audio queries and ABTI module in improving performance.In summary, the key innovation is the use of audio-conditioned queries and audio-bridged interaction to model fine-grained audio-visual correspondence and temporal context for accurate audio visual segmentation. The proposed AQFormer architecture achieves new state-of-the-art results on this task.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper proposes a new Audio-Queried Transformer architecture for audio visual segmentation that establishes explicit object-level correspondence between audio and visual modalities using conditional queries and facilitates temporal modeling through an Audio-Bridged Temporal Interaction module, achieving state-of-the-art performance.


## How does this paper compare to other research in the same field?

Here are my thoughts on how this paper compares to other research in the field of audio visual segmentation:- This paper proposes a new method called Audio-Queried Transformer (AQFormer) for the task of audio visual segmentation (AVS). AVS aims to identify and segment sounding objects in video frames. - The key novelty of this paper is using audio-conditioned object queries to establish explicit object-level correspondence between audio and visual modalities. The audio queries gather visual information about associated sounding objects across frames. This differs from prior work like TPAVI that uses pixel-level fusion of audio and visual features.- The proposed Audio-Bridged Temporal Interaction (ABTI) module is also novel, using audio to bridge interaction between visual features of different frames. This provides temporal context in a more focused manner compared to prior dense pixel-level interaction.- For related tasks, this paper compares to prior work in sound source localization and audio-visual learning that establish correspondence at the segment or patch level rather than pixel-level like required for AVS. The audio queries in AQFormer allow finer object shape segmentation.- Compared to video object segmentation methods that track objects, a key difference is AVS requires identifying sounding vs silent objects based on audio-visual correspondence, rather than using a provided reference object.- Experiments show AQFormer significantly outperforms prior AVS work TPAVI, achieving over 5-9% absolute gains on AVS benchmarks. This demonstrates the effectiveness of the proposed object-level audio-visual modeling.In summary, the key novelty of this work is the object-level audio-visual correspondence via audio queries, contrasting prior pixel-level fusion or coarser correspondence modeling approaches. The results validate that this approach better identifies sounding objects in videos for the AVS task.
