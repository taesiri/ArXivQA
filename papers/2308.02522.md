# [Evaluating ChatGPT and GPT-4 for Visual Programming](https://arxiv.org/abs/2308.02522)

## What is the central research question or hypothesis that this paper addresses?

 The central research question this paper addresses is: Do state-of-the-art generative models show advanced capabilities in visual programming on par with their capabilities in text-based Python programming?The paper investigates the capabilities of state-of-the-art generative AI models like ChatGPT (based on GPT-3.5) and GPT-4 in visual programming domains that are popularly used for K-8 programming education, such as Hour of Code and Karel. The goal is to evaluate whether these models have reached the same level of proficiency in visual programming tasks as they have demonstrated in text-based programming domains like Python. The paper systematically tests the models on various scenarios like execution trace generation, solution synthesis, and task synthesis to assess their skills in spatial reasoning, logic, and programming when applied to visual domains. The results indicate that the models still struggle considerably in visual programming compared to text programming, suggesting more work is needed to improve their capabilities. The central research question drives the experiments and analysis throughout the paper.


## What is the main contribution of this paper?

 The main contribution of this paper is an evaluation of two state-of-the-art large language models - ChatGPT (based on GPT-3.5) and GPT-4 - for various scenarios in visual programming domains popularly used for K-8 education. Specifically, the authors evaluate these models on three scenarios: execution trace generation, solution synthesis, and task synthesis. They use ten reference tasks from Hour of Code and Karel domains, and assess the performance using expert annotations. The key findings are:- Both models struggle with combining spatial, logical, and programming skills needed for visual programming, performing poorly compared to their capabilities in text-based programming.- GPT-4 shows substantial improvements over ChatGPT, but still achieves only mediocre performance on the elementary-level visual programming tasks.- In execution trace generation, GPT-4 achieves only 60% overall correctness. The generated traces often have incoherent transitions and incorrect sensing. - In solution synthesis, GPT-4 solutions are more similar to minimal solutions than ChatGPT, but achieve only 40% in overall quality. The solutions tend to be unnecessarily complex.- In task synthesis, both models perform very poorly, with only 10-20% overall quality. The generated tasks are often not solvable by the input code.In summary, the main contribution is a rigorous benchmarking of latest generative models on visual programming scenarios, highlighting their limitations and the need for developing techniques to improve their performance in this important educational domain.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 The paper evaluates ChatGPT and GPT-4 in visual programming domains like Hour of Code and Karel. The key finding is that these models struggle to combine spatial, logical, and programming skills crucial for visual programming and perform poorly compared to their capabilities in text-based programming.


## How does this paper compare to other research in the same field?

 This paper evaluates the capabilities of large language models (LLMs) like ChatGPT and GPT-4 for visual programming tasks, which is an emerging area of research. Here are some key ways this work compares to prior research:- Focus on visual programming: Most prior work studying LLMs for programming education has focused on text-based programming, especially Python. This paper provides a novel contribution by investigating visual programming domains like Hour of Code maze challenges and Karel programming.- Elementary-level tasks: The evaluation uses basic tasks suitable for K-8 education. This contrasts with much prior work that uses complex programming problems from high school or college courses. The elementary-level tasks pose different challenges for LLMs like combining spatial, logical and programming skills.- Multiple task scenarios: The paper comprehensive evaluates LLMs across three relevant scenarios - generating execution traces, synthesizing solutions, and synthesizing new tasks. Looking at different facets provides a broader picture of capabilities and limitations. - Expert annotations: The paper uses expert-based assessment to evaluate quality, rather than fully automated metrics. This allows incorporating nuanced qualitative judgments. Prior work has used both automated and human-based assessments.- Comparison of two models: By evaluating both ChatGPT and GPT-4, the paper provides useful insights into their relative strengths/weaknesses and the improvements from GPT-3.5 to GPT-4. Prior work has mostly studied capabilities of a single model.Overall, this paper provides a rigorous investigation of an underexplored but important topic. The analysis on visual programming with elementary-level tasks using expert annotations helps advance understanding over prior work and exposes limitations of current LLMs in this domain. The results highlight exciting open challenges for developing LLMs tailored to introductory visual programming.


## What future research directions do the authors suggest?

 Based on my reading of the paper, here are some of the main future research directions suggested by the authors:- Curating novel benchmarks for visual programming that the research community can use to evaluate new versions of generative AI models like ChatGPT and GPT-4. The paper notes they only used a small set of basic reference tasks from two visual programming domains. Creating more comprehensive benchmarks would allow more rigorous testing.- Evaluating alternate generative models, in particular open-source variants beyond the proprietary ChatGPT and GPT-4 models. The capabilities of other models on visual programming is still an open question.- Developing new techniques to improve the performance of generative AI models in visual programming domains. The paper suggests ideas like leveraging symbolic methods, automated prompting, or fine-tuning models specifically for visual programming.- Conducting studies with larger sets of more complex reference tasks to provide a more comprehensive assessment. The current study used a small simple set.- Scaling up the evaluation by generating and assessing multiple outputs per instance to account for the stochastic nature of generative models. The current study looked at single outputs.- Extending the study to other visual programming domains beyond Hour of Code mazes and Karel. Testing on domains like Scratch could reveal new insights.In summary, the main suggested future work revolves around creating better benchmarks, evaluating more models, developing new techniques to improve performance, and conducting more comprehensive and scaled up studies. Advancing research in these areas could lead to generative AI models that are much more capable at visual programming.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the paper:This paper evaluates the capabilities of two state-of-the-art generative AI models, ChatGPT (based on GPT-3.5) and GPT-4, on visual programming tasks from the domains of Hour of Code's Maze Challenge and Karel programming. The models are tested on three different scenarios - generating execution traces, synthesizing solutions, and synthesizing new tasks - using ten reference tasks of increasing complexity. The models' outputs are evaluated by experts on relevant metrics like correctness, similarity to ideal solutions, and solvability. The results show that while GPT-4 outperforms ChatGPT, both models struggle to combine the spatial, logical, and programming skills needed for visual programming. The authors suggest that novel benchmarks and techniques need to be developed to improve generative models' performance on visual programming domains commonly used in K-8 education.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the paper:This paper evaluates the capabilities of two state-of-the-art generative AI models, ChatGPT (based on GPT-3.5) and GPT-4, for visual programming tasks commonly used in K-8 education. The authors consider three different scenarios that test the models' abilities in visual programming: generating execution traces for given code, synthesizing minimal solution code for given tasks, and generating new tasks solvable by given code snippets. The models are evaluated on ten reference tasks drawn from the Hour of Code Maze domain and the Karel programming language, using a combination of automatic and human evaluation. The results indicate that while GPT-4 shows some improvement over ChatGPT, both models still struggle significantly on visual programming compared to prior results on text programming languages. The models fail to combine spatial, logical and programming skills effectively for the visual tasks. Neither model could generate completely coherent traces or minimal solution code, and they performed very poorly on the task generation scenario. The authors suggest several directions for future work, including new benchmark tasks for visual programming, alternate models, and techniques to adapt models to improve performance in this domain. Overall, the study highlights major limitations of current AI models for education applications involving visual programming.


## Summarize the main method used in the paper in one paragraph.

 The main method used in this paper is expert evaluation of ChatGPT and GPT-4 on visual programming tasks. The authors evaluate ChatGPT and GPT-4 on three key scenarios relevant for visual programming education: generating execution traces, synthesizing solutions, and synthesizing tasks. They use ten reference tasks from the Hour of Code Maze and Karel domains as the basis for evaluation. For each scenario, the authors manually interact with ChatGPT and GPT-4 via their web platforms to generate outputs, selecting the best output out of multiple queries. They then have a human expert evaluator assess the quality of the generated outputs based on scenario-specific metrics such as correctness, similarity to reference solutions, and solvability. The results show that while GPT-4 outperforms ChatGPT, both models still struggle significantly on visual programming compared to previous results on text programming. This highlights the need for developing techniques to improve generative models' capabilities in visual programming education scenarios.


## What problem or question is the paper addressing?

 The paper is addressing the capabilities of state-of-the-art generative AI models like ChatGPT and GPT-4 in the context of visual programming domains that are commonly used for K-8 programming education. The key research question is:"Do state-of-the-art generative models show advanced capabilities in visual programming on par with their capabilities in text-based Python programming?"The motivation is to understand if these latest AI models, which have shown impressive performance in text-based programming domains, can also demonstrate strong capabilities when it comes to visual programming paradigms involving spatial, logical and programming skills. This is an important question since visual programming is commonly used to introduce younger students to coding concepts.The paper evaluates the performance of ChatGPT and GPT-4 across three key scenarios relevant to visual programming education - generating execution traces, synthesizing solutions for tasks, and synthesizing new tasks given a solution code. The goal is to benchmark the skills of these models in analyzing, solving and generating visual programming problems using elementary tasks from Hour of Code and Karel domains.In summary, the key research question focuses on assessing if state-of-the-art generative AI models have mastered visual programming skills comparable to their prowess in text-based coding domains. The results have implications for the potential for using these models to support programming education for younger students who are typically introduced to coding via visual languages.
