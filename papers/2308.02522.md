# Evaluating ChatGPT and GPT-4 for Visual Programming

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper addresses is: Do state-of-the-art generative models show advanced capabilities in visual programming on par with their capabilities in text-based Python programming?The paper investigates the capabilities of state-of-the-art generative AI models like ChatGPT (based on GPT-3.5) and GPT-4 in visual programming domains that are popularly used for K-8 programming education, such as Hour of Code and Karel. The goal is to evaluate whether these models have reached the same level of proficiency in visual programming tasks as they have demonstrated in text-based programming domains like Python. The paper systematically tests the models on various scenarios like execution trace generation, solution synthesis, and task synthesis to assess their skills in spatial reasoning, logic, and programming when applied to visual domains. The results indicate that the models still struggle considerably in visual programming compared to text programming, suggesting more work is needed to improve their capabilities. The central research question drives the experiments and analysis throughout the paper.


## What is the main contribution of this paper?

The main contribution of this paper is an evaluation of two state-of-the-art large language models - ChatGPT (based on GPT-3.5) and GPT-4 - for various scenarios in visual programming domains popularly used for K-8 education. Specifically, the authors evaluate these models on three scenarios: execution trace generation, solution synthesis, and task synthesis. They use ten reference tasks from Hour of Code and Karel domains, and assess the performance using expert annotations. The key findings are:- Both models struggle with combining spatial, logical, and programming skills needed for visual programming, performing poorly compared to their capabilities in text-based programming.- GPT-4 shows substantial improvements over ChatGPT, but still achieves only mediocre performance on the elementary-level visual programming tasks.- In execution trace generation, GPT-4 achieves only 60% overall correctness. The generated traces often have incoherent transitions and incorrect sensing. - In solution synthesis, GPT-4 solutions are more similar to minimal solutions than ChatGPT, but achieve only 40% in overall quality. The solutions tend to be unnecessarily complex.- In task synthesis, both models perform very poorly, with only 10-20% overall quality. The generated tasks are often not solvable by the input code.In summary, the main contribution is a rigorous benchmarking of latest generative models on visual programming scenarios, highlighting their limitations and the need for developing techniques to improve their performance in this important educational domain.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

The paper evaluates ChatGPT and GPT-4 in visual programming domains like Hour of Code and Karel. The key finding is that these models struggle to combine spatial, logical, and programming skills crucial for visual programming and perform poorly compared to their capabilities in text-based programming.


## How does this paper compare to other research in the same field?

This paper evaluates the capabilities of large language models (LLMs) like ChatGPT and GPT-4 for visual programming tasks, which is an emerging area of research. Here are some key ways this work compares to prior research:- Focus on visual programming: Most prior work studying LLMs for programming education has focused on text-based programming, especially Python. This paper provides a novel contribution by investigating visual programming domains like Hour of Code maze challenges and Karel programming.- Elementary-level tasks: The evaluation uses basic tasks suitable for K-8 education. This contrasts with much prior work that uses complex programming problems from high school or college courses. The elementary-level tasks pose different challenges for LLMs like combining spatial, logical and programming skills.- Multiple task scenarios: The paper comprehensive evaluates LLMs across three relevant scenarios - generating execution traces, synthesizing solutions, and synthesizing new tasks. Looking at different facets provides a broader picture of capabilities and limitations. - Expert annotations: The paper uses expert-based assessment to evaluate quality, rather than fully automated metrics. This allows incorporating nuanced qualitative judgments. Prior work has used both automated and human-based assessments.- Comparison of two models: By evaluating both ChatGPT and GPT-4, the paper provides useful insights into their relative strengths/weaknesses and the improvements from GPT-3.5 to GPT-4. Prior work has mostly studied capabilities of a single model.Overall, this paper provides a rigorous investigation of an underexplored but important topic. The analysis on visual programming with elementary-level tasks using expert annotations helps advance understanding over prior work and exposes limitations of current LLMs in this domain. The results highlight exciting open challenges for developing LLMs tailored to introductory visual programming.
