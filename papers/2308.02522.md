# Evaluating ChatGPT and GPT-4 for Visual Programming

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper addresses is: Do state-of-the-art generative models show advanced capabilities in visual programming on par with their capabilities in text-based Python programming?The paper investigates the capabilities of state-of-the-art generative AI models like ChatGPT (based on GPT-3.5) and GPT-4 in visual programming domains that are popularly used for K-8 programming education, such as Hour of Code and Karel. The goal is to evaluate whether these models have reached the same level of proficiency in visual programming tasks as they have demonstrated in text-based programming domains like Python. The paper systematically tests the models on various scenarios like execution trace generation, solution synthesis, and task synthesis to assess their skills in spatial reasoning, logic, and programming when applied to visual domains. The results indicate that the models still struggle considerably in visual programming compared to text programming, suggesting more work is needed to improve their capabilities. The central research question drives the experiments and analysis throughout the paper.


## What is the main contribution of this paper?

The main contribution of this paper is an evaluation of two state-of-the-art large language models - ChatGPT (based on GPT-3.5) and GPT-4 - for various scenarios in visual programming domains popularly used for K-8 education. Specifically, the authors evaluate these models on three scenarios: execution trace generation, solution synthesis, and task synthesis. They use ten reference tasks from Hour of Code and Karel domains, and assess the performance using expert annotations. The key findings are:- Both models struggle with combining spatial, logical, and programming skills needed for visual programming, performing poorly compared to their capabilities in text-based programming.- GPT-4 shows substantial improvements over ChatGPT, but still achieves only mediocre performance on the elementary-level visual programming tasks.- In execution trace generation, GPT-4 achieves only 60% overall correctness. The generated traces often have incoherent transitions and incorrect sensing. - In solution synthesis, GPT-4 solutions are more similar to minimal solutions than ChatGPT, but achieve only 40% in overall quality. The solutions tend to be unnecessarily complex.- In task synthesis, both models perform very poorly, with only 10-20% overall quality. The generated tasks are often not solvable by the input code.In summary, the main contribution is a rigorous benchmarking of latest generative models on visual programming scenarios, highlighting their limitations and the need for developing techniques to improve their performance in this important educational domain.
