# [PEFA: Parameter-Free Adapters for Large-scale Embedding-based Retrieval   Models](https://arxiv.org/abs/2312.02429)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a summary paragraph of the key points from the paper:

This paper proposes a plug-and-play framework called PEFA that can enhance existing embedding-based retrieval models (\ERMs) for document retrieval tasks. PEFA incorporates nearest neighbors searched from an approximate nearest neighbor index to smooth the original rankings from the \ERM. Specifically, it linearly interpolates the score vectors from the \ERM and the nearest neighbors to produce improved rankings. Experiments on Natural Questions and TriviaQA datasets demonstrate that PEFA delivers significant gains over standalone \ERMs across various models like SBERT, DPR, MPNet, etc. For instance, on Natural Questions dataset, PEFA improves MPNet's Recall@10 from 80.82\% to 88.72\%, outperforming previous state-of-the-art methods. Ablation studies reveal the impact of the interpolation coefficient and number of neighbors on performance. A key advantage of PEFA is its model-agnostic nature - it can be readily applied to any existing \ERMs without architectural changes or retraining to boost their effectiveness.
