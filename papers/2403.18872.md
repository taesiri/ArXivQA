# [Targeted Visualization of the Backbone of Encoder LLMs](https://arxiv.org/abs/2403.18872)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Encoder-based large language models (LLMs) like BERT are state-of-the-art in NLP but have risks like bias and vulnerability to attacks. 
- There is a need for explainable AI methods to detect such issues. 
- Existing global explainability methods in NLP using dimensionality reduction are limited, often just using t-SNE on embeddings. More advanced methods from other domains are not widely explored.

Proposed Solution:
- Apply DeepView, a recent method to visually inspect classifiers' decision functions in 2D, to BERT NLP models. 
- Modify DeepView for NLP: use BERT embeddings as input space, use cosine distance for regularization.
- Analyze applicability across settings: frozen, fine-tuned, multi-task BERT; normal and adversarially perturbed data.

Contributions:
- Procedure to apply DeepView to different BERT encoder settings.
- Showcase DeepView's usefulness: quickly detect adversarials among thousands of points, understand multi-task model latent space organization, compare model classification strategies. 
- Analysis of embedding spaces w.r.t. proportions of unsupervised vs discriminative distance metrics.
- DeepView helps find model synergies, similarities between models, isolated difficult regions.

In summary, the paper demonstrates the promise of adapting visualization tools like DeepView to understand and analyze risks, biases and decision making in complex NLP models. The methods help bring interpretability and transparency to black-box LLMs.
