# [PeCo: Perceptual Codebook for BERT Pre-training of Vision Transformers](https://arxiv.org/abs/2111.12710)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question is:

How can we learn a better prediction target that aligns with human perception for masked image modeling (MIM) in BERT pre-training of vision transformers? 

The key hypotheses are:

- Current prediction targets like per-pixel regression or discrete tokens from VQ-VAE trained with reconstruction loss disagree with human perceptual judgments. 

- Enforcing perceptual similarity during VQ-VAE training can help learn a perceptual codebook that agrees better with human perception.

- Using this perceptual codebook as prediction targets for MIM will improve BERT pre-training and downstream transfer performance.

In summary, the paper proposes and evaluates a new perceptual codebook as a prediction target for MIM that aims to align better with human perception and enable better BERT pre-training for vision transformers. The central hypothesis is that a perceptually-aligned prediction target will improve pre-training and downstream tasks compared to existing targets like per-pixel or standard VQ-VAE tokens.


## What is the main contribution of this paper?

 Based on my reading of the paper, the main contribution is proposing a new prediction target called "PeCo" (Perceptual Codebook) for BERT pre-training of vision transformers. Specifically:

- The paper observes that current prediction targets for masked image modeling (MIM) in BERT pre-training disagree with human perceptual judgments of image similarity. 

- To address this, the authors propose learning a "perceptual codebook" as the prediction target, where perceptually similar images have close representations in the codebook space. 

- They enforce perceptual similarity during codebook learning by using a perceptual loss based on deep features from a self-supervised vision transformer model. 

- Experiments show the proposed PeCo codebook aligns better with human judgments and leads to improved transfer performance on downstream tasks like image classification, object detection, and semantic segmentation compared to strong baselines.

In summary, the main contribution is proposing a new perceptually-aligned prediction target for MIM in vision BERT pre-training, which improves downstream transfer performance. The key ideas are using a perceptual loss during codebook learning and adopting self-supervised deep features to capture perceptual similarity.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the key points from the paper:

The paper proposes learning a perceptual codebook for BERT pre-training of vision transformers by enforcing perceptual similarity during discrete token learning, showing this aligns better with human judgments and achieves superior transfer performance on image classification, object detection and segmentation compared to using other codebooks.


## How does this paper compare to other research in the same field?

 Based on my reading of the paper, here is a summary of how it compares to other related work:

- This paper focuses on learning a better prediction target for masked image modeling (MIM) in vision transformer pre-training. Most prior work on MIM has focused on different model architectures and training strategies, while using standard pixel-level or discrete token prediction targets. This paper argues these targets do not align well with human perception.

- The key idea proposed is to learn a "perceptual codebook" for the prediction target that better captures semantic similarity between image patches. This is achieved by adding a perceptual loss term when training the VQ-VAE used to generate discrete tokens. 

- This idea of improving the discrete token space is novel compared to prior MIM methods like MAE, SimMIM, BEiT etc. These works use standard VQ-VAE training or DALL-E tokens. The perceptual codebook idea is inspired by work on perceptual losses for image generation.

- The proposed method outperforms strong baselines like BEiT and MAE on image classification, object detection, and semantic segmentation. The gains suggest that the perceptual codebook indeed provides a better foundation for pre-training.

- The idea of perceptual loss for visual representations has been explored before in other contexts like style transfer and feature visualization. But its application to transformer pre-training appears novel and impactful.

- Compared to contrastive methods like MoCo, this work follows the masked prediction paradigm. The gains show it is a promising direction complementary to contrastive learning.

In summary, the key novelty is the idea of improving discrete visual tokens for MIM via perceptual losses. This simple but effective idea for better aligning the prediction target with human perception sets it apart from prior work. The impressive empirical gains validate its benefits for pre-training vision transformers.
