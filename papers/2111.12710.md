# [PeCo: Perceptual Codebook for BERT Pre-training of Vision Transformers](https://arxiv.org/abs/2111.12710)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question is:

How can we learn a better prediction target that aligns with human perception for masked image modeling (MIM) in BERT pre-training of vision transformers? 

The key hypotheses are:

- Current prediction targets like per-pixel regression or discrete tokens from VQ-VAE trained with reconstruction loss disagree with human perceptual judgments. 

- Enforcing perceptual similarity during VQ-VAE training can help learn a perceptual codebook that agrees better with human perception.

- Using this perceptual codebook as prediction targets for MIM will improve BERT pre-training and downstream transfer performance.

In summary, the paper proposes and evaluates a new perceptual codebook as a prediction target for MIM that aims to align better with human perception and enable better BERT pre-training for vision transformers. The central hypothesis is that a perceptually-aligned prediction target will improve pre-training and downstream tasks compared to existing targets like per-pixel or standard VQ-VAE tokens.


## What is the main contribution of this paper?

 Based on my reading of the paper, the main contribution is proposing a new prediction target called "PeCo" (Perceptual Codebook) for BERT pre-training of vision transformers. Specifically:

- The paper observes that current prediction targets for masked image modeling (MIM) in BERT pre-training disagree with human perceptual judgments of image similarity. 

- To address this, the authors propose learning a "perceptual codebook" as the prediction target, where perceptually similar images have close representations in the codebook space. 

- They enforce perceptual similarity during codebook learning by using a perceptual loss based on deep features from a self-supervised vision transformer model. 

- Experiments show the proposed PeCo codebook aligns better with human judgments and leads to improved transfer performance on downstream tasks like image classification, object detection, and semantic segmentation compared to strong baselines.

In summary, the main contribution is proposing a new perceptually-aligned prediction target for MIM in vision BERT pre-training, which improves downstream transfer performance. The key ideas are using a perceptual loss during codebook learning and adopting self-supervised deep features to capture perceptual similarity.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the key points from the paper:

The paper proposes learning a perceptual codebook for BERT pre-training of vision transformers by enforcing perceptual similarity during discrete token learning, showing this aligns better with human judgments and achieves superior transfer performance on image classification, object detection and segmentation compared to using other codebooks.
