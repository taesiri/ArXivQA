# [Graph Convolutions Enrich the Self-Attention in Transformers!](https://arxiv.org/abs/2312.04234)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a summary paragraph of the key points from the paper:

This paper proposes a novel graph filter-based self-attention (GFSA) mechanism to enrich Transformers with more diverse frequency information and address the oversmoothing problem. The key idea is to interpret the original self-attention as a simple graph filter and redesign it from a graph signal processing perspective. Specifically, the proposed GFSA mechanism learns a more complex graph filter consisting of an identity term and two matrix polynomial terms - the original self-attention matrix plus an approximated higher order term. This allows GFSA to capture both low and high frequency signals, alleviating the tendency of standard self-attention to act as a low-pass filter that loses distinctiveness. GFSA is shown to deliver consistent performance improvements across Transformers in computer vision, natural language processing, speech recognition and other domains, with negligible computational overhead. For instance, GFSA boosted top-1 accuracy in vision by 1.63\%, BLEU score in translation by 1.05\% and word error rate in speech recognition by 6.53\%, with marginal increase in parameters and training time. Thus, the proposed graph filter perspective provides a simple yet effective approach to address limitations of the prevalent self-attention mechanism.
