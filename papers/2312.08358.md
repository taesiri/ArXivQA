# [Distributional Preference Learning: Understanding and Accounting for   Hidden Context in RLHF](https://arxiv.org/abs/2312.08358)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a high-quality paragraph summarizing the key points of the paper:

This paper studies the problem of hidden context in reinforcement learning from human feedback (RLHF), which arises when the information used by annotators to provide preference labels differs from what is seen by the learned preference model. They show that standard preference learning methods implicitly aggregate over hidden context via a voting rule called Borda count, which can lead to counterintuitive outcomes. A key result relates preference learning from diverse annotators to social choice theory. This demonstrates that annotators have incentives to manipulate preference labels to influence the final model. To mitigate issues of hidden context, the authors propose distributional preference learning (DPL), which estimates a distribution over utility values to expose uncertainty from missing information. On a large language model experiment, DPL detects conflicting objectives in the data and enables risk-averse optimization to reduce harmful behaviors. Overall, identifying and accounting for hidden context is crucial for safe and beneficial deployment of AI systems that learn preferences.
