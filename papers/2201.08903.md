# [Universal Online Learning with Unbounded Losses: Memory Is All You Need](https://arxiv.org/abs/2201.08903)

## What is the central research question or hypothesis that this paper addresses?

This paper addresses an open problem posed in a prior work by Hanneke (2021) on the subject of universally consistent online learning with unbounded losses. Specifically, the open problem asks whether, for unbounded losses, the family of processes admitting universal learning (denoted SUOL) are precisely those having a finite number of distinct values almost surely (denoted FS).The main result of this paper is proving that indeed SUOL = FS when the loss is unbounded. This resolves the open problem completely.Some key points:- The result implies a dramatic simplification of the theory of universal online learning with unbounded losses. In particular, the simple "memorization" algorithm that just remembers past examples is shown to be an optimistically universal learning rule. - The proof relies on constructing random measurable partitions of the instance space X that "shatter" X finely enough so that any process taking infinite values will visit infinitely many partition sets.- As a consequence, the paper also simplifies prior results on the equivalences between universal learning in online, inductive, and self-adaptive settings when the loss is unbounded.In summary, the main contribution is resolving a prior open problem by proving an equivalence between two conditions on stochastic processes, thereby simplifying the theory of universal online learning for unbounded losses. The proof technique of constructing random partitions may be of independent interest.
