# [Universal Online Learning with Unbounded Losses: Memory Is All You Need](https://arxiv.org/abs/2201.08903)

## What is the central research question or hypothesis that this paper addresses?

This paper addresses an open problem posed in a prior work by Hanneke (2021) on the subject of universally consistent online learning with unbounded losses. Specifically, the open problem asks whether, for unbounded losses, the family of processes admitting universal learning (denoted SUOL) are precisely those having a finite number of distinct values almost surely (denoted FS).The main result of this paper is proving that indeed SUOL = FS when the loss is unbounded. This resolves the open problem completely.Some key points:- The result implies a dramatic simplification of the theory of universal online learning with unbounded losses. In particular, the simple "memorization" algorithm that just remembers past examples is shown to be an optimistically universal learning rule. - The proof relies on constructing random measurable partitions of the instance space X that "shatter" X finely enough so that any process taking infinite values will visit infinitely many partition sets.- As a consequence, the paper also simplifies prior results on the equivalences between universal learning in online, inductive, and self-adaptive settings when the loss is unbounded.In summary, the main contribution is resolving a prior open problem by proving an equivalence between two conditions on stochastic processes, thereby simplifying the theory of universal online learning for unbounded losses. The proof technique of constructing random partitions may be of independent interest.


## What is the main contribution of this paper?

This paper resolves an open problem posed in a previous work by Hanneke (2021) regarding the characterizations of stochastic processes that admit universal online learning with unbounded losses. The main contributions are:- It proves that for any unbounded loss function, a stochastic process admits universal online learning if and only if it almost surely takes only a finite number of distinct values. This completely resolves the open problem posed by Hanneke.- As a consequence, this result greatly simplifies the theory of universal online learning with unbounded losses. In particular, it shows that the simple "memorization" algorithm that just remembers past input-output pairs is an optimistically universal online learning rule. This is much simpler than the algorithm proposed in the original work by Hanneke.- The proof relies on constructing random measurable partitions of the input space that can isolate distinct values taken by the stochastic process. This construction may be of independent interest.- The results are extended to inductive and self-adaptive learning setups, showing they admit the same characterizations.- The paper also considers a noisy (non-realizable) setting and proves the existence of an optimistically universal Bayes consistent learning rule when the loss is unbounded.In summary, the paper resolves a major open problem regarding universal online learning with minimal assumptions, significantly simplifying the theory and providing intuitive characterizations. The random partition technique introduced could be useful for tackling other learning theory problems involving stochastic processes.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key point:This paper resolves an open problem on universal online learning with unbounded losses, proving that the processes admitting universal learning are exactly those with finite support almost surely, allowing dramatically simpler optimistically universal learning algorithms.


## How does this paper compare to other research in the same field?

This paper makes an important contribution to the field of online and universal learning theory by resolving an open problem posed in a previous work on the subject. Specifically:- The paper resolves Open Problem 4 from Hanneke 2021 on characterizing the set of processes that admit universal online learning with unbounded losses. It proves that this set is equivalent to processes with finite support almost surely.- This dramatically simplifies the theory and algorithms for universal online learning with unbounded losses compared to the original results of Hanneke 2021. In particular, it shows the simple "memorization" algorithm is optimistically universal.- The proof technique relies on a novel construction of random measurable partitions that could find other applications in learning theory. - The result also simplifies the characterization of processes admitting universal inductive and self-adaptive learning with unbounded losses.- The paper further extends the results to the non-realizable Bayesian setting, providing an optimistically universal Bayes consistent learner.Overall, this represents an important step forward in understanding the fundamental limits of online and universal learning under minimal assumptions. The proof techniques are innovative and help clarify the types of processes that allow for universal learning with unbounded losses. By resolving an open problem, the work also moves the field forward and simplifies theory and algorithms compared to prior work. The results on inductive, self-adaptive, and Bayesian learning nicely complement the main contributions as well.


## What future research directions do the authors suggest?

The authors suggest a few potential directions for future work:1. Investigate universal online learning under additional moment conditions on the sequence Y_t. The authors note that most prior work on unbounded losses requires moment conditions on Y_t (e.g. finite variance in regression). So it would be interesting to study whether stronger positive results are possible by adding moment conditions, while still allowing very general stochastic processes X_t beyond just i.i.d.2. Study universal online learning with bounded losses. The characterization of the set of processes admitting universal consistency remains open for bounded losses. The authors conjecture a specific condition based on the growth rate of the number of visited sets in measurable partitions. 3. Explore connections to metric compression schemes. The authors note the results on optimistically universal Bayes consistent learning connect to recent work on compression-based algorithms. Further exploring these connections could be interesting.4. The inductive and self-adaptive learning setups with bounded losses remain not fully resolved. While the self-adaptive case admits an optimistically universal learner, the inductive case does not. Further understanding the differences between these models in the bounded loss case is an open problem.5. Apply the constructive techniques for random measurable partitions developed here to other open problems in learning theory. The proof techniques used here to construct the partitions may be useful for other settings as well.In summary, the main open directions are: (1) incorporating moment conditions on Y_t, (2) resolving bounded losses, (3) connections to compression schemes, (4) differences between inductive and self-adaptive learning, and (5) applying the proof techniques more broadly. The results here provide a foundation for studying universal online learning under minimal assumptions.


## Summarize the paper in one paragraph.

Here is a one paragraph summary of the paper:This paper resolves an open problem posed in a previous work by Hanneke (2021) regarding the theory of universally consistent online learning with non-i.i.d. processes and unbounded losses. Specifically, Hanneke had defined the notion of an "optimistically universal" learning rule that achieves low long-run average loss whenever this is achievable by some learning rule, and posed the open question of whether for unbounded losses the family of processes admitting universal learning are precisely those with a finite number of distinct values almost surely. This paper proves that this is indeed the case, allowing for a dramatically simpler formulation of an optimistically universal learning rule as simply the "memorization" strategy that remembers all past data points. The proof relies on constructing random measurable partitions of the instance space. As a consequence, the paper also simplifies and extends previous results on the equivalence of universal learning in online, inductive, and self-adaptive settings with unbounded losses. An extension to Bayes consistency in a noisy setting is also provided.


## Summarize the paper in two paragraphs.

Here is a two paragraph summary of the paper:This paper resolves an open problem posed in a previous work by Hanneke (2021) regarding universal online learning with non-i.i.d. processes and unbounded losses. The notion of an "optimistically universal learning rule" was defined by Hanneke (2021) to study learning theory under minimal assumptions. Such a rule achieves low long-run average loss whenever this goal is achievable by some learning rule for the given data generating process. Hanneke (2021) posed as an open problem whether, for unbounded losses, the processes admitting universal learning are exactly those with a finite number of distinct values almost surely. This paper proves this conjecture, showing the equivalence does indeed hold. As a result, the memorization algorithm that simply remembers past data is an optimistically universal learning rule for unbounded losses. The proof involves constructing random measurable partitions of the instance space. This characterization also simplifies the theory of inductive and self-adaptive learning with unbounded losses. The results are extended to a noisy setting, providing an optimistically universal Bayes consistent learning rule. Overall, this paper resolves the open problem posed by Hanneke (2021) regarding characterizing the processes that admit universal online learning for unbounded losses.
