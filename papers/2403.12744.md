# [Instructing Large Language Models to Identify and Ignore Irrelevant   Conditions](https://arxiv.org/abs/2403.12744)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
Existing chain-of-thought (CoT) prompting methods have shown promise in eliciting multi-step reasoning abilities from large language models (LLMs) to solve math word problems (MWPs). However, they get confused by irrelevant conditions in the problem descriptions, resulting in low accuracy. The paper points out that simply instructing the LLM to "ignore irrelevant conditions" is not enough, as the irrelevant conditions are not explicitly identified. 

Proposed Solution: 
The paper proposes a new approach called I^3C that:

1) Identifies a set of irrelevant condition candidates using a semantic relevance score between each condition and the question. Conditions with relevance below a threshold are selected as candidates.

2) Verifies if the candidates are indeed irrelevant by prompting the LLM with a natural language question about each condition's relevance. 

3) Constructs a new I^3C instruction with all verification outputs, instructing the LLM on which conditions are irrelevant.  

The I^3C instruction is added to existing CoT prompts. An advanced variant called I^3C-Select also automatically selects the most confusing problems and their reasoning paths as demonstrations for few-shot learning.

Main Contributions:

- Proposes a novel approach I^3C to identify and instruct LLMs to ignore irrelevant conditions when solving math word problems, avoiding confusion.

- Develops an effective prompt augmentation method that boosts performance of any CoT prompting baseline.

- Designs an automatic demonstration selection algorithm to enable few-shot learning.

- Experiments show state-of-the-art accuracy on 8 MWP datasets. I^3C achieves 5-12% absolute improvement over the strongest baseline.

In summary, the paper makes significant contributions towards enhancing LLMs' reasoning ability on complex MWPs in the presence of irrelevant information. The proposed methods are model-agnostic and demonstrate strong empirical results.
