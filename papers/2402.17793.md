# [A Surprising Failure? Multimodal LLMs and the NLVR Challenge](https://arxiv.org/abs/2402.17793)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

This paper evaluates three state-of-the-art multimodal large language models (MLLMs) - GPT-4V, Gemini Pro, and IDEFICS - on the Natural Language Visual Reasoning (NLVR) task. NLVR requires determining if a human-written sentence is true or false with respect to a synthetic image containing simple geometric shapes. The goal of NLVR is to assess models' ability to demonstrate compositional and spatial reasoning skills.

The models are evaluated in both zero-shot and few-shot settings. For zero-shot, the best performing prompt is selected for each model from a set of candidate prompts. For few-shot, the prompts are conditioned on 5 training examples before evaluating on the test set. The models are compared to human performance and previous state-of-the-art on NLVR.

The key results are:
- All models perform significantly worse than human performance and previous SOTA: The best accuracy is 59.9% by GPT-4V with zero-shot prompting, compared to 95.4% for humans and 78.3% for previous SOTA.
- Few-shot prompting hurts performance for GPT-4V and IDEFICS but helps Gemini Pro. 
- Fine-tuning helps IDEFICS, achieving 59.7% accuracy, but still far below human performance.
- The models are biased in their predictions, skewed towards True or False.

The key conclusion is that despite the strong performance of MLLMs on several vision-language tasks, they still struggle with the fundamental compositional and spatial reasoning skills tested by NLVR. There remains significant room for improvement.
