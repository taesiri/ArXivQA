# [Local Reweighting for Adversarial Training](https://arxiv.org/abs/2106.15776v1)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question appears to be: 

How to improve the robustness of neural networks against adversarial attacks through a modified adversarial training approach that reweights instances locally instead of globally?

The key hypotheses seem to be:

1) Adversarial training methods that globally reweight instances based on some measure of "importance" can over-discard instances. This may significantly reduce robustness against unseen attacks compared to standard adversarial training with no reweighting. 

2) The vulnerability of instances is attack-dependent. An instance deemed unimportant under one attack may still be important for defending against a different attack.

3) A local reweighting approach, which assigns weights to each adversarial variant of an instance separately rather than discarding entire instances globally, can improve robustness against the attacks simulated during training while also maintaining better generalization against unseen attacks.

In summary, the central research question is how to design an effective adversarial training method using instance reweighting that improves robustness against a diverse set of attacks, both simulated and unseen during training. The key hypothesis is that local reweighting of instance variants can achieve this better than global instance reweighting or no reweighting.


## What is the main contribution of this paper?

 Based on my reading of the paper, here is a summary of the main contribution:

The paper proposes a new adversarial training method called Locally Reweighted Adversarial Training (LRAT) to improve model robustness against adversarial attacks. 

The key ideas are:

- Prior work on Instance Reweighted Adversarial Training (IRAT) assigns smaller/larger weights to instances less/more vulnerable to a given attack during training. However, IRAT leads to significantly reduced robustness on unseen attacks. 

- The paper argues that instance vulnerability is attack-dependent. The same instance can be vulnerable or not depending on the attack type. Thus, IRAT's global reweighting is misleading when the attack in training is mis-specified.

- LRAT pairs each instance with its adversarial variants and performs local reweighting within each pair, without global reweighting across instances. The rationale is to fit the instance if immune to the given attack but retain the pair to passively defend unseen attacks. 

- LRAT uses a general vulnerability-based reweighting strategy applicable to various attacks, unlike the heuristics of IRAT limited to one attack type.

- Experiments show LRAT outperforms IRAT and standard adversarial training when trained on one attack type but tested on other unseen attacks.

In summary, the main contribution is a new adversarial training method LRAT that performs local instead of global reweighting to improve robustness against diverse adversarial attacks. The proposal of attack-dependent local reweighting is the key idea.
