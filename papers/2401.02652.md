# [Adaptive Discounting of Training Time Attacks](https://arxiv.org/abs/2401.02652)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Reinforcement learning (RL) agents are vulnerable to training-time attacks (TTAs) where an adversary poisons the agent's training environment to manipulate its learned behavior. 
- Prior constructive TTAs focused on enforcing target behaviors that are optimal for the victim's objectives but unachievable due to environment constraints. 
- This paper investigates a more challenging setting - enforcing non-optimal target behaviors that are infeasible due to both environment constraints and misalignment with the victim's rewards.

Proposed Solution:
- The authors develop a variant of DDPG called γDDPG that uses a dynamic discount factor γ to balance two attack objectives: maximizing attack accuracy (victim's adoption of the target behavior) and minimizing attack effort (changes to the environment).
- γ adapts based on current attack accuracy and effort to bound effort while enabling accuracy maximization. This reduces uncertainty and creates a bounded search space for finding high-accuracy, low-effort attack strategies.  
- Four adaptive discount functions are proposed based on KL divergence or Wasserstein distance between combinations of the current environment, current/target victim behavior and original environment.

Main Contributions:
- Novel constructive black-box TTA setting where target behavior is non-optimal for the victim's objectives.
- γDDPG algorithm with dynamic discounting to enable dual-priority multi-objective optimization for TTA.
- Using discount factor to create bounded search spaces that reduce uncertainty and facilitate optimization.
- Comparative analysis of proposed discount functions and demonstration of improved attack performance over prior state-of-the-art.

In summary, this paper pushes TTAs to a more challenging setting and develops a specialized RL algorithm to efficiently learn manipulation strategies in this setting. The dynamic discounting approach is shown to be promising for balancing multiple attack objectives.


## Summarize the paper in one sentence.

 Here is a one-sentence summary of the paper:

The paper proposes a novel reinforcement learning algorithm called $\gamma$DDPG that uses an adaptive discount factor to balance multiple attack objectives and reduce uncertainty in order to efficiently push victim agents towards non-optimal target behaviors during training-time attacks.


## What is the main contribution of this paper?

 Based on my understanding, the main contribution of this paper is proposing a new reinforcement learning algorithm called $\gamma$DDPG that uses an adaptive Bellman discount function to support dual-priority dual-objective optimization. Specifically:

- It introduces a novel category of training-time environment-poisoning attacks that aim to push the victim agent towards adopting a non-optimal target behavior that is un-adoptable due to both environment dynamics and non-optimality with respect to the victim's objectives.

- The adaptive discount function in $\gamma$DDPG creates a bounded search space at each timestep based on current attack accuracy and effort. This allows prioritizing attack accuracy (higher priority objective) while bounding attack effort (lower priority objective). 

- The bounded search space also reduces the effect of uncertainty in the partially observable environment, improving optimization capability.

- Experiments show that the Wasserstein distance based dynamic discount in $\gamma$DDPG outperforms the best fixed discount baseline and state-of-the-art attack method in terms of attack accuracy, effort, and other metrics.

In summary, the key innovation is using an adaptive discount function in RL to support dual-priority multi-objective optimization and construct a more potent training-time attack.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, here are some of the key keywords and terms:

- Constructive training-time attacks
- Environment poisoning
- Reinforcement learning
- Dual-priority dual-objective MDP
- Dynamic discount function
- Attack accuracy
- Attacker effort  
- $\gamma$DDPG algorithm
- Adaptive Bellman discount
- Bounded search space
- Target behavior
- Victim environment dynamics
- Wasserstein distance
- Kullback-Leibler divergence rate

The paper introduces a constructive training-time attack framework where an attacker reinforcement learning agent modifies the environment dynamics to force a victim agent to learn a specific target behavior. Key concepts include using a dynamic discount function in the $\gamma$DDPG algorithm to balance the objectives of maximizing attack accuracy and minimizing attacker effort, as well as using metrics like Wasserstein distance and KL divergence to measure attack performance. The adaptive discount creates a bounded search space to aid optimization. Overall, the key focus is on developing more efficient constructive attacks that can enforce non-optimal target behaviors on victims by poisoning their training environments.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in the paper:

1. The paper proposes using both reward and adaptive discounting to encode and prioritize attack objectives in a dual-priority dual-objective framework. How does encoding the attack accuracy objective in the reward and the attack effort objective in the discount function allow for more effective optimization compared to prior approaches?

2. The proposed $\gamma$DDPG algorithm uses a dynamic discount factor to bound the search space based on current attack accuracy and effort. How does this bounded search space help reduce uncertainty and improve optimization capability compared to using a fixed discount? 

3. The paper introduces four different formulations for the adaptive discount - TargetKLR, TargetWD, KLR, and WD. What are the relative advantages and disadvantages of using KL divergence versus Wasserstein distance in these formulations? 

4. The attacker's state space consists of both the current victim environment dynamics and an encoding of the victim's behavior. Why is it necessary to include information about the victim's behavior in the state space for an effective attack?

5. An autoencoder model is used to learn a low-dimensional encoding of the victim's behavior traces. What are the benefits of using an autoencoder over directly including the raw behavior traces in the attacker's state space?

6. During testing, the attack strategies are evaluated on victim agents initialized with different random seeds. Why is testing generalization to different victim initializations important for assessing attack performance?  

7. The results show that the WD dynamic discount achieves the best test-time performance. What specific advantages does the WD formulation provide over other discounts in this context?

8. How would the proposed attack method need to be modified to work in continuous state and action spaces instead of the discrete environment used in the paper?

9. The paper assumes a white-box victim environment but black-box access to the victim's internal mechanisms. How could the attack be enhanced if additional white-box information about the victim's learning process was available?

10. The attack objective is to force the victim to learn a non-optimal behavior according to its own objectives. What are some ways the victim could detect or defend against this type of attack?
