# [Reward learning from human preferences and demonstrations in Atari](https://arxiv.org/abs/1811.06521)

## What is the central research question or hypothesis that this paper addresses?

 The central research question this paper addresses is how to learn a reward function for reinforcement learning agents directly from human feedback, without relying on manually specified reward functions. 

The paper investigates combining two approaches for communicating human preferences to an agent: expert demonstrations and trajectory preferences. The key hypothesis is that combining these two forms of human feedback - demonstrations to guide exploration and preferences for more efficient communication - will allow an agent to learn a reward function that approximates the intended behavior, and optimize a policy accordingly to solve complex tasks.

The paper tests this hypothesis by training deep neural network agents with Q-learning on 9 Atari games. The agents are provided with expert demonstrations and preference feedback (from either synthetic or human annotators) to learn reward models, which are then used to train the policies. The results show that the combination of demonstrations and preferences outperforms using either in isolation, validating the central hypothesis.

In summary, the key research question is how agents can learn from human feedback, with a hypothesis that demonstrations and preferences together can teach a reward function to solve complex tasks like Atari games. The experiments confirm that this combination is more effective than either approach alone.


## What is the main contribution of this paper?

 The main contribution of this paper is combining human demonstrations and trajectory preferences to learn a reward model that can be used to train a reinforcement learning agent to play Atari games. Specifically:

- They show that adding a small number of expert demonstrations to preference-based reward learning substantially improves performance compared to just using preferences or just imitating the demonstrations. 

- They demonstrate this approach on 9 Atari games, achieving superhuman performance on 2 games without access to the true game reward signals.

- They analyze the quality of the learned reward function, including its alignment with the true reward and susceptibility to reward hacking. 

- They find that synthetic preferences are generally more effective for learning than real human feedback, except in one game where the human provides useful implicit shaping.

- They show their approach works with both value-based (DQN) and policy gradient (A3C) RL algorithms.

So in summary, the key contribution is showing that combining demonstrations and preferences is an effective approach for learning complex reward functions and training RL agents without access to the true rewards. The analysis also provides insights into the strengths and weaknesses of learning rewards from human feedback.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the key points in the paper:

The paper proposes combining imitation learning from expert demonstrations with reward learning from human preferences to train deep reinforcement learning agents to play Atari games, showing this approach can exceed human performance on some games without access to the true reward signal.


## How does this paper compare to other research in the same field?

 Here are some key ways this paper on reward learning compares to other related work:

- It combines two approaches to learning from human feedback - expert demonstrations and trajectory preferences. Most prior work focused on just one of these approaches. Combining them provides benefits of both - demonstrations aid exploration while preferences allow exceeding human performance.

- It trains a deep neural network to model the reward function, rather than training the policy directly on human feedback. This allows optimizing the policy with standard RL algorithms using the learned reward.

- It evaluates the approach on a diverse set of 9 Atari games. Many prior papers only used 1-2 environments. Testing on more games provides a more thorough evaluation.

- It investigates the quality and alignment of the learned reward function in detail. Most prior work evaluated only end task performance, not reward accuracy. 

- It compares human versus synthetic feedback. Human feedback often differs from the true game reward, which highlights issues like reward hacking.

- It shows the approach works with both policy gradient (A3C) and value-based (DQN) RL algorithms. Many papers only try one algorithm.

- It examines how varying the amount of human feedback affects results. More feedback improves performance, but even little feedback combined with demos works decently.

Overall, the breadth and rigor of the experiments provides significant evidence that this hybrid approach combining preferences and demonstrations can effectively train RL agents without access to the true reward function. The analysis also sheds light on potential issues to address in future work.


## What future research directions do the authors suggest?

 Based on my reading of the paper, here are some of the future research directions the authors suggest:

- Investigate other RL algorithms besides DQN/DQfD and A3C to see if learning rewards generalizes across different algorithms.

- Try different methods for selecting video clips to label, such as active learning based on the reward model's uncertainty.

- Explore other techniques to improve the sample efficiency of the reward model, such as semi-supervised learning leveraging the unlabeled environment observations.

- Study methods to increase the robustness of DQfD to differences between demo observations and agent observations.

- Compare reward learning performance when using real human feedback versus synthetic oracle feedback.

- Evaluate the approach on more complex environments beyond Atari games.

- Further analyze the reward model alignment and sources of misalignment between true and learned reward. 

- Develop techniques to avoid reward hacking, where the agent exploits inaccuracies in the learned reward function.

- Experiment with different schedules for phasing out the expert demonstrations to prevent negative transfer.

- Investigate whether sharing representations between the policy and reward model can improve learning.

- Evaluate how well the method scales to using preferences from multiple different annotators.

So in summary, they suggest directions such as testing different RL algorithms, improving sample efficiency, avoiding reward hacking, conducting more robustness testing, and scaling up the approach to more complex domains and multiple annotators. The key theme is enhancing and evaluating the generality of their human-in-the-loop reward learning approach.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the paper:

The paper proposes a method for training reinforcement learning agents to play Atari games using human feedback instead of game rewards. The approach combines imitation learning from expert demonstrations and reward learning from human preferences over short video clips of agent gameplay. First, the agent is pretrained on demonstrations using behavioral cloning and DQfD. Then a neural network reward model is trained on clip preferences provided by humans online as the agent collects more experience. The agent policy is trained with Q-learning, using rewards predicted by the reward model. On 9 Atari games, this approach outperforms learning from just demonstrations or just preferences. Combining the two feedback types requires less human time and enables learning in hard exploration games like Montezuma's Revenge. The method achieves superhuman scores on Pong and Enduro without using game rewards. Reward quality and alignment are analyzed, showing reasonable correlation with true rewards. Overall, the work demonstrates that complex behaviors can be learned from limited non-expert human feedback combined with demonstrations.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the key points from the paper:

This paper proposes a method for training reinforcement learning agents to play Atari games without access to the game's reward function. The approach combines learning from two types of human feedback: expert demonstrations of gameplay, and preferences between pairs of short gameplay clips. First, the agent is pretrained on the demonstrations using imitation learning. This provides an initial policy and exploration data to elicit preferences from the human. The preferences are used to train a neural network reward model. This model provides rewards to train the agent policy using deep Q-learning, while continuing to collect preferences. 

The method is evaluated on 9 Atari games, comparing performance when using just demonstrations, just preferences, or both. The combination consistently outperforms either approach alone. In 2 games, it exceeds human performance without using the true game reward signal. The learned reward model aligns well with true game reward when preferences are simulated based on game score. With human preferences, misalignments can occur due to differing human reward objectives. Overall, this work demonstrates that combining demonstrations and preferences enables deep reinforcement learning without manually specifying rewards, across diverse and complex environments.


## Summarize the main method used in the paper in one paragraph.

 The main method used in this paper is a combination of imitation learning from expert demonstrations and reinforcement learning from human preferences. 

The agent is first pretrained on expert demonstrations using behavioral cloning. This provides a good initialization for the policy. Then the agent collects some initial trajectories which are used to elicit preferences from a human. These preferences are used to train a reward model. The policy is further pretrained on the demonstrations using the rewards predicted by this initial reward model. Then begins the main training loop where the policy is trained with RL using the predicted rewards, while more preferences are collected from the human to continue training the reward model. So the key components are: pretraining the policy on demonstrations, collecting preferences to train a reward model, using the predicted rewards to train the policy with RL, and continuously collecting more preferences during training to improve the reward model.
