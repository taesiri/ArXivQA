# [The Duet of Representations and How Explanations Exacerbate It](https://arxiv.org/abs/2402.08379)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Algorithms effect causal representations relating features to labels in humans' perceptions. These can conflict with humans' prior beliefs and lead to suboptimal decision making when algorithms are used to aid human judgment.  
- Explanations are meant to facilitate human-algorithm communication but they direct attention to sparse, cogent representations. This increases the salience of any conflict, exacerbating disagreements in representations and worsening decision quality.

Proposed Solution: 
- The authors conduct a field experiment in a public employment agency where counselors use an ML model to predict candidates' risk of long-term unemployment. One group gets explanations from SHAP in addition.
- They identify a feature where the algorithm's representation conflicts with counselors' priors using LASSO regression on model predictions and human actions. The conflict feature is college education.
- Regressions show that exposing the conflict feature via explanations reduces decision quality and confidence when counselors adjust the model's predictions, indicating worse information processing.

Main Contributions:
- Identify and formalize the "duet of representations" concept where algorithm representations conflict with human priors
- Show via field experiment that explanations exacerbate this conflict and hurt decision quality by increasing psychological salience
- Argue communication requires understanding, reciprocity, negotiability and shared reality rather than just extracting representations

In summary, the key insight is that explanations highlight conflicts between human and algorithm representations. This focuses attention in a way that worsens decision making. More cooperative, two-way communication is needed instead.


## Summarize the paper in one sentence.

 Here is a one-sentence summary of the paper:

The paper presents a field experiment showing that explanations which make conflicting feature representations salient can exacerbate the conflict between an algorithm's causal representation and a human's prior beliefs, worsening judgment quality when the human adjusts the algorithm's prediction.


## What is the main contribution of this paper?

 Based on my reading of the paper, the main contribution is:

The paper presents empirical evidence from a field experiment showing that explanations can exacerbate the conflict between the causal representation effected by a machine learning model and a human's prior beliefs. Specifically, by directing attention to features where there is disagreement, explanations make this conflict more salient. This can adversely affect the quality of human decision-making, as evidenced by the worse performance of counselors who were shown explanations highlighting the feature "college education" which conflicted with their prior belief. 

The paper argues that effective human-AI collaboration requires going beyond just communicating representations extracted by explanations. It proposes four desiderata - understanding, reciprocity, negotiability, and a shared reality - to enable a more extensive communicative rationality that can improve decision quality in the face of conflicting representations.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, the keywords or key terms associated with it include:

- human-AI interaction 
- communication
- causal representations
- prior beliefs
- biases
- explanations
- epistemic standpoint
- salience 
- conflict
- information processing

The paper discusses issues around how humans interact and communicate with AI algorithms epistemically, how causal representations effected by algorithms can conflict with humans' prior beliefs, and how explanations can exacerbate this conflict by increasing the salience of features where there is disagreement. This can then negatively impact the quality of human decision-making. The paper suggests desiderata around understanding, reciprocity, negotiability, and shared reality to enable more robust human-algorithm interaction.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in the paper:

1. The paper argues that effective collaboration between humans and algorithms requires understanding the "whys" behind the causal representations effected by algorithms. What specific steps could be taken to help humans better understand these "whys"? For example, providing more details on model training, data collection processes, etc.

2. The paper identifies college education as a feature where the human prior belief conflicts with the representation attributed to the algorithm. What are some potential reasons why college education could be positively associated with long-term unemployment in the model? How could these potential reasons be better communicated to human users? 

3. The authors use LASSO regression to extract sparse linear models representing the prior human belief and the representation attributed to the algorithm. What are the limitations of this approach? Could other methods like partial dependence plots better capture potential non-linear relationships and interactions?

4. The paper argues that explanations can direct attention and thus exacerbate the conflict between human prior beliefs and representations attributed to algorithms. What alternative explanation methods could potentially minimize this effect? For example, conditional expectation plots?  

5. The desiderata for communicative rationality between humans and algorithms are proposed to be understanding, reciprocity, negotiability and shared reality. What specific steps could be taken in the context of this field experiment to work towards satisfying each of these desiderata?

6. The explanations are generated post-hoc using SHAP after the model has already been trained. How could explanation methods be directly incorporated into model training instead? What effect could this have?

7. What validation approaches could be used to evaluate whether the extracted human prior belief accurately reflects the true underlying belief? Surveys? Follow-up interviews? Blind prediction tasks?

8. What data would need to be collected to assess converegence towards a shared reality over time as proposed under the fourth desideratum? Longitudinal accuracy data for both the algorithm and human decisions? Qualitative interviews on evolution of views over time?  

9. How could the proposed approach extend to more complex real-world collaborative human-AI systems involving multiple algorithms and interfaces? What additional complications need to be addressed?

10. The paper studies decision quality and confidence, but what other potential outcome variables could be examined to study the effect of explanations and communicative rationality? Decision time? Perceived trustworthiness of the algorithm over time? Others?
