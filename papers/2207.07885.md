# [Clover: Towards A Unified Video-Language Alignment and Fusion Model](https://arxiv.org/abs/2207.07885)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading, the central research question/hypothesis of this paper is: 

How can we build a unified video-language model that achieves both high efficiency and performance for solving various video understanding tasks like text-video retrieval and video question answering?

The authors observe that existing video-language pre-training methods rely on simple supervised or contrastive pre-text tasks that cannot align and fuse features from different modalities well. This leads to models compromising between efficiency and performance when transferred to different downstream tasks. 

To address this, the authors propose a new pre-training method called Clover that better correlates cross-modal alignment and fusion via a novel tri-modal alignment task. They hypothesize that by explicitly associating the uni-modal and multi-modal encoders, Clover can achieve mutual improvement in alignment and fusion capabilities. This will lead to a unified model that attains both high efficiency and performance on multiple video understanding tasks.

In summary, the central hypothesis is that by correlating cross-modal alignment and fusion during pre-training, their model Clover can achieve state-of-the-art performance on both efficient retrieval and joint reasoning tasks. The key novelty is the tri-modal alignment pre-training task.


## What is the main contribution of this paper?

 Based on my reading, the main contributions of this paper are:

1. It proposes Clover, a new end-to-end video-language pre-training method that unifies video-language alignment and fusion. 

2. It introduces a novel tri-modal alignment (TMA) pre-training task that correlates the uni-modal encoders and multi-modal encoder. TMA helps get the cross-modal alignment and fusion capability mutually improved.

3. It proposes a semantic masking strategy and a pair-wise ranking loss to further enhance the tri-modal alignment. 

4. It establishes new state-of-the-art results on multiple downstream tasks, including three text-video retrieval tasks under both zero-shot and fine-tuning settings, and eight video question answering tasks.

In summary, the key innovation is the proposed tri-modal alignment pre-training task that enables a unified and efficient video-language model with strong alignment and fusion capabilities. The experiments demonstrate its effectiveness and superior performance on various downstream tasks compared to previous methods.
