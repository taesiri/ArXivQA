# [Neural Parameter Allocation Search](https://arxiv.org/abs/2006.10598)

## What is the central research question or hypothesis that this paper addresses?

The central research hypothesis of this paper is that generalized parameter sharing techniques can be automatically learned to effectively implement neural network architectures within arbitrary parameter budgets, without requiring any changes to the architecture or loss function. The authors propose Neural Parameter Allocation Search (NPAS) as a novel task to optimize how parameters are assigned and generated for layers in a network given a fixed budget. To address NPAS, they introduce Shapeshifter Networks (SSNs) which learn mappings of layers to parameter groups and methods to generate weights from a limited budget. The key ideas are:- Parameter sharing can be automated and generalized beyond identical layers through learned mappings and weight generation.- SSNs enable training with limited parameters (to reduce memory) as well as excess parameters (to boost capacity), and work for any architecture.- Automatically learning where to share parameters avoids relying on hand-crafted heuristics that have limited flexibility. - SSNs demonstrate improved performance over prior specialized parameter sharing methods, especially when using very constrained budgets.- SSNs can also be combined with other techniques like pruning and distillation for further gains.In summary, the central hypothesis is that generalized parameter sharing can be automated to train networks within any budget without changing the architecture, and SSNs are proposed as an effective approach to accomplish this. Evaluating SSNs across diverse tasks and budgets is intended to demonstrate the viability of this idea.
