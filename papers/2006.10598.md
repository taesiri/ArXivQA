# [Neural Parameter Allocation Search](https://arxiv.org/abs/2006.10598)

## What is the central research question or hypothesis that this paper addresses?

The central research hypothesis of this paper is that generalized parameter sharing techniques can be automatically learned to effectively implement neural network architectures within arbitrary parameter budgets, without requiring any changes to the architecture or loss function. The authors propose Neural Parameter Allocation Search (NPAS) as a novel task to optimize how parameters are assigned and generated for layers in a network given a fixed budget. To address NPAS, they introduce Shapeshifter Networks (SSNs) which learn mappings of layers to parameter groups and methods to generate weights from a limited budget. The key ideas are:- Parameter sharing can be automated and generalized beyond identical layers through learned mappings and weight generation.- SSNs enable training with limited parameters (to reduce memory) as well as excess parameters (to boost capacity), and work for any architecture.- Automatically learning where to share parameters avoids relying on hand-crafted heuristics that have limited flexibility. - SSNs demonstrate improved performance over prior specialized parameter sharing methods, especially when using very constrained budgets.- SSNs can also be combined with other techniques like pruning and distillation for further gains.In summary, the central hypothesis is that generalized parameter sharing can be automated to train networks within any budget without changing the architecture, and SSNs are proposed as an effective approach to accomplish this. Evaluating SSNs across diverse tasks and budgets is intended to demonstrate the viability of this idea.


## What is the main contribution of this paper?

This paper introduces Neural Parameter Allocation Search (NPAS), a novel task where the goal is to train a neural network with a fixed parameter budget. The authors propose Shapeshifter Networks (SSNs) to address NPAS by automatically learning where and how to share parameters in a network. The main contributions are:- Proposing NPAS, a new task that generalizes prior work on parameter sharing to support arbitrary parameter budgets and network architectures. This enables exploring new regimes like low-budget to reduce memory and high-budget to improve accuracy.- Introducing SSNs which can morph a fixed set of parameters into the desired weights for any network architecture. SSNs have three main components: 1) Automatically mapping layers to parameter groups where sharing is effective.2) Generating weights from the associated parameters using downsampling when there are excess parameters or upsampling when there are insufficient parameters. 3) Learning these mappings and generation functions in a way that works for any network architecture.- Demonstrating SSNs effectively implement NPAS on a variety of tasks and models, creating compact models in low-budget regimes and improving accuracy in high-budget regimes.- Showing SSNs can be combined with other techniques like pruning and distillation for further gains.In summary, the main contribution is proposing and providing a solution (SSNs) for a new problem (NPAS) that generalizes prior work on parameter sharing to arbitrary budgets and architectures. The benefits are shown across diverse tasks and models.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper introduces Neural Parameter Allocation Search (NPAS), a novel task of training neural networks with arbitrary fixed parameter budgets, and proposes Shapeshifter Networks (SSNs) which automatically learn how to share parameters across layers to effectively solve NPAS.


## How does this paper compare to other research in the same field?

This paper introduces a new task called Neural Parameter Allocation Search (NPAS) for optimally distributing a fixed parameter budget across the layers of a neural network architecture. The key novelties are:- NPAS generalizes prior work on cross-layer parameter sharing, which relied on hand-crafted heuristics and could only share parameters between identical layers. NPAS has no architectural constraints and can automatically learn where to best allocate parameters.- NPAS operates in two novel regimes - low-budget, which creates compact models like prior work; and high-budget, which boosts capacity and performance without changing model architecture. - The proposed Shapeshifter Networks (SSNs) are the first method that can automatically learn where and how to share parameters between layers of different sizes/types. Prior methods like Shared WideResNets relied on manual heuristics.- SSNs can implement models using any parameter budget through a combination of parameter downsampling and a new upsampling method. No prior work could adapt to budgets where layers need more parameters than available.So in summary, this paper significantly advances research on cross-layer parameter sharing by making it fully automated, task-agnostic, and applicable to any parameter budget. The low-budget regime reduces memory/communication costs like prior work, while the high-budget regime is entirely novel. The experiments comprehensively demonstrate these advantages over prior art on both vision and language tasks using CNNs and Transformers.The key limitations compared to other research is that NPAS does not aim to reduce computational costs like pruning or quantization methods. But the paper shows it can be combined with these orthogonal techniques for further benefits. Overall, this is an important advancement for efficient deep learning that makes parameter sharing more generally applicable.
