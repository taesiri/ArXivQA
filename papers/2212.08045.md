# [CLIPPO: Image-and-Language Understanding from Pixels Only](https://arxiv.org/abs/2212.08045)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading, the key research questions and hypotheses explored in this paper are:

- Can a pure pixel-based model perform well on image, text, and multimodal tasks compared to models like CLIP that use separate text and image encoders? The authors hypothesize that a unified pixel-based model called CLIPPO can achieve competitive performance. 

- Can a pixel-based model like CLIPPO learn good language understanding capabilities, without explicit word-level losses like language modeling or masked language modeling? The authors hypothesize that CLIPPO can learn decent language understanding through image/text contrastive learning.

- Can a pixel-based model avoid issues with multilingual tokenization and achieve strong multilingual performance? The authors hypothesize CLIPPO will have advantages for multilingual learning compared to tokenized models.

- Can CLIPPO perform visual question answering by simply rendering the text and image together, without needing an explicit fusion module? The authors test if CLIPPO can effectively combine text and images for VQA.

In summary, the key hypotheses are around developing a unified pixel-based model for multimodal learning that avoids modality-specific components like tokenizers, and testing if such a model can effectively learn joint image and text understanding. The authors evaluate CLIPPO across a range of image, text, and multimodal tasks to assess these hypotheses.
