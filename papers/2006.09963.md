# [GCC: Graph Contrastive Coding for Graph Neural Network Pre-Training](https://arxiv.org/abs/2006.09963)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the main research question/hypothesis seems to be:Whether using the paradigm of pre-training and fine-tuning for graph neural networks (GNNs) can produce graph representations that capture universal structural properties and patterns across different networks, and can transfer well to new graphs and tasks.In particular, the authors hypothesize that:"Representative graph structural patterns are universal and transferable across networks."The paper aims to validate this hypothesis by proposing a self-supervised pre-training framework called Graph Contrastive Coding (GCC) that is designed to learn structural graph representations that can transfer to new graphs and tasks through fine-tuning. The goal is to show that the pre-training and fine-tuning approach used in NLP and computer vision can also be effective for graph representation learning.The central premise seems to be that while prior graph representation learning methods have focused on learning representations customized to particular graphs, there may exist common underlying structural patterns across different graph types that can be captured by proper pre-training objectives and transferred via fine-tuning. The paper presents GCC as a way to do this pre-training in a self-supervised manner and shows strong performance on downstream tasks, providing evidence for the hypothesis.
