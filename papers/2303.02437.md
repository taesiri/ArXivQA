# [ConZIC: Controllable Zero-shot Image Captioning by Sampling-Based   Polishing](https://arxiv.org/abs/2303.02437)

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper aims to address is how to perform controllable image captioning in a zero-shot setting. Specifically, the key questions are:1) How to perform zero-shot image captioning without any supervised training data? 2) How to generate diverse and accurate image captions in a zero-shot manner?3) How to control the generated captions (e.g. sentiment, length, style) without any supervised training?4) How to improve the inference speed and flexibility compared to previous zero-shot image captioning methods?The main hypothesis is that by combining a masked language model like BERT with a visual-semantic model like CLIP, along with Gibbs sampling, it is possible to perform controllable zero-shot image captioning that is faster, more diverse and more flexible than previous approaches. The key contributions to test this hypothesis are:- Proposing a Gibbs sampling based language model called Gibbs-BERT that allows flexible and efficient search for caption words compared to autoregressive models.- Integrating Gibbs-BERT with CLIP for image-text matching to perform zero-shot image captioning without training.- Introducing control signals through classifiers to achieve controllable zero-shot image captioning.- Demonstrating superior performance of the proposed method called ConZIC over previous state-of-the-art in terms of speed, accuracy, diversity and controllability through extensive experiments.In summary, the central hypothesis is around achieving high-quality controllable zero-shot image captioning by combining large pretrained models like BERT and CLIP with a new sampling based search strategy. The paper provides both modeling innovations and empirical validation to test this hypothesis.


## What is the main contribution of this paper?

The main contributions of this paper are:1. Proposes ConZIC, a framework for controllable zero-shot image captioning. ConZIC is the first method to enable controllable capabilities in the zero-shot image captioning setting. 2. Develops a novel sampling-based language model called Gibbs-BERT by combining Gibbs sampling and masked language models. Compared to autoregressive language models, Gibbs-BERT allows more flexible generation order and bidirectional context, enabling faster, more diverse, and self-correcting caption generation.3. Demonstrates superior performance of ConZIC over previous state-of-the-art zero-shot image captioning method ZeroCap, in terms of diversity, controllability, and speed. ConZIC achieves about 5x speedup and 1.5x higher diversity scores compared to ZeroCap.4. Explores controllable zero-shot image captioning capabilities on four types of control signals - length, infilling, style, and parts-of-speech. The controllability is realized by introducing task-specific discriminators without any training.5. Provides comprehensive experiments and analysis to demonstrate the effectiveness of ConZIC framework as well as the sampling-based Gibbs-BERT language model. The code is made publicly available.In summary, the main contribution is proposing a novel and effective framework ConZIC to enable, for the first time, controllable zero-shot image captioning. This is achieved through a new sampling-based language model design.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper proposes ConZIC, a novel framework for controllable zero-shot image captioning that utilizes a Gibbs sampling-based language model called Gibbs-BERT to iteratively polish captions by considering the full context, enabling faster and more diverse caption generation without needing curated training data.


## How does this paper compare to other research in the same field?

Here are some key similarities and differences between this paper and other research on zero-shot image captioning:Similarities:- Uses CLIP as an off-the-shelf image encoder and GPT-2/BERT as text decoders. This is a common framework for zero-shot captioning papers. - Aims to generate diverse, controllable captions without training on paired image-caption datasets. Shares the goal of zero-shot generalization with other recent work.- Evaluates on standard captioning datasets like MS-COCO and introduces metrics like CLIPScore to measure image relevance of generated captions.Differences:- Proposes a new non-autoregressive sampling-based captioning model called Gibbs-BERT rather than using pure GPT-2. Claims this allows more flexible caption generation.- Specifically focuses on controllable captioning abilities like varying length, infilling words, sentiment, etc. Most prior zero-shot papers did not examine controllability.- Achieves state-of-the-art diversity scores compared to ZeroCap and other baselines. But still lags behind supervised methods on accuracy.- First paper to examine zero-shot captioning on sketch images rather than just natural photos. Shows better generalization than supervised approaches. - Does not require iterative optimization like ZeroCap. Claims 5x faster inference speed through sampling.Overall, this paper pushes state-of-the-art in zero-shot controllable captioning by proposing Gibbs-BERT and evaluating various control tasks. The gains in diversity and speed over ZeroCap are notable contributions. But accuracy and relevance still lag behind supervised approaches. The framework is similar to other works leveraging CLIP and Transformers but with a novel sampling-based decoder.


## What future research directions do the authors suggest?

Based on my reading of the paper, here are some of the main future research directions the authors suggest:- Improving the accuracy and diversity of zero-shot image captioning, especially for small objects/targets in images. The paper notes that both ZeroCap and ConZIC often miss describing small objects in images, so developing methods to better capture and describe small targets could be an area for improvement. - Exploring better evaluation metrics, especially for controllable image captioning tasks. The paper points out that using existing supervised metrics like BLEU may not be ideal for evaluating zero-shot controllable captioning, so designing more appropriate metrics is important.- Trying better pretrained models like RoBERTa and ALIGN instead of BERT and CLIP. The framework is model-agnostic, so the authors suggest exploring if more advanced models can further boost performance.- Studying how to better control which parts of the image are described. Currently the models look at the whole image, but allowing control over image regions to describe could be useful.- Improving inference speed further, especially for generating longer captions. The sampling process helps but there are likely ways to optimize speed more.- Expanding the diversity of control signals, like controlling persona, emotions, visual relations etc. The current controls demonstrate capability but more signals could be interesting.- Testing the method on a wider range of image datasets and domains beyond MSCOCO. Generalization to other datasets would be important future work.- Combining with detectors/taggers to provide extra input signals about image content. This could help guide caption generation and description of small objects.So in summary, the main future directions are improving accuracy (especially for small targets), exploring better evaluation, testing on more datasets, expanding control signals, improving speed, and combining external information to guide generation. Advancing controllable zero-shot image captioning appears to be an exciting and promising research direction overall.


## Summarize the paper in one paragraph.

Here is a one paragraph summary of the key points in the paper:The paper proposes a new framework called ConZIC for controllable zero-shot image captioning. The key innovation is a sampling-based language model called Gibbs-BERT that allows flexible word generation order and continuous polishing of the caption. This overcomes limitations of prior zero-shot image captioning methods like ZeroCap which use autoregressive language models and gradient-based searching. The framework integrates Gibbs-BERT with CLIP for image-text matching and a discriminator module for controllability. Experiments demonstrate superior performance to ZeroCap in accuracy, diversity, and speed. The method also enables controllable image captioning on various signals like length, style, and parts-of-speech without any training. Overall, ConZIC advances the state-of-the-art in zero-shot image captioning through its flexible sampling approach and controllability, while avoiding expensive training. The code is available on GitHub.
