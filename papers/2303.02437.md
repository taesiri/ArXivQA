# [ConZIC: Controllable Zero-shot Image Captioning by Sampling-Based   Polishing](https://arxiv.org/abs/2303.02437)

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper aims to address is how to perform controllable image captioning in a zero-shot setting. Specifically, the key questions are:1) How to perform zero-shot image captioning without any supervised training data? 2) How to generate diverse and accurate image captions in a zero-shot manner?3) How to control the generated captions (e.g. sentiment, length, style) without any supervised training?4) How to improve the inference speed and flexibility compared to previous zero-shot image captioning methods?The main hypothesis is that by combining a masked language model like BERT with a visual-semantic model like CLIP, along with Gibbs sampling, it is possible to perform controllable zero-shot image captioning that is faster, more diverse and more flexible than previous approaches. The key contributions to test this hypothesis are:- Proposing a Gibbs sampling based language model called Gibbs-BERT that allows flexible and efficient search for caption words compared to autoregressive models.- Integrating Gibbs-BERT with CLIP for image-text matching to perform zero-shot image captioning without training.- Introducing control signals through classifiers to achieve controllable zero-shot image captioning.- Demonstrating superior performance of the proposed method called ConZIC over previous state-of-the-art in terms of speed, accuracy, diversity and controllability through extensive experiments.In summary, the central hypothesis is around achieving high-quality controllable zero-shot image captioning by combining large pretrained models like BERT and CLIP with a new sampling based search strategy. The paper provides both modeling innovations and empirical validation to test this hypothesis.
