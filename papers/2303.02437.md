# [ConZIC: Controllable Zero-shot Image Captioning by Sampling-Based   Polishing](https://arxiv.org/abs/2303.02437)

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper aims to address is how to perform controllable image captioning in a zero-shot setting. Specifically, the key questions are:1) How to perform zero-shot image captioning without any supervised training data? 2) How to generate diverse and accurate image captions in a zero-shot manner?3) How to control the generated captions (e.g. sentiment, length, style) without any supervised training?4) How to improve the inference speed and flexibility compared to previous zero-shot image captioning methods?The main hypothesis is that by combining a masked language model like BERT with a visual-semantic model like CLIP, along with Gibbs sampling, it is possible to perform controllable zero-shot image captioning that is faster, more diverse and more flexible than previous approaches. The key contributions to test this hypothesis are:- Proposing a Gibbs sampling based language model called Gibbs-BERT that allows flexible and efficient search for caption words compared to autoregressive models.- Integrating Gibbs-BERT with CLIP for image-text matching to perform zero-shot image captioning without training.- Introducing control signals through classifiers to achieve controllable zero-shot image captioning.- Demonstrating superior performance of the proposed method called ConZIC over previous state-of-the-art in terms of speed, accuracy, diversity and controllability through extensive experiments.In summary, the central hypothesis is around achieving high-quality controllable zero-shot image captioning by combining large pretrained models like BERT and CLIP with a new sampling based search strategy. The paper provides both modeling innovations and empirical validation to test this hypothesis.


## What is the main contribution of this paper?

The main contributions of this paper are:1. Proposes ConZIC, a framework for controllable zero-shot image captioning. ConZIC is the first method to enable controllable capabilities in the zero-shot image captioning setting. 2. Develops a novel sampling-based language model called Gibbs-BERT by combining Gibbs sampling and masked language models. Compared to autoregressive language models, Gibbs-BERT allows more flexible generation order and bidirectional context, enabling faster, more diverse, and self-correcting caption generation.3. Demonstrates superior performance of ConZIC over previous state-of-the-art zero-shot image captioning method ZeroCap, in terms of diversity, controllability, and speed. ConZIC achieves about 5x speedup and 1.5x higher diversity scores compared to ZeroCap.4. Explores controllable zero-shot image captioning capabilities on four types of control signals - length, infilling, style, and parts-of-speech. The controllability is realized by introducing task-specific discriminators without any training.5. Provides comprehensive experiments and analysis to demonstrate the effectiveness of ConZIC framework as well as the sampling-based Gibbs-BERT language model. The code is made publicly available.In summary, the main contribution is proposing a novel and effective framework ConZIC to enable, for the first time, controllable zero-shot image captioning. This is achieved through a new sampling-based language model design.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper proposes ConZIC, a novel framework for controllable zero-shot image captioning that utilizes a Gibbs sampling-based language model called Gibbs-BERT to iteratively polish captions by considering the full context, enabling faster and more diverse caption generation without needing curated training data.
