# [ConZIC: Controllable Zero-shot Image Captioning by Sampling-Based   Polishing](https://arxiv.org/abs/2303.02437)

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper aims to address is how to perform controllable image captioning in a zero-shot setting. Specifically, the key questions are:1) How to perform zero-shot image captioning without any supervised training data? 2) How to generate diverse and accurate image captions in a zero-shot manner?3) How to control the generated captions (e.g. sentiment, length, style) without any supervised training?4) How to improve the inference speed and flexibility compared to previous zero-shot image captioning methods?The main hypothesis is that by combining a masked language model like BERT with a visual-semantic model like CLIP, along with Gibbs sampling, it is possible to perform controllable zero-shot image captioning that is faster, more diverse and more flexible than previous approaches. The key contributions to test this hypothesis are:- Proposing a Gibbs sampling based language model called Gibbs-BERT that allows flexible and efficient search for caption words compared to autoregressive models.- Integrating Gibbs-BERT with CLIP for image-text matching to perform zero-shot image captioning without training.- Introducing control signals through classifiers to achieve controllable zero-shot image captioning.- Demonstrating superior performance of the proposed method called ConZIC over previous state-of-the-art in terms of speed, accuracy, diversity and controllability through extensive experiments.In summary, the central hypothesis is around achieving high-quality controllable zero-shot image captioning by combining large pretrained models like BERT and CLIP with a new sampling based search strategy. The paper provides both modeling innovations and empirical validation to test this hypothesis.


## What is the main contribution of this paper?

The main contributions of this paper are:1. Proposes ConZIC, a framework for controllable zero-shot image captioning. ConZIC is the first method to enable controllable capabilities in the zero-shot image captioning setting. 2. Develops a novel sampling-based language model called Gibbs-BERT by combining Gibbs sampling and masked language models. Compared to autoregressive language models, Gibbs-BERT allows more flexible generation order and bidirectional context, enabling faster, more diverse, and self-correcting caption generation.3. Demonstrates superior performance of ConZIC over previous state-of-the-art zero-shot image captioning method ZeroCap, in terms of diversity, controllability, and speed. ConZIC achieves about 5x speedup and 1.5x higher diversity scores compared to ZeroCap.4. Explores controllable zero-shot image captioning capabilities on four types of control signals - length, infilling, style, and parts-of-speech. The controllability is realized by introducing task-specific discriminators without any training.5. Provides comprehensive experiments and analysis to demonstrate the effectiveness of ConZIC framework as well as the sampling-based Gibbs-BERT language model. The code is made publicly available.In summary, the main contribution is proposing a novel and effective framework ConZIC to enable, for the first time, controllable zero-shot image captioning. This is achieved through a new sampling-based language model design.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper proposes ConZIC, a novel framework for controllable zero-shot image captioning that utilizes a Gibbs sampling-based language model called Gibbs-BERT to iteratively polish captions by considering the full context, enabling faster and more diverse caption generation without needing curated training data.


## How does this paper compare to other research in the same field?

Here are some key similarities and differences between this paper and other research on zero-shot image captioning:Similarities:- Uses CLIP as an off-the-shelf image encoder and GPT-2/BERT as text decoders. This is a common framework for zero-shot captioning papers. - Aims to generate diverse, controllable captions without training on paired image-caption datasets. Shares the goal of zero-shot generalization with other recent work.- Evaluates on standard captioning datasets like MS-COCO and introduces metrics like CLIPScore to measure image relevance of generated captions.Differences:- Proposes a new non-autoregressive sampling-based captioning model called Gibbs-BERT rather than using pure GPT-2. Claims this allows more flexible caption generation.- Specifically focuses on controllable captioning abilities like varying length, infilling words, sentiment, etc. Most prior zero-shot papers did not examine controllability.- Achieves state-of-the-art diversity scores compared to ZeroCap and other baselines. But still lags behind supervised methods on accuracy.- First paper to examine zero-shot captioning on sketch images rather than just natural photos. Shows better generalization than supervised approaches. - Does not require iterative optimization like ZeroCap. Claims 5x faster inference speed through sampling.Overall, this paper pushes state-of-the-art in zero-shot controllable captioning by proposing Gibbs-BERT and evaluating various control tasks. The gains in diversity and speed over ZeroCap are notable contributions. But accuracy and relevance still lag behind supervised approaches. The framework is similar to other works leveraging CLIP and Transformers but with a novel sampling-based decoder.
