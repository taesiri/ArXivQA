# [Large Multilingual Models Pivot Zero-Shot Multimodal Learning across   Languages](https://arxiv.org/abs/2308.12038)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the key research focus of this paper is proposing an effective training paradigm called MpM (Multilingual language models Pivot Multimodal learning across languages) for training large multimodal models in low-resource non-English languages. The central hypothesis is that by leveraging a pretrained multilingual language model as a pivot, multimodal models trained solely on English image-text data can generalize well to other languages like Chinese in a zero-shot manner, even surpassing models trained on native language image-text data.Specifically, the paper aims to address the challenge of developing competitive multimodal models for non-English languages, which is difficult due to the scarcity of large-scale high-quality image-text data in those languages. The proposed MpM paradigm tries to mitigate the dependence on native language image-text data by transferring visual knowledge learned from English data through the "language bridge" provided by the multilingual LLM.The effectiveness of MpM is demonstrated through comprehensive experiments on Chinese image-to-text and text-to-image tasks. The results show the models trained with MpM can outperform existing Chinese multimodal models, even without using any Chinese image-text data for pretraining. This supports the hypothesis that multilingual LMs can pivot effective zero-shot transfer of multimodal knowledge across languages.In summary, the key research question is how to develop competitive multimodal models for low-resource non-English languages. MpM provides an effective solution by utilizing multilingual LMs to transfer and align visual knowledge from English to the target language.


## What is the main contribution of this paper?

Based on my reading, the main contributions of this paper are:1. It proposes MpM, an effective training paradigm for large multimodal models in low-resource non-English languages. MpM utilizes a multilingual language model as a pivot to transfer multimodal knowledge from English to other languages in a zero-shot manner. 2. It develops VisCPM, a series of Chinese multimodal models for image-to-text and text-to-image generation. VisCPM models achieve state-of-the-art performance among open-source Chinese multimodal models by pretraining exclusively on English data and transferring to Chinese via the bilingual language model.3. It validates the effectiveness of MpM by applying it to train multimodal chatbots in 6 languages (English, German, French, Spanish, Italian, Portuguese), demonstrating the generalization capability of MpM.4. It open-sources the pre-trained VisCPM models to facilitate research, providing model weights, training details, and evaluation benchmarks.In summary, the key contribution is proposing MpM for low-resource multimodal learning and demonstrating its effectiveness by developing high-performance Chinese and multilingual multimodal models VisCPM. The paper highlights that leveraging multilingual language models is an effective way to address the lack of native multimodal data in non-English languages.


## How does this paper compare to other research in the same field?

Here are a few key ways this paper compares and contrasts with other related work in multilingual multimodal learning:- Focus on leveraging pretrained multilingual LLMs: This paper specifically focuses on effectively utilizing pretrained multilingual LLMs to enable multimodal learning across languages. In contrast, many other works try to simultaneously learn multilingual and multimodal representations from scratch. - Demonstrates strong zero-shot transfer: A key result is that models pretrained on English image-text data transfer surprisingly well to other languages in a zero-shot manner, even outperforming models trained on native language data. Other works have shown some preliminary cross-lingual transfer capabilities, but not to this extent.- Systematic training paradigm: The proposed MpM training paradigm provides a principled and systematic framework for training multilingual multimodal models by dividing the process into multilingual alignment and multimodal alignment stages.- State-of-the-art for Chinese: The VisCPM models achieve new state-of-the-art results for open-source Chinese multimodal models, validating the effectiveness of the proposed approach.- Generalization to multiple languages: This work shows the approach can be extended to multiple languages by developing a conversational model across 6 languages, demonstrating wider applicability.- Focus on low-resource languages: A key motivation is enabling multimodal learning for low-resource non-English languages. This contrasts with many works centered on English.In summary, this work provides novel insights into cross-lingual transfer for multimodality via pretrained LLMs, proposes a systematic training paradigm, and achieves strong results - especially for lower-resource languages. The zero-shot transfer abilities and multilingual results are particularly noteworthy comparisons to prior work.


## What future research directions do the authors suggest?

Based on my reading of the paper, here are some of the main future research directions suggested by the authors:- Improving the quality and diversity of non-English multimodal datasets for pretraining. The authors note the lack of large-scale high-quality image-text datasets in non-English languages as a key challenge. Developing better multilingual multimodal datasets could help further improve performance.- Exploring different model architectures and training techniques for multilingual multimodal learning. The authors propose a general training paradigm but note it is agnostic to specific model designs. Testing different encoder-decoder architectures, contrastive learning methods etc. may lead to better models. - Extending the approach to more languages beyond those explored in the paper. The authors showcase promising results on 6 languages but suggest expanding to more languages to further demonstrate generalization.- Combining multilingual multimodal pretraining with specialized fine-tuning. The authors focus on pretraining, but task-specific fine-tuning could adapt models better to downstream applications.- Reducing the computational requirements of large multimodal models. The authors note the high training costs, so work on knowledge distillation and model compression could improve efficiency.- Studying the theoretical underpinnings behind the cross-lingual generalization. While empirical results are shown, analyzing the formal principles could provide more insight.- Evaluating performance on a broader range of multimodal tasks. The paper tests mainly conversational QA, but assessing models on additional applications like captioning, retrieval etc. could reveal strengths/weaknesses.In summary, the authors highlight promising future work on improving multilingual datasets, model architectures, expanding language coverage, task-specific tuning, model efficiency, theoretical analysis, and evaluation across diverse tasks. Advancing research in these areas could build upon the authors' proposed training paradigm to enable even better multilingual multimodal AI systems.


## Summarize the paper in one paragraph.

Here is a one paragraph summary of the paper:This paper proposes a training paradigm called MpM (Multilingual language models Pivot zero-shot Multimodal learning across languages) for training large multimodal models in low-resource non-English languages. The key idea is to leverage a pretrained multilingual language model (LLM) as a pivot to transfer multimodal knowledge from data-rich English to low-resource target languages. Specifically, they first pretrain a multilingual LLM to align representations across languages. Then they train the visual modules exclusively on English image-text data while keeping the LLM fixed. This aligns the visual semantics with English representations in the LLM. Thanks to the multilinguality of the LLM, this visual knowledge can zero-shot transfer to other languages. They implement this idea for Chinese by developing VisCPM, a series of Chinese multimodal models including VisCPM-Chat for image-to-text generation and VisCPM-Paint for text-to-image generation. Experiments show VisCPM models achieve state-of-the-art performance among Chinese open-source multimodal models, even surpassing models trained on native Chinese data. This demonstrates the effectiveness of MpM for low-resource multimodal learning. They also generalize MpM to train a multilingual conversational model supporting 6 languages.


## Summarize the paper in two paragraphs.

Here is a two paragraph summary of the key points from the paper:The paper proposes MpM, an effective training paradigm for large multimodal models in non-English languages. MpM utilizes multilingual language models as a bridge to transfer multimodal knowledge from resource-rich languages like English to low-resource languages. The key insight is that visual semantics are largely shared across languages. So a multilingual model pretrained on English image-text data can generalize zero-shot to other languages, as the visual concepts have already been aligned to the shared multilingual space. Taking Chinese as an example, the authors develop VisCPM, a series of Chinese multimodal models under the MpM framework. VisCPM-Chat and VisCPM-Paint achieve state-of-the-art performance among Chinese models by pretraining exclusively on English data. Further experiments validate the effectiveness of MpM by showing competitive zero-shot multimodal performance in 5 additional languages. The proposed training paradigm could enable rapid development of multimodal AI for diverse languages. The open-sourced VisCPM models also serve as a valuable reference for fellow researchers.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper proposes MpM, an effective training paradigm for multilingual multimodal models, which leverages multilingual language models to enable zero-shot transfer of visual knowledge from English to low-resource languages, achieving strong performance in Chinese without native Chinese data.


## Summarize the main method used in the paper in one paragraph.

Here is a one paragraph summary of the main method used in the paper:The paper proposes MpM (Multilingual language models Pivot Multimodal learning across languages), an effective training paradigm for training large multimodal models in non-English languages. MpM utilizes a pretrained multilingual large language model (LLM) as a pivotal intermediary between visual signals and target non-English languages. It trains the multimodal model in two stages - multilingual alignment and multimodal alignment. In the multilingual alignment stage, a multilingual LLM is used to provide aligned representations between English and the target language. In the multimodal alignment stage, the visual modules are trained exclusively on English image-text data to align the visual semantics with English representations from the LLM. Through the multilingual LLM as a pivot, the model can generalize zero-shot to other languages for multimodal tasks. MpM is demonstrated through VisCPM, a series of Chinese multimodal models that achieve state-of-the-art performance by training only on English data. The method's effectiveness highlights how multilingual LLMs can promote multimodal learning in low-resource languages.


## What problem or question is the paper addressing?

Based on my reading, this paper is addressing the challenge of training large multimodal models for non-English languages, which lack the extensive image-text data resources that exist for English. The key problem is that most progress in multimodal AI has focused on English, while other languages lag behind due to limited image-text data. Building competitive multimodal models in those languages is difficult because models like BLIP-2 and Stable Diffusion require 100M+ high-quality image-text pairs for pretraining. To tackle this problem, the paper proposes a new training paradigm called MpM (Multilingual language models Pivot Multimodal learning across languages). The key idea is to leverage a pretrained multilingual language model as a bridge to transfer visual knowledge from a pivot language like English to the target non-English language. By using the multilingual model to align representations across languages, models pretrained only on English image-text data can generalize well to other languages like Chinese in a zero-shot manner for image-to-text and text-to-image generation. This allows building multimodal models for low-resource languages by relying primarily on English data.The paper demonstrates this paradigm by developing Chinese multimodal models VisCPM, which achieve state-of-the-art performance among Chinese open-source models despite training on mostly English data. The approach also generalizes to other languages like German, French, Spanish etc.In summary, the key contribution is an effective training strategy to promote multimodal AI across languages by transferring visual knowledge through multilingual models. This helps mitigate the image-text data scarcity in non-English languages.
