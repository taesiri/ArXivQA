# [UniAR: Unifying Human Attention and Response Prediction on Visual   Content](https://arxiv.org/abs/2312.10175)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
Most prior work has focused on modeling implicit human perceptual behavior like attention or explicit behavior like preferences/ratings separately and in isolation. Moreover, research has been fragmented due to focusing on specific tasks within a single domain. There hasn't been much work on unified modeling of both implicit (like attention) and explicit (like preferences) human behavior across diverse types of visual content.

Proposed Solution: 
The paper proposes UniAR (Unifying User Attention and Response), a multimodal transformer model that takes an image and a text prompt as input and can predict human attention heatmaps, scanpath/sequence of viewing, and subjective ratings/preferences as outputs. The text prompt encodes relevant info about the domain, task to guide model's prediction. 

The model architecture uses a Vision Transformer to encode image, word embeddings for text, and a Transformer encoder to fuse them. It has three separate prediction heads for heatmaps, scanpaths and ratings. The text prompt also helps model generalize across domains and tasks.

Main Contributions:
1) Proposed UniAR as the first unified model to predict both implicit attention patterns and explicit subjective ratings over images, web pages and graphic designs.

2) Trained UniAR on 10 public datasets covering different domains and tasks. Showed it matches or outperforms state of art models dedicated to those specific domains/tasks. Also showed good generalization capability to unseen domain-task combinations in zero-shot setting.

In summary, the paper proposed a unified multimodal approach via UniAR to model human perceptual and preference behavior across diverse visual content, achieving promising performance against benchmarks while also exhibiting an ability to generalize across domains and tasks.
