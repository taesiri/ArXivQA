# [FedBRB: An Effective Solution to the Small-to-Large Scenario in   Device-Heterogeneity Federated Learning](https://arxiv.org/abs/2402.17202)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem Statement: 
The paper addresses the "small-to-large scenario" in federated learning where the global model is larger than any of the local client models. This scenario is becoming increasingly common as there is growing interest in training larger models using federated learning, but many clients have computational constraints that limit the size of models they can train locally. Existing device-heterogeneity federated learning methods have limitations in handling this scenario.

Proposed Solution:
The paper proposes a new method called FedBRB (Block-wise Rolling and weighted Broadcast) to address the small-to-large scenario. The key ideas are:

1) Partition the global model into blocks based on the size of the smallest local client model. Clients then train sub-models assembled from a subset of blocks.

2) Use "block-wise rolling" to ensure every block gets trained over successive rounds. The server shifts the starting block in each round to traverse the entire global model space. 

3) Apply "block weighted broadcast" during aggregation to rapidly share block updates between sub-models, accelerating information flow.

Main Contributions:

- Clearly defines and explains the importance of the "small-to-large scenario" in federated learning

- Proposes FedBRB, a new approach specifically designed to address limitations of prior methods for this scenario

- FedBRB enables full coverage of the global model parameters during training

- Experiments show FedBRB achieves state-of-the-art performance in the small-to-large setting, significantly outperforming existing methods

- FedBRB allows even clients with minimal model sizes to contribute effectively and outperform baselines with larger local models

In summary, the key innovation is FedBRB, which makes it feasible to collaboratively train a large global model using smaller local client models in a privacy-preserving federated learning setting. The block-wise techniques used allow clients to participate based on their capabilities while still fully training the global model.
