# [How you feelin'? Learning Emotions and Mental States in Movie Scenes](https://arxiv.org/abs/2304.05634)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper outline, it seems the central research question is:

How can we develop multimodal models to effectively predict emotions and mental states of movie characters at the scene level, by jointly analyzing video, dialog, and character appearances?

The key hypotheses appear to be:

1) Modeling emotions at the scene level for longer durations (30-60 seconds) lends itself better to multi-label classification, as characters may portray multiple emotions simultaneously or have transitions during interactions.

2) Recognizing complex emotions and mental states requires going beyond just facial expressions to understand the larger visual, dialog, and story context. 

3) A multimodal Transformer-based architecture can effectively encode information from video, dialog, and characters to make joint predictions of emotions and mental states for movie scenes and characters.

4) The model can learn to attend to facial expressions for more expressive emotions vs. video/dialog context for mental states based on the self-attention mechanism.

The proposed EmotX model seems designed to test these hypotheses for movie scene understanding, using multi-label emotion annotations from the MovieGraphs dataset. The experiments analyze performance on different emotion labels sets, compare to adapted state-of-the-art methods, and provide analysis of self-attention scores.
