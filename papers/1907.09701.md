# Benchmarking Attribution Methods with Relative Feature Importance

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be: How can we quantitatively evaluate and benchmark the correctness of different attribution methods for explaining machine learning model predictions?The key points are:- Currently there is a lack of ground truth data on the true importance of input features for model predictions. This makes it hard to quantitatively evaluate the correctness of attribution methods.- The authors propose a framework called BAM (Benchmarking Attribution Methods) for evaluating attribution methods using models and data where relative feature importance is known by construction.- BAM includes: (1) A semi-natural image dataset where objects are artificially pasted onto scenes to create common features of known relative importance. (2) Models trained on this dataset where feature importance can be controlled. (3) Metrics to quantitatively compare attributions from different methods using the models and data.- The metrics evaluate attribution differences between pairs of models and pairs of inputs based on the constructed relative feature importance. This tests whether methods correctly attribute less importance to less important features.- Evaluation of several attribution methods suggests some methods are more prone to false positive explanations, assigning higher attributions to less important features. The rankings of methods also differ based on the choice of metric.In summary, the central hypothesis is that the proposed BAM framework enables quantitative benchmarking and evaluation of attribution methods using constructed data where relative feature importance is known. The metrics can reveal differences in how prone methods are to false positive explanations.


## What is the main contribution of this paper?

The main contribution of this paper is proposing a framework for Benchmarking Attribution Methods (BAM) to quantitatively evaluate and compare different attribution methods for interpreting machine learning models. The key components of the framework are:1) A carefully crafted dataset and models trained with known relative feature importance. The dataset contains images with objects pasted onto scenes, where the objects act as "common features" with controlled relative importance to the models. 2) Three complementary metrics to evaluate attribution methods by comparing feature attributions between pairs of models and pairs of inputs:- Model contrast score (MCS): Compares attribution differences for the same input between two models where a feature is more or less important.- Input dependence rate (IDR): Compares attribution differences for a model between two inputs, one with and without the common feature. - Input independence rate (IIR): Compares attribution differences for a model between two functionally similar inputs (that produce the same model output).3) Evaluation of several widely used attribution methods like GradCAM, Integrated Gradients, Guided Backprop using the proposed framework. The results reveal differences between methods in terms of their tendency to produce false positive explanations.In summary, the key contribution is a quantitative evaluation framework and dataset for systematically benchmarking and comparing attribution methods based on controlled relative feature importance.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper proposes a framework called BAM for quantitatively evaluating attribution methods, which includes a dataset and models with controlled relative feature importance and metrics that compare attributions between models and inputs to test for false positive explanations.


## How does this paper compare to other research in the same field?

Here are my thoughts on how this paper compares to other research in the field of evaluating attribution methods for interpretability:- The key contribution of this paper is proposing a new framework (BAM) for quantitatively evaluating attribution methods using models and data with known relative feature importance. Most prior work focused on qualitative human evaluations or perturbation-based tests. The BAM framework allows more controlled and efficient quantitative evaluations.- The BAM dataset with objects overlayed on scenes is quite unique. Most prior work used natural datasets where ground truth feature importance is unknown. The constructed BAM dataset enables testing attribution methods against known relative importance.- The proposed metrics of model contrast score, input dependence rate, and input independence rate are novel ways to quantitatively measure desirable properties of attribution methods. Other work tended to rely on accuracy drop from feature removal or human judgments. - The paper evaluates a range of attribution methods (GradCAM, Vanilla Gradient, SmoothGrad, Integrated Gradients, etc.) using the BAM framework. Most prior evaluations focused on saliency maps or a smaller set of methods. The head-to-head comparison on multiple metrics provides useful insights.- A limitation is that the semi-synthetic BAM dataset may not fully generalize to real-world images. But the authors provide thoughtful discussion on why performance on BAM can indicate generalization ability.Overall, this paper pushes forward quantitative evaluation of attribution methods in a more controlled and efficient manner. The BAM framework and metrics offer the community new tools for method validation and selection. The insights from comparing different attribution methods are also valuable. This methodology represents an advance over prior work that relied more on perturbations or human judgments.
