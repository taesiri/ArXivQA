# Benchmarking Attribution Methods with Relative Feature Importance

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be: How can we quantitatively evaluate and benchmark the correctness of different attribution methods for explaining machine learning model predictions?The key points are:- Currently there is a lack of ground truth data on the true importance of input features for model predictions. This makes it hard to quantitatively evaluate the correctness of attribution methods.- The authors propose a framework called BAM (Benchmarking Attribution Methods) for evaluating attribution methods using models and data where relative feature importance is known by construction.- BAM includes: (1) A semi-natural image dataset where objects are artificially pasted onto scenes to create common features of known relative importance. (2) Models trained on this dataset where feature importance can be controlled. (3) Metrics to quantitatively compare attributions from different methods using the models and data.- The metrics evaluate attribution differences between pairs of models and pairs of inputs based on the constructed relative feature importance. This tests whether methods correctly attribute less importance to less important features.- Evaluation of several attribution methods suggests some methods are more prone to false positive explanations, assigning higher attributions to less important features. The rankings of methods also differ based on the choice of metric.In summary, the central hypothesis is that the proposed BAM framework enables quantitative benchmarking and evaluation of attribution methods using constructed data where relative feature importance is known. The metrics can reveal differences in how prone methods are to false positive explanations.


## What is the main contribution of this paper?

The main contribution of this paper is proposing a framework for Benchmarking Attribution Methods (BAM) to quantitatively evaluate and compare different attribution methods for interpreting machine learning models. The key components of the framework are:1) A carefully crafted dataset and models trained with known relative feature importance. The dataset contains images with objects pasted onto scenes, where the objects act as "common features" with controlled relative importance to the models. 2) Three complementary metrics to evaluate attribution methods by comparing feature attributions between pairs of models and pairs of inputs:- Model contrast score (MCS): Compares attribution differences for the same input between two models where a feature is more or less important.- Input dependence rate (IDR): Compares attribution differences for a model between two inputs, one with and without the common feature. - Input independence rate (IIR): Compares attribution differences for a model between two functionally similar inputs (that produce the same model output).3) Evaluation of several widely used attribution methods like GradCAM, Integrated Gradients, Guided Backprop using the proposed framework. The results reveal differences between methods in terms of their tendency to produce false positive explanations.In summary, the key contribution is a quantitative evaluation framework and dataset for systematically benchmarking and comparing attribution methods based on controlled relative feature importance.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper proposes a framework called BAM for quantitatively evaluating attribution methods, which includes a dataset and models with controlled relative feature importance and metrics that compare attributions between models and inputs to test for false positive explanations.
