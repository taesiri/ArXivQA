# [CoRe-GD: A Hierarchical Framework for Scalable Graph Visualization with   GNNs](https://arxiv.org/abs/2402.06706)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
Graph visualization, also known as graph drawing, aims to find geometric node embeddings of graphs that optimize certain aesthetic criteria like stress minimization. Stress measures how well distances between nodes in the embedding match shortest path distances in the original graph. Minimizing stress helps generate readable graph visualizations. However, optimizing stress is computationally challenging due to its inherent complexity. Existing methods rely on heuristics and do not scale well. 

Proposed Solution:
The paper proposes CoRe-GD, a scalable neural graph drawing framework that can learn to optimize stress in a more principled way. The key ideas are:

1) Hierarchical coarsening: Successively merge nodes to create a series of coarsened graphs, starting with the original fine-grained graph and ending with a coarse overview graph. Optimization begins at the coarsest level before progressively uncoarsening to refine the layout. This prioritizes global structure before local placement.

2) Positional rewiring: Nodes positioned close in the embedding but far in the original graph can lead to high stress. Rewire the graph based on intermediate positions so such nodes can exchange messages, enhancing information flow. Tested techniques: k-NN graphs, Delaunay triangulations, radius graphs.

The framework combines graph neural networks with the above ideas for scalability. It is trained end-to-end with stress as the loss.

Main Contributions:

- Propose CoRe-GD, a scalable neural graph drawing framework incorporating efficient coarsening and novel positional rewiring  

- Achieve state-of-the-art stress minimization performance on graph drawing benchmarks like Rome dataset

- Demonstrate the capability to scale to larger graphs than prior neural methods  

- Introduce techniques like encoded beacon distances and replay buffer to facilitate training deeper recurrent GNN models

- Show latent node embeddings from CoRe-GD can be reused as effective positional encodings for graph neural networks
