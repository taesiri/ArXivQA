# ["Task Success" is not Enough: Investigating the Use of Video-Language   Models as Behavior Critics for Catching Undesirable Agent Behaviors](https://arxiv.org/abs/2402.04210)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Large generative models like language models can produce meaningful candidate solutions, but often fail to satisfy constraints or user preferences in one shot. Their power is better harnessed when coupled with external critics/verifiers that provide feedback.  
- In embodied AI contexts, verification has focused on goal reachability rather than constraints beyond task success (e.g. robot should handle objects carefully). But given the scope of tasks, it's infeasible to manually specify verifiers. 
- Questions: Can video-language models (VLMs), which are approximately omniscient, act as Behavior Critics to catch undesirable but goal-reaching robot behaviors in videos? How to understand and utilize VLM critiques?

Methodology:
- Collect videos of robot manipulation tasks with goal-reaching yet unsafe/undersirable behaviors.
- Prompt VLMs (GPT-4V and Gemini Pro) to comment on undesirability in videos and provide pairwise trajectory preferences. 
- Benchmark to evaluate recall (coverage of human-labeled events) and precision (validity) of critiques.
- Manually categorize critique failure modes like visual grounding errors, causal errors.
- Experiment with providing grounding feedback to VLMs to refine critiques iteratively.

Contributions:
- First comprehensive study evaluating VLMs as open-domain Behavior Critics for robot tasks, with analysis of strengths/limitations.
- A benchmark with goal-reaching but undesirable robot manipulation videos across household tasks.
- Findings: GPT-4V identifies ~70% human-labeled events but has 62% precision. Visual grounding is a major limitation. Precision can be improved to 98% given perfect grounding feedback.
- Guidelines provided on effectively utilizing VLM critiques in embodied AI systems.

The paper demonstrates the promise of leveraging VLMs as critics to complement policy learning, while highlighting current limitations that need to be addressed regarding grounding accuracy. The analysis and benchmark serve as a valuable foundation for future work on integrating external AI feedback to make robot behaviors more safe and human-compatible.
