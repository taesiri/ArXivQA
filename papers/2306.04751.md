# [How Far Can Camels Go? Exploring the State of Instruction Tuning on Open   Resources](https://arxiv.org/abs/2306.04751)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central research question of this paper is: How do different open instruction tuning datasets and models compare in improving language models' capabilities on a diverse set of benchmark tests and open-ended instruction following? Specifically, the paper systematically compares the performance of various open instruction tuning datasets (e.g. Dolly, FLAN, Alpaca) and models of different sizes (7B to 65B parameters) on benchmarks testing skills like reasoning, factuality, coding, and multilinguality. It also evaluates their open-ended instruction following abilities through model-based (GPT-4) and human evaluations. The key hypothesis seems to be that while different datasets may excel at improving certain specialized skills, combining multiple datasets together into a mixture model like TÃ¼lu should lead to the best overall performance across diverse evaluation metrics. The paper aims to rigorously test this hypothesis through comprehensive experiments across datasets, model sizes, and evaluation settings. The goal is to provide guidance on how to develop better open instruction-tuned models.


## What is the main contribution of this paper?

This paper makes several key contributions:1. Provides a comprehensive evaluation of a wide variety of publicly-available instruction-tuning datasets and models, ranging from 6.7B to 65B parameters. The authors systematically evaluate their capabilities on factual knowledge, reasoning, multilinguality, coding, and open-ended instruction following through automatic metrics, model-based evaluation, and human evaluation.2. Shows that different instruction datasets promote different capabilities, with no single dataset providing the best performance across the board. Combining multiple datasets results in better overall performance on average.3. Demonstrates that the quality of the base model is extremely important, with better pretrained models like LLaMa consistently outperforming comparable models after instruction tuning.4. Introduces Tulu, a suite of instruction-tuned LLaMa models trained on combinations of datasets, with Tulu 65B being a 65B LLaMa model finetuned on a diverse mix of data. Tulu provides strong performance across tasks, although still lags behind proprietary models like ChatGPT.5. Finds that model and human preference evaluations may not fully reflect differences in model capabilities, as they strongly correlate with length/diversity of generations. The authors argue for the need for systematic, multifaceted evaluation performed in this work.6. Provides insights on the limits of existing open models and datasets, while releasing code, data, models like Tulu to facilitate future research in improving instruction-tuned models.In summary, this paper provides the most comprehensive evaluation of open instruction tuning resources to date, highlights their strengths and weaknesses, and introduces models and resources to further advance research in this direction. The key insight is that while progress has been made, there is still significant room for improving both base models and instruction datasets.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper provides a comprehensive evaluation of publicly available instruction tuning datasets and models ranging from 6.7B to 65B parameters, finding that model scale, base model quality, and diversity of tuning data are key factors determining performance across evaluations of knowledge, reasoning, multilinguality, coding, and open-ended instruction following ability, with their proposed LLaMa model fine-tuned on a mix of datasets achieving strong but still subpar performance compared to proprietary models like ChatGPT and GPT-4.
