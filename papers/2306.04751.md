# [How Far Can Camels Go? Exploring the State of Instruction Tuning on Open   Resources](https://arxiv.org/abs/2306.04751)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central research question of this paper is: How do different open instruction tuning datasets and models compare in improving language models' capabilities on a diverse set of benchmark tests and open-ended instruction following? Specifically, the paper systematically compares the performance of various open instruction tuning datasets (e.g. Dolly, FLAN, Alpaca) and models of different sizes (7B to 65B parameters) on benchmarks testing skills like reasoning, factuality, coding, and multilinguality. It also evaluates their open-ended instruction following abilities through model-based (GPT-4) and human evaluations. The key hypothesis seems to be that while different datasets may excel at improving certain specialized skills, combining multiple datasets together into a mixture model like TÃ¼lu should lead to the best overall performance across diverse evaluation metrics. The paper aims to rigorously test this hypothesis through comprehensive experiments across datasets, model sizes, and evaluation settings. The goal is to provide guidance on how to develop better open instruction-tuned models.
