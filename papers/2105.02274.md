# [Rethinking Search: Making Domain Experts out of Dilettantes](https://arxiv.org/abs/2105.02274)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be: How can ideas from classical information retrieval and recent advances in pre-trained language models be synthesized to create next-generation information retrieval systems that can provide high-quality, human expert-like responses to user information needs?The key ideas and components to address this question include:- Moving from language models to "corpus models" that jointly model term-term, term-document, and document-document relationships. This allows retrieving documents directly from the model without a separate index.- Using the model in a multi-task setting for various IR tasks like retrieval, question answering, summarization, etc. This allows one consolidated model to handle many tasks. - Leveraging the model's ability for zero-shot and few-shot learning to adapt it to new tasks and corpora with limited labeled data.- Generating high-quality responses by ensuring they are authoritative, unbiased, transparent, diverse, accessible, etc.- Adding capabilities like reasoning, multiple modalities, multiple languages, leveraging document structure, etc. to enhance the model.- Replacing the traditional "index-retrieve-rank" paradigm with a single consolidated model that encodes all corpus knowledge.So in summary, the key hypothesis is that synthesizing classical IR and modern language models can produce IR systems with much greater capabilities compared to what exists today. The paper outlines a research agenda for achieving this vision.


## What is the main contribution of this paper?

The main contribution of this paper is proposing a new "model-based" paradigm for information retrieval that moves beyond the traditional "index-retrieve-then-rank" approach. The key ideas are:- Replacing indexes with a consolidated pre-trained model that encodes all the knowledge in a corpus. This eliminates the distinction between retrieval and ranking.- Using the model for a wide range of IR tasks via multi-task learning. The same model can do document retrieval, question answering, summarization, etc. - Leveraging the model's ability to generalize with little labeled data to adapt to new tasks and corpora via zero-shot and few-shot learning.- Synthesizing high quality, "domain expert" responses by making connections between terms and documents explicit in the model. This allows citing sources and evidence for answers.- Overcoming limitations of current pre-trained language models that are "dilettantes" with only superficial knowledge. The proposed model would have much deeper knowledge and understanding of documents and the corpus.In summary, the paper proposes a new way of thinking about IR systems by consolidating everything into a single pre-trained model. This has the potential to significantly advance IR capabilities if the research challenges around training, scalability, response generation quality, etc. can be addressed.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

The paper proposes a new model-based approach to information retrieval that replaces traditional indexing and separate retrieval/ranking stages with a single consolidated neural model trained directly on the corpus. The key idea is to leverage recent advances in pre-trained language models and multi-task learning to create an end-to-end neural IR system that encodes all knowledge about the corpus and can solve a wide range of IR tasks in a unified manner.


## How does this paper compare to other research in the same field?

This paper presents a vision for a new paradigm of information retrieval systems based on large pre-trained language models. Here are some key points on how it compares to other related work:- It proposes moving away from the traditional "index-retrieve-then-rank" paradigm that has been dominant in IR for decades. Instead, it advocates for a consolidated model-based approach that replaces indexing and retrieval with model training and inference. This is a significant departure from most prior IR research.- The envisioned system would leverage recent advances in pre-trained language models and multi-task learning to handle a wide range of IR tasks in a unified framework. This builds upon previous work using neural models for ranking, but takes it a step further by consolidating multiple tasks into one model.- A key goal is generating high quality, human expert-like responses, with supporting evidence and citations. This goes beyond most question answering and summarization systems today which lack deep semantic understanding and reasoning.- The proposal explores research directions like incremental learning, multi-modality, and interpretability that aim to make such model-based IR systems practical. Most prior work has not addressed the systems building and engineering challenges to the same degree.Overall, the paper presents an ambitious vision for transforming IR using modern NLP that, if realized, could enable significantly more capable systems. It tackles longstanding challenges around semantic search and reasoning in a novel way compared to incremental extensions of traditional IR models. The proposed research agenda could catalyze new cross-disciplinary work spanning IR, NLP, and ML.
