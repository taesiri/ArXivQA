# [Random Search as a Baseline for Sparse Neural Network Architecture   Search](https://arxiv.org/abs/2403.08265)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Sparse neural networks can achieve similar or better performance than dense networks while being more parameter efficient. This has motivated work on learning or searching for good sparse network architectures. 
- However, current methods lack standard baselines for comparison, hindering reliable assessment and reproducibility.

Proposed Solution:
- Propose Random Search as a baseline method for finding good initialized sparse sub-networks within overparameterized neural networks. 
- Method called "Weedout" - performs Random Search to find better sparse networks in a population based on early validation performance. Keeps good solutions ("weeds out" bad ones).
- Apply to image classification task using Wide ResNets on CIFAR-10. Compare performance of networks found by Weedout vs pure random sparse networks.

Key Results:
- Sparse Wide ResNets still achieve relatively good accuracy even at 80% sparsity.
- Weedout is able to find better initialized sparse networks based on early validation performance.
- However, after full training, networks found by Weedout perform the same as random sparse networks. Weedout early advantage is quickly overridden.

Main Contributions:
- Proposes Random Search as a baseline method for sparse neural network architecture search.
- Experiments on CIFAR-10 classification comparing Weedout found sparse networks against random sparse networks in Wide ResNets.
- Shows Weedout is unable to find sparse networks that outperform random ones post full training.
- Concludes passing this simple baseline could indicate more effective sparsity search methods beyond random.
