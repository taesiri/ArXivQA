# [Can Temporal Information Help with Contrastive Self-Supervised Learning?](https://arxiv.org/abs/2011.13046)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central research question this paper tries to address is: Can temporal information help with contrastive self-supervised learning for videos?More specifically, the authors aim to investigate:1) Whether simply adding temporal augmentations to existing contrastive self-supervised learning (CSL) frameworks is sufficient to improve video representation learning. 2) How to better incorporate temporal information into CSL frameworks to learn useful video representations.3) Whether using multiple video prediction tasks jointly can further help with video CSL. The key hypothesis is that directly applying temporal augmentations is not enough, but explicitly modeling temporal information as extra self-supervision signals can significantly improve video CSL.To summarize, the paper focuses on studying the effects of temporal information on video contrastive self-supervised learning, and proposes a new framework called TaCo that integrates temporal knowledge more effectively for learning better video representations.


## What is the main contribution of this paper?

The main contributions of this paper appear to be:1. Proposing TaCo (Temporal-aware Contrastive learning), a new framework for integrating temporal information into contrastive self-supervised learning for videos. 2. Providing the first detailed study on the effects of temporal information in video contrastive self-supervised learning. The authors find that simply adding temporal augmentations does not help or can even hurt performance. 3. Based on the observation that temporal transformations should provide extra self-supervision, TaCo uses selected temporal transforms not just for augmentation but also as pretext tasks with corresponding task heads. The overall loss is a combination of contrastive loss and task loss.4. Demonstrating through experiments that TaCo boosts performance over standard video contrastive learning baselines across different settings. The best TaCo model achieves 85.1% on UCF-101 and 51.6% on HMDB-51, improving over prior state-of-the-art.5. Analyzing which temporal pretext tasks are better suited for video contrastive learning, and showing certain tasks harmonize well in multi-task settings.So in summary, the main contribution is proposing TaCo as a new way to effectively incorporate temporal information into video contrastive self-supervised learning, through using temporal transformations as both augmentations and pretext tasks. The paper provides extensive analysis and demonstrates improved performance over baselines.
