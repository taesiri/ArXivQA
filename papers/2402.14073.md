# [Improving Language Understanding from Screenshots](https://arxiv.org/abs/2402.14073)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
Existing screenshot language models, which process both text and images in a single input, substantially lag behind text-only models on language understanding tasks. This disparity significantly restricts their utility for real-world applications. Hence, improving their text ability is crucial.  

Proposed Solution:  
The authors adopt a simplified text-only screenshot setting to focus on architecture and training objective changes. They propose a novel Patch-and-Text Prediction (PTP) training objective, which masks and predicts both image patches of screenshots and text within screenshots. This helps learn both visual features and language understanding.

Key Contributions:
1) The PTP objective masks and predicts screenshot patches and text, unlike prior works that predict either patches or text. Combining both is shown to significantly improve performance.

2) Extensive ablation studies are conducted on masking rates, patch sizes, and training stability designs. A pre-trained PTP model achieves comparable performance to BERT on 6 out of 8 GLUE tasks, improving up to 8% over prior screenshot LMs.

3) PTP is extended to autoregressive screenshot LMs by feeding both patches and text to a decoder. Experiments show these models can effectively utilize screenshot context to reduce perplexity on subsequent text.

Overall, the paper introduces PTP to improve text ability of screenshot LMs, with comparisons to strong baselines. Detailed analyses provide insights into optimal model configurations. Limitations are also discussed to inspire future research on more powerful and practical screenshot LMs.
