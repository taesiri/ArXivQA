# [Associative Memories in the Feature Space](https://arxiv.org/abs/2402.10814)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Existing associative memory models like Hopfield networks fail to retrieve visual memories like images when there are even small corruptions like noise, rotations, croppings etc. This is because they compute similarities in the raw pixel space which lacks semantic information.

Proposed Solution: 
- Introduce a class of "semantic" associative memory models that augment standard models with a feature mapping function φ. This embeds memories into a semantic space where similarities are more meaningful. 

- Use neural nets pretrained with contrastive loss as the feature mapping φ. This maps corrupted versions of the same image closer in the embedding space.

- Test semantic memory models on complex image datasets like CIFAR10 and STL10. Show significant boost over standard Hopfield nets.

- Also propose a "fully-semantic" memory model that stores only the embeddings rather than full images. Uses generative mapping ψ to reconstruct images. More memory efficient and biologically plausible.

Main Contributions:

- Definition and evaluation of semantic memory models using pretrained contrastive models as the feature mapping. Enables associative recall of complex visual data.

- Analysis of different similarities in embedding space - cosine, L1, L2. L1 similarity works best overall.

- Semantic models much more robust to corruptions and perturbation of images.

- Fully-semantic model more memory efficient and biologically plausible but reconstructed images not identical to originals.

In summary, key idea is to leverage semantic embeddings to enable associative memory models to store and recall complex visual data instead of raw pixels. Contrastive pretraining provides the semantic mapping.
