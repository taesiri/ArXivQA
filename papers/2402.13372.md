# [EvoGrad: A Dynamic Take on the Winograd Schema Challenge with Human   Adversaries](https://arxiv.org/abs/2402.13372)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- While large language models (LLMs) achieve high performance on the Winograd Schema Challenge (WSC), a common-sense reasoning task, they struggle when instances are slightly altered or reworded. This reveals deficiencies in their robustness and stability.
- Existing WSC datasets are limited in size and diversity. They may also unintentionally introduce biases. 
- There is a need for diverse, inclusive, and dynamic WSC benchmarks to comprehensively evaluate model capabilities.

Proposed Solution: 
- The authors introduce EvoGrad, an open-source platform dedicated to the continuous expansion of nuanced WSC instances. 
- EvoGrad allows global users to actively contribute perturbed versions of sentences, fostering diversity.
- It combines human creativity with ChatGPT's efficiency to generate elaborate perturbations of seed sentences, expanding the dataset from 182 to 3,691 instances.
- Wordnet is also utilized to introduce synonymous substitutions, enhancing variability.

Main Contributions:
- A novel data construction approach combining human and model contributions for high-quality, dynamic datasets.
- Introduction of the "error depth" metric to quantify model stability against progressive perturbations.
- An interactive, user-centric platform for public participation in dataset curation and model evaluation.
- Analysis highlighting performance gaps between humans and LLMs, emphasizing the value of EvoGrad's dynamic framework in uncovering model limitations.

The paper demonstrates how EvoGrad's unique human-and-model-in-the-loop methodology, paired with its focus on diversity and inclusion, facilitates the comprehensive testing of common-sense reasoning capabilities in LLMs as they continue to evolve.
