# [Case-Based or Rule-Based: How Do Transformers Do the Math?](https://arxiv.org/abs/2402.17709)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem Statement
- Despite impressive capabilities, large language models (LLMs) still struggle with some basic math reasoning tasks like addition of large numbers. This is in contrast to humans who can easily generalize the rules of addition to numbers of any length after learning. 

- It is debated whether LLMs' reasoning abilities come from capturing underlying generalizable rules (rule-based reasoning) or relying on similar cases seen during training (case-based reasoning). This work aims to explore which mechanism LLMs employ for math reasoning.

Methods
- The concepts of rule-based reasoning (applying abstract rules to new problems) and case-based reasoning (relying on similar training examples) are formally defined. 

- Intervention experiments are conducted on 5 math tasks by removing certain continuous regions of training data to isolate test samples. Significant performance drops are observed, providing evidence that LLMs perform case-based reasoning.

- For addition, removing surrounding cases causes accuracy "holes" in test regions. This holds even with scratchpad for step-by-step workings, showing reliance on seen sub-calculations.

- In-context learning experiments also reveal dependence on similar examples over randomly sampled ones.

Proposed Solution  
- A Rule-Following Fine-Tuning (RFFT) method is introduced to shift LLMs from case-based to rule-based reasoning.

- Explicit rules are provided to models as input, and models are trained to step-by-step quote and follow rules to solve problems.

Results
- With RFFT, LLMs successfully generalize up to 12-digit addition despite only seeing 1-5 digits during training, significantly outperforming prior work.

- Analysis reveals models can select the right rules but occasionally make basic mistakes in rule execution, limiting strict generalization.

Main Contributions
- Provides direct evidence that transformers perform case-based math reasoning through intervention experiments.

- Proposes an effective RFFT technique to mitigate limitations of case-based reasoning by teaching explicit rule-based reasoning.

- Demonstrates enhanced systematic generalization on addition problems despite limited training data.
