# [Minds versus Machines: Rethinking Entailment Verification with Language   Models](https://arxiv.org/abs/2402.03686)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem Statement
- Evaluating inference capabilities between humans and state-of-the-art language models (LLMs) is important for advancing language understanding. 
- Existing textual inference datasets are limited as they contain short, simple premises instead of multi-sentence, complex reasoning.

Methods 
- Created an entailment verification benchmark with datasets across 3 categories - NLI, contextual QA, rationales. These have multi-sentence premises requiring different reasoning skills.
- Evaluated both human performance (via Amazon Mechanical Turk) and various LLMs like RoBERTa, GPT-3, Entailer, Flan-T5 on this benchmark. 
- Analyzed the performance across 4 reasoning types - simple deductive, complex deductive, missing entity/commonsense knowledge, missing localized knowledge.

Key Findings
- LLMs better at tasks needing multi-hop reasoning over long contexts while humans better at simple deductive reasoning tasks.
- LLMs superior at retrieving entity-grounded knowledge and humans better at inferring missing commonsense knowledge.  
- Proposed two model finetuning strategies - classification and ranking-based learning. The ranking approach learns a softer decision boundary.
- Finetuned model rivals GPT-4 performance and outperforms GPT-3 on the benchmark. Demonstrated the model's utility in filtering inconsistent rationales.

Main Contributions
- Comprehensive analysis of human vs. LLMs inference abilities over diverse reasoning types on multi-sentence contexts.
- Identification of strengths and weaknesses - LLMs better at complex reasoning, humans at simple deductive reasoning involving substitutions, negations etc.
- State-of-the-art entailment verification model finetuned with ranking-based learning, available publicly. 
- Demonstrated improvements from using model to filter inconsistent rationales in LLM explanations.
