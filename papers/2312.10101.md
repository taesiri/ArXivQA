# [A Review of Repository Level Prompting for LLMs](https://arxiv.org/abs/2312.10101)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem Statement
- As coding tasks become more complex, large language models (LLMs) have shown promise in automated code generation on isolated problems. However, performance drops significantly on real-world repository-level tasks which require understanding context and dependencies.  

- Two methods have been proposed to address this - Repository-Level Prompt Generation which optimizes prompts, and RepoCoder which uses iterative retrieval and generation. This paper reviews these methods, their trade-offs and best practices.

Repository-Level Prompt Generation
- Compiles code prompts from relevant segments in the repository to provide context for the LLM (Codex). Evaluates prompts based on if they lead to successful code generation.

- Models prompt choices as a vector and defines a loss function to classify useful prompts. Uses this to train models RLPG-H and RLPG-R to select optimal prompts.

- Retrieves prompts from current, parent, child, imported and sibling files and classes to provide relevant context.

RepoCoder
- Uses BM25 retrieval on code before and after the gap to find relevant snippets. Iteratively regenerates code using previous output as query.

- Repository contexts include names of imported modules and identifiers in code around the gap. Fuzzy matching further expands the context.

Results
- Both methods show significant gains over baseline Codex, with over 25% reduction in edit distance and 35% boost in success rate.

- RepoCoder achieves 44.6% success on repository test cases, much higher than in-file retrieval. RLPG-H gives best performance on prompt selection.

- Analysis shows parent, sibling and child classes provide highly useful context for generation.

Recommendations
- Combine iterative prompt refinement from RepoCoder with optimized prompt selection from RLPG for best results.

- Integrate these methods into more complex benchmarks like SWE-Bench to solve limitations of current retrieval techniques.

- Look towards agent-based LLMs as inference times fall, to further advance state-of-the-art.
