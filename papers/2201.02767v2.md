# [QuadTree Attention for Vision Transformers](https://arxiv.org/abs/2201.02767v2)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading, the main research question this paper addresses is: How can we design an efficient vision transformer that captures both fine image details and long-range dependencies?

The key ideas and contributions of the paper are:

- Proposes QuadTree Attention, a new attention mechanism for vision transformers that reduces computational complexity from quadratic to linear. 

- QuadTree Attention computes attention in a coarse-to-fine manner using token pyramids. It selects top K patches with highest attention scores at each level, allowing attention at finer levels to focus only on relevant regions.

- This allows capturing both local fine details as well as global context efficiently in a single model, overcoming limitations of prior works like Swin Transformer and PVT which compromise on one or the other.

- Achieves SOTA results on tasks requiring both cross attention (feature matching, stereo) and self-attention (image classification, object detection) with significant reductions in compute compared to relevant baselines.

So in summary, the main research contribution is an efficient transformer attention mechanism that can capture both local and global information well, enabling SOTA results on a variety of vision tasks with reduced compute. The key idea is the quadtree-based coarse-to-fine attention.


## What is the main contribution of this paper?

 Here is a summary of the main contributions of the paper "Quadtree Attention for Vision Transformers":

- The paper proposes a new attention mechanism called Quadtree Attention that reduces the computational complexity of vision transformers from quadratic to linear. 

- Quadtree Attention builds token pyramids and computes attention in a coarse-to-fine manner. At each level, it selects the top K patches with the highest attention scores. Then at the next finer level, it only computes attention within the relevant regions corresponding to the top K patches from the previous coarser level. This allows it to skip computation in irrelevant regions.

- The paper shows that Quadtree Attention achieves state-of-the-art performance on various computer vision tasks while being computationally efficient:

    - In feature matching on ScanNet, it achieves 4% higher AUC compared to linear transformer with similar FLOPs.

    - In stereo matching, it achieves similar accuracy as standard transformer but with 50% less FLOPs and 40% less memory.

    - In ImageNet image classification, it achieves 1-1.5% higher top-1 accuracy compared to previous state-of-the-art transformers like Swin and PVT.

    - In COCO object detection using RetinaNet, it achieves 1.8% higher AP than PVTv2 backbone with fewer FLOPs.

    - In semantic segmentation on ADE20K, it achieves 0.7-2.4% higher mIoU compared to PVTv2.

- Overall, the paper introduces an efficient attention mechanism that achieves strong performance across multiple vision tasks by computing attention selectively in a coarse-to-fine manner using a quadtree structure. The computational efficiency allows transformers to be applied to high resolution vision tasks effectively.
