# [Phoneme-aware Encoding for Prefix-tree-based Contextual ASR](https://arxiv.org/abs/2312.09582)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Contextual biasing in ASR aims to improve recognition of uncommon context-specific words (e.g. names) by biasing towards a provided list of such words likely to appear in the context. 
- However, existing methods rely solely on textual representations and do not leverage pronunciation information, which can be crucial for rare words with unusual pronunciations. This limitation is more severe for languages like Japanese where characters do not reflect pronunciation.

Proposed Solution:
- Introduce phoneme-aware encoding in Tree-Constrained Pointer Generator (TCPGen), a state-of-the-art prefix tree-based contextual biasing method for ASR. 
- Obtain subword-level phoneme representation by aligning phonemes to subwords using attention weights of seq2seq G2P conversion or EM-based alignment. Sum the weighted phoneme embeddings to obtain node encodings.
- Make the attention query in TCPGen phoneme-aware by incorporating phoneme predictions from auxiliary CTC loss.

Main Contributions:
- First work to leverage pronunciation information for prefix-tree contextual biasing methods through subword-level phoneme encoding and phoneme-aware attention.
- Confirm consistent gains over standard TCPGen on both English (Librispeech) and Japanese (CSJ) speech datasets, showing cross-lingual effectiveness. 
- Analysis revealing optimal alignment and phoneme embedding approaches differ across languages. Attention-weights suit English better while hard EM-alignment works better for Japanese.

In summary, the paper proposes a novel way to effectively incorporate pronunciation information in TCPGen through phoneme encoding and attention to improve contextual biasing, especially for uncommon words.
