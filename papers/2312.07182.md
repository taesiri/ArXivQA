# [Classifying complex documents: comparing bespoke solutions to large   language models](https://arxiv.org/abs/2312.07182)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a high-quality one-paragraph summary of the key points from the paper:

This paper explores using large language models (LLMs) for a complex document classification task, comparing an LLM-based approach to a custom bag-of-words model. The authors classified 30,000 legal documents from courthouses into binary categories and 9 sub-categories. They found that a bespoke model combining n-gram encoding with a convolutional neural net achieved the highest accuracy of 96%. However, fine-tuning the GPT-3.5 LLM with only 2,000 labeled examples achieved comparable performance. Although the LLM accuracy plateaued below bespoke model levels, the authors argue LLMs represent good value given lower implementation costs. Limitations are lack of control, data privacy, and model transparency. The authors conclude that while bespoke solutions may be preferable for business critical workflows, fine-tuned LLMs can rapidly prototype high-performing solutions for document classification at lower cost, acting as a baseline while requiring additional validation layers to control for possible hallucinations.


## Summarize the paper in one sentence.

 This paper compares the performance of a custom bag-of-words model, an off-the-shelf large language model (GPT-3.5), and a fine-tuned version of GPT-3.5 on a complex document classification task involving 30,000 oil and gas legal records, finding that the custom model performs best but fine-tuning GPT-3.5 leads to comparable accuracy.


## What is the main contribution of this paper?

 Based on my reading of the paper, the main contribution is:

Comparing the performance of three different models for classifying complex legal documents into binary and multi-label categories: 
1) A bespoke bag-of-words model with a convolutional neural network
2) An off-the-shelf large language model (LLM): GPT-3.5
3) A fine-tuned version of the LLM (GPT-3.5)

The key findings are:

- The bespoke model achieves the highest accuracy overall
- The off-the-shelf LLM performs significantly worse 
- Fine-tuning the LLM improves its performance considerably, but still does not reach the level of the bespoke model
- The LLM requires less labeled data for training (2,000 documents vs. 30,000 for bespoke model)
- Fine-tuning brings LLM performance to comparable but still lower levels than the bespoke model, representing good "value for effort"

In summary, the main contribution is a comparison of bespoke and LLM models for a complex document classification task, providing guidance on the tradeoffs between accuracy, effort and cost.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, I would identify the following as some of the key keywords or terms:

- Document classification
- Large language models (LLMs)
- Fine-tuning
- Bespoke/custom models
- Bag-of-words (BOW) model
- Convolutional neural network
- GPT-3.5
- Accuracy
- Cost-effectiveness
- Data labeling
- Training data size
- Performance comparison
- Value for effort
- Lack of control
- Data sharing 
- Validation layer
- Task suitability

The paper compares different approaches to classifying complex legal documents, specifically contrasting a bespoke natural language processing model against using and fine-tuning the GPT-3.5 large language model. It analyzes the accuracy, cost-effectiveness, and other practical considerations of each approach. Key terms cover the different models used, their training and tuning, performance metrics, and the tradeoffs discussed when deciding which approach to take.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in the paper:

1. The paper proposes using a bag-of-words model combined with a convolutional neural network for document classification. What are the advantages and disadvantages of using a bag-of-words model compared to other NLP techniques for this task?

2. The optimal context window was found to be 800 tokens for binary classification and 1500 tokens for multi-class classification. What factors likely determined this optimal context window length? How could the optimal value be further validated?  

3. The paper argues that fine-tuned large language models can achieve comparable but lower performance compared to bespoke models. What factors may account for this performance gap? How could this gap potentially be reduced?

4. The authors note that fine-tuned LLMs can sometimes "hallucinate" predictions not actually present in the taxonomy. What techniques could help reduce such hallucinated predictions from fine-tuned LLMs?  

5. The dataset used contains some amount of label noise, limiting maximum achievable accuracy. What data cleaning or model techniques could help improve accuracy in the presence of noisy labels?  

6. What types of documents or classification tasks would be poorly suited to the techniques used in this paper? When would alternative techniques be more appropriate?

7. The authors argue bespoke solutions allow more control compared to relying on third-party LLMs. What specific risks and downsides arise from relying on third-party LLMs?  

8. How was the selection of n-gram size and other hyperparameters optimized in this work? What impact would suboptimal hyperparameter choices have?

9. What other evaluation metrics beyond accuracy could provide additional insights into model performance for this task?

10. The work relies on a manually labeled dataset which can be expensive. How could semi-supervised or unsupervised techniques help reduce labeling costs?
