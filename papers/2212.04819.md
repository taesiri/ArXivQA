# [Phone2Proc: Bringing Robust Robots Into Our Chaotic World](https://arxiv.org/abs/2212.04819)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading, the central research question this paper addresses is: How can we train embodied AI agents in simulation that can effectively transfer and perform robustly when deployed onto physical robots in real-world environments? 

The key ideas and contributions are:

- Phone2Proc: A method to quickly scan a real-world environment using a phone, and leverage that scan to procedurally generate a large set of simulated training environments that match that target scene.

- Extensive real-world robot experiments: The paper conducts 234 evaluation episodes across 5 diverse real environments, which represents a large-scale sim-to-real evaluation. 

- Significantly improved sim-to-real transfer: Phone2Proc improves average success rate from 34.7% to 70.7% compared to prior state-of-the-art in the real world tests across the 5 environments.

- Robustness to real-world variation: Phone2Proc agents are shown to be robust to changes like lighting variation, clutter, object rearrangement, and human movement, unlike agents trained on static reconstructions.

In summary, the paper introduces Phone2Proc to narrow the sim-to-real gap by generating targeted procedural training environments, and demonstrates through large-scale real-world experiments that this approach significantly improves sim-to-real transfer and produces more robust embodied AI agents.


## What is the main contribution of this paper?

 Based on my reading of the paper, the main contribution is a method called Phone2Proc that uses a phone scan of a real-world environment and procedural generation to create a distribution of training scenes tailored to that environment. The key ideas are:

- Using an iPhone app and Apple's RoomPlan API to quickly scan a real-world target scene and generate a layout with 3D object placements. 

- Leveraging procedural generation with ProcTHOR to take the phone scan and produce many variations of the scene with randomized objects, materials, lighting, etc.

- Training embodied agents (e.g. for navigation) on this distribution of simulated scenes that match the real-world space.

- Showing significant improvements in transferring these agents to the physical world compared to prior simulation-only methods, through extensive real robot experiments.

In summary, the main contribution is an effective approach to reduce the sim-to-real gap by tailoring the simulation training scenes to match the target real environment, as captured through a quick phone scan. This allows training robust policies that transfer much better to the physical world compared to training on more generic simulations.
