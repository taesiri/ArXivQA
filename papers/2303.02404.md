# [Fine-Grained Classification with Noisy Labels](https://arxiv.org/abs/2303.02404)

## What is the central research question or hypothesis that this paper addresses?

 The central research question addressed in this paper is how to develop an effective learning algorithm to handle noisy labels in fine-grained image classification datasets. 

The key hypotheses are:

1. Existing learning with noisy labels (LNL) methods may not perform well on fine-grained classification tasks due to the large inter-class similarity. 

2. Encouraging discriminative feature representations can help confront label noise and facilitate learning of fine-grained differences. 

3. A noise-tolerant supervised contrastive learning approach can mitigate the effects of label noise and learn robust features.

4. A stochastic feature transformation module can further augment contrastive learning without manual feature engineering.

5. The proposed framework can boost various LNL algorithms when applied to fine-grained datasets.

In summary, the paper proposes a new Stochastic Noise-tolerated Supervised Contrastive Learning (SNSCL) framework to address the challenging problem of handling noisy labels in fine-grained classification. The central hypothesis is that SNSCL can learn robust discriminative features to mitigate noise and improve classification accuracy.


## What is the main contribution of this paper?

 The main contributions of this paper are:

1. It proposes a new task called LNL-FG (Learning with Noisy Labels for Fine-Grained classification) which is more challenging than typical LNL on generic image classification. The authors show empirically that many existing LNL methods perform poorly on LNL-FG.

2. It proposes a novel framework called SNSCL (Stochastic Noise-tolerated Supervised Contrastive Learning) to address the LNL-FG task. SNSCL contains two main components:

(a) A noise-tolerated contrastive loss that incorporates a weight-aware mechanism to correct noisy labels and selectively update the momentum queue to avoid inserting noisy samples. This overcomes the noise sensitivity issue of typical supervised contrastive learning.

(b) A stochastic module that generates a probabilistic distribution over feature embeddings and samples augmented embeddings from it. This avoids manually designing augmentation strategies.

3. Extensive experiments on four fine-grained datasets and two real-world noisy datasets demonstrate SNSCL achieves state-of-the-art performance. Ablation studies verify the efficacy of each component in SNSCL.

In summary, the key contribution is proposing the LNL-FG task and an effective SNSCL framework to address it. SNSCL enhances robustness on LNL-FG via noise-tolerated contrastive representation learning and stochastic data augmentations.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the paper:

The paper proposes a new framework called Stochastic Noise-Tolerated Supervised Contrastive Learning (SNSCL) to improve the performance of deep learning models on fine-grained classification tasks with noisy labels.
