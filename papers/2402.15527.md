# [PCA-Bench: Evaluating Multimodal Large Language Models in   Perception-Cognition-Action Chain](https://arxiv.org/abs/2402.15527)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Existing benchmarks for evaluating multimodal large language models (MLLMs) often focus on individual capabilities rather than integrated skills. They also lack error localization techniques to identify the source of inaccuracies.

- It is important to assess MLLMs' ability for complex decision making using multiple integrated skills like perception, reasoning, and action. This is relevant for applications in embodied AI agents. 

Proposed Solution - PCA-Bench:
- Introduces a benchmark with instances across 3 domains - autonomous driving, robotics, and gaming.

- Annotates each instance with image, question, actions, answer, reasoning, and key concepts to enable fine-grained evaluation.

- Proposes PCA-Eval protocol to automatically evaluate perception, cognition, and action scores and localize errors. Shows high correlation (>0.8 kappa) with human assessments.

- Presents Embodied Instruction Evolution (EIE) method to automatically generate additional annotated instances by integrating LLMs with environments.

Key Contributions:
- PCA-Bench focuses on integrated decision making abilities using perception, reasoning, and action instead of individual skills.

- PCA-Eval enables automatic fine-grained evaluation and error localization for the decision chain.

- Analysis shows GPT-4V outperforms other MLLMs. Finetuning with EIE generated data significantly boosts open-source MLLMs, occasionally surpassing GPT-4V.

- Findings suggest MLLMs show promise for decision making in embodied agents. PCA-Bench facilitates assessment of integrated capabilities.

In summary, this paper introduces a new benchmark to assess MLLMs' integrated skills for decision making by combining perception, reasoning and action. The benchmark enables error localization and can synthesize additional annotated data. Experiments underscore the potential while also highlighting gaps to focus improvement efforts.
