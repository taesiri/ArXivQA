# [Mismatch Quest: Visual and Textual Feedback for Image-Text Misalignment](https://arxiv.org/abs/2312.03766)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Existing image-text alignment models can only provide binary assessments of whether an image and text match or not, but cannot pinpoint the exact source of misalignment when a mismatch is detected. 
- Detailed feedback on mismatches would allow better understanding of alignment failures and help improve generative models.

Proposed Solution:
- Introduce a new method (\method) to generate training data with plausible misaligned image-text pairs and corresponding textual and visual (bounding box) explanations of the misalignments.
- Leverage capabilities of large language models (LLMs) and visual grounding models to automatically construct this training data.
- Filter the training data to ensure high quality using entailment models.  
- Train alignment evaluation models on this data to predict both alignment labels and detailed feedback.

Main Contributions:
- A novel feedback-centric data generation procedure to create a large-scale training set (\trainset) with over 3 million text-image pairs showing different types of misalignments and detailed textual and visual explanations.
- A new human-curated test set (\testset) with ground truth alignment and explanation annotations to evaluate models.
- Trained models that outperform baselines in predicting alignment, generating textual explanations, and visually indicating misalignments within images on both the automatic metrics and human evaluations.
- Demonstrated improved understanding of misalignment causes and ability to provide feedback with both text and visual pointers.
- Publicly released training set, benchmark test set, and models to facilitate further research.

The paper introduces an end-to-end framework and datasets to equip alignment models with detailed textual and visual feedback generation capabilities beyond binary scoring, helping diagnose alignment failures in vision-language models.
