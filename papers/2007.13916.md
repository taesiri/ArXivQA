# [Demystifying Contrastive Self-Supervised Learning: Invariances,   Augmentations and Dataset Biases](https://arxiv.org/abs/2007.13916)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the key research questions and hypotheses addressed in this paper are:- What explains the recent performance gains of contrastive self-supervised learning methods like MoCo and PIRL on downstream tasks like object detection and image classification? The paper hypothesizes that these gains come mainly from occlusion invariance learned through aggressive cropping augmentations.- Do contrastive self-supervised methods learn all the necessary invariances for object recognition tasks? The paper hypothesizes that while these methods learn occlusion invariance, they are inferior at learning viewpoint and category instance invariance compared to supervised methods. - Are the aggressive cropping augmentations an ideal strategy for learning useful representations? The paper hypothesizes that the cropping relies heavily on dataset bias and may not be scalable to more complex datasets.- Can alternative data like videos be used to learn better viewpoint and deformation invariance? The paper proposes that leveraging temporal transformations in videos can help learn representations with higher viewpoint invariance in a self-supervised manner.In summary, the key hypotheses are around understanding why contrastive self-supervised learning works so well recently, analyzing the limitations of current approaches, and proposing videos as an alternative form of data to learn better representations. The experiments aim to verify if aggressive cropping indeed leads to occlusion invariance, and if videos can improve viewpoint invariance.


## What is the main contribution of this paper?

The main contributions of this paper are:1. Presenting a framework to evaluate invariances encoded in self-supervised visual representations. The invariances considered are viewpoint, occlusion, illumination, instance, etc. which are relevant for object recognition tasks.2. Using this framework to analyze two recent contrastive self-supervised learning methods - MoCo and PIRL. The analysis shows these methods achieve gains in occlusion invariance compared to supervised pretraining, but are inferior in terms of viewpoint and instance invariance. 3. Demonstrating through experiments that the gains of contrastive self-supervised learning rely heavily on the object-centric bias of datasets like ImageNet. When trained on a scene-centric dataset like MSCOCO, the representations have lower discriminative power.4. Proposing an alternative approach to improve invariances by leveraging naturally occurring transformations in videos through temporal correspondences between frames and regions. This approach is shown to achieve better viewpoint and instance invariance compared to MoCo trained on the same data.5. Showing improved performance of video-based self-supervised learning on downstream tasks like image classification and semantic segmentation compared to MoCo trained on frames.In summary, the paper provides an analysis framework to understand self-supervised visual representations and uses it to highlight strengths and weaknesses of current contrastive learning methods. It also proposes an alternative approach using videos that helps mitigate some of the weaknesses.


## How does this paper compare to other research in the same field?

Here are my thoughts on how this paper compares to other research in the field of contrastive self-supervised learning:- This paper provides an in-depth analysis and "demystification" of recent gains made by contrastive self-supervised learning methods like MoCo, PIRL, and SimCLR. It investigates the reasons behind their strong performance on downstream tasks through diagnostic experiments. This kind of thorough analysis is not common in most self-supervised learning papers which simply report benchmark performances.- The paper examines the invariances captured by contrastive SSL methods compared to supervised learning. It finds these methods are better at learning occlusion invariance due to aggressive cropping augmentations, but still lag in viewpoint and instance invariance critical for object recognition. This analysis of invariances is quite insightful.- The paper hypothesizes that contrastive SSL relies on dataset bias and limited model capacity to learn useful representations despite the issues with aggressive cropping. The diagnostic experiments support these hypotheses well. Most prior works do not study these aspects in-depth.- The paper proposes an alternative approach using videos and temporal transformations to improve invariances lacking in image-based contrastive SSL. Learning from videos has been explored before, but incorporating temporal transformations into contrastive learning frameworks is novel.- Compared to some other analysis works like Understanding Contrastive Representation Learning (Wang et al. 2020), this paper provides more task-focused insights related to object recognition abilities. The analysis of biases is also more thorough.In summary, this paper provides valuable insights into contrastive SSL through extensive diagnostic experiments and analysis. The examination of invariances, dataset biases, model capacity limitations, and benefits of videos is quite comprehensive compared to most prior works which focus on benchmark performances. The insights could inform future improvements to self-supervised techniques.


## What future research directions do the authors suggest?

Based on my reading of the paper, some of the main future research directions suggested by the authors are:- Developing better augmentation strategies for contrastive self-supervised learning. The authors argue that aggressive cropping can be harmful and relies too much on dataset bias. They suggest exploring alternative augmentation techniques that improve invariance properties like viewpoint and deformation invariance.- Exploring alternative training data beyond static images, such as videos. The authors show that leveraging naturally occurring transformations in videos can lead to improved viewpoint invariance in the learned representations. More work could be done to develop video-based techniques.- Investigating other potential biases in standard benchmark datasets like ImageNet and PASCAL VOC that may advantage contrastive self-supervised methods. The authors suggest evaluating on datasets with less co-occurrence and background biases to better measure discriminative power.- Scaling up contrastive self-supervised approaches to larger and more diverse datasets beyond ImageNet. The authors argue reliance on ImageNet's object-centric nature needs to be reduced.- Developing better analysis techniques and theoretical understanding of what makes self-supervised methods work. The invariance analysis methodology proposed here could be extended.- Exploring hybrid supervised and self-supervised approaches to get the benefits of both. Self-supervision might help learn certain invariances while supervision provides discriminative signals.In summary, key directions are rethinking augmentation strategies, leveraging alternative data like videos, addressing dataset biases, scaling up to new data, improving analysis techniques, and combining self-supervision with supervision. Reducing reliance on ImageNet and investigating the true discriminative capabilities of the learned representations seem to be critical goals according to the authors.
