# [StepFormer: Self-supervised Step Discovery and Localization in   Instructional Videos](https://arxiv.org/abs/2304.13265)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the main research question seems to be: 

How can we develop a self-supervised model to discover and localize key instructional steps in long, untrimmed instructional videos, without relying on any human annotations or video-level labels during training?

The key points are:

- The goal is to develop a model for key-step discovery and localization in instructional videos. 

- The model should be trained in a completely self-supervised manner, without any human annotations or video-level labels.

- It should handle long, untrimmed instructional videos, where only a small portion may be relevant to the actual steps.

- The model should both discover the steps (i.e. determine what they are) and also temporally localize them in the video.

The main hypothesis seems to be that it is possible to train such a self-supervised model by extracting verb phrases from automatically generated video subtitles and using sequence-to-sequence alignment techniques to match steps and narrations. The proposed StepFormer model aims to demonstrate this capability.

In summary, the key research question is around developing an unsupervised model for instructional video understanding that can discover and localize steps without any human supervision, just using narrations. StepFormer is proposed as a solution.


## What is the main contribution of this paper?

 Based on my reading of the paper, the main contributions appear to be:

1. Proposing StepFormer, a novel self-supervised approach for key-step discovery and localization in instructional videos. StepFormer uses a transformer decoder architecture with learned queries to predict ordered step slots capturing the key steps in a video.

2. Explicitly modeling the temporal order of steps and using it to design effective training and inference procedures. This includes using an order-aware sequence-to-sequence loss to enforce temporal ordering of the predicted steps during training. The inferred step ordering is also used at test time for step localization via alignment.

3. Demonstrating that StepFormer can be trained in a completely self-supervised manner on a large dataset of instructional videos using only the automatically generated subtitles as supervision. The pre-trained model transfers well to downstream datasets, achieving state-of-the-art unsupervised step localization performance without any dataset-specific fine-tuning.

In summary, the main contribution appears to be proposing a new self-supervised transformer-based model, StepFormer, for unsupervised discovery and localization of instruction steps in videos. The method is shown to work well without relying on any manual annotations.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the key points from the paper:

The paper proposes StepFormer, a self-supervised transformer model that discovers and localizes instruction steps in long untrimmed videos using only automatically generated subtitles as supervision, and shows it achieves state-of-the-art performance on step localization across multiple datasets without any dataset-specific adaptation.


## How does this paper compare to other research in the same field?

 Here are my thoughts on how this paper compares to other research in the field of self-supervised instructional video understanding:

- The key contribution of this paper is presenting StepFormer, a novel self-supervised approach for discovering and localizing key instruction steps in long, untrimmed videos. This is an important goal in the field of instructional video understanding. 

- Most prior work on step localization requires some form of supervision, such as video-level labels or ordered/unordered step transcripts. StepFormer is completely unsupervised - it relies only on the videos and corresponding narrations for training. This makes it more scalable since it doesn't require manual annotation.

- Compared to prior unsupervised methods like Kukleva et al. and Elhamifar et al., StepFormer uses the accompanying narrations as supervision rather than just the video. This allows it to learn better step representations. The narrations provide a natural form of alignment supervision.

- Unlike other methods, StepFormer does not need to be trained on the same dataset used for evaluation. It can generalize to new datasets without finetuning. Prior unsupervised models are dataset-specific.

- StepFormer establishes new state-of-the-art results on unsupervised step localization across multiple benchmarks, outperforming prior unsupervised and weakly supervised methods.

- An interesting emergent capability is StepFormer's ability to perform zero-shot localization given step text descriptions at test time. This shows it learns representations that align well with both video and language.

In summary, the self-supervised approach, ability to transfer across datasets, and state-of-the-art results are the key strengths of StepFormer compared to related work on unsupervised instructional video understanding. The use of narrations as weak supervision and modeling of step order seem to be important to its success.


## What future research directions do the authors suggest?

 Based on my reading of the paper, some of the main future research directions suggested by the authors include:

- Exploring other methods for processing the video narrations/subtitles to extract richer supervision signals. The authors currently only extract verb phrases, but suggest exploring extracting other types of textual descriptors like noun phrases or complete sentences. 

- Extending StepFormer to even larger video datasets, since it is designed to be scalable. The authors suggest pre-training on datasets larger than HowTo100M.

- Developing methods to improve StepFormer's generalization to new downstream tasks and datasets without any fine-tuning, since it currently performs zero-shot evaluation.

- Exploring semi-supervised variants of StepFormer that can take advantage of limited ground truth annotations when available. The current method is fully self-supervised.

- Extending StepFormer for related video understanding tasks beyond step localization, like procedure segmentation, action segmentation, etc.

- Developing attention mechanisms to enable StepFormer to focus on specific objects and interactions for more fine-grained step understanding.

- Combining StepFormer with other modalities like audio and object detections to further improve step discovery and localization.

In summary, the main future directions are around scaling up the approach, improving generalization, incorporating limited supervision when available, extending it to related tasks, and combining it with other modalities. The authors frame StepFormer as a general and scalable approach for procedure understanding that can be built upon in many promising ways.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the paper:

The paper proposes StepFormer, a self-supervised approach for key-step discovery and localization in instructional videos. StepFormer is a transformer decoder that takes a long untrimmed video as input and outputs an ordered sequence of step slots capturing the key steps in the video. It is trained on a large dataset of instructional videos using only the automatically generated subtitles as supervision. Specifically, verb phrases are extracted from the subtitles and embedded using a pretrained video-language model. These phrase embeddings are aligned to the step slots predicted by StepFormer using an order-aware loss function that handles noise and enforces temporal order. At inference time, StepFormer outputs step slots conditioned only on the input video, and the slots are aligned to the video frames using sequence alignment to localize the discovered steps temporally. Experiments show StepFormer outperforms previous weakly- and un-supervised methods for step localization across multiple datasets, without any dataset-specific tuning. It also demonstrates an emergent capability for zero-shot step localization from text prompts. The key innovations are the transformer architecture for step discovery, use of subtitles for self-supervision, and explicitly modeling step order for training and inference.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the paper:

The paper proposes StepFormer, a self-supervised model for discovering and localizing key instruction steps in long, untrimmed instructional videos. Instructional videos often contain irrelevant segments, so detecting the key steps is an important problem. StepFormer takes a video as input and outputs an ordered sequence of step slots that capture the instruction steps in the video. It is trained on a large unlabeled dataset of instructional videos, using the automatically generated subtitles as the only supervision signal. Specifically, StepFormer uses a transformer decoder architecture with learnable queries that attend to informative parts of the video. The queries are trained with an order-aware loss that aligns them to verb phrases extracted from the subtitles, while allowing irrelevant narrations to be ignored. 

At test time, StepFormer inputs a video and outputs step slots representing key instruction steps. These slots are aligned to the video frames using sequence alignment to temporally localize the steps in the video. Experiments show StepFormer significantly outperforms previous weakly supervised and unsupervised methods on step detection and localization across multiple benchmarks. It also demonstrates strong zero-shot localization ability when provided instruction text descriptions. Overall, StepFormer provides an effective self-supervised solution for discovering and localizing key steps in instructional videos without human supervision.


## Summarize the main method used in the paper in one paragraph.

 The paper proposes StepFormer, a self-supervised model for discovering and localizing instruction steps in videos. The key ideas are:

- StepFormer is implemented as a transformer decoder that takes a video as input and outputs an ordered sequence of step slots. The step slots are learned queries that attend to informative parts of the video.

- StepFormer is trained on a large dataset of instructional videos using the automatically generated subtitles as the only supervision. Specifically, verb phrases are extracted from the subtitles and embedded. 

- An order-aware alignment loss is used to match the step slots with relevant verb phrases, enforcing their temporal order. Irrelevant phrases are allowed to be dropped. A contrastive loss promotes similarity of matched slots and phrases.

- At inference, the predicted step slots are aligned to the input video using sequence alignment. This localizes the steps in the video, respecting their order.

The main novelty is the self-supervised training of the model using subtitles, and the use of explicit temporal ordering of steps for effective training and inference. Experiments show StepFormer outperforms previous weakly supervised and unsupervised methods for step localization.
