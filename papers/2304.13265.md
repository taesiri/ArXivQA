# [StepFormer: Self-supervised Step Discovery and Localization in   Instructional Videos](https://arxiv.org/abs/2304.13265)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the main research question seems to be: 

How can we develop a self-supervised model to discover and localize key instructional steps in long, untrimmed instructional videos, without relying on any human annotations or video-level labels during training?

The key points are:

- The goal is to develop a model for key-step discovery and localization in instructional videos. 

- The model should be trained in a completely self-supervised manner, without any human annotations or video-level labels.

- It should handle long, untrimmed instructional videos, where only a small portion may be relevant to the actual steps.

- The model should both discover the steps (i.e. determine what they are) and also temporally localize them in the video.

The main hypothesis seems to be that it is possible to train such a self-supervised model by extracting verb phrases from automatically generated video subtitles and using sequence-to-sequence alignment techniques to match steps and narrations. The proposed StepFormer model aims to demonstrate this capability.

In summary, the key research question is around developing an unsupervised model for instructional video understanding that can discover and localize steps without any human supervision, just using narrations. StepFormer is proposed as a solution.


## What is the main contribution of this paper?

 Based on my reading of the paper, the main contributions appear to be:

1. Proposing StepFormer, a novel self-supervised approach for key-step discovery and localization in instructional videos. StepFormer uses a transformer decoder architecture with learned queries to predict ordered step slots capturing the key steps in a video.

2. Explicitly modeling the temporal order of steps and using it to design effective training and inference procedures. This includes using an order-aware sequence-to-sequence loss to enforce temporal ordering of the predicted steps during training. The inferred step ordering is also used at test time for step localization via alignment.

3. Demonstrating that StepFormer can be trained in a completely self-supervised manner on a large dataset of instructional videos using only the automatically generated subtitles as supervision. The pre-trained model transfers well to downstream datasets, achieving state-of-the-art unsupervised step localization performance without any dataset-specific fine-tuning.

In summary, the main contribution appears to be proposing a new self-supervised transformer-based model, StepFormer, for unsupervised discovery and localization of instruction steps in videos. The method is shown to work well without relying on any manual annotations.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the key points from the paper:

The paper proposes StepFormer, a self-supervised transformer model that discovers and localizes instruction steps in long untrimmed videos using only automatically generated subtitles as supervision, and shows it achieves state-of-the-art performance on step localization across multiple datasets without any dataset-specific adaptation.


## How does this paper compare to other research in the same field?

 Here are my thoughts on how this paper compares to other research in the field of self-supervised instructional video understanding:

- The key contribution of this paper is presenting StepFormer, a novel self-supervised approach for discovering and localizing key instruction steps in long, untrimmed videos. This is an important goal in the field of instructional video understanding. 

- Most prior work on step localization requires some form of supervision, such as video-level labels or ordered/unordered step transcripts. StepFormer is completely unsupervised - it relies only on the videos and corresponding narrations for training. This makes it more scalable since it doesn't require manual annotation.

- Compared to prior unsupervised methods like Kukleva et al. and Elhamifar et al., StepFormer uses the accompanying narrations as supervision rather than just the video. This allows it to learn better step representations. The narrations provide a natural form of alignment supervision.

- Unlike other methods, StepFormer does not need to be trained on the same dataset used for evaluation. It can generalize to new datasets without finetuning. Prior unsupervised models are dataset-specific.

- StepFormer establishes new state-of-the-art results on unsupervised step localization across multiple benchmarks, outperforming prior unsupervised and weakly supervised methods.

- An interesting emergent capability is StepFormer's ability to perform zero-shot localization given step text descriptions at test time. This shows it learns representations that align well with both video and language.

In summary, the self-supervised approach, ability to transfer across datasets, and state-of-the-art results are the key strengths of StepFormer compared to related work on unsupervised instructional video understanding. The use of narrations as weak supervision and modeling of step order seem to be important to its success.
