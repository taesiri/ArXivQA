# NeuroLogic A*esque Decoding: Constrained Text Generation with Lookahead   Heuristics

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central hypothesis seems to be that existing decoding methods for neural text generation, which rely on maximizing sequence likelihood in a left-to-right fashion, are insufficient for generating high quality coherent text. The authors hypothesize that directly sampling from or maximizing sequence likelihood at each local step is an inadequate decoding heuristic, and can lead to issues like repetitive, incoherent or generic text.To address this, the paper proposes a new decoding approach called A* Heuristic Decoding, which incorporates estimates of future states to guide the local decoding decisions. The central hypothesis is that by looking ahead and leveraging information about plausible future states, the decoding process can be improved to generate higher quality and more coherent text while avoiding common pitfalls like repetition.In summary, the key hypothesis is that existing likelihood-based decoding heuristics are insufficient, and that explicitly modeling future states through a lookahead mechanism like the proposed A* Heuristic Decoding can lead to better text generation. The paper aims to demonstrate this through empirical evaluations across a range of text generation tasks.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contribution appears to be proposing a new decoding method called A* Heuristic for neural text generation. The key ideas are:- Motivated by limitations of standard decoding methods like beam search and sampling, which can lead to repetitive or incoherent text, the authors propose using estimates of future generation states to guide the current generation. - They draw inspiration from the A* search algorithm, which uses heuristics to guide search towards more promising results. The proposed A* Heuristic decoding method incorporates lightweight heuristics to approximate future losses and guide generation.- The heuristics are based on looking ahead by generating possible continuations of the partially decoded text so far. Different strategies are explored for efficiently generating good lookahead continuations.- The method can flexibly incorporate estimates of future costs like perplexity or constraint satisfaction. It's applied to both unconstrained text generation as well as constrained text generation using lexical constraints.- For constrained text generation, A* Heuristic is combined with NeuroLogic decoding to create an improved algorithm called A*-NeuroLogic. The lookahead heuristics help guide generation towards satisfying more constraints.- Experiments on 5 text generation benchmarks in both constrained and unconstrained settings show performance gains over competitive baselines brought by A* Heuristic decoding. The gains are notable especially in low-resource scenarios with limited supervision.In summary, the main contribution seems to be proposing a way to improve neural text generation by incorporating lightweight heuristics that look ahead to guide the search, inspired by A* search algorithms. Both unconstrained and constrained setups are considered, with strong empirical results.
