# [The Wisdom of Crowds: Temporal Progressive Attention for Early Action   Prediction](https://arxiv.org/abs/2204.13340)

## What is the central research question or hypothesis that this paper addresses?

 The central research question this paper addresses is how to effectively predict actions early from partially observed videos. The key ideas and hypotheses are:

- Modeling the temporal evolution of actions through progressively larger video scales is better suited for early prediction compared to using a single scale. 

- Utilizing multiple transformer towers, one per scale, to attend to fine-to-coarse representations can effectively model the ongoing action. 

- Aggregating predictions from these towers by considering their confidence and collective agreement can further improve early action prediction.

In summary, the main hypothesis is that a temporally progressive multi-scale representation combined with transformer towers and adaptive aggregation leads to improved performance on early action prediction from partially observed videos. The experiments demonstrate the effectiveness of this approach across datasets and backbones.


## What is the main contribution of this paper?

 The main contribution of this paper is proposing a temporally progressive attention model called TemPr for early action prediction. The key ideas are:

- Using multiple scales (fine to coarse temporal segments) of the partially observed video as input to capture the evolution of actions over different durations. 

- Employing multiple transformer attention towers, one for each scale, to model features and make predictions. Towers attend to features using a cross-attention bottleneck and stacked self-attention blocks.

- Aggregating predictions from the towers by considering both the agreement between tower predictions and their individual confidences. 

- Showcasing state-of-the-art performance on early action prediction across multiple datasets (UCF-101, Epic Kitchens, NTU-RGB, Something-Something) and encoder architectures.

In summary, the paper introduces a novel way of representing partial videos using progressive scales and modeling them with transformer towers for early action prediction. The tower predictions are aggregated adaptively based on agreement and confidence. This approach achieves strong results across diverse datasets compared to prior work.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here's a one sentence TL;DR of the paper:

The paper proposes a Temporally Progressive (TemPr) model for early action prediction that uses multiple attention towers to capture features at different temporal scales from partially observed videos and aggregates their predictions based on confidence and agreement.


## How does this paper compare to other research in the same field?

 Here are some key ways this paper compares to other research in the field of early action prediction:

- Uses progressive temporal sampling at multiple scales: This differs from most prior work that uses uniform sampling or focuses on a single temporal scale. Sampling at multiple fine-to-coarse scales allows the model to capture discriminative motion patterns over different durations.

- Employs multiple transformer towers: Rather than using a single model, this paper proposes an ensemble of transformer towers operating on the different temporal scales. This allows combining predictions in an adaptive way. Most prior work uses a single model. 

- Achieves state-of-the-art results: The proposed TemPr model with the multi-scale progressive sampling and tower ensemble achieves new state-of-the-art results across several datasets, outperforming recent methods.

- Evaluates on large-scale datasets: Many prior works evaluate early action prediction on small datasets of less than 100K videos. This paper additionally benchmarks on large datasets like Something-Something V2 and EPIC-Kitchens with 200K+ videos, demonstrating stronger generalizability.

- Detailed ablation studies: The paper provides extensive ablation studies analyzing the impact of different components like sampling strategies, tower designs, prediction aggregation, etc. This provides useful analysis and insights beyond just reporting end results.

Overall, the unique multi-scale progressive sampling and tower ensemble approach allows capturing subtle motions and patterns over different durations. The strong results across diverse datasets with detailed ablations demonstrate the effectiveness of the proposed method over existing research. The large-scale evaluation also shows the generalization capability.


## What future research directions do the authors suggest?

 Based on my reading of the paper, the main future research directions suggested by the authors are:

- Investigating other progressive sampling strategies beyond fine-to-coarse temporal scales. The authors propose a simple increasing scale strategy in this work, but other types of progressive sampling could be explored. 

- Applying the proposed temporal progressive attention approach to other video understanding tasks like action detection and anticipation. The authors demonstrate results on early action prediction, but the approach could potentially be beneficial in other tasks that deal with partial video observations.

- Exploring different aggregation functions for combining the predictions from the multiple attention towers. The adaptive confidence-based aggregation function is shown to work well, but there may be room for improvement here.

- Evaluating the approach on a wider range of video datasets spanning different complexities and action types. The authors demonstrate results on four datasets, but testing on more diverse data could further analyze the generalizability.

- Combining the temporal progressive attention model with complimentary approaches like instance-specific feature learning as in ERA. The authors show the temporal modeling helps, but integrating it with other techniques could further push state-of-the-art.

- Adapting the model for online prediction settings where latency and computations are constrained. The focus is offline prediction currently, but modifying for online use cases could be valuable.

- Analyzing the learned representations within and across the multiple attention towers. This could provide insights into what different temporal scales are capturing.

In summary, the main future directions revolve around exploring progressive sampling strategies, applying the approach to new tasks and datasets, integrating with complementary techniques, and analytically understanding the learned representations. The authors provide a solid set of initial experiments and results to build upon in future work.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the paper:

The paper proposes a new approach called TemPr (Temporal Progressive) for early action prediction from partially observed videos. The key idea is to sample the observed part of the video at multiple temporal scales, from fine to coarse, and perform prediction using attention-based towers on each scale. Specifically, the observed video is divided into multiple scales consisting of subsequences of increasing lengths. Each scale is encoded into a feature representation and passed to a separate transformer tower. The towers apply cross-attention and self-attention blocks to model features specific to their scale. A shared classifier then makes predictions per scale, which are aggregated into a final prediction based on their agreement and individual confidences. Experiments on four datasets - UCF101, EPIC-KITCHENS, NTU-RGB, and Something-Something demonstrate state-of-the-art performance. Ablations provide insights into the contributions of the components like progressive sampling, attention towers, and aggregation. The multi-scale temporal modeling allows capturing discriminative patterns in the observed video for early prediction.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the paper:

The paper proposes a new approach called Temporal Progressive Attention (TemPr) for early action prediction in videos. Early action prediction involves inferring the ongoing action label from only a partially observed video. The key idea is to represent the observed video using multiple fine-to-coarse temporal scales. For each scale, a transformer attention tower is used to model the features. The towers use a cross-attention block followed by self-attention blocks. This allows efficiently modeling the progression of features over time at each scale. The predictions from the towers are aggregated using a learnable weighted function that considers both the agreement between towers and their individual confidences. 

Experiments are conducted on four datasets - UCF101, EPIC-KITCHENS, NTU-RGB, and Something-Something. The proposed TemPr approach with 4 towers outperforms prior state-of-the-art methods across different encoders and datasets. Detailed ablation studies demonstrate the benefits of the progressive multi-scale sampling, attention towers, and aggregation function. The paper provides useful insights into modeling partial video observations for early action prediction. The proposed TemPr approach offers an effective way to capture discriminative spatio-temporal representations over fine-to-coarse scales.


## Summarize the main method used in the paper in one paragraph.

 The paper proposes a Temporal Progressive (TemPr) approach for early action prediction (EAP). The key idea is to represent the partially observed video using multiple temporal scales, from fine to coarse, and attend to features from each scale using separate transformer towers. 

Specifically, the observed video is sampled at n progressively increasing temporal scales. For each scale, a set of frames are randomly sampled and passed through a shared encoder to extract spatio-temporal features. These features are attended by a transformer tower consisting of a cross-attention block and stacked self-attention blocks. The cross-attention block uses a latent bottleneck array to reduce computation. Each tower makes predictions using a shared classifier. 

Finally, an aggregation function combines the predictions from all towers by considering both the agreement between tower predictions and the confidence of individual towers. This allows the model to leverage both fine details and longer context for early action prediction. Experiments on multiple datasets demonstrate improved accuracy compared to prior state-of-the-art methods.


## What problem or question is the paper addressing?

 The paper is addressing the problem of early action prediction (EAP) from partially observed videos. EAP aims to predict the action label of a video from only observing the initial portion of the video. This is challenging since the full action has not yet unfolded. 

The key question the paper tries to address is: how can we model and represent the observed partial video effectively for early action prediction?

Some key points:

- The paper proposes a new approach called Temporal Progressive (TemPr) attention to model the observed partial video using multiple temporal scales. 

- It uses transformer towers over different temporal scales (fine to coarse) to capture discriminative action features and patterns.

- The towers use a bottleneck cross-attention design to efficiently model the partial video features per scale.

- Predictions from the towers are aggregated based on their confidence and collective agreement.

- Experiments on 4 datasets demonstrate state-of-the-art performance over various backbones. Ablations verify the design choices.

In summary, the paper introduces a new way to model partial videos for early action prediction using progressive temporal attention, achieving improved performance over prior state-of-the-art approaches. The core focus is on effectively representing the partial observation for this task.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and keywords are:

- Early action prediction (EAP): The main task that the paper focuses on, which is predicting the ongoing action from a partially observed video. 

- Temporal progressive sampling: The proposed approach of sampling the observed video at multiple temporally increasing scales to represent it from fine to coarse.

- Attention towers: The transformer-based modules proposed, one per scale, to model the features of that scale.

- Adaptive aggregation: The proposed method to accumulate predictions from the individual attention towers based on their confidence and agreement. 

- Observation ratio: The ratio of observed frames vs total frames used to define a partially observed video in EAP.

- Bottleneck attention: The cross-attention block in each tower uses a latent bottleneck array for efficiency.

- Action recognition vs EAP: The paper differentiates the task of EAP from the more typical action recognition on fully observed videos.

- Multi-scale representations: Using multiple temporal scales is motivated by prior works that use scales for images or video recognition.

So in summary, the key terms revolve around early prediction, temporal progressive sampling, bottleneck attention towers, and adaptive aggregation of predictions. The task is differentiated from action recognition and related to prior multi-scale representations.
