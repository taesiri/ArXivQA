# Errors are Useful Prompts: Instruction Guided Task Programming with   Verifier-Assisted Iterative Prompting

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question appears to be:How can we develop a method to translate natural language instructions into executable robot task plans, especially for domain-specific languages with scarce training data?The key hypotheses tested in this work seem to be:1) Automated iterative prompting between a generator (large language model) and a verifier can improve the success rate of generating valid structured task plans from natural language, particularly for unfamiliar domain-specific languages. 2) The quality of the task plans generated by the proposed method (\ourmodel) will be better than existing baseline methods, as evaluated by expert comparisons.3) The task plans generated by \ourmodel can be successfully executed by a real robot when integrated with a task and motion planning framework, demonstrating the feasibility of translating natural language instructions to robotic actions.To summarize, the central research question focuses on developing a technique to translate natural language to executable robot plans for domain-specific languages, using iterative prompting and verification to handle data scarcity issues. The key hypotheses examine whether this approach can improve plan generation success, quality compared to baselines, and real-world feasibility when executed by a robot. The experiments and results provide evidence to support these hypotheses.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contribution appears to be proposing a novel framework called CLAIRify to generate structured task plans for domain-specific languages using natural language instructions. The key aspects of their contribution are:1. They propose combining a large language model (LLM) based generator with a rule-based verifier in an iterative loop to generate syntactically valid programs in unfamiliar domain-specific languages (DSLs). 2. The LLM generator leverages the in-context learning ability of LLMs by providing a description of the target DSL as part of the prompt. This allows generating structured outputs for unfamiliar DSLs in a zero-shot manner.3. The rule-based verifier checks the output of the LLM generator for syntactic correctness based on the rules of the DSL. It provides feedback on any errors back to the generator to correct errors in the next iteration. 4. This iterative prompting between generator and verifier ensures the final output adheres to the syntax of the target DSL.5. They demonstrate the effectiveness of their proposed framework CLAIRify on generating plans in a chemistry DSL called XDL. It outperforms prior XDL generation methods.6. They show the generated XDL plans can be executed on a real robot by integrating them with a task and motion planning framework.In summary, the key contribution is proposing an iterative prompting technique between an LLM generator and rule-based verifier to generate syntactically valid programs in unfamiliar DSLs from natural language in a zero-shot manner. The effectiveness is demonstrated for chemistry experiment planning.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key point from the paper:The paper proposes a novel approach called CLAIRify that combines automatic iterative prompting with program verification to generate syntactically valid robot task plans from natural language instructions for unfamiliar domain-specific languages.


## How does this paper compare to other research in the same field?

Here are a few thoughts on how this paper compares to other research in the same field:- The paper introduces a new method called CLAIRify that combines iterative prompting of a large language model with a verifier to generate structured task plans from natural language instructions. This approach of iterative prompting between a generator and verifier is novel compared to other methods that rely solely on large language models or other techniques.- The paper demonstrates CLAIRify on the task of generating plans in the chemistry domain, specifically using the Chemical Description Language (XDL). Most prior work has focused on generating programs or plans in more common languages like Python. Applying these techniques to domain-specific languages with less training data is an important contribution.- The paper shows CLAIRify can outperform prior XDL generation methods like SynthReader that use rule-based approaches. This helps demonstrate the power of iterative prompting with large language models compared to hand-crafted rules and patterns.- The paper integrates the generated XDL plans with a real robot system using a task and motion planning framework. Testing the plans on a physical system is a key validation step not present in much related work. - The idea of using a verifier to constrain and validate the outputs of a large language model aligns with other recent work like LEVER and Toolformer. However, CLAIRify's application to structured task planning and real-world robotics experiments helps advance research in this direction.Overall, the paper introduces a novel technique for structured task planning that combines the benefits of large language models and verifiers. The chemistry domain experiments and real robot demonstrations help validate the method in ways lacking in much prior work. The results suggest iterative prompting is a promising approach for generating executable plans from natural language.
