# Errors are Useful Prompts: Instruction Guided Task Programming with
  Verifier-Assisted Iterative Prompting

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question appears to be:How can we develop a method to translate natural language instructions into executable robot task plans, especially for domain-specific languages with scarce training data?The key hypotheses tested in this work seem to be:1) Automated iterative prompting between a generator (large language model) and a verifier can improve the success rate of generating valid structured task plans from natural language, particularly for unfamiliar domain-specific languages. 2) The quality of the task plans generated by the proposed method (\ourmodel) will be better than existing baseline methods, as evaluated by expert comparisons.3) The task plans generated by \ourmodel can be successfully executed by a real robot when integrated with a task and motion planning framework, demonstrating the feasibility of translating natural language instructions to robotic actions.To summarize, the central research question focuses on developing a technique to translate natural language to executable robot plans for domain-specific languages, using iterative prompting and verification to handle data scarcity issues. The key hypotheses examine whether this approach can improve plan generation success, quality compared to baselines, and real-world feasibility when executed by a robot. The experiments and results provide evidence to support these hypotheses.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contribution appears to be proposing a novel framework called CLAIRify to generate structured task plans for domain-specific languages using natural language instructions. The key aspects of their contribution are:1. They propose combining a large language model (LLM) based generator with a rule-based verifier in an iterative loop to generate syntactically valid programs in unfamiliar domain-specific languages (DSLs). 2. The LLM generator leverages the in-context learning ability of LLMs by providing a description of the target DSL as part of the prompt. This allows generating structured outputs for unfamiliar DSLs in a zero-shot manner.3. The rule-based verifier checks the output of the LLM generator for syntactic correctness based on the rules of the DSL. It provides feedback on any errors back to the generator to correct errors in the next iteration. 4. This iterative prompting between generator and verifier ensures the final output adheres to the syntax of the target DSL.5. They demonstrate the effectiveness of their proposed framework CLAIRify on generating plans in a chemistry DSL called XDL. It outperforms prior XDL generation methods.6. They show the generated XDL plans can be executed on a real robot by integrating them with a task and motion planning framework.In summary, the key contribution is proposing an iterative prompting technique between an LLM generator and rule-based verifier to generate syntactically valid programs in unfamiliar DSLs from natural language in a zero-shot manner. The effectiveness is demonstrated for chemistry experiment planning.
