# Multimodal Grounding for Embodied AI via Augmented Reality Headsets for
  Natural Language Driven Task Planning

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be:How can multimodal grounding between an embodied AI agent and a human operator be achieved via augmented reality headsets to enable natural language driven task planning for industrial applications?The key hypotheses appear to be:1) An augmented reality headset can be used as an effective interface to mediate multimodal information exchange (visual, speech inputs and outputs) between an embodied AI agent and a human operator. 2) Large language models like GPT-3 can be adapted via prompting techniques to ground natural language commands from the human operator into executable robot actions defined in a platform-independent format like UMRF.3) This multimodal grounding approach enables intuitive human-robot teaming for industrial inspection and manipulation tasks, with the human providing high-level instructions and the robot executing the lower-level actions.4) Prompt engineering for the large language models is a fragile process, requiring careful design and analysis to develop robust prompts that can generalize well.In summary, the central research direction is using augmented reality headsets and prompting of large language models to achieve natural language driven task planning for human-robot teams, with a focus on industrial applications. The key hypotheses relate to demonstrating the feasibility of this approach and characterizing the prompt engineering challenges.
