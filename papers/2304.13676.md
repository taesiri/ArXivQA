# Multimodal Grounding for Embodied AI via Augmented Reality Headsets for   Natural Language Driven Task Planning

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be:How can multimodal grounding between an embodied AI agent and a human operator be achieved via augmented reality headsets to enable natural language driven task planning for industrial applications?The key hypotheses appear to be:1) An augmented reality headset can be used as an effective interface to mediate multimodal information exchange (visual, speech inputs and outputs) between an embodied AI agent and a human operator. 2) Large language models like GPT-3 can be adapted via prompting techniques to ground natural language commands from the human operator into executable robot actions defined in a platform-independent format like UMRF.3) This multimodal grounding approach enables intuitive human-robot teaming for industrial inspection and manipulation tasks, with the human providing high-level instructions and the robot executing the lower-level actions.4) Prompt engineering for the large language models is a fragile process, requiring careful design and analysis to develop robust prompts that can generalize well.In summary, the central research direction is using augmented reality headsets and prompting of large language models to achieve natural language driven task planning for human-robot teams, with a focus on industrial applications. The key hypotheses relate to demonstrating the feasibility of this approach and characterizing the prompt engineering challenges.


## What is the main contribution of this paper?

The main contributions of this paper are:1. Successful demonstration of utilizing an Augmented Reality (AR) headset to mediate multimodal information between an Embodied AI agent and a human operator for industrial inspection tasks. 2. Novel application of Embodied AI to industrial domains and use of AR headset for multimodal grounding. 3. Quantitative and qualitative analysis on prompt design for Embodied AI agents, highlighting potential fragility issues. 4. Discussion on merits and challenges of adopting Embodied AI agents for multimodal task planning.Specifically, the authors show how an AR headset can be used to capture natural language commands and virtual markers from a human operator, and pass this multimodal information to prompt an Embodied AI agent based on GPT-3 to generate robot commands. They demonstrate this approach allows intuitive human-robot teaming for inspection tasks, with the human providing high-level instructions and the robot executing them autonomously.The analysis on prompt design reveals issues with fragility - small changes to prompts can significantly impact performance, posing challenges for real-world deployment. Overall, the paper makes valuable contributions in exploring Embodied AI for industrial settings using AR headsets for multimodal grounding, providing both a successful demonstration and an analysis of the limitations of current methods.
