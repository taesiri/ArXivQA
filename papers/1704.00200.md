# Towards Building Large Scale Multimodal Domain-Aware Conversation   Systems

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the main research focus is on creating and introducing a new large-scale dataset for multimodal dialog systems research, particularly in the retail/e-commerce domain. The key hypotheses or claims appear to be:- Multimodal dialog systems (involving both text and images) are becoming increasingly important for domains like retail, travel, etc. However, progress in deep learning research for this area has been hindered by the lack of large-scale conversational datasets. - The authors have created a new dataset called MMD (Multimodal Dialogs) to address this need, consisting of over 150K dialog sessions in the fashion retail domain.- The dataset enables research on several key capabilities needed for multimodal dialog systems, such as generating text responses, retrieving relevant images, employing domain knowledge, and modeling different user behaviors. - The authors propose baseline neural encoder-decoder models for two key tasks on this dataset - text response generation and best image response selection. The results demonstrate the feasibility of these tasks and provide a starting point for further research.- By releasing this dataset and defining tasks/metrics, the authors aim to spur more research into multimodal dialog systems, especially on the challenges highlighted by the baseline results on this dataset.In summary, the main hypothesis is that creating this large multimodal dialog dataset will open up new research avenues for data-driven deep learning approaches for multimodal conversational AI. The baseline results support the viability of the dataset for this purpose.


## What is the main contribution of this paper?

The main contributions of this paper are:- It introduces a new task of multimodal, domain-aware conversations between two agents involving both text and images. This is more complex than previous visual dialog tasks that just involve questions and answers about a single image. - It proposes a new large-scale dataset called the Multimodal Dialogs (MMD) dataset for this task. The dataset has over 150K dialog sessions between shoppers and sales agents in the fashion retail domain. It was created through a semi-automated process with extensive involvement of fashion domain experts.- It defines 5 new sub-tasks for evaluating multimodal conversation systems: text response generation, image response retrieval/generation, leveraging domain knowledge, and user modeling. - It proposes baseline neural encoder-decoder models for two sub-tasks - text response generation and best image response selection. Experiments demonstrate the feasibility of these tasks on the new dataset.- It analyzes the performance of the models on different dialog states and highlights challenges that can drive further research, such as the need for better multimodal fusion and attention mechanisms.In summary, the key contribution is the new multimodal dialog dataset and suite of tasks defined on it, which can enable further research into multimodal conversational agents. The neural baselines help establish performance on these new tasks.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper introduces a new large-scale multimodal dataset called MMD for building conversation agents that can seamlessly use both text and images, proposes tasks and baselines using this dataset, and highlights challenges that can drive future research in multimodal conversational AI.
