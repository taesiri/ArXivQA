# [Towards Building Large Scale Multimodal Domain-Aware Conversation   Systems](https://arxiv.org/abs/1704.00200)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the main research focus is on creating and introducing a new large-scale dataset for multimodal dialog systems research, particularly in the retail/e-commerce domain. The key hypotheses or claims appear to be:- Multimodal dialog systems (involving both text and images) are becoming increasingly important for domains like retail, travel, etc. However, progress in deep learning research for this area has been hindered by the lack of large-scale conversational datasets. - The authors have created a new dataset called MMD (Multimodal Dialogs) to address this need, consisting of over 150K dialog sessions in the fashion retail domain.- The dataset enables research on several key capabilities needed for multimodal dialog systems, such as generating text responses, retrieving relevant images, employing domain knowledge, and modeling different user behaviors. - The authors propose baseline neural encoder-decoder models for two key tasks on this dataset - text response generation and best image response selection. The results demonstrate the feasibility of these tasks and provide a starting point for further research.- By releasing this dataset and defining tasks/metrics, the authors aim to spur more research into multimodal dialog systems, especially on the challenges highlighted by the baseline results on this dataset.In summary, the main hypothesis is that creating this large multimodal dialog dataset will open up new research avenues for data-driven deep learning approaches for multimodal conversational AI. The baseline results support the viability of the dataset for this purpose.


## What is the main contribution of this paper?

The main contributions of this paper are:- It introduces a new task of multimodal, domain-aware conversations between two agents involving both text and images. This is more complex than previous visual dialog tasks that just involve questions and answers about a single image. - It proposes a new large-scale dataset called the Multimodal Dialogs (MMD) dataset for this task. The dataset has over 150K dialog sessions between shoppers and sales agents in the fashion retail domain. It was created through a semi-automated process with extensive involvement of fashion domain experts.- It defines 5 new sub-tasks for evaluating multimodal conversation systems: text response generation, image response retrieval/generation, leveraging domain knowledge, and user modeling. - It proposes baseline neural encoder-decoder models for two sub-tasks - text response generation and best image response selection. Experiments demonstrate the feasibility of these tasks on the new dataset.- It analyzes the performance of the models on different dialog states and highlights challenges that can drive further research, such as the need for better multimodal fusion and attention mechanisms.In summary, the key contribution is the new multimodal dialog dataset and suite of tasks defined on it, which can enable further research into multimodal conversational agents. The neural baselines help establish performance on these new tasks.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper introduces a new large-scale multimodal dataset called MMD for building conversation agents that can seamlessly use both text and images, proposes tasks and baselines using this dataset, and highlights challenges that can drive future research in multimodal conversational AI.


## How does this paper compare to other research in the same field?

Here are a few key ways this paper on multimodal dialogs compares to other related work:- Datasets: The paper introduces a new large-scale dataset of over 150K multimodal dialogs in the retail/fashion domain. This is much larger and more complex than existing multimodal dialog datasets like VisDial which focus on dialogs about a single image. - Tasks: The paper proposes several new tasks like multimodal response generation and selection. Existing work has mostly focused on visual question answering with dialogs, not general multimodal conversations.- Models: The paper presents baseline encoder-decoder models for text and image response tasks. Other work has not really explored neural models for full multimodal dialog systems before.- Domain knowledge: One unique aspect is the incorporation of structured domain knowledge into the dialog models. Most prior work uses generic dialog models without any domain grounding.- Semi-automated curation: The dataset was collected via a semi-automated process with extensive fashion expert involvement. This differs from fully automated simulation or crowdsourcing based approaches.Overall, the scale and complexity of the dataset, the more realistic task formulations, and the focus on domain-aware multimodal dialog agents distinguish this work from prior research. The new models and empirical analysis also advance the state of the art, though there is scope for improvement as noted in the paper.


## What future research directions do the authors suggest?

Based on my reading of the paper, the main future research directions suggested by the authors are:- Developing better multimodal attention models. The authors found that adding attention did not improve performance in their experiments. They suggest the need for more advanced attention mechanisms that can effectively leverage both text and images. - Improving performance on text response generation for harder dialog states like "give-image-description" that require deeper reasoning and integration of visual features and domain knowledge. The authors' models had poor performance on such states.- Scaling up image retrieval and ranking to handle selecting the best response from a large corpus of images, rather than just a small set of candidates. The authors show performance degrades significantly as the number of candidate images increases.- Developing better models of user behavior and preferences to enable more natural conversational interactions. The authors suggest user modeling as an important direction for future work.- Evaluating performance on individual dialog states to better understand challenges involved in different conversational scenarios and drive focused research. The authors provide some initial analysis along these lines.- Leveraging the structured domain knowledge and full product catalog more effectively. The authors' models used limited context, but the full dataset provides an opportunity to exploit richer background knowledge.- Exploring the other sub-tasks like employing domain knowledge and multimodal response generation using techniques like conditional GANs. The authors establish initial baselines on a subset of the tasks.In summary, the authors point to many opportunities for advancing multimodal dialogue agents by improving modelling of language, vision and reasoning over this new benchmark dataset. Their work establishes some initial baselines but substantial progress is possible on all the tasks they define.


## Summarize the paper in one paragraph.

Here is a one paragraph summary of the paper:The paper introduces a new large-scale dataset called Multimodal Dialogs (MMD) for multimodal dialogue research. The dataset contains over 150K conversations between shoppers and sales agents in the fashion retail domain, with both the dialog context and responses containing text and images. The dialogs were created through a semi-automated process involving extensive coordination with fashion domain experts to ensure realism. The paper proposes 5 sub-tasks that can be evaluated using this dataset - text response generation, image response retrieval/generation, leveraging domain knowledge, and user modeling. Baseline neural encoder-decoder models are presented for the text response and image response retrieval tasks. The models encode the multimodal dialog context hierarchically and attend to this while decoding the response. Experiments show the feasibility of these tasks on the dataset, while also highlighting scope for improvement. Overall, the MMD dataset along with the baseline models and proposed sub-tasks open up new research avenues into multimodal conversation systems.


## Summarize the paper in two paragraphs.

Here is a two paragraph summary of the paper:This paper introduces a new large-scale dataset called the Multimodal Dialogs (MMD) dataset for multimodal conversational agents. The dataset contains over 150K dialog sessions between shoppers and sales agents in the fashion retail domain, with both the dialog history and responses containing a mix of text and images. The authors worked closely with 20 domain experts to design realistic conversation flows and states that are seen in real-world multimodal conversations. The dataset enables research on several key tasks for multimodal dialog systems, including generating text responses, selecting the best image response, and leveraging both dialog context and structured domain knowledge. The paper proposes neural encoder-decoder models as baselines for two key tasks - generating a textual response and selecting the best image response. Experiments demonstrate the feasibility of these tasks on the new dataset and that incorporating multimodal context improves performance over just using text. The authors also evaluate model performance on different dialog states and for retrieving from larger candidate image sets, revealing challenges that suggest avenues for future research. Overall, the new large-scale MMD dataset and proposal of several multimodal conversation tasks open up new research directions in this area.


## Summarize the main method used in the paper in one paragraph.

Here is a one paragraph summary of the main method used in the paper:The paper introduces a new large-scale dataset called the Multimodal Dialogs (MMD) dataset for multimodal conversation research. The dataset was created through a semi-automated process involving extensive collaboration with domain experts in the fashion retail industry. The experts provided insights into typical conversation flows and key dialog states in fashion sales conversations. Based on their inputs, the authors designed an expert model automata with 84 states capturing different conversation intents. For each state, they created representative utterance patterns involving text and images. The experts gave feedback on example dialogs generated by the automata, which was used to refine it. By iteratively improving the automata through expert feedback, the authors produced over 150K dialog sessions between shoppers and sales agents with 40 turns on average. To evaluate the feasibility of multimodal conversation tasks using this dataset, the authors propose neural encoder-decoder models for text response generation and best image response selection. The encoders learn representations of the multimodal dialog context, while the decoders generate relevant text or select the best image response. Experiments demonstrate the promise of these baseline models on the new tasks and datasets.
