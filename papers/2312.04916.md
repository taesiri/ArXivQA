# [EE-LLM: Large-Scale Training and Inference of Early-Exit Large Language   Models with 3D Parallelism](https://arxiv.org/abs/2312.04916)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a high-quality paragraph summarizing the key points of the paper:

This paper introduces EE-LLM, a system for scaling up training and inference of early-exit large language models (LLMs) using massive 3D parallelism. It allows adding flexible early-exit layers to Transformer models and training them at scale. The system implements a lightweight method to facilitate backpropagation of the aggregated early-exit losses through pipeline stages. It also leverages idle resources in the pipeline schedule to minimize training overhead of early exits. For inference, EE-LLM provides two approaches compatible with key-value caching: one based on caching recomputation, the other using a novel pipeline parallelism. Experiments show that EE-LLM achieves great training efficiency with negligible overhead. It also delivers outstanding inference speedup without compromising output quality or model capacity. The system aims to be a useful tool for further research and adoption of early-exit LLMs. Its source code is publicly available.
