# [Large-Scale Actionless Video Pre-Training via Discrete Diffusion for   Efficient Policy Learning](https://arxiv.org/abs/2402.14407)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem: Learning generalist embodied agents capable of completing a wide variety of tasks is challenging, primarily due to the scarcity of action-labeled robotic datasets. In contrast, there exists a vast amount of unlabeled human videos capturing intricate tasks and physical world interactions. The key research questions are how to effectively utilize these human videos to facilitate robot policy learning and how to bridge the domain gap between human demonstrations and robots.

Proposed Solution: The paper introduces a video-based policy learning framework called VPDD that utilizes a unified discrete diffusion model to combine self-supervised pre-training on human videos and policy fine-tuning on a small number of action-labeled robot videos. 

The key ideas are:
(1) Learn a shared visual representation for both human and robot videos using a VQ-VAE, reducing domain gaps. 
(2) Perform video prediction on the codebook tokens as a self-supervised pretext task to capture temporal dynamics and commonsense knowledge.
(3) Formulate video prediction and action prediction as conditional generative modeling problems within a unified discrete diffusion process.

During pre-training, the discrete diffusion model predicts future video tokens conditioned on observed video tokens. For fine-tuning, it learns to simultaneously generate future video and action tokens on robot videos in a few-shot manner. The predicted videos provide foresight to guide policy learning concentrated on action tokens.

Main Contributions:
(1) A new way to leverage unlabeled human videos to facilitate robot policy learning by video prediction.
(2) A unified framework combining discrete diffusion for self-supervised video pre-training and policy fine-tuning.
(3) State-of-the-art results on challenging embodied tasks surpassing prior arts, showcasing superior generalization ability.

In summary, the key innovation is the unified video-conditioned discrete diffusion model for bridging the gap between human videos and robot policy learning. Both extensive experiments and analyses validate the efficacy of this idea.
