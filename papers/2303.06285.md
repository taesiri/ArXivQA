# [DeltaEdit: Exploring Text-free Training for Text-Driven Image   Manipulation](https://arxiv.org/abs/2303.06285)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the key research goals of this paper are:1. To propose a novel framework named DeltaEdit for text-driven image manipulation that is easy to train and flexible to use in inference without requiring per text-prompt training or tuning. 2. To identify and leverage the delta image-text space of CLIP features, which is better semantically aligned compared to the original CLIP space. This enables using image features as pseudo text conditions during training.3. To design the DeltaEdit network to learn mappings from differences in CLIP image features to differences in StyleGAN embeddings during training. Then directly apply the mappings to differences in CLIP text features during inference to achieve manipulation.4. To demonstrate a text-free training manner for DeltaEdit that only relies on easily collected image data and does not require expensive text annotations. Once trained, it can generalize to various unseen text prompts during inference.5. To extensively evaluate DeltaEdit on different datasets and show it achieves strong performance in terms of quality, efficiency, and generalization ability compared to previous text-driven manipulation methods that rely on per text optimization or training.In summary, the key idea is to enable flexible text-driven image manipulation without needing expensive text data annotation or tuning by identifying and utilizing the better aligned delta space of CLIP. The DeltaEdit framework and text-free training approach are proposed to achieve this goal.
