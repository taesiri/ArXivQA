# [H$_2$O: Heavy-Hitter Oracle for Efficient Generative Inference of Large   Language Models](https://arxiv.org/abs/2306.14048)

## What is the central research question or hypothesis that this paper addresses?

This paper seems to investigate the phenomenon of "Heavy Hitters" (HH) in large language models (LLMs) and proposes an approach called "Heavy Hitter Oracle" (H2O) to leverage HH for more efficient generative inference in LLMs.The key research questions/hypotheses appear to be:- Do HH emerge naturally in LLMs and if so, what causes their emergence? The paper hypothesizes that HH emerge due to the frequent co-occurrence of certain tokens in the training data.- Are HH important for LLM functionality and performance? The paper hypothesizes that removing HH will significantly degrade model performance. - Can HH be exploited to improve efficiency of LLM inference without compromising accuracy? The paper proposes H2O, which selectively retains HH in the KV cache, as a method to achieve this.So in summary, the central goals seem to be:1) Understand the causes and importance of HH in LLMs2) Leverage insights about HH to design H2O for efficient LLM inference while maintaining accuracyThe key hypothesis is that retaining only HH in the KV cache can significantly reduce memory requirements and improve throughput, without degrading quality of generated text. The paper aims to validate this hypothesis empirically across various LLM architectures, tasks, and sequence lengths.


## What is the main contribution of this paper?

This paper introduces a new method called Heavy-Hitter Oracle (H$_2$O) to improve the efficiency of generative inference for large language models (LLMs). The key contributions are:1. It identifies an important phenomenon in LLMs called Heavy Hitters (H$_2$), which are tokens that contribute most to the attention scores during text generation. 2. It provides both empirical evidence and theoretical analysis showing that retaining the key-value cache (KV cache) of only the H$_2$ tokens and most recent tokens can significantly reduce the memory footprint and improve throughput, without compromising accuracy.3. It formulates the KV cache eviction problem as a dynamic submodular maximization problem and proves theoretical guarantees for the proposed greedy algorithm. 4. It implements the H$_2$O framework and demonstrates up to 29x higher throughput compared to leading inference systems like DeepSpeed and HuggingFace with comparable accuracy across various tasks and models.5. The framework has two components - a KV cache eviction policy based on H$_2$, and a model loading strategy that keeps H$_2$ parameters in GPU memory. This effectively reduces both the transient KV cache size and model parameter loading cost.In summary, the main contribution is an efficient and theoretically-grounded framework for generative inference of large language models, that exploits the heavy-hitter phenomenon to significantly improve throughput and reduce latency. The empirical results on various models and datasets demonstrate the effectiveness of this approach.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence TL;DR summary of the paper:The paper proposes a framework called Heavy-Hitter Oracle (H2O) that efficiently reduces the memory footprint of the key-value cache in large language model inference by retaining only the embeddings of influential tokens and recent tokens.


## How does this paper compare to other research in the same field?

Here are some key points on how this paper compares to other related work:- Most prior work on efficient inference for large language models has focused on model compression techniques like pruning and quantization to reduce the model size. This paper tackles a different bottleneck - the memory footprint of the key-value cache during generation. So it provides a novel perspective.- For reducing the key-value cache, some relevant techniques are sparse attention and prompt compression. However, as discussed in the introduction, directly applying them to pretrained LLMs often degrades accuracy. This paper shows a way to leverage heavy hitters to reduce the cache size while maintaining accuracy.- Theoretically, the paper formulates the key-value cache eviction problem as a variant of dynamic submodular maximization and provides analysis. This connects the problem to submodular optimization, which has not been done before. - Empirically, the paper demonstrates significant improvements in throughput and latency over state-of-the-art systems like DeepSpeed, HuggingFace Accelerate, and FlexGen across a range of models and tasks. The gains are much larger than prior work.- The concept of heavy hitters, which underlies the proposed method, is new and shown to be important for efficient inference. The comprehensive investigation into their emergence and properties is novel.- Overall, the paper makes both theoretical and empirical contributions to efficient inference for LLMs. It tackles the under-explored key-value cache bottleneck from a fresh perspective compared to prior work focused on model compression and system optimizations. The gains over strong baselines demonstrate the impact.In summary, the paper introduces a novel angle to efficient LLM inference, supported by both algorithmic insights and extensive experiments. It advances the state-of-the-art in this area through its unique approach.


## What future research directions do the authors suggest?

Based on my reading of the paper, some of the main future research directions suggested by the authors are:- Further exploring efficient ways to implement the KV cache in large language models, such as optimizing data structures and operations. The authors designed a simple but effective framework called H2O, but there is still room for more advanced algorithm design and engineering in this area. - Investigating the theoretical properties and guarantees of attention mechanisms and the KV cache in large language models. The authors provided an initial theoretical analysis by formulating it as a dynamic submodular maximization problem, but more work can be done to strengthen the theory.- Studying how to combine the KV cache techniques with other optimizations like model compression, efficient attention mechanisms, and system-level improvements to further push the efficiency limits of large language model deployment. The authors showed compatibility with quantization as an example.- Generalizing the KV cache framework to other domains beyond natural language processing where similar sequence transduction models are used, such as speech and vision. The idea of exploiting heavy hitters may transfer to recurrent models in other modalities.- Exploring the societal impacts of more efficient large language models, including issues around fairness, bias, misinformation, and resource usage. Making LLMs more deployable comes with ethical considerations.- Developing more comprehensive benchmarks and standardized metrics to systematically evaluate inference techniques for large language models, which can facilitate progress in this area.So in summary, the authors laid a solid foundation and there are many exciting avenues to build upon their work on optimizing the memory footprint and throughput of large language models using insights like heavy hitters. Both the theoretical and systems aspects can be expanded going forward.


## Summarize the paper in one paragraph.

Here is a one paragraph summary of the paper:The paper presents a LaTeX document defining an article formatted according to the NeurIPS 2023 guidelines. It includes the document class, packages for formatting, author and title definition, abstract, introduction section, and bibliography formatting. The document structure demonstrates how to format a NeurIPS paper, including formatting the title, authors, affiliations, abstract, sections, figures, tables, equations, captions, citations, and references according to the conference requirements. The LaTeX code provides a template that can be used to prepare papers for submission to the NeurIPS 2023 conference.


## Summarize the paper in two paragraphs.

Here is a two paragraph summary of the paper:The paper proposes a novel framework called Heavy-Hitter Oracle (H2O) to address the key bottleneck of maintaining a large transient Key-Value (KV) cache during the inference phase of large language models (LLMs). LLMs require substantial GPU memory to store the KV cache in addition to model parameters when generating long sequences. The paper first shows the attention matrices of LLMs are highly sparse at inference time. This suggests the potential to significantly reduce the KV cache size. Further, the paper discovers the existence of heavy-hitter tokens that contribute most of the value in attention scores. Based on this, H2O incorporates a KV cache policy that retains embeddings of only the heavy-hitter and most recent tokens. The paper validates H2O extensively on models like OPT, LLaMA, and GPT-NeoX across diverse tasks. Implementation in FlexGen shows H2O improves throughput by up to 29x and reduces latency by up to 1.9x over state-of-the-art systems like DeepSpeed and Hugging Face Accelerate. Theoretically, the KV cache eviction problem is formulated as a variant of dynamic submodular maximization. Under mild assumptions, the paper proves approximation guarantees for the proposed greedy algorithm. Overall, H2O offers an effective solution to accelerate inference for memory-intensive LLMs, particularly for long sequence generation.


## Summarize the main method used in the paper in one paragraph.

Here is a one paragraph summary of the main method used in the paper:The paper proposes a novel framework called Heavy Hitter Oracle (H2O) to reduce the memory footprint of the key-value (KV) cache during inference with large language models (LLMs). The key insight is that attention scores in LLMs follow a power-law distribution, where a small number of tokens termed "heavy hitters" contribute substantially to the overall attention scores. Based on this, H2O employs a simple KV cache eviction policy that retains embeddings only for heavy hitters and recent tokens, while evicting others. Specifically, at each decoding step, it computes attention scores to identify heavy hitters, adds the embedding of the newest token to the KV cache, and evicts the embedding of the token with the smallest accumulated attention score so far. Experiments on models like OPT and LLaMA demonstrate that H2O can reduce the KV cache size 5-10x with negligible accuracy drops, and improve throughput by up to 29x over leading inference systems. The key novelty is the identification and exploitation of heavy hitters for efficient KV cache management during LLM inference.
