# OpenViDial 2.0: A Larger-Scale, Open-Domain Dialogue Generation Dataset   with Visual Contexts

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the main goal is to introduce OpenViDial 2.0, which is a larger-scale open-domain multi-modal dialogue dataset compared to the previous version OpenViDial 1.0. Specifically, OpenViDial 2.0 contains 5.6 million dialogue turns paired with visual contexts extracted from movies/TV shows, which is around 4 times larger than OpenViDial 1.0. The authors hope this larger dataset can facilitate future research on open-domain multi-modal dialog generation, such as multi-modal pretraining for dialog models. In summary, the central hypothesis is that releasing a bigger multi-modal dialog dataset (OpenViDial 2.0) can benefit the research community working on open-domain dialog generation with visual contexts.


## What is the main contribution of this paper?

The main contribution of this paper is the release of OpenViDial 2.0, a large-scale open-domain multi-modal dialogue dataset. Specifically:- OpenViDial 2.0 contains 5.6 million dialogue turns paired with visual contexts extracted from movies and TV shows. This is 4 times larger than the previous OpenViDial 1.0 dataset.- Each dialogue turn consists of a textual utterance and a corresponding image representing the visual context. - The text is preprocessed using BPE tokenization. The images are in resolutions of 1280x720, 1920x1080 or 2048x1080.- Experiments using models from prior work show OpenViDial 2.0 can facilitate research on multi-modal open-domain dialog generation.In summary, the key contribution is the construction and release of OpenViDial 2.0 as a large-scale multi-modal dialog dataset to help drive further research in this area.
