# [Model-based Asynchronous Hyperparameter and Neural Architecture Search](https://arxiv.org/abs/2003.10865)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is: How can we develop an efficient model-based asynchronous multi-fidelity method for hyperparameter and neural architecture search that outperforms current state-of-the-art methods? The key points are:- The paper introduces a new model-based asynchronous approach called A-BOHB that combines the strengths of asynchronous Hyperband and Gaussian process-based Bayesian optimization for hyperparameter and architecture search. - A-BOHB uses a probabilistic model that can simultaneously reason across hyperparameters, architectures, and resource levels (e.g. training epochs) and supports decision-making with pending evaluations. - The paper shows A-BOHB can find high-quality solutions faster than synchronous BOHB, asynchronous Hyperband, and other methods on a range of benchmarks including optimizing MLPs, CNNs, LSTMs, and NAS architectures.- A-BOHB makes efficient use of parallel compute resources by avoiding synchronization overhead and idle times associated with synchronous methods like BOHB.- The gains are especially significant on expensive benchmarks where training runs can take varying amounts of time. In several cases, A-BOHB achieves competitive performance using half the parallel resources as asynchronous random search.So in summary, the key hypothesis is that the proposed asynchronous model-based approach A-BOHB will outperform current state-of-the-art methods for hyperparameter and architecture search by efficiently leveraging asynchrony, multi-fidelity modeling, and Bayesian optimization. The experiments aim to validate this hypothesis across various benchmarks.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contribution seems to be introducing a model-based asynchronous multi-fidelity method for hyperparameter and neural architecture search. The key aspects of their proposed method include:- Using a probabilistic model (Gaussian process) that can simultaneously reason across hyperparameters and resource levels (number of epochs). This allows exploiting correlations across different fidelities of the objective function.- Supporting asynchronous parallel search, where the model can handle pending/incomplete evaluations and make decisions accordingly. This avoids wasteful synchronization overhead. - Combining the strengths of asynchronous Hyperband (a multi-fidelity scheduling method) with Bayesian optimization based on the Gaussian process model. - Demonstrating effectiveness of their proposed asynchronous BOHB method compared to state-of-the-art approaches like Hyperband, Bayesian optimization, and others on a range of benchmark problems including neural architecture search, tabular data, and image classification.- Implementing their methods in a distributed framework that can leverage parallel resources efficiently.In summary, the main contribution seems to be a new model-based asynchronous multi-fidelity approach for hyperparameter and architecture search that can more efficiently utilize parallel resources compared to prior methods. The combination of multi-fidelity modeling and asynchronous parallel search allows it to outperform other state-of-the-art methods on challenging benchmark problems.


## How does this paper compare to other research in the same field?

Here are a few thoughts on how this paper compares to other related research:- This paper presents a novel model-based asynchronous method for hyperparameter and neural architecture search. Other recent work in this field has also explored asynchronous parallel Bayesian optimization (BO) methods, but not in combination with multi-fidelity optimization via Hyperband as done here.- Compared to synchronous BO methods like BOHB, this work shows that exploiting asynchronicity can lead to more efficient use of parallel compute resources and faster time to convergence. This aligns with findings from other asynchronous BO papers.- The proposed method is compared directly to asynchronous Hyperband (e.g. ASHA) and shows clear improvements from incorporating a probabilistic model to guide architecture/hyperparameter selections. This demonstrates the value of model-based optimization over pure random search in this setting.- For neural architecture search, this paper shows competitive results on NAS benchmarks compared to state-of-the-art methods like regularized evolution. The focus here is more on efficient methodology rather than pushing accuracy.- The experiments cover a broad range of problem types - tabular data, image classification, language modeling. This helps demonstrate the general utility of the proposed techniques.- The code for the method and experiments is made publicly available, which facilitates reproducibility and future research progress in this area.Overall, this paper makes nice contributions in introducing a novel asynchronous model-based optimization approach for neural architecture and hyperparameter search. It is supported by strong experimental results across various domains compared to reasonable baselines. The work fits well within the growing research interest in developing more efficient search methods in this area.


## What future research directions do the authors suggest?

Based on my review of the paper, some of the main future research directions suggested by the authors include:- Developing more sophisticated acquisition functions for Bayesian optimization that can better balance exploration and exploitation in the asynchronous parallel setting. The authors note that their approach of optimizing expected improvement is rather myopic, and does not take into account the downstream impact of choosing a configuration on future stopping/promotion decisions. They suggest exploring more non-myopic acquisition functions.- Investigating more principled ways of making model-based scheduling decisions, beyond just successive halving. The authors propose going beyond the simple promotion/stopping rules of Hyperband and leveraging their probabilistic surrogate model to make more informed scheduling choices.- Researching advanced bracket sampling strategies and rung specifications. The authors note that their method of sampling brackets from Hyperband's distribution may not be optimal and can likely be improved. Additionally, the choice of rung levels could be adaptively set rather than fixed a priori.- Comparing against a wider range of baselines, especially some recent asynchronous Bayesian optimization methods. The authors note they were unable to compare against some existing methods like Freeze-Thaw Bayesian optimization.- Analyzing the performance on an even broader range of benchmark problems, including additional dataset domains and neural architecture types.- Open-sourcing their distributed HNAS system to spur follow-up research and real-world usage. The authors plan to release their implementation of the various asynchronous algorithms compared.In summary, the main future directions focus on developing more advanced acquisition functions, scheduling policies, bracket sampling strategies, and benchmark evaluations to further improve the efficiency and effectiveness of asynchronous parallel Bayesian optimization for neural architecture and hyperparameter search.


## Summarize the paper in one paragraph.

Here is a one paragraph summary of the paper:The paper introduces a model-based asynchronous multi-fidelity method for hyperparameter and neural architecture search that combines the strengths of asynchronous Hyperband and Gaussian process-based Bayesian optimization. The method uses a probabilistic model that can simultaneously reason across hyperparameters and resource levels, and supports decision-making in the presence of pending evaluations. It is evaluated on a range of challenging benchmarks for tabular data, image classification and language modelling, and demonstrates substantial speed-ups over current state-of-the-art methods. The proposed techniques along with asynchronous baselines are implemented in a distributed framework that will be open sourced. Key contributions include clarifying differences between existing asynchronous Hyperband variants, introducing the new model-based extension, and empirically showing that it can achieve competitive performance using half the computational resources compared to sampling uniformly at random.


## Summarize the paper in two paragraphs.

Here is a two paragraph summary of the paper:This paper introduces a model-based asynchronous multi-fidelity method for hyperparameter and neural architecture search that combines the strengths of asynchronous Hyperband and Gaussian process-based Bayesian optimization. The method uses a probabilistic model that can simultaneously reason across hyperparameters and resource levels (e.g. training epochs), and supports decision-making in the presence of pending evaluations. The key contributions are: 1) clarifying differences between existing asynchronous Hyperband extensions, in particular the stopping and promotion variants, 2) introducing a new joint Gaussian process model that handles multi-fidelity and asynchrony, 3) demonstrating that sampling from the model often achieves the same performance with half the computational resources compared to random sampling, and 4) showing on neural network benchmarks that the proposed asynchronous BOHB method is more efficient in terms of wall-clock time than other state-of-the-art algorithms like synchronous BOHB. The benefits come from exploiting low-fidelity approximations and asynchronous parallel scheduling.


## Summarize the main method used in the paper in one paragraph.

Here is a one paragraph summary of the main method used in the paper:The paper introduces a model-based asynchronous multi-fidelity method for hyperparameter and neural architecture search that combines strengths of asynchronous Hyperband and Gaussian process-based Bayesian optimization. The method uses a probabilistic model that can simultaneously reason across hyperparameters and resource levels (e.g. training epochs) and supports decision-making in the presence of pending evaluations. Specifically, it employs a joint Gaussian process surrogate model over configurations and resource levels that captures correlations between low and high fidelity evaluations. The Gaussian process is used to guide the search by selecting promising configurations to evaluate based on an acquisition function. Asynchronous parallel scheduling of configurations is handled by fantasizing pending outcomes in the Gaussian process and making continue/stop decisions independently for each worker. Compared to synchronous methods like Hyperband, the asynchronous approach reduces idle time and speeds up promotions to higher fidelities. Compared to asynchronous Hyperband, the model-based search focuses evaluations in more promising regions to accelerate convergence.
