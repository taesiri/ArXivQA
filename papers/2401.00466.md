# [Online Symbolic Music Alignment with Offline Reinforcement Learning](https://arxiv.org/abs/2401.00466)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper "Online Symbolic Music Alignment with Offline Reinforcement Learning":

Problem:
The paper addresses the problem of symbolic music alignment, which involves matching performed MIDI notes to corresponding score notes encoded in MusicXML. Two categories are online alignment, where only the current performance context is available, versus offline alignment where the full performance is known. Online alignment is useful for real-time score following applications. Prior work in these areas has limitations in accuracy or efficiency.

Proposed Solution:
The paper proposes two main solutions:

1) An offline alignment method using a two-step Dynamic Time Warping (DTW) approach, first aligning based only on pitch, then aligning onsets. This achieves state-of-the-art accuracy.

2) An online alignment method using reinforcement learning (RL). An RL agent is trained offline to predict the current score position given limited context. This agent drives an overall online alignment model that also incorporates tempo estimation.

Main Contributions:
- Two-step DTW method for highly accurate offline alignment, improving over prior work
- Novel formulation of online alignment as an RL problem, using offline training
- RL agent that accurately predicts score positions 
- Complete online alignment model using RL agent combined with tempo estimation
- Thorough evaluation showing state-of-the-art accuracy for offline alignment, and strong results for online alignment/score following

The key novelty is the use of modern machine learning (RL and Transformer networks) for symbolic music alignment in both online and offline settings. The offline RL formulation enables stable training. Both proposed solutions outperform relevant prior work, advancing the state-of-the-art.
