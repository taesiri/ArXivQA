# [Emerging Properties in Self-Supervised Vision Transformers](https://arxiv.org/abs/2104.14294)

## What is the central research question or hypothesis that this paper addresses?

This paper presents a self-supervised learning method called DINO (which stands for Distillation with No Labels) and studies its application to Vision Transformer (ViT) models. The central hypothesis is that self-supervised pre-training can provide unique benefits to ViT models compared to convolutional neural networks (CNNs), such as:- The self-attention layers in ViT can learn to segment objects in images, without any pixel-level supervision. - The global image features from ViT pretrained with DINO perform very well on nearest neighbor retrieval, outperforming CNNs.- ViT models pretrained with DINO transfer better to downstream tasks compared to supervised pretraining, unlike CNNs where supervised pretraining transfers better.In summary, the central hypothesis is that self-supervised pretraining with DINO provides complementary benefits to ViT models that are not realized with CNNs or supervised pretraining, enabling ViTs to learn implicit spatial/structural information about images. The paper explores this through extensive experimentation and analysis.


## What is the main contribution of this paper?

This paper presents a simple and effective self-supervised learning method called DINO. The main contributions are:- DINO formulates self-supervised learning as a knowledge distillation problem without labels, where a student network is trained to match the output of a teacher network. The teacher is dynamically built using a momentum encoder on the student. - DINO shows strong synergy between self-distillation and Vision Transformers (ViT). It achieves state-of-the-art ImageNet classification accuracy among self-supervised methods when applied to ViT.- The features learned by DINO exhibit some unique properties compared to supervised ViTs and convolutional networks:    - They contain explicit object segmentation information in the self-attention maps.    - They are very effective for nearest neighbor retrieval without finetuning.- DINO simplifies previous self-supervised approaches by showing centering and sharpening the teacher outputs is sufficient to avoid collapse, without needing other techniques like contrastive loss, predictor networks, or advanced normalizations.- DINO can work well across different architectures like ViT and ResNets, batch sizes, and train lengths, highlighting its flexibility.In summary, the main contribution is presenting a simple and flexible self-supervised distillation method that unlocks excellent performance when combined with Vision Transformers, and results in features with unique properties like built-in segmentation maps and nearest neighbor retrieval ability.
