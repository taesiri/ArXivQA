# [Less Is More -- On the Importance of Sparsification for Transformers and   Graph Neural Networks for TSP](https://arxiv.org/abs/2403.17159)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
Most prior work using graph neural networks (GNNs) or transformers to encode traveling salesman problem (TSP) instances and produce node embeddings passes the complete, dense TSP graphs to the encoder architectures. However, dense graphs lead to information flooding in message passing based models like GNNs, resulting in similar, non-discriminative node embeddings. 

Proposed Solution: 
The paper proposes two data preprocessing methods to sparsify the TSP graphs before passing them to encoders: k-nearest neighbors (k-NN) and a minimum spanning tree based approach called 1-Tree. Sparsification allows the encoders to focus on the most promising connections only. An analysis shows 1-Tree retains more optimal TSP edges compared to k-NN in sparse graph representations, especially for non-uniform data. 

The paper incorporates the proposed sparsification as a preprocessing step for GNNs and via derived attention masks for transformers. Ensembles of encoders using multiple sparsification levels are also introduced to balance between focusing on most relevant parts while retaining overall connectivity.

Main Contributions:
- Two data preprocessing methods for TSP graphs that allow GNN/transformer encoders to focus on most promising parts of instances
- Empirical evaluation showing sparsification consistently improves performance of GAT and GCN encoders across settings
- Proposed ensembles further improve performance, reducing optimality gap significantly
- New state-of-the-art transformer encoder for TSP via 1-Tree based attention masking, reducing gap from 0.16% to 0.10% for n=100  

Overall, the paper demonstrates the efficacy of graph sparsification as a preprocessing technique to enable more powerful GNN and transformer encoders for learning combinatorial optimization problems like TSP.
