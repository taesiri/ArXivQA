# [Optimizing Likelihood-free Inference using Self-supervised Neural   Symmetry Embeddings](https://arxiv.org/abs/2312.07615)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Likelihood-free inference (LFI) is emerging as a powerful technique for fast and effective parameter estimation from simulations. However, LFI models tend to be complex and computationally expensive to train.

- Physical systems often have symmetries where some parameters are invariant to other intrinsic parameters of interest. For example, the time of arrival of a signal does not affect the inference of physical parameters that characterize the signal shape. 

- Conventional LFI is unaware of these symmetries and has to marginalize over invariant nuisance parameters. This necessitates larger networks and more training.

Proposed Solution:
- Utilize self-supervision to learn representations that are invariant to nuisance parameters like time shifts. This is done by using data augmentations and contrastive losses.

- Perform LFI using normalizing flows conditioned on the self-supervised representation instead of directly on the data. This allows the flow to focus only on the intrinsic physical parameters.

Key Contributions:
- Demonstrate a technique to optimize LFI by marginalizing symmetries like time-translation invariance using self-supervised learning. This allows for smaller networks and reduced compute for training.

- Apply the technique on two simple physical systems - a damped harmonic oscillator and a sine-gaussian pulse model. 

- Show posteriors obtained from normalizing flows with self-supervised conditioning are accurate and have significantly fewer parameters compared to flows conditioned directly on data.

- The technique can be generalized to marginalize nuisance parameters in other physical systems and optimize likelihood-free inference.

In summary, the key idea is to use self-supervision to build invariant representations of data which can then be exploited to design efficient amortized inference models like normalizing flows for parameter estimation. This has the potential to help scale likelihood-free inference.
