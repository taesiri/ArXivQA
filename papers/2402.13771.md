# [Mask-up: Investigating Biases in Face Re-identification for Masked Faces](https://arxiv.org/abs/2402.13771)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem Statement:
- Face recognition systems (FRSs) are widely used for surveillance, policing, etc. but have shown biases against marginalized groups. 
- Wearing masks became common after COVID-19, but masks occlude faces which impacts FRS accuracy. This can enable malicious use to hide identities.
- Prior work showed humans are also unreliable for face recognition tasks.

Objectives:
- Audit 13 FRSs (4 commercial, 9 open source) for masked face re-identification on 5 datasets with 14,722 images.
- Compare performance to 85 human participants on a subset of data.
- Analyze impact of mask type/color on accuracy and biases.
- Explain open source model predictions using activation maps.

Experiments:
- 1-to-1 reID between original and masked face images 
- 1-to-N reID between masked probe and multiple unmasked gallery images
- Online survey for humans to attempt 1-to-N reID on subset of data

Key Results:
- Commercial FRS AWS Rekognition performs best overall. Open source VGG-Face performs best.
- Accuracies vary significantly based on mask type/color. N95 masks are most challenging.
- Biases exist against minority groups, exacerbated for certain mask types.
- Humans achieve only 40% accuracy for 1-to-N reID, lower under time constraints. Still prone to biases.  
- Activation maps indicate masks shift facial regions models focus on, impacting accuracy.

Contributions:
- Most extensive audit of commercial and open source FRSs on masked face re-identification.
- Analysis of impact of various mask types/colors on accuracy and biases.
- Comparison of human vs. automated system performance.
- Insights into model limitations that could guide future work.

Takeaways: 
- Caution warranted when deploying FRSs for masked faces due to reliability and bias concerns.
- More robust and equitable models needed that account for mask occlusions.


## Summarize the paper in one sentence.

 Here is a one sentence summary of the paper:

This paper audits commercial and open-source automated face recognition systems for biases in masked face re-identification tasks, finding poor performance and biases exacerbated by mask type and color, questioning the suitability of human oversight, and calling for improved design principles that account for real-world conditions.


## What is the main contribution of this paper?

 Based on my reading, the main contributions of this paper are:

1) It audits 13 face recognition systems (4 commercial, 9 open-source) on the task of face re-identification between masked and unmasked faces using 5 benchmark datasets. This evaluates the robustness and biases of these systems in a realistic masked face scenario.

2) It studies the impact of different mask types (surgical, N95, cloth) and colors (blue, light skin-tone, dark skin-tone) on the accuracy and biases of face re-identification. This provides insights into how certain mask attributes can be more challenging for face recognition systems.

3) It analyzes the activation maps and attention regions of open-source face recognition models to understand why they may fail at re-identification under masking and occlusion. This provides clues into mitigating such biases.

4) It compares human performance to automated systems via a survey with 85 participants. This benchmark reveals that humans are also prone to biases, make more errors under time pressure, and are not scalable. Hence a human-in-the-loop approach has limitations over just using AI systems.

In summary, the paper demonstrates through comprehensive experiments that face re-identification of masked faces is non-trivial, prone to demographic biases, and needs cautions deployment in real-world applications like surveillance and access control systems.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key keywords and terms associated with this paper include:

- Face recognition systems (FRSs)
- Biases 
- Fair classification
- Masked face recognition
- Adversarial audits
- Intersectional groups
- 1-to-1 face re-identification 
- 1-to-N face re-identification
- Commercial FRS platforms (Face++, Amazon AWS Rekognition, Microsoft Azure Face, FaceX)
- Open-source FRS platforms (VGG-Face, FaceNet, OpenFace, DeepFace, etc)
- Mask types (surgical, N95, cloth)
- Grad-CAM activation maps
- Human-in-the-loop face recognition

The paper conducts an extensive audit of commercial and open-source face recognition systems on the task of re-identifying individuals between masked and unmasked face images. It looks at potential biases in these systems against intersectional groups based on gender, race, and mask type. Both 1-to-1 and 1-to-N re-identification tasks are evaluated. The paper also analyzes model explanations using Grad-CAM and compares automated systems with human performance.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 potential in-depth questions about the method proposed in this paper:

1. This paper performs an audit study of commercial and open-source face recognition systems. What are some of the key benefits and limitations of using an audit study methodology compared to other evaluation approaches?

2. The paper uses both 1:1 and 1:N face re-identification tasks during the audits. What is the key difference between these two tasks and why is 1:N more challenging? 

3. The paper evaluates performance on both unmasked and masked face images. What type of mask (surgical, N95, cloth) posed the biggest challenge for accuracy across the face recognition systems? Were there differences in how mask color impacted performance?

4. The paper argues that human reviewers cannot effectively serve as a check against biases in automated systems. What evidence from the study supports this claim? What are some alternatives the authors propose instead of using human reviewers?

5. How did the performance of commercial face recognition services like Amazon Rekognition and Microsoft Azure compare to open source alternatives like VGG-Face and OpenFace? Were there noticeable differences in accuracy or biases?

6. Were there noticeable disparities in accuracy across gender or racial groups? If so, which groups tended to have lower accuracy across the services evaluated? 

7. The paper utilizes activation maps to provide explanations for why open source models fail at identification. What key observations did the authors make based on these activation maps? How could this information be used to create more robust models?

8. What differences did the authors observe between the institutional participants and the Amazon Mechanical Turk participants in the human subject experiments? What explanations are provided?

9. How did imposing a time deadline impact human performance at the face re-identification task compared to experiments without a deadline? What explanations do the authors provide?

10. This study focuses exclusively on face re-identification. How could the methodology be extended or adapted to audit face recognition systems on additional tasks like emotion recognition or demographic classification? What new challenges might arise?
