# [Where is my Wallet? Modeling Object Proposal Sets for Egocentric Visual   Query Localization](https://arxiv.org/abs/2211.10528)

## What is the central research question or hypothesis that this paper addresses?

 This paper focuses on tackling the challenging problem of egocentric visual query localization. The main research questions/hypotheses it addresses are:

1. Current query-conditioned models and datasets for visual query localization suffer from implicit biases that hurt performance. These biases include:

- Domain bias: Models are only trained on frames with well-posed objects in clear view, while real-world egocentric videos contain blurry and out-of-focus objects from uncommon view angles. 

- Task bias: Models are trained with the query object always present, but at test time it is mostly absent from frames. This leads to false positives.

2. These biases can be reduced by:

- Expanding limited annotations with unlabeled frames containing diverse viewpoints and backgrounds without the query object present.

- Dynamically dropping object proposals during training to simulate query object absence.

- Using a novel Conditioned Contextual Transformer module to incorporate global context from object proposal sets while conditioning on the query.

3. The proposed techniques will improve egocentric query detection, leading to better visual query localization in 2D and 3D.

In summary, the main hypotheses are that tackling the identified dataset/model biases will improve performance on this challenging visual query localization task, especially using the proposed transformer-based model architecture and data augmentation techniques. The experiments aim to validate these hypotheses.


## What is the main contribution of this paper?

 According to my understanding, the main contributions of this paper are:

1. It identifies and tackles implicit biases in current query-conditioned model design and visual query datasets, including domain bias and task bias. 

2. It proposes methods to expand limited annotations and dynamically drop object proposals during training to solve the biases.

3. It develops a novel transformer-based module called Conditioned Contextual Transformer (\Method) that allows incorporating object-proposal set context while using the query information. 

4. It shows that the proposed adaptations can significantly improve egocentric query detection and visual query localization in both 2D and 3D configurations. 

5. The improved context-aware query object detector achieves state-of-the-art results, ranking 1st and 2nd in the VQ2D and VQ3D tasks in the 2nd Ego4D challenge.

6. It also demonstrates the relevance of the proposed model in few-shot detection, where it achieves new state-of-the-art results.

In summary, the main contribution is developing techniques to tackle biases and limitations in existing methods for egocentric visual query localization, leading to a better query object detector and localization system. The core ideas include expanding annotations, balanced proposal set sampling, and the Conditioned Contextual Transformer module.
