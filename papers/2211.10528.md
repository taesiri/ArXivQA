# [Where is my Wallet? Modeling Object Proposal Sets for Egocentric Visual   Query Localization](https://arxiv.org/abs/2211.10528)

## What is the central research question or hypothesis that this paper addresses?

 This paper focuses on tackling the challenging problem of egocentric visual query localization. The main research questions/hypotheses it addresses are:

1. Current query-conditioned models and datasets for visual query localization suffer from implicit biases that hurt performance. These biases include:

- Domain bias: Models are only trained on frames with well-posed objects in clear view, while real-world egocentric videos contain blurry and out-of-focus objects from uncommon view angles. 

- Task bias: Models are trained with the query object always present, but at test time it is mostly absent from frames. This leads to false positives.

2. These biases can be reduced by:

- Expanding limited annotations with unlabeled frames containing diverse viewpoints and backgrounds without the query object present.

- Dynamically dropping object proposals during training to simulate query object absence.

- Using a novel Conditioned Contextual Transformer module to incorporate global context from object proposal sets while conditioning on the query.

3. The proposed techniques will improve egocentric query detection, leading to better visual query localization in 2D and 3D.

In summary, the main hypotheses are that tackling the identified dataset/model biases will improve performance on this challenging visual query localization task, especially using the proposed transformer-based model architecture and data augmentation techniques. The experiments aim to validate these hypotheses.
