# [Who's Harry Potter? Approximate Unlearning in LLMs](https://arxiv.org/abs/2310.02238)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Large language models (LLMs) are trained on massive datasets that often contain problematic content like copyrighted text. This poses legal and ethical issues for LLM developers and users.
- Retraining LLMs from scratch without the problematic data is infeasible due to the enormous compute resources required. 
- There is a need for techniques to selectively "unlearn" subsets of an LLM's training data post-training.

Proposed Solution:
- The paper introduces a novel approximate unlearning technique to erase specific training data from an LLM without full retraining.

- The technique has 3 main components:
   1. Use a reinforced model to identify tokens most related to the target data. 
   2. Replace idiosyncratic expressions in the target data with generic versions and generate alternative token labels using the baseline LLM's own predictions.
   3. Finetune the LLM on the alternative labels to effectively erase the original text from the LLM's memory.

- The method is applied to unlearn the Harry Potter books from the Llama2-7b LLM while retaining performance on NLP benchmarks.

Key Contributions:
- First paper to demonstrate an effective unlearning technique for generative language models.
- Shows it's possible to make an LLM "forget" intricate details about the Harry Potter books with minimal compute compared to full retraining. 
- Performance on NLP benchmarks stays largely unaffected after unlearning.
- Establishes approximate unlearning as a promising first step towards more adaptable, responsible, and legally compliant LLMs.
- Technique may have limitations with certain types of content but provides a foundation for future research.

In summary, the paper introduces a novel 3-step approach to approximately unlearn subsets of problematic training data from large language models post-training, while retaining broad capabilities. This is demonstrated via erasing intricate knowledge of Harry Potter plots from an LLM. The method serves as an initial step towards developing more adaptable LLMs aligned with ethical, legal and user-defined constraints.
