# [kNN-ICL: Compositional Task-Oriented Parsing Generalization with Nearest   Neighbor In-Context Learning](https://arxiv.org/abs/2312.10771)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Task-oriented parsing (TOP) aims to map natural language commands into structured outputs like intent/slot tags to enable conversational assistants. 
- Obtaining high-quality labeled data is challenging. Large language models (LLMs) can perform well in few-shot scenarios but have length constraints.
- Key research questions: How to effectively leverage LLMs for TOP? What defines a good prompt design? How to overcome length constraints?

Methods:
- Frame TOP as a code generation task by mapping semantic parse trees to Python code.  
- Analyze prompt design factors like API docs and exemplar selection methods (random, similarity-based).
- Propose kNN-ICL which enables accessing all examples during inference via nearest neighbor search and interpolation with LLM. Addresses length constraints and eases prompt design.   

Contributions:  
- Examine prompt design strategies for LLMs on TOP and find similarity-based selection effective, with stronger models benefiting more from docs.
- Introduce kNN-ICL which outperforms baseline kNN-LM by over 10% for GPT-NeoX and CodeGen, showing the impact of prompt conditioning.
- Demonstrate LLMs can achieve comparable performance to supervised models on TOP. kNN-ICL further improves results by seamlessly integrating all demo examples.

Key conclusion: Prompt design significantly impacts LLM performance on TOP. kNN-ICL reduces the need for complex prompt engineering while improving comprehension of complex requests using nearest neighbors.
