# [Understanding the Limitations of Variational Mutual Information   Estimators](https://arxiv.org/abs/1910.06222)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the main research question it addresses is: What are the limitations of existing variational approaches for estimating mutual information (MI), and how can we develop improved variational estimators with better bias-variance tradeoffs?In particular, the paper:- Theoretically shows that certain variational MI estimators like MINE can have variance that grows exponentially with the true underlying MI. This leads to poor bias-variance tradeoffs.- Empirically demonstrates that existing variational MI estimators fail to satisfy basic properties of MI like data processing and additivity under independence. - Provides a unified perspective of variational MI estimators as optimization over valid density ratios. This highlights the role of partition function estimation as a source of high variance.- Proposes a new estimator called SMILE that focuses on reducing variance in partition function estimates to achieve better bias-variance tradeoffs.- Shows improved performance of SMILE over existing estimators like MINE, NWJ, CPC on benchmark tasks while satisfying more self-consistency properties.So in summary, the key hypothesis is that variational MI estimators have limitations like exponential variance growth and self-consistency violations, and the paper aims to understand these limitations and develop improved estimators. The SMILE estimator is proposed as a solution that mitigates some of these issues.


## What is the main contribution of this paper?

This paper analyzes the limitations of variational mutual information estimators based on neural networks. The main contributions are:- It theoretically shows that certain estimators like MINE can have variance that grows exponentially with the true mutual information. This leads to poor bias-variance tradeoffs. - It proposes a set of self-consistency tests for mutual information estimators based on properties like independence, data processing and additivity. Empirically it demonstrates that existing estimators fail these tests on image datasets.- It provides a unified perspective on variational MI estimators as optimization over valid density ratios. From this view, it develops a new estimator called SMILE that focuses on reducing variance.- Empirical results on benchmark tasks show SMILE has improved bias-variance tradeoffs compared to prior estimators like MINE and CPC. It also performs better on the proposed self-consistency tests.In summary, the key contribution is analyzing limitations of existing variational MI estimators, both theoretical and empirical, and developing an improved estimator that mitigates some of these issues through a variance reduction approach. The analyses and new estimator help better understand the properties of these methods for estimating mutual information.
