# DisCup: Discriminator Cooperative Unlikelihood Prompt-tuning for   Controllable Text Generation

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question addressed is:How can we develop an effective and efficient approach for attribute-controllable text generation that can produce high-quality and diverse texts while maintaining good control over desired attributes?The key points are:- The paper focuses on attribute-controllable text generation (CTG), where the goal is to generate text that satisfies desired attributes like sentiment, topic, etc. - Existing approaches have limitations in terms of control performance, text quality, computational efficiency, or ability to capture relationships between attributes.- The paper proposes a new approach called DisCup that combines the strengths of prompt-tuning and decoding-time methods to address these limitations. - DisCup moves the attribute discriminator from decoding-time to training phase to augment prompt learning. It uses unlikelihood training to optimize prompts.- The central hypothesis is that this approach will achieve better attribute control, text quality and efficiency compared to existing CTG methods.In summary, the key research question is how to develop an effective and efficient CTG approach. DisCup is proposed as a way to combine prompt-tuning and decoding-time methods to potentially achieve better performance on multiple fronts. The paper presents experiments on sentiment control and toxicity avoidance tasks to evaluate this hypothesis.


## What is the main contribution of this paper?

The main contribution of this paper is proposing DisCup, a novel method for attribute-controllable text generation. The key ideas of DisCup are:1. Using an attribute discriminator to select likely and unlikely candidate tokens from the top-k tokens generated by a frozen pre-trained language model (PLM). This allows leveraging the knowledge from the discriminator during training.2. Optimizing the control prompts with an unlikelihood training objective, which encourages generating the likely tokens while avoiding the unlikely ones selected by the discriminator. 3. The candidate tokens come from the frozen PLM rather than ground truth tokens from the training data. This helps prevent overfitting to the training data.4. DisCup only requires optimizing a small set of continuous prompt tokens rather than fine-tuning the entire PLM. This is more parameter-efficient.Through experiments on sentiment control and toxicity avoidance, DisCup is shown to achieve superior attribute control performance compared to prior methods like vanilla prompt tuning, while maintaining high generation quality and diversity. The results highlight the promise of prompt-based methods for controllable text generation.In summary, the key contribution is proposing a novel prompt-tuning approach that incorporates the discriminator's knowledge to achieve better attribute control, leverages unlikelihood training for improved optimization, and is more parameter-efficient. The results demonstrate the effectiveness of this method for controllable text generation.


## How does this paper compare to other research in the same field?

Here are a few key ways this paper compares to other related research on controllable text generation:- The paper proposes a new approach called DisCup that combines prompt tuning with an attribute discriminator. This is a novel way of combining advantages from both prompt tuning methods and decoding-time methods for controllable generation. Most prior work focused on one method or the other.- DisCup achieves state-of-the-art performance on sentiment control and toxicity avoidance benchmarks while being efficient, only requiring around 10 prompt tokens. This shows promise for prompt-based methods being practical for controllable generation.- The unlikelihood training objective used in DisCup helps improve control performance by steering away from unwanted attributes. This builds on prior work on unlikelihood training but applies it in a new way for controllable generation.- Experiments show DisCup can overcome some limitations of vanilla prompt tuning like overfitting to training data and not capturing relationships between attributes. The discriminator and unlikelihood training help provide inter-attribute knowledge.- Compared to decoding-time methods, DisCup can generate higher quality texts by keeping generation close to the original pretrained language model. Decoding methods often degrade text quality more.- DisCup still has some limitations, like relying heavily on the base language model quality and difficulty extending to more complex generation tasks like table-to-text. But overall it demonstrates promise for prompt-based controllable generation.In summary, the key innovations of this paper compared to prior work are the combination of prompt tuning and decoding-time methods via a discriminator, and the unlikelihood training approach. The results demonstrate these as effective new techniques for controllable text generation.


## What future research directions do the authors suggest?

Based on my reading of the paper, some of the main future research directions suggested by the authors include:- Exploring the application of DisCup to more fine-grained controlled text generation tasks beyond attribute control, such as table-to-text generation. The authors state that the comprehensive ability of DisCup is currently limited to attribute control tasks.- Further improving the diversity of generated texts while maintaining fluency. The authors note there is a trade-off between diversity and perplexity that could be further explored.- Expanding the analysis on the impact of different hyperparameter choices, such as the size of the re-ranked candidate tokens and the length of control prompts. The authors provide some analysis but suggest more could be done.- Evaluating the approach on other language models beyond GPT-2, as the performance is correlated with the base language model chosen.- Exploring additional techniques to reduce overfitting the characteristics of the training data beyond just the target attributes. Overcoming this challenge is a key motivation for the proposed approach.- Applying the method to other text generation tasks that require controllability, such as dialogue systems. The authors frame controllable text generation as an important challenge.In summary, the main suggestions are to expand the application of the approach to other tasks and models, further improve text diversity and reduce overfitting, and provide more analysis on the impact of key hyperparameters. The core DisCup approach seems promising as a new way to achieve controllable text generation.
