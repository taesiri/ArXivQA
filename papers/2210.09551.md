# DisCup: Discriminator Cooperative Unlikelihood Prompt-tuning for   Controllable Text Generation

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question addressed is:How can we develop an effective and efficient approach for attribute-controllable text generation that can produce high-quality and diverse texts while maintaining good control over desired attributes?The key points are:- The paper focuses on attribute-controllable text generation (CTG), where the goal is to generate text that satisfies desired attributes like sentiment, topic, etc. - Existing approaches have limitations in terms of control performance, text quality, computational efficiency, or ability to capture relationships between attributes.- The paper proposes a new approach called DisCup that combines the strengths of prompt-tuning and decoding-time methods to address these limitations. - DisCup moves the attribute discriminator from decoding-time to training phase to augment prompt learning. It uses unlikelihood training to optimize prompts.- The central hypothesis is that this approach will achieve better attribute control, text quality and efficiency compared to existing CTG methods.In summary, the key research question is how to develop an effective and efficient CTG approach. DisCup is proposed as a way to combine prompt-tuning and decoding-time methods to potentially achieve better performance on multiple fronts. The paper presents experiments on sentiment control and toxicity avoidance tasks to evaluate this hypothesis.


## What is the main contribution of this paper?

The main contribution of this paper is proposing DisCup, a novel method for attribute-controllable text generation. The key ideas of DisCup are:1. Using an attribute discriminator to select likely and unlikely candidate tokens from the top-k tokens generated by a frozen pre-trained language model (PLM). This allows leveraging the knowledge from the discriminator during training.2. Optimizing the control prompts with an unlikelihood training objective, which encourages generating the likely tokens while avoiding the unlikely ones selected by the discriminator. 3. The candidate tokens come from the frozen PLM rather than ground truth tokens from the training data. This helps prevent overfitting to the training data.4. DisCup only requires optimizing a small set of continuous prompt tokens rather than fine-tuning the entire PLM. This is more parameter-efficient.Through experiments on sentiment control and toxicity avoidance, DisCup is shown to achieve superior attribute control performance compared to prior methods like vanilla prompt tuning, while maintaining high generation quality and diversity. The results highlight the promise of prompt-based methods for controllable text generation.In summary, the key contribution is proposing a novel prompt-tuning approach that incorporates the discriminator's knowledge to achieve better attribute control, leverages unlikelihood training for improved optimization, and is more parameter-efficient. The results demonstrate the effectiveness of this method for controllable text generation.
