# [Exploring Multi-Document Information Consolidation for Scientific   Sentiment Summarization](https://arxiv.org/abs/2402.18005)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem: 
Modern large language models (LLMs) can generate plausible summaries of multiple documents, but it is uncertain if they truly possess the ability to consolidate information, especially for documents with opinionated information. Sentiment summarization datasets are limited to product reviews and are not suitable for studying information consolidation. 

Solution:
The paper proposes the task of "scientific sentiment summarization" using meta-reviews in scientific peer review as a testbed. A three-layer framework is hypothesized for how human meta-reviewers consolidate sentiments from multiple review documents - (1) identify judgements, (2) reorganize and aggregate sentiments by criteria facets, (3) generate meta-review expressing final judgements.

Contributions:
- Initialize scientific sentiment summarization and hypothesize a three-layer consolidation framework for meta-review generation
- Collect human annotations of judgements in meta-reviews and source documents 
- Propose two evaluation metrics - reference-based FacetEval, and reference-free FusionEval to assess sentiment quality in generated meta-reviews
- Show empirical validation of the framework when incorporated into prompts for LLMs to generate meta-reviews

The paper demonstrates the potential of incorporating hypothesized human consolidation logic to improve LLM performance on sentiment summarization. Limitations include small annotated dataset size and English-only experiments. The authors advise against solely relying on LLM-generated meta-reviews without human verification.
