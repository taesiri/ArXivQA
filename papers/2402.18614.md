# [Deep Neural Network Models Trained With A Fixed Random Classifier   Transfer Better Across Domains](https://arxiv.org/abs/2402.18614)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
The paper explores how to improve the transferability of deep neural networks (DNNs) to new target domains, especially out-of-distribution domains that are very different from the source domain used to pretrain the model. Better transferability allows DNNs to adapt more effectively to new data distributions.

Proposed Solution: 
The paper proposes training DNNs with a fixed classifier that follows an "equiangular tight frame" (ETF) geometry. This implicitly minimizes within-class feature variability during training, resulting in more separable feature representations that focus more on class separation rather than domain-specific variations. 

The ETF geometry aligns with properties of the recently discovered neural collapse (NC) phenomenon. NC shows that in later training stages, DNN features converge to class means with minimal within-class variability. The paper shows theoretically that a linear random projection classifier leads to features with minimal class covariance, aligning with NC.

The paper trains ResNet CNNs on ImageNet with a fixed random ETF classifier. It then fine-tunes and evaluates the models on in-distribution and especially out-of-distribution target datasets.

Main Contributions:
- Establishes link between fixed ETF classifiers and implicit regularization of class covariances based on neural collapse theory
- Shows fixed ETF classifiers lead to improved transfer learning performance, especially on out-of-distribution target datasets (up to 19% gain)
- Compares favorably to baselines without covariance regularization and explicit covariance whitening methods
- Provides theoretical analysis and empirical evidence on multiple CNN architectures and fine-grained image classification datasets

The work provides a practical and effective strategy to enhance model transferability by regularizing class covariance during pretraining. It also gives theoretical and empirical insight into the underpinnings of why this approach is effective in the context of neural collapse.
