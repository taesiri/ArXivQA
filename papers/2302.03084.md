# [Pic2Word: Mapping Pictures to Words for Zero-shot Composed Image   Retrieval](https://arxiv.org/abs/2302.03084)

## What is the central research question or hypothesis that this paper addresses?

 This paper addresses the problem of composed image retrieval (CIR) without requiring labeled training data. The key research question is: 

How can we build a single CIR model that can perform diverse CIR tasks, such as object composition, attribute editing, and domain conversion, without requiring expensive labeled triplet datasets for training?

To address this, the paper proposes a new task called zero-shot composed image retrieval (ZS-CIR) and introduces a novel method called Pic2Word. The key ideas are:

- Representing an image as a pseudo word token using a learned mapping network, so that it can be flexibly composed with text descriptions by the language model. 

- Training the mapping network using only image-caption pairs and unlabeled images, without requiring triplet labels.

- Composing the pseudo image token with text descriptions at test time to perform diverse ZS-CIR tasks.

So in summary, the central hypothesis is that representing images as word tokens and composing them with text can enable zero-shot learning for diverse CIR tasks, removing the need for expensive labeled datasets. The Pic2Word method and experiments aim to validate this hypothesis.


## What is the main contribution of this paper?

 The main contribution of this paper is proposing a new task called zero-shot composed image retrieval (ZS-CIR) and a novel method called Pic2Word to tackle this task. 

Specifically, the key contributions are:

1. Proposing the ZS-CIR task, whose goal is to build a single composed image retrieval model that can perform diverse tasks like object composition, attribute editing, domain conversion, etc without requiring expensive labeled datasets.

2. Proposing Pic2Word, a method to solve ZS-CIR using only weakly labeled image-caption pairs and unlabeled images. It transforms an input image into a pseudo language token so that pre-trained language models can flexibly compose the image and text features.

3. Showing that Pic2Word trained using only weak supervision performs on par or better than recent supervised methods on various ZS-CIR tasks. For example, it improves over baselines by 10-100% on domain conversion, object composition, scene manipulation, and attribute editing without requiring any labeled triplets.

In summary, the main contribution is proposing the ZS-CIR task and Pic2Word method to solve it using only weakly labeled data, demonstrating strong performance on par with supervised methods. The key novelty is representing images as tokens to leverage pre-trained language models for flexible image-text composition.
