# [Pic2Word: Mapping Pictures to Words for Zero-shot Composed Image   Retrieval](https://arxiv.org/abs/2302.03084)

## What is the central research question or hypothesis that this paper addresses?

 This paper addresses the problem of composed image retrieval (CIR) without requiring labeled training data. The key research question is: 

How can we build a single CIR model that can perform diverse CIR tasks, such as object composition, attribute editing, and domain conversion, without requiring expensive labeled triplet datasets for training?

To address this, the paper proposes a new task called zero-shot composed image retrieval (ZS-CIR) and introduces a novel method called Pic2Word. The key ideas are:

- Representing an image as a pseudo word token using a learned mapping network, so that it can be flexibly composed with text descriptions by the language model. 

- Training the mapping network using only image-caption pairs and unlabeled images, without requiring triplet labels.

- Composing the pseudo image token with text descriptions at test time to perform diverse ZS-CIR tasks.

So in summary, the central hypothesis is that representing images as word tokens and composing them with text can enable zero-shot learning for diverse CIR tasks, removing the need for expensive labeled datasets. The Pic2Word method and experiments aim to validate this hypothesis.
