# [Pic2Word: Mapping Pictures to Words for Zero-shot Composed Image   Retrieval](https://arxiv.org/abs/2302.03084)

## What is the central research question or hypothesis that this paper addresses?

 This paper addresses the problem of composed image retrieval (CIR) without requiring labeled training data. The key research question is: 

How can we build a single CIR model that can perform diverse CIR tasks, such as object composition, attribute editing, and domain conversion, without requiring expensive labeled triplet datasets for training?

To address this, the paper proposes a new task called zero-shot composed image retrieval (ZS-CIR) and introduces a novel method called Pic2Word. The key ideas are:

- Representing an image as a pseudo word token using a learned mapping network, so that it can be flexibly composed with text descriptions by the language model. 

- Training the mapping network using only image-caption pairs and unlabeled images, without requiring triplet labels.

- Composing the pseudo image token with text descriptions at test time to perform diverse ZS-CIR tasks.

So in summary, the central hypothesis is that representing images as word tokens and composing them with text can enable zero-shot learning for diverse CIR tasks, removing the need for expensive labeled datasets. The Pic2Word method and experiments aim to validate this hypothesis.


## What is the main contribution of this paper?

 The main contribution of this paper is proposing a new task called zero-shot composed image retrieval (ZS-CIR) and a novel method called Pic2Word to tackle this task. 

Specifically, the key contributions are:

1. Proposing the ZS-CIR task, whose goal is to build a single composed image retrieval model that can perform diverse tasks like object composition, attribute editing, domain conversion, etc without requiring expensive labeled datasets.

2. Proposing Pic2Word, a method to solve ZS-CIR using only weakly labeled image-caption pairs and unlabeled images. It transforms an input image into a pseudo language token so that pre-trained language models can flexibly compose the image and text features.

3. Showing that Pic2Word trained using only weak supervision performs on par or better than recent supervised methods on various ZS-CIR tasks. For example, it improves over baselines by 10-100% on domain conversion, object composition, scene manipulation, and attribute editing without requiring any labeled triplets.

In summary, the main contribution is proposing the ZS-CIR task and Pic2Word method to solve it using only weakly labeled data, demonstrating strong performance on par with supervised methods. The key novelty is representing images as tokens to leverage pre-trained language models for flexible image-text composition.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the key points from the paper:

The paper proposes a new zero-shot composed image retrieval method called Pic2Word that maps an image to a word token using a learned lightweight mapping network, allowing flexible composition of image and text queries using a pretrained language model without requiring expensive labeled training data.


## How does this paper compare to other research in the same field?

 Here are a few key ways this paper compares to other research in zero-shot composed image retrieval:

- It proposes a new task formulation, zero-shot composed image retrieval (ZS-CIR), where the goal is to build a single model that can perform diverse CIR tasks without requiring expensive labeled training data. This is a novel problem formulation not explored in prior work. 

- The proposed method, Pic2Word, introduces a simple yet effective approach to map an image to a pseudo language token using a lightweight mapping network. This allows leveraging the compositional abilities of pre-trained language models for ZS-CIR. Other methods rely on late or early fusion of separate image and text encoders.

- Experiments demonstrate Pic2Word outperforms baselines by a large margin across diverse ZS-CIR tasks like domain conversion, object composition, and attribute editing. The performance is comparable or better than recent supervised CIR methods that require labeled datasets. This demonstrates the effectiveness of Pic2Word for zero-shot generalization.

- Pic2Word only requires image-caption pairs and unlabeled images for training, rather than expensive triplet supervision like most prior CIR work. The training data is considerably cheaper to collect at scale.

- Analyses provide insights into the model, effects of training data, and comparison to supervised approaches. For example, Pic2Word outperforms supervised methods given limited training triplets.

Overall, the key novelties are proposing the ZS-CIR problem, introducing a simple but effective mapping network approach to compose image and text for this task, and showing strong zero-shot generalization performance comparable to supervised models. The work demonstrates the promise of leveraging pre-trained vision-language models for zero-shot image retrieval.


## What future research directions do the authors suggest?

 The authors suggest several future research directions:

- Improving generalization across domains and compositions: The paper focuses on zero-shot composed image retrieval, but the authors suggest exploring few-shot learning where the model gets a small amount of training data in the target domain/task. This could improve generalization.

- Incorporating multiple image tokens: The current approach represents an image with a single token. Using multiple tokens could allow capturing more visual details. 

- Exploring different network architectures: The paper uses a simple MLP network for the image-to-token mapping. Trying more sophisticated networks like transformers could improve performance.

- Applying the approach to text-to-image generation: The idea of mapping images to tokens could potentially be useful for conditional image generation guided by text descriptions. This is an interesting future direction.

- Evaluating on instance-level tasks: The current domain transformation experiment uses class-level evaluation. More granular instance-level evaluation could provide further insights.

- Analysis on what makes a good pseudo token: The paper hypothesizes certain biases in existing datasets challenged zero-shot learning. Further analysis on what constitutes a "good" pseudo token could inform better prompt/dataset design.

In summary, the main future directions are improving generalization, incorporating multiple tokens per image, exploring different network architectures, and applying the idea to generative tasks like text-to-image synthesis. Analyzing the properties of good pseudo tokens is also an interesting direction.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the key points from the paper:

The paper proposes a new task called zero-shot composed image retrieval (ZS-CIR). The goal is to build a single model that can perform diverse composed image retrieval tasks, such as modifying attributes, composing objects, or converting image domains. This is achieved without requiring an expensive labeled dataset of image triplets during training. Instead, the model is trained using only large-scale image-caption pairs and unlabeled images. 

The proposed method, called Pic2Word, leverages a pre-trained vision-language model like CLIP. It introduces a lightweight mapping network that transforms the CLIP image embeddings into corresponding pseudo word tokens in the text encoder space. This allows flexibly composing the query image and text features using the pre-trained language capabilities. The mapping network is trained with a contrastive loss to reconstruct the image embeddings. At test time, the predicted pseudo token is simply inserted into prompt templates with the query text. Experiments across four datasets show the approach improves over zero-shot baselines by 10-100% and achieves performance comparable to recent supervised methods, without requiring any expensive labeled training data.


## Summarize the main method used in the paper in one paragraph.

 Here is a one paragraph summary of the main method used in the paper:

The paper proposes Pic2Word, a novel method for zero-shot composed image retrieval (ZS-CIR). The goal of ZS-CIR is to build a single model that can perform diverse composed image retrieval tasks without requiring expensive labeled training data. Pic2Word leverages a pre-trained contrastive vision-language model like CLIP which consists of an image encoder and text encoder. Pic2Word trains a lightweight mapping network to transform image embeddings from the CLIP vision encoder into pseudo language tokens that can be fed into the CLIP text encoder. The mapping network is trained with a contrastive loss using only unlabeled images to reconstruct the image embeddings, without any labeled image-text pairs. At test time, the predicted pseudo token for a query image is inserted into a prompt sentence alongside the text description. The composed query embedding generated by the CLIP text encoder using the prompt sentence with the pseudo token can then be compared to candidate image embeddings for retrieval. This approach allows harnessing the linguistic capabilities of CLIP's text encoder to flexibly combine image and text information for diverse ZS-CIR tasks.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the key points:

The paper proposes a new task called zero-shot composed image retrieval (ZS-CIR) for retrieving images using a query composed of an image and text description without requiring expensive labeled training data. The key idea is to leverage pre-trained vision-language models like CLIP and introduce a lightweight mapping network to convert the CLIP image features into a pseudo language token. This token can then be seamlessly composed with the text query by the CLIP text encoder to form the composed query embedding. The mapping network is trained with a contrastive loss using only unlabeled images to reconstruct the image embeddings. At test time, the composed query embedding is compared to candidate images for retrieval. Experiments on diverse ZS-CIR tasks like domain conversion, object composition, scene manipulation, and fashion attribute editing demonstrate the approach outperforms zero-shot baselines and is competitive with recent supervised CIR methods, without requiring triplet labels for training. The zero-shot capability enables solving new CIR tasks without collecting task-specific training data.


## What problem or question is the paper addressing?

 This paper is addressing the problem of composed image retrieval (CIR). Specifically, it is proposing a method to do "zero-shot" composed image retrieval, where the model is trained without requiring an expensive labeled dataset of image triplets (query image, text modifier, target image). The key questions it addresses are:

1) How can we build a single CIR model that can perform well on diverse tasks like attribute editing, object composition, domain conversion etc without requiring labeled data from each specific task? 

2) How can we effectively compose information from an image and text query using pretrained vision-language models like CLIP, without needing a labeled CIR dataset?

The paper proposes a novel method called "Pic2Word" to address these challenges. The key ideas are:

- Propose a new zero-shot CIR task that aims to build one model for diverse tasks without labeled data.

- Learn a mapping network to convert CLIP image embeddings into "pseudo word tokens" that can be seamlessly composed with text by the language model. 

- Train this mapping network using only unlabeled images and contrastive loss.

- At test time, compose the pseudo token with text query to retrieve images.

So in summary, the paper addresses the problem of expensive labeled data needs in CIR, and proposes a zero-shot learning method to learn CIR models from unlabeled data by casting images as tokens.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key keywords and terms are:

- Zero-shot composed image retrieval (ZS-CIR): This refers to the new task proposed where the goal is to build a composed image retrieval model without requiring expensive labeled training datasets. Instead, the model is trained using image-caption pairs and unlabeled images.

- Composed image retrieval (CIR): The existing task where a query composed of an image plus text is used to retrieve matching images. The paper discusses limitations of existing CIR methods.

- Contrastive language-image pretraining (CLIP): The first stage of the proposed framework that trains a vision and language encoder on image-caption pairs to align embeddings. 

- Mapping network: The novel component proposed that converts a CLIP visual embedding into a pseudo language token embedding compatible with the CLIP text encoder. This allows flexibly composing image and text features.

- Pseudo language token: The output of the mapping network that represents the image as if it were a real word token so it can be seamlessly combined with text descriptions.

- Cyclic contrastive loss: The loss function used to train the mapping network to reconstruct the original image embedding, which requires only unlabeled images.

- Domain conversion, object/scene composition, attribute manipulation: Some of the diverse CIR tasks evaluated in the experiments to demonstrate the effectiveness and generalization of the proposed approach.

- Zero-shot learning: The paper presents the first zero-shot approach to CIR without requiring triplet-labeled training data like previous supervised methods.
