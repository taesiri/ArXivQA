# [Pic2Word: Mapping Pictures to Words for Zero-shot Composed Image   Retrieval](https://arxiv.org/abs/2302.03084)

## What is the central research question or hypothesis that this paper addresses?

 This paper addresses the problem of composed image retrieval (CIR) without requiring labeled training data. The key research question is: 

How can we build a single CIR model that can perform diverse CIR tasks, such as object composition, attribute editing, and domain conversion, without requiring expensive labeled triplet datasets for training?

To address this, the paper proposes a new task called zero-shot composed image retrieval (ZS-CIR) and introduces a novel method called Pic2Word. The key ideas are:

- Representing an image as a pseudo word token using a learned mapping network, so that it can be flexibly composed with text descriptions by the language model. 

- Training the mapping network using only image-caption pairs and unlabeled images, without requiring triplet labels.

- Composing the pseudo image token with text descriptions at test time to perform diverse ZS-CIR tasks.

So in summary, the central hypothesis is that representing images as word tokens and composing them with text can enable zero-shot learning for diverse CIR tasks, removing the need for expensive labeled datasets. The Pic2Word method and experiments aim to validate this hypothesis.


## What is the main contribution of this paper?

 The main contribution of this paper is proposing a new task called zero-shot composed image retrieval (ZS-CIR) and a novel method called Pic2Word to tackle this task. 

Specifically, the key contributions are:

1. Proposing the ZS-CIR task, whose goal is to build a single composed image retrieval model that can perform diverse tasks like object composition, attribute editing, domain conversion, etc without requiring expensive labeled datasets.

2. Proposing Pic2Word, a method to solve ZS-CIR using only weakly labeled image-caption pairs and unlabeled images. It transforms an input image into a pseudo language token so that pre-trained language models can flexibly compose the image and text features.

3. Showing that Pic2Word trained using only weak supervision performs on par or better than recent supervised methods on various ZS-CIR tasks. For example, it improves over baselines by 10-100% on domain conversion, object composition, scene manipulation, and attribute editing without requiring any labeled triplets.

In summary, the main contribution is proposing the ZS-CIR task and Pic2Word method to solve it using only weakly labeled data, demonstrating strong performance on par with supervised methods. The key novelty is representing images as tokens to leverage pre-trained language models for flexible image-text composition.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the key points from the paper:

The paper proposes a new zero-shot composed image retrieval method called Pic2Word that maps an image to a word token using a learned lightweight mapping network, allowing flexible composition of image and text queries using a pretrained language model without requiring expensive labeled training data.


## How does this paper compare to other research in the same field?

 Here are a few key ways this paper compares to other research in zero-shot composed image retrieval:

- It proposes a new task formulation, zero-shot composed image retrieval (ZS-CIR), where the goal is to build a single model that can perform diverse CIR tasks without requiring expensive labeled training data. This is a novel problem formulation not explored in prior work. 

- The proposed method, Pic2Word, introduces a simple yet effective approach to map an image to a pseudo language token using a lightweight mapping network. This allows leveraging the compositional abilities of pre-trained language models for ZS-CIR. Other methods rely on late or early fusion of separate image and text encoders.

- Experiments demonstrate Pic2Word outperforms baselines by a large margin across diverse ZS-CIR tasks like domain conversion, object composition, and attribute editing. The performance is comparable or better than recent supervised CIR methods that require labeled datasets. This demonstrates the effectiveness of Pic2Word for zero-shot generalization.

- Pic2Word only requires image-caption pairs and unlabeled images for training, rather than expensive triplet supervision like most prior CIR work. The training data is considerably cheaper to collect at scale.

- Analyses provide insights into the model, effects of training data, and comparison to supervised approaches. For example, Pic2Word outperforms supervised methods given limited training triplets.

Overall, the key novelties are proposing the ZS-CIR problem, introducing a simple but effective mapping network approach to compose image and text for this task, and showing strong zero-shot generalization performance comparable to supervised models. The work demonstrates the promise of leveraging pre-trained vision-language models for zero-shot image retrieval.


## What future research directions do the authors suggest?

 The authors suggest several future research directions:

- Improving generalization across domains and compositions: The paper focuses on zero-shot composed image retrieval, but the authors suggest exploring few-shot learning where the model gets a small amount of training data in the target domain/task. This could improve generalization.

- Incorporating multiple image tokens: The current approach represents an image with a single token. Using multiple tokens could allow capturing more visual details. 

- Exploring different network architectures: The paper uses a simple MLP network for the image-to-token mapping. Trying more sophisticated networks like transformers could improve performance.

- Applying the approach to text-to-image generation: The idea of mapping images to tokens could potentially be useful for conditional image generation guided by text descriptions. This is an interesting future direction.

- Evaluating on instance-level tasks: The current domain transformation experiment uses class-level evaluation. More granular instance-level evaluation could provide further insights.

- Analysis on what makes a good pseudo token: The paper hypothesizes certain biases in existing datasets challenged zero-shot learning. Further analysis on what constitutes a "good" pseudo token could inform better prompt/dataset design.

In summary, the main future directions are improving generalization, incorporating multiple tokens per image, exploring different network architectures, and applying the idea to generative tasks like text-to-image synthesis. Analyzing the properties of good pseudo tokens is also an interesting direction.
