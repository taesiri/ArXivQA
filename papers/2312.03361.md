# [KhabarChin: Automatic Detection of Important News in the Persian   Language](https://arxiv.org/abs/2312.03361)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a high-quality paragraph summarizing the key points of the paper:

This paper introduces the novel task of automatically detecting important news articles in Persian. The authors created a new dataset, KhabarChin, consisting of 7,869 Persian news articles labeled as important or unimportant by annotators. They faced challenges of high disagreement among annotators and class imbalance, which they overcame by developing annotator guidelines and a scoring algorithm to identify potentially important articles. A variety of machine learning methods were benchmarked, with XLM-RoBERTa achieving the best Macro F1-score of 74.50% on the test set for classifying important news. The paper also introduces a secondary task of identifying important sentences within important news articles using a weakly supervised similarity-based approach. The overall contribution is pioneering the underexplored Persian important news detection task and releasing a dataset to facilitate future research. Both the data and models provide strong baselines for Persian news analysis.


## Summarize the paper in one sentence.

 This paper introduces a new dataset and benchmark for the task of automatically detecting important news articles in Persian, addresses key challenges of disagreement and class imbalance, and evaluates various machine learning methods including transformer models for news-level classification and a similarity-based approach for sentence-level detection within important news.


## What is the main contribution of this paper?

 Based on the content provided, the main contributions of this paper are:

1) It introduces the novel task of automatic detection of important news in the Persian language, which has not been previously explored. 

2) It creates a new benchmark dataset called "Khabarchin" containing 7,869 annotated Persian news articles for detecting important news. The paper describes the data collection, sampling, and annotation methodology in detail.

3) It proposes several machine learning and deep learning models for the task of important news classification, including traditional models like SVM, Random Forest as well as transformer models like ParsBERT and XLM-Roberta. Evaluation results on the dataset are provided.

4) It also introduces a secondary task of identifying important sentences within important news articles in a weakly supervised manner. A similarity based approach is proposed for detecting important sentences.

So in summary, the key contributions are pioneering the task of Persian important news detection, creation of a dataset, proposal of models, and introduction of important sentence identification as a secondary task.


## What are the keywords or key terms associated with this paper?

 Based on my review of the paper, some of the key terms and keywords associated with this paper include:

- Important news detection
- Persian language
- Text classification
- Machine learning models
- Transformer models
- Dataset creation
- Data annotation
- Data imbalance
- Inter-annotator agreement
- Weak supervision
- Sentence similarity
- Sentence importance detection

The paper introduces the new task of automatically detecting important news articles in the Persian language. It details the creation of a new dataset called KhabarChin for this purpose. Challenges like data imbalance and disagreements between annotators are discussed. Various machine learning and transformer models are evaluated for classifying news articles as important or unimportant. An additional task of identifying important sentences within important news is also introduced. Overall, the key focus is on important news detection in Persian text.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in this paper:

1. The paper mentions facing two main challenges in the initial data annotation process - high disagreement among annotators and class imbalance. What solutions did the authors propose to mitigate these issues?

2. Explain in detail the algorithm used to calculate the overall attitude score for news articles based on their comments (Algorithm 1). What is the intuition behind using negative attitude to identify potentially important news?

3. The authors use a threshold of 5 comments to divide the news articles into two categories. What is the rationale behind selecting this particular threshold value? How did they evaluate the efficacy of this categorization method?

4. Describe the annotation process in detail, including the use of annotator meetings and guidelines. How does this iterative process of refinement help improve annotation quality? 

5. For the task of important sentence detection, a weakly supervised approach is used. Explain this approach, including the mathematical formulation, in depth. What is the core concept it is based on?

6. The paper evaluates both traditional ML models and transformer models for news classification. Analyze and compare their performance. Why do context-based models like ParsBert underperform?

7. Discuss the evaluation metrics used in the paper, including micro and macro F1 scores. Why are macro metrics better suited for imbalanced datasets? Justify.

8. How exactly is the similarity between sentences computed in the important sentence detection method? Explain the concept of cosine similarity used.

9. For hyperparameter tuning in the important sentence detection task, Bayesian optimization with Optuna is utilized. Elaborate on how this technique works.

10. The paper claims transformer models have higher recall in identifying important news. Analyze why this is the case based on how they model semantic representations.


## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Detecting important news is crucial for people to stay informed efficiently, but it is challenging. 
- Previous work has not explored detecting important news in Persian language.
- Key challenges are high disagreement among annotators on what constitutes important news, and class imbalance between important/unimportant news.

Proposed Solution:
- Created a new dataset called KhabarChin for detecting important Persian news. Contains 7,869 news articles annotated by humans.
- Implemented heuristic to sample more potentially important articles to mitigate class imbalance issue. Used article's number of comments as signal. 
- Developed guideline and multi-stage process for annotators to agree on important news definition and resolve disagreements.  
- Introduce secondary task of identifying important sentences within important news articles using weak supervision.

Models and Results:  
- Benchmarked conventional ML models and transformer models like ParsBERT and XLM-Roberta for news classification task. 
- XLM-Roberta achieved best Macro F1-score of 74.5%. Context turns out to be less useful. 
- For sentence detection, similarity-based method outperforms baseline by using similarities to unimportant vs important sentences.

Main Contributions:
- First exploration of important Persian news detection.
- New dataset created and publicly released to enable further research.
- Solutions provided for key challenges of annotation disagreement and label imbalance.
- Benchmark of models for news and sentence-level detection tasks.

The paper introduces a novel problem area and dataset to facilitate future work on detecting important Persian news. Both news-level and sentence-level tasks are examined through models and heuristics to address inherent challenges.
