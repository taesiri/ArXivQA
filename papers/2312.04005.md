# [KOALA: Self-Attention Matters in Knowledge Distillation of Latent   Diffusion Models for Memory-Efficient and Fast Image Synthesis](https://arxiv.org/abs/2312.04005)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a summary paragraph of the key points from the paper:

This paper proposes KOALA, an efficient knowledge distillation approach to compress Stable Diffusion XL (SDXL) for faster and more memory-efficient text-to-image generation while retaining decent image quality. The authors first analyze SDXL's computationally heavy U-Net architecture and design two efficient variants called KOALA-1B and KOALA-700M which are 54\% and 69% smaller. They then explore effective U-Net distillation techniques and identify self-attention features from transformer blocks, especially in the decoder stages, as the most important for transferring knowledge. The proposed KOALA models distilled from SDXL consistently outperform previous compression methods on human preference scores and text-image alignment benchmarks. Notably, KOALA-700M runs over 2x faster than SDXL on consumer GPUs with at least 11GB memory, while producing 768x768 images on par with SDM-v2.0 on 8GB GPUs. Thus, KOALA promises to expand access to high-quality text-to-image generation under resource constraints.
