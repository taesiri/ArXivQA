# [BertNet: Harvesting Knowledge Graphs with Arbitrary Relations from   Pretrained Language Models](https://arxiv.org/abs/2206.14268)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a summary paragraph of the key points from the paper:

This paper proposes a novel framework called BertNet for automatically harvesting knowledge graphs of arbitrary new relations from pretrained language models like BERT and RoBERTa. Given a simple relation definition with a prompt and a few example entity pairs as input, the method generates a diverse set of weighted paraphrased prompts and uses an efficient search and rescoring process to extract consistent and accurate knowledge tuples from the language model. Experiments demonstrate harvesting knowledge graphs with over 400 relations, including complex 3-ary relations, with decent accuracy. The resulting symbolic knowledge graphs serve as interpretations of the source language models, revealing insights about impact of model size, pretraining strategies etc. on knowledge quality. A key advantage is flexibility to uncovered arbitrary new relations beyond text corpora or existing knowledge graphs. Limitations include focus on generic domain and risk of harvesting problematic knowledge from poorly trained language models. Overall, it presents an automated approach to interpret and analyze factual knowledge in language models.


## Summarize the paper in one sentence.

 Here is a one-sentence summary of the paper:

The paper proposes a new approach to automatically extract extensive knowledge graphs of arbitrary new relations from pretrained language models using minimal user input of relation definitions and example entity pairs.


## What is the main contribution of this paper?

 Based on my reading of the paper, the main contribution is proposing a new approach/framework to automatically harvest massive knowledge graphs (KGs) of arbitrary new relations from pretrained language models (LMs). The key aspects of their approach include:

1) It only requires minimal user input of a relation definition (a prompt and a few example entity pairs) to extract a KG around that relation. This allows handling arbitrary new relations not seen in existing KGs or text corpora.

2) It has an effective search strategy combined with scoring/reranking to efficiently discover accurate and diverse knowledge tuples from the vast space of possible entity pairs.

3) It adapts and enhances previous prompt paraphrasing and weighting methods to create multiple diverse prompts expressing the target relation, leading to more consistent outcome KGs. 

4) The resulting symbolic KGs serve as interpretations to better understand the knowledge encoded in the base LMs from which they are extracted. Experiments show new insights about factors like model size, distillation, etc. on the knowledge capacity.

In summary, the main contribution is the proposal of a novel framework/approach to automatically construct full-fledged KGs involving any new relations from pretrained language models, with minimal human input.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and concepts related to this work include:

- Knowledge graphs (KGs) - Graphical representations of facts and relationships between entities. The paper focuses on automatically constructing knowledge graphs from language models.

- Automatic knowledge base construction (AKBC) - The task of automatically creating knowledge bases/graphs from unstructured or semi-structured data.

- Language models (LMs) - Neural network models pretrained on large amounts of text data to generate fluent language. Used in the paper as implicit knowledge bases.

- Prompting - A method of querying language models by providing a natural language prompt with masked tokens to be predicted. Used to extract knowledge tuples from LMs.

- Consistency - The notion that model predictions should remain invariant to changes in surface form. Improving consistency is a focus of the proposed approach.  

- Symbolic knowledge distillation - Converting the implicit knowledge in neural models into an explicit symbolic knowledge base. Related to the high-level goal.

- Knowledge tuple - A fact representing a relationship between entities, denoted as (head entity, relation, tail entity). The paper extracts these from LMs.

- Prompt weighting - Assigning scores to automatically generated prompts based on their compatibility with example entity pairs. Helps improve consistency.

So in summary, key concepts include knowledge graphs, using language models for knowledge base construction, prompting methods, knowledge tuple extraction, consistency, and prompt weighting.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 potential in-depth questions about the method proposed in this paper:

1. The paper proposes an approach to extract knowledge graphs with arbitrary new relations from pretrained language models. How does this approach differ from traditional knowledge graph construction methods based on crowdsourcing or text mining? What are the main advantages?

2. The approach relies on automatically creating a diverse set of prompts to express the target relation, along with confidence scores for weighting the prompts. What is the intuition behind using multiple weighted prompts instead of just the initial human-provided prompt? How are the confidence scores for weighting prompts computed?

3. The paper describes an efficient "search and rescore" mechanism to discover entity pairs that are consistent across the diverse set of prompts. Can you explain the search strategy in more detail? Why is it more efficient than exhaustively enumerating all possible entity pairs? 

4. How does the paper generalize the approach to handle more complex relations, such as highly customized predicate expressions or n-ary relations? What modifications were made to the scoring function and search strategy?

5. The resulting knowledge graphs are viewed as a symbolic interpretation of the neural language models they are extracted from. What insights does the paper provide into knowledge capacities of models based on analyzing factors like model size, pretraining strategies, and distillation?

6. What are the limitations of solely relying on language models as the source of knowledge? Could the approach potentially retrieve incorrect or unethical knowledge if applied to poorly trained models? How can the risks be mitigated?

7. The paper focuses on extracting knowledge from generic language models. How could the approach be adapted to harvest domain-specific knowledge from models trained on narrow corpora like scientific literature or electronic health records?

8. Could the scoring functions be improved by incorporating external information like ontologies or commonsense constraints on the plausibility of entity relations? What challenges would this entail?

9. How well does the approach scale to very large language models? At what point would efficiency and compute resources become a bottleneck?

10. The paper demonstrates superiority over prior work like COMET and LPAQA on scoring known knowledge. How does end-to-end performance compare to these methods when extracting entirely new knowledge rather than completing known graphs? What are the tradeoffs?
