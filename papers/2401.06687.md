# [Proximal Causal Inference With Text Data](https://arxiv.org/abs/2401.06687)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Causal effect estimation from observational data is challenging due to confounding bias. Some recent work has tried to use unstructured text data as proxies for confounders that are unobserved or imperfectly measured. 
- However, these approaches assume the analyst has supervised labels of the confounders from the text for a subset of instances. This constraint is often infeasible due to privacy restrictions or high costs of hand-labeling.
- The paper addresses settings where an important confounding variable is completely unobserved and the goal is to estimate causal effects from observational data containing text.

Proposed Solution:
- The authors propose a new method that splits the pre-treatment text data into two halves. 
- Two separate zero-shot models are then used to infer two proxy variables (W and Z) from the two halves of text data. 
- These proxies are applied in the proximal g-formula framework for causal identification and estimation.

Main Contributions:
- Theoretical proof that splitting text and using separate zero-shot models satisfies the identification conditions of proximal causal inference, while other intuitive approaches do not.
- Proposal of an odds ratio based heuristic to detect violations of the untestable proximal conditions.
- Evaluation on synthetic and semi-synthetic data based on MIMIC-III clinical notes. Results show the method has low bias and good coverage when the odds ratio heuristic passes.
- Novel combination of proximal causal inference and zero-shot classifiers to address settings where an important confounder is completely unobserved. Expands options for text-specific causal inference.

In summary, the paper offers a new approach to estimating causal effects from observational data containing text when there is an unmeasured confounder variable and it cannot be directly labeled from the text due to restrictions. Theoretical and empirical results highlight the benefits of the method.


## Summarize the paper in one sentence.

 This paper proposes a new causal inference method that splits pre-treatment text data, infers two proxies from two different zero-shot models on the separate splits, and applies these proxies in the proximal g-formula to estimate causal effects when an important confounding variable is unobserved.


## What is the main contribution of this paper?

 The main contribution of this paper is proposing a new causal inference method that splits pre-treatment text data, infers two proxies from two different zero-shot models on the separate splits, and applies these proxies in the proximal g-formula. Specifically:

- They propose using zero-shot classifiers on split pre-treatment text data to infer proxies that satisfy the identification conditions for proximal causal inference when an important confounder is unmeasured. 

- They provide theoretical proofs that their method satisfies the proximal identification conditions while other alternative approaches do not.

- They propose an odds ratio falsification heuristic to detect violations of the proximal identification conditions.

- Through synthetic and semi-synthetic experiments, they demonstrate that their method produces estimates with low bias and good confidence interval coverage compared to alternatives, and that their odds ratio heuristic effectively determines when the identification conditions are violated.

In summary, the key contribution is a novel combination of proximal causal inference and zero-shot text classification to estimate causal effects when a key confounder is unmeasured, expanding the set of text-specific causal inference methods.


## What are the keywords or key terms associated with this paper?

 Based on my review of the paper, some of the key terms and concepts related to this work include:

- Causal inference - Estimating the effect of interventions/treatments on outcomes. A primary focus of the paper.

- Confounding bias - When a variable affects both the treatment and outcome, biasing causal estimates in observational studies.

- Text data as proxies - Using unstructured text, such as clinical notes, as proxies for unmeasured confounding variables to reduce this bias. 

- Proximal causal inference - A method that can identify causal effects by using two proxies for an unmeasured confounder satisfying certain conditions.

- Zero-shot models - Models that perform an unseen task with no supervised training examples, used here to predict proxies.

- Identification conditions - Conditions like independence and completeness required for proximal causal inference to produce unbiased estimates.

- Falsification heuristics - Methods to detect violations of untestable assumptions like the identification conditions.

- Odds ratio - A statistic used here as the basis for a falsification heuristic to flag problematic proxy pairs.

- Synthetic and semi-synthetic experiments - Used to evaluate the methods with access to ground truth causal effects.

In summary, key terms cover causal inference methodology, use of text data, proximal causal inference concepts, and the experimental procedures.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 potential in-depth questions about the method proposed in the paper:

1. The paper proposes splitting the pre-treatment text data into two halves and using separate zero-shot models on each half to predict proxies $W$ and $Z$. What is the rationale behind splitting the text rather than using the full text? How might the performance change if you varied the splitting proportion (e.g. 60/40 rather than 50/50)?

2. The paper assumes the unmeasured confounder $U$ is binary (assumption S1). How might you extend the method if $U$ was instead continuous or categorical with more than 2 levels? Would identification still be possible?

3. The odds ratio falsification heuristic relies on the analyst choosing a reasonable value for $\gamma_{\text{high}}$. What guidance does the paper provide on setting this parameter? How sensitive are the results to different choices of $\gamma_{\text{high}}$? 

4. Could you apply this method if the text data was available for only a subset of units rather than all units? What identification assumptions would be required?

5. The synthetic experiments show worse performance when using a single zero-shot model compared to two separate models. Why might this occur in practice even though the theory suggests a single model could be valid?

6. What types of zero-shot models are best suited for this approach? How do model choice and hyperparameter settings impact the quality of the predicted proxies?

7. The paper focuses on linear models for the proximal g-formula estimation. How could more flexible machine learning models like neural networks be incorporated? What identification assumptions would need to hold?

8. How does performance scale with the amount of pre-treatment text data available? How does the method perform with very short texts?

9. The paper addresses settings when the analyst cannot access any labeled examples of $U$. How would performance change if you had access to labeled $U$ for a small subset, such as 10\% of units?  

10. How does this method compare to other text-based causal inference methods like causal topic models or propensity matching on document embeddings? What are the tradeoffs?
