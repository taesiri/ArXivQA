# [On the Challenges of Using Black-Box APIs for Toxicity Evaluation in   Research](https://arxiv.org/abs/2304.12397)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question seems to be: 

How have changes to the Perspective API over time impacted the reproducibility of research results that rely on it for toxicity evaluation?

The authors investigate this by:

1) Empirically evaluating how toxicity scores from the RealToxicityPrompts (RTP) dataset have changed when using a recent version of the Perspective API compared to the original scores. 

2) Analyzing the impact on model rankings and metrics using the widely used HELM benchmark before and after rescoring generations with the updated API. 

3) Replicating toxicity mitigation benchmarks proposed in past work and examining if results change when using the latest API.

Overall, the paper aims to understand the implications of black-box API changes on the validity and reproducibility of toxicity evaluation research that relies on inherited scores. The authors suggest caution in making apples-to-apples comparisons between studies scored at different times and provide recommendations for evaluating toxicity more robustly.


## What is the main contribution of this paper?

 Based on my reading of the paper abstract, the main contributions appear to be:

1. Empirically validating that toxicity scores from the RealToxicityPrompts (RTP) dataset have changed substantially when using a recent version of the Perspective API compared to when the dataset was originally released. Specifically, there is a 49% relative decrease in the number of toxic prompts.

2. Demonstrating the impact of API changes on the rankings and toxicity metrics of language models evaluated on the widely used HELM benchmark. Updating the toxicity scores leads to different rankings and conclusions about model toxicity.

3. Showing that research papers proposing toxicity mitigation techniques up until recently can exhibit different findings when evaluations are redone using an updated API. This challenges the reproducibility of previous results. 

4. Providing recommendations for evaluating model toxicity more robustly in light of the challenges posed by black-box toxicity APIs that frequently change. Key suggestions include rescoring generations, clearly reporting scoring dates, and releasing evaluated text.

In summary, the main contribution is highlighting the challenges of using black-box commercial APIs like Perspective for toxicity evaluation in research, empirically demonstrating the impacts on reproducibility, and providing guidance for more rigorous benchmarking. The findings suggest caution is needed when comparing toxicity-related studies that rely on the API.


## How does this paper compare to other research in the same field?

 Here are a few thoughts on how this paper compares to other research in the field of evaluating toxicity detection systems:

- The focus on reproducibility is an important contribution. Many papers propose new techniques for toxicity detection or mitigation, but few systematically study how changes to commercial APIs like Perspective impact the reproducibility of past results. This paper highlights an underappreciated challenge.

- Re-evaluating models on the HELM benchmark over time provides compelling evidence that subtle API changes can alter model rankings and perceived risks. This rigorously demonstrates the reproducibility issues that can arise from relying on black-box APIs.

- Replicating past work on toxicity mitigation techniques confirms that published results may no longer hold just months later when systems are rescored. This emphasizes the need for caution when directly comparing results across papers.

- The analysis of changing score distributions on RTP prompts adds useful context about how toxicity definitions have evolved. Qualitative examples highlight types of content perceived differently today.

- The set of recommendations in the conclusion are constructive. Having API versioning, prompt/generation release, and rescoring controls will help. The calls for more communication and structured evaluation are well-reasoned.

Overall, the paper makes a valuable contribution by systematically demonstrating the challenges of reproducibility when using black-box APIs for toxicity evaluation. The analysis provides concrete evidence of the issues, and the recommendations help chart a path for more rigorous benchmarking in future work. The focus on reproducibility and rigor differentiate it from most prior work in this space.


## What future research directions do the authors suggest?

 Based on my reading of the paper, some key future research directions suggested by the authors include:

- Developing more structured approaches to evaluating toxicity over time. The authors recommend establishing control sets of sequences that are rescored periodically with the Perspective API as it gets updated. This can help identify when toxicity scores have drifted andprompt re-evaluation of models.

- Improving communication around API changes. The authors suggest API providers should more clearly version models and notify users of updates. On the research side, authors should report details like when toxicity scoring was performed. 

- Releasing model generations to enable rescoring. By open sourcing model outputs, future researchers can rescore using updated APIs to enable more apples-to-apples comparisons.

- Rethinking use of static toxicity-stratified datasets like RTP. The authors suggest ignoring the predefined toxicity stratification of RTP and instead rescoring prompts alongside model outputs to mitigate issues with outdated labels.

- Exploring other evaluation frameworks like BOLD that prompt with non-toxic text to assess model stereotyping, rather than relying solely on toxicity.

- Establishing clearer guidelines for rigorous evaluation of model toxicity over time. The paper lays groundwork here but additional community input could help standardize best practices.

- Investigating the impact of API changes for other attributes beyond just toxicity. The authors show pronoun and other scores also drift over time.

Overall the authors advocate for more transparency, updated benchmarking, releasing model outputs, and developing more robust protocols for evaluating and comparing toxicity over time. Addressing these areas could significantly improve the rigor of language model toxicity assessment.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the paper:

The paper examines the challenges of using black-box APIs like Perspective for toxicity evaluation in research. It finds that changes to the Perspective API over time, without clear documentation of those changes, can significantly impact research reproducibility. For example, rescoring the widely used RealToxicityPrompts dataset with a newer version of Perspective led to a 49% drop in the number of prompts classified as toxic. Rescoring models benchmarked in the HELM framework also led to changes in toxicity rankings. Similarly, rescoring mitigation techniques proposed in past research changed the relative effectiveness. The authors suggest these API changes, without transparency, make it difficult to compare toxicity results over time or across studies. They recommend Perspective provide clearer versioning information, authors rescore generations when possible, and toxicity evaluations accompany details on when scoring occurred. Overall, reliance on black-box commercial APIs poses reproducibility issues for toxicity evaluations in AI research.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the paper:

The paper examines the challenges of using black-box APIs like Perspective for toxicity evaluation in research. It focuses on the reproducibility issues caused by the fact that these commercial APIs are frequently retrained without notice to users. The authors first empirically validate that the toxicity scores from the widely used RealToxicityPrompts dataset have substantially changed when using a more recent version of Perspective API. Rescoring the dataset led to a 49% decrease in the number of toxic prompts. They then look at the impact on the rankings of models in the HELM benchmark for various toxicity metrics. Rescoring the model generations led to 24 ranking changes across 13 models for the Toxic Fraction metric. The paper also replicates previously proposed toxicity mitigation techniques and shows that recent work from just months ago already exhibits different toxicity metrics when rescored today. 

Based on these findings, the authors make several recommendations: API providers should better version models and notify users of changes, authors should rescore generations and explicitly state when scoring occurred, benchmarks should establish control sets to rescore and validate previous results. Overall, the work demonstrates the challenges in reproducing toxicity evaluations and comparisons over time when relying on black-box APIs like Perspective that frequently change. Caution is needed in making apples-to-apples comparisons or reused benchmarks between studies. The findings suggest the need for more rigorous tracking of toxicity evaluation over time.


## Summarize the main method used in the paper in one paragraph.

 Here is a one paragraph summary of the main method used in the paper:

The paper proposes a new method for toxicity detection and mitigation in language models. The key idea is to use a dynamic benchmarking approach where model generations are rescored over time using the latest version of the Perspective API. This allows evaluating the impact of API changes on toxicity metrics and model rankings. The authors rescore generations from widely used benchmarks like RTP and HELM. They show that toxicity metrics and model rankings change significantly when evaluations use the latest API, undermining reproducibility. The results indicate that past benchmarking studies comparing toxicity mitigation techniques may have drawn inaccurate conclusions by relying on outdated API versions. To address this, the authors recommend authors rescore generations before comparisons, release scores/generations to enable re-evaluation, and encourage API providers to better version models and notify users of changes. The rescoring methodology provides a framework to evaluate model toxicity more rigorously over time.


## What problem or question is the paper addressing?

 Based on my reading, the key points of this paper are:

- The paper examines the challenges of using black-box APIs like Perspective for toxicity evaluation in research. Specifically, it looks at how changes to these commercial APIs over time can impact research reproducibility.

- The main problem is that APIs like Perspective are not static but get frequently retrained and updated to address weaknesses and biases. However, these changes are often not well communicated. 

- This poses challenges for research that relies on inherited toxicity scores from these APIs to compare models or propose new techniques. Using outdated API versions can lead to inaccurate or irreproducible findings.

- The paper empirically demonstrates the impact of API changes on toxicity score distributions and rankings of models on benchmarks like RealToxicityPrompts and HELM. It shows that toxicity scores and model rankings can shift significantly when rescored on a newer API version.

- The authors replicate results from recent toxicity mitigation papers and find that even studies from late 2022 were affected when rescored on the latest Perspective API. This indicates issues reproducing comparative evaluations.

- Overall, the use of black-box commercial APIs with frequent silent updates introduces reproducibility issues and potential for biased conclusions in toxicity evaluation research. The paper recommends more structured evaluation and transparency around scoring to mitigate these problems.

In summary, the key problem is the lack of versioning and communication around API changes, which impacts the reproducibility of toxicity evaluation research that relies on inherited scores. The paper demonstrates and raises awareness of this issue through empirical analysis.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the key points from the paper:

The paper empirically shows that relying on black-box commercial APIs like Perspective for toxicity evaluation in research poses challenges for reproducibility, as unseen API updates affect toxicity scores, rankings of models, and conclusions of techniques when evaluations are compared over time.


## What are the keywords or key terms associated with this paper?

 Based on reading the abstract and skimming the paper, some key terms and concepts seem to be:

- Toxicity detection - The paper focuses on evaluating toxicity and the challenges of using automated APIs like Perspective for this. Toxicity detection is a central theme.

- Reproducibility - The paper examines how changes to the Perspective API over time impacts the reproducibility of past toxicity evaluation research. Reproducibility is a main focus.

- Benchmarking - The paper looks at benchmark datasets like RealToxicityPrompts and benchmark models like those in HELM. Benchmarking model toxicity is a key application. 

- Language models - The paper evaluates the toxicity of various language models like GPT-3, BLOOM, TNLGv2. Language model toxicity is a key research area discussed.

- Mitigation techniques - The paper replicates past work on techniques to mitigate toxicity in language models. These mitigation techniques are an important research direction.

- Perspective API - The Perspective API is the main black-box commercial tool used for automatic toxicity scoring throughout the paper.

- RealToxicityPrompts - This dataset based on Perspective API scores is widely used for benchmarking and is analyzed in the paper.

- HELM - This living benchmark for language models leverages RealToxicityPrompts and is re-evaluated in the paper.

- Rankings - The paper examines how toxicity rankings of models change over time with Perspective API updates. Rankings are used to assess models.

So in summary, key terms cover toxicity evaluation, benchmarking, reproducibility, language models, mitigation techniques, and the use of automatic scoring APIs like Perspective. The analysis aims to understand the challenges of using black-box APIs for toxicity research over time.


## Based on the paper, what questions should we ask to create a comprehensive summary of it? Please list at least 10 questions.

 Here are 10 potential questions to ask to create a comprehensive summary of the paper:

1. What is the main research problem addressed in the paper?

2. What methods or approaches does the paper propose to address the research problem? 

3. What are the key findings or results presented in the paper?

4. What datasets were used in the experiments?

5. How was model toxicity evaluated in the paper (e.g. using Perspective API)? 

6. Which language models or techniques were compared in the experiments?

7. How robust or reproducible were the findings, given potential changes to the Perspective API over time?

8. What are the limitations or potential weaknesses of the proposed approach? 

9. What recommendations does the paper make regarding evaluating model toxicity?

10. What directions for future work does the paper suggest?

Asking these types of questions should help summarize the key information about the research problem, proposed methods, experiments, results, limitations, and recommendations of the paper. The questions cover the motivation, techniques, findings, implications, and future directions. Additional questions could further probe the problem definition, experimental details, related work comparisons, or potential societal impacts. The goal is to extract the core elements and contributions of the paper through directed questions.


## Design 10 in-depth questions about method proposed in this paper and avoid simple question.

 Here are 10 in-depth questions about the methodology proposed in this paper:

1. The paper proposes using the Perspective API to rescore toxicity of existing datasets and model generations. What are some potential limitations or biases introduced by relying solely on this commercial black-box API for toxicity scoring? Could the results differ if multiple toxicity scoring tools were used instead?

2. The authors rescored prompts and generations from the RealToxicityPrompts (RTP) dataset. How robust are the conclusions from analyzing this one dataset? Would the findings hold if other popular toxicity datasets were rescored and analyzed as well? 

3. The paper examines impacts on the rankings and metrics of models benchmarked in HELM. Are there any other potential implications of the API changes beyond shifts in model rankings? For example, could it affect perceived progress in the field if toxicity appears reduced due to rescoring?

4. The authors replicate results from 6 toxicity mitigation techniques published from 2019-2023. What motivated the selection of these particular methods to analyze? Are there any other recent mitigation techniques that should have been considered as well?

5. For the mitigation techniques benchmarked, the paper mainly focuses on how toxicity metrics changed through rescoring. Are there any other factors aside from toxicity metrics that could be impacted by API changes when analyzing mitigation techniques?

6. The authors suggest rescoring text sequences used in experiments to ensure appropriate comparisons. What are some challenges or practical limitations that need to be considered regarding rescoring previously released model generations?

7. Beyond rescoring, what other steps could researchers take to make toxicity evaluations using black-box APIs more robust and reproducible? For example, using multiple scoring tools, releasing details on scoring methodology, etc.

8. The paper lays out several recommendations for the research community regarding use of black-box APIs for toxicity. How feasible and realistic are these recommendations? What could help drive adoption?

9. The Perspective API does not appear to version or document changes. Should APIs used for academic research be held to higher standards of transparency and versioning? What role do API providers play in enabling reproducibility?

10. The paper focuses on toxicity evaluation specifically. To what extent could the challenges and recommendations discussed also apply when black-box APIs are used to evaluate other attributes like sentiment, content quality, etc.? What parallels can be drawn?


## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a high-quality summary paragraph of the key points from the paper:

This paper investigates the reproducibility challenges that arise from using black-box commercial APIs, like Perspective API, to evaluate model toxicity in NLP research. The authors first show that toxicity scores from the RealToxicityPrompts dataset have substantially decreased when rescored with a newer Perspective API version, with 49% fewer prompts classified as toxic. They then demonstrate that rescoring model generations on the HELM benchmark with an updated API leads to different model rankings, with 13 models having their results significantly change. Next, the paper replicates past work and finds toxicity mitigation techniques benchmarked even in late 2022 have different perceived performance when rescored today. Based on these findings, the authors establish best practices, including rescoring any inherited generations and prompts to ensure proper evaluation. They argue that lack of API versioning and communication of updates poses reproducibility issues. The key conclusions are that toxicity is not static, APIs change unlabeled, and past benchmarks have led to inaccurate comparisons, requiring more rigor in future toxicity evaluations to enable sound reproducibility.


## Summarize the paper in one sentence.

 The paper examines how changes to the Perspective API over time impact the reproducibility of academic research results that rely on it for toxicity scoring, finding that API updates can lead to different toxicity assessments and rankings of language models.


## Summarize the paper in one paragraphs.

 Here is a one paragraph summary of the paper:

This paper evaluates the implications of changes to the Perspective API toxicity model on the reproducibility of academic research in language model safety. The authors first show that toxicity scores in the widely used RealToxicityPrompts dataset have substantially decreased when rescored recently, with 49% fewer prompts classified as toxic. Rescoring generations from models in the HELM benchmark also leads to different toxicity rankings, with 13 models changing position. Replicating past work on toxicity mitigation techniques shows that papers published even months ago would have different results if rescored now. The authors conclude that the lack of versioning and communication around changes to commercial black-box APIs like Perspective poses challenges to reproducing and fairly comparing toxicity evaluation results over time. They recommend that researchers rescore generations themselves before comparisons, and that API providers better communicate changes.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in this paper:

1. The authors recommend rescoring generations and prompts with the latest version of Perspective API when comparing models over time. What are some challenges associated with requiring authors to rescore generations, especially for models that are not easily accessible? How can the field balance reproducibility with practical constraints?

2. The authors suggest that changes to commercial black-box APIs should be more clearly communicated to users. What kinds of changes would be most important for API providers to communicate? How frequently should they provide updates? What channels would be most effective for disseminating this information?

3. The authors evaluated the impact of API changes on the reproducibility of 6 previous techniques for toxicity mitigation in LMs. What other approaches could they have benchmarked to provide a more comprehensive view? How could the set of techniques be expanded in future work?

4. The authors focused their evaluation on the Perspective API's toxicity attribute. How might changes to the API's other attributes, like sexualization and threat, impact the reproducibility of toxicity-related research over time? 

5. The authors suggest establishing a control set of sequences to rescore with each addition of a model to the HELM benchmark. What considerations should go into creating this control set? How large should it be and should it include both toxic and non-toxic prompts?

6. The authors only examine a subset of the evaluation scenarios in HELM. How might API changes affect reproducibility in the other scenarios related to capabilities like summarization and translation?

7. The authors observe lower toxicity scores when rescoring with the latest API, indicating a change in the operational definition of toxicity. What could have driven this shift? How can the field reconcile these differences in toxicity definitions over time?

8. The authors focus on evaluating generations, but how might changes to the API impact techniques that directly optimize prompts like prompt programming? What additional analyses could be done?

9. The authors measure reproducibility along 3 axes: values, findings, and conclusions. What other frameworks are available for systematically evaluating reproducibility? How could they be applied in this context?

10. The authors suggest releasing model generations to enable rescoring. What other types of artifacts like training datasets and model parameters could also improve reproducibility? What are limitations to releasing these artifacts?
