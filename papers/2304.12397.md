# [On the Challenges of Using Black-Box APIs for Toxicity Evaluation in   Research](https://arxiv.org/abs/2304.12397)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question seems to be: How have changes to the Perspective API over time impacted the reproducibility of research results that rely on it for toxicity evaluation?The authors investigate this by:1) Empirically evaluating how toxicity scores from the RealToxicityPrompts (RTP) dataset have changed when using a recent version of the Perspective API compared to the original scores. 2) Analyzing the impact on model rankings and metrics using the widely used HELM benchmark before and after rescoring generations with the updated API. 3) Replicating toxicity mitigation benchmarks proposed in past work and examining if results change when using the latest API.Overall, the paper aims to understand the implications of black-box API changes on the validity and reproducibility of toxicity evaluation research that relies on inherited scores. The authors suggest caution in making apples-to-apples comparisons between studies scored at different times and provide recommendations for evaluating toxicity more robustly.


## What is the main contribution of this paper?

 Based on my reading of the paper abstract, the main contributions appear to be:1. Empirically validating that toxicity scores from the RealToxicityPrompts (RTP) dataset have changed substantially when using a recent version of the Perspective API compared to when the dataset was originally released. Specifically, there is a 49% relative decrease in the number of toxic prompts.2. Demonstrating the impact of API changes on the rankings and toxicity metrics of language models evaluated on the widely used HELM benchmark. Updating the toxicity scores leads to different rankings and conclusions about model toxicity.3. Showing that research papers proposing toxicity mitigation techniques up until recently can exhibit different findings when evaluations are redone using an updated API. This challenges the reproducibility of previous results. 4. Providing recommendations for evaluating model toxicity more robustly in light of the challenges posed by black-box toxicity APIs that frequently change. Key suggestions include rescoring generations, clearly reporting scoring dates, and releasing evaluated text.In summary, the main contribution is highlighting the challenges of using black-box commercial APIs like Perspective for toxicity evaluation in research, empirically demonstrating the impacts on reproducibility, and providing guidance for more rigorous benchmarking. The findings suggest caution is needed when comparing toxicity-related studies that rely on the API.


## How does this paper compare to other research in the same field?

 Here are a few thoughts on how this paper compares to other research in the field of evaluating toxicity detection systems:- The focus on reproducibility is an important contribution. Many papers propose new techniques for toxicity detection or mitigation, but few systematically study how changes to commercial APIs like Perspective impact the reproducibility of past results. This paper highlights an underappreciated challenge.- Re-evaluating models on the HELM benchmark over time provides compelling evidence that subtle API changes can alter model rankings and perceived risks. This rigorously demonstrates the reproducibility issues that can arise from relying on black-box APIs.- Replicating past work on toxicity mitigation techniques confirms that published results may no longer hold just months later when systems are rescored. This emphasizes the need for caution when directly comparing results across papers.- The analysis of changing score distributions on RTP prompts adds useful context about how toxicity definitions have evolved. Qualitative examples highlight types of content perceived differently today.- The set of recommendations in the conclusion are constructive. Having API versioning, prompt/generation release, and rescoring controls will help. The calls for more communication and structured evaluation are well-reasoned.Overall, the paper makes a valuable contribution by systematically demonstrating the challenges of reproducibility when using black-box APIs for toxicity evaluation. The analysis provides concrete evidence of the issues, and the recommendations help chart a path for more rigorous benchmarking in future work. The focus on reproducibility and rigor differentiate it from most prior work in this space.


## What future research directions do the authors suggest?

 Based on my reading of the paper, some key future research directions suggested by the authors include:- Developing more structured approaches to evaluating toxicity over time. The authors recommend establishing control sets of sequences that are rescored periodically with the Perspective API as it gets updated. This can help identify when toxicity scores have drifted andprompt re-evaluation of models.- Improving communication around API changes. The authors suggest API providers should more clearly version models and notify users of updates. On the research side, authors should report details like when toxicity scoring was performed. - Releasing model generations to enable rescoring. By open sourcing model outputs, future researchers can rescore using updated APIs to enable more apples-to-apples comparisons.- Rethinking use of static toxicity-stratified datasets like RTP. The authors suggest ignoring the predefined toxicity stratification of RTP and instead rescoring prompts alongside model outputs to mitigate issues with outdated labels.- Exploring other evaluation frameworks like BOLD that prompt with non-toxic text to assess model stereotyping, rather than relying solely on toxicity.- Establishing clearer guidelines for rigorous evaluation of model toxicity over time. The paper lays groundwork here but additional community input could help standardize best practices.- Investigating the impact of API changes for other attributes beyond just toxicity. The authors show pronoun and other scores also drift over time.Overall the authors advocate for more transparency, updated benchmarking, releasing model outputs, and developing more robust protocols for evaluating and comparing toxicity over time. Addressing these areas could significantly improve the rigor of language model toxicity assessment.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the paper:The paper examines the challenges of using black-box APIs like Perspective for toxicity evaluation in research. It finds that changes to the Perspective API over time, without clear documentation of those changes, can significantly impact research reproducibility. For example, rescoring the widely used RealToxicityPrompts dataset with a newer version of Perspective led to a 49% drop in the number of prompts classified as toxic. Rescoring models benchmarked in the HELM framework also led to changes in toxicity rankings. Similarly, rescoring mitigation techniques proposed in past research changed the relative effectiveness. The authors suggest these API changes, without transparency, make it difficult to compare toxicity results over time or across studies. They recommend Perspective provide clearer versioning information, authors rescore generations when possible, and toxicity evaluations accompany details on when scoring occurred. Overall, reliance on black-box commercial APIs poses reproducibility issues for toxicity evaluations in AI research.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the paper:The paper examines the challenges of using black-box APIs like Perspective for toxicity evaluation in research. It focuses on the reproducibility issues caused by the fact that these commercial APIs are frequently retrained without notice to users. The authors first empirically validate that the toxicity scores from the widely used RealToxicityPrompts dataset have substantially changed when using a more recent version of Perspective API. Rescoring the dataset led to a 49% decrease in the number of toxic prompts. They then look at the impact on the rankings of models in the HELM benchmark for various toxicity metrics. Rescoring the model generations led to 24 ranking changes across 13 models for the Toxic Fraction metric. The paper also replicates previously proposed toxicity mitigation techniques and shows that recent work from just months ago already exhibits different toxicity metrics when rescored today. Based on these findings, the authors make several recommendations: API providers should better version models and notify users of changes, authors should rescore generations and explicitly state when scoring occurred, benchmarks should establish control sets to rescore and validate previous results. Overall, the work demonstrates the challenges in reproducing toxicity evaluations and comparisons over time when relying on black-box APIs like Perspective that frequently change. Caution is needed in making apples-to-apples comparisons or reused benchmarks between studies. The findings suggest the need for more rigorous tracking of toxicity evaluation over time.


## Summarize the main method used in the paper in one paragraph.

 Here is a one paragraph summary of the main method used in the paper:The paper proposes a new method for toxicity detection and mitigation in language models. The key idea is to use a dynamic benchmarking approach where model generations are rescored over time using the latest version of the Perspective API. This allows evaluating the impact of API changes on toxicity metrics and model rankings. The authors rescore generations from widely used benchmarks like RTP and HELM. They show that toxicity metrics and model rankings change significantly when evaluations use the latest API, undermining reproducibility. The results indicate that past benchmarking studies comparing toxicity mitigation techniques may have drawn inaccurate conclusions by relying on outdated API versions. To address this, the authors recommend authors rescore generations before comparisons, release scores/generations to enable re-evaluation, and encourage API providers to better version models and notify users of changes. The rescoring methodology provides a framework to evaluate model toxicity more rigorously over time.


## What problem or question is the paper addressing?

 Based on my reading, the key points of this paper are:- The paper examines the challenges of using black-box APIs like Perspective for toxicity evaluation in research. Specifically, it looks at how changes to these commercial APIs over time can impact research reproducibility.- The main problem is that APIs like Perspective are not static but get frequently retrained and updated to address weaknesses and biases. However, these changes are often not well communicated. - This poses challenges for research that relies on inherited toxicity scores from these APIs to compare models or propose new techniques. Using outdated API versions can lead to inaccurate or irreproducible findings.- The paper empirically demonstrates the impact of API changes on toxicity score distributions and rankings of models on benchmarks like RealToxicityPrompts and HELM. It shows that toxicity scores and model rankings can shift significantly when rescored on a newer API version.- The authors replicate results from recent toxicity mitigation papers and find that even studies from late 2022 were affected when rescored on the latest Perspective API. This indicates issues reproducing comparative evaluations.- Overall, the use of black-box commercial APIs with frequent silent updates introduces reproducibility issues and potential for biased conclusions in toxicity evaluation research. The paper recommends more structured evaluation and transparency around scoring to mitigate these problems.In summary, the key problem is the lack of versioning and communication around API changes, which impacts the reproducibility of toxicity evaluation research that relies on inherited scores. The paper demonstrates and raises awareness of this issue through empirical analysis.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the key points from the paper:The paper empirically shows that relying on black-box commercial APIs like Perspective for toxicity evaluation in research poses challenges for reproducibility, as unseen API updates affect toxicity scores, rankings of models, and conclusions of techniques when evaluations are compared over time.
