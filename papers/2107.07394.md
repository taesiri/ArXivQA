# [Explore and Control with Adversarial Surprise](https://arxiv.org/abs/2107.07394)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the introduction, the central research question seems to be: How can we develop an unsupervised reinforcement learning method that leads to the emergence of meaningful and complex behaviors in high-dimensional, stochastic environments?The key hypotheses appear to be:1) A good unsupervised RL method should balance exploration and control. Neither pure novelty-seeking nor pure surprise minimization succeeds on their own. 2) Formulating the learning problem as a multi-agent competition can provide a mechanism for driving the emergence of complexity.3) An information-theoretic objective focused on maximizing/minimizing surprise provides a general and principled way to achieve exploration and control in stochastic environments.4) The proposed method, Adversarial Surprise, will outperform prior novelty-seeking, surprise minimization, and multi-agent competition methods, and lead to better coverage and more meaningful learned behaviors in complex, stochastic environments.Does this seem like an accurate high-level summary of the core research question and hypotheses? Let me know if you would like me to clarify or expand on any part of the summary.


## What is the main contribution of this paper?

Based on my reading, the main contributions of this paper appear to be:1. The proposal of a new unsupervised reinforcement learning algorithm called Adversarial Surprise (AS). AS is based on an adversarial game between two policies - an Explore policy that seeks to maximize observation entropy/surprise, and a Control policy that tries to minimize it. 2. A theoretical analysis showing that under certain assumptions, AS can fully explore the latent state space of stochastic Block MDP environments. This addresses limitations of prior surprise maximizing or minimizing methods in stochastic settings.3. Empirical demonstrations that AS can effectively explore stochastic environments and attain better coverage than methods like RND, ASP, and SMiRL.4. Experiments showing AS can learn meaningful emergent behaviors and achieve good zero-shot transfer performance in complex environments like Atari and VizDoom without any reward signal. This indicates it acquires useful skills, outperforming novelty seeking, surprise minimizing, and adversarial baselines. 5. Results illustrating how the adversarial competition leads to the emergence of increasingly complex skills in phases, transitioning through several qualitatively distinct behaviors over the course of training.In summary, the main contribution seems to be the proposal and empirical evaluation of the Adversarial Surprise algorithm, including theoretical and experimental support showing it is effective for exploration and skill discovery in stochastic settings compared to alternative approaches. The emergent curriculum results also help illustrate how the adversarial competition gives rise to progressively more sophisticated behaviors.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper proposes a new unsupervised reinforcement learning method called Adversarial Surprise, which uses a two-player game between policies that try to maximize and minimize surprise/entropy over observations in order to drive the emergence of complex behaviors and enable effective exploration of stochastic, partially observed environments.


## How does this paper compare to other research in the same field?

Here are some ways this paper compares to other research in unsupervised reinforcement learning:- It builds on ideas from surprise minimization and the free energy principle like SMiRL, but incorporates surprise minimization into a two-player game to address limitations like the dark room problem. - It uses a multi-agent competition like Asymmetric Self Play (ASP) to drive emergence of complexity, but formulates the objective in terms of general information theoretic quantities instead of goal reaching, allowing it to work well in stochastic environments where ASP fails.- It aims to balance exploration and control, avoiding issues in prior work of becoming stuck in high entropy "noisy TV" states or low entropy "dark room" states.- It provides a theoretical analysis showing the method maximizes coverage of the state space in block MDPs, unlike other multi-agent or novelty seeking methods.- Empirically it shows better exploration and state space coverage compared to novelty-seeking methods like RND and multi-agent methods like ASP.- It demonstrates better zero-shot transfer performance in Atari and VizDoom compared to novelty seeking (RND), surprise minimization (SMiRL), and adversarial/multi-agent methods (ASP, AGAC).To summarize, the key differences are:1) Balancing exploration and control to avoid limitations of pure novelty seeking or surprise minimization2) Being formulated in terms of general information theory objectives rather than goal reaching specifically3) Providing theoretical guarantees on coverage in block MDPs4) Empirically demonstrating improved exploration, coverage, and transfer over similar methods.Let me know if you would like me to expand on any of these points!


## What future research directions do the authors suggest?

The paper does not explicitly suggest future research directions. It focuses on presenting the Adversarial Surprise algorithm and evaluating its performance on exploration, zero-shot transfer, and emergence of complex behaviors in stochastic block MDPs, Atari games, and ViZDoom. Based on the paper's findings and limitations, some potential future research directions could include:- Evaluating Adversarial Surprise on a broader range of environments beyond block MDPs, Atari, and ViZDoom to further demonstrate generality.- Combining Adversarial Surprise with hierarchical RL by training a meta-policy to decide when to invoke the Explore vs Control sub-policies. This could help balance exploration and exploitation.- Using Adversarial Surprise for skill acquisition in complex domains like robotics, where the unsupervised behaviors could facilitate downstream task learning.- Extending the theoretical analysis beyond block MDPs to characterize the conditions under which Adversarial Surprise provably maximizes state coverage. - Comparing Adversarial Surprise to a wider range of unsupervised RL techniques like DIAYN, EMPOWERMENT, etc.- Studying whether the skills and exploration strategies learned by Adversarial Surprise transfer to unseen tasks, not just within the same environment.- Developing extensions of Adversarial Surprise that combine intrinsic and extrinsic rewards, rather than being purely unsupervised.So in summary, the main directions are: 1) broader evaluation, 2) hierarchical extensions, 3) real-world applications like robotics, 4) more general theory, 5) comparison to more baselines, 6) studying transfer, and 7) combinations with extrinsic rewards.


## Summarize the paper in one paragraph.

Here is a one paragraph summary of the key points from the paper:The free energy principle proposes that biological agents minimize surprise or state entropy to remain in safe, stable states. The surprise-minimizing RL algorithm SMiRL implements this idea, and learns behaviors that reduce entropy in stochastic environments, even without external rewards. However, SMiRL struggles in partially observed environments, where it can fall prey to the "dark room problem" - remaining in predictable but uninteresting parts of the state space and not learning meaningful behaviors. The proposed method, Adversarial Surprise (AS), builds on surprise minimization but incorporates it into a two-player game to alleviate this issue. In AS, one policy tries to minimize surprise while another policy acts to maximize surprise for the first, forcing it out of dark rooms. Experiments show AS explores more thoroughly and learns more complex skills than SMiRL and other baselines in stochastic, partially observed environments.


## Summarize the paper in two paragraphs.

Here is a summary of the key points from the paper in two paragraphs:The paper proposes a new unsupervised reinforcement learning method called Adversarial Surprise (AS) that is designed to balance exploration and control. AS is based on an adversarial game between two policies - an Explore policy that seeks to maximize surprise, and a Control policy that aims to minimize surprise. The Explore policy puts the agent into novel situations, while the Control policy must recover by manipulating the environment into more predictable states. By competing over observation entropy, the two policies drive increasingly complex exploration and control behavior. The authors provide a theoretical analysis showing that AS can fully explore the state space of stochastic block MDPs, overcoming limitations like the "noisy TV" and "dark room" problems. Empirically, they demonstrate that AS outperforms prior unsupervised RL methods like RND, SMiRL and ASP on measures of state space coverage and zero-shot transfer in Atari/VizDoom environments. The emergent behaviors learned by AS exhibit clear transitions in complexity, eventually solving challenging exploration/control problems. Overall, AS offers a way to balance exploration and control for unsupervised learning without requiring hand-crafted rewards.
