# [Explore and Control with Adversarial Surprise](https://arxiv.org/abs/2107.07394)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the introduction, the central research question seems to be: How can we develop an unsupervised reinforcement learning method that leads to the emergence of meaningful and complex behaviors in high-dimensional, stochastic environments?The key hypotheses appear to be:1) A good unsupervised RL method should balance exploration and control. Neither pure novelty-seeking nor pure surprise minimization succeeds on their own. 2) Formulating the learning problem as a multi-agent competition can provide a mechanism for driving the emergence of complexity.3) An information-theoretic objective focused on maximizing/minimizing surprise provides a general and principled way to achieve exploration and control in stochastic environments.4) The proposed method, Adversarial Surprise, will outperform prior novelty-seeking, surprise minimization, and multi-agent competition methods, and lead to better coverage and more meaningful learned behaviors in complex, stochastic environments.Does this seem like an accurate high-level summary of the core research question and hypotheses? Let me know if you would like me to clarify or expand on any part of the summary.
