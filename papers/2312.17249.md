# [Do Androids Know They're Only Dreaming of Electric Sheep?](https://arxiv.org/abs/2312.17249)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem: 
- Detecting hallucinations (statements not grounded in given knowledge sources) in language model responses is important for grounded language generation tasks. 
- Doing this evaluation by generating responses and then analyzing them separately is inefficient.
- Prior work has not explored using internal language model representations during decoding to detect hallucinations.

Method:
- The authors create labeled datasets of language model responses with human-annotated minimal spans of hallucination over 3 tasks: abstractive summarization, knowledge-grounded dialogue, and data-to-text generation.
- They collect both organic hallucinations (from sampled model responses) and synthetic hallucinations (by editing inputs/outputs).
- They train probes (linear classifiers) on language model decoder hidden states to predict which tokens are part of a hallucinatory span. 
- Several probe variants are explored, including ensembles over layers and hidden state types (attention, feed-forward).

Key Findings:
- Probes can match or exceed baselines that analyze only surface responses, showing probing is efficient for evaluation.
- Synthetic hallucinations don't transfer as well to detecting organic hallucinations.  
- Middle layers show the strongest hallucination signals. Feed-forward states are most informative.
- Extrinsic hallucinations have more salient internal signals than intrinsic ones.
- Probe accuracy generalizes across model sizes but not across tasks.

In summary, the work demonstrates that probing decoder hidden states is a promising approach for detecting hallucinations during language model decoding, yielding insights into where hallucination signals manifest within transformers. The method could be used to avoid generating ungrounded responses.
