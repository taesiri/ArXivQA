# [StableVideo: Text-driven Consistency-aware Diffusion Video Editing](https://arxiv.org/abs/2308.09592)

## What is the central research question or hypothesis that this paper addresses?

From my understanding, the central research question this paper tries to address is:How can we apply text-driven diffusion models to edit videos while maintaining temporal consistency in the geometry and appearance of objects? The key challenges are:1) Diffusion models struggle to modify objects in videos while keeping their appearance consistent across frames.2) Existing diffusion models lack temporal dependency constraints to generate objects with consistent appearance over time. The paper proposes a consistency-aware diffusion video editing approach called StableVideo to tackle these challenges. The main hypotheses are:1) Introducing an inter-frame propagation mechanism can enable diffusion models to generate objects with consistent geometry across frames.2) Leveraging the concept of layered representations (like neural layered atlases) can help propagate edited content between frames to achieve temporal consistency.So in summary, the central research question is how to enable text-driven diffusion models to edit videos with geometric and temporal consistency, which the paper addresses through a novel inter-frame propagation mechanism and aggregation framework built on layered representations. The key hypotheses are around propagating information between frames and using layered decompositions.


## What is the main contribution of this paper?

Based on my understanding, the main contributions of this paper are:1. It proposes a novel approach for diffusion video editing that can achieve temporal consistency. Specifically, it introduces an inter-frame propagation mechanism and an atlas aggregation network on top of existing text-driven diffusion models. 2. The inter-frame propagation mechanism enables the diffusion model to generate foreground objects with consistent geometry across time by considering both structural and appearance information from previous frames.3. The atlas aggregation network helps maintain temporal coherence by aggregating the edits made on key frames to generate edited atlas layers. This leverages the concept of layered atlas approaches for video propagation.4. Together, these components allow manipulating the appearance of objects in videos through text prompts while preserving geometric and temporal consistency. 5. The paper demonstrates superior qualitative and quantitative results compared to prior arts in video editing like Tune-A-Video and Text2LIVE. The approach also has lower complexity than video diffusion models.In summary, the key innovation is enabling diffusion models for consistent video editing by propagating information across frames and aggregating edits into atlas layers. This addresses a significant challenge in applying these powerful generative models to practical video editing scenarios.
