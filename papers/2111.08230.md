# [Selective Ensembles for Consistent Predictions](https://arxiv.org/abs/2111.08230)

## What is the central research question or hypothesis that this paper addresses?

This paper addresses the question of how to mitigate inconsistent model behavior across inconsequential changes in the training environment. The key hypothesis is that using selective ensembling, a modeling approach that allows ensembles to abstain from prediction when there is insufficient confidence in the consensus prediction, can provide bounded prediction inconsistency over randomness in the training pipeline. The main contributions are:1. Demonstrating that deep models exhibit inconsistent predictions and feature attributions across small changes like random initialization and leave-one-out training set differences. 2. Introducing selective ensembling, which uses statistical hypothesis testing on the predictions of an ensemble's constituent models to determine when to abstain.3. Proving bounds on the disagreement rate between selective ensembles.4. Empirically showing selective ensembles achieve zero disagreements on several benchmark datasets while maintaining reasonable abstention rates.So in summary, the paper hypothesizes that selective ensembling can mitigate inconsistencies arising from randomness during training, by allowing the ensemble to abstain when there is no clear consensus among the constituents. The theoretical analysis and experiments support this claim.


## What is the main contribution of this paper?

This paper proposes a new method called selective ensembling to address the problem of prediction instability in deep learning models. The key contributions are:1. It shows that deep learning models can produce inconsistent predictions and feature attributions when trained with small differences like random seeds or leave-one-out data perturbations. This suggests the decision process of these models is sensitive to minor training variations.2. It introduces selective ensembling, which takes the majority vote of predictions from an ensemble of models but abstains if the vote is not statistically significant. This provides a probabilistic guarantee on matching the "true" majority prediction.3. It proves bounds on the disagreement rate between selective ensembles and on the variance component of their error. This theoretically shows selective ensembling promotes consistency.4. Empirically, it demonstrates selective ensembling consistently predicts all points on several datasets across model variations while maintaining reasonable abstention rates below 5%. It also shows improved consistency in feature attributions.In summary, selective ensembling is a novel approach to address instability in deep learning models by leveraging hypothesis testing. It provides theoretical and empirical evidence that the method produces consistent predictions and explanations across minor training differences. The key insight is allowing the ensemble to abstain when uncertain improves consistency.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper introduces selective ensembling, a technique that uses hypothesis testing on the predictions of an ensemble of models to bound disagreement rates and provide consistent predictions even when constituent models exhibit instability due to small changes in training.


## How does this paper compare to other research in the same field?

This paper introduces a new technique called selective ensembling to mitigate inconsistent predictions and explanations from deep learning models trained with different random seed or leave-one-out data splits. Here are some key ways it compares to related work:- Prediction Inconsistency: Prior work has shown deep models can be inconsistent across small changes in training, even models with similar accuracy. This paper confirms that finding and proposes selective ensembling to bound disagreement rates.- Explanation Inconsistency: Prior work has studied instability of explanations, but mainly in an adversarial context. This paper shows gradient-based explanations can be inconsistent naturally across model retraining. Ensembling helps stabilize explanations too.- Ensembling: Traditional ensembling reduces variance but doesn't eliminate inconsistency. This paper adapts ensembling with ideas from randomized smoothing to guarantee consistency.- Uncertainty Estimation: Estimating uncertainty could identify inconsistent predictions to abstain on, but no guarantee. This paper provides guaranteed bounds. Also uncertainty != inconsistency.- Conformal Inference: Identifies points differing from training data, while this paper targets consistency over a known model distribution. Both could help identify inconsistent points though.- Objectives: Most prior work aims to maximize accuracy, while this paper targets consistency, which may be preferred in high-stakes applications.So in summary, it offers a new take on ensembling focused on consistency, with theoretical guarantees, and empirically demonstrates effectiveness on both predictions and explanations. The consistency objective itself is also novel compared to typical accuracy goals.


## What future research directions do the authors suggest?

The paper suggests several promising directions for future research:- Developing more robust algorithms and theories for selective prediction. The paper introduces selective ensembles as one method, but more work is needed to understand when and why selective prediction can improve reliability.- Exploring selective prediction in broader contexts. The paper focuses on consistency in deep learning models, but selective prediction may be useful in other settings like robotics. - Studying social impacts of selective prediction. Selective prediction introduces new issues around transparency and fairness that require investigation.- Connecting selective prediction to other areas like conformal prediction and uncertainty quantification. There may be fruitful links between the goals of selective prediction and techniques in these other fields.- Implementing selective prediction in real-world systems. More deployment experience will reveal strengths and weaknesses of selective prediction in practice.- Developing user interfaces for selective prediction. New interface designs could help users understand and interact with selective classifiers.In summary, the paper proposes selective prediction as a way to improve model reliability but notes many open questions around its theory, applications, social impacts, connections to related work, real-world usage, and interface design. Advancing understanding in these areas is critical for realizing the potential benefits of selective prediction across machine learning.


## Summarize the paper in one paragraph.

Here is a one paragraph summary of the paper "Selective Ensembles for Consistent Predictions":The paper introduces a technique called selective ensembling to address the problem of inconsistent predictions and explanations from deep learning models trained on similar data. It first shows theoretically and empirically that deep models can produce very different predictions and feature attributions for the same input despite only small differences in training, like random seed or leaving one datapoint out. To mitigate this, selective ensembling runs a statistical test on the predictions from an ensemble of models to determine if there is a clear majority class prediction for an input. If so, it outputs the majority vote, but if the vote is too close, it abstains from prediction. This allows selective ensembles to guarantee bounded inconsistency between their predictions. Experiments show selective ensembles eliminate inconsistent predictions on several datasets while maintaining reasonable abstention rates. Ensembling is also shown to improve consistency of feature attributions. Overall, selective ensembling provides a way to rigorously control variability in deep models.


## Summarize the paper in two paragraphs.

Here is a two paragraph summary of the paper "Selective Ensembles for Consistent Predictions":The paper addresses the problem of inconsistency in deep learning models. Specifically, it shows that deep models trained in slightly different ways, such as with different random initialization or small changes to the training data, can produce inconsistent predictions and explanations on individual data points. This is problematic for real-world deployment of models. To address this, the authors propose using "selective ensembles". A selective ensemble runs multiple models on a data point and only makes a prediction if there is statistical agreement among the models. Otherwise, it abstains. The key theoretical result is that selective ensembles can guarantee that the probability of disagreement with the true majority prediction, or between two selective ensembles, is bounded. Empirically, the authors show on several datasets that selective ensembles of just 10 models produce zero inconsistently predicted points across different training conditions. They also demonstrate improved stability in gradient-based feature attributions compared to individual models. Overall, selective ensembles provide a way to mitigate inconsistency, making deep models more reliable for real-world use cases requiring stability. The method strikingly reduces variability from inconsequential factors in model training while maintaining accuracy.
