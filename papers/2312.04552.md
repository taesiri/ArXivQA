# [Generating Illustrated Instructions](https://arxiv.org/abs/2312.04552)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a high-quality paragraph summarizing the key points of the paper:

This paper introduces the novel task of generating "Illustrated Instructions" - customized visual instructions tailored to a user's needs. The authors identify unique requirements for this task: goal faithfulness (images match the overall goal), step faithfulness (each image matches its caption), and cross-image consistency (images in a sequence are coherent). To tackle this, they propose StackedDiffusion, which combines large language models (LLMs) to generate text instructions with text-to-image (T2I) diffusion models to generate corresponding illustrations. Specifically, StackedDiffusion spatially stacks multiple latent representations during training and generation to encourage consistency across the image sequence. Extensive experiments demonstrate that StackedDiffusion convincingly outperforms all baselines and even matches human performance in 30% of cases. Notably, by combining flexible text generation and tailored visuals, StackedDiffusion enables new applications like personalized instructions, error correction, and goal suggestions - far beyond what static instructional articles on the web can provide. The method thus represents an important step towards models that can not just tell users how to accomplish tasks, but also show them.
