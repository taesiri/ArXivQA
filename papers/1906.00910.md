# [Learning Representations by Maximizing Mutual Information Across Views](https://arxiv.org/abs/1906.00910)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is: How can we learn useful visual representations in a self-supervised manner by maximizing mutual information between features extracted from multiple views of the same data? The key ideas proposed to address this question are:- Generate multiple views of each image by applying different augmentations/croppings. - Maximize mutual information between global features extracted from one view and local features extracted from another view of the same image. This forces the model to learn representations that capture high-level semantic factors that are invariant across the different views.- Predict features at multiple scales simultaneously rather than just between global and local features.- Use a more powerful encoder architecture based on ResNet.- Extend the model to use mixture-based representations, where segmentation-like behavior emerges.The central hypothesis is that by maximizing mutual information between features extracted from multiple views of the same underlying data, the model will learn representations that capture high-level semantic factors of variation in the data. The experiments aim to demonstrate that representations learned this way transfer well to downstream tasks and outperform prior self-supervised methods.In summary, the key innovation is the multi-view mutual information maximization approach to self-supervised representation learning. The paper aims to demonstrate the effectiveness of this idea empirically across several standard datasets.


## What is the main contribution of this paper?

The main contribution of this paper is proposing a self-supervised approach to learn image representations by maximizing mutual information between features extracted from multiple views of the same image. Specifically, the paper makes the following key contributions:- Introduces Augmented Multiscale Deep InfoMax (AMDIM), an extension of Deep InfoMax that maximizes mutual information between features from multiple augmented views of an image sampled at different scales.- Predicts features across independently augmented copies of each image to encourage representations invariant to data augmentations.- Predicts features simultaneously across multiple scales to capture cross-scale dependencies. - Uses a more powerful encoder architecture based on ResNet that controls for receptive field overlap.- Extends the model to mixture-based representations, where segmentation emerges as a natural side effect.- Achieves state-of-the-art results on CIFAR10, CIFAR100, STL10, ImageNet, and Places205 using standard linear evaluation protocol, improving over prior self-supervised methods by a significant margin.In summary, the key innovation is in maximizing mutual information between features extracted from multiple views and scales of the same image in a computationally tractable manner to learn useful representations without any manual supervision. The proposed AMDIM model outperforms prior work on standard self-supervised learning benchmarks.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points in this paper:The paper proposes a self-supervised learning approach based on maximizing mutual information between features extracted from multiple augmented views of images, achieving state-of-the-art results on CIFAR10, CIFAR100, STL10, ImageNet and Places205 datasets.


## How does this paper compare to other research in the same field?

Here are some thoughts on how this paper compares to other research in the field of self-supervised representation learning:- The main idea of maximizing mutual information (MI) between different views of the data is similar to other recent methods like CPC and DIM. However, this paper takes the MI maximization further by doing it across augmented views and multiple scales simultaneously.- The encoder architecture is based on ResNet, which is a common backbone, but modified in some key ways like controlling receptive field size. Many recent methods use standard architectures like ResNet or VGG.- The results significantly advance state-of-the-art across several standard datasets. On ImageNet linear evaluation, this method reaches 68.1% compared to the previous best of 55.4% by CPC. It also shows strong transfer learning results on Places205.- The computational requirements are reasonable, with models training on 4-8 GPUs. Some other recent self-supervised methods require much larger compute resources.- The method does not rely on a specialized loss like contrastive loss, which some other approaches require. The noise contrastive estimation bound provides a more general way to maximize mutual information.- The mixture model extension is unique and shows intriguing emergent segmentation behavior, even though not tuned for ImageNet.Overall, I'd say this paper makes important contributions over prior work by maximizing MI more thoroughly with augmented views and multiple scales, achieving much stronger results, with reasonable compute requirements. The results significantly advance the state-of-the-art in self-supervised representation learning. The mixture model extension is also novel and hints at future work on learning disentangled representations.
