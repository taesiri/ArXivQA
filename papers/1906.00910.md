# [Learning Representations by Maximizing Mutual Information Across Views](https://arxiv.org/abs/1906.00910)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is: How can we learn useful visual representations in a self-supervised manner by maximizing mutual information between features extracted from multiple views of the same data? The key ideas proposed to address this question are:- Generate multiple views of each image by applying different augmentations/croppings. - Maximize mutual information between global features extracted from one view and local features extracted from another view of the same image. This forces the model to learn representations that capture high-level semantic factors that are invariant across the different views.- Predict features at multiple scales simultaneously rather than just between global and local features.- Use a more powerful encoder architecture based on ResNet.- Extend the model to use mixture-based representations, where segmentation-like behavior emerges.The central hypothesis is that by maximizing mutual information between features extracted from multiple views of the same underlying data, the model will learn representations that capture high-level semantic factors of variation in the data. The experiments aim to demonstrate that representations learned this way transfer well to downstream tasks and outperform prior self-supervised methods.In summary, the key innovation is the multi-view mutual information maximization approach to self-supervised representation learning. The paper aims to demonstrate the effectiveness of this idea empirically across several standard datasets.


## What is the main contribution of this paper?

The main contribution of this paper is proposing a self-supervised approach to learn image representations by maximizing mutual information between features extracted from multiple views of the same image. Specifically, the paper makes the following key contributions:- Introduces Augmented Multiscale Deep InfoMax (AMDIM), an extension of Deep InfoMax that maximizes mutual information between features from multiple augmented views of an image sampled at different scales.- Predicts features across independently augmented copies of each image to encourage representations invariant to data augmentations.- Predicts features simultaneously across multiple scales to capture cross-scale dependencies. - Uses a more powerful encoder architecture based on ResNet that controls for receptive field overlap.- Extends the model to mixture-based representations, where segmentation emerges as a natural side effect.- Achieves state-of-the-art results on CIFAR10, CIFAR100, STL10, ImageNet, and Places205 using standard linear evaluation protocol, improving over prior self-supervised methods by a significant margin.In summary, the key innovation is in maximizing mutual information between features extracted from multiple views and scales of the same image in a computationally tractable manner to learn useful representations without any manual supervision. The proposed AMDIM model outperforms prior work on standard self-supervised learning benchmarks.
