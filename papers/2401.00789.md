# [Retrieval-Augmented Egocentric Video Captioning](https://arxiv.org/abs/2401.00789)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper "Retrieval-Augmented Egocentric Video Captioning":

Problem:
- Understanding human actions from egocentric videos recorded from a first-person view is challenging. 
- Prior work has focused on representation learning on egocentric videos only, overlooking the potential benefit of leveraging the large corpus of third-person (exocentric) videos.

Proposed Solution:
- Develop EgoInstructor, a retrieval-augmented multimodal captioning model that automatically retrieves semantically relevant third-person instructional videos to enhance the video captioning of egocentric videos.

Key Technical Contributions:
- Devise an automatic pipeline to discover ego-exo video pairs from distinct large-scale egocentric and exocentric datasets by aligning their captions.
- Train a cross-view retrieval module with a novel EgoExoNCE loss that pulls egocentric and exocentric video features closer by aligning them to shared text features describing similar actions.
- Develop a multimodal captioning model that takes the egocentric video and retrieved exocentric videos+captions as references to generate the ego-video caption.

Key Results:
- The cross-view retrieval module demonstrates superior performance across 7 benchmarks including video-text, video-video retrieval and multiple choice tasks.
- For egocentric video captioning on Ego4d, EgoInstructor shows significant improvements by leveraging exocentric instructional videos, outperforming prior captioning methods.
- Qualitative results demonstrate EgoInstructor generates more accurate captions by incorporating relevant information from retrieved third-person videos.

In summary, the paper explores a retrieval-augmented approach to transfer knowledge from exocentric to egocentric videos and shows the benefit for enhancing egocentric video understanding. The core technical novelty lies in the cross-view representation learning and multimodal captioning model design.
