# [Overview of the PromptCBLUE Shared Task in CHIP2023](https://arxiv.org/abs/2312.17522)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem: 
- Evaluating capabilities of large language models (LLMs) on medical natural language processing (NLP) tasks in Chinese is an important research problem. 
- Existing benchmarks like CBLUE have limitations such as not supporting prompt-based fine-tuning of LLMs.

Proposed Solution:
- The authors propose PromptCBLUE, an extensive prompt-based benchmark covering diverse medical NLP tasks organized into 5 groups - information extraction, text classification, inference, symptom status understanding, content generation.  
- PromptCBLUE reformulates CBLUE tasks into prompt-response format to evaluate in-context learning and prompt-based fine-tuning. 
- Two tracks are held - parameter-efficient prompt tuning track to evaluate multi-task prompt tuning methods; in-context learning track to evaluate reasoning skills.

Shared Task & Results:
- PromptCBLUE challenge held with 600 participating teams shows top tuned LLMs can achieve over 70% on test set but in-context learning caps at 40%.
- Winning methods use similarity based demonstration retrieval for in-context learning; Low-rank adaptation with training techniques like quantization and sample balancing for tuning.

Main Contributions:
- Creation of extensive Chinese medical prompt benchmark covering diverse realistic tasks.
- Comprehensive evaluation of capabilities of LLMs for medical NLP via prompt-based learning.
- Analysis of state-of-the-art methods for parameter-efficient tuning and in-context learning of LLMs.
- Insights into design of medical NLP systems using large pretrained models.

In summary, the paper introduces PromptCBLUE to facilitate research into adapting powerful LLMs for practical Chinese medical NLP applications via prompt-based learning. The benchmark and shared task provide analysis of current limits of LLMs in this domain.


## Summarize the paper in one sentence.

 This paper provides an overview of the PromptCBLUE shared task, a large-scale Chinese medical prompt tuning benchmark, held at the CHIP-2023 conference, analyzing the winning methods from the parameter-efficient fine-tuning track and in-context learning track.


## What is the main contribution of this paper?

 This paper mainly provides an overview of the PromptCBLUE shared task held at the CHIP-2023 conference. Its main contributions are:

1) Introducing the PromptCBLUE benchmark, which is the first large-scale Chinese medical prompt tuning benchmark converted from the CBLUE dataset. It contains 18 sub-tasks across 5 cohorts covering various medical NLP tasks.

2) Providing an overview of the PromptCBLUE shared task, which attracted over 600 participating teams from industry and academia. The shared task had two tracks - a parameter-efficient fine-tuning (PEFT) track and an in-context learning (ICL) track.

3) Analyzing the techniques used by the winning teams in both tracks, including model selections, data augmentation strategies, parameter-efficient fine-tuning methods for the PEFT track, and demonstration selection strategies for the ICL track.

4) Reviewing the overall results and success of the shared task, which helped advance research into applying large language models for medical NLP.

In summary, this paper mainly contributes by documenting and analyzing the PromptCBLUE benchmark, dataset, and associated shared task, providing a review of techniques and results that can inform future work.


## What are the keywords or key terms associated with this paper?

 Based on my review of the paper content, the key terms associated with this paper include:

PromptCBLUE benchmark, large language models, medical natural language processing, parameter efficient fine-tuning, in-context learning, prompt tuning, in-context learning, demonstration selection, Chinese LLMs


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the methods proposed in this paper:

1. The paper mentions using chain-of-thought (COT) prompting for some complex information extraction tasks like CMeIE. Can you explain in more detail how COT prompting works for these tasks and why it is beneficial? 

2. When augmenting the training data, the paper mentions randomly replacing tokens with <unk> for some sentence classification tasks. What is the motivation behind this data augmentation strategy? How does it improve model robustness?

3. For the parameter-efficient fine-tuning methods, why does the paper state that LoRA has become the most popular approach, especially with the rise of large language models? What specific advantages does LoRA have over other methods?

4. The paper analyzes differences between the winning methods for the PEFT and ICL tracks. What were some key differences in techniques between these two tracks and why were certain methods more suitable for one track over the other?

5. When selecting demonstrations for the ICL track, the paper discusses similarity-based retrieval and a knapsack-based selection strategy. Can you elaborate on how these demonstration selection strategies work and what are their tradeoffs?  

6. For the knapsack-based demonstration selection, how exactly is the knapsack formulation applied? What constraints need to be satisfied and what is being optimized?

7. The paper states that different sentence embedding models were used by teams for retrieving similar demonstrations in the ICL track. What types of models were used and what are their advantages and disadvantages?

8. When augmenting the PEFT track training data, how exactly were the prompt templates used to incorporate additional CBLUE benchmark data? Were any changes/modifications made to the templates?

9. For the data augmentation in the PEFT track, what percentage of tokens were replaced by <unk> for the sentence classification tasks? Was any analysis done on the optimal masking percentage? 

10. The paper mentions quantization techniques like QLoRA were used to reduce memory requirements during PEFT track fine-tuning. Can you explain how these quantization techniques work and quantify the memory savings achieved?
