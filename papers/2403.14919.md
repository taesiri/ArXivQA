# [Hierarchical Skip Decoding for Efficient Autoregressive Text Generation](https://arxiv.org/abs/2403.14919)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Autoregressive decoding is commonly used for text generation with pre-trained language models, but it is computationally expensive during inference. 
- Reducing computational workload while maintaining text quality is a major challenge.

Method: 
- The paper proposes a novel decoding strategy called Hierarchical Skip Decoding (HSD).  
- HSD adaptively skips transformer layers in a hierarchical manner based on the current sequence length. This allocates computation dynamically.
- Unlike other methods requiring additional components, HSD is a flexible plug-and-play strategy easily applied to any pre-trained language model.

Experiments:
- HSD is evaluated on 5 text generation datasets with two models - GPT-2 and Phi-2.
- It significantly outperforms existing methods like CALM and SkipDecode in balancing efficiency and quality.  
- With ~50% of layers skipped, HSD sustains 90% of text quality compared to full autoregressive decoding.

Main Contributions:
- Introduction of HSD that combines hierarchical layer skipping and scheduled computation budget for efficient text generation.
- Comprehensive experiments demonstrating HSD's superior performance over competitive methods. 
- A flexible plug-and-play strategy to reduce inference cost of autoregressive models while maintaining high text quality.

In summary, the paper proposes HSD, a novel decoding approach for efficient text generation that hierarchically skips layers based on sequence length. Experiments on multiple models and datasets show HSD maintains high quality with up to 50% layer reduction versus full autoregressive decoding.
