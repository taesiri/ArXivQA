# [LACoS-BLOOM: Low-rank Adaptation with Contrastive objective on 8 bits   Siamese-BLOOM](https://arxiv.org/abs/2305.06404)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is: How can we efficiently fine-tune large multilingual language models like BLOOM to generate semantically meaningful text embeddings for downstream tasks?The key elements of this research question are:- Efficiently fine-tuning large models: The paper aims to fine-tune models with billions of parameters (up to 7.1 billion for BLOOM) in a computationally efficient manner using only a single GPU. This allows more practical application compared to cluster training. - Multilingual language models: The focus is on models like BLOOM that are pre-trained on many languages, not just English. This allows generating embeddings for broad multilingual applications.- Semantically meaningful embeddings: The goal is to fine-tune BLOOM to produce text embeddings that capture semantic similarity for tasks like retrieval and clustering.- Techniques used: The paper proposes techniques like 8-bit quantization, low-rank adapters, and contrastive learning with Siamese networks to enable efficient in-domain fine-tuning at scale.In summary, the core research question is how to efficiently fine-tune large multilingual models like BLOOM to produce useful semantic embeddings across languages using limited computational resources. The techniques proposed aim to address this question.


## What is the main contribution of this paper?

The main contributions of this paper are:1. Proposing LACoS-BLOOM, a parameter efficient fine-tuning method to generate semantically meaningful multilingual text embeddings from the large language model BLOOM.2. Using 8-bit quantization to reduce the footprint of the large BLOOM model so it can be trained end-to-end on a single GPU machine. 3. Applying a scalable adapter module called LoRA during fine-tuning to only update a small subset of parameters (<1%).4. Enhancing the embeddings with a Siamese network architecture and multiple negative ranking loss to improve semantic textual similarity with limited labeled data.5. Demonstrating significant improvements in performance over baseline methods on semantic textual similarity tasks in both English and multilingual settings. 6. Showing the ability to efficiently fine-tune very large versions of BLOOM (up to 7.1 billion parameters) on a single GPU through the proposed techniques.In summary, the main contribution is proposing an efficient and effective method called LACoS-BLOOM to fine-tune very large multilingual language models to generate high quality semantically meaningful text embeddings for downstream tasks. The techniques allow large models to be adapted with limited compute resources.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper proposes an efficient method called LACoS-BLOOM to extract semantically meaningful multilingual text embeddings from the large language model BLOOM by using 8-bit quantization, low-rank adapters, and a Siamese network with contrastive loss.
