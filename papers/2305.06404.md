# [LACoS-BLOOM: Low-rank Adaptation with Contrastive objective on 8 bits   Siamese-BLOOM](https://arxiv.org/abs/2305.06404)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is: How can we efficiently fine-tune large multilingual language models like BLOOM to generate semantically meaningful text embeddings for downstream tasks?The key elements of this research question are:- Efficiently fine-tuning large models: The paper aims to fine-tune models with billions of parameters (up to 7.1 billion for BLOOM) in a computationally efficient manner using only a single GPU. This allows more practical application compared to cluster training. - Multilingual language models: The focus is on models like BLOOM that are pre-trained on many languages, not just English. This allows generating embeddings for broad multilingual applications.- Semantically meaningful embeddings: The goal is to fine-tune BLOOM to produce text embeddings that capture semantic similarity for tasks like retrieval and clustering.- Techniques used: The paper proposes techniques like 8-bit quantization, low-rank adapters, and contrastive learning with Siamese networks to enable efficient in-domain fine-tuning at scale.In summary, the core research question is how to efficiently fine-tune large multilingual models like BLOOM to produce useful semantic embeddings across languages using limited computational resources. The techniques proposed aim to address this question.
