# [Towards Modeling Learner Performance with Large Language Models](https://arxiv.org/abs/2403.14661)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Knowledge tracing models track how students learn over time by modeling their performance on questions in intelligent tutoring systems. Existing approaches have limitations. 
- The paper explores whether large language models (LLMs), which have shown impressive abilities for pattern recognition and sequence modeling, can offer a new approach for knowledge tracing.

Methodology:
- The authors evaluate two LLM-based approaches on 3 real-world educational datasets:
   1) Zero-shot prompting of GPT-3.5
   2) Fine-tuning LLMs (BERT, GPT-2, GPT-3) by representing student interaction data as text prompts and completions
- They compare LLM performance to naive baselines and standard knowledge tracing models like Bayesian Knowledge Tracing (BKT), Deep Knowledge Tracing (DKT) and others across metrics including AUC, accuracy, RMSE.

Results:
- Fine-tuned LLMs consistently beat zero-shot and all baselines but do not surpass state-of-the-art knowledge tracing models.
- Fine-tuned LLMs perform on par with BKT and their performance improves relative to other models as dataset size grows.
- Model scale impacts effectiveness, with only the largest model (GPT-3) being usable.

Conclusions:
- LLMs show promise as a new family of knowledge tracing models based on their ability to recognize patterns in text representing student data.
- With further refinements to prompting strategies and model architectures, LLM-based approaches could become competitive alternatives for modeling student learning over time.

So in summary, the paper demonstrates LLMs' potential for knowledge tracing despite not achieving state-of-the-art performance, and points to prompts, scale and learning as areas to address in future work.
