# [Comparing Feature Engineering and End-to-End Deep Learning for Autism   Spectrum Disorder Assessment based on Fullbody-Tracking](https://arxiv.org/abs/2311.14533)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a detailed summary paragraph of the key points from the paper:

This paper presents a comparison between feature engineering and end-to-end deep learning models for autism spectrum disorder (ASD) assessment using full-body tracking in a virtual reality environment. The study recruited 81 children aged 3-7 years, with 39 diagnosed with ASD. Participants engaged in 12 distinct motor tasks in VR while being tracked by an RGB-D camera. Handcrafted kinetic features were extracted, and machine learning models were trained and compared to a 3D convolutional neural network tailored for video action recognition. Rigorous nested cross-validation ensured reliable performance estimates. Results showed feature engineering achieved slightly higher accuracy (2.6% on average) in most tasks, while end-to-end learning exhibited lower variability across tasks and more balanced true positive and negative rates. Differences in mean AUC were not statistically significant between approaches, but variance was significantly lower for end-to-end models, indicating greater reliability across contexts. The best end-to-end model surpassed state-of-the-art studies using deep learning for ASD assessment. Overall, while feature engineering can achieve strong accuracy in targeted applications, end-to-end models enable robust ASD detection without handcrafting expertise or task specificity.


## Summarize the paper in one sentence.

 This paper compares feature engineered machine learning models and end-to-end deep learning models for assessing Autism Spectrum Disorder based on full-body tracking in a virtual reality environment across multiple motor tasks, finding that while feature engineered models can achieve higher accuracy on specific tasks, deep learning models demonstrate greater robustness and generalizability across tasks.


## What is the main contribution of this paper?

 Based on my reading of the paper, I would summarize the main contributions as:

1) Introducing a newly trained 3DCNN ResNet tailored for end-to-end kinematic ASD classification, using an existing deep learning architecture for action recognition. 

2) Demonstrating superior performance of both individual machine learning models and the end-to-end model compared to previous state-of-the-art results.

3) Emphasizing model reliability through a dedicated focus on repeated cross-validation techniques, ensuring a reliable and accurate performance estimation.  

4) Showcasing the end-to-end model's capacity for enhanced generalization across various specific domain datasets/tasks. 

In particular, the authors focus on comparing feature engineered models versus end-to-end deep learning, evaluating them across diverse tasks in a VR environment. Their analysis reveals that while feature engineered models can achieve slightly higher accuracy in some tasks, the end-to-end model demonstrates greater reliability and consistency across tasks. The deep learning model is also easier to implement as it does not require hand-crafting features. Overall, this comprehensive analysis and the emphasis on proper validation techniques stand out as the main contributions.


## What are the keywords or key terms associated with this paper?

 Based on my review of the paper, some of the key keywords and terms associated with it are:

- Autism Spectrum Disorder (ASD)
- Kinematic analysis 
- Movement data
- Motor abnormalities
- Feature engineering
- End-to-end 
- Deep Learning
- Spatio-temporal
- Convolutional Neural Network (CNN3D)
- Residual Network (ResNet)

The paper compares feature engineering and end-to-end deep learning models for assessing ASD based on full-body tracking/kinematic data collected from children performing various motor tasks in a virtual reality environment. It utilizes hand-crafted features from the movement data as well as a 3D convolutional neural network architecture to classify subjects. The key focus is on model validation, generalization, and reliability.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in the paper:

1. The paper mentions using a CAVE system for data collection instead of a traditional HMD. What were the specific concerns regarding discomfort in ASD individuals that motivated this choice? How does a CAVE system help address those concerns compared to an HMD?

2. The data preprocessing step involves identifying the participant's body from other people in the scene over time. What assumptions did the authors make about the participant's movement to facilitate this identification process? How robust is this approach if those assumptions are violated? 

3. The feature engineering process aggregates the features of neighboring joints into larger body part groups. What was the rationale behind choosing the specific body part aggregations shown in Figure 2? How sensitive are the results to different aggregation strategies?

4. The paper compares linear and non-linear ML models for the feature engineered approach and found inferior performance for non-linear models. What are some reasons that could explain this outcome? Would using a larger dataset potentially change this result?

5. For the 3DCNN architecture, what considerations influenced the choices of batch size, video length, number of epochs etc? How sensitive is performance to changes in those hyperparameter values? 

6. Data augmentation is performed to introduce variability along the horizontal axis. Why not also introduce variability along other axes? Would that enhance or hurt model generalization?

7. The deep network is at a comparative disadvantage during validation compared to the ML models. What steps could be taken to level the playing field? How might that impact the relative performances?

8. For tasks like object manipulation, deep learning outperforms feature engineering by a large margin. What innate abilities of DL models might explain this result? 

9. The differences in mean AUC between models are not statistically significant. What factors contribute to the high variance that prevents statistically significant conclusions?

10. The study uses a limited sample size. How would using a larger dataset potentially impact the conclusions regarding comparative model performances? What additional validation analyses could be enabled?
