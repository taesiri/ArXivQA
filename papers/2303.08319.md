# [FAQ: Feature Aggregated Queries for Transformer-based Video Object   Detectors](https://arxiv.org/abs/2303.08319)

## What is the central research question or hypothesis that this paper addresses?

 The central research question addressed in this paper is how to improve video object detection performance using Transformer-based models. 

The key hypothesis is that aggregating and enhancing the quality of the queries for the Transformer decoder can help handle issues like feature degradation in videos and improve detection accuracy.

Specifically, the paper proposes and evaluates two main ideas:

1) Vanilla Query Aggregation (VQA): Aggregating queries from neighboring frames using a weighted average based on feature similarity.

2) Dynamic Query Aggregation (DQA): Generating dynamic queries adapted to each frame's features, instead of using fixed random queries. The dynamic queries are aggregated across frames.

The overall goal is to improve video object detection accuracy by focusing on the unique properties of Transformer-based detectors, namely the quer


## What is the main contribution of this paper?

 The main contribution of this paper is proposing a module to aggregate the queries for Transformer-based video object detectors in order to improve their performance on video tasks. Specifically:

- They propose a vanilla query aggregation (VQA) module that aggregates the queries from neighboring frames using weighted averaging to enhance the query representations. 

- They extend VQA to a dynamic query aggregation (DQA) module that generates and aggregates queries based on the features of the input frames rather than using randomly initialized queries. 

- DQA contains basic queries that are randomly initialized and dynamic queries that are generated from the basic queries using a mapping function based on the input frame features.

- During training, they aggregate both dynamic and basic queries but only use the dynamic queries during inference.

- They show DQA can be integrated into various Transformer-based detectors like Deformable DETR and improve their performance on video object detection tasks like ImageNet VID by over 2.4% mAP.

In summary, the key contribution is proposing query aggregation modules specifically designed for Transformer architectures to improve their applicability and performance on video domains. This provides a new perspective compared to prior work on aggregating features or detection results.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the key points from the paper:

The paper proposes a new method called Feature Aggregated Queries (FAQ) to improve Transformer-based video object detectors by aggregating and enhancing the query features from neighboring frames to handle issues like motion blur and occlusion.


## How does this paper compare to other research in the same field?

 Here are a few key points comparing this paper to other research in video object detection:

- This paper focuses on improving query-based object detectors like DETR for video. Most prior work has focused on adapting other types of models like Faster R-CNN or CenterNet to video. 

- The main idea is to aggregate queries across frames to handle motion and occlusion. Other methods typically aggregate features across frames. Aggregating queries is a novel approach.

- The proposed query aggregation modules are model-agnostic and can work with different Transformer detectors. Other video detection methods are often tailored to specific base detectors.

- Both a simple weighted averaging module and more advanced dynamic query generation method are proposed. The dynamic method adapts queries to each frame.

- The method achieves strong results, outperforming prior feature aggregation techniques like TransVOD when added to DETR models. This validates the efficacy of query aggregation.

- The computational overhead of the proposed modules is limited, making them practical. Other techniques like adding a separate temporal encoder can be more costly.

Overall, this query aggregation approach is innovative compared to prior video detection research. Evaluating on multiple DETR models shows the general value of this idea. The results demonstrate aggregating queries can be more effective than aggregating features for handling video challenges. This provides a new direction for adapting Transformer detectors to video analysis.
