# [FAQ: Feature Aggregated Queries for Transformer-based Video Object   Detectors](https://arxiv.org/abs/2303.08319)

## What is the central research question or hypothesis that this paper addresses?

 The central research question addressed in this paper is how to improve video object detection performance using Transformer-based models. 

The key hypothesis is that aggregating and enhancing the quality of the queries for the Transformer decoder can help handle issues like feature degradation in videos and improve detection accuracy.

Specifically, the paper proposes and evaluates two main ideas:

1) Vanilla Query Aggregation (VQA): Aggregating queries from neighboring frames using a weighted average based on feature similarity.

2) Dynamic Query Aggregation (DQA): Generating dynamic queries adapted to each frame's features, instead of using fixed random queries. The dynamic queries are aggregated across frames.

The overall goal is to improve video object detection accuracy by focusing on the unique properties of Transformer-based detectors, namely the quer


## What is the main contribution of this paper?

 The main contribution of this paper is proposing a module to aggregate the queries for Transformer-based video object detectors in order to improve their performance on video tasks. Specifically:

- They propose a vanilla query aggregation (VQA) module that aggregates the queries from neighboring frames using weighted averaging to enhance the query representations. 

- They extend VQA to a dynamic query aggregation (DQA) module that generates and aggregates queries based on the features of the input frames rather than using randomly initialized queries. 

- DQA contains basic queries that are randomly initialized and dynamic queries that are generated from the basic queries using a mapping function based on the input frame features.

- During training, they aggregate both dynamic and basic queries but only use the dynamic queries during inference.

- They show DQA can be integrated into various Transformer-based detectors like Deformable DETR and improve their performance on video object detection tasks like ImageNet VID by over 2.4% mAP.

In summary, the key contribution is proposing query aggregation modules specifically designed for Transformer architectures to improve their applicability and performance on video domains. This provides a new perspective compared to prior work on aggregating features or detection results.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the key points from the paper:

The paper proposes a new method called Feature Aggregated Queries (FAQ) to improve Transformer-based video object detectors by aggregating and enhancing the query features from neighboring frames to handle issues like motion blur and occlusion.
