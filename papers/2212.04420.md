# [Generating Holistic 3D Human Motion from Speech](https://arxiv.org/abs/2212.04420)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading, the central research question this paper addresses is how to generate realistic and diverse 3D holistic body motions, including body pose, hand gestures, and facial expressions, from human speech recordings. 

The key points are:

- The paper proposes an approach called TalkSHOW for speech-to-motion generation. The goal is to take a speech recording as input and generate synchronized 3D body motions as output.

- Existing datasets for this task are limited. So the authors build a new dataset of 3D holistic body meshes with synchronous speech captured from in-the-wild videos.

- The proposed TalkSHOW method models the face, body, and hands separately since they correlate differently with the speech signal. 

- For the face, a simple encoder-decoder model is used to generate facial expressions from speech, aiming to produce accurate lip shapes. 

- For the body and hands, a novel framework based on compositional vector quantized variational autoencoders (VQ-VAEs) and a cross-conditional autoregressive model is proposed to generate more diverse and realistic motions.

- Experiments demonstrate TalkSHOW generates motions that are more realistic, diverse, and synchronized with the speech compared to previous methods and baselines.

In summary, the key hypothesis is that modeling the face, body, and hands separately based on their correlation to speech, using simple encoders-decoders for faces and more complex VQ-VAE and autoregressive models for body/hands, can produce high quality 3D holistic body motions from speech recordings. The experiments aim to validate this hypothesis.
