# [VideoBERT: A Joint Model for Video and Language Representation Learning](https://arxiv.org/abs/1904.01766)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is: How can we learn high-level video representations that capture semantically meaningful and temporally long-range structure in a self-supervised manner, by leveraging both visual and linguistic modalities?The key hypothesis seems to be that by adapting the BERT language model to jointly learn distributions over sequences of visual and linguistic tokens derived from video, the model can discover useful high-level semantic features for video understanding tasks, without needing manual labels.In particular, the paper proposes VideoBERT, which extends BERT to model joint distributions over visual and linguistic sequences. The visual sequences are obtained via vector quantization of pretrained video features, while linguistic sequences come from ASR transcripts. VideoBERT is pretrained on a large dataset of instructional cooking videos from YouTube using masked token prediction and linguistic-visual alignment objectives. The authors then demonstrate how VideoBERT can be applied to video classification and captioning in a zero-shot or transfer learning setting, showing it captures high-level semantics and outperforms prior work. The paper also analyzes benefits of large-scale pretraining data and cross-modal learning.In summary, the core research question is how to learn high-level video representations in a self-supervised cross-modal manner, which VideoBERT aims to address via an extension of the BERT language model to the video domain.


## What is the main contribution of this paper?

The main contribution of this paper is developing VideoBERT (VBERT), a joint model for video and language representation learning. The key ideas are:- Extending BERT to jointly model video and language by representing video as a sequence of discrete visual tokens obtained via vector quantization of video features. - Pretraining VBERT on a large dataset of instructional cooking videos from YouTube (312K videos, 23K hours) using masked language modeling objectives adapted to both visual and linguistic sequences, as well as an additional objective predicting alignment between linguistic and visual sequences.- Demonstrating VBERT can be used in a zero-shot setting for action classification on YouCook2 by querying the model to fill in blanks. VBERT reaches competitive accuracy compared to a supervised baseline without using YouCook2 labels. - Showing VBERT features boost performance of a Transformer model for video captioning on YouCook2. Adding VBERT features improves over all metrics compared to using only S3D features, and combining VBERT + S3D reaches state-of-the-art.- Providing qualitative results showing VBERT learns high-level semantics and long-range temporal dynamics. It can generate plausible visual predictions from text and plausible future events from visual contexts.In summary, the main contribution is developing VideoBERT, a joint vision-language model leveraging BERT, and showing it learns high-level semantic features useful for video understanding tasks in a zero-shot transfer setting. The large-scale pretraining is key to its performance.
