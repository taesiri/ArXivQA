# [VideoBERT: A Joint Model for Video and Language Representation Learning](https://arxiv.org/abs/1904.01766)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is: How can we learn high-level video representations that capture semantically meaningful and temporally long-range structure in a self-supervised manner, by leveraging both visual and linguistic modalities?The key hypothesis seems to be that by adapting the BERT language model to jointly learn distributions over sequences of visual and linguistic tokens derived from video, the model can discover useful high-level semantic features for video understanding tasks, without needing manual labels.In particular, the paper proposes VideoBERT, which extends BERT to model joint distributions over visual and linguistic sequences. The visual sequences are obtained via vector quantization of pretrained video features, while linguistic sequences come from ASR transcripts. VideoBERT is pretrained on a large dataset of instructional cooking videos from YouTube using masked token prediction and linguistic-visual alignment objectives. The authors then demonstrate how VideoBERT can be applied to video classification and captioning in a zero-shot or transfer learning setting, showing it captures high-level semantics and outperforms prior work. The paper also analyzes benefits of large-scale pretraining data and cross-modal learning.In summary, the core research question is how to learn high-level video representations in a self-supervised cross-modal manner, which VideoBERT aims to address via an extension of the BERT language model to the video domain.


## What is the main contribution of this paper?

The main contribution of this paper is developing VideoBERT (VBERT), a joint model for video and language representation learning. The key ideas are:- Extending BERT to jointly model video and language by representing video as a sequence of discrete visual tokens obtained via vector quantization of video features. - Pretraining VBERT on a large dataset of instructional cooking videos from YouTube (312K videos, 23K hours) using masked language modeling objectives adapted to both visual and linguistic sequences, as well as an additional objective predicting alignment between linguistic and visual sequences.- Demonstrating VBERT can be used in a zero-shot setting for action classification on YouCook2 by querying the model to fill in blanks. VBERT reaches competitive accuracy compared to a supervised baseline without using YouCook2 labels. - Showing VBERT features boost performance of a Transformer model for video captioning on YouCook2. Adding VBERT features improves over all metrics compared to using only S3D features, and combining VBERT + S3D reaches state-of-the-art.- Providing qualitative results showing VBERT learns high-level semantics and long-range temporal dynamics. It can generate plausible visual predictions from text and plausible future events from visual contexts.In summary, the main contribution is developing VideoBERT, a joint vision-language model leveraging BERT, and showing it learns high-level semantic features useful for video understanding tasks in a zero-shot transfer setting. The large-scale pretraining is key to its performance.


## How does this paper compare to other research in the same field?

Here are a few key ways this paper on VideoBERT compares to other research on video representation learning:- It focuses on learning high-level semantic features that correspond to actions/events over longer timescales, rather than just low-level features like textures and short-term motion patterns. This allows it to capture more complex events unfolding over minutes.- It leverages language as a source of supervision, by modeling the joint distribution of visual and linguistic tokens derived from video and speech. This allows self-supervised pretraining without needing manual labels.- It adapts the powerful BERT language model to the video domain, allowing it to capture bidirectional context and long-range dependencies for both visual and linguistic sequences. - It uses a simple but effective approach of discretizing video features via vector quantization into "visual words". This encourages learning semantic representations.- It is trained on a large-scale dataset of 312K cooking videos scraped from YouTube, much bigger than previous video datasets like YouCook2. This benefits its performance.- It shows strong results on zero-shot action classification and video captioning. The captioning model outperforms prior state-of-the-art on YouCook2, indicating it learns better features.- It demonstrates visually plausible text-to-video generation capabilities.Overall, the key novelty is in adapting BERT for joint modeling of visual and linguistic sequences in a self-supervised way at scale, enabling learning of high-level video representations. The simple but effective approach allows training complex models on large amounts of unlabeled video data.
