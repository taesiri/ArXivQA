# [B-PROP: Bootstrapped Pre-training with Representative Words Prediction   for Ad-hoc Retrieval](https://arxiv.org/abs/2104.09791)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is:How can we improve upon existing pre-training methods for information retrieval by leveraging powerful contextual language models like BERT?Specifically, the paper proposes a new pre-training method called B-PROP that aims to address limitations of prior methods like PROP. The key ideas behind B-PROP are:- Use BERT's contextual language modeling capabilities to replace the unigram language model used in PROP for sampling representative words from documents. This should allow capturing better document semantics.- Introduce a novel contrastive sampling method inspired by divergence-from-randomness to obtain a more informative distribution over terms for sampling representative words. This contrasts the document term distribution against a collection-level random distribution.- Retrain BERT itself on the tailored pre-training objectives of masked language modeling and representative word prediction based on the improved sampling.The hypothesis is that by improving the sampling of representative words through BERT and the contrastive method, and retraining BERT on these tailored objectives, the pre-trained model will achieve better performance on downstream ad-hoc retrieval tasks. The experiments aim to demonstrate the effectiveness of B-PROP over strong baselines like BERT and PROP.In summary, the key research question is whether the proposed B-PROP pre-training approach can advance state-of-the-art on ad-hoc retrieval by addressing limitations in prior pre-training strategies. The paper hypothesizes the improvements in representative word sampling and retraining BERT will lead to gains.


## What is the main contribution of this paper?

The main contribution of this paper is proposing B-PROP, a bootstrapped pre-training method with representative words prediction for ad-hoc retrieval. Specifically, the key ideas and contributions include:- Leveraging the powerful contextual language model BERT to replace the unigram language model for constructing the representative words prediction (ROP) pre-training task in PROP. This allows sampling more meaningful representative words from documents.- Introducing a novel contrastive sampling method inspired by divergence-from-randomness to compute the informativeness of words based on BERT's attention. This results in a better term distribution for sampling representative words. - Re-training BERT itself with the tailored objectives of masked language modeling and representative words prediction based on the contrastive sampling. - Evaluating B-PROP on 7 standard ad-hoc retrieval datasets and showing that it significantly outperforms strong baselines including BERT and PROP. B-PROP pushes forward the state-of-the-art on these benchmarks.- Demonstrating the effectiveness of B-PROP under low resource settings with limited labeled data. It also shows promising zero-shot retrieval performance.In summary, the key contribution is proposing the B-PROP pre-training framework that advances state-of-the-art ad-hoc retrieval performance by carefully designing pre-training objectives and sampling strategies tailored for IR based on BERT.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper proposes a bootstrapped pre-training method called B-PROP that leverages BERT's contextual language model capabilities and a novel contrastive sampling approach to predict representative words, achieving state-of-the-art performance on a variety of ad-hoc retrieval benchmarks.


## How does this paper compare to other research in the same field?

Here are a few key points comparing this paper to other research on pre-training models for information retrieval:- This paper proposes a new pre-training method called B-PROP that is tailored for ad-hoc retrieval tasks. It builds on prior work like PROP, but uses BERT's contextual language model instead of a unigram language model during pre-training. This allows it to sample more representative words for the pre-training task.- Most prior work has focused on directly applying existing pre-trained language models like BERT to IR tasks. B-PROP and PROP are some of the first attempts to design pre-training objectives specifically for retrieval. - B-PROP achieves new state-of-the-art results on several standard ad-hoc retrieval benchmarks, outperforming strong baselines like BERT and PROP. This demonstrates the effectiveness of tailoring the pre-training objective to the downstream tasks.- The proposed contrastive sampling method in B-PROP is novel compared to prior pre-training methods for IR. It is inspired by divergence-from-randomness and helps B-PROP learn a more informative term distribution.- The authors pretrain B-PROP on both Wikipedia and a large-scale document corpus from MS MARCO. They find that pretrained models perform better when the pretraining corpus is similar to the downstream retrieval corpus.- B-PROP is shown to be data-efficient, achieving strong performance even with very limited labeled data for fine-tuning. This could be useful for real-world IR applications with scarce labeled data.Overall, this paper makes excellent progress on pretraining for ad-hoc retrieval compared to prior works. The tailored pretraining objective and contrastive sampling method seem highly effective based on the results.


## What future research directions do the authors suggest?

Based on my reading of the paper, some of the main future research directions suggested by the authors are:- Exploring the ROP objective for first-stage retrieval. The authors propose using the ROP objective for re-ranking in this work. They suggest exploring using it for first-stage retrieval as well, which could further improve search performance. - Studying the impact of pre-training data scale and domain. The results show pre-training on data similar to the downstream tasks improves performance. The authors suggest studying this impact more systematically.- Applying contrastive learning methods. The contrastive method proposed helps distinguish informative words. The authors suggest exploring contrastive learning more broadly for pre-training.- Low resource and domain adaptation. The authors show B-PROP works well in low resource settings. They suggest exploring domain adaptation, transferring models to new domains with limited labeled data.- New pre-training objectives. While B-PROP improves over prior work, there is opportunity to design even better objectives tailored to IR. Exploring new pre-training tasks is suggested.- Combining objectives and data. The authors use ROP and MLM objectives. Studying combining other objectives and multi-task learning is proposed. Using heterogeneous data sources for pre-training is also suggested.In summary, the main future directions are developing variants of B-PROP itself, like new objectives and contrastive learning; improving low resource performance; and studying how to best leverage pre-training data.
