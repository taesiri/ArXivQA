# [B-PROP: Bootstrapped Pre-training with Representative Words Prediction   for Ad-hoc Retrieval](https://arxiv.org/abs/2104.09791)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question is:

How can we improve upon existing pre-training methods for information retrieval by leveraging powerful contextual language models like BERT?

Specifically, the paper proposes a new pre-training method called B-PROP that aims to address limitations of prior methods like PROP. The key ideas behind B-PROP are:

- Use BERT's contextual language modeling capabilities to replace the unigram language model used in PROP for sampling representative words from documents. This should allow capturing better document semantics.

- Introduce a novel contrastive sampling method inspired by divergence-from-randomness to obtain a more informative distribution over terms for sampling representative words. This contrasts the document term distribution against a collection-level random distribution.

- Retrain BERT itself on the tailored pre-training objectives of masked language modeling and representative word prediction based on the improved sampling.

The hypothesis is that by improving the sampling of representative words through BERT and the contrastive method, and retraining BERT on these tailored objectives, the pre-trained model will achieve better performance on downstream ad-hoc retrieval tasks. The experiments aim to demonstrate the effectiveness of B-PROP over strong baselines like BERT and PROP.

In summary, the key research question is whether the proposed B-PROP pre-training approach can advance state-of-the-art on ad-hoc retrieval by addressing limitations in prior pre-training strategies. The paper hypothesizes the improvements in representative word sampling and retraining BERT will lead to gains.


## What is the main contribution of this paper?

 The main contribution of this paper is proposing B-PROP, a bootstrapped pre-training method with representative words prediction for ad-hoc retrieval. Specifically, the key ideas and contributions include:

- Leveraging the powerful contextual language model BERT to replace the unigram language model for constructing the representative words prediction (ROP) pre-training task in PROP. This allows sampling more meaningful representative words from documents.

- Introducing a novel contrastive sampling method inspired by divergence-from-randomness to compute the informativeness of words based on BERT's attention. This results in a better term distribution for sampling representative words. 

- Re-training BERT itself with the tailored objectives of masked language modeling and representative words prediction based on the contrastive sampling. 

- Evaluating B-PROP on 7 standard ad-hoc retrieval datasets and showing that it significantly outperforms strong baselines including BERT and PROP. B-PROP pushes forward the state-of-the-art on these benchmarks.

- Demonstrating the effectiveness of B-PROP under low resource settings with limited labeled data. It also shows promising zero-shot retrieval performance.

In summary, the key contribution is proposing the B-PROP pre-training framework that advances state-of-the-art ad-hoc retrieval performance by carefully designing pre-training objectives and sampling strategies tailored for IR based on BERT.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the key points from the paper:

The paper proposes a bootstrapped pre-training method called B-PROP that leverages BERT's contextual language model capabilities and a novel contrastive sampling approach to predict representative words, achieving state-of-the-art performance on a variety of ad-hoc retrieval benchmarks.


## How does this paper compare to other research in the same field?

 Here are a few key points comparing this paper to other research on pre-training models for information retrieval:

- This paper proposes a new pre-training method called B-PROP that is tailored for ad-hoc retrieval tasks. It builds on prior work like PROP, but uses BERT's contextual language model instead of a unigram language model during pre-training. This allows it to sample more representative words for the pre-training task.

- Most prior work has focused on directly applying existing pre-trained language models like BERT to IR tasks. B-PROP and PROP are some of the first attempts to design pre-training objectives specifically for retrieval. 

- B-PROP achieves new state-of-the-art results on several standard ad-hoc retrieval benchmarks, outperforming strong baselines like BERT and PROP. This demonstrates the effectiveness of tailoring the pre-training objective to the downstream tasks.

- The proposed contrastive sampling method in B-PROP is novel compared to prior pre-training methods for IR. It is inspired by divergence-from-randomness and helps B-PROP learn a more informative term distribution.

- The authors pretrain B-PROP on both Wikipedia and a large-scale document corpus from MS MARCO. They find that pretrained models perform better when the pretraining corpus is similar to the downstream retrieval corpus.

- B-PROP is shown to be data-efficient, achieving strong performance even with very limited labeled data for fine-tuning. This could be useful for real-world IR applications with scarce labeled data.

Overall, this paper makes excellent progress on pretraining for ad-hoc retrieval compared to prior works. The tailored pretraining objective and contrastive sampling method seem highly effective based on the results.


## What future research directions do the authors suggest?

 Based on my reading of the paper, some of the main future research directions suggested by the authors are:

- Exploring the ROP objective for first-stage retrieval. The authors propose using the ROP objective for re-ranking in this work. They suggest exploring using it for first-stage retrieval as well, which could further improve search performance. 

- Studying the impact of pre-training data scale and domain. The results show pre-training on data similar to the downstream tasks improves performance. The authors suggest studying this impact more systematically.

- Applying contrastive learning methods. The contrastive method proposed helps distinguish informative words. The authors suggest exploring contrastive learning more broadly for pre-training.

- Low resource and domain adaptation. The authors show B-PROP works well in low resource settings. They suggest exploring domain adaptation, transferring models to new domains with limited labeled data.

- New pre-training objectives. While B-PROP improves over prior work, there is opportunity to design even better objectives tailored to IR. Exploring new pre-training tasks is suggested.

- Combining objectives and data. The authors use ROP and MLM objectives. Studying combining other objectives and multi-task learning is proposed. Using heterogeneous data sources for pre-training is also suggested.

In summary, the main future directions are developing variants of B-PROP itself, like new objectives and contrastive learning; improving low resource performance; and studying how to best leverage pre-training data.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the paper:

This paper proposes a bootstrapped pre-training method called B-PROP for ad-hoc retrieval based on BERT. B-PROP aims to improve upon PROP, a recent pre-training method that showed state-of-the-art results on ad-hoc retrieval benchmarks. The key idea is to leverage BERT's powerful contextual language model to replace the unigram language model used in PROP for constructing the representative words prediction (ROP) pre-training task. This allows sampling more informative words from documents. Specifically, they introduce a contrastive sampling method inspired by divergence-from-randomness to distinguish informative words from common words using BERT's attention. B-PROP is pre-trained with the tailored ROP objective and masked language modeling on two large corpora. When fine-tuned on 7 ad-hoc retrieval datasets, B-PROP achieves significant improvements over PROP and other baselines, pushing forward the state-of-the-art. It also shows strong low-resource performance.


## Summarize the paper in two paragraphs.

 Here are two paragraphs summarizing the key points of the paper:

This paper proposes B-PROP, a bootstrapped pre-training method with representative words prediction for ad-hoc retrieval. B-PROP leverages the BERT language model to replace the unigram language model used in prior work PROP for constructing the representative words prediction (ROP) pre-training task. The key idea is to use a contrastive method inspired by divergence-from-randomness to sample more informative words from documents when creating ROP training data. Specifically, the contrastive method computes the cross-entropy between a document's term distribution based on BERT attention weights and the collection's overall term distribution. Sampling words according to the resulting contrastive distribution produces more discriminative words for pre-training. B-PROP is pre-trained with the ROP and masked language modeling objectives, then fine-tuned on downstream ranking tasks. Experiments on 7 standard ad-hoc retrieval benchmarks show B-PROP significantly outperforms strong baselines including BERT and PROP. The gains are especially large for datasets with limited labeled training data.

In summary, the key contributions are: (1) B-PROP replaces the unigram language model in PROP with BERT for constructing the ROP pre-training task. (2) A novel contrastive method based on divergence-from-randomness is proposed to sample more informative words from documents when creating ROP examples. (3) Extensive experiments demonstrate the effectiveness of B-PROP, especially under low resource settings. B-PROP advances state-of-the-art on multiple standard ad-hoc retrieval benchmarks.


## Summarize the main method used in the paper in one paragraph.

 Here is a one paragraph summary of the main method used in the paper:

The paper proposes a bootstrapped pre-training method called B-PROP for ad-hoc retrieval. B-PROP is based on BERT and aims to improve upon the recently proposed PROP method. The key idea is to leverage BERT's contextual language model to replace the unigram language model used in PROP for constructing the representative words prediction (ROP) pre-training task. To sample more informative words from documents, the authors introduce a contrastive sampling method inspired by divergence-from-randomness. This contrastive method computes the cross-entropy between the document term distribution from BERT and a collection-level random term distribution to obtain a more representative distribution. Representative word pairs are then sampled from this contrastive distribution for the ROP task. B-PROP jointly trains on the ROP and masked language modeling objectives to learn representations tailored for ad-hoc retrieval. The bootstrapped BERT model is then fine-tuned on downstream retrieval tasks. Experiments show B-PROP outperforms strong baselines including BERT and PROP on several standard ad-hoc retrieval datasets.


## What problem or question is the paper addressing?

 The paper is addressing the problem that the effectiveness of the recently proposed PROP method for pre-training in information retrieval might be bounded by the classical unigram language model used in its construction of the representative words prediction (ROP) task. Specifically, the term independence assumption of the unigram language model ignores correlations between words, which may affect the quality of sampling representative words from a document. 

To address this problem, the paper proposes a bootstrapped pre-training method called B-PROP that leverages the powerful contextual language model BERT to replace the unigram language model for constructing the ROP task. The key idea is to use a contrastive method inspired by divergence-from-randomness to sample more representative words from documents using BERT's self-attention mechanism. B-PROP then re-trains BERT itself on the tailored ROP objective towards better pre-training for ad-hoc retrieval tasks.

In summary, the key problem is that PROP's effectiveness for IR pre-training may be limited by using a weaker unigram language model for constructing its ROP task. B-PROP aims to address this by using the more powerful BERT language model in a novel contrastive sampling approach to obtain better representative words for pre-training.
