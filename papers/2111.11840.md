# [Subgraph Permutation Equivariant Networks](https://arxiv.org/abs/2111.11840)

## What is the main contribution of this paper?

Based on my reading, the main contributions of this paper are:1. It develops a new graph neural network framework called Subgraph Permutation Equivariant Networks (SPEN) that operates on subgraphs of the input graph while maintaining permutation equivariance. 2. It proposes a novel way to extract subgraphs based on k-hop ego networks and group them into "bags" based on their automorphism group. This allows the model to be equivariant to permutations of nodes within each subgraph as well as permutations of entire subgraphs.3. It shows theoretically and experimentally that operating on subgraphs makes the model more scalable compared to global permutation equivariant models that operate on the entire graph.4. It proves that SPEN is more expressive than both message passing neural networks on subgraphs and higher-order global permutation equivariant models. This is achieved through the combination of subgraph operators and automorphism group equivariance.5. The model achieves competitive results on graph classification benchmarks, matching or exceeding state-of-the-art methods on 6 out of 7 datasets.In summary, the key innovations are using subgraphs and their associated automorphism groups to create a graph network that is more scalable, expressive and achieves strong empirical performance compared to prior graph neural network architectures. The framework is general and applicable across a range of graph learning tasks.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper proposes a new graph neural network framework called Subgraph Permutation Equivariant Networks (SPEN) that operates on subgraphs to improve scalability and expressivity compared to global permutation equivariant networks, through using a base encoder that is permutation equivariant and constraining the linear map to be equivariant to the automorphism groups of the subgraphs.


## How does this paper compare to other research in the same field?

This paper presents a new graph neural network model called Subgraph Permutation Equivariant Networks (SPEN). Here are some key ways it compares to other research on improving graph neural networks:- Uses subgraphs for more localized computation like DropGNN, Stars, and ESAN, but also enforces automorphism group equivariance across the subgraphs. This is a novel combination.- Achieves permutation equivariance like PPGN, IGN, and GNGN but through local computation on subgraphs rather than whole graphs. This improves scalability while maintaining expressive power.- Requires less hardcoded structural encoding than methods like GSN and SIN since the subgraph automorphism groups arise naturally from the data. This makes the approach more general.- Demonstrates strong empirical performance on par with recent state-of-the-art methods like GSN, DSS, and CIN across multiple graph benchmark datasets.Overall, SPEN combines ideas from past work on subgraphs, automorphism groups, and local equivariance in a novel way to create a scalable and expressive graph neural network. The experiments show it achieves state-of-the-art or comparable accuracy while requiring less problem-specific engineering than some other methods. The approach seems promising for creating graph networks that can effectively learn from permutation-based symmetries.
