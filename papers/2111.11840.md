# [Subgraph Permutation Equivariant Networks](https://arxiv.org/abs/2111.11840)

## What is the main contribution of this paper?

 Based on my reading, the main contributions of this paper are:

1. It develops a new graph neural network framework called Subgraph Permutation Equivariant Networks (SPEN) that operates on subgraphs of the input graph while maintaining permutation equivariance. 

2. It proposes a novel way to extract subgraphs based on k-hop ego networks and group them into "bags" based on their automorphism group. This allows the model to be equivariant to permutations of nodes within each subgraph as well as permutations of entire subgraphs.

3. It shows theoretically and experimentally that operating on subgraphs makes the model more scalable compared to global permutation equivariant models that operate on the entire graph.

4. It proves that SPEN is more expressive than both message passing neural networks on subgraphs and higher-order global permutation equivariant models. This is achieved through the combination of subgraph operators and automorphism group equivariance.

5. The model achieves competitive results on graph classification benchmarks, matching or exceeding state-of-the-art methods on 6 out of 7 datasets.

In summary, the key innovations are using subgraphs and their associated automorphism groups to create a graph network that is more scalable, expressive and achieves strong empirical performance compared to prior graph neural network architectures. The framework is general and applicable across a range of graph learning tasks.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the key points from the paper:

The paper proposes a new graph neural network framework called Subgraph Permutation Equivariant Networks (SPEN) that operates on subgraphs to improve scalability and expressivity compared to global permutation equivariant networks, through using a base encoder that is permutation equivariant and constraining the linear map to be equivariant to the automorphism groups of the subgraphs.


## How does this paper compare to other research in the same field?

 This paper presents a new graph neural network model called Subgraph Permutation Equivariant Networks (SPEN). Here are some key ways it compares to other research on improving graph neural networks:

- Uses subgraphs for more localized computation like DropGNN, Stars, and ESAN, but also enforces automorphism group equivariance across the subgraphs. This is a novel combination.

- Achieves permutation equivariance like PPGN, IGN, and GNGN but through local computation on subgraphs rather than whole graphs. This improves scalability while maintaining expressive power.

- Requires less hardcoded structural encoding than methods like GSN and SIN since the subgraph automorphism groups arise naturally from the data. This makes the approach more general.

- Demonstrates strong empirical performance on par with recent state-of-the-art methods like GSN, DSS, and CIN across multiple graph benchmark datasets.

Overall, SPEN combines ideas from past work on subgraphs, automorphism groups, and local equivariance in a novel way to create a scalable and expressive graph neural network. The experiments show it achieves state-of-the-art or comparable accuracy while requiring less problem-specific engineering than some other methods. The approach seems promising for creating graph networks that can effectively learn from permutation-based symmetries.


## What future research directions do the authors suggest?

 Based on my reading of the paper, here are some of the key future research directions suggested by the authors:

- Exploring higher order permutation representations in the SPEN model to potentially improve performance on challenging datasets like IMDB-M. The authors state that using higher order representations could increase the expressive power of the model in line with higher levels of the Weisfeiler-Lehman test.

- Investigating alternative parameterizations of the automorphism constraint when it needs to be "bunched" due to few subgraphs in a bag. The authors suggest this could lead to improved results on datasets like IMDB-M where they had to parameterize the constraint.

- Applying the SPEN framework to other domains beyond graph classification, such as graph regression or graph generation tasks. The general framework could likely be adapted to other graph learning problems.

- Extending the theoretical analysis of the model's expressive power. While the authors provide some analysis following prior work, further theoretical characterization of the model's capabilities could be done.

- Analyzing the impact of different subgraph extraction policies beyond k-hop ego networks. The authors use a simple k-hop policy but other policies could be explored.

- Combining the SPEN model with complementary methods like attention or graph pooling to incorporate both global and local views of the graph. This could allow balancing expressiveness and scalability.

- Exploring learned aggregation functions within the local subgraph updates instead of simple averaging. This could potentially help learn more optimal ways to aggregate subgraph information.

- Applying the automorphism equivariant concepts to other data types such as point clouds or meshes beyond just graphs. The core ideas could generalize.

So in summary, the authors point to a number of promising ways to build on their work around theoretical analysis, model architecture, subgraph extraction policies, and applications to new domains or data types. The SPEN framework seems to offer a rich space for future research.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the paper:

The paper presents a new framework for building graph neural networks called Subgraph Permutation Equivariant Networks (SPEN). The key ideas are 1) representing the graph as a bag of subgraphs, 2) using a permutation equivariant base encoder that operates on the subgraphs, and 3) constraining the model to be equivariant to the automorphism groups of the subgraphs. This improves on prior work in several ways. First, it is more scalable than global permutation methods since it operates on subgraphs rather than the whole graph. Second, it is more expressive than message passing networks that lack permutation equivariance. Third, it does not require injecting structural information into the feature space like some other expressive methods. Theoretically, the model is provably more expressive than message passing networks and the subgraph-based modeling results in improved scalability over global methods. Empirically, the model achieves competitive results on graph classification benchmarks, obtaining state-of-the-art or comparable accuracy on 6 out of 7 datasets tested.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the paper:

The paper proposes a new graph neural network framework called Subgraph Permutation Equivariant Networks (SPEN). The key idea is to operate on overlapping subgraphs of the input graph rather than the entire graph. This improves scalability compared to global permutation equivariant networks since computations are localized. The subgraphs are extracted using a k-ego network policy, where a subgraph is created for each node consisting of its k-hop neighbors. The subgraphs are grouped into bags based on their size, with each bag corresponding to a different automorphism group. The model processes each bag with a separate function that is equivariant to permutations of nodes within subgraphs and automorphisms across subgraphs. Using subgraphs also improves expressivity compared to global methods since distinguishing substructures may exist even if entire graphs are non-isomorphic. Experiments on graph classification benchmarks show SPEN achieves statistically indistinguishable results from state-of-the-art methods on most datasets.

In summary, the main contributions are: 1) a subgraph extraction method compatible with automorphism equivariance, 2) improved scalability compared to global permutation methods by localizing computations, 3) improved expressivity by incorporating subgraph information, 4) a choice of automorphism groups based on subgraph sizes, 5) competitive results on graph classification benchmarks. The method provides a general framework for building graph networks that operate on subgraphs and are equivariant to permutations and automorphisms.


## Summarize the main method used in the paper in one paragraph.

 Here is a one paragraph summary of the main method used in the paper:

The paper proposes Subgraph Permutation Equivariant Networks (SPEN), a new graph neural network framework that operates on subgraphs while maintaining permutation equivariance. SPEN first extracts overlapping ego-network subgraphs from the input graph. It then groups the subgraphs into bags based on their size, with each bag corresponding to a different automorphism group. SPEN processes the bags of subgraphs using separate graph neural networks that share weights within each automorphism group. By constraining the networks to be equivariant to permutations of nodes within subgraphs and to automorphisms between subgraphs, SPEN aims to improve expressivity and scalability compared to global permutation equivariant models. The overall framework allows flexibility in choosing the subgraph extraction method, permutation equivariant base encoder, and representation spaces. Experiments show SPEN achieves competitive accuracy on graph classification benchmarks while requiring less memory than global models.


## What is the central research question or hypothesis that this paper addresses?

 Based on my reading, the central research question this paper addresses is: How can we develop a graph neural network framework that improves upon the expressive power and scalability limitations of prior graph neural networks? 

Specifically, the authors aim to develop a graph neural network architecture that:

- Has greater expressive power than prior graph neural networks like message passing neural networks or higher-dimensional permutation equivariant graph networks.

- Is more scalable than global permutation equivariant graph networks. 

- Does not require pre-encoding structural information into the feature space, unlike some prior works.

- Provides a general framework where the representation spaces can be chosen per layer.

To address these challenges, the paper introduces the Subgraph Permutation Equivariant Network (SPEN) framework, which operates on subgraphs and incorporates automorphism group equivariance constraints. The central hypothesis is that by extracting subgraphs and designing layers equivariant to subgraph automorphism groups, the model can achieve better expressivity and scalability compared to prior graph neural network architectures.


## What problem or question is the paper addressing?

 Based on my reading, the key points of this paper are:

- The paper aims to develop a new graph neural network framework called Subgraph Permutation Equivariant Networks (SPEN). The goal is to build GNNs that are more expressive and scalable compared to prior methods like message passing neural networks (MPNNs). 

- MPNNs have limitations in expressive power, as shown by prior work. Recent approaches to overcome this either lack scalability or require encoding structural information into the feature space. 

- SPEN aims to overcome these limitations by:

1) Operating on subgraphs rather than the full graph, which improves scalability.

2) Utilizing a permutation equivariant base encoder.

3) Constraining the linear map to be equivariant to the automorphism groups of the subgraphs.

- This allows SPEN to capture both permutation symmetries within subgraphs and symmetries across subgraphs.

- Theoretical analysis shows SPEN is more expressive than MPNNs and higher-dimensional GNNs.

- Experiments on graph classification benchmarks show SPEN achieves competitive results to state-of-the-art methods on most datasets.

In summary, the key contribution is developing a GNN framework that is more expressive and scalable by using subgraph architectures and capturing permutation symmetries at both local and global levels. The results demonstrate improved performance over prior graph neural network methods.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and concepts in this work are:

- Subgraph Permutation Equivariant Networks (SPEN): The name of the proposed graph neural network framework in this paper. It operates on subgraphs and uses a permutation equivariant base update function.

- Automorphism equivariance: A key property of the SPEN model. The linear map in each layer is constrained to be equivariant to the automorphism groups of the bags of subgraphs. This enables weight sharing and improved expressivity.

- $k$-ego network subgraphs: The subgraph extraction policy used in SPEN. For each node, it extracts the $k$-hop neighborhood as a subgraph. This gives a bag of subgraphs with automorphism symmetries.

- Permutation equivariance: A core requirement for the base update function in SPEN layers. The layers map between permutation representation spaces like $\rho_1 \oplus \rho_2$.

- Naturality constraint: The condition that requires linear maps to respect graph isomorphisms. Used to derive the automorphism equivariance in SPEN.

- Expressivity: A key aspect analyzed. SPEN is shown to be more expressive than message passing networks and subgraph methods.

- Scalability: SPEN operates on local subgraphs so scales better than global permutation methods. Analyzed theoretically and empirically.

- Graph benchmarks: SPEN is evaluated on TUDataset graph classification tasks and shown to achieve competitive accuracy.

In summary, the key themes are designing a graph neural network using subgraph and permutation equivariance concepts to improve expressivity and scalability. The SPEN model and experiments demonstrate these benefits.


## Based on the paper, what questions should we ask to create a comprehensive summary of it? Please list at least 10 questions.

 Here are 10 suggested questions to ask in order to create a comprehensive summary of the paper:

1. What is the main research question or problem being addressed in the paper? 

2. What are the key contributions or main findings of the paper?

3. What methods or techniques did the authors use to address the research problem?

4. What datasets were used in the experiments?

5. What were the main results on each dataset or experiment? 

6. How do the results compare to prior or related work in the field?

7. What limitations or shortcomings does the paper identify?

8. What future work or next steps do the authors suggest?

9. How does this research contribute to the broader field or community?

10. Did the authors release any code or data to accompany the paper? If so, what are the details?

Asking these types of questions will help summarize the key information from the paper, including the problem statement, methods, results, comparisons, limitations, and contributions. The goal is to understand the big picture as well as the details needed to write an effective summary. Additional questions could probe for more specifics on the techniques, experiments, results etc. depending on the paper.


## Design 10 in-depth questions about method proposed in this paper and avoid simple question.

 Here are 10 in-depth questions about the method proposed in this paper:

1. The paper proposes a new framework called Subgraph Permutation Equivariant Networks (SPEN). What are the key components of this framework and how do they improve upon prior graph neural network models?

2. SPEN operates on subgraphs rather than the full graph. How does the choice of subgraph selection policy affect the expressivity and scalability of the model? What theoretical guarantees can be made about the expressivity?

3. The paper claims SPEN is more expressive than higher-dimensional global permutation equivariant models. What is the intuition behind this claim? How does operating on subgraphs unlock additional expressive power beyond just using a higher-dimensional base model?

4. SPEN constrains the model to be equivariant to the automorphism groups of the subgraphs. Why is this a useful inductive bias? How does it differ from just using global permutation equivariance?

5. The linear layers in SPEN map between permutation equivariant spaces like ρ1⊕ρ2. How does the choice of input/output representation impact model expressivity and computational complexity? 

6. What weighting schemes or parameter sharing strategies are used in SPEN? How do these reduce the risk of overfitting compared to fully independent parameters per subgraph?

7. The paper shows theoretically and empirically that SPEN scales better than global models. What causes this improvement in scalability? How well does SPEN scale in practice to large graphs?

8. How does the performance of SPEN compare to prior state-of-the-art methods on graph classification benchmarks? What explains its strong performance?

9. What variations or extensions of the SPEN framework could be explored in future work? Are there other potential subgraph selection policies or equivariance constraints that could prove useful?

10. The paper claims SPEN is a general framework for building more expressive and scalable graph networks. Do you agree with this assessment? What are the most novel or significant contributions of this work?


## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a summary paragraph of the key points from the paper:

The paper proposes a new graph neural network framework called Subgraph Permutation Equivariant Networks (SPEN) for building models that operate on subgraphs while using a base update function that is permutation equivariant. The method extracts k-hop ego network subgraphs from the input graph and processes them with automorphism equivariant linear layers. Each layer maps between tensor representation spaces like graphs and sets while enforcing equivariance to subgraph automorphisms and permutations. This approach attempts to improve on limitations of prior methods like poor scalability of global permutation methods and lack of expressivity compared to WL tests. Theoretical analysis shows the model is more expressive than subgraph MPNNs and empirically demonstrates improved scalability over global methods. Experiments on graph classification benchmarks achieve competitive accuracy compared to state-of-the-art techniques on most datasets. The framework offers a novel subgraph extraction policy and automorphism group choice to develop expressive and scalable graph networks through local permutation equivariance constraints. Key innovations are operating on ego-network subgraphs, enforcing equivariance to their automorphism groups, and using higher-order permutation representations in the base GNN model.


## Summarize the paper in one sentence.

 The paper proposes a new graph neural network architecture called Subgraph Permutation Equivariant Networks (SPEN) that operates on subgraphs while using a base update function that is permutation equivariant.


## Summarize the paper in one paragraphs.

 Here is a one paragraph summary of the paper:

The paper proposes a new graph neural network framework called Subgraph Permutation Equivariant Networks (SPEN) for building models that operate on subgraphs while preserving equivariance to permutations of nodes within subgraphs and to automorphisms of bags of subgraphs. SPEN extracts ego-network subgraphs for each node in the input graph and groups them into bags by subgraph size, with each bag forming an automorphism group. It then processes each bag with a separate function that uses a permutation equivariant base encoder, mapping between tensor representations of different orders. This allows the model to be equivariant to node permutations within subgraphs and automorphisms of the subgraph bags. Theoretical analysis shows SPEN is more expressive than message passing networks and scales better than global permutation methods. Experiments on graph classification benchmarks demonstrate SPEN achieves competitive accuracy to state-of-the-art methods while using less GPU memory. The framework provides a scalable way to build graph networks with stronger expressivity through incorporating subgraph and automorphism symmetries.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 potential in-depth questions about the method proposed in the paper:

1. The paper proposes a novel framework called Subgraph Permutation Equivariant Networks (SPEN). How does operating on subgraphs rather than the full graph improve the scalability and expressivity of the model compared to global permutation equivariant networks?

2. The paper extracts subgraphs using a k-ego network policy. What are the advantages of using k-ego networks over other possible subgraph extraction policies? How does this choice impact the resulting automorphism groups?

3. The model places subgraphs into bags where each bag corresponds to a different automorphism group. Why is it beneficial to process subgraphs from different automorphism groups separately? How does this automorphism equivariance constraint improve the expressivity?

4. The base graph neural network used in SPEN operates on tensor representations like $\rho_1 \oplus \rho_2$. Why is it useful to have representations of different orders? How does this allow projecting between graph and node feature spaces?

5. The paper proves both theoretically and empirically that SPEN has better scalability than global methods. What is the computational complexity of SPEN and how does it depend on the number of nodes n and subgraph size m?

6. The authors show SPEN is more expressive than both subgraph MPNNs and higher order GNNs. What limitations of these other methods does SPEN overcome? What allows SPEN to distinguish more graphs?

7. How exactly does the naturality constraint used in SPEN improve expressivity? Why is it important that each automorphism group has its own linear map? 

8. The paper demonstrates strong performance on graph classification tasks. On which datasets does SPEN achieve particularly good performance compared to prior methods? Are there any datasets where it underperforms?

9. What design choices need to be made when implementing SPEN, such as the subgraph size k, number of layers, and permutation representations used? How do these choices impact model performance?

10. The paper mentions further improving performance on certain datasets as future work. What techniques could potentially be used to achieve better accuracy, such as increasing the order of permutation representations?
