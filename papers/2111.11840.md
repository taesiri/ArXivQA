# [Subgraph Permutation Equivariant Networks](https://arxiv.org/abs/2111.11840)

## What is the main contribution of this paper?

Based on my reading, the main contributions of this paper are:1. It develops a new graph neural network framework called Subgraph Permutation Equivariant Networks (SPEN) that operates on subgraphs of the input graph while maintaining permutation equivariance. 2. It proposes a novel way to extract subgraphs based on k-hop ego networks and group them into "bags" based on their automorphism group. This allows the model to be equivariant to permutations of nodes within each subgraph as well as permutations of entire subgraphs.3. It shows theoretically and experimentally that operating on subgraphs makes the model more scalable compared to global permutation equivariant models that operate on the entire graph.4. It proves that SPEN is more expressive than both message passing neural networks on subgraphs and higher-order global permutation equivariant models. This is achieved through the combination of subgraph operators and automorphism group equivariance.5. The model achieves competitive results on graph classification benchmarks, matching or exceeding state-of-the-art methods on 6 out of 7 datasets.In summary, the key innovations are using subgraphs and their associated automorphism groups to create a graph network that is more scalable, expressive and achieves strong empirical performance compared to prior graph neural network architectures. The framework is general and applicable across a range of graph learning tasks.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper proposes a new graph neural network framework called Subgraph Permutation Equivariant Networks (SPEN) that operates on subgraphs to improve scalability and expressivity compared to global permutation equivariant networks, through using a base encoder that is permutation equivariant and constraining the linear map to be equivariant to the automorphism groups of the subgraphs.


## How does this paper compare to other research in the same field?

This paper presents a new graph neural network model called Subgraph Permutation Equivariant Networks (SPEN). Here are some key ways it compares to other research on improving graph neural networks:- Uses subgraphs for more localized computation like DropGNN, Stars, and ESAN, but also enforces automorphism group equivariance across the subgraphs. This is a novel combination.- Achieves permutation equivariance like PPGN, IGN, and GNGN but through local computation on subgraphs rather than whole graphs. This improves scalability while maintaining expressive power.- Requires less hardcoded structural encoding than methods like GSN and SIN since the subgraph automorphism groups arise naturally from the data. This makes the approach more general.- Demonstrates strong empirical performance on par with recent state-of-the-art methods like GSN, DSS, and CIN across multiple graph benchmark datasets.Overall, SPEN combines ideas from past work on subgraphs, automorphism groups, and local equivariance in a novel way to create a scalable and expressive graph neural network. The experiments show it achieves state-of-the-art or comparable accuracy while requiring less problem-specific engineering than some other methods. The approach seems promising for creating graph networks that can effectively learn from permutation-based symmetries.


## What future research directions do the authors suggest?

Based on my reading of the paper, here are some of the key future research directions suggested by the authors:- Exploring higher order permutation representations in the SPEN model to potentially improve performance on challenging datasets like IMDB-M. The authors state that using higher order representations could increase the expressive power of the model in line with higher levels of the Weisfeiler-Lehman test.- Investigating alternative parameterizations of the automorphism constraint when it needs to be "bunched" due to few subgraphs in a bag. The authors suggest this could lead to improved results on datasets like IMDB-M where they had to parameterize the constraint.- Applying the SPEN framework to other domains beyond graph classification, such as graph regression or graph generation tasks. The general framework could likely be adapted to other graph learning problems.- Extending the theoretical analysis of the model's expressive power. While the authors provide some analysis following prior work, further theoretical characterization of the model's capabilities could be done.- Analyzing the impact of different subgraph extraction policies beyond k-hop ego networks. The authors use a simple k-hop policy but other policies could be explored.- Combining the SPEN model with complementary methods like attention or graph pooling to incorporate both global and local views of the graph. This could allow balancing expressiveness and scalability.- Exploring learned aggregation functions within the local subgraph updates instead of simple averaging. This could potentially help learn more optimal ways to aggregate subgraph information.- Applying the automorphism equivariant concepts to other data types such as point clouds or meshes beyond just graphs. The core ideas could generalize.So in summary, the authors point to a number of promising ways to build on their work around theoretical analysis, model architecture, subgraph extraction policies, and applications to new domains or data types. The SPEN framework seems to offer a rich space for future research.


## Summarize the paper in one paragraph.

Here is a one paragraph summary of the paper:The paper presents a new framework for building graph neural networks called Subgraph Permutation Equivariant Networks (SPEN). The key ideas are 1) representing the graph as a bag of subgraphs, 2) using a permutation equivariant base encoder that operates on the subgraphs, and 3) constraining the model to be equivariant to the automorphism groups of the subgraphs. This improves on prior work in several ways. First, it is more scalable than global permutation methods since it operates on subgraphs rather than the whole graph. Second, it is more expressive than message passing networks that lack permutation equivariance. Third, it does not require injecting structural information into the feature space like some other expressive methods. Theoretically, the model is provably more expressive than message passing networks and the subgraph-based modeling results in improved scalability over global methods. Empirically, the model achieves competitive results on graph classification benchmarks, obtaining state-of-the-art or comparable accuracy on 6 out of 7 datasets tested.


## Summarize the paper in two paragraphs.

Here is a two paragraph summary of the paper:The paper proposes a new graph neural network framework called Subgraph Permutation Equivariant Networks (SPEN). The key idea is to operate on overlapping subgraphs of the input graph rather than the entire graph. This improves scalability compared to global permutation equivariant networks since computations are localized. The subgraphs are extracted using a k-ego network policy, where a subgraph is created for each node consisting of its k-hop neighbors. The subgraphs are grouped into bags based on their size, with each bag corresponding to a different automorphism group. The model processes each bag with a separate function that is equivariant to permutations of nodes within subgraphs and automorphisms across subgraphs. Using subgraphs also improves expressivity compared to global methods since distinguishing substructures may exist even if entire graphs are non-isomorphic. Experiments on graph classification benchmarks show SPEN achieves statistically indistinguishable results from state-of-the-art methods on most datasets.In summary, the main contributions are: 1) a subgraph extraction method compatible with automorphism equivariance, 2) improved scalability compared to global permutation methods by localizing computations, 3) improved expressivity by incorporating subgraph information, 4) a choice of automorphism groups based on subgraph sizes, 5) competitive results on graph classification benchmarks. The method provides a general framework for building graph networks that operate on subgraphs and are equivariant to permutations and automorphisms.


## Summarize the main method used in the paper in one paragraph.

Here is a one paragraph summary of the main method used in the paper:The paper proposes Subgraph Permutation Equivariant Networks (SPEN), a new graph neural network framework that operates on subgraphs while maintaining permutation equivariance. SPEN first extracts overlapping ego-network subgraphs from the input graph. It then groups the subgraphs into bags based on their size, with each bag corresponding to a different automorphism group. SPEN processes the bags of subgraphs using separate graph neural networks that share weights within each automorphism group. By constraining the networks to be equivariant to permutations of nodes within subgraphs and to automorphisms between subgraphs, SPEN aims to improve expressivity and scalability compared to global permutation equivariant models. The overall framework allows flexibility in choosing the subgraph extraction method, permutation equivariant base encoder, and representation spaces. Experiments show SPEN achieves competitive accuracy on graph classification benchmarks while requiring less memory than global models.


## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central research question this paper addresses is: How can we develop a graph neural network framework that improves upon the expressive power and scalability limitations of prior graph neural networks? Specifically, the authors aim to develop a graph neural network architecture that:- Has greater expressive power than prior graph neural networks like message passing neural networks or higher-dimensional permutation equivariant graph networks.- Is more scalable than global permutation equivariant graph networks. - Does not require pre-encoding structural information into the feature space, unlike some prior works.- Provides a general framework where the representation spaces can be chosen per layer.To address these challenges, the paper introduces the Subgraph Permutation Equivariant Network (SPEN) framework, which operates on subgraphs and incorporates automorphism group equivariance constraints. The central hypothesis is that by extracting subgraphs and designing layers equivariant to subgraph automorphism groups, the model can achieve better expressivity and scalability compared to prior graph neural network architectures.


## What problem or question is the paper addressing?

Based on my reading, the key points of this paper are:- The paper aims to develop a new graph neural network framework called Subgraph Permutation Equivariant Networks (SPEN). The goal is to build GNNs that are more expressive and scalable compared to prior methods like message passing neural networks (MPNNs). - MPNNs have limitations in expressive power, as shown by prior work. Recent approaches to overcome this either lack scalability or require encoding structural information into the feature space. - SPEN aims to overcome these limitations by:1) Operating on subgraphs rather than the full graph, which improves scalability.2) Utilizing a permutation equivariant base encoder.3) Constraining the linear map to be equivariant to the automorphism groups of the subgraphs.- This allows SPEN to capture both permutation symmetries within subgraphs and symmetries across subgraphs.- Theoretical analysis shows SPEN is more expressive than MPNNs and higher-dimensional GNNs.- Experiments on graph classification benchmarks show SPEN achieves competitive results to state-of-the-art methods on most datasets.In summary, the key contribution is developing a GNN framework that is more expressive and scalable by using subgraph architectures and capturing permutation symmetries at both local and global levels. The results demonstrate improved performance over prior graph neural network methods.
