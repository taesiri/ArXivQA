# [Align your Latents: High-Resolution Video Synthesis with Latent   Diffusion Models](https://arxiv.org/abs/2304.08818)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading, the central research question/hypothesis of this paper is: 

How can latent diffusion models (LDMs), which have been very successful for high-quality image generation, be extended efficiently to high-resolution video generation?

Specifically, the authors propose Video LDMs, which leverage pre-trained image LDMs and turn them into video generators by introducing a temporal dimension and fine-tuning only on encoded image sequences (videos). Their key insight is that huge image datasets can be used to pre-train the spatial layers of the LDM, while the smaller available video datasets are sufficient to train the temporal alignment layers. 

The authors validate their method by focusing on two real-world applications: 1) Simulation of high-resolution real-world driving data for autonomous driving research. 2) Text-to-video generation for creative content creation.

The central hypothesis is that by building on pre-trained image LDMs and only training lightweight temporal alignment layers, the proposed Video LDM approach will be much more efficient and scalable compared to previous video diffusion models, while achieving state-of-the-art video quality and long-term consistency.
