# [DR-Tune: Improving Fine-tuning of Pretrained Visual Models by   Distribution Regularization with Semantic Calibration](https://arxiv.org/abs/2308.12058)

## What is the central research question or hypothesis that this paper addresses?

This paper proposes a new framework called DR-Tune for fine-tuning pretrained visual models on downstream tasks. The central hypothesis is that fine-tuning can be improved by:1) Using distribution regularization (DR) to regularize the task-specific head during fine-tuning, rather than imposing constraints on the weights or features of the full model. Specifically, DR enforces the head to decrease classification error on the pretrained feature distribution, which prevents overfitting. 2) Developing a semantic calibration (SC) module to align the pretrained and downstream feature distributions. This reduces the bias and interference caused by semantic drift between the two distributions.So in summary, the main research questions are:- Can regulating the task head using the pretrained feature distribution improve fine-tuning and prevent overfitting, compared to regularizing the full model? - Can aligning the pretrained and downstream features distributions via rotation and translation matrices reduce semantic drift and improve fine-tuning performance?The paper aims to validate these hypotheses through extensive experiments combining DR-Tune with different backbones, datasets, and pretraining strategies. The results consistently show improved performance compared to prior fine-tuning techniques.


## What is the main contribution of this paper?

The main contribution of this paper is proposing a novel fine-tuning framework called DR-Tune, which improves fine-tuning of pretrained visual models by enforcing distribution regularization with semantic calibration. The key ideas are:1. DR-Tune employs distribution regularization on the task-specific head, rather than imposing constraints on the backbone weights or features like previous methods. This allows sufficient optimization of the downstream encoder towards its task. 2. DR-Tune uses the pretrained feature distribution to regularize the head, preventing overfitting and enabling it to learn smoother classification boundaries.3. It introduces a semantic calibration module to align the pretrained and downstream feature distributions, which reduces the bias/interference from semantic drift between them. 4. Extensive experiments show DR-Tune consistently improves fine-tuning performance when combined with different backbones and pretraining strategies. It outperforms previous state-of-the-art methods that impose ad-hoc constraints or rely solely on task-specific losses.In summary, the main contribution is a new fine-tuning approach that regularizes the task head based on pretrained features, while calibrating distributions to alleviate semantic drift. This improves optimization and reduces overfitting compared to prior arts.
