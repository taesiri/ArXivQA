# [MaLLaM -- Malaysia Large Language Model](https://arxiv.org/abs/2401.14680)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- There is a gap in large language models pretrained from scratch specifically for the Malaysian language and context. Existing models still carry traces of English language biases.

Proposed Solution:
- The authors train MaLLaM, a large language model pretrained entirely from scratch on a 349GB Malaysian text dataset equivalent to 90 billion tokens. Models with 1.1 billion, 3 billion and 5 billion parameters are trained.

- A Byte Pair Encoding (BPE) tokenizer is pretrained on 85GB of text to handle longer subwords in languages like Malay, Mandarin, Tamil etc. This gives up to 43% reduction in token size compared to other tokenizers.

- Training is done using 80 A100 GPUs managed efficiently via Kubernetes, with datasets processed in a streaming format using the MosaicML library.

Main Contributions:

- MaLLaM represents the first large language model pretrained from scratch specifically for the Malaysian language, without traces of English biases.

- With only a single epoch of pretraining, MaLLaM matches or exceeds the performance of other models on tatabahasa tasks. The 5B parameter model gets the best score.

- Fine-tuned models demonstrate strong performance on tasks like multiturn QA, sentiment analysis and natural language instructions.

- The models and tokenizer are open-sourced to advance Malaysian NLP research. MaLLaM paves the way for better language understanding and generation focused on Malaysian linguistic nuances.

In summary, MaLLaM is the first Malaysian-specific large language model, outperforming existing models. It captures the intricacies of the Malaysian language better than English-influenced models. The open-sourced models, data and code aim to drive innovation in Malaysian NLP.


## Summarize the paper in one sentence.

 MaLLaM is a large language model trained from scratch on 90 billion tokens of Malaysian text data to enhance natural language understanding and generation capabilities for the nuances of the Malay language.


## What is the main contribution of this paper?

 Based on my reading of the paper, the main contribution is the development of MaLLaM (Malaysia Large Language Model) - a large language model specifically pre-trained from scratch using a 349GB dataset equivalent to 90 billion tokens sourced from Malaysian contexts. The key highlights are:

1) MaLLaM is trained entirely from scratch on a Malaysian-specific dataset to capture the nuances and intricacies of the Malaysian language without residual English influences. This sets it apart from adaptations of English models.

2) The models demonstrate competitive performance on tasks like natural language understanding despite the smaller 90 billion token dataset. This underscores the effectiveness of the approach. 

3) MaLLaM contributes pretrained models with 1.1 billion, 3 billion and 5 billion parameters, made openly available to the research community to advance NLP tasks for the Malaysian language.

In summary, the main contribution is an indigenous large language model for Malaysian grounded in local context and datasets that can enable further research and applications in this domain.


## What are the keywords or key terms associated with this paper?

 Based on my review of the paper, some of the main keywords or key terms associated with it are:

- Large language models (LLMs)
- Natural language processing
- Malay language
- Malaysia
- Pretraining
- Byte pair encoding (BPE)
- Tokenization
- Dataset
- Instructions
- Fine-tuning
- Evaluation
- Tatabahasa 
- Multiturn QA
- Coding QA
- Acknowledgements
- Conclusion

The paper introduces MaLLaM, a large language model pretrained from scratch specifically for the Malaysian language context. It discusses the model architecture, training methodology, dataset preparation, tokenization approach, infrastructure setup, pretraining and fine-tuning details, and evaluates the model's performance on tasks like grammatical correctness, question answering, and coding. Key terms reflect this focus on building an indigenous Malaysian LLM, the processes involved, and assessing its capabilities.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in the paper:

1. The paper mentions using a BPE tokenizer trained on a 85GB deduplicated text dataset. What considerations went into choosing the vocabulary size of 32,000 for this tokenizer? How does this compare to tokenizers used in other Malay language models?

2. Section 3 discusses the composition of the training data including Malay Wikipedia, government documents, social media content etc. What steps were taken to ensure a balanced, representative dataset for pre-training? How was the suitability and relevance of different data sources determined?  

3. The paper talks about converting some of the Malay Wikipedia content into Jawi script. What percentage was converted and what method was used for conversion? How does the inclusion of Jawi script content enhance the capabilities of the model?

4. Section 4 mentions using the MinHash algorithm for deduplication with specific parameter values. What analysis was done to arrive at this particular configuration? How do these parameters impact the deduplication process? 

5. The pre-training uses causal language modeling with cross-entropy loss. What other pre-training objectives were considered? Why was causal LM with cross-entropy preferred over other alternatives?

6. During pre-training (Section 7), the learning rate was temporarily reduced by 30% to address stability issues. What indicators were used to determine stability and decide on reverting to the original learning rate?  

7. The fine-tuning process uses an instruction dataset from Malaysian Mistral. What is the size of this dataset? What type of instructions are included and what tasks do they target? How suitable is this dataset for fine-tuning the Malay language model?

8. For supervised fine-tuning, a constant learning rate of 2e-5 is used. How was this value chosen? Would a variable learning rate schedule have been more appropriate? What impacts this decision?

9. The deployed fine-tuned models are compared to ChatGPT and Malaysian Mistral. What metrics are used for this evaluation? Is the comparison approach comprehensive enough to determine superiority over these models? 

10. What challenges were faced in adapting the Mistral architecture for the Malay language? How do linguistic properties of Malay motivate custom architectural modifications and design choices compared to English?
