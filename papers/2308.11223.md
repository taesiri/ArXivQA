# [LDP-Feat: Image Features with Local Differential Privacy](https://arxiv.org/abs/2308.11223)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be: How can we develop a privacy-preserving method for image features that provides strong utility for downstream tasks while also having theoretical guarantees on privacy protection?The authors motivate this question by showing that prior work on visually obfuscating image features to protect privacy can still leak private information via novel inversion attacks. Thus, they argue that methods with formal privacy guarantees are needed.To address this, the paper introduces a new method called LDP-Feat that privatizes image features using local differential privacy. The key hypothesis seems to be that formulating feature obfuscation through the lens of differential privacy can yield both utility and provable privacy guarantees.The authors then make the following main contributions:1) They propose two new attacks on prior work to show it still leaks private information. 2) They introduce LDP-Feat, the first method to privatize image features with local differential privacy.3) They demonstrate LDP-Feat provides utility for downstream tasks like localization while achieving privacy guarantees.So in summary, the central research question is how to develop an image feature privacy method with both utility and formal privacy guarantees, which they address through a new LDP-based approach. Evaluating the efficacy of this approach is the main hypothesis.


## What is the main contribution of this paper?

This paper makes two main contributions:1. It proposes two novel inversion attacks against the adversarial affine subspace embedding method for image feature privatization from Dusmanu et al. (CVPR 2021). Specifically, it introduces a "database attack" that assumes access to the database used to sample adversarial descriptors, and a "clustering attack" that does not require access to the original database. Both attacks are able to approximately recover the original raw descriptors from the privatized subspace embeddings.2. It proposes a new method called LDP-Feat for privatizing image features using local differential privacy (LDP). Unlike prior visual privacy methods, LDP-Feat provides a rigorous privacy guarantee in the form of epsilon-LDP. The method perturbs descriptors by replacing them with random subsets sampled from a fixed dictionary, making it robust against the proposed inversion attacks. Experiments show LDP-Feat achieves strong utility on visual localization while providing guaranteed privacy.In summary, the main contribution is introducing new inversion attacks to break prior affine subspace embedding privacy, and proposing a novel LDP-based approach that provides guaranteed privacy for image features. The attacks reveal weaknesses of prior methods, while LDP-Feat demonstrates the advantages of formulating visual privacy through the lens of differential privacy.
