# [FinGPT-HPC: Efficient Pretraining and Finetuning Large Language Models   for Financial Applications with High-Performance Computing](https://arxiv.org/abs/2402.13533)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem: 
Large language models (LLMs) are computationally intensive as the computation workload and memory footprint grow quadratically with the model dimensions. Over 99% of parameters in LLMs come from the linear layers in the transformer structure, which contributes over 80% of computation and model size. There are three key challenges in pretraining and finetuning LLMs: 
1) Large dataset needed for pretraining exceeds GPU/CPU memory 
2) Memory footprint exceeds GPU memory during pretraining and finetuning 
3) Low GPU utilization with model parallelism during pretraining 

Proposed Solution:
The paper presents efficient pretraining and finetuning methods for LLMs using low-rank structure and quantization techniques to address the above challenges. 

For pretraining, the method replaces a linear layer in the transformer with two narrower linear layers. This allows reducing parameters by orders of magnitude. Further quantization to 8/4 bits reduces memory. Optimization techniques like recomputing intermediate variables, pipelined model parallelism, and parallel weight decomposition are proposed.

For finetuning, low-rank matrices are added to the parallel path of the pretrained linear layer. The pretrained weights are frozen, only low-rank ones are updated. Quantization further reduces memory. Caching techniques reduce inference computations.

Main Contributions:
- New pretraining and finetuning paradigm with low-rank structure and quantization to reduce parameters, memory, and increase speed
- Recomputing technique to reduce intermediate variables for lower memory footprint
- Pipelined model parallelism to improve GPU utilization during pretraining 
- Extensive experiments showing 1.3x speedup and 2.64x compression for pretraining without accuracy loss
- 6.3% and 24% higher accuracy for finetuning on general and financial tasks respectively
- 6.3x lower GPU memory consumption compared to baseline LLM finetuning
