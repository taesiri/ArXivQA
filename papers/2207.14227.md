# [Visual Recognition by Request](https://arxiv.org/abs/2207.14227)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading, the central research question addressed in this paper is how to push visual recognition towards unlimited granularity. The key ideas proposed are:

- Decomposing visual recognition into atomic tasks called "requests" to allow variable recognition granularity. There are two types of requests: whole-to-part semantic segmentation and instance segmentation.

- Using a hierarchical, text-based knowledge base to define the visual concepts and their relationships. This allows for open-domain recognition. 

- Learning from highly incomplete annotations by only requiring annotations when requested. This alleviates the conflict between annotation granularity and certainty.

- Allowing easy insertion of new concepts into the knowledge base with minimal annotation effort, thanks to the text-based knowledge representation and data versioning.

In summary, the main hypothesis is that the proposed "visual recognition by request" paradigm along with the knowledge base representation and incomplete annotation learning can push visual recognition closer to unlimited granularity like humans. The experiments on CPP and ADE20K datasets aim to validate this hypothesis.


## What is the main contribution of this paper?

 The main contribution of this paper is proposing the novel paradigm of "visual recognition by request" (ViRReq). It aims to address the issue of unlimited granularity in visual recognition, which refers to humans' ability to recognize visual contents at arbitrarily fine levels of detail. 

The key ideas of ViRReq are:

- Decomposing visual recognition into atomic tasks called "requests". There are two types of requests: whole-to-part semantic segmentation and instance segmentation.

- Using a hierarchical, text-based "knowledge base" to guide the definition of requests and assist recognition. 

- Allowing flexible control over recognition granularity by making annotations and predictions only when requested. This helps deal with the tradeoff between granularity and annotation certainty.

The paper establishes a baseline by integrating language-driven recognition into semantic/instance segmentation methods. It demonstrates ViRReq's abilities on two datasets with hierarchical part annotations:

- Learning complex whole-part hierarchies from highly incomplete annotations.

- Inserting new concepts with minimal labeling efforts, thanks to the text-based knowledge base.

In summary, the main contribution is proposing the ViRReq paradigm to make progress towards unlimited visual recognition granularity. The paper provides both methodology and baseline experiments around this idea.
