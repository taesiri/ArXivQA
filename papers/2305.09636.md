# [SoundStorm: Efficient Parallel Audio Generation](https://arxiv.org/abs/2305.09636)

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper addresses is how to efficiently generate long, high-quality audio sequences by modeling the tokens of a neural audio codec. Specifically, the paper proposes a method called SoundStorm that can generate audio two orders of magnitude faster than prior autoregressive approaches while maintaining high perceptual quality. The key ideas are:- Using an architecture adapted to the hierarchical structure of the audio tokens produced by residual vector quantization in SoundStream. This allows reducing the sequence length that needs to be modeled.- A parallel, iterative decoding scheme inspired by MaskGIT that predicts the RVQ tokens level-by-level instead of autoregressively. This exploits the conditional independence of finer RVQ levels and enables parallel generation.- A masking scheme during training that mimics the inference procedure.Overall, the paper shows that by designing the model architecture and decoding scheme while accounting for the structure of the discrete audio representations, high-quality and efficient long-form audio generation can be achieved. The results demonstrate that SoundStorm matches the quality of autoregressive modeling baselines while being up to 100x faster.


## What is the main contribution of this paper?

The main contribution of this paper is presenting SoundStorm, a model for efficient and high-quality audio generation. Specifically:- SoundStorm uses a parallel, non-autoregressive decoding scheme to generate audio tokens from a neural audio codec much faster than previous autoregressive approaches like AudioLM. - The model architecture is adapted to the hierarchical residual vector quantization structure of the audio codec tokens, allowing it to scale to longer sequences.- SoundStorm matches the audio quality of AudioLM's acoustic stages while being 100x faster. It also improves consistency in terms of speaker identity and acoustic conditions.- The paper shows SoundStorm can be combined with a text-to-semantic model to generate natural sounding dialogues while precisely controlling speaker turns and voices.In summary, SoundStorm pushes the efficiency and scalability of neural discrete audio generation while maintaining high audio quality. This is enabled by an architecture and parallel decoding scheme tailored to the hierarchical token structure of modern neural audio codecs.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

The paper presents SoundStorm, a model for efficient, non-autoregressive audio generation by modeling the hierarchical token structure of a neural audio codec. The key ideas are using a bidirectional Transformer adapted to the token hierarchy, and a parallel, iterative decoding scheme inspired by MaskGIT. The model can generate high-quality 30-second audio samples in 0.5 seconds on a TPU-v4, outperforming autoregressive approaches. The main result is that SoundStorm, combined with a text-to-semantic model, enables fast and controllable synthesis of natural, multi-speaker dialogues.In one sentence: SoundStorm enables fast synthesis of high-quality audio by exploiting the hierarchical token structure of a neural codec with a parallel decoding scheme.
