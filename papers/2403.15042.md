# [LLM2LLM: Boosting LLMs with Novel Iterative Data Enhancement](https://arxiv.org/abs/2403.15042)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Pretrained large language models (LLMs) achieve state-of-the-art performance on many NLP tasks, but still struggle when fine-tuned with small, specialized datasets. 
- Collecting more high-quality data can be expensive and time-consuming. 
- Existing data augmentation techniques like EDA fail to effectively expand training data for fine-tuning LLMs on new specialized tasks.

Proposed Solution: 
- The paper proposes LLM2LLM, a targeted and iterative data augmentation framework.  
- It uses a teacher LLM to generate synthetic data based on examples the student LLM gets wrong during training. This focuses on harder examples.
- The synthetic data is added back into the training set and the process repeats.

Key Details:
- Fine-tune student LLM on initial seed dataset
- Evaluate student LLM on training data, extract wrong predictions 
- Teacher LLM generates synthetic data similar to wrong examples
- Add synthetic data back into training set
- Repeat process for several iterations

Contributions:
- Proposes the LLM2LLM iterative data augmentation framework 
- Achieves up to 24.2% improvement on GSM8K, 32.6% on CaseHOLD, 32.0% on SNIPS, 52.6% on TREC, 39.8% on SST-2
- Reduces need for manually collecting data
- Allows scaling to new tasks and domains, especially in low-data regime
- Shows promise for making LLMs more performant and scalable

The key insight is to leverage the teacher LLM to focus on harder examples the student LLM gets wrong, and iteratively inject targeted synthetic data to improve performance. This makes more efficient use of limited data budgets.
