# [Stochastic Bandits with Linear Constraints](https://arxiv.org/abs/2006.10185)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem: 
The paper studies a constrained contextual linear bandit setting where the agent needs to maximize cumulative expected reward over T rounds by selecting policies/actions from given sets, while satisfying a linear constraint that the expected cost of each selected policy needs to be below a threshold τ. This models many real-world applications like clinical trials, wireless networks, etc. The key challenge is to balance exploration of new actions and exploitation of actions estimated to be optimal so far, while ensuring the cost constraint is always satisfied.

Proposed Solution:
The paper proposes an Optimistic Pessimistic Linear Bandit (OPLB) algorithm that uses confidence intervals for reward and cost estimates. It is optimistic about reward (selects action with highest upper confidence bound) but pessimistic about cost (selects action satisfying the constraint according to lower confidence bound). The key innovations are:

1) Using asymmetric confidence intervals for reward and cost to ensure optimism. The ratio of interval widths is set to be proportional to 1/(τ-c0) where c0 is the cost of a known safe action. This allows proving an O(d√T/(τ-c0)) regret bound.

2) For multi-armed bandits, proposing a specialized Optimistic Pessimistic Bandit (OPB) algorithm that solves an efficient linear program to select actions. This achieves a O(√KT/(τ-c0)) regret improving on simply using OPLB by a √K factor.


Main Contributions:

1) New constrained contextual linear bandit framework with a per-round linear constraint on expected cost of selected policy

2) OPLB algorithm with asymmetric optimistic and pessimistic confidence bounds 

3) Regret analysis proving O(d√T/(τ-c0)) bound for OPLB  

4) Efficient OPB algorithm for constrained multi-armed bandits with O(√KT/(τ-c0)) regret

5) Lower bound proving the regret dependencies on key problem parameters T, K, τ-c0 are unimprovable

In summary, the paper provides an innovative approach for constrained contextual bandits with provably optimal algorithms and regret bounds. The idea of asymmetric optimism and pessimism is the main conceptual contribution.


## Summarize the paper in one sentence.

 Here is a one-sentence summary of the paper:

The paper proposes and analyzes an optimistic-pessimistic upper confidence bound algorithm for a constrained contextual linear bandit setting where the goal is to maximize cumulative reward subject to the expected cost of each policy being below a given threshold.


## What is the main contribution of this paper?

 The main contributions of this paper are:

1) It formulates and studies a new constrained contextual linear bandit setting, where the goal is to produce a sequence of policies that maximize cumulative reward while satisfying a constraint on the expected cost of each policy (not just each action). 

2) It proposes an upper confidence bound algorithm called Optimistic Pessimistic Linear Bandit (OPLB) for this setting and proves an $\tilde{O}(\frac{d\sqrt{T}}{\tau-c_0})$ regret bound, showing the dependence on the slack in the constraint.

3) It specializes the results to multi-armed bandits, proposes an efficient algorithm called Optimistic Pessimistic Bandit (OPB), and proves an improved $\tilde{O}(\frac{\sqrt{KT}}{\tau-c_0})$ regret over simply using OPLB. 

4) It provides a lower bound that matches the regret upper bound's dependence on key problem parameters.

5) It validates the algorithms experimentally and shows their performance matches the theory.

In summary, the main contribution is formulating a new constrained bandit problem, developing optimism-based algorithms for it, and providing matching upper and lower regret bounds. The results also show optimism can be effectively combined with pessimism for constraint satisfaction.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and concepts include:

- Stochastic bandits
- Linear bandits
- Contextual bandits
- Multi-armed bandits (MAB)
- Linear constraints
- Optimism in the face of uncertainty (OFU)
- Upper confidence bound (UCB)
- Thompson sampling (TS)  
- Pseudo-regret
- Optimistic pessimistic linear bandit (OPLB) algorithm
- Optimistic reward
- Pessimistic cost
- Safe policy set
- Optimistic pessimistic bandit (OPB) algorithm
- Regret analysis
- Lower bounds

The paper studies a constrained contextual linear bandit setting where the goal is to maximize cumulative reward over T rounds while satisfying an expected cost constraint on each policy. Key algorithms proposed are OPLB and OPB which balance optimism for reward and pessimism for cost. Theoretical regret bounds and lower bounds are provided, with simulations to demonstrate the empirical behavior. The setting generalizes both linear and multi-armed bandits.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 potential in-depth questions about the method proposed in this paper:

1. The paper proposes an Optimistic Pessimistic Linear Bandit (OPLB) algorithm. Explain in detail how this algorithm balances optimism and pessimism to address the constrained contextual bandit problem. 

2. A key aspect of OPLB is the use of asymmetric confidence intervals for the reward and cost parameters. Explain why this asymmetry is important and how the confidence interval sizes impact the regret bound.

3. The regret analysis relies on a careful decomposition of the regret. Walk through each term ((I)-(V)) in the regret decomposition in Eq. 6 and explain how they are bounded. What is the significance of the term (I) being non-positive?

4. Explain the significance of Lemma 3 regarding the optimism of OPLB. How does the choice of $\alpha_r$ and $\alpha_c$ ensure optimism? Discuss the two cases considered in the proof when $\pi_t^* \in \Pi_t$ and when $\pi_t^* \notin \Pi_t$.  

5. The Optimistic Pessimistic Bandit (OPB) algorithm is proposed for the multi-armed bandit case. Explain why the regret bound for OPB has an improved dependence on $K$ compared to simply using OPLB. 

6. Discuss Lemma 5 which shows that the linear program solved by OPB has an optimal solution supported on at most 2 arms. Why is this useful and how does it impact the computational complexity?

7. Explain the lower bound instance constructed in Section 4.3. Walk through the specifics of how the two bandit instances $\nu$ and $\nu'$ are designed to prove the lower bound.

8. How would you extend OPLB to the case where the safe action's cost $c_0$ is unknown? Discuss the approach outlined in Section C.1.

9. Consider the setting studied by Amani et al. (2019) where the constraint must hold for the action $x_t$ rather than the policy $\pi_t$. How does this compare to the setting studied in this paper? What similarities and differences are there in the results?

10. This paper studies linear stochastic bandits. How would you extend the optimism-pessimism approach to tackle constrained contexts in other settings like tabular MDPs or nonlinear bandits? What challenges might arise?
