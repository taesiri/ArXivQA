# [A Practical Upper Bound for the Worst-Case Attribution Deviations](https://arxiv.org/abs/2303.00340)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is:How can we quantitatively bound the worst-case deviation in model attributions when inputs are perturbed within a certain region while maintaining the original classification output?The key points are:- The paper aims to derive theoretical upper bounds on the maximum difference in model attributions when inputs are perturbed, but classification outputs remain unchanged. - This provides a measure of the robustness of model attributions against perturbations. Smaller upper bounds indicate more robust attributions.- The attributions considered are for gradient-based methods like integrated gradients, which are widely used for model interpretability.- The perturbations are constrained to be small in either L2 or Linfinity norm so that inputs remain visually similar. - Upper bounds are derived both with and without constraining the classification output to remain unchanged. The bounds with label constraints are tighter.- The theoretical bounds are validated on image datasets like CIFAR-10 against perturbation methods like PGD attacks.So in summary, the central hypothesis is that tight theoretical upper bounds can be derived on attribution deviations under input perturbations, providing a robustness measure for model interpretations.


## What is the main contribution of this paper?

The main contribution of this paper is proposing an upper bound to quantify the worst-case attribution deviations after images are perturbed while the classification results remain unchanged. Specifically:- The paper formulates a constrained optimization problem to find the maximum change of attributions under certain norm constraints on the perturbation and label constraint. This provides a general framework to quantify attribution robustness.- For Euclidean distance and cosine distance metrics, tight upper bounds are derived for $\ell_2$ norm bounded perturbations, with and without label constraints. - For more challenging $\ell_\infty$ norm bounded perturbations, the paper provides two practical approaches to estimate the upper bounds.- The proposed bounds are evaluated on different datasets, models and attack methods. Experiments show the theoretical upper bounds can effectively measure the robustness by bounding the attribution differences between original and attacked images.In summary, the key contribution is proposing the first theoretical quantification of worst-case robustness of attributions to perturbations. This provides a foundation to analyze the attribution deviations more rigorously.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key point of the paper:The paper proposes a theoretical upper bound to quantify the worst-case attribution differences between unperturbed images and perturbed images that cause no change in classification, providing a measure of model robustness against adversarial attacks on attributions.
