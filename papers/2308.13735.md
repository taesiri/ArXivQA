# [MST-compression: Compressing and Accelerating Binary Neural Networks   with Minimum Spanning Tree](https://arxiv.org/abs/2308.13735)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is: How can we compress and accelerate binary neural networks (BNNs) while maintaining accuracy? The key points are:- BNNs use 1-bit representations for weights and activations, which greatly reduces computational cost and memory storage compared to full-precision networks. However, as networks get deeper and wider, the computational burden remains challenging even for BNNs.- The paper proposes a new method called "MST compression" to compress and accelerate BNNs by reordering the calculation of output channels in binary convolutional layers. - The method is based on constructing a minimum spanning tree (MST) over the output channels, where edge distances represent the number of different weight values between two channels. The MST provides an optimal order to reuse output calculations and minimize total operations.- They also propose a learning algorithm to optimize the MST during training by minimizing the total edge distances, further improving compression ratio. - Experiments on benchmark models like VGG and ResNets demonstrate significant compression and acceleration over baselines with minimal accuracy loss.So in summary, the central hypothesis is that using MST compression can effectively reduce computation and memory costs for BNNs while maintaining accuracy, enabling their deployment on resource-constrained devices. The paper presents a method and experiments to validate this hypothesis.


## What is the main contribution of this paper?

The main contribution of this paper is proposing a novel method called MST-compression to compress and accelerate binary neural networks (BNNs). The key ideas and contributions are:- Leveraging minimum spanning tree (MST) to reorder the output channel calculations in binary convolutions, aiming to reduce computational cost and number of parameters. - Proposing a learning algorithm to optimize the MST during training to minimize the total MST distance, further improving compression ratio.- Providing corresponding hardware acceleration architecture that can effectively implement the proposed compression method.Specifically, the paper makes the following contributions:- Introduces and analyzes the effectiveness of using MST to reduce the computational cost of binary convolutions.- Proposes a training algorithm that reduces MST distance and depth during training to maximize compression ratio. - Presents a hardware accelerator design for BNNs with MST compression.- Conducts experiments on benchmark models, showing the proposed method achieves significant compression ratios (up to 13.5x fewer parameters and 5.51x fewer operations) with minimal accuracy loss.- Compares hardware implementation results, demonstrating high throughput and better resource efficiency compared to prior works.In summary, the key contribution is a comprehensive compression framework spanning algorithm design, training optimization, and hardware acceleration for efficient BNNs using MST. The experiments validate the effectiveness of this approach.
