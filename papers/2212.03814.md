# [iQuery: Instruments as Queries for Audio-Visual Sound Separation](https://arxiv.org/abs/2212.03814)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading, the key research focus of this paper is on improving audio-visual sound separation by proposing a novel query-based approach called iQuery. 

Specifically, the paper identifies two main limitations with prior audio-visual separation methods:

1. It is difficult to balance decoder-centric and feature-centric approaches to enforce cross-modality consistency and cross-instrument contrast. 

2. To learn a new musical instrument, one has to retrain the entire network via self-supervision.

To address these limitations, the central hypothesis of this paper is:

- Formulating the audio-visual separation problem from a query-based perspective, where each audio query learns to segment one instrument sound, can better disentangle different sound sources and generalize to new instruments.

To test this hypothesis, the paper proposes the iQuery framework which represents different instruments as trainable audio queries. The queries are initialized using visual features from object detectors ("visually named"), then refined using cross-modal and self-attention in a transformer architecture. 

The key claims are:

- The query-based approach helps explicitly disentangle sound sources through learnable audio prototypes.

- The method can generalize to new instruments by adding an audio prompt query while freezing most of the network.

- Experiments demonstrate improved performance over prior methods on audio-visual separation benchmarks.

In summary, the central research question is whether a query-based approach can overcome limitations of prior techniques for this audio-visual separation task. The key ideas are using audio queries as explicit prototypes and an adaptable query mechanism for better generalization.


## What is the main contribution of this paper?

 Based on my understanding, the main contributions of this paper are:

1. It proposes a new audio-visual sound separation method called iQuery, which uses a query-based Mask Transformer architecture to disentangle different sound sources. This allows explicitly separating sounds through learnable audio prototype queries, unlike previous methods that use more implicit disentanglement.

2. It introduces an audio prompt mechanism for fine-tuning the model when generalizing to new sound classes. By inserting additional trainable query prototypes as audio prompts, the model can adapt to new instruments by only fine-tuning the query embeddings while freezing most of the Transformer. This makes the model more flexible and expandable. 

3. It unifies feature-centric and decoder-centric approaches for sound separation through the query-based attention design. The learnable query prototypes enforce consistency within an instrument and contrast across instruments at both the feature encoding level and the decoder segmentation level.

4. Extensive experiments show the proposed method achieves state-of-the-art performance on three audio-visual separation benchmarks. Ablation studies verify the effectiveness of its core designs for disentanglement and adaptability.

In summary, the main contribution is proposing a novel query-based Transformer approach to audio-visual sound separation that is more explicit, flexible, and achieves superior performance compared to previous methods. The tunable query design brings improved disentanglement, expandability and unifies feature learning with decoder segmentation.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the key points from the paper:

The paper proposes a novel audio-visual sound separation method called iQuery that uses a query-based Mask Transformer architecture to explicitly disentangle different sound sources through learnable audio prototype queries initialized by visual features, enabling better generalization to new instruments via audio prompt fine-tuning.
