# [iQuery: Instruments as Queries for Audio-Visual Sound Separation](https://arxiv.org/abs/2212.03814)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading, the key research focus of this paper is on improving audio-visual sound separation by proposing a novel query-based approach called iQuery. 

Specifically, the paper identifies two main limitations with prior audio-visual separation methods:

1. It is difficult to balance decoder-centric and feature-centric approaches to enforce cross-modality consistency and cross-instrument contrast. 

2. To learn a new musical instrument, one has to retrain the entire network via self-supervision.

To address these limitations, the central hypothesis of this paper is:

- Formulating the audio-visual separation problem from a query-based perspective, where each audio query learns to segment one instrument sound, can better disentangle different sound sources and generalize to new instruments.

To test this hypothesis, the paper proposes the iQuery framework which represents different instruments as trainable audio queries. The queries are initialized using visual features from object detectors ("visually named"), then refined using cross-modal and self-attention in a transformer architecture. 

The key claims are:

- The query-based approach helps explicitly disentangle sound sources through learnable audio prototypes.

- The method can generalize to new instruments by adding an audio prompt query while freezing most of the network.

- Experiments demonstrate improved performance over prior methods on audio-visual separation benchmarks.

In summary, the central research question is whether a query-based approach can overcome limitations of prior techniques for this audio-visual separation task. The key ideas are using audio queries as explicit prototypes and an adaptable query mechanism for better generalization.
