# [Structured Pruning is All You Need for Pruning CNNs at Initialization](https://arxiv.org/abs/2203.02549)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be:Can structured pruning at initialization achieve competitive accuracy compared to unstructured pruning at initialization while also being hardware friendly? The key hypotheses appear to be:1) The accuracy of CNN models pruned using pruning-at-initialization (PAI) methods depends only on the fraction of remaining parameters per layer (layer-wise density), not on which specific parameters are pruned.2) Using a new proxy for model accuracy called Synaptic Expectation (SynExp), PAI can be formulated as a convex optimization problem that directly solves for the optimal layer-wise density. 3) Based on this formulation, a structured PAI method called PreCrop that prunes models in the channel dimension can achieve accuracy comparable to unstructured PAI methods.4) A variant called PreConfig can further optimize the channel width of each layer to improve accuracy at almost zero cost compared to standard neural architecture search techniques.The paper seems to experimentally validate these hypotheses by comparing PreCrop and PreConfig against state-of-the-art unstructured PAI methods like SynFlow on modern CNN architectures and datasets. The key result appears to be that structured pruning via PreCrop/PreConfig can match or exceed the accuracy of SynFlow while also being hardware friendly.


## What is the main contribution of this paper?

This paper proposes a new pruning-at-initialization (PAI) method called Structured Pruning is All You Need for Pruning CNNs at Initialization. The main contributions are:- They propose to use the expectation of the sum of importance scores, rather than the sum, as a proxy for model accuracy after pruning. This allows them to formulate PAI as a convex optimization problem that directly solves for the optimal layer-wise density.- They prove a theorem that the accuracy after pruning only depends on the layer-wise density, regardless of the granularity of pruning. This opens up the possibility for coarse-grained structured pruning. - They propose a structured PAI method called PreCrop that prunes channels to achieve regularity. Compared to prior unstructured PAI methods like SynFlow, PreCrop achieves higher accuracy with fewer FLOPs on ImageNet.- They propose PreConfig, which relaxes the density constraints in PreCrop, allowing layers to expand. This provides a way to tune the width of layers at almost no cost. PreConfig achieves higher accuracy than the original models using fewer parameters.In summary, the key ideas are formulating PAI as a convex optimization problem based on a new accuracy proxy, proving that only layer-wise density matters, and proposing structured pruning methods PreCrop and PreConfig that can match or improve accuracy compared to prior unstructured PAI techniques. The overall contribution is a better structured PAI approach.
