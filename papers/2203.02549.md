# [Structured Pruning is All You Need for Pruning CNNs at Initialization](https://arxiv.org/abs/2203.02549)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be:Can structured pruning at initialization achieve competitive accuracy compared to unstructured pruning at initialization while also being hardware friendly? The key hypotheses appear to be:1) The accuracy of CNN models pruned using pruning-at-initialization (PAI) methods depends only on the fraction of remaining parameters per layer (layer-wise density), not on which specific parameters are pruned.2) Using a new proxy for model accuracy called Synaptic Expectation (SynExp), PAI can be formulated as a convex optimization problem that directly solves for the optimal layer-wise density. 3) Based on this formulation, a structured PAI method called PreCrop that prunes models in the channel dimension can achieve accuracy comparable to unstructured PAI methods.4) A variant called PreConfig can further optimize the channel width of each layer to improve accuracy at almost zero cost compared to standard neural architecture search techniques.The paper seems to experimentally validate these hypotheses by comparing PreCrop and PreConfig against state-of-the-art unstructured PAI methods like SynFlow on modern CNN architectures and datasets. The key result appears to be that structured pruning via PreCrop/PreConfig can match or exceed the accuracy of SynFlow while also being hardware friendly.


## What is the main contribution of this paper?

This paper proposes a new pruning-at-initialization (PAI) method called Structured Pruning is All You Need for Pruning CNNs at Initialization. The main contributions are:- They propose to use the expectation of the sum of importance scores, rather than the sum, as a proxy for model accuracy after pruning. This allows them to formulate PAI as a convex optimization problem that directly solves for the optimal layer-wise density.- They prove a theorem that the accuracy after pruning only depends on the layer-wise density, regardless of the granularity of pruning. This opens up the possibility for coarse-grained structured pruning. - They propose a structured PAI method called PreCrop that prunes channels to achieve regularity. Compared to prior unstructured PAI methods like SynFlow, PreCrop achieves higher accuracy with fewer FLOPs on ImageNet.- They propose PreConfig, which relaxes the density constraints in PreCrop, allowing layers to expand. This provides a way to tune the width of layers at almost no cost. PreConfig achieves higher accuracy than the original models using fewer parameters.In summary, the key ideas are formulating PAI as a convex optimization problem based on a new accuracy proxy, proving that only layer-wise density matters, and proposing structured pruning methods PreCrop and PreConfig that can match or improve accuracy compared to prior unstructured PAI techniques. The overall contribution is a better structured PAI approach.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper proposes a structured pruning-at-initialization method called PreCrop that prunes CNN models at the channel level based on a convex optimization formulation of layer-wise density, and shows it can achieve higher accuracy than prior unstructured pruning-at-initialization techniques.


## How does this paper compare to other research in the same field?

Here are a few key ways this paper on pruning CNNs at initialization compares to other related work:- It proposes a new theoretical framework and metric (SynExp) for analyzing pruning-at-initialization (PAI) methods, proving an invariance theorem about layer densities. This provides new theoretical insight into why PAI methods work.- It introduces a new structured PAI method (PreCrop) that prunes channels rather than individual weights. This makes the method more hardware-friendly than prior unstructured PAI techniques.- Empirically, PreCrop matches or improves accuracy compared to the previous state-of-the-art PAI method SynFlow across various CNN architectures and datasets. For example, on ImageNet it achieves 2.7% higher accuracy for MobileNetV2.- The paper proposes a way to optimize layer widths PreConfig as an extremely fast neural architecture search method. This allows tweaking width dimensions at near-zero cost compared to regular NAS.- Overall, the paper provides both new theory and effective methods to understand and improve pruning of modern CNNs at initialization. The accuracy improvements, hardware-friendly structure, and ultra-fast architecture tuning help advance the state-of-the-art in efficient model compression.Some key differences compared to prior work are the theoretical analysis framework, structured pruning approach, accuracy improvements on various models/datasets, and formulation as an ultra-fast NAS technique. The paper builds nicely on top of previous PAI research while making several notable contributions.


## What future research directions do the authors suggest?

Based on my reading of the paper, some of the main future research directions suggested by the authors include:- Exploring different proxy functions for model accuracy in the pruning-at-initialization formulation, beyond just the expectation of the sum of importance scores. The authors mention trying other statistics like variance. - Applying the proposed structured pruning-at-initialization approach to additional CNN architectures beyond ResNet, MobileNetV2, and EfficientNet.- Further analysis into why the layer-wise density is the key factor determining model accuracy after pruning-at-initialization. The authors suggest theoretical analysis of how the lottery ticket hypothesis relates to their findings.- Extending the pruning-at-initialization optimization framework to support other resource constraints beyond just FLOPs and parameter counts, like latency, memory usage, etc.- Leveraging the ultra-fast neural architecture search enabled by PreConfig to do joint search over width, depth, and other dimensions. The authors suggest combining it with techniques like weight sharing.- Applying the structured pruning techniques to additional domains like natural language processing and investigating their effectiveness.- Further analysis of the tradeoffs between unstructured vs structured pruning in the context of pruning-at-initialization.So in summary, the main future directions are exploring new proxy accuracy functions, applying the methods to new models/tasks, theoretical analysis, supporting more constraints, combining with NAS, and analysis of pruning techniques. Overall the authors suggest many interesting avenues for extending their work.
