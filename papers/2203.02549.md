# [Structured Pruning is All You Need for Pruning CNNs at Initialization](https://arxiv.org/abs/2203.02549)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be:Can structured pruning at initialization achieve competitive accuracy compared to unstructured pruning at initialization while also being hardware friendly? The key hypotheses appear to be:1) The accuracy of CNN models pruned using pruning-at-initialization (PAI) methods depends only on the fraction of remaining parameters per layer (layer-wise density), not on which specific parameters are pruned.2) Using a new proxy for model accuracy called Synaptic Expectation (SynExp), PAI can be formulated as a convex optimization problem that directly solves for the optimal layer-wise density. 3) Based on this formulation, a structured PAI method called PreCrop that prunes models in the channel dimension can achieve accuracy comparable to unstructured PAI methods.4) A variant called PreConfig can further optimize the channel width of each layer to improve accuracy at almost zero cost compared to standard neural architecture search techniques.The paper seems to experimentally validate these hypotheses by comparing PreCrop and PreConfig against state-of-the-art unstructured PAI methods like SynFlow on modern CNN architectures and datasets. The key result appears to be that structured pruning via PreCrop/PreConfig can match or exceed the accuracy of SynFlow while also being hardware friendly.
