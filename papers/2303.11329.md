# [Sound Localization from Motion: Jointly Learning Sound Direction and   Camera Rotation](https://arxiv.org/abs/2303.11329)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question it addresses is: 

How can we jointly learn to localize sound sources and estimate camera rotation using only self-supervision from paired but unlabeled audio and visual data?

The key ideas and contributions are:

- Proposing the novel problem of "sound localization from motion" (SLfM), which involves jointly estimating sound direction and camera rotation from audio-visual data. 

- Presenting a self-supervised approach to learn these tasks, without requiring labeled training data. The models provide supervision for each other - the visual model predicts camera rotation while the audio model predicts sound direction. Their outputs are forced to be geometrically consistent.

- Introducing a cross-view binauralization pretext task to learn representations suitable for SLfM. This involves predicting binaural sound for a target view using a source view's audio and images from both views.

- Achieving strong performance on SLfM, including competitive results on real datasets for sound localization and camera pose estimation, using a completely self-supervised model trained on synthetic data.

In summary, the key hypothesis is that the correlated audio-visual signals from rotational motion can provide free supervision for jointly learning sound localization and camera pose estimation without labels. The paper presents models to validate this hypothesis and shows promising results.


## What is the main contribution of this paper?

 This paper proposes a self-supervised method for jointly learning camera rotation estimation from images and sound direction localization from binaural audio, without requiring labeled data. The key ideas are:

- They introduce the problem of "sound localization from motion" (SLfM), which involves jointly estimating the change in camera pose from two images along with the direction of a sound source from binaural audio. 

- They propose to learn representations suitable for this task through a cross-modal pretext task called "cross-view binauralization". This involves synthesizing binaural audio for a target viewpoint by conditioning on mono audio and an audio-visual observation from a different source viewpoint.

- The learned audio-visual representations are then used in a self-supervised framework to train models for visual pose estimation and binaural sound localization. The two models provide supervisory signals for each other, without ground truth labels.

- Experiments show they can accurately estimate pose on real images and localize sounds, competitive with supervised and classic methods. The features learned through their pretext task also outperform alternatives.

In summary, the main contribution is a self-supervised framework and audio-visual representation learning approach for jointly estimating camera motion and sound directions by exploiting the geometric consistency between the two modalities. The method avoids the need for labeled pose or sound direction data.
