# [Sound Localization from Motion: Jointly Learning Sound Direction and   Camera Rotation](https://arxiv.org/abs/2303.11329)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question it addresses is: How can we jointly learn to localize sound sources and estimate camera rotation using only self-supervision from paired but unlabeled audio and visual data?The key ideas and contributions are:- Proposing the novel problem of "sound localization from motion" (SLfM), which involves jointly estimating sound direction and camera rotation from audio-visual data. - Presenting a self-supervised approach to learn these tasks, without requiring labeled training data. The models provide supervision for each other - the visual model predicts camera rotation while the audio model predicts sound direction. Their outputs are forced to be geometrically consistent.- Introducing a cross-view binauralization pretext task to learn representations suitable for SLfM. This involves predicting binaural sound for a target view using a source view's audio and images from both views.- Achieving strong performance on SLfM, including competitive results on real datasets for sound localization and camera pose estimation, using a completely self-supervised model trained on synthetic data.In summary, the key hypothesis is that the correlated audio-visual signals from rotational motion can provide free supervision for jointly learning sound localization and camera pose estimation without labels. The paper presents models to validate this hypothesis and shows promising results.
