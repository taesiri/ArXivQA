# [Sound Localization from Motion: Jointly Learning Sound Direction and   Camera Rotation](https://arxiv.org/abs/2303.11329)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question it addresses is: 

How can we jointly learn to localize sound sources and estimate camera rotation using only self-supervision from paired but unlabeled audio and visual data?

The key ideas and contributions are:

- Proposing the novel problem of "sound localization from motion" (SLfM), which involves jointly estimating sound direction and camera rotation from audio-visual data. 

- Presenting a self-supervised approach to learn these tasks, without requiring labeled training data. The models provide supervision for each other - the visual model predicts camera rotation while the audio model predicts sound direction. Their outputs are forced to be geometrically consistent.

- Introducing a cross-view binauralization pretext task to learn representations suitable for SLfM. This involves predicting binaural sound for a target view using a source view's audio and images from both views.

- Achieving strong performance on SLfM, including competitive results on real datasets for sound localization and camera pose estimation, using a completely self-supervised model trained on synthetic data.

In summary, the key hypothesis is that the correlated audio-visual signals from rotational motion can provide free supervision for jointly learning sound localization and camera pose estimation without labels. The paper presents models to validate this hypothesis and shows promising results.


## What is the main contribution of this paper?

 This paper proposes a self-supervised method for jointly learning camera rotation estimation from images and sound direction localization from binaural audio, without requiring labeled data. The key ideas are:

- They introduce the problem of "sound localization from motion" (SLfM), which involves jointly estimating the change in camera pose from two images along with the direction of a sound source from binaural audio. 

- They propose to learn representations suitable for this task through a cross-modal pretext task called "cross-view binauralization". This involves synthesizing binaural audio for a target viewpoint by conditioning on mono audio and an audio-visual observation from a different source viewpoint.

- The learned audio-visual representations are then used in a self-supervised framework to train models for visual pose estimation and binaural sound localization. The two models provide supervisory signals for each other, without ground truth labels.

- Experiments show they can accurately estimate pose on real images and localize sounds, competitive with supervised and classic methods. The features learned through their pretext task also outperform alternatives.

In summary, the main contribution is a self-supervised framework and audio-visual representation learning approach for jointly estimating camera motion and sound directions by exploiting the geometric consistency between the two modalities. The method avoids the need for labeled pose or sound direction data.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the paper: 

The paper proposes a self-supervised method for jointly learning to estimate camera rotation from images and localize sound sources from binaural audio by enforcing consistency between the two tasks.


## How does this paper compare to other research in the same field?

 Here are a few key ways this paper compares to other research in the field of joint audio-visual learning:

- It introduces a new self-supervised task called "sound localization from motion" (SLfM) that aims to jointly learn sound direction estimation and camera rotation estimation from paired but unlabeled audio-visual data. This is a novel formulation that provides complementary self-supervision signals between the two modalities.

- Most prior work has focused on either self-supervised sound localization or camera pose estimation separately. Jointly learning them together with mutual consistency constraints is a unique aspect of this work.

- For sound localization, the method uses binaural audio cues rather than reconstructed spatial audio as in some other self-supervised approaches. It is trained on binaural room impulse responses to model real-world acoustics.

- For camera pose estimation, other self-supervised visual methods often rely on photometric consistency or feature matching. This method uses cross-modal audio consistency as supervision instead, which is a new source of self-supervision.

- The cross-view binauralization pretext task used to learn representations is also novel, requiring implicitly estimating sound source locations and view differences without direct supervision.

- Experiments show the approach achieves strong performance on both tasks competitively with state-of-the-art self-supervised methods, demonstrating the benefits of joint audio-visual learning.

- The method is developed and evaluated on both synthetic and real-world datasets, showing generalization ability.

Overall, the key novelty is in the joint self-supervised formulation and the use of cross-modal audio-visual signals, rather than relying only on cues from one modality like most prior works. The results validate that these complementary signals provide useful supervision for perceptual tasks.
