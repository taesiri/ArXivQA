# [Grounding 3D Object Affordance from 2D Interactions in Images](https://arxiv.org/abs/2303.10437)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question it seeks to address is:

How can we ground 3D object affordances from 2D human-object interactions depicted in images? 

In other words, the goal is to develop methods that can anticipate the parts of a 3D object that afford certain interactions, based on seeing examples of those interactions in 2D images. 

The key hypotheses/claims behind this question seem to be:

- Humans can perceive affordances of 3D objects by observing 2D images/videos of interactions with those objects. 

- Modeling the interaction context between humans/objects/scenes in images provides useful clues about the affordances of an object.

- Objects of the same category tend to have similar "combination schemes" of parts that enable certain affordances. Aligning features between object instances can enable transferring affordance knowledge.

So in summary, the paper introduces a new task formulation of grounding 3D affordances from 2D interactions, based on the hypothesis that important clues about an object's affordances can be obtained from how humans interact with that category of object in images. The proposed methods attempt to model these interaction contexts and align object part features to transfer affordance knowledge to new 3D object instances.
