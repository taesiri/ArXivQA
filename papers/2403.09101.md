# [Soften to Defend: Towards Adversarial Robustness via Self-Guided Label   Refinement](https://arxiv.org/abs/2403.09101)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Adversarial training (AT) is effective for making deep neural networks robust against adversarial attacks. However, most AT methods suffer from "robust overfitting", where there is a large gap between training and test set robust accuracy.

- The paper identifies a connection between robust overfitting and excessive memorization of noisy labels during AT. The gradient norm keeps increasing non-monotonically when robust overfitting happens, indicating the model starts memorizing noisy labels.  

- The label noise is mainly caused by distribution mismatch between clean and adversarial examples and improper hard label assignments. Hard one-hot labels are overconfident and uninformative.

Proposed Solution: 
- The paper proposes a Self-Guided Label Refinement (SGLR) approach to combat robust overfitting in AT.

- SGLR first uses the model's predictions to refine the hard labels into soft labels that are more accurate and informative. This retains similarity information between classes.

- It then calibrates training by dynamically incorporating knowledge from self-distilled models using exponential moving average. This does not need external teachers.

- SGLR reduces the memorization of noisy labels, making training more efficient. Theoretical analysis shows SGLR reduces the mutual information between labels and weights.

Main Contributions:
- Identifies connection between robust overfitting, gradient norm increase and label noise memorization.

- Proposes SGLR method to refine labels and calibrate training to combat robust overfitting. Achieves superior accuracy and robustness.

- Closes the generalization gap to 0.5% on CIFAR-10, significantly reducing overfitting. Robust accuracy reaches 56.4%.

- Shows SGLR is noise-tolerant. Achieves consistent test set improvement across datasets, perturbations and architectures.

- Provides theoretical analysis using information theory and connections to related works.
