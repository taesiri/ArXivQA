# [A Curious Case of Remarkable Resilience to Gradient Attacks via Fully   Convolutional and Differentiable Front End with a Skip Connection](https://arxiv.org/abs/2402.17018)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
The paper investigates weaknesses in the popular AutoAttack evaluation suite for estimating adversarial robustness of image classifiers. Specifically, the authors show that models enhanced with a differentiable denoising front-end and trained with a variant of adversarial training can appear highly robust to AutoAttack, while actually having near-zero robustness against adaptive attacks that bypass the front-end.

Proposed Solution: 
The authors propose augmenting standard image classifiers like ResNets and Vision Transformers with a denoising convolutional front-end model called DnCNN. This front-end model is frozen and the composite model is trained for 1 epoch or less using adversarial training with a very small learning rate. 

It is shown that this recipe produces models that retain full clean accuracy, but have extreme gradient masking - standard projected gradient descent attacks only degrade performance by a few percent, despite it being possible to reach 0% accuracy with adaptive attacks. Further, simple randomized ensembles of such models can achieve near state-of-the-art robust accuracy on CIFAR and ImageNet under AutoAttack, while having virtually no robustness under adaptive attacks.

Main Contributions:

- Showing that differentiable front-ends trained with adversarial training can cause extreme gradient masking even without non-differentiable components.

- Demonstrating that the popular AutoAttack suite can greatly overestimate robustness for such front-end enhanced models and their randomized ensembles, with near SOTA AutoAttack accuracy but 0% robustness under adaptive attacks.

- Providing evidence that this is due to gradient masking and not numerical instability or vanishing gradients.

- Showing black-box attacks like SQUARE and zero-order PGD have limited effectiveness against the gradient masking exhibited by their approach.

- Arguing that the findings further demonstrate that adaptive attacks are crucial for reliably evaluating adversarial robustness.

In summary, the key contribution is revealing limitations of current standardized evaluation to reliably estimate robustness for certain types of gradient masking defenses. The authors argue for the importance of adaptive attacks in robustness evaluation.
