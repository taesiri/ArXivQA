# AdaPlanner: Adaptive Planning from Feedback with Language Models

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central research question this paper addresses is how to develop an adaptive planning agent using large language models (LLMs) that can effectively leverage environment feedback to refine its plans. The key hypothesis is that by designing an explicit closed-loop planning approach with both in-plan and out-of-plan refinement strategies, the LLM agent can adaptively modify its plans based on environment observations and improve its sequential decision-making performance.Specifically, the paper proposes a method called AdaPlanner that allows the LLM agent to:- Generate an initial plan by decomposing complex tasks into sub-goals. - Perform in-plan refinement to extract useful information from aligned feedback using an ask_LLM() action.- Conduct out-of-plan refinement to revise the entire plan when there is misaligned feedback from the environment.The central hypothesis is that this adaptive closed-loop planning approach with both levels of refinement will enable more effective planning and improve the agent's ability to complete tasks successfully compared to existing LLM methods. The experiments aim to validate whether AdaPlanner can outperform current state-of-the-art baselines in text-based environments.


## What is the main contribution of this paper?

Based on my reading of the paper abstract, the main contributions appear to be:1. The paper proposes AdaPlanner, a new closed-loop planning approach that enables large language models (LLMs) to adaptively refine generated plans based on feedback from the environment. 2. AdaPlanner introduces two refinement strategies - in-plan refinement to extract useful information from observations to ground actions, and out-of-plan refinement to revise the entire plan when deviations occur.3. To reduce LLM hallucination, AdaPlanner uses code-style prompting for plan generation and refinement.4. AdaPlanner also incorporates a skill discovery mechanism to accumulate successful experiences and guide future planning with few-shot examples.5. Comprehensive experiments in ALFWorld and MiniWoB++ environments demonstrate that AdaPlanner achieves state-of-the-art performance while utilizing significantly fewer samples than existing methods.In summary, the main contribution appears to be the proposal of AdaPlanner, an adaptive closed-loop planning approach for LLMs that integrates code prompting, in-plan and out-of-plan refinement strategies, and skill discovery to enhance planning performance and sample efficiency. The effectiveness of AdaPlanner is validated through experiments on decision-making benchmarks.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

The paper proposes AdaPlanner, a closed-loop approach that allows large language models (LLMs) to adaptively refine their self-generated plans in response to environmental feedback. The key ideas are using LLMs for both initial planning and plan refinement, handling in-plan and out-of-plan feedback differently, and using code-style prompting and skill discovery to mitigate hallucination and improve sample efficiency. The main contribution is demonstrating how adaptive closed-loop planning with LLMs can achieve state-of-the-art performance on text-based decision making benchmarks while utilizing significantly fewer samples.In one sentence: The paper introduces AdaPlanner, an adaptive closed-loop planning approach that enables LLMs to refine plans from environmental feedback, achieving superior performance with high sample efficiency on text-based decision making tasks.


## How does this paper compare to other research in the same field?

Here are some thoughts on how this paper compares to other research in the field of using large language models for decision making:- The paper proposes a novel explicit closed-loop approach called AdaPlanner that enables the language model agent to continuously refine its plans based on environment feedback. This sets it apart from most prior methods that either use open-loop planning without adaptation or only do implicit closed-loop refinement where actions are adjusted but the overall plan stays fixed. The full plan refinement of AdaPlanner allows more flexibility to correct issues that could propagate.- AdaPlanner relies solely on prompting the language model, without any training. Many other approaches require a training phase on task-specific data which limits their generalization. Prompting alone makes AdaPlanner widely applicable.- The code-style prompting used in AdaPlanner is unique and helps mitigate hallucination issues that can arise with language models. Other prompting-based methods mostly use natural language which can be more ambiguous. The code prompts add structure.- AdaPlanner incorporates skill discovery to build up a memory bank of successful plans to guide future planning. This improves sample efficiency compared to methods that generate plans completely from scratch each time. Skill discovery for boosting performance on new tasks is an innovative element.- Comprehensively, AdaPlanner combines explicit closed-loop planning, code-style prompting, and skill discovery in a novel way. The experiments demonstrate superior performance over existing baselines in multiple text-based environments. The approach appears widely generalizable. The code prompting in particular helps address language model limitations. Overall, this paper pushes forward the state-of-the-art in applying language models for planning and decision making.In summary, the novelty of AdaPlanner is in its adaptive closed-loop architecture, code prompting strategy, skill discovery mechanism, and how these elements come together to advance language model agents. The approach differs meaningfully from prior methods and demonstrates advancement of the field.
