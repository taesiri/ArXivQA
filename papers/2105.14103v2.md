# [An Attention Free Transformer](https://arxiv.org/abs/2105.14103v2)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question is: How can we develop an efficient alternative to the standard transformer architecture that eliminates the need for dot product self-attention while maintaining competitive performance?

The key hypothesis seems to be that an "attention-free" operation that combines the key/value vectors with learned position biases in an element-wise fashion can provide competitive performance and efficiency compared to standard self-attention, while removing the quadratic complexity bottleneck. 

Specifically, the paper introduces and evaluates the proposed Attention Free Transformer (AFT) and its variants (AFT-local, AFT-conv, etc.) across image modeling, language modeling, and image classification tasks. The goal is to demonstrate that AFT can match or exceed standard transformer performance across these domains while providing greatly improved efficiency in terms of computational complexity and memory usage.

In summary, the core research question is whether the proposed attention-free mechanisms can serve as an effective replacement for standard self-attention in transformers, providing efficiency gains without sacrificing too much performance. The paper aims to empirically validate this hypothesis across several domains.


## What is the main contribution of this paper?

 Based on my reading of the paper, the main contributions are:

1. Proposing Attention Free Transformer (AFT), a new variant of Transformers that eliminates the need for dot product self-attention. Instead, AFT combines the key and value with learned position biases first, and then multiplies the result with the query in an element-wise fashion. 

2. Introducing two model variants AFT-local and AFT-conv that apply the idea of locality and spatial weight sharing while still maintaining global connectivity.

3. Demonstrating through experiments on image autoregressive modeling, language modeling, and image classification that AFT achieves competitive performance compared to standard Transformers and other efficient variants, while providing improved efficiency in terms of computational and memory complexity.

4. Providing analysis and ablation studies on the design choices of AFT like the factorized parameterization, effect of locality, compatibility with Transformers, and inducing sparsity.

In summary, the main contribution is proposing AFT as a more efficient replacement to standard dot product attention in Transformers, while showing it can achieve highly competitive performance across diverse tasks. The variants like AFT-local and AFT-conv further improve the efficiency and performance.
