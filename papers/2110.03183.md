# [Attention is All You Need? Good Embeddings with Statistics are   enough:Large Scale Audio Understanding without Transformers/ Convolutions/   BERTs/ Mixers/ Attention/ RNNs or ....](https://arxiv.org/abs/2110.03183)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be:Can we achieve competitive performance on large-scale audio understanding tasks using only simple neural network architectures and statistics on learned embeddings, without relying on more complex methods like convolutional neural networks, Transformers, attention, etc?The key hypothesis appears to be that by learning embeddings on different representations of the mel spectrogram input (envelopes, patches, downsampled version) using basic autoencoders, clustering them, and doing statistics on the codebook assignments (bag-of-words style), they can build acoustic models that perform surprisingly well compared to state-of-the-art convolutional and Transformer models.The paper seems aimed at showing that strong baseline performance can be achieved on audio tasks using techniques that could have been applied over a decade ago, without needing the complex neural architectures that dominate current research. The goal is to demonstrate the representational power of embeddings plus statistics, and set a simple but hard-to-beat baseline.
