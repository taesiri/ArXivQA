# [DITTO: Demonstration Imitation by Trajectory Transformation](https://arxiv.org/abs/2403.15203)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem: Teaching robots new manipulation skills quickly and conveniently is important for broader adoption of robots. A common approach is imitation learning where humans demonstrate skills by teleoperating the robot or physical kinesthetic teaching. However, these approaches have downsides like requiring significant operator training or restricting the robot's workspace. An alternative is to have robots passively observe human demonstrations, but this suffers from embodiment differences between humans and robots.

Proposed Solution: The paper proposes DITTO, a novel modular method for one-shot transfer of manipulation skills from RGB-D videos of human demonstrations to robots. It consists of two main stages:

1) Trajectory Extraction: Given a video of a human demonstration, object masks and poses over time are extracted using segmentation and correspondence methods. This gives the trajectory of the manipulated object and other relevant objects like containers.

2) Trajectory Generation: For a new scene, objects are re-detected and relative poses are estimated. The extracted demonstration trajectory is warped based on the new object poses and interpolated if multiple objects are involved. Grasps on the target object are predicted and robot joint trajectories computed to replicate motions along the warped trajectory using inverse kinematics.

Key technical details:
- Uses Hands23 for object-hand segmentation and RAFT optical flow for within-video pose tracking.
- Compares RAFT and LoFTR flows for re-detection across videos.
- Warps trajectories based on object or container pose changes using SE(3) transformations and slerp interpolation. 
- Employs off-the-shelf grasping method to generate end-effector trajectories.

Main Contributions:
1) A new modular pipeline for one-shot RGB-D video to robot imitation learning using object-centric trajectories.
2) Systematic comparison of design choices for correspondence and re-detection components.
3) Extensive real robot evaluations over 10 manipulation tasks to validate the approach.
4) Open-source code and data benchmark for reproducibility.
