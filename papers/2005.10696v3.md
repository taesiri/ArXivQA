# [Novel Policy Seeking with Constrained Optimization](https://arxiv.org/abs/2005.10696v3)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading, the key research question addressed in this paper is:

How can we enable reinforcement learning agents to discover diverse and novel solutions for a given task, akin to how humans approach problem solving? 

The paper proposes a new method called "Interior Policy Differentiation (IPD)" to generate novel and well-performing policies for RL agents. The key ideas are:

- Defining a new metric space to measure differences between policies, allowing novelty to be quantified. 

- Formulating the novelty-seeking problem as a constrained optimization rather than multi-objective optimization. This avoids hindering performance on the primary task while seeking diversity.

- Deriving a practical algorithm inspired by interior point methods that bounds policy search into "feasible regions" to ensure novelty, without needing explicit novelty rewards.

The paper aims to show that this constrained optimization perspective allows generating diverse yet high-performing policies, overcoming limitations of prior multi-objective approaches that often sacrifice performance for novelty. The main hypothesis is that IPD can achieve substantially better novelty and task performance compared to previous methods.

Does this summarize the key research question and contribution? Let me know if you need any clarification or have a different interpretation of the core ideas.


## What is the main contribution of this paper?

 Based on my reading of the paper, the main contributions appear to be:

1. The introduction of a new metric space to measure differences between policies. Specifically, the authors propose using the Wasserstein metric rather than total variation distance. 

2. Formulating the problem of generating diverse policies as a constrained optimization problem rather than a multi-objective optimization problem. This helps avoid some issues like performance decay when seeking novelty.

3. Proposing a practical algorithm called Interior Policy Differentiation (IPD) for constrained optimization of policies. This algorithm terminates episodes early to implicitly enforce diversity constraints.

4. Demonstrating improved performance of IPD compared to prior methods on MuJoCo benchmark environments. The authors show IPD can generate more diverse policies without sacrificing performance on the main task.

In summary, the key novelty seems to be the formulation as a constrained optimization problem and the proposed IPD algorithm that takes advantage of this perspective to efficiently generate diverse, high-performing policies. The introduction of the Wasserstein metric is also a contribution, though the main benefit comes from the constrained optimization view.
