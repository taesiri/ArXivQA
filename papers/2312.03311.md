# [On the Nystrom Approximation for Preconditioning in Kernel Machines](https://arxiv.org/abs/2312.03311)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a summary paragraph of the key points from the paper:

This paper analyzes the use of Nyström approximation to compute an approximated spectral preconditioner for accelerating iterative optimization algorithms when training kernel machines. The analysis shows that using just logarithmic (in the number of training samples $n$) Nyström samples suffices to obtain an approximated preconditioner that speeds up gradient descent nearly as much as using the exact preconditioner, while also reducing the computational and storage overheads from quadratic to nearly linear. Specifically, the main result proves that with high probability the condition number using the approximated preconditioner created with $s=\Omega(\frac{\log^4 n}{\epsilon^4})$ Nyström samples has condition number at most $(1+\epsilon)^4$ times that of using the exact preconditioner. This translates to a guaranteed speedup over unpreconditioned gradient descent by an extra factor of nearly $\frac{1}{(1+\epsilon)^4}$. The proof hinges on bounding the distortion when mapping functions via the square root of approximated covariance matrix versus the square root of exact covariance matrix in terms of the operator norm distance between these matrices, and uses matrix concentration inequalities. Overall, this rigorously quantifies the approximation-computation tradeoffs when using Nyström approximation for preconditioning in scalable kernel methods.
