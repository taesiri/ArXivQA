# [Unsupervised Graph Neural Architecture Search with Disentangled   Self-supervision](https://arxiv.org/abs/2403.05064)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
The paper studies the problem of unsupervised graph neural architecture search (GNAS). Existing GNAS methods rely heavily on supervised labels during the search process and fail to handle scenarios where labels are scarce or unavailable. The key challenges are: 1) Discovering latent graph factors that drive the formation of graph data; 2) Capturing the relations between factors and optimal neural architectures. These factors and architectures are highly entangled, making the problem difficult.

Proposed Solution: 
The paper proposes a novel framework called Disentangled Self-supervised Graph Neural Architecture Search (DSGAS) that can discover optimal architectures capturing various latent factors without labels. The key ideas are:

1) Design a disentangled graph super-network with multiple architectures that are optimized simultaneously towards disentangled factors. 

2) Propose self-supervised training to estimate architecture performance under each factor. It first infers factors conditioned on architectures, then conducts factor-specific pretext tasks using corresponding architectures.

3) Introduce contrastive search with architecture augmentations. An instance discrimination task is designed to encourage discovering architectures with distinct capabilities in capturing factors.

Main Contributions:

- First work on unsupervised GNAS that can discover neural architectures without labels.

- Propose three novel components - disentangled super-network, self-supervised training with joint architecture-graph disentanglement, and contrastive search to capture relations between architectures and graph factors.

- Experiments on 11 datasets demonstrate superior performance over SOTA GNAS baselines under both unsupervised and semi-supervised settings. Ablations verify the efficacy of each proposed component.

In summary, the paper studies the important yet untouched problem of unsupervised GNAS. It proposes an end-to-end solution with disentangled self-supervision that shows great promises.


## Summarize the paper in one sentence.

 Here is a one-sentence summary of the paper:

The paper proposes a novel unsupervised graph neural architecture search method called Disentangled Self-supervised Graph Neural Architecture Search (DSGAS) that can discover optimal graph neural network architectures capturing various latent graph factors in a self-supervised fashion without relying on labels.


## What is the main contribution of this paper?

 The main contribution of this paper is proposing a novel unsupervised graph neural architecture search method called Disentangled Self-supervised Graph Neural Architecture Search (DSGAS). Specifically:

1) The paper proposes the first method to study the problem of unsupervised graph neural architecture search, which aims to automatically discover optimal graph neural network architectures without relying on label supervision. This allows the method to handle scenarios where labels are scarce or unavailable.

2) The paper introduces three novel components to enable unsupervised GNAS: i) a disentangled graph architecture super-network that can incorporate and optimize multiple architectures simultaneously, ii) a self-supervised training method with joint architecture-graph disentanglement to accurately estimate architecture performance, and iii) a contrastive search method with architecture augmentations to discover architectures capturing different latent graph factors.

3) Extensive experiments on 11 real-world datasets demonstrate that DSGAS significantly outperforms state-of-the-art GNAS methods in the unsupervised setting. Detailed ablation studies also verify the efficacy of each proposed component.

In summary, the key contribution is proposing the first unsupervised GNAS approach along with three new techniques to enable discovering optimal architectures without label supervision.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and keywords associated with this paper include:

- Graph neural architecture search (GNAS)
- Unsupervised graph neural architecture search
- Disentangled graph architecture super-network
- Self-supervised training 
- Joint architecture-graph disentanglement
- Contrastive search
- Architecture augmentations
- Graph neural networks (GNNs)
- Graph representation learning
- Neural architecture search (NAS)

The paper proposes a new method called "Disentangled Self-supervised Graph Neural Architecture Search" (DSGAS) for unsupervised GNAS. The key ideas include using a disentangled super-network to capture different latent graph factors, a self-supervised training method with joint architecture-graph disentanglement to estimate architecture performance, and a contrastive search method with architecture augmentations to discover architectures capturing different factors. Overall, the paper tackles the new problem of unsupervised GNAS using concepts of disentanglement and self-supervision.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in this paper:

1. The key innovation of this paper is proposing a disentangled self-supervised framework for unsupervised graph neural architecture search. Can you explain in detail the rationale behind adopting a disentangled design? What are the main challenges it aims to address? 

2. One main component of the proposed method is the disentangled graph architecture super-network. Can you walk through how the disentangled super-network layer and overall architecture are constructed? What is the intuition behind having multiple architectures that capture different latent factors?

3. The self-supervised training module contains an architecture-aware latent factor inference step. What is the motivation behind involving the architectures in the latent factor inference? How does the model formally represent and infer the factor probabilities? 

4. The paper mentions jointly disentangling the architectures and graphs within a common latent space during self-supervised training. Can you explain what mechanism allows this joint disentanglement and how it leads to more accurate architecture performance estimation?

5. Contrastive search with architecture augmentations is proposed to further encourage disentanglement of architectures. What insights motivate the design of this module? How do the architecture-level instance discrimination task and augmentations achieve the goal?

6. Three architecture augmentation methods are introduced, including operation choice perturbation, weight perturbation, and embedding dropout. Can you explain in detail how each of these augmentation techniques modifies the architecture? 

7. The ablation study shows that removing the contrastive search module leads to a significant performance drop. What does this result potentially imply about the role and impact of contrastive search?

8. For the architecture visualizations in Figure 4, it is mentioned that architectures share some operations but have overall complex internal connections. What does this observation tell us about the searched architectures?

9. How does the number of latent factors K affect the complexity and performance of the searched architectures? What flexibility does it provide in terms of the search space?

10. One limitation mentioned is extending the method to heterogeneous graphs. What additional challenges do you think heterogeneous graphs would introduce to the current disentangled self-supervised framework?
