# [Unsupervised Graph Neural Architecture Search with Disentangled   Self-supervision](https://arxiv.org/abs/2403.05064)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
The paper studies the problem of unsupervised graph neural architecture search (GNAS). Existing GNAS methods rely heavily on supervised labels during the search process and fail to handle scenarios where labels are scarce or unavailable. The key challenges are: 1) Discovering latent graph factors that drive the formation of graph data; 2) Capturing the relations between factors and optimal neural architectures. These factors and architectures are highly entangled, making the problem difficult.

Proposed Solution: 
The paper proposes a novel framework called Disentangled Self-supervised Graph Neural Architecture Search (DSGAS) that can discover optimal architectures capturing various latent factors without labels. The key ideas are:

1) Design a disentangled graph super-network with multiple architectures that are optimized simultaneously towards disentangled factors. 

2) Propose self-supervised training to estimate architecture performance under each factor. It first infers factors conditioned on architectures, then conducts factor-specific pretext tasks using corresponding architectures.

3) Introduce contrastive search with architecture augmentations. An instance discrimination task is designed to encourage discovering architectures with distinct capabilities in capturing factors.

Main Contributions:

- First work on unsupervised GNAS that can discover neural architectures without labels.

- Propose three novel components - disentangled super-network, self-supervised training with joint architecture-graph disentanglement, and contrastive search to capture relations between architectures and graph factors.

- Experiments on 11 datasets demonstrate superior performance over SOTA GNAS baselines under both unsupervised and semi-supervised settings. Ablations verify the efficacy of each proposed component.

In summary, the paper studies the important yet untouched problem of unsupervised GNAS. It proposes an end-to-end solution with disentangled self-supervision that shows great promises.
