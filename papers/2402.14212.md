# [Moonwalk: Inverse-Forward Differentiation](https://arxiv.org/abs/2402.14212)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Backpropagation is effective for computing gradients in neural networks but has high memory overhead from storing activations during forward and backward passes, limiting model scalability. 
- Forward-mode differentiation reduces memory usage by not storing activations but is computationally expensive, requiring computation of full Jacobians.
- Prior projected forward-mode methods introduce variance and sacrifice gradient fidelity.

Proposed Solution:
- Restrict analysis to invertible neural networks where layer inputs can be reconstructed from outputs. 
- Introduce "Moonwalk", a novel forward-mode method exploiting an identity allowing the gradient w.r.t. layer inputs to be used in an inverse Jacobian product along with the Jacobian w.r.t. parameters to compute true parameter gradients in forward mode.

Key Contributions:  
1. Moonwalk: A pure forward-mode method that computes true gradients with complexity O(n^3L + ndL) using the input gradient and vector inverse-Jacobian products, being the first forward-mode method to significantly reduce memory while having reasonable time complexity.

2. Mixed-mode Moonwalk: A variant using backpropagation for the input gradient computation to achieve O(n^2L + ndL) complexity comparable to backpropagation but with smaller memory footprint of O(M_x L + M_θ) vs O(M_x L + M_θ L).

Overall, Moonwalk variants offer the first forward-mode methods to compute true gradients in invertible networks at comparable or better time complexity than backpropagation while using less memory, with the pure version optimized for memory reduction and the mixed version optimizing time complexity. The methods showcase robust performance across architectures.
