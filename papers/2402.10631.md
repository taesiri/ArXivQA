# [BitDistiller: Unleashing the Potential of Sub-4-Bit LLMs via   Self-Distillation](https://arxiv.org/abs/2402.10631)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key aspects of the paper:

Problem:
- Deploying large language models (LLMs) is challenging due to their substantial memory and computational requirements. 
- Weight quantization can enhance efficiency but going below 4-bits causes significant performance degradation, especially for smaller models or complex reasoning tasks.
- Existing post-training quantization (PTQ) and quantization-aware training (QAT) methods still have room for improvement in terms of maximally preserving weight fidelity and enabling effective low-bit representation learning.

Proposed Solution:
- BitDistiller, a framework that combines QAT with knowledge distillation (KD) for boosting ultra-low precision (sub-4-bit) LLM performance.
- Employs asymmetric quantization and clipping to maximally maintain capabilities of full-precision model. 
- Proposes Confidence-Aware KL divergence (CAKLD) objective for self-distillation where full-precision model teaches low-precision model. CAKLD trades off mode-seeking and mode-covering behaviors based on model confidence.

Main Contributions:
- Significantly outperforms existing PTQ and QAT methods for both 3-bit and 2-bit quantization on diverse language and reasoning tasks.
- More cost-effective, requiring less training data and compute resources.
- Establishes favorable bit-level scaling laws for sub-4-bit regime, unlocking potential for deployment on resource-constrained devices without compromising much performance.
- In-depth analyses provide insights into quantization techniques, distillation objectives, and self-distillation efficacy.

In summary, BitDistiller pushes state-of-the-art for ultra low-bit LLM quantization via effectively combining QAT and KD, enabling efficient deployment without severely degrading capabilities.
