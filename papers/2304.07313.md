# [M2T: Masking Transformers Twice for Faster Decoding](https://arxiv.org/abs/2304.07313)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research questions/hypotheses appear to be:

- Can bidirectional transformers trained for masked token prediction be effectively applied to neural image compression to achieve state-of-the-art results? 

- Can a deterministic, fixed schedule for unmasking tokens during inference perform as well as or better than adaptive, input-dependent schedules?

- Can a model that masks both the transformer inputs and attentions ("masking twice") provide a good speed-quality tradeoff by enabling caching?

The key ideas and contributions seem to be:

- Showing that a simple MaskGIT-like transformer can achieve SOTA image compression, without needing special positional encodings or multi-scale architectures.

- Demonstrating that a fixed unmasking schedule works just as well as adaptive schedules for compression.

- Proposing a new "masking twice" model that masks both inputs and attentions, enabling caching and faster inference while maintaining good rate-distortion performance.

So in summary, the main goals appear to be pushing the state-of-the-art in neural image compression with transformers, and developing faster transformer architectures for this task. The key hypotheses are around the efficacy of fixed schedules and double masking.


## What is the main contribution of this paper?

 The main contributions of this paper are:

1. A MaskGIT-like transformer model (M2T) for neural image compression that achieves state-of-the-art rate-distortion performance. The model uses standard transformers applied to image patches and does not require a multi-scale model.

2. A faster variant of the model (M2T) that masks both the input and the attention layers. This allows caching of activations during inference, leading to 2.7x-4.8x speedups over the MaskGIT-like model with only a small increase in bitrate. 

In summary, the key contributions are a simple transformer architecture for image compression that is state-of-the-art in rate-distortion performance, and a modification to make it significantly faster with minimal performance cost. The models demonstrate competitive performance compared to prior work while using standard transformer components.


## How does this paper compare to other research in the same field?

 Here are some thoughts on how this paper compares to other related work in neural image compression:

- This paper introduces two new transformer-based models (MT and M2T) for neural image compression that achieve state-of-the-art rate-distortion performance. Previous work had explored using transformers in this domain, but the methods were often slow or lagged in RD performance. 

- Compared to prior work using convolutional neural networks like ELIC, the proposed MT model achieves better rate-distortion performance on standard test sets like Kodak. The M2T model trades off a small increase in rate for substantially faster decoding.

- Relative to autoregressive transformer models like ContextFormer, the proposed models are much faster due to the use of masking rather than full autoregressive modeling. The M2T model in particular can decode high resolution images in under a second.

- The models rely on standard transformer encoder architectures and scalar quantization, making them simpler than methods that use things like hyperpriors or multi-scale transforms. The patching scheme allows handling arbitrary resolutions.

- The introduction of the new M2T model bridges ideas from masked image modeling and autoregressive transformers. It generalizes autoregressive and masked approaches and provides a useful speed-quality tradeoff.

- The visualizations of model uncertainty during decoding provide some interesting insights into the coarse-to-fine progressive transmission enabled by the masking schedule.

- The comparison of different masking schedules sheds light on the rate-distortion tradeoffs in these masked transformer models. The proposed QLDS schedule works well.

In summary, this paper pushes state-of-the-art in neural compression with conceptually simple and fast transformer models, while also providing modeling insights that advance understanding of how to effectively apply transformers in this domain. The models and analysis help move these methods closer to practical application.
