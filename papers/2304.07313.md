# [M2T: Masking Transformers Twice for Faster Decoding](https://arxiv.org/abs/2304.07313)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research questions/hypotheses appear to be:- Can bidirectional transformers trained for masked token prediction be effectively applied to neural image compression to achieve state-of-the-art results? - Can a deterministic, fixed schedule for unmasking tokens during inference perform as well as or better than adaptive, input-dependent schedules?- Can a model that masks both the transformer inputs and attentions ("masking twice") provide a good speed-quality tradeoff by enabling caching?The key ideas and contributions seem to be:- Showing that a simple MaskGIT-like transformer can achieve SOTA image compression, without needing special positional encodings or multi-scale architectures.- Demonstrating that a fixed unmasking schedule works just as well as adaptive schedules for compression.- Proposing a new "masking twice" model that masks both inputs and attentions, enabling caching and faster inference while maintaining good rate-distortion performance.So in summary, the main goals appear to be pushing the state-of-the-art in neural image compression with transformers, and developing faster transformer architectures for this task. The key hypotheses are around the efficacy of fixed schedules and double masking.
