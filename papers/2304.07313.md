# [M2T: Masking Transformers Twice for Faster Decoding](https://arxiv.org/abs/2304.07313)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research questions/hypotheses appear to be:

- Can bidirectional transformers trained for masked token prediction be effectively applied to neural image compression to achieve state-of-the-art results? 

- Can a deterministic, fixed schedule for unmasking tokens during inference perform as well as or better than adaptive, input-dependent schedules?

- Can a model that masks both the transformer inputs and attentions ("masking twice") provide a good speed-quality tradeoff by enabling caching?

The key ideas and contributions seem to be:

- Showing that a simple MaskGIT-like transformer can achieve SOTA image compression, without needing special positional encodings or multi-scale architectures.

- Demonstrating that a fixed unmasking schedule works just as well as adaptive schedules for compression.

- Proposing a new "masking twice" model that masks both inputs and attentions, enabling caching and faster inference while maintaining good rate-distortion performance.

So in summary, the main goals appear to be pushing the state-of-the-art in neural image compression with transformers, and developing faster transformer architectures for this task. The key hypotheses are around the efficacy of fixed schedules and double masking.


## What is the main contribution of this paper?

 The main contributions of this paper are:

1. A MaskGIT-like transformer model (M2T) for neural image compression that achieves state-of-the-art rate-distortion performance. The model uses standard transformers applied to image patches and does not require a multi-scale model.

2. A faster variant of the model (M2T) that masks both the input and the attention layers. This allows caching of activations during inference, leading to 2.7x-4.8x speedups over the MaskGIT-like model with only a small increase in bitrate. 

In summary, the key contributions are a simple transformer architecture for image compression that is state-of-the-art in rate-distortion performance, and a modification to make it significantly faster with minimal performance cost. The models demonstrate competitive performance compared to prior work while using standard transformer components.
