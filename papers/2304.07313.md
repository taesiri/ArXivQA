# [M2T: Masking Transformers Twice for Faster Decoding](https://arxiv.org/abs/2304.07313)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research questions/hypotheses appear to be:

- Can bidirectional transformers trained for masked token prediction be effectively applied to neural image compression to achieve state-of-the-art results? 

- Can a deterministic, fixed schedule for unmasking tokens during inference perform as well as or better than adaptive, input-dependent schedules?

- Can a model that masks both the transformer inputs and attentions ("masking twice") provide a good speed-quality tradeoff by enabling caching?

The key ideas and contributions seem to be:

- Showing that a simple MaskGIT-like transformer can achieve SOTA image compression, without needing special positional encodings or multi-scale architectures.

- Demonstrating that a fixed unmasking schedule works just as well as adaptive schedules for compression.

- Proposing a new "masking twice" model that masks both inputs and attentions, enabling caching and faster inference while maintaining good rate-distortion performance.

So in summary, the main goals appear to be pushing the state-of-the-art in neural image compression with transformers, and developing faster transformer architectures for this task. The key hypotheses are around the efficacy of fixed schedules and double masking.


## What is the main contribution of this paper?

 The main contributions of this paper are:

1. A MaskGIT-like transformer model (M2T) for neural image compression that achieves state-of-the-art rate-distortion performance. The model uses standard transformers applied to image patches and does not require a multi-scale model.

2. A faster variant of the model (M2T) that masks both the input and the attention layers. This allows caching of activations during inference, leading to 2.7x-4.8x speedups over the MaskGIT-like model with only a small increase in bitrate. 

In summary, the key contributions are a simple transformer architecture for image compression that is state-of-the-art in rate-distortion performance, and a modification to make it significantly faster with minimal performance cost. The models demonstrate competitive performance compared to prior work while using standard transformer components.


## How does this paper compare to other research in the same field?

 Here are some thoughts on how this paper compares to other related work in neural image compression:

- This paper introduces two new transformer-based models (MT and M2T) for neural image compression that achieve state-of-the-art rate-distortion performance. Previous work had explored using transformers in this domain, but the methods were often slow or lagged in RD performance. 

- Compared to prior work using convolutional neural networks like ELIC, the proposed MT model achieves better rate-distortion performance on standard test sets like Kodak. The M2T model trades off a small increase in rate for substantially faster decoding.

- Relative to autoregressive transformer models like ContextFormer, the proposed models are much faster due to the use of masking rather than full autoregressive modeling. The M2T model in particular can decode high resolution images in under a second.

- The models rely on standard transformer encoder architectures and scalar quantization, making them simpler than methods that use things like hyperpriors or multi-scale transforms. The patching scheme allows handling arbitrary resolutions.

- The introduction of the new M2T model bridges ideas from masked image modeling and autoregressive transformers. It generalizes autoregressive and masked approaches and provides a useful speed-quality tradeoff.

- The visualizations of model uncertainty during decoding provide some interesting insights into the coarse-to-fine progressive transmission enabled by the masking schedule.

- The comparison of different masking schedules sheds light on the rate-distortion tradeoffs in these masked transformer models. The proposed QLDS schedule works well.

In summary, this paper pushes state-of-the-art in neural compression with conceptually simple and fast transformer models, while also providing modeling insights that advance understanding of how to effectively apply transformers in this domain. The models and analysis help move these methods closer to practical application.


## What future research directions do the authors suggest?

 Based on my reading of the paper, some of the main future research directions suggested by the authors are:

- Exploring different masking schedules for the M2T model. The authors tested deterministic and random schedules, but suggest there may be better schedules to uncover tokens that could improve compression performance.

- Applying the M2T model to video compression. The authors demonstrated promising results on image compression, so video could be a natural next step. The temporal dimension in video could allow for additional speedups.

- Testing different transformer architectures. The authors used a standard BERT-style transformer, but other architectures like sparse or efficient transformers may provide further speedups.

- Combining with neural synthesis models at very low bitrates. The authors suggest combining their approach with generative models like VQ-VAE could allow compressing down to 0.1 bpp or below.

- Exploring different tokenization strategies beyond patches. The patched tokenization used works well but may not capture all cross-patch dependencies. 

- Applying the techniques to other modalities like audio, point clouds, etc. The core ideas could generalize.

In summary, the main directions are exploring different masking strategies, architectures, and modalities to further improve the efficiency and effectiveness of the approach. The combination of masked transformers and neural synthesis also seems promising for extreme compression.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the paper:

The paper proposes two transformer-based models for neural image compression that achieve state-of-the-art rate-distortion performance. The first model (M2T) is similar to MaskGIT in using masked transformers for image generation, but shows that a fixed, deterministic schedule for unmasking tokens during inference works just as well as adaptive schedules. This allows the introduction of the second model that masks both the input and the attention, making the model causal and enabling caching for faster inference. Experiments demonstrate that the first model outperforms previous compression methods, while the second model provides a 2.7-4.8x speedup over the first with minimal performance degradation. Overall, the work demonstrates how simple modifications to standard transformers can yield powerful and practical neural image compression models.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the paper:

The paper presents two transformer-based models for neural image compression. The first model, called \vanilla, is similar to MaskGIT and predicts distributions over image patches in a masked, iterative fashion during inference. This model achieves state-of-the-art rate-distortion performance on standard benchmarks. However, its inference speed is relatively slow. 

To address this, the authors propose a second model called \perm that masks both the input and the attention. The input masking creates a causal dependence that, together with the attention masking, enables caching of activations during inference. This results in a 2.7-4.8x speedup over \vanilla with only a small drop in rate-distortion performance. The authors demonstrate \perm can compress large megapixel images in under a second on consumer GPUs, making it practical for real-world usage. In summary, the work pushes the state-of-the-art in neural image compression while also improving inference speed.


## Summarize the main method used in the paper in one paragraph.

 The main method used in this paper is a transformer-based approach for neural image compression. The key ideas are:

1) They use a standard transformer encoder architecture (similar to BERT) to model the probability distribution of a quantized image representation. This allows them to entropy code the representation at very low bitrates. 

2) They explore two models: (a) A vanilla MaskGIT-like model that uses input masking but standard transformer attention during training. (b) A novel model called M2T that masks both the input and the attention, making the model causal and allowing activation caching during inference. 

3) M2T bridges between MaskGIT-style transformers and autoregressive transformers, by using variable-sized groups at the input and corresponding attention masking. It is able to achieve a 2.7-4.8x speedup over the vanilla model with only a small increase in bitrate.

4) They demonstrate state-of-the-art rate-distortion performance on Kodak and CLIC datasets using the vanilla model. The M2T model retains most of these gains while being much faster.

5) Both models rely on standard transformer components and scalar quantization, without needing a hyperprior or multi-scale decomposition like much prior work on neural compression. Overall, the paper presents a simple yet effective way to leverage transformers for state-of-the-art and practical neural image compression.
