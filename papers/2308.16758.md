# [Towards High-Fidelity Text-Guided 3D Face Generation and Manipulation   Using only Images](https://arxiv.org/abs/2308.16758)

## What is the central research question or hypothesis that this paper addresses?

This paper proposes a novel method for generating 3D faces given textual descriptions. The central research question it aims to address is: how to generate realistic and semantically consistent 3D faces from text, given the limited availability of text-3D face pairs for training. The key hypotheses of this work are:1) It is possible to learn to generate text-guided 3D faces by using only text-2D face image pairs, by transferring the semantic consistency from text to 2D faces to guide 3D face generation. 2) Adding global text-to-face contrastive learning and fine-grained text-to-face alignment during training can further enhance the semantic consistency between the generated 3D faces and input text descriptions.3) A directional classifier guidance approach during inference can help generate more creative and style-controlled 3D faces guided by the text.In summary, this paper explores a new text-guided 3D face generation framework to address the lack of text-3D training data, and proposes several techniques to improve semantic alignment between generated 3D faces and input texts. The main hypothesis is that high quality and semantically consistent text-guided 3D faces can be generated through the proposed model trained on only text-2D face data.


## What is the main contribution of this paper?

The main contribution of this paper is proposing a novel method for text-guided 3D face generation that can synthesize high-quality 3D faces with multi-view consistency from textual descriptions, despite the lack of large-scale text-3D face data pairs for training. The key highlights are:- They adopt an unconditional 3D face generation framework and equip it with text conditioning using only text-2D face data, transferring the text-image semantics to guide 3D face generation. - Two text-to-face cross-modal alignment techniques are proposed, including global contrastive learning and fine-grained alignment, to improve semantic consistency between generated 3D faces and input texts.- Directional classifier guidance is utilized during inference to encourage creativity and generate novel out-of-domain styles not seen during training. - Extensive experiments show their method can generate more realistic and aesthetically pleasing 3D faces with better consistency than baselines, while enabling applications like text-guided editing and single-view 3D face reconstruction.In summary, the main contribution is presenting a complete framework to address the challenging task of text-guided 3D face generation through innovative techniques to align cross-modal text-face semantics despite limited 3D supervision. The results showcase state-of-the-art performance and creative generation capabilities.
