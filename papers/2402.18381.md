# [Large Language Models As Evolution Strategies](https://arxiv.org/abs/2402.18381)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem
- Recently, large language models (LLMs) have shown impressive capabilities for in-context learning, where they can learn algorithms and solve problems just from the prompt without any gradient updates. 

- This paper investigates whether LLMs can perform black-box optimization (BBO) and act as evolution strategies (ES) when prompted appropriately. Specifically, can an LLM optimize the weights of a neural network using evolutionary/genetic algorithms, without taking gradients?

Method
- The authors propose a prompting strategy to turn LLMs into ES algorithms (called EvoLLM). This involves representing solutions as discretized integers, sorting them from least-to-best fitness, prompting the LLM to propose a new "mean" parameter setting that improves on the best fitness so far.

- To handle large search spaces, solutions are split into blocks and separate LLM queries are made per block. LLM outputs are parsed back into floating point numbers to sample new candidate solutions.  

- The prompt design is ablated - the LLM is robust to prompt details, but degrades without fitness scores or improving solution sequences.

Results
- EvoLLM outperforms baselines like random search on BBOB optimization tasks, small network control tasks, across LLMs like GPT-4, PaLM. Smaller LLMs perform better than larger ones.

- Solution representation matters - discretized integers work better than raw text. Too coarse/fine resolutions degrade performance. Limited context still works.

- Fine-tuning on BBO trajectories of "teacher" algorithms like Hill Climbing further improves EvoLLM.

Contributions
- First method prompting LLMs to perform ES and optimize neural networks without taking gradients
- Systematic proposal and evaluation of prompt design strategies 
- Demonstration that EvoLLM outperforms baselines on BBOB and control tasks
- Shows pre-trained capabilities of LLMs for BBO, potential for further improvements via fine-tuning
