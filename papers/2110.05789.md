# [Learning Discrete Representations via Constrained Clustering for   Effective and Efficient Dense Retrieval](https://arxiv.org/abs/2110.05789)

## What is the central research question or hypothesis that this paper addresses?

Based on my understanding, the key research question this paper tries to address is: How to jointly learn discrete representations for document embeddings to enable efficient dense retrieval without significantly hurting ranking effectiveness?The authors propose a novel model called RepCONC to tackle this challenge. The key ideas include:1) Modeling quantization as constrained clustering during joint training of dual encoders and quantization methods like product quantization (PQ). 2) Introducing a uniform clustering constraint to require document embeddings to be equally quantized to all centroids. This helps produce more balanced and distinguishable discrete codes compared to unconstrained clustering.3) Approximating the computation of the uniform clustering constraint as an optimal transport problem for efficiency.4) Employing inverted file system on top of PQ to enable fast approximate nearest neighbor search on CPUs. By jointly optimizing the discrete codes and encoders under the constrained clustering framework, RepCONC aims to achieve substantially higher efficiency in memory and computation time while preserving the ranking effectiveness of dense retrieval models. Experiments on two standard ad-hoc retrieval benchmarks demonstrate the advantages of RepCONC over both vector quantization baselines and a wide range of retrieval models.In summary, the core focus is on developing a constrained clustering approach to learn efficient yet effective discrete representations for dense retrieval. Both the theoretical modeling and empirical results try to demonstrate its effectiveness in balancing efficiency and accuracy.


## What is the main contribution of this paper?

Based on my reading, the main contributions of this paper are:1. It proposes RepCONC, a novel retrieval model that jointly learns discrete representations and dual-encoders for dense retrieval. RepCONC models quantization as constrained clustering to enable end-to-end optimization of Product Quantization parameters and dual-encoders. 2. It introduces a uniform clustering constraint during training to require document embeddings to be equally quantized to all centroids. This helps tackle the issue of unbalanced clustering distribution and indistinguishable discrete codes. The constraint is approximately solved as an optimal transport problem.3. It integrates inverted file system into RepCONC to enable efficient vector search on CPUs. 4. Through experiments on two retrieval benchmarks, it demonstrates RepCONC's advantages over competitive baselines in terms of retrieval effectiveness, memory efficiency, and time efficiency. The ablation study shows the effectiveness of constrained clustering.In summary, the key contribution is proposing constrained clustering for end-to-end learning of efficient and effective dense representations. The uniform clustering constraint and its connection to optimal transport are novel. Integrating inverted file system also enables efficient search on CPUs.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper presents RepCONC, a new retrieval model that jointly trains dual-encoders and Product Quantization to learn discrete document representations that enable efficient nearest neighbor search through constrained clustering optimization and inverted indexes.


## How does this paper compare to other research in the same field?

This paper presents RepCONC, a new dense retrieval model that learns discrete representations of documents via constrained clustering. Here is a summary of how it compares to related work:- Most existing dense retrieval models use brute-force search over uncompressed embeddings, which is inefficient in memory and computation. RepCONC jointly optimizes dual-encoders and product quantization to enable compressed document representations and fast approximate nearest neighbor search. This allows more efficient retrieval compared to models like ANCE, RepBERT, etc.- Some recent work has also explored learning compressed representations, like JPQ and DPQ. However, RepCONC introduces a novel constrained clustering approach during training to optimize the discrete codes, while prior methods use fixed codes or heuristics like batch k-means. Experiments show RepCONC outperforms these baselines.- RepCONC also employs inverted indexing on top of product quantization for additional speedup. This enables very fast CPU-based search compared to uncompressed models that rely purely on GPUs. The two-stage retrieval enabled by RepCONC outperforms end-to-end models like COIL and ColBERT.- The uniform clustering constraint and optimal transport solution used during training are unique contributions not explored in prior work. Ablations show these are critical to RepCONC's effectiveness.In summary, RepCONC pushes state-of-the-art in dense retrieval by enabling much more efficient search without sacrificing accuracy. The constrained clustering approach and joint optimization framework appear novel and help the model surpass previous baselines. The work addresses a key limitation of dense retrieval and seems like an important advance for scalable and fast neural search.
