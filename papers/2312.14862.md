# [YAYI 2: Multilingual Open-Source Large Language Models](https://arxiv.org/abs/2312.14862)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Large language models (LLMs) have shown impressive capabilities, but most state-of-the-art models are proprietary and not easily accessible. Open-source LLMs tend to focus on English and perform poorly on Chinese.
- There is a lack of high-quality, large-scale Chinese training data and models tailored for Chinese applications.

Proposed Solution - YAYI2:  
- Pretrain 30B parameter multilingual models (base and chat) from scratch on 2.65 trillion cleaned Chinese-English tokens (41.5% Chinese).
- Develop rigorous data processing pipeline for 240TB raw web data spanning news, books, codes etc. 
- Use efficiently optimized model architecture leveraging FlashAttention2 and multi-query attention.
- Perform supervised fine-tuning on millions of Chinese instruction-output pairs across diverse domains and tasks.
- Further align models to human preferences through reinforcement learning from human feedback.

Main Contributions:
- Release the first open-source 30B parameter models tailored for Chinese applications.
- Demonstrate state-of-the-art results across Chinese and English knowledge, reasoning and programming benchmarks, outperforming models 2x the size. 
- Enable long context reasoning with up to 128k tokens for conversational ability.
- Provide model training details to facilitate research reproducibility.
- Highlight extensive safety measures taken during data curation and model development.
