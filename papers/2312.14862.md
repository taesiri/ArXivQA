# [YAYI 2: Multilingual Open-Source Large Language Models](https://arxiv.org/abs/2312.14862)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Large language models (LLMs) have shown impressive capabilities, but most state-of-the-art models are proprietary and not easily accessible. Open-source LLMs tend to focus on English and perform poorly on Chinese.
- There is a lack of high-quality, large-scale Chinese training data and models tailored for Chinese applications.

Proposed Solution - YAYI2:  
- Pretrain 30B parameter multilingual models (base and chat) from scratch on 2.65 trillion cleaned Chinese-English tokens (41.5% Chinese).
- Develop rigorous data processing pipeline for 240TB raw web data spanning news, books, codes etc. 
- Use efficiently optimized model architecture leveraging FlashAttention2 and multi-query attention.
- Perform supervised fine-tuning on millions of Chinese instruction-output pairs across diverse domains and tasks.
- Further align models to human preferences through reinforcement learning from human feedback.

Main Contributions:
- Release the first open-source 30B parameter models tailored for Chinese applications.
- Demonstrate state-of-the-art results across Chinese and English knowledge, reasoning and programming benchmarks, outperforming models 2x the size. 
- Enable long context reasoning with up to 128k tokens for conversational ability.
- Provide model training details to facilitate research reproducibility.
- Highlight extensive safety measures taken during data curation and model development.


## Summarize the paper in one sentence.

 This paper proposes YAYI 2, a series of multilingual large language models with 30 billion parameters, pre-trained on 2.65 trillion tokens and aligned through supervised fine-tuning and reinforcement learning, which achieves state-of-the-art performance among similar-sized models on benchmarks of knowledge, math, and programming.


## What is the main contribution of this paper?

 This paper proposes YAYI 2, a series of multilingual large language models (LLMs) including base and chat models with 30 billion parameters. The main contributions are:

1. It constructs a multilingual pre-training corpus containing 2.65 trillion tokens and designs a rigorous data processing pipeline. 41.5% of the data is Chinese text.

2. It elaborates the model architecture, training strategies and tricks of YAYI 2 models. The models adopt FlashAttention 2, multi-query attention, SwiGLU activation, etc. to improve efficiency.

3. It aligns YAYI 2 base model through supervised fine-tuning on millions of high-quality instruction-output pairs and reinforcement learning from human feedback. The model supports long instructions, multi-turn conversations and domain-specific applications. 

4. It evaluates YAYI 2 base model on benchmarks of knowledge, math, and programming. Results show superiority over similar-sized open-source LLMs, and even some larger models.

In summary, this paper proposes the YAYI 2 series of LLMs, describes the model training details, and demonstrates strong performance on multiple benchmarks, delivering impressive capabilities especially for Chinese-related applications.


## What are the keywords or key terms associated with this paper?

 Based on my review of the paper, some of the key terms and keywords related to this work include:

- Large language models (LLMs)
- Multilingual models
- Pre-training 
- Data processing pipeline
- Tokenization  
- Model architecture
- Training strategies
- Supervised fine-tuning (SFT)  
- Reinforcement learning from human feedback (RLHF)
- Safety and alignment
- Evaluations and benchmarks
- Knowledge understanding 
- Mathematical reasoning
- Programming ability

The paper proposes YAYI 2, a series of multilingual large language models with 30 billion parameters, pre-trained on a diverse multilingual dataset spanning over 10 languages. It provides details on the model training, alignment, safety mechanisms, and comprehensive evaluations on benchmarks covering different capabilities. The key terms reflect the main themes and contributions discussed in this technical report.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 potential in-depth questions about the method proposed in this paper:

1. The paper mentions using a comprehensive data security filtering and classification system during data preparation. Could you elaborate on the specifics of this system, such as what algorithms are used and how it identifies inappropriate or harmful content?

2. When discussing the pre-training data sources, the paper excludes certain problematic open datasets like OSCAR. What issues did you find with those datasets that led to their exclusion? What steps did you take to vet datasets for inclusion?

3. In the discussion of multi-turn conversations for alignment, you use both context-relevant and context-irrelevant samples. What was the rationale behind including both types? Does the model handle these two types differently during training?

4. The paper mentions adapting the model for efficient inference on Ascend 910B chips. What specifics challenges did you face during this process compared to inference on GPUs? How did you address issues like large parameter counts?

5. When sampling instruction data for alignment, the paper uses dynamic weighted sampling to maintain balance. What metrics did you use to assess imbalance and how frequently does the sampling distribution require adjustment?

6. For reinforcement learning, how did you generate the two responses per prompt used to train the reward model? What differences in sampling strategy yielded sufficiently different outputs?

7. In the pre-training data preprocessing pipeline, after heuristic cleaning, you utilize both paragraph and sentence level deduplication. When would redundant content be caught at one level and missed at the other?

8. For toxicity filtering during data preprocessing, how was the classification model trained to broaden recognition capabilities and improve generalizability? What techniques did this include?

9. The YaRN method is used during inference for improving context lengths beyond 4K tokens. How specifically does the integration of NTK and sliding windows enhance extrapolation stability at very long contexts?

10. When training the multilingual tokenizer, the paper utilizes raw text without normalization. Why is training on raw text advantageous? Does this increase the complexity of BPE training in any way?
