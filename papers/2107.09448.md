# [DNN is not all you need: Parallelizing Non-Neural ML Algorithms on   Ultra-Low-Power IoT Processors](https://arxiv.org/abs/2107.09448)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be: How can key non-neural machine learning (ML) algorithms be efficiently parallelized to run on resource-constrained parallel ultra-low power (PULP) IoT processors?The authors aim to optimize the parallel design of six widely-used non-neural ML algorithms (including SVM, logistic regression, Gaussian Naive Bayes, etc.) to maximize performance on two RISC-V based PULP platforms - the commercial GAP8 chip and the PULP-OPEN research architecture. The key ideas and contributions appear to be:- Comparing performance of optimized sequential implementations on GAP8 using different floating point emulation libraries vs. native FP support on PULP-OPEN- Parallelization strategies and fine-grained analysis to maximize parallel speedup on the multi-core clusters of the PULP platforms- Experimental results demonstrating the achieved speedups and efficiencies when running the parallelized non-neural ML algorithms on the PULP platforms- Comparison to ARM Cortex-M4 showing the performance benefits of using the PULP architectures, especially with multi-core parallelismSo in summary, the central hypothesis seems to be that careful optimization and parallelization of key non-neural ML algorithms can enable efficient deployment of ML at the edge on parallel ultra-low power IoT processors like PULP. The paper provides extensive experimental analysis to demonstrate and validate this idea.


## What is the main contribution of this paper?

Based on my reading, the main contributions of this paper are:- The authors optimize the parallel design of six key non-neural machine learning algorithms (logistic regression, support vector machine, Gaussian naive Bayes, random forest, k-nearest neighbors, k-means) to maximize performance on two RISC-V based parallel ultra-low power (PULP) platforms - GAP8 and PULP-OPEN. - They analyze the performance of these algorithms using two different floating point emulation libraries on GAP8, which lacks native FPU support, versus using the native FPU on PULP-OPEN. Their optimized FP emulation library (RVfplib) achieves 1.61x average speedup over the standard libgcc emulation on GAP8. The FPU support on PULP-OPEN leads to up to 32x speedup over emulation on GAP8.- They evaluate the parallel performance of the optimized implementations on the 8-core clusters of GAP8 and PULP-OPEN, achieving near ideal speedups of 6.56-7.64x versus single core. Detailed analysis highlights the architectural factors limiting parallel speedups on each platform.- Comparison with ARM Cortex-M4 shows 1.36-2.39x speedup on single core PULP-OPEN over Cortex-M4, and 9.27-15.85x speedup using 8-core PULP-OPEN.In summary, the key contributions are the optimized parallel design and implementation of non-neural ML algorithms on PULP platforms, extensive performance analysis exploring different hardware configurations, and demonstration of significant speedups over ARM Cortex-M4.
