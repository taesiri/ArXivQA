# [A Static Evaluation of Code Completion by Large Language Models](https://arxiv.org/abs/2306.03203)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be:How can we effectively evaluate code completion suggestions from large language models using static analysis instead of execution-based methods?The key points related to this question are:- Execution-based evaluation of model-generated code is accurate but expensive and focused on simple problems. Static analysis with linters can complement it by catching a wide range of errors efficiently without running code.- The authors propose a pipeline to statically analyze errors in model-generated Python code completions using AST parsing and the Pyflakes linter.- They apply this pipeline to evaluate completions from CodeGen models on a new real-world function completion benchmark with 100K examples.- The analysis reveals common errors like undefined names and unused variables, as well as how factors like temperature, model scale, and context impact errors.So in summary, the central hypothesis is that static analysis can efficiently evaluate code generation models on complex real-world code, in order to understand their weaknesses and improve their quality. The paper presents evidence for this via the proposed analysis pipeline and experiments.


## What is the main contribution of this paper?

Based on my reading, the main contributions of this paper appear to be:1. Proposing a static evaluation framework to detect errors in code completions generated by large language models. The key components are parsing code snippets into abstract syntax trees (ASTs) and then analyzing them using the Pyflakes static analysis tool.2. Building a dataset of 100K Python function completion problems by sampling from open source GitHub repos. This simulates real-world usage of code auto-completion. 3. Evaluating public large language models (e.g. CodeGen) by generating 1 million completions on this dataset. The static analysis reveals common errors made by the models, like undefined names and unused variables.4. Showing the effects of various factors on errors through experiments, including sampling temperature, model size, and errors existing in the input context. For example, higher temperatures lead to more errors, while larger models help with some error types but not all.5. Providing insights into weaknesses of current code generation models, which can inform future research to improve code quality and accuracy.In summary, the main contribution appears to be proposing a novel static analysis framework for evaluating code generation models, and using it to analyze errors in a large scale experiment on real-world Python code. The findings reveal strengths and weaknesses of existing models.
