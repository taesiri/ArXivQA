# [A Static Evaluation of Code Completion by Large Language Models](https://arxiv.org/abs/2306.03203)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be:How can we effectively evaluate code completion suggestions from large language models using static analysis instead of execution-based methods?The key points related to this question are:- Execution-based evaluation of model-generated code is accurate but expensive and focused on simple problems. Static analysis with linters can complement it by catching a wide range of errors efficiently without running code.- The authors propose a pipeline to statically analyze errors in model-generated Python code completions using AST parsing and the Pyflakes linter.- They apply this pipeline to evaluate completions from CodeGen models on a new real-world function completion benchmark with 100K examples.- The analysis reveals common errors like undefined names and unused variables, as well as how factors like temperature, model scale, and context impact errors.So in summary, the central hypothesis is that static analysis can efficiently evaluate code generation models on complex real-world code, in order to understand their weaknesses and improve their quality. The paper presents evidence for this via the proposed analysis pipeline and experiments.
