# [A Static Evaluation of Code Completion by Large Language Models](https://arxiv.org/abs/2306.03203)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be:How can we effectively evaluate code completion suggestions from large language models using static analysis instead of execution-based methods?The key points related to this question are:- Execution-based evaluation of model-generated code is accurate but expensive and focused on simple problems. Static analysis with linters can complement it by catching a wide range of errors efficiently without running code.- The authors propose a pipeline to statically analyze errors in model-generated Python code completions using AST parsing and the Pyflakes linter.- They apply this pipeline to evaluate completions from CodeGen models on a new real-world function completion benchmark with 100K examples.- The analysis reveals common errors like undefined names and unused variables, as well as how factors like temperature, model scale, and context impact errors.So in summary, the central hypothesis is that static analysis can efficiently evaluate code generation models on complex real-world code, in order to understand their weaknesses and improve their quality. The paper presents evidence for this via the proposed analysis pipeline and experiments.


## What is the main contribution of this paper?

Based on my reading, the main contributions of this paper appear to be:1. Proposing a static evaluation framework to detect errors in code completions generated by large language models. The key components are parsing code snippets into abstract syntax trees (ASTs) and then analyzing them using the Pyflakes static analysis tool.2. Building a dataset of 100K Python function completion problems by sampling from open source GitHub repos. This simulates real-world usage of code auto-completion. 3. Evaluating public large language models (e.g. CodeGen) by generating 1 million completions on this dataset. The static analysis reveals common errors made by the models, like undefined names and unused variables.4. Showing the effects of various factors on errors through experiments, including sampling temperature, model size, and errors existing in the input context. For example, higher temperatures lead to more errors, while larger models help with some error types but not all.5. Providing insights into weaknesses of current code generation models, which can inform future research to improve code quality and accuracy.In summary, the main contribution appears to be proposing a novel static analysis framework for evaluating code generation models, and using it to analyze errors in a large scale experiment on real-world Python code. The findings reveal strengths and weaknesses of existing models.


## How does this paper compare to other research in the same field?

Here are a few key points on how this paper compares to related work in code completion evaluation:- The paper focuses on static analysis for evaluating code completions, rather than execution-based evaluation which has been more common. Static analysis is more efficient and can handle real-world code, but cannot evaluate functional correctness.- The function completion dataset from GitHub repositories is more realistic than many existing benchmark datasets that use algorithmic problems or synthetic examples. Evaluating on real-world code is important.- The analysis examines a broad range of static error types using the Pyflakes tool. Many prior works have focused on only detecting a few specific bug types. - The large scale evaluation on 1 million completions provides insights on how factors like model scale, temperature, and context impact different error rates. Most prior work evaluates on much smaller samples.- The paper demonstrates the value in leveraging existing static analysis tools to evaluate generative models. Others have proposed developing specialized analysis methods for code generation.Overall, the key novelties seem to be the more realistic evaluation dataset, the large scale static analysis across error types, and insights on how model and data factors correlate with bugs. The findings on errors like undefined names highlight challenges for current models.
