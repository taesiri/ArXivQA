# [Towards Deeper Graph Neural Networks](https://arxiv.org/abs/2007.09296)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is: How can we enable deeper graph neural networks that leverage larger receptive fields without suffering from performance deterioration like over-smoothing?The key points are:- Current graph neural networks like GCN suffer from performance degradation when stacking multiple layers to increase receptive field size. This has been attributed to the over-smoothing issue where node representations become indistinguishable. - The paper argues that the key factor contributing to this performance degradation is the entanglement of feature transformation and propagation in graph convolutions. - By decoupling feature transformation and propagation, the paper shows that deeper graph networks can leverage larger receptive fields without over-smoothing.- The paper also provides theoretical analysis on very deep models that aligns with the over-smoothing issue. - Based on these insights, the paper proposes a Deep Adaptive Graph Neural Network (DAGNN) that can incorporate information from large, adaptive receptive fields to learn node representations.In summary, the central research question is how to build deeper graph neural networks that can leverage larger receptive fields without suffering from issues like over-smoothing that deteriorate performance. The key hypothesis is decoupling feature transformation from propagation can achieve this.
