# [Towards Deeper Graph Neural Networks](https://arxiv.org/abs/2007.09296)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is: How can we enable deeper graph neural networks that leverage larger receptive fields without suffering from performance deterioration like over-smoothing?The key points are:- Current graph neural networks like GCN suffer from performance degradation when stacking multiple layers to increase receptive field size. This has been attributed to the over-smoothing issue where node representations become indistinguishable. - The paper argues that the key factor contributing to this performance degradation is the entanglement of feature transformation and propagation in graph convolutions. - By decoupling feature transformation and propagation, the paper shows that deeper graph networks can leverage larger receptive fields without over-smoothing.- The paper also provides theoretical analysis on very deep models that aligns with the over-smoothing issue. - Based on these insights, the paper proposes a Deep Adaptive Graph Neural Network (DAGNN) that can incorporate information from large, adaptive receptive fields to learn node representations.In summary, the central research question is how to build deeper graph neural networks that can leverage larger receptive fields without suffering from issues like over-smoothing that deteriorate performance. The key hypothesis is decoupling feature transformation from propagation can achieve this.


## What is the main contribution of this paper?

Based on my reading, the main contributions of this paper are:1. It provides a systematic analysis of the performance deterioration issue in deep graph neural networks, attributing it primarily to the entanglement of feature transformation and propagation in graph convolution operations. 2. It argues that by decoupling feature transformation from propagation, deeper graph neural networks can leverage larger receptive fields without suffering from performance degradation. The over-smoothing issue only affects performance at extremely large receptive fields.3. It gives a theoretical analysis of the above observations for very deep models, providing a rigorous description of the over-smoothing issue. 4. Based on the analysis, it proposes a Deep Adaptive Graph Neural Network (DAGNN) that can incorporate information from large and adaptive receptive fields to learn node representations.5. It conducts extensive experiments on citation, co-authorship and co-purchase datasets, demonstrating the effectiveness of the proposed DAGNN model compared to prior state-of-the-art methods.In summary, the key contribution is providing new insights into building deeper graph neural networks by decoupling feature transformation and propagation, along with proposing an effective model DAGNN based on this. The analysis and experiments support these insights.


## How does this paper compare to other research in the same field?

Here are some key ways this paper compares to other research in the field of graph neural networks:- Focus on performance deterioration in deep graph neural networks: The paper focuses specifically on analyzing why stacking multiple layers in graph neural networks leads to performance degradation, an issue that has been observed but not deeply studied in prior work. - Systematic analysis of over-smoothing: The paper provides a systematic empirical and theoretical analysis of the over-smoothing phenomenon that has been hypothesized to cause performance deterioration. It argues over-smoothing alone does not fully explain the problem.- Decoupling of representation transformation and propagation: A key insight is that entanglement of these two operations is a major factor compromising performance in deep models. The paper shows decoupling them allows building deeper models without degradation.- Adaptive neighborhood aggregation: The proposed model incorporates an adaptive adjustment mechanism to aggregate information from different receptive fields tailored to each node. This is a novel way to leverage multi-scale structure.- Theoretical analysis of deep models: The paper provides formal convergence analysis of common propagation schemes in infinitely deep models, giving a rigorous take on over-smoothing.- State-of-the-art performance: Experiments demonstrate the proposed DAGNN model achieves new state-of-the-art results on several citation, co-authorship and co-purchase graphs.Overall, the analysis and innovations around building deeper graph neural networks with adaptive neighborhood aggregation seem to be significant contributions compared to prior work focused on shallow models and analyzing over-smoothing in isolation. The paper provides both theoretical and empirical justification for its ideas.
