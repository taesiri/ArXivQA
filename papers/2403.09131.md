# [ProSwitch: Knowledge-Guided Language Model Fine-Tuning to Generate   Professional and Non-Professional Styled Text](https://arxiv.org/abs/2403.09131)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Large language models (LLMs) like ChatGPT have shown ability to generate high-quality text, but their capacity to switch between different styles via fine-tuning is underexplored. 
- In particular, the ability to produce both professional and non-professional styled text in response to queries has not been well studied.
- Being able to switch between formal and informal styles could enhance user comprehension and information retrieval.

Proposed Solution:
- The authors introduce ProSwitch, a novel methodology to equip LLMs with the ability to produce both professional and non-professional responses. 
- ProSwitch has 3 main phases:
   1) Data preparation to gather domain knowledge and training corpus
   2) Instruction tuning to optimize LM with multiple levels of prompt formats 
   3) Comprehensive evaluation to assess professionalism discrimination and quality

Main Contributions:
- First study on generating professional vs non-professional text by exploiting external knowledge and internal LM knowledge
- Analysis of multi-level instruction tuning strategy for this task 
- Comprehensive evaluation methodology assessing various aspects like terminology use, reasoning, and language quality

- Show that ProSwitch outperforms general and specialized LM baselines in switching between styles while maintaining quality
- Demonstrate feasibility of acquiring professionalism discrimination without compromising text generation skills

In summary, the paper introduces a novel approach to improve LMs' ability to switch between professional and non-professional styles for textual responses, with a rigorous evaluation methodology. The results highlight the promise of instruction tuning guided by domain knowledge for controllable text generation.
