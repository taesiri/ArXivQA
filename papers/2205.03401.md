# The Unreliability of Explanations in Few-shot Prompting for Textual   Reasoning

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central research question this paper addresses is: Can prompting large language models (LLMs) like GPT-3 with explanations improve their performance on textual reasoning tasks like question answering and natural language inference? The paper specifically investigates whether including explanations in the prompt examples for few-shot in-context learning leads to accuracy improvements over standard few-shot learning without explanations. The authors test this hypothesis across three datasets spanning question answering and natural language inference.In summary, the main research question is whether explanations in prompts can boost the in-context learning capabilities of LLMs for textual reasoning tasks. The paper examines this through experiments on QA and NLI datasets.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contributions appear to be:- Investigating the nature of explanations generated by large language models (LLMs) when prompted to produce them, focusing on tasks involving reasoning over text (question answering and natural language inference). - Finding that simply including explanations in prompts does not substantially improve few-shot in-context learning performance for textual reasoning across different LLMs.- Showing that the explanations generated by LLMs are often unreliable - they may be inconsistent with the model's predictions or not factually grounded in the input, even on simple tasks. - Demonstrating that unreliable explanations are actually indicative of incorrect predictions. By approximating explanation reliability with simple lexical overlap features, the predictions can be improved by rejecting or downweighting examples with unreliable explanations.- Proposing a general framework for using explanations to calibrate LLM predictions post-hoc and showing this improves performance across several textual reasoning datasets.So in summary, the key ideas are studying explanation generation by LLMs for textual reasoning, finding they are unreliable but still useful for calibration, and leveraging them to improve in-context learning.
