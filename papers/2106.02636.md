# [MERLOT: Multimodal Neural Script Knowledge Models](https://arxiv.org/abs/2106.02636)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central hypothesis of this paper is that learning from unlabeled videos and their transcripts can produce models with improved commonsense reasoning and temporal understanding, compared to learning from static images paired with captions. Specifically, the authors introduce a model called MERLOT that is pretrained on a large corpus of YouTube videos and ASR transcripts in a self-supervised manner, without any human annotations. They hypothesize that by training the model with objectives that involve contextualizing frames over time, matching frames to contextualized transcripts, and temporal reordering, MERLOT will learn improved multimodal representations that transfer better to downstream tasks requiring temporal reasoning.The paper then validates this hypothesis through experiments showing that MERLOT achieves state-of-the-art performance on a diverse set of 12 video and image QA datasets when finetuned. Additional analyses provide evidence that:- Pretraining on videos works better than static images- Using a large and diverse video corpus improves performance - The model continues improving with more pretraining, suggesting the representations have not saturatedOverall, the central hypothesis is that self-supervised pretraining on videos can learn improved temporal commonsense reasoning, which the paper aims to validate through the MERLOT model and experiments.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contributions are:1. They introduce MERLOT, a model that learns multimodal script knowledge by self-supervised pretraining on large amounts of unlabelled YouTube videos and transcripts.2. They collect and curate a new diverse dataset called YT-Temporal-180M from 6 million YouTube videos for pretraining MERLOT. This dataset covers a wide range of everyday events and situations.3. They propose a combination of pretraining objectives like contrastive frame-transcript matching, masked language modeling with attention masking, and temporal reordering that allows MERLOT to learn full-stack visual reasoning from recognition to cognition level. 4. Extensive experiments show MERLOT achieves new state-of-the-art results on 12 video QA datasets when finetuned, demonstrating its strong multimodal and temporal representations.5. Ablation studies provide insights into the benefits of training on diverse video data, using multiple complementary pretraining objectives, and continuing to pretrain for more iterations.6. Qualitative analyses illustrate MERLOT's capability for temporal commonsense reasoning in zero-shot settings like ordering scrambled image sequences into coherent narratives.In summary, the main contribution is developing the MERLOT model to learn rich multimodal script knowledge from unlabeled videos at scale, and showing its effectiveness on a variety of static and video reasoning tasks. The new pretraining dataset, objectives, and analyses are also key contributions.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence TL;DR summary of the paper:The paper introduces MERLOT, a model that learns multimodal script knowledge by self-supervised pretraining on 6 million unlabelled YouTube videos with speech transcripts, achieving state-of-the-art performance when finetuned on 12 different video QA datasets and demonstrating strong zero-shot temporal reasoning ability.
