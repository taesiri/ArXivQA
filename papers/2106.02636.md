# [MERLOT: Multimodal Neural Script Knowledge Models](https://arxiv.org/abs/2106.02636)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central hypothesis of this paper is that learning from unlabeled videos and their transcripts can produce models with improved commonsense reasoning and temporal understanding, compared to learning from static images paired with captions. Specifically, the authors introduce a model called MERLOT that is pretrained on a large corpus of YouTube videos and ASR transcripts in a self-supervised manner, without any human annotations. They hypothesize that by training the model with objectives that involve contextualizing frames over time, matching frames to contextualized transcripts, and temporal reordering, MERLOT will learn improved multimodal representations that transfer better to downstream tasks requiring temporal reasoning.The paper then validates this hypothesis through experiments showing that MERLOT achieves state-of-the-art performance on a diverse set of 12 video and image QA datasets when finetuned. Additional analyses provide evidence that:- Pretraining on videos works better than static images- Using a large and diverse video corpus improves performance - The model continues improving with more pretraining, suggesting the representations have not saturatedOverall, the central hypothesis is that self-supervised pretraining on videos can learn improved temporal commonsense reasoning, which the paper aims to validate through the MERLOT model and experiments.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contributions are:1. They introduce MERLOT, a model that learns multimodal script knowledge by self-supervised pretraining on large amounts of unlabelled YouTube videos and transcripts.2. They collect and curate a new diverse dataset called YT-Temporal-180M from 6 million YouTube videos for pretraining MERLOT. This dataset covers a wide range of everyday events and situations.3. They propose a combination of pretraining objectives like contrastive frame-transcript matching, masked language modeling with attention masking, and temporal reordering that allows MERLOT to learn full-stack visual reasoning from recognition to cognition level. 4. Extensive experiments show MERLOT achieves new state-of-the-art results on 12 video QA datasets when finetuned, demonstrating its strong multimodal and temporal representations.5. Ablation studies provide insights into the benefits of training on diverse video data, using multiple complementary pretraining objectives, and continuing to pretrain for more iterations.6. Qualitative analyses illustrate MERLOT's capability for temporal commonsense reasoning in zero-shot settings like ordering scrambled image sequences into coherent narratives.In summary, the main contribution is developing the MERLOT model to learn rich multimodal script knowledge from unlabeled videos at scale, and showing its effectiveness on a variety of static and video reasoning tasks. The new pretraining dataset, objectives, and analyses are also key contributions.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence TL;DR summary of the paper:The paper introduces MERLOT, a model that learns multimodal script knowledge by self-supervised pretraining on 6 million unlabelled YouTube videos with speech transcripts, achieving state-of-the-art performance when finetuned on 12 different video QA datasets and demonstrating strong zero-shot temporal reasoning ability.


## How does this paper compare to other research in the same field?

Here are a few key ways this paper compares to other related research:- This paper focuses on learning temporal commonsense reasoning from videos and their transcripts in a self-supervised manner. Much prior work has focused on learning from image-caption pairs, which provide more literal descriptions of static images. Learning from videos and transcripts allows the model to learn richer representations of events unfolding over time.- The approach trains the model using a combination of objectives at both the frame level (e.g. matching frames to contextualized transcripts) and the video level (e.g. unmasking words and reordering scrambled frames). This encourages the model to learn full-stack reasoning spanning both recognition and cognition. Other work has often focused more narrowly on either visual recognition or high-level reasoning.- The model achieves state-of-the-art results on a diverse set of 12 video and image reasoning benchmarks when finetuned. This demonstrates the versatility of the self-supervised pretraining approach across different types of reasoning. Other models relying on supervised pretraining tend to be more domain-specific.- Ablation studies demonstrate the importance of diverse video data, temporal objectives, and long pretraining for learning rich commonsense knowledge. This highlights key factors in self-supervised learning that may not have been extensively analyzed in some prior work focusing on supervised pretraining.- The model exhibits strong zero-shot transfer of temporal reasoning ability from videos to static images. This shows these skills effectively transfer across domains, whereas prior work has often focused learning on datasets from the same domain as the target task.Overall, the self-supervised pretraining approach, objectives combining recognition and reasoning, strong and versatile results on many benchmarks, and ablation studies provide useful insights extending the current research direction of learning joint multimodal representations.
