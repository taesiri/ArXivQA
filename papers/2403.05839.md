# [Long-term Frame-Event Visual Tracking: Benchmark Dataset and Baseline](https://arxiv.org/abs/2403.05839)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Current event/frame-event based trackers are evaluated on short-term tracking datasets, but real-world tracking involves long-term scenarios. The performance of existing algorithms in long-term settings is unclear.
- RGB frames and event streams have incomplete information due to challenging factors (e.g. low illumination, occlusion) and spatial sparsity of events. Conventional fusion methods perform poorly.

Proposed Solution:
- Propose FELT, the first long-term large-scale frame-event tracking benchmark with 742 videos and 1.6 million frame-event pairs covering 45 object classes and 14 attributes.

- Propose AMTTrack, a novel tracking framework with an associative memory Transformer backbone by introducing modern Hopfield layers into multi-head self-attention blocks. This allows full mining of spatial-temporal information for RGB-event fusion.

Main Contributions:

- FELT benchmark for long-term frame-event tracking, which is the largest dataset with long videos.

- Re-train and evaluate 15 baseline trackers on FELT to facilitate future comparisons.

- Novel AMTTrack tracker with unified Transformer backbone empowered by Hopfield layers for associative memory to handle incomplete/uncertain frame-event signals.

- Experiments on FELT and other datasets validate state-of-the-art performance of AMTTrack for long-term RGB-event tracking. The tracker and dataset enable further research.
