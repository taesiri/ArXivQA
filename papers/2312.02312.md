# [Visual Encoders for Data-Efficient Imitation Learning in Modern Video   Games](https://arxiv.org/abs/2312.02312)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

This paper presents a systematic study comparing end-to-end trained visual encoders to publicly available pre-trained visual encoders for imitation learning in modern video games. The goal is to enable wider participation in research on game-playing agents by amortizing the high costs of training agents in complex games across the community. 

The games studied are Minecraft, Minecraft Dungeons, and Counter-Strike: Global Offensive. The authors train behavioral cloning agents to mimic human gameplay demonstrations using different visual encoders to embed the game images before feeding them to a recurrent policy network.

They compare 12 distinct end-to-end trained encoders based on Residual Networks (ResNets) and Vision Transformers (ViTs), varying the architecture, input image size, and use of data augmentation. They also evaluate 10 different pre-trained encoders spanning four categories: self-supervised (e.g. DINOv2), language-contrastive (e.g. CLIP), supervised classification (e.g. FocalNet), and reconstruction (e.g. stable diffusion VAE).

The results show that small $128Ã—128$ images are sufficient for end-to-end encoders, even in complex games like Minecraft Dungeons. Image augmentation consistently improves performance. Among pre-trained encoders, DINOv2 consistently achieves the best results, outperforming end-to-end encoders. There is no clear correlation between encoder size and performance.

In lower data regimes, end-to-end and DINOv2 encoders perform comparably well, suggesting pre-trained encoders could enable effective imitation learning with very limited gameplay data. Analysis of encoder activations indicates they focus on task-relevant image regions.

The key contributions are:
(1) First study comparing end-to-end vs. pre-trained encoders for imitation learning in multiple modern games.
(2) Demonstrates DINOv2 outperforms other pre-trained and end-to-end encoders.
(3) Shows small images and little data can work well, enabling wider access to game AI research.
