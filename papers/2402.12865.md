# [Backward Lens: Projecting Language Model Gradients into the Vocabulary   Space](https://arxiv.org/abs/2402.12865)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Understanding how knowledge is acquired, stored, and recalled in large language models (LMs) like Transformers is an open research question. 
- Recent methods have projected model weights and activations into the vocabulary space to interpret LMs, but not gradients.

Proposed Solution: 
- The paper develops methods to project gradient matrices into vocabulary tokens during backpropagation.
- It shows gradients are low-rank matrices that can be decomposed into sparse outer products of forward/backward pass vectors.  
- This allows interpreting gradients by projecting small representative subspaces rather than huge matrices.

Key Contributions:

1) Proves gradient matrix ranks match number of input tokens, except the last layer which is rank 1.

2) Introduces Backward Lens to project gradients to tokens using logit lens on forward inputs or backward error vectors.

3) Identifies "imprint and shift" mechanism in MLP layers where gradients imprint forward inputs into first layer and shift second layer towards target embedding.

4) Achieves strong editing performance with a fast 1-step "forward shift" method that approximates gradients by outer product of input and target embedding.  

5) Provides extensive analysis and examples showing evolution of projected tokens across layers/tokens in forward and backward passes during editing.

Overall, the key innovation is interpreting gradients by projecting them into human-readable tokens. This reveals new insights into how models store knowledge and how backpropagation modifies that knowledge. A simple 1-step editing method is also introduced based on these discoveries.
