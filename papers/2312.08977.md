# [Weighted Ensemble Models Are Strong Continual Learners](https://arxiv.org/abs/2312.08977)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a summary paragraph of the key points from the paper:

This paper proposes a new continual learning method called Continual Fisher-weighted Model Averaging (CoFiMA) to address the stability-plasticity dilemma in class-incremental learning. The core idea is to balance acquiring new knowledge while retaining previously learned concepts by averaging model weights from the current task and previous tasks. Specifically, after training on each new task, CoFiMA averages the weights of the current model with the model from the previous task using a hyperparameter λ to control the relative weighting. Additionally, CoFiMA weighs each parameter based on its Fisher information to selectively ensemble important weights. Experiments across diverse benchmarks and vision transformer backbones demonstrate consistent gains over state-of-the-art methods like SLCA. The simplicity and effectiveness of CoFiMA in mitigating catastrophic forgetting highlights its potential for continual learning, where balancing plasticity and stability remains an open challenge. Key strengths are selectively merging pertinent weights between successive tasks and leveraging Fisher information to determine the significance of each parameter.


## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
The paper tackles the problem of continual learning (CL) where models need to incrementally learn from a stream of data distributions without forgetting previously learned knowledge. This is challenging as neural networks are susceptible to catastrophic forgetting when trained sequentially on different tasks. The paper specifically focuses on class-incremental learning under limited memory and compute constraints.

Proposed Solution: 
The paper proposes a simple yet effective approach called Continual Fisher-weighted Model Averaging (CoFiMA). The key idea is to balance plasticity (learning new tasks) and stability (retaining old knowledge) by ensembling models after each task through weight averaging. Specifically, after learning a task t, the updated model weights θt are averaged with weights from the previous task θt−1 to obtain the task model θt*. This allows learning new tasks without deviating too much from previous solutions. 

Additionally, CoFiMA uses Fisher information weighting to selectively average more important weights based on their significance for previous tasks. This prevents averaging weights that lead to regions of high loss. The recursive update rule is:  
θt∗= (λFtθt + (1−λ)F∗t−1θ∗t−1))/(λFt +(1−λ)F∗t−1)

Here Ft is the Fisher information matrix of θt and λ balances old vs new task. This retains performance on old tasks while allowing sufficient plasticity.


Main Contributions:

- Proposes CoMA, a conceptually simple continual learning method which performs weight averaging between successive tasks.

- Introduces CoFiMA, an extension of CoMA using Fisher information weighting to selectively ensemble important weights.

- Achieves state-of-the-art performance on multiple continual learning benchmarks using different pretrained model backbones.

- Comprehensive analysis showing CoFiMA consistently outperforms existing class-incremental learning techniques.

In summary, the paper presents a very simple yet effective technique for continual learning that balances stability and plasticity through selective weight ensembling based on Fisher information. Extensive experiments validate the effectiveness of CoFiMA over strong baselines.
