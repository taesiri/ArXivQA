# [Overwriting Pretrained Bias with Finetuning Data](https://arxiv.org/abs/2303.06167)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading, the central research question this paper addresses is: What are the implications of bias in pretrained vision models for downstream tasks, and can this bias be mitigated through interventions on the finetuning dataset? 

Specifically, the paper investigates two forms of bias - spurious correlations and underrepresentation - and studies whether these biases in pretrained models affect the finetuned models. The key findings are:

1. Biases like spurious correlations and underrepresentation can transfer from pretrained models to finetuned models, especially when certain factors like high correlation, low salience, and low finetuning data are present. 

2. However, these biases can be effectively reduced through targeted interventions on the finetuning dataset, even with a small number of examples. The distribution of the finetuning data can be manipulated to counteract biases from the pretrained model.

3. Carefully curating the finetuning dataset, even if it deviates from the test set distribution, allows retaining the performance benefits of pretrained models while mitigating their potential biases. 

So in summary, the paper shows that while pretrained model biases can propagate downstream, it is possible to overcome this via thoughtful finetuning data collection/curation, with minimal impact on model performance. The key implication is that finetuning data should be a priority for bias mitigation.


## What is the main contribution of this paper?

 This paper investigates whether biases from pretrained machine learning models transfer to finetuned models, and whether any such biases can be mitigated. The main contributions are:

- The authors show that biases in the form of spurious correlations and underrepresentation can propagate from pretrained models to finetuned models. This is especially likely when the finetuning dataset has high correlation with the sensitive attribute, low salience, and few samples. 

- However, the authors find these biases can often be corrected by manipulating the finetuning dataset, even with minimal impact on performance. For spurious correlations, adjusting the correlation level in the finetuning data reduces bias. For underrepresentation, adjusting the proportion of underrepresented groups helps. 

- The findings suggest careful curation of the finetuning dataset is important to reduce bias from pretraining, often with negligible impact on performance. Intervening at finetuning is more feasible than changing massive pretrained datasets.

In summary, this paper demonstrates that while pretrained model biases can transfer downstream, thoughtful finetuning data collection and sampling can override pretrained biases without sacrificing accuracy gains. The key is leveraging finetuning as an effective intervention point for mitigating bias.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the paper:

The paper investigates whether biases from pretrained image models propagate to downstream tasks through finetuning, finding that while finetuned models can inherit pretrained biases, especially when the finetuning data is limited, these biases can often be corrected by thoughtful interventions on the finetuning dataset without compromising performance.


## How does this paper compare to other research in the same field?

 This paper experimentally investigates how biases in pretrained visual models can transfer to downstream models via fine-tuning, and how such bias transfer can be mitigated. Here are some key ways it relates to other work:

- It builds on prior work that has measured bias in pretrained model features (e.g. Goyal et al., Sirotkin et al.) by going a step further to evaluate the implications on downstream tasks via fine-tuning. This is an important extension as merely having biased features does not necessarily mean the downstream model will be biased.

- It is complementary to work in NLP that has studied bias transfer in text models (e.g. Steed et al., Goldfarb-Tarrant et al.), by providing analysis for the image domain. The authors make connections between their findings and trends observed in NLP.

- The operationalization of bias using spurious correlations and underrepresentation follows common definitions in the computer vision literature. The analysis on spurious correlations relates to work such as Zhao et al. and Wang et al., while the underrepresentation analysis connects to issues raised by Buolamwini et al. and DeVries et al. 

- The authors take a practical perspective on mitigation by studying the impact of the fine-tuning data distribution, rather than developing new algorithms. This relates to discussions around careful dataset curation in fairness research.

Overall, the paper makes an important empirical contribution in quantifying the transfer of pretrained biases in vision models. It also provides practical insights into data-centric mitigation approaches by manipulating the fine-tuning set. Situated in the visual fairness literature, it helps address open questions around the role of pretraining and provides clarity to guide research and practice.


## What future research directions do the authors suggest?

 Based on my reading of the paper, some of the main future research directions suggested by the authors are:

- Exploring bias transference for additional operationalizations of bias beyond spurious correlations and underrepresentation. The authors focused on these two notions of bias due to their prevalence, but acknowledge there are other forms of bias like stereotypical representations that may exhibit different transfer properties. 

- Studying larger pretrained models, as the authors conducted all experiments using ResNet50. With the size of pretrained models increasing, it will be important to analyze if the findings generalize.

- Considering more complex real-world scenarios where there may be multiple interacting biases present, even ones that are at odds with each other. The authors used focused interventions targeting specific forms of bias, but discuss the need to balance potentially conflicting tensions when curating finetuning datasets.

- Developing techniques to select the best interventions on the finetuning dataset to counteract pretrained model biases, while retaining high performance. The authors manually explored interventions like changing correlation levels, but suggest automating this process.

- Evaluating the efficacy of algorithmic interventions in addition to finetuning data interventions to mitigate bias propagation. The authors focused only on data interventions, but discuss algorithmic approaches as complementary.

- Studying the effects of freezing different layers rather than finetuning the full model, as the authors note frozen layers may be more prone to retaining pretrained biases.

- Expanding the analysis to additional model architectures beyond convolutional networks. The authors validated a subset of experiments transferred across architectures, but more exploration would be beneficial.

In summary, the key directions are studying a wider range of bias types and models, developing automated intervention recommendation techniques, and combining data and algorithmic intervention approaches. The authors provide a solid foundation focusing on data interventions for two important biases and vision models.


## Summarize the paper in one paragraph.

 The paper "Overwriting Pretrained Bias with Finetuning Data" investigates whether biases in pretrained image recognition models propagate into models finetuned on the pretrained weights. The authors conceptualize bias as spurious correlations between labels and sensitive attributes like gender, as well as underrepresentation of certain groups in the training data. Using image datasets like CelebA, COCO, and Dollar Street, they show that finetuned models can inherit biases from pretrained models, especially when the biases are highly correlated with the target task, are more salient than the true task signal, and when finetuning data is limited. However, they also demonstrate that these inherited biases can often be significantly reduced through minor interventions on the composition and weighting of the finetuning dataset, frequently with minimal impact on model performance. They conclude that careful curation of finetuning datasets is crucial for mitigating bias from pretrained models, and can help retain the benefits of transfer learning while promoting fairness. The key insight is that manipulation of finetuning data is an effective, targeted way to overcome biases in pretrained models.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the key points from the paper:

This paper investigates the transfer of bias from pretrained models to finetuned models. The authors conceptualize bias in two ways - as spurious correlations between a sensitive attribute and target task, and as underrepresentation of a particular group in the dataset. Using CelebA, COCO, Dollar Street and other datasets, they pretrain models with different amounts of bias and finetune them on downstream tasks. 

The authors find that finetuned models can inherit the biases present in pretrained models, especially when the bias is strongly correlated to the target task, the bias signal is more salient than the true task signal, and there is little finetuning data. However, they also show that these inherited biases can often be corrected through interventions on the finetuning dataset. By manipulating factors like the correlation strength or representation proportions, much of the pretrained bias can be mitigated with little impact on performance. The findings suggest that careful curation of the finetuning dataset is crucial, and can compensate for biases in the pretrained model. Overall, the work provides insights into how biases propagate from pretrained to finetuned models, and actionable ways to counteract this.


## Summarize the main method used in the paper in one paragraph.

 The paper introduces a method to overcome biases inherited from pretrained models when finetuning on downstream tasks. The key idea is that while finetuned models can inherit spurious correlations or underrepresentation biases from the pretrained models, these biases can be corrected for by manipulating the distribution of the finetuning dataset. 

Specifically, the authors consider two notions of bias - spurious correlations between a target task and a sensitive attribute, and underrepresentation of certain groups or subgroups in the training data. They first show that finetuned models can inherit these biases from pretrained models, especially when the finetuning data is small, or the bias is highly correlated with the target task. 

However, they also demonstrate that by modifying the distribution of the finetuning data, much of this inherited bias can be corrected for. For spurious correlations, they change the correlation strength in the finetuning data to be less biased than the test data. For underrepresentation, they modify the proportion of underrepresented groups in the finetuning data. In both cases, they are able to retain much of the performance gains from pretraining while significantly reducing bias, through relatively minor tweaks to the finetuning data distribution.

The key method is careful curation of the finetuning dataset, even if it means deviating from the test set distribution, in order to compensate for biases inherited from the pretrained model. This allows preserving the benefits of transfer learning while mitigating its potential downsides.


## What problem or question is the paper addressing?

 Based on my reading, the key points of this paper are:

- The paper investigates how biases from pretrained models transfer to downstream tasks when using transfer learning. It looks at two types of biases: spurious correlations (between a target task and sensitive attribute) and underrepresentation (of a particular group in the dataset).

- The authors find that biases in pretrained models can propagate to finetuned models. For spurious correlations, this happens more when the finetuning task has high correlation to the sensitive attribute, low salience, and small finetuning data. 

- However, the paper shows these pretrained biases can often be corrected by interventions on the finetuning data distribution, without much performance impact. For example, changing the correlation strength in the finetuning data can reduce spurious correlation biases from the pretrained model.

- The main takeaway is that careful curation of the finetuning dataset is important to overcome pretrained biases. The finetuning data can compensate for biases in the pretrained model, allowing retention of the model's performance benefits.

In summary, this paper examines how biases propagate from pretrained to finetuned models, and shows targeted finetuning data interventions can mitigate pretrained biases without sacrificing much performance. The key insight is the importance of finetuning data curation to overcome pretrained biases.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and concepts are:

- Pretrained models - The paper examines finetuning pretrained models (e.g. models pretrained on ImageNet) for downstream computer vision tasks. The effect of biases in the pretrained models is a main focus.

- Transfer learning - Finetuning a pretrained model on a downstream task is a common transfer learning technique studied in the paper. 

- Bias - The paper investigates two forms of bias: spurious correlations and underrepresentation. It studies how these biases may propagate from pretrained to finetuned models.

- Spurious correlations - Biases in the form of spurious correlations between the target task and a sensitive attribute (e.g. gender). The paper examines how these may propagate to the finetuned model.

- Underrepresentation - Biases arising from certain groups being underrepresented in the training data. The paper studies how this underrepresentation in the pretrained model may affect the finetuned model.

- Finetuning data - A major finding is that careful curation of the finetuning dataset can help overcome biases from the pretrained model, often with little impact on performance. Manipulating the finetuning data distribution is a key method studied.

- Performance vs. fairness tradeoff - The paper shows that manipulating the finetuning data can improve model fairness without compromising performance gains from pretraining. This challenges the assumption that there is an inherent tradeoff between the two.

In summary, the key focus is on studying bias propagation from pretrained to finetuned models, and how interventions on the finetuning data can mitigate such biases. The notions of spurious correlations and underrepresentation are two conceptualizations of bias explored.


## Based on the paper, what questions should we ask to create a comprehensive summary of it? Please list at least 10 questions.

 Here are 10 potential questions to ask to summarize the key points of this paper:

1. What is the motivation for studying bias transference from pretrained to finetuned models? Why is this an important issue?

2. How does the paper define and operationalize "bias"? What are the two forms of bias studied?

3. What datasets were used in the experiments? How were they used for pretraining and finetuning? 

4. What were the main findings regarding whether finetuned models inherit bias from pretrained models? Under what conditions does this happen more?

5. How did the paper manipulate factors like correlation level and finetuning data distribution to mitigate bias propagation? What interventions worked best?

6. What were the key results for bias in the form of spurious correlations? How could this be corrected via finetuning data?

7. What were the key results for bias in the form of underrepresentation? How could this be corrected via finetuning data?

8. What implications do the findings have for how practitioners should think about using pretrained models?

9. What are the limitations of the study? What biases or scenarios weren't considered?

10. What are the overall conclusions and key takeaways? How should this inform future work on bias in transfer learning?


## Design 10 in-depth questions about method proposed in this paper and avoid simple question.

 Here are 10 in-depth questions about the method proposed in the paper:

1. The paper proposes overwriting pretrained bias with finetuning data. How does the definition and operationalization of "bias" in this work compare to other common definitions in the literature? What are the advantages and disadvantages of this particular conceptualization?

2. The authors find that finetuned models can inherit spurious correlations from pretrained models, especially when correlation level is high, salience is low, and number of finetuning samples is small. What might be the explanations for why these three factors in particular lead to greater bias propagation? 

3. For the experiments on spurious correlations, the authors use false positive rate (FPR) difference between groups as the measure of bias. What are some limitations of using FPR difference to capture bias? Are there other metrics you would suggest using instead or in conjunction?

4. The experiments manipulate the correlation level and proportion of underrepresented groups in the finetuning data to reduce bias. How does directly manipulating the dataset distribution in this way compare to other bias mitigation techniques in terms of effectiveness, feasibility, and ethical considerations?

5. The paper argues that interventions on the finetuning dataset can be more efficient than trying to remove bias from the pretrained model. Do you agree? In what situations might it be preferable to modify the pretrained model instead?

6. The experiments in the paper focus on computer vision tasks. Do you expect the findings to generalize well to other modalities like NLP? What differences might we expect to see?

7. The authors use ResNet50 architecture for all experiments. How might the choice of model architecture impact the transfer of bias from pretrained to finetuned models? Is it important to validate results across different architectures?

8. What are some ways the experimental setup could be expanded or improved to better evaluate the method's real-world applicability and implications? For example, testing on more complex downstream tasks.

9. The paper argues careful finetuning data curation is needed to account for pretrained model biases. What risks or harms might arise from curating datasets in ways that diverge from real test set distributions? 

10. How well do you think the lessons from this work will hold up as pretrained models continue to grow rapidly in size and complexity? Will the finetuning data still play as much of a role in determining bias?


## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a high-quality paragraph summarizing the key points of the paper:

This paper investigates whether biases in pretrained image models propagate to downstream tasks when the models are finetuned. The authors explore two types of bias: spurious correlations between a sensitive attribute (gender) and target task, and underrepresentation of certain groups in the pretrained model. Through experiments on CelebA, COCO, and other datasets, they find that finetuned models can indeed inherit biases from pretrained models. However, the paper shows that these biases can often be significantly reduced by intervening on the distribution of the finetuning dataset, even if it causes the finetuning distribution to deviate from the test set distribution. For example, reducing spurious correlations in the finetuning data allows retaining the performance benefits of a biased pretrained model while improving fairness. Similarly, increasing the representation of underrepresented groups in the finetuning data can overcome deficiencies in the pretrained model. The key implication is that curating the finetuning dataset thoughtfully is crucial for correcting biases from pretrained models. Overall, the paper provides evidence that while pretrained biases can propagate downstream, this effect can frequently be mitigated through targeted data interventions when finetuning.


## Summarize the paper in one sentence.

 The paper investigates how bias in the form of spurious correlations and underrepresentation can propagate from a pretrained model to a finetuned one, but shows this bias can be corrected for through targeted interventions on the distribution of the finetuning dataset.


## Summarize the paper in one paragraphs.

 Here is a one paragraph summary of the paper:

This paper explores whether biases in pretrained image recognition models propagate to downstream models that are finetuned on top of them. The authors consider two types of bias - spurious correlations between the target label and sensitive attributes like gender, and underrepresentation where the pretrained model is not exposed to certain subgroups within a class during pretraining. Through experiments on CelebA, COCO, and other datasets, they find that both forms of bias can propagate from the pretrained to the finetuned model. However, they also show that these biases can be significantly reduced by carefully curating the distribution of the finetuning dataset, even if it means the finetuning distribution deviates from the test distribution. For example, reducing spurious correlations in the finetuning data allows the model to retain the performance benefits of a biased pretrained model while improving fairness. The findings imply that while using less biased pretrained models is good, interventions on the finetuning dataset can be even more effective for mitigating bias. So finetuning data collection and curation deserves significant care and consideration.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in this paper:

1. The paper studies bias transferrence under two conceptualizations: spurious correlations and underrepresentation. How might other types of biases, like stereotyping or exclusion, transfer differently? What experiments could be designed to study this?

2. The paper argues that the finetuning dataset is a better site for bias mitigation than the pretraining dataset. However, what responsibilities do creators of pretrained models have in documenting and mitigating known biases? 

3. For spurious correlations, the paper manipulates the correlation level in the finetuning dataset. Are there any risks or downsides to training on a dataset that does not match the test set distribution? How might this impact generalizability?

4. The paper finds that with sufficient finetuning data, pretrained model biases fade. However, what if the finetuning data itself contains biases? How could the methodology account for potentially biased data?

5. The experiments focus on image classification tasks. How might findings differ for other computer vision tasks like detection, segmentation, etc.? What experiments could explore this?

6. The paper uses gender as the sensitive attribute. How might findings differ for other attributes like race, age, etc. that have different visual salience? What factors determine sensitivity to bias propagation?

7. For underrepresentation, the paper reweights finetuning samples. How else could sample weighting or data augmentation help mitigate underrepresentation biases? What are the limits of these approaches? 

8. The paper argues fairer finetuning can compensate for pretrained bias. But how do we know when a pretrain bias is too harmful to use at all? What ethical lines need to be drawn?

9. The methodology focuses on dataset interventions. How could it complement or compare to algorithmic interventions like debiasing, adversarial learning, etc.?

10. The paper studies a few predefined biases. How could the methodology be extended to allow for discovering new unknown biases that arise during finetuning? What mechanisms could help reveal new biases?
