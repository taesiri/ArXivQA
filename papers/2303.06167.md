# [Overwriting Pretrained Bias with Finetuning Data](https://arxiv.org/abs/2303.06167)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading, the central research question this paper addresses is: What are the implications of bias in pretrained vision models for downstream tasks, and can this bias be mitigated through interventions on the finetuning dataset? 

Specifically, the paper investigates two forms of bias - spurious correlations and underrepresentation - and studies whether these biases in pretrained models affect the finetuned models. The key findings are:

1. Biases like spurious correlations and underrepresentation can transfer from pretrained models to finetuned models, especially when certain factors like high correlation, low salience, and low finetuning data are present. 

2. However, these biases can be effectively reduced through targeted interventions on the finetuning dataset, even with a small number of examples. The distribution of the finetuning data can be manipulated to counteract biases from the pretrained model.

3. Carefully curating the finetuning dataset, even if it deviates from the test set distribution, allows retaining the performance benefits of pretrained models while mitigating their potential biases. 

So in summary, the paper shows that while pretrained model biases can propagate downstream, it is possible to overcome this via thoughtful finetuning data collection/curation, with minimal impact on model performance. The key implication is that finetuning data should be a priority for bias mitigation.


## What is the main contribution of this paper?

 This paper investigates whether biases from pretrained machine learning models transfer to finetuned models, and whether any such biases can be mitigated. The main contributions are:

- The authors show that biases in the form of spurious correlations and underrepresentation can propagate from pretrained models to finetuned models. This is especially likely when the finetuning dataset has high correlation with the sensitive attribute, low salience, and few samples. 

- However, the authors find these biases can often be corrected by manipulating the finetuning dataset, even with minimal impact on performance. For spurious correlations, adjusting the correlation level in the finetuning data reduces bias. For underrepresentation, adjusting the proportion of underrepresented groups helps. 

- The findings suggest careful curation of the finetuning dataset is important to reduce bias from pretraining, often with negligible impact on performance. Intervening at finetuning is more feasible than changing massive pretrained datasets.

In summary, this paper demonstrates that while pretrained model biases can transfer downstream, thoughtful finetuning data collection and sampling can override pretrained biases without sacrificing accuracy gains. The key is leveraging finetuning as an effective intervention point for mitigating bias.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the paper:

The paper investigates whether biases from pretrained image models propagate to downstream tasks through finetuning, finding that while finetuned models can inherit pretrained biases, especially when the finetuning data is limited, these biases can often be corrected by thoughtful interventions on the finetuning dataset without compromising performance.


## How does this paper compare to other research in the same field?

 This paper experimentally investigates how biases in pretrained visual models can transfer to downstream models via fine-tuning, and how such bias transfer can be mitigated. Here are some key ways it relates to other work:

- It builds on prior work that has measured bias in pretrained model features (e.g. Goyal et al., Sirotkin et al.) by going a step further to evaluate the implications on downstream tasks via fine-tuning. This is an important extension as merely having biased features does not necessarily mean the downstream model will be biased.

- It is complementary to work in NLP that has studied bias transfer in text models (e.g. Steed et al., Goldfarb-Tarrant et al.), by providing analysis for the image domain. The authors make connections between their findings and trends observed in NLP.

- The operationalization of bias using spurious correlations and underrepresentation follows common definitions in the computer vision literature. The analysis on spurious correlations relates to work such as Zhao et al. and Wang et al., while the underrepresentation analysis connects to issues raised by Buolamwini et al. and DeVries et al. 

- The authors take a practical perspective on mitigation by studying the impact of the fine-tuning data distribution, rather than developing new algorithms. This relates to discussions around careful dataset curation in fairness research.

Overall, the paper makes an important empirical contribution in quantifying the transfer of pretrained biases in vision models. It also provides practical insights into data-centric mitigation approaches by manipulating the fine-tuning set. Situated in the visual fairness literature, it helps address open questions around the role of pretraining and provides clarity to guide research and practice.


## What future research directions do the authors suggest?

 Based on my reading of the paper, some of the main future research directions suggested by the authors are:

- Exploring bias transference for additional operationalizations of bias beyond spurious correlations and underrepresentation. The authors focused on these two notions of bias due to their prevalence, but acknowledge there are other forms of bias like stereotypical representations that may exhibit different transfer properties. 

- Studying larger pretrained models, as the authors conducted all experiments using ResNet50. With the size of pretrained models increasing, it will be important to analyze if the findings generalize.

- Considering more complex real-world scenarios where there may be multiple interacting biases present, even ones that are at odds with each other. The authors used focused interventions targeting specific forms of bias, but discuss the need to balance potentially conflicting tensions when curating finetuning datasets.

- Developing techniques to select the best interventions on the finetuning dataset to counteract pretrained model biases, while retaining high performance. The authors manually explored interventions like changing correlation levels, but suggest automating this process.

- Evaluating the efficacy of algorithmic interventions in addition to finetuning data interventions to mitigate bias propagation. The authors focused only on data interventions, but discuss algorithmic approaches as complementary.

- Studying the effects of freezing different layers rather than finetuning the full model, as the authors note frozen layers may be more prone to retaining pretrained biases.

- Expanding the analysis to additional model architectures beyond convolutional networks. The authors validated a subset of experiments transferred across architectures, but more exploration would be beneficial.

In summary, the key directions are studying a wider range of bias types and models, developing automated intervention recommendation techniques, and combining data and algorithmic intervention approaches. The authors provide a solid foundation focusing on data interventions for two important biases and vision models.


## Summarize the paper in one paragraph.

 The paper "Overwriting Pretrained Bias with Finetuning Data" investigates whether biases in pretrained image recognition models propagate into models finetuned on the pretrained weights. The authors conceptualize bias as spurious correlations between labels and sensitive attributes like gender, as well as underrepresentation of certain groups in the training data. Using image datasets like CelebA, COCO, and Dollar Street, they show that finetuned models can inherit biases from pretrained models, especially when the biases are highly correlated with the target task, are more salient than the true task signal, and when finetuning data is limited. However, they also demonstrate that these inherited biases can often be significantly reduced through minor interventions on the composition and weighting of the finetuning dataset, frequently with minimal impact on model performance. They conclude that careful curation of finetuning datasets is crucial for mitigating bias from pretrained models, and can help retain the benefits of transfer learning while promoting fairness. The key insight is that manipulation of finetuning data is an effective, targeted way to overcome biases in pretrained models.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the key points from the paper:

This paper investigates the transfer of bias from pretrained models to finetuned models. The authors conceptualize bias in two ways - as spurious correlations between a sensitive attribute and target task, and as underrepresentation of a particular group in the dataset. Using CelebA, COCO, Dollar Street and other datasets, they pretrain models with different amounts of bias and finetune them on downstream tasks. 

The authors find that finetuned models can inherit the biases present in pretrained models, especially when the bias is strongly correlated to the target task, the bias signal is more salient than the true task signal, and there is little finetuning data. However, they also show that these inherited biases can often be corrected through interventions on the finetuning dataset. By manipulating factors like the correlation strength or representation proportions, much of the pretrained bias can be mitigated with little impact on performance. The findings suggest that careful curation of the finetuning dataset is crucial, and can compensate for biases in the pretrained model. Overall, the work provides insights into how biases propagate from pretrained to finetuned models, and actionable ways to counteract this.


## Summarize the main method used in the paper in one paragraph.

 The paper introduces a method to overcome biases inherited from pretrained models when finetuning on downstream tasks. The key idea is that while finetuned models can inherit spurious correlations or underrepresentation biases from the pretrained models, these biases can be corrected for by manipulating the distribution of the finetuning dataset. 

Specifically, the authors consider two notions of bias - spurious correlations between a target task and a sensitive attribute, and underrepresentation of certain groups or subgroups in the training data. They first show that finetuned models can inherit these biases from pretrained models, especially when the finetuning data is small, or the bias is highly correlated with the target task. 

However, they also demonstrate that by modifying the distribution of the finetuning data, much of this inherited bias can be corrected for. For spurious correlations, they change the correlation strength in the finetuning data to be less biased than the test data. For underrepresentation, they modify the proportion of underrepresented groups in the finetuning data. In both cases, they are able to retain much of the performance gains from pretraining while significantly reducing bias, through relatively minor tweaks to the finetuning data distribution.

The key method is careful curation of the finetuning dataset, even if it means deviating from the test set distribution, in order to compensate for biases inherited from the pretrained model. This allows preserving the benefits of transfer learning while mitigating its potential downsides.


## What problem or question is the paper addressing?

 Based on my reading, the key points of this paper are:

- The paper investigates how biases from pretrained models transfer to downstream tasks when using transfer learning. It looks at two types of biases: spurious correlations (between a target task and sensitive attribute) and underrepresentation (of a particular group in the dataset).

- The authors find that biases in pretrained models can propagate to finetuned models. For spurious correlations, this happens more when the finetuning task has high correlation to the sensitive attribute, low salience, and small finetuning data. 

- However, the paper shows these pretrained biases can often be corrected by interventions on the finetuning data distribution, without much performance impact. For example, changing the correlation strength in the finetuning data can reduce spurious correlation biases from the pretrained model.

- The main takeaway is that careful curation of the finetuning dataset is important to overcome pretrained biases. The finetuning data can compensate for biases in the pretrained model, allowing retention of the model's performance benefits.

In summary, this paper examines how biases propagate from pretrained to finetuned models, and shows targeted finetuning data interventions can mitigate pretrained biases without sacrificing much performance. The key insight is the importance of finetuning data curation to overcome pretrained biases.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and concepts are:

- Pretrained models - The paper examines finetuning pretrained models (e.g. models pretrained on ImageNet) for downstream computer vision tasks. The effect of biases in the pretrained models is a main focus.

- Transfer learning - Finetuning a pretrained model on a downstream task is a common transfer learning technique studied in the paper. 

- Bias - The paper investigates two forms of bias: spurious correlations and underrepresentation. It studies how these biases may propagate from pretrained to finetuned models.

- Spurious correlations - Biases in the form of spurious correlations between the target task and a sensitive attribute (e.g. gender). The paper examines how these may propagate to the finetuned model.

- Underrepresentation - Biases arising from certain groups being underrepresented in the training data. The paper studies how this underrepresentation in the pretrained model may affect the finetuned model.

- Finetuning data - A major finding is that careful curation of the finetuning dataset can help overcome biases from the pretrained model, often with little impact on performance. Manipulating the finetuning data distribution is a key method studied.

- Performance vs. fairness tradeoff - The paper shows that manipulating the finetuning data can improve model fairness without compromising performance gains from pretraining. This challenges the assumption that there is an inherent tradeoff between the two.

In summary, the key focus is on studying bias propagation from pretrained to finetuned models, and how interventions on the finetuning data can mitigate such biases. The notions of spurious correlations and underrepresentation are two conceptualizations of bias explored.
