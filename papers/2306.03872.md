# [Deductive Verification of Chain-of-Thought Reasoning](https://arxiv.org/abs/2306.03872)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question appears to be:How can we enable large language models to perform explicit, rigorous, and trustworthy deductive reasoning through self-verification?The key points related to this research question are:- The authors aim to have language models perform deductive reasoning that is explicit, rigorous, and coherent, similar to the logical reasoning emphasized in ancient Greek logic. - They want to ensure the validity and trustworthiness of the reasoning process through self-verification by the language models.- Directly verifying the validity of an entire complex deductive reasoning chain is challenging, even for advanced models. - The authors propose decomposing the verification into step-by-step subprocesses, each only receiving necessary context/premises.- They introduce "Natural Program", a natural language-based deductive reasoning format to facilitate this decomposition and step-wise verification.- By integrating verification into each deductive reasoning stage, they significantly enhance the rigor and trustworthiness of the generated reasoning steps and answers.In summary, the central research question focuses on how to achieve explicit, rigorous and trustworthy deductive reasoning from language models through a self-verification process based on the proposed Natural Program reasoning format.


## What is the main contribution of this paper?

The main contribution of this paper is proposing a framework to enable large language models (LLMs) to perform explicit, rigorous and verifiable deductive reasoning. Specifically:1. The paper introduces a "Natural Program" format for representing deductive reasoning chains in natural language. This allows LLMs to generate reasoning steps along with their minimal premises. 2. The paper proposes decomposing the verification of long deductive reasoning chains into step-by-step verification of individual steps using only their minimal premises. This avoids distraction from irrelevant information.3. The paper integrates the step-by-step deductive verification into the LLM's reasoning process through a Unanimity-Plurality Voting strategy. This enhances the rigor and trustworthiness of the reasoning steps and final answers.4. Through experiments on arithmetic and commonsense reasoning datasets, the paper demonstrates that the proposed framework significantly improves the reliability and interpretability of the LLM's reasoning outputs. In summary, the key contribution is developing an end-to-end framework to make LLMs perform more careful, explicit and verifiable deductive reasoning in natural language. The Natural Program format and step-by-step verification technique are critical components that enable the realization of this framework.
