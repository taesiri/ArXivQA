# [Learning Domain-Independent Heuristics for Grounded and Lifted Planning](https://arxiv.org/abs/2312.11143)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Learning effective domain-independent heuristics for guiding heuristic search planners is an open challenge. 
- Existing graph neural network (GNN) models like STRIPS-HGN have limitations in terms of expressiveness and scalability that prevent them from learning optimal heuristics.

Proposed Solution:
- Introduce 3 new graph representations of planning tasks suitable for learning heuristics with GNNs:
    - Grounded graphs: STRIPS Learning Graph (SLG) and FDR Learning Graph (FLG)
    - Lifted graph: Lifted Learning Graph (LLG) - first lifted representation for domain-independent learning
- Provide theoretical analysis on expressiveness of Message Passing Neural Networks (MPNNs) on these graphs in terms of which heuristics they can learn
- Propose GOOSE planner that uses MPNNs on these graphs to learn heuristic functions to guide search

Main Contributions:
- First method for learning domain-independent heuristics from only the lifted representation
- Establish that MPNNs on the new grounded graphs are more expressive than STRIPS-HGN
- Show MPNNs cannot learn optimal heuristic with these graphs, but can learn admissible heuristics
- Experiments show GOOSE with learned heuristics solves much larger problems than prior works, achieves greater coverage than h^FF in several domains
- Lifted heuristics in GOOSE achieve best coverage on certain domains over all other models

In summary, the paper introduces novel graph representations for learning more effective domain-independent heuristics that can generalize to larger problems, supported by strong theoretical results and empirical validation. The lifted heuristic also enables scaling to larger problems.


## Summarize the paper in one sentence.

 Here is a one sentence summary of the paper:

The paper presents three new graph representations of planning tasks suitable for learning domain-independent heuristics with graph neural networks, provides a theoretical analysis of their expressiveness, and empirically evaluates their effectiveness for guiding search.


## What is the main contribution of this paper?

 The main contributions of this paper are:

1. It presents three novel graph representations of planning tasks suitable for learning domain-independent heuristics using graph neural networks: the STRIPS learning graph (SLG), the FDR learning graph (FLG), and the lifted learning graph (LLG). 

2. It provides the first method for learning domain-independent heuristics directly from the lifted representation of a planning task, without needing to ground it first. This is done using the LLG representation.

3. It offers a theoretical analysis categorizing what domain-independent heuristics message passing neural networks are able to learn using each graph representation. For example, it shows MPNNs can learn h^add and h^max using SLG and FLG but not using LLG.

4. It introduces the GOOSE planner which uses these graph representations and MPNNs to learn heuristic functions, and shows GOOSE is able to solve significantly larger problems than prior work using learned heuristics.

5. In experiments, GOOSE with heuristics learned using the LLG representation provides the best coverage on several planning domains compared to baselines. The domain-independent heuristics learned also outperform h^FF and prior work on some domains.

In summary, the key innovation is the new lifted graph representation (LLG) which enables more effective learning of domain-independent heuristics, including being able to solve much larger planning problems than prior work.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper's abstract and introduction, some of the key terms and concepts related to this paper include:

- Graph neural networks (GNNs)
- Domain-independent heuristics
- Lifted planning
- Message-passing neural networks (MPNNs)
- Graph representations of planning tasks
- Expressiveness of graph neural networks
- Generalization of learned heuristics
- STRIPS-HGN
- Grounded graphs
- Lifted graphs

The paper introduces novel graph representations of planning tasks suitable for learning domain-independent heuristics using graph neural networks. A key contribution is the first method for learning heuristics directly from a lifted representation of a planning task. The paper also analyzes the expressiveness of different graph neural network architectures acting on these planning graphs in terms of what heuristics they can represent. Experiments demonstrate the ability of the learned heuristics to generalize and guide search on larger unseen planning problems. So in summary, the key focus areas relate to planning representations, graph neural networks, generalization, and expressiveness.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 potential in-depth questions about the method proposed in this paper:

1. The paper introduces three novel graph representations for planning tasks - grounded graphs (SLG and FLG) and lifted graphs (LLG). How do these representations capture different aspects of the planning task compared to previous graph representations like ASG? What are the key differences?

2. The paper shows theoretically that the proposed graph representations paired with MPNNs can learn admissible heuristics like h^add and h^max. What modifications would be needed for the graph representation or MPNN architecture to learn inadmissible heuristics like h^FF?

3. The LLG representation uses a novel learned positional encoding (IF) to handle unbounded predicate arity. Why is handling unbounded arity important? How does the IF encoding achieve better generalization compared to alternatives like having a fixed maximum arity?

4. The paper proves MPNNs on the proposed graphs cannot learn optimal heuristics like h^plus in the general case. But domain-dependent optimal heuristics are still possible. What specific properties of a planning domain would enable learning optimal heuristics with the proposed approach?

5. The grounded and lifted graph representations have complimentary strengths and weaknesses. In what scenarios would you expect the lifted LLG representation to outperform the grounded SLG/FLG representations and vice versa?

6. The experiments show the lifted representation achieves the best performance when trained in a domain-dependent manner. Why does the lifted representation fail to produce useful domain-independent heuristics? How could the lifted approach be adapted to improve domain independence?

7. The paper compares performance against strong baselines like LAMA and h^FF. What are other state-of-the-art planning techniques or heuristics it would be useful to compare against in future work?

8. The graphs are used specifically for heuristic learning in this work. What are other potential planning applications where the learned graph representations could provide value (e.g. learning policies, symmetries)?

9. The proposed approach performed poorly in the Sokoban domain which is known to be very challenging. What modifications could make the approach more effective for very hard planning domains like Sokoban?

10. What are some promising future research directions for improving the expressiveness of graph neural networks to better capture planning structure and heuristics? Could ideas from GNN variants like HGNNs and GNN logic be beneficial?
