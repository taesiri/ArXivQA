# [REALM: Retrieval-Augmented Language Model Pre-Training](https://arxiv.org/abs/2002.08909)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central hypothesis is that augmenting language model pre-training with a learned knowledge retriever will improve performance on knowledge-intensive natural language processing tasks like open-domain question answering. Specifically, the paper proposes REALM (Retrieval-Augmented Language Model), which adds a neural retriever module that retrieves relevant documents from a corpus during pre-training and fine-tuning. The key ideas are:- The retriever is trained jointly with the language model in an unsupervised way, using the language modeling objective. This allows it to learn what documents are useful for reducing perplexity.- The model is optimized end-to-end, backpropagating through the retrieval step to learn better retrievals.- This approach allows incorporating knowledge from large textual corpora in a modular and interpretable way, compared to storing all knowledge implicitly in the model parameters.The central hypothesis is that this retrieval-augmented pre-training approach will improve performance on open-domain QA, by teaching the model to retrieve and utilize external knowledge. The paper evaluates this hypothesis by pre-training REALM models and fine-tuning them on open-domain QA datasets, where they outperform previous state-of-the-art systems.


## What is the main contribution of this paper?

The main contribution of this paper is presenting REALM (Retrieval-Augmented Language Model), a new method for pre-training language models that incorporates latent knowledge retrieval. The key ideas are:- Augmenting a masked language model with a neural retriever module that retrieves relevant documents from a corpus to help predict masked tokens.- Pre-training the retriever and language model end-to-end by maximizing the marginal likelihood over retrieved documents. This encourages retrievals that improve perplexity.- Using maximum inner product search to enable scaling to large corpora during pre-training.- Evaluating on open-domain question answering and showing significant gains over previous state-of-the-art methods, including large pretrained language models like T5.In summary, the main contribution is presenting a new way to inject knowledge into language models by retrieving from corpora on-the-fly, and showing its effectiveness on knowledge-intensive tasks like open-domain QA. The retriever provides interpretability and modularity compared to storing all knowledge in the parameters.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper proposes REALM, a new pre-training method that augments language model pre-training with a latent knowledge retriever that retrieves relevant documents from a corpus to help predict masked tokens, achieving state-of-the-art results on open-domain question answering benchmarks.


## How does this paper compare to other research in the same field?

Here are a few key ways this paper compares to other research in open-domain question answering and knowledge-based language models:- It proposes REALM, a new model that augments language model pre-training with an explicit neural knowledge retriever. This differs from most prior work that looked at either retrieval-based QA systems with non-learned components or huge language models like T5 that store knowledge implicitly. - REALM achieves state-of-the-art results on multiple open-domain QA benchmarks, outperforming previous approaches including very large models like 11B parameter T5. This demonstrates the effectiveness of the proposed pre-training framework.- The paper provides an analysis of the model, including ablations and examples showing that the retriever learns to retrieve genuinely useful documents. This sheds light on why the approach works.- REALM is designed to be more interpretable and modular compared to gigantic black-box models. The authors argue this could lead to other benefits like easier adaptation to new knowledge.- The method of pre-training a retriever using a latent variable language model objective seems novel compared to prior work on learned retrieval for QA.- There is still limited analysis of the knowledge being captured by REALM versus in parameter weights. More inspection could reveal strengths/weaknesses of the different approaches.In summary, this paper introduces a new way of incorporating world knowledge into language models that obtains strong empirical results. It also provides some useful analysis and discussion about the benefits of explicit retrieval versus implicit parameter storage for knowledge.
