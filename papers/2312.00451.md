# [FSGS: Real-Time Few-shot View Synthesis using Gaussian Splatting](https://arxiv.org/abs/2312.00451)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a detailed summary paragraph of the key points in this paper:

This paper proposes FSGS, a real-time few-shot novel view synthesis framework that can generate high-quality novel views from as few as 3 input images. FSGS represents scenes using 3D Gaussians initialized from sparse SfM points. To address insufficient scene coverage from limited views, it proposes proximity-guided Gaussian unpooling which strategically grows new Gaussians in representative locations to fill gaps. To regularize the geometry of grown Gaussians, FSGS incorporates monocular depth priors on both input and synthesized pseudo-views into training, enabled by a differentiable depth rasterizer implementation. Experiments on scene-level LLFF and Mip-NeRF360 datasets, and object-level Blender dataset validate that FSGS achieves state-of-the-art quality with 200+ FPS rendering, significantly faster than top-performing baselines. Ablations verify the contribution of individual components. Evaluations on a new smartphone captured dataset further demonstrate the generalization ability and practical application potential of FSGS for real-time novel view synthesis using extremely sparse views.
