# [How to choose your best allies for a transferable attack?](https://arxiv.org/abs/2304.02312)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question is: How can we estimate the transferability of adversarial examples between a source model and a target model, and select the best source model for attacking a given target model?

More specifically, the paper seems to address the following key questions:

- How should we measure transferability in a way that considers both attack success rate and distortion? The paper argues that only looking at attack success rate provides an incomplete picture.

- What factors affect transferability of adversarial examples? The paper examines how transferability depends on the input image, source model, and attack method. 

- Can we estimate transferability between a source and target model using a limited number of queries to the target model? The paper proposes using fingerprinting methods and estimating adversarial example quality on source models.

- Can this transferability estimate be used to select the best source model for attacking a given target? The paper introduces the FiT score that combines model similarity and adversarial example quality to select good source models.

- How does this source model selection impact transferability in different attack scenarios like single-model attacks, ensemble attacks, etc? The paper evaluates the FiT selection method in different settings.

So in summary, the central hypothesis is that by carefully estimating transferability and selecting source models, the attacker can achieve much higher attack success rates against black-box models compared to random or arbitrary selection. The FiT score is proposed to enable this.


## What is the main contribution of this paper?

 This paper proposes FiTEst, a new methodology to estimate the transferability of adversarial examples between a source model and a target model. The key contributions are:

- It proposes a new transferability score that considers the distortion of adversarial examples, rather than just the attack success rate. This provides a more fair comparison between different attacks. 

- It highlights the large variability in transferability performance depending on the choice of source model, target model, and attack method. Transferable attacks can sometimes perform worse than black-box attacks without careful selection.

- It introduces a selection mechanism called FiTEst that allows an attacker to choose a good source model for a given target using a combination of model similarity estimation and quality of transferable examples.

- Experiments show FiTEst is effective at selecting the best source model for single model attacks, ensemble attacks, and with different combinations of attack methods. With FiTEst, transferability is significantly improved and approaches white-box attack performance.

In summary, the main contribution is the proposal of FiTEst to help select the best ally (source model) for generating transferable attacks against a target model. This is done by considering model similarity and quality of transferable examples rather than just attack success rate.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the key points from the paper:

The paper proposes a new methodology to evaluate the transferability of adversarial examples by comparing the distortion trade-off of transferable attacks to white box and black box attacks, and introduces a selection mechanism called FiT to choose the best source model for transferable attacks using only a few queries to the target model.


## How does this paper compare to other research in the same field?

 Here are a few key ways this paper compares to other research on adversarial transferability:

- It proposes a new methodology for evaluating transferability that considers distortion/perturbation in addition to just attack success rate. This provides a more nuanced evaluation compared to only looking at attack success rate. 

- It highlights the importance of carefully selecting the source model for transfer attacks, showing that random selection can often perform worse than black-box attacks. Most prior work does not evaluate performance across a wide range of source models.

- It proposes a new selection mechanism (FiT) to identify good source models with minimal queries to the target model. This is a novel contribution compared to prior work.

- It evaluates performance across a large set of models from different architectures. Many prior papers only evaluate on a small set of similar models, so the diversity here is an advantage.

- It compares multiple attack methods (both transferable attacks and traditional white-box attacks) in terms of transferability. Looking at multiple attack paradigms is less common.

- It considers both single model and ensemble model attacks. Many papers focus on just one. Evaluating both provides a more complete picture.

Overall, the large-scale experiments across diverse models and attacks make the results more convincing compared to prior work. The proposed FiT selection method is also a notable new technique for improving transferability. The focus on distortion is another key differentiator from most existing literature.


## What future research directions do the authors suggest?

 The authors suggest several future research directions in the paper:

- Evaluate the proposed approach on different datasets and tasks beyond ImageNet classification. They note that transferability properties may vary for different data distributions and tasks. 

- Investigate the underlying causes of image dependence in transferability. The paper showed that transferability can vary significantly for the same source model depending on the input image. Further research could examine what properties of images affect transferability.

- Design an improved selection mechanism that is able to identify the single best source model for all images. The proposed FiT score performed well in selecting good source models on average across images, but was not always able to identify the best source for every individual image. Developing a selection method that can reliably pick the best source model for each input could further improve transferability.

- Study ensemble attacks with a larger pool of available source models. The authors were limited to studying ensemble attacks with up to 20 source models due to computational constraints. Evaluating ensemble attacks drawing from a larger pool of sources could provide additional insights.

- Evaluate transferability between models trained on different datasets. This paper focused on transferability between models trained on ImageNet. Evaluating transferability when source and target models are trained on different datasets is an important direction.

- Develop adaptive attacks that can query the target model and use feedback to improve transferability. The proposed approach only uses a small number of target queries for fingerprinting, but incorporating additional active queries could further enhance transferability.

In summary, the main future directions are studying transferability on more diverse tasks and data, further investigating the underlying factors driving transferability, designing improved selection mechanisms, evaluating larger-scale ensemble attacks, considering transfer learning scenarios, and developing adaptive attacks.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the key points in the paper:

This paper proposes a new methodology for evaluating the transferability of adversarial examples by comparing the distortion (perturbation norm) required to fool a target model using a transferable attack versus white box and black box attacks. The authors argue that relying solely on attack success rate overlooks the degree of distortion, and show that transferable attacks can perform worse than black box attacks without proper source model selection. They propose a FIT score to select a good source model with minimal queries to the target by combining a fingerprinting-based model similarity score with an estimation of the adversarial example quality using available source models. Experiments demonstrate FIT is effective for model selection in single-model and ensemble-model attacks, achieving high transferability that approaches white box results. Overall, the paper highlights the importance of source model selection for transferable attacks and introduces a method to estimate transferability and choose an optimal source model.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the key points from the paper:

The paper proposes a new methodology for evaluating the transferability of adversarial examples between deep neural network models. Rather than just using attack success rate, the authors argue for also considering the distortion required to fool the target model. They introduce a new metric that compares the distortion-success tradeoff of a transfer attack to that of a white box attack (on the target model) and a black box attack. 

The paper shows there is large variability in transferability, depending on the input image, source model, and attack method. They propose a selection mechanism called FIT to help choose the best source model for a given target, using a combination of architectural similarity and estimating adversarial example quality. Experiments demonstrate FIT is effective for selecting good source models in various scenarios like single model attacks, ensemble attacks, and combining multiple attacks. The key insight is carefully selecting the source model is crucial to achieve high transferability, instead of just relying on models with similar architectures or randomness.


## Summarize the main method used in the paper in one paragraph.

 Here is a one paragraph summary of the main method used in the paper:

The paper proposes a new method called FiTEst for estimating the transferability of adversarial examples between a source model and a target model. The method combines architectural similarity between the models, estimated using fingerprinting techniques, with an estimation of the quality of the adversarial examples generated on the source model. To estimate adversarial quality, the method generates adversarial examples using the source model and evaluates their transferability to other available source models without querying the target model. The architectural similarity and estimated adversarial quality are combined into a FiTEst score that aims to predict how transferable an attack from the given source model will be against the target model for a specific input image. The FiTEst score can be used by an attacker to select the best source model for attacking a target model in scenarios like single-model attacks, ensemble-model attacks, and multiple attacks with different methods.


## What problem or question is the paper addressing?

 The key points from this paper are:

- It addresses the problem of evaluating the transferability of adversarial examples between machine learning models. Most prior work measures transferability only using Attack Success Rate (ASR), but the authors argue this is insufficient and unfairly evaluates transferability. 

- The paper proposes a new methodology to evaluate transferability that considers the full tradeoff between ASR and distortion for a transferable attack, and compares it to this tradeoff for white box and black box attacks. This gives a more comprehensive view of how useful a transferable attack really is.

- The authors show there is high variability in transferability based on the choice of source model, input image, and attack method. Transferable attacks can even perform worse than black box attacks without proper source model selection. 

- The paper introduces a new selection mechanism called FIT to choose the best source model for a given target model and input image. This combines model similarity estimated by fingerprinting and quality of the adversarial example estimated from available source models.

- Experiments show FIT is highly effective at selecting good source models in various scenarios like single model attacks, ensemble attacks, and combining multiple attack methods. With FIT, transferability results approach those of white box attacks.

In summary, the key contribution is a new methodology and selection technique to properly evaluate transferability of adversarial examples and achieve high success rates by careful source model selection. This highlights the importance of all components in transfer attacks, not just the attack method.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and concepts are:

- Transferability - The ability of adversarial examples crafted on one model to fool other models. A key property enabling black-box attacks.

- Adversarial examples - Inputs crafted with small perturbations to fool machine learning models and cause misclassification. 

- Attack success rate (ASR) - The commonly used metric to evaluate the effectiveness of adversarial attacks by measuring the percentage of examples that fool the model.

- Distortion - The amount of perturbation added to craft adversarial examples, measured by Lp norms such as L2 norm. Lower distortion attacks are more imperceptible.

- Black-box attacks - Attacks where the attacker has no access to the target model parameters or gradients. Relies on transferability or querying the target model.

- White-box attacks - Attacks where the attacker has full access to the target model internals to compute gradients and optimize the adversarial perturbation. 

- Transferable attacks - Attacks designed to improve transferability by avoiding overfitting perturbations to the source model.

- Fingerprinting - Techniques to identify model similarity by analyzing model predictions on select inputs. Used here for model selection.

- Model selection - Selecting the best source model for crafting transferable attacks against a target model. Critical for achieving high attack success rates.

The key focus of the paper is improving transferable attacks through better model selection using fingerprinting and other heuristics. The main contributions are around the proposed FIT score for this task.


## Based on the paper, what questions should we ask to create a comprehensive summary of it? Please list at least 10 questions.

 Here are 10 potential questions to ask to create a comprehensive summary of this paper:

1. What is the main problem or limitation that this paper aims to address? (e.g. issues with assessing transferability purely based on attack success rate)

2. What is the key proposal or contribution of this paper? (e.g. proposing a new methodology to evaluate transferability)  

3. What assumptions does the paper make? (e.g. attacker has access to multiple source models, state-of-the-art white box and black box attacks)

4. How does the paper define and measure transferability? What is the proposed transferability score?

5. What factors does the paper investigate that affect transferability? (e.g. data, model, attack dependence)  

6. What experiments does the paper conduct to analyze these factors affecting transferability?

7. How does the paper justify/evaluate the proposed transferability methodology? (e.g. comparing to white box and black box attacks)

8. What is the proposed FIT score for selecting the best source model? How is it calculated?

9. What are the main results? How effective is FIT at selecting good source models?

10. What are the limitations and potential future work highlighted in the paper?


## Design 10 in-depth questions about method proposed in this paper and avoid simple question.

 Here are 10 in-depth questions about the method proposed in this paper:

1. The paper proposes a new transferability measure that compares the distortion of a transferable attack to a white box and black box attack. How does comparing distortion provide a more comprehensive view of transferability compared to just using attack success rate (ASR)?

2. The paper highlights the large variability in transferability performance based on the choice of source model. What are some key factors that determine whether a source model will be effective at attacking a given target model? How can these dependencies be leveraged to improve source model selection?

3. The proposed FIT score combines model similarity and adversarial example quality to estimate transferability. How was model similarity measured and why was fingerprinting used? What are some limitations of using fingerprinting for this purpose?  

4. The adversarial example quality score TransQ leverages the attacker's access to multiple source models. How exactly is TransQ computed and why is it effective at estimating transferability? What are its advantages over using ASR?

5. For single model attacks, how well does the FIT score perform compared to random selection and selecting based on model similarity alone? What causes the FIT selection to outperform these other methods?

6. How does the FIT score extend to selecting an ensemble of models for attack? Why can a small, high-quality ensemble outperform a larger ensemble of randomly selected models?

7. The results show FIT selection allowing traditional white box attacks to achieve higher transferability than transferable attacks with random selection. Why do you think white box attacks can be competitive in some cases?

8. What differences did you observe in the performance of the FIT score for the different transferable attacks tested (DI2-FGSM, TAIG, DWP)? How does the choice of attack method interact with model selection?

9. What are some limitations of the proposed FIT score? In what scenarios might it fail to select an optimal source model? How could the score be improved?

10. The paper focuses on model selection for single image attacks. How could the conclusions be extended to generating universal adversarial perturbations that can fool a model on any input? Would a different source model selection approach be needed?
