# [Partial Network Cloning](https://arxiv.org/abs/2303.10597)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading, the key research question addressed in this paper is: 

How can we efficiently transfer partial functionality from a source pre-trained neural network to a target pre-trained neural network, without modifying the weights of either network?

The authors propose a new approach called "partial network cloning" to tackle this challenge. The key ideas are:

1) Localize the part of the source network that is most relevant to the target functionality, without changing any weights. This is done using a local-performance based pruning technique. 

2) Insert the extracted module into the target network by cascading it to a certain layer, again without modifying weights. The insertion position is optimized to maximize performance on the target task while minimizing negative impact on the original task.

3) Jointly optimize the localization and insertion to find the best partial clone.

The goal is to enable flexible customization of pre-trained models without altering them, for applications like incremental learning and model reuse. The core research hypothesis seems to be that it's possible to effectively clone functionality between fixed neural networks through this localized extraction and insertion approach.
