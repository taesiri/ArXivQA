# [Active learning with biased non-response to label requests](https://arxiv.org/abs/2312.08150)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a summary paragraph of the key points from the paper:

This paper examines how non-response to label requests can impact active learning model performance over time. The authors conceptualize two mechanisms through which non-response degrades performance: a volume effect from having fewer training examples, and an imbalance effect from biasing the distribution of training data. They demonstrate through simulations that biased non-response is more detrimental than random non-response. To mitigate this, they propose adjusting the active learning utility function by the probability of response, which they term UCB-EU. Experiments show their method improves performance in many cases by preventing wasted queries in low-response regions. However, in some cases non-response still forces suboptimal decision boundaries by restricting the observable feature space. The method is demonstrated in a real-world e-commerce recommendation scenario using Taobao data. Key findings are that UCB-EU leads to performance gains, but relies on having an accurate non-response model. The work provides both a better understanding of non-response in active learning, and a practical solution to help address it in contexts with implicit user labeling.


## Summarize the paper in one sentence.

 Here is a one sentence summary of the paper:

The paper studies how different types of non-response to label requests in active learning impact model performance over time and proposes a method to correct for biased non-response by incorporating estimated response probabilities into the sample selection strategy.


## What is the main contribution of this paper?

 Based on my reading of the paper, the main contributions are:

1) Conceptualizing how different types of non-response (MCAR vs MAR) can impact active learning model performance differently, especially through "volume" and "imbalance" effects. 

2) Proposing an algorithmic solution called "Upper Confidence Bound of the Expected Utility (UCB-EU)" to account for non-response probabilities when selecting queries in active learning. This involves modifying the utility calculation to incorporate a prediction of the probability of response.

3) Empirically demonstrating when and how much the proposed UCB-EU method can mitigate performance declines due to non-response on synthetic and real-world datasets. The method works well in many cases but can still struggle under certain data distributions.

In summary, the key contribution is formally characterizing the problem of biased non-response in active learning and providing a practical solution to help address it, supported by experiments on synthetic and real-world e-commerce data.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the main keywords or key terms associated with it are:

- Active learning
- Non-response
- Missing data
- Biased non-response
- Label requests
- Querying strategy
- Expected utility
- Upper confidence bound
- E-commerce
- CTR prediction

The paper focuses on active learning in the presence of non-response or missing labels when making label requests. It looks at the impact of biased non-response on model performance. The main methods proposed are adjusting the querying strategy to optimize expected utility and using an upper confidence bound approach. The techniques are evaluated in simulation and on an e-commerce dataset for CTR prediction.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in this paper:

1. The paper proposes adjusting the utility function for active learning by incorporating the probability of response. What are some of the key challenges in estimating this probability of response accurately, especially early on when there is little data?

2. The adjustment to the utility function treats the probability of response as independent for each sample. Is this a reasonable assumption? Could there be dependencies in the non-response that would require more complex modeling?  

3. The empirical evaluations rely largely on synthetic datasets. What kinds of real-world datasets would be most valuable to also test the method on to better evaluate its practical utility?

4. The method is evaluated primarily using uncertainty sampling and QBC acquisition functions. How might it perform with more complex acquisition functions like expected model change? Would any modifications be needed?

5. The paper identifies regions of the feature space that act as "black holes" that absorb queries without providing labels. Can you suggest any mechanisms to detect the presence of such regions and adapt the sampling strategy accordingly?  

6. The method does not help much for the Synthetic 3 DGP. Can you hypothesize some properties of datasets where the correction would not help or could potentially hurt performance?

7. The case study uses CTR as a proxy for probability of response. When might this be an inadequate assumption for modelling response rates?

8. The method adjusts for bias in the training data but does not address potential bias in the evaluation data. How should models be evaluated in the presence of biased non-response?

9. The method relies on accurate estimates of uncertainty to calculate upper confidence bounds. For complex blackbox models, how could uncertainty be quantified and propagated for this purpose?

10. The paper focuses on classification problems. How would the sampling strategy need to be adapted for regression problems with continuous labels? Would the challenges posed by non-response differ?


## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Active learning (AL) methods aim to efficiently improve ML models by selecting the most informative examples to label from a pool of unlabeled data. 
- Most AL methods assume all label requests receive a response, but in reality there is often non-response (missing labels).
- Biased non-response that depends on the examples' features is particularly harmful, causing "volume" and "imbalance" effects that degrade model performance. This type of non-response is common when labels rely on user interactions.

Proposed Solution:
- Conceptualize how different types of missingness (MCAR, MAR) impact model performance. MAR is most problematic.
- Propose expected utility sampling that incorporates probability of response into the AL utility calculation. This reduces wasted queries on hard-to-label examples.  
- Implement the solution as "Upper Confidence Bound of Expected Utility" (UCB-EU), which uses uncertainty estimates of response rate.

Contributions:
- Demonstrate empirically that biased non-response substantially degrades AL performance, while MCAR has less impact.
- Show UCB-EU improves performance in many cases by avoiding non-responsive regions of feature space.
- Find that avoiding non-responsive regions can overly focus AL on a narrow subspace, leading to worse decision boundaries.
- Apply method to ecommerce click data, showing benefits especially when the non-response model is accurate.

In summary, the paper formalizes the problem of biased non-response in AL, proposes an algorithmic solution, and demonstrates when it is beneficial as well as settings where challenges still remain.
