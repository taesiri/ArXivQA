# [Amortized Reparametrization: Efficient and Scalable Variational   Inference for Latent SDEs](https://arxiv.org/abs/2312.10550)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Continuous time models like neural ODEs are useful for modeling dynamical systems, but training them is computationally demanding due to reliance on adjoint methods for gradient estimation. Adjoint methods require solving an initial value problem during each iteration, which is expensive for stiff models or long time series.

Proposed Solution:
- The authors propose a new method called "Amortized Reparametrization for Continuous Time Auto-Encoding" (ARCTA) for variational inference in latent stochastic differential equations (SDEs). 

- Key idea is to eliminate the need for differential equation solvers by using a reparametrization trick and a novel amortization strategy. This reduces the inference problem to approximating expectations w.r.t normal distributions.

- Specifically, they split the timeseries into small batches and approximate the posterior over the latent state for each batch using an encoder network. The latent state posterior is constrained to be a Gaussian process to enable tractable inference.

- They use a recently derived reparametrization for expectations under linear SDEs to rewrite the evidence lower bound (ELBO) purely in terms of Gaussians, avoiding any differential equation solving.

- This allows them to obtain low-variance, unbiased stochastic gradients of the ELBO that scale independently of data size, timeseries length and model stiffness.


Main Contributions:

- Proposes a computationally efficient amortized variational inference method for latent SDEs based on a novel reparametrization trick.

- Achieves over 10x fewer model evaluations compared to adjoint methods in experiments, while avoiding stability issues for long chaotic time series.

- Enables efficient inference of nonlinear latent SDEs from high-dimensional time series data without any differential equation solving.

- Makes state-of-the-art latent SDE models feasible to train on commodity hardware by reducing compute requirements.

- Demonstrates competitive performance on benchmark timeseries modeling tasks like motion capture data.

In summary, the paper presents a scalable and efficient method for variational inference in latent stochastic differential equations that eliminates the need for costly differential equation solvers. This helps democratize recent advances in dynamical system modeling using neural SDEs.
