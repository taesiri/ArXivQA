# [Effective Use of Variational Embedding Capacity in Expressive End-to-End   Speech Synthesis](https://arxiv.org/abs/1906.03402)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the main research questions/hypotheses addressed are:

1) Can a unified framework based on "embedding capacity" (representational mutual information between embeddings and data) be used to analyze and configure latent variable models for speech synthesis?

2) Can modifying the variational posterior in these models to match the true posterior enable effective style/prosody transfer and high-quality prior samples from a single model?

3) Can hierarchical decomposition of embedding capacity allow separate control over the fidelity and variability of transfer when using these models?

In more detail:

- The authors propose using embedding capacity, formalized as representational mutual information, as a unified way to analyze different latent variable models for speech synthesis. They show it correlates with reconstruction quality/similarity to references.

- They modify the variational posterior to add dependencies on text/speaker, matching the true posterior's form. This enables high-fidelity prosody/style transfer and natural prior samples from the same model across a range of capacities.

- A hierarchical decomposition of capacity is introduced. By controlling the capacity allocated to different subsets of latents, they can trade off between transfer fidelity vs variability when sampling.

So in summary, the main hypotheses are around using capacity as a unified framework for these models, the benefits of matching the true posterior, and the advantages of hierarchical capacity decomposition. The experiments aim to demonstrate these hypotheses.


## What is the main contribution of this paper?

 The main contributions of this paper are:

1. It proposes using embedding capacity (representational mutual information) as a unified framework for analyzing and comparing latent variable models for speech synthesis. This allows evaluating the tradeoffs between precision and generality in heuristic models as well as directly controlling capacity in variational models.

2. It shows that modifying the variational posterior in conditional VAEs to match the true posterior allows the same model to be used effectively for high-precision prosody transfer, text-agnostic style transfer, and generating natural sounding prior samples. 

3. For multi-speaker models, matching the posterior to the true form helps preserve speaker identity during inter-speaker prosody transfer and when sampling from the prior.

4. It introduces a method to hierarchically decompose capacity across two sets of latents, allowing part of the variation to be specified while sampling the rest from the prior. This increases the versatility of the model.

5. The proposed model, Capacitron, demonstrates these contributions empirically, showing the ability to target different embedding capacities and enabling both high-fidelity and variable transfer as well as high-quality prior sampling using the same model architecture.

In summary, the key innovation is using embedding capacity as a unified framework for analyzing and configuring latent variable speech models, and showing how modifying the variational posterior can improve multi-purpose usage and transfer robustness. The proposed hierarchical capacity decomposition also increases model flexibility.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the key points from the paper:

The paper proposes using embedding capacity (representational mutual information between embeddings and data) as a unified framework for analyzing and configuring latent variable models for speech synthesis, and shows that modifying the variational posterior to match the true posterior enables high quality style/prosody transfer and prior sampling in a single model.


## How does this paper compare to other research in the same field?

 Here are a few key points about how this paper compares to other related work in end-to-end speech synthesis:

- The paper proposes analyzing latent variable models for speech synthesis in terms of their "embedding capacity" - the amount of information the latent embedding captures about the data. This provides a unifying framework for understanding and comparing different model architectures.

- It builds on previous heuristic/non-variational approaches like Global Style Tokens (Wang et al. 2018) and prosody transfer (Skerry-Ryan et al. 2018) by using a variational autoencoder framework. This allows more explicit control over embedding capacity.

- Compared to previous variational TTS models like Hsu et al. (2018) and Zhang et al. (2019), this work shows the importance of conditioning the variational posterior on text and speaker inputs to match the true posterior. This leads to better performance.

- The proposed Capacitron model demonstrates how the same architecture can achieve high-fidelity prosody transfer, text-agnostic style transfer, and high-quality prior sampling by controlling embedding capacity. This is more flexible than previous models targeting only one of these applications.

- For style transfer, the hierarchical decomposition of latents in Capacitron provides a way to control the tradeoff between transfer fidelity and sample variation, which is novel compared to earlier hierarchical TTS models.

- The inter-speaker transfer results show Capacitron can preserve speaker identity better than heuristic transfer models, likely due to the information limiting properties of variational methods.

So in summary, this paper brings together variational methods with conditioning strategies to develop a very flexible end-to-end TTS model that unifies and advances several threads of prior work through the lens of embedding capacity. The analyses and techniques introduced provide a strong foundation for future research in controllable speech synthesis.
