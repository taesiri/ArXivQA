# [Effective Use of Variational Embedding Capacity in Expressive End-to-End   Speech Synthesis](https://arxiv.org/abs/1906.03402)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the main research questions/hypotheses addressed are:1) Can a unified framework based on "embedding capacity" (representational mutual information between embeddings and data) be used to analyze and configure latent variable models for speech synthesis?2) Can modifying the variational posterior in these models to match the true posterior enable effective style/prosody transfer and high-quality prior samples from a single model?3) Can hierarchical decomposition of embedding capacity allow separate control over the fidelity and variability of transfer when using these models?In more detail:- The authors propose using embedding capacity, formalized as representational mutual information, as a unified way to analyze different latent variable models for speech synthesis. They show it correlates with reconstruction quality/similarity to references.- They modify the variational posterior to add dependencies on text/speaker, matching the true posterior's form. This enables high-fidelity prosody/style transfer and natural prior samples from the same model across a range of capacities.- A hierarchical decomposition of capacity is introduced. By controlling the capacity allocated to different subsets of latents, they can trade off between transfer fidelity vs variability when sampling.So in summary, the main hypotheses are around using capacity as a unified framework for these models, the benefits of matching the true posterior, and the advantages of hierarchical capacity decomposition. The experiments aim to demonstrate these hypotheses.
