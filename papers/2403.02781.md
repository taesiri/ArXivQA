# [PromptKD: Unsupervised Prompt Distillation for Vision-Language Models](https://arxiv.org/abs/2403.02781)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper "PromptKD: Unsupervised Prompt Distillation for Vision-Language Models":

Problem:
Existing methods for adapting large vision-language models (VLMs) like CLIP to downstream tasks primarily focus on designing various efficient prompt learning formats using scarce labeled data. They do not fully exploit the potential of prompts as effective knowledge distillers to transfer knowledge from a larger teacher VLM to a lightweight student VLM using abundant unlabeled in-domain data. 

Proposed Solution:
This paper proposes an unsupervised domain-specific prompt distillation framework called PromptKD. It distills the knowledge from a large CLIP teacher to a lightweight CLIP student model through prompt-driven imitation using extensive unlabeled images from the target domain. The framework has two key stages:

1) Teacher Pre-training: Pre-train a large CLIP teacher using advanced prompt learning methods on labeled domain data. Then store the text features of all classes from the teacher text encoder as shared class vectors.

2) Student Prompt Distillation: Share the stored teacher class vectors across teacher and student image encoders to calculate logits. Train student image encoder prompts and projector by aligning teacher and student logits via KL divergence on unlabeled domain images, encouraging student to mimic teacher.

Main Contributions:

- First method to perform unsupervised domain-specific prompt distillation for CLIP using unlabeled domain data.

- Leverages unique decoupled modalities of CLIP by reusing pre-stored teacher text features as class vectors, avoiding text encoder computations.

- Enables distillation on extensive unlabeled domain images by using teacher to generate soft labels, eliminating need for labeled data.

- Achieves new state-of-the-art performance on 11 datasets. For example, averages 2.7% and 4.63% accuracy gains on base and novel classes.

In summary, PromptKD advances prompt learning for VLMs from manual design to unsupervised distillation, transferring teacher knowledge to students via prompt imitation on abundant unlabeled domain data.


## Summarize the paper in one sentence.

 Here is a one-sentence summary of the paper:

The paper introduces an unsupervised domain prompt distillation framework for CLIP that transfers knowledge from a large teacher CLIP model to a lightweight student CLIP model through prompt imitation on extensive unlabeled domain data.


## What is the main contribution of this paper?

 According to the paper, the main contributions are:

1. The authors propose a novel unsupervised framework called "PromptKD" where prompts act as domain knowledge distillers, allowing a lightweight CLIP student model to absorb knowledge from a large CLIP teacher model using extensive unlabeled domain data. 

2. The method leverages CLIP's unique decoupled-modality property to reuse pre-stored teacher text features without incurring additional computation costs from the text branch, facilitating distillation and inference.

3. With the teacher-student paradigm, the method can utilize the teacher to generate soft labels on extensive unlabeled domain data, enabling training of the student without labeled images. 

4. Extensive experiments on 11 datasets demonstrate the effectiveness of the proposed method, where PromptKD outperforms previous methods and achieves state-of-the-art performance.

In summary, the main contribution is an unsupervised prompt-based knowledge distillation framework for CLIP that enables training on unlabeled domain data by reusing pre-computed teacher text features, achieving superior performance over previous methods.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper's content, some of the key terms and concepts associated with this paper include:

- Prompt learning - The paper focuses on using prompt learning techniques to adapt vision-language models like CLIP to downstream tasks. This involves using learnable soft prompts rather than manually designed hard prompts.

- Knowledge distillation - The paper proposes a novel prompt distillation framework to transfer knowledge from a large CLIP teacher model to a lightweight CLIP student model using extensive unlabeled domain data.

- Decoupled modality - The paper leverages CLIP's unique decoupled modality characteristic where the text and image encoders can be separated. This allows pre-computed teacher text features to be reused without recomputation. 

- Unlabeled domain data - The prompt distillation process enables training the student on large amounts of unlabeled in-domain data by using the teacher to generate soft labels.

- Text features as class vectors - The pre-stored teacher text features are used as shared classification weight vectors (class vectors) between the teacher and student image encoders.

- Logits alignment - The core of the distillation process is aligning the logits (predictions) of the teacher and student models via KL divergence loss on the unlabeled data.

- Transductive zero-shot learning - The student training paradigm corresponds to the transductive zero-shot learning setting where all the unlabeled data is used rather than just the labeled data.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in the paper:

1. The paper proposes a two-stage framework consisting of teacher pre-training and student prompt distillation. Can you explain in detail the motivation and design choices behind this two-stage approach? What are the advantages compared to end-to-end training?

2. In the teacher pre-training stage, existing advanced prompt learning methods are used. What is the rationale for using prompt learning over other adaptation methods for the teacher model? How do the teacher prompts aid knowledge transfer in later stages?  

3. The paper reuses pre-stored teacher text features as shared class vectors between teacher and student models. Explain the uniqueness of the text features that enables them to be pre-computed and shared in this manner. Why can't the same be applied to image features?

4. How exactly are the pre-stored teacher text features incorporated into the training process of the student model? Walk through the forward pass and explain where and how these text features are utilized.  

5. The student model only trains the image branch along with learnable prompts and a projector module. Why is training only a part of the model preferred over end-to-end fine-tuning? What challenges arise from this design choice?

6. Explain the role of the learnable visual prompts in the student image encoder. How do they encourage the mimicry of teacher outputs? Could you design alternate prompt mechanisms to achieve distillation?

7. What is the purpose of the projector added to the student image encoder? Why is an additional projector needed for distillation and how does it facilitate alignment? Analyze design variations.  

8. The paper claims efficient knowledge transfer without labeled data by using the teacher to generate soft pseudo-labels on unlabeled images. Critically analyze this approach - what caveats need consideration?

9. The inference process solely depends on the student image branch and stored teacher text features. Discuss the computational and efficiency benefits this offers over traditional teacher-student paradigms.

10. The method registers consistent improvements across diverse recognition datasets. Speculate on what intrinsic traits enable the wide applicability and analyze limitations. Suggest potential remedies.
