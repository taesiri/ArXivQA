# [PromptKD: Unsupervised Prompt Distillation for Vision-Language Models](https://arxiv.org/abs/2403.02781)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper "PromptKD: Unsupervised Prompt Distillation for Vision-Language Models":

Problem:
Existing methods for adapting large vision-language models (VLMs) like CLIP to downstream tasks primarily focus on designing various efficient prompt learning formats using scarce labeled data. They do not fully exploit the potential of prompts as effective knowledge distillers to transfer knowledge from a larger teacher VLM to a lightweight student VLM using abundant unlabeled in-domain data. 

Proposed Solution:
This paper proposes an unsupervised domain-specific prompt distillation framework called PromptKD. It distills the knowledge from a large CLIP teacher to a lightweight CLIP student model through prompt-driven imitation using extensive unlabeled images from the target domain. The framework has two key stages:

1) Teacher Pre-training: Pre-train a large CLIP teacher using advanced prompt learning methods on labeled domain data. Then store the text features of all classes from the teacher text encoder as shared class vectors.

2) Student Prompt Distillation: Share the stored teacher class vectors across teacher and student image encoders to calculate logits. Train student image encoder prompts and projector by aligning teacher and student logits via KL divergence on unlabeled domain images, encouraging student to mimic teacher.

Main Contributions:

- First method to perform unsupervised domain-specific prompt distillation for CLIP using unlabeled domain data.

- Leverages unique decoupled modalities of CLIP by reusing pre-stored teacher text features as class vectors, avoiding text encoder computations.

- Enables distillation on extensive unlabeled domain images by using teacher to generate soft labels, eliminating need for labeled data.

- Achieves new state-of-the-art performance on 11 datasets. For example, averages 2.7% and 4.63% accuracy gains on base and novel classes.

In summary, PromptKD advances prompt learning for VLMs from manual design to unsupervised distillation, transferring teacher knowledge to students via prompt imitation on abundant unlabeled domain data.
