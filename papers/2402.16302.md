# [Graph Diffusion Policy Optimization](https://arxiv.org/abs/2402.16302)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Graph generation has important applications like drug design, but directly optimizing graph diffusion models for specific objectives is challenging as most reward signals are non-differentiable. 

- Prior works that align diffusion models to rewards use policy gradients like REINFORCE, but directly applying them to graph models fails empirically.

Solution:
- The paper formulates graph diffusion as a Markov Decision Process (MDP) and aims to optimize an expected reward over generated graphs.

- It introduces a modified policy gradient method called "eager policy gradient", tailored for graph diffusion models. This replaces the gradient $\nabla_\theta \log p_\theta(\bm{G}_{t-1} | \bm{G}_t)$ in REINFORCE with $\nabla_\theta \log p_\theta(\bm{G}_0 | \bm{G}_t)$, focusing on optimizing the final generated graph $\bm{G}_0$ instead of every timestep.

- The proposed method Graph Diffusion Policy Optimization (GDPO) estimates this eager policy gradient using Monte Carlo rollouts from the current graph diffusion model.

Contributions:
- GDPO is the first effective method to align graph diffusion models with arbitrary black-box rewards, addressing a notable gap.

- It proves effective on various graph generation tasks like drug design and architecture search, achieving superior performance over prior graph generative models.

- For example, in molecular graph tasks, GDPO obtained 5.72% higher hit ratio and 1.48% better top 5% docking scores on average compared to state-of-the-art baselines.

In summary, the key innovation is an eager policy gradient approach tailored for effectively optimizing discrete graph diffusion models for any rewards. Experiments demonstrate GDPO's superior performance on diverse graph generation benchmarks.
