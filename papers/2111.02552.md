# [Is Bang-Bang Control All You Need? Solving Continuous Control with   Bernoulli Policies](https://arxiv.org/abs/2111.02552)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central research question this paper investigates is:To what extent does bang-bang control, where actions are restricted to the minimum or maximum values, emerge in reinforcement learning for continuous control problems, and how does this relate to performance?The key hypothesis seems to be that restricting policies to bang-bang control can achieve competitive performance on common benchmark tasks compared to standard Gaussian policies, despite the expectation that more refined continuous actions should be required. The paper provides theoretical grounding for why bang-bang behavior may arise, as well as extensive empirical analysis across algorithms and environments to evaluate this hypothesis.In summary, the paper aims to understand:- The prevalence and performance of bang-bang policies learned via RL on continuous control benchmarks.- The theoretical underpinnings for emergence of bang-bang control from an optimal control perspective. - How characteristics like exploration, action costs, and task objectives relate to bang-bang vs continuous policies.The overall goal is to improve understanding of learned behaviors in continuous control RL, particularly when they deviate from common assumptions, which can inform future benchmarking, algorithm design, and applications.


## What is the main contribution of this paper?

Based on my reading, the main contributions of this paper are:1. It provides extensive empirical evidence that bang-bang control policies emerge and perform well on standard continuous control benchmarks across a variety of recent RL algorithms. The authors show this by replacing the commonly used Gaussian policy with a Bernoulli policy that only selects extremal actions.2. It draws theoretical connections between the emergence of bang-bang behavior in RL and solutions from optimal control theory. In particular, it shows bang-bang control arises as optimal in minimum-time problems.3. It discusses challenges and trade-offs when trying to avoid bang-bang behavior in continuous control RL, such as the negative impact on exploration. It also evaluates modifications like action penalties.4. It demonstrates that bang-bang policies exhibit similar robustness to perturbations as Gaussian policies on simulated robotic control tasks.5. It provides additional analysis and experiments to disentangle the effects of exploration and final solution quality when comparing policy types. This includes distilling a bang-bang policy from a trained Gaussian teacher.In summary, the paper provides a comprehensive empirical and theoretical analysis of bang-bang emergence and performance in continuous control RL across various algorithms, environments, and experimental setups. It highlights open challenges like benchmark design when aiming to avoid bang-bang solutions.
