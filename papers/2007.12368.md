# [Self-Supervised Learning Across Domains](https://arxiv.org/abs/2007.12368)

## What is the central research question or hypothesis that this paper addresses?

This paper addresses the research question of how self-supervised learning can be used to improve domain generalization and adaptation for visual recognition tasks. The key hypotheses are:1. Self-supervised pretraining on tasks like solving jigsaw puzzles or recognizing image rotations can provide useful invariances and regularization that improve generalization to new visual domains.2. Integrating self-supervised objectives like jigsaw puzzles or rotation recognition into a multi-task framework together with supervised learning can boost performance on domain generalization and adaptation benchmarks. 3. The self-supervised signals help focus the model on intrinsic shape properties rather than superficial statistics of a particular domain. This allows the learned representations to transfer better to new domains.4. A multi-task approach combining self-supervision and supervision within a single model works better than separate pretraining and finetuning stages for domain generalization.5. Self-supervision can complement and improve existing domain generalization and adaptation techniques by providing useful inductive biases.The paper presents extensive experiments to evaluate these hypotheses on standard domain generalization and adaptation datasets using self-supervised pretraining, multi-task learning, and combinations with existing methods. The results generally validate the potential of self-supervision for improving robustness across visual domains.
