# [Retro-FPN: Retrospective Feature Pyramid Network for Point Cloud   Semantic Segmentation](https://arxiv.org/abs/2308.09314)

## What is the central research question or hypothesis that this paper addresses?

The central research question addressed in this paper is how to effectively utilize the inherent feature pyramid in prevalent encoder-decoder architectures for point cloud semantic segmentation. Specifically, the paper proposes a method called "Retrospective Feature Pyramid Network (Retro-FPN)" to improve per-point semantic feature prediction by modeling the feature propagation process as an explicit and retrospective refining process on point-level semantic information. The key ideas and components of Retro-FPN are:- Explicitly predicting per-point labels for all decoder layers to extract point-level semantic features at each stage. This allows region information to flow into points. - Introducing a "retro-transformer" in each layer to retrospectively summarize useful semantic information from the previous layer and refine the current semantic features. The retro-transformer contains a cross-attention block and a semantic gate unit.- The cross-attention block takes the current features as queries to attentively aggregate semantic contexts from surrounding points in the previous layer. - The semantic gate unit selectively incorporates the summarized contexts from the previous layer to refine the current semantic features.So in summary, the central hypothesis is that explicitly modeling the pyramidal feature propagation as a retrospective refining process on point-level semantics can better exploit the feature pyramid and improve per-point feature prediction for point cloud semantic segmentation. Retro-FPN is proposed to verify this hypothesis.


## What is the main contribution of this paper?

The main contribution of this paper is proposing Retro-FPN, a retrospective feature pyramid network for point cloud semantic segmentation. Specifically:- It proposes to model the feature propagation in hierarchical decoders as an explicit and retrospective refining process on point-level semantic information. This allows exploiting the feature pyramid more effectively. - It introduces a novel retro-transformer module in each pyramid layer to retrospectively summarize semantic contexts from the previous layer and refine the current semantic features. The retro-transformer uses a local cross-attention block and a semantic gate unit.- It integrates Retro-FPN with various backbones like Point Transformer, KPConv, and MinkowskiNet. Experiments on S3DIS, ScanNet, and SemanticKITTI datasets show consistent and significant improvements over state-of-the-art methods.In summary, the key novelty is explicitly modeling the pyramidal feature propagation as a context-aware, retrospective refining process on per-point semantic features. This improves per-point semantic prediction and unleashes the potential of prevailing hierarchical decoders.
