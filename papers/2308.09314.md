# [Retro-FPN: Retrospective Feature Pyramid Network for Point Cloud   Semantic Segmentation](https://arxiv.org/abs/2308.09314)

## What is the central research question or hypothesis that this paper addresses?

The central research question addressed in this paper is how to effectively utilize the inherent feature pyramid in prevalent encoder-decoder architectures for point cloud semantic segmentation. Specifically, the paper proposes a method called "Retrospective Feature Pyramid Network (Retro-FPN)" to improve per-point semantic feature prediction by modeling the feature propagation process as an explicit and retrospective refining process on point-level semantic information. The key ideas and components of Retro-FPN are:- Explicitly predicting per-point labels for all decoder layers to extract point-level semantic features at each stage. This allows region information to flow into points. - Introducing a "retro-transformer" in each layer to retrospectively summarize useful semantic information from the previous layer and refine the current semantic features. The retro-transformer contains a cross-attention block and a semantic gate unit.- The cross-attention block takes the current features as queries to attentively aggregate semantic contexts from surrounding points in the previous layer. - The semantic gate unit selectively incorporates the summarized contexts from the previous layer to refine the current semantic features.So in summary, the central hypothesis is that explicitly modeling the pyramidal feature propagation as a retrospective refining process on point-level semantics can better exploit the feature pyramid and improve per-point feature prediction for point cloud semantic segmentation. Retro-FPN is proposed to verify this hypothesis.


## What is the main contribution of this paper?

The main contribution of this paper is proposing Retro-FPN, a retrospective feature pyramid network for point cloud semantic segmentation. Specifically:- It proposes to model the feature propagation in hierarchical decoders as an explicit and retrospective refining process on point-level semantic information. This allows exploiting the feature pyramid more effectively. - It introduces a novel retro-transformer module in each pyramid layer to retrospectively summarize semantic contexts from the previous layer and refine the current semantic features. The retro-transformer uses a local cross-attention block and a semantic gate unit.- It integrates Retro-FPN with various backbones like Point Transformer, KPConv, and MinkowskiNet. Experiments on S3DIS, ScanNet, and SemanticKITTI datasets show consistent and significant improvements over state-of-the-art methods.In summary, the key novelty is explicitly modeling the pyramidal feature propagation as a context-aware, retrospective refining process on per-point semantic features. This improves per-point semantic prediction and unleashes the potential of prevailing hierarchical decoders.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper proposes Retro-FPN, a retrospective feature pyramid network, to improve per-point semantic feature prediction for 3D point clouds by modeling the feature propagation in a decoder as an explicit and retrospective refining process on point-level semantic information through the use of a novel retro-transformer module.


## How does this paper compare to other research in the same field?

Here are a few key points comparing this paper to other related work in point cloud semantic segmentation:- The paper focuses on improving the decoding stage in typical encoder-decoder architectures for point cloud segmentation. Much prior work has focused on developing better encoding methods or improving feature aggregation techniques. This paper takes a different approach by looking at refining the propagation and use of features in the decoder.- The proposed Retro-FPN method models the feature propagation in the decoder as an explicit, retrospective refining process operating on per-point semantic features. This differs from other approaches like point interpolation or naive upsampling and fusion of decoder features.- To enable this refining process, the method introduces a novel retro-transformer module that can summarize semantic context from the prior decoder level and selectively refine the features. This cross-level, context-aware refinement is a unique aspect compared to other work.- Retro-FPN is designed as a general plug-and-play module that can be integrated with different backbone networks, including both point-based and voxel-based architectures. Many other methods are tailored to specific backbone encoder designs.- Experiments integrate Retro-FPN with multiple state-of-the-art backbones and show consistent improvements in segmentation performance on various datasets. This demonstrates the broad applicability and effectiveness of the approach compared to methods that show gains on single architectures.In summary, the key novelties are the focus on refining the decoder features, the explicit point-level refining process, the retrospection and refinement enabled by the retro-transformer module, and the general applicability to diverse backbone encoders. These differentiating factors help Retro-FPN push state-of-the-art results for point cloud semantic segmentation.
