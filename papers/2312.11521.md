# [Large Language Models are Complex Table Parsers](https://arxiv.org/abs/2312.11521)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Complex tables with multi-level headers and merged cells are ubiquitous but complex Table QA remains a challenging task in NLP. 
- Prior work has focused more on simple flat tables and recently introduced complex table QA datasets like HiTAB and AIT-QA show state-of-the-art models still underperform.

Proposed Solution:
- Incorporate the powerful Generative Pre-trained Transformer 3.5 (GPT-3.5) to address the complex table QA task.
- Reconstruct complex tables into tuples encoding hierarchical structure, position information, and cell content. 
- Design specific prompt templates to enhance GPT-3.5's hierarchical structure awareness through explanatory descriptions and chain-of-thought style logical reasoning guidance.   
- Handle input length limits by prompting GPT-3.5 through single-turn or multi-turn dialogues, with code snippets to assist the multi-turn case.

Main Contributions:
- Novel approach leveraging GPT-3.5 as a parser for complex tables via table reformatting and tailored prompting.
- Resolve GPT-3.5 input token constraints by crafting single-turn and multi-turn prompt templates.
- Achieve new state-of-the-art results on HiTAB and AIT-QA complex table QA datasets, demonstrating GPT-3.5's effectiveness on this task.

The key innovation is enhancing GPT-3.5's perceptual capability regarding complex table structures to better unleash its reasoning and language generation competencies for the complex table QA task. Careful prompt engineering allows passing complex table understanding to the model.
