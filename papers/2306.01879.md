# [VisualGPTScore: Visio-Linguistic Reasoning with Multimodal Generative   Pre-Training Scores](https://arxiv.org/abs/2306.01879)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be:Can multimodal generative pre-training scores, such as the likelihood of text given an image, outperform discriminative scores on benchmarks designed to test for compositional visio-linguistic reasoning?The key hypothesis appears to be that generative pre-training scores, such as VisualGPTScore which models the conditional likelihood P(text|image), will demonstrate stronger compositional reasoning abilities compared to mainstream discriminative scores used by most vision-language models. The paper challenges the prevailing view that vision-language models are "bag-of-words" models that lack compositional understanding. It tests this hypothesis by evaluating VisualGPTScore on several recent benchmarks like ARO, Crepe, and Winoground that are designed to assess compositional reasoning. The paper also proposes an information-theoretic factorization of VisualGPTScore to diagnose issues in existing benchmarks and debias results.In summary, the central research question is whether generative pre-training scores can outperform discriminative scores on compositional reasoning benchmarks, in order to demonstrate that vision-language models have greater compositional capacities than commonly believed. The key hypothesis is that modeling the conditional likelihood P(text|image) will lead to stronger compositional reasoning compared to discriminative matching objectives.
