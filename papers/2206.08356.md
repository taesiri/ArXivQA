# [OmniMAE: Single Model Masked Pretraining on Images and Videos](https://arxiv.org/abs/2206.08356)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the main research question seems to be:

Can a single, unified transformer model be trained to perform well on both image and video recognition tasks? 

The authors hypothesize that by using masked autoencoding as the pretraining objective, they can train a vanilla Vision Transformer (ViT) model jointly on images and videos that will learn good general visual representations. This is in contrast to prior work that trains specialized models separately for images vs videos, or requires modified transformer architectures tailored for computer vision.

The key ideas explored are:

- Using masked autoencoding for self-supervised pretraining, where a percentage of image/video patches are masked out and the model must reconstruct them. This allows pretraining without human annotations.

- Showing that a simple ViT model can work effectively for both images and videos when trained this way, despite having no built-in inductive biases for computer vision like some other architectures.

- Demonstrating that joint pretraining on images and videos improves transfer performance on downstream tasks compared to models trained on a single modality.

- Using very high masking ratios during pretraining (e.g. 90% images, 95% video) to substantially reduce training compute and enable scaling to larger models.

So in summary, the main hypothesis is that masked pretraining enables a single vanilla ViT model to achieve strong performance on both images and videos, which they verify through extensive experiments.


## What is the main contribution of this paper?

 Based on the abstract and introduction, the main contributions of this paper seem to be:

- Proposing OmniMAE, a single Vision Transformer model for images and videos trained with masked autoencoding. Previous work has focused on separate models for each modality. 

- Showing that masked autoencoding can effectively pretrain a unified model on multiple visual modalities. Prior attempts at joint modeling have typically required specialized architectures or resulted in worse performance.

- Demonstrating that OmniMAE achieves strong performance on both image and video recognition benchmarks, comparable or better than specialized single-modality models, using just a simple Vision Transformer architecture.

- Enabling training of large OmniMAE models by using extremely high masking ratios of 90% on images and 95% on videos to reduce computation and memory usage substantially.

- Analyzing design choices like masking strategies, decoder architecture, dataset ratios, etc. to understand what works best for joint masked pretraining across modalities.

In summary, the main contribution seems to be proposing and analyzing OmniMAE, a single masked autoencoder model that achieves strong performance on both images and videos using just a simple Transformer architecture. The joint training enables unified modeling across visual modalities.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the key points from the paper:

The paper proposes OmniMAE, a single Masked Autoencoder Transformer model trained on images and videos using masked reconstruction that achieves strong performance on downstream tasks in both modalities.


## How does this paper compare to other research in the same field?

 Based on my reading of the paper, here is a summary of how it compares to other research in the same field:

- The paper focuses on training a single unified vision transformer model for both images and videos using masked autoencoding. This is a relatively new direction compared to much prior work that trains specialized models for each modality. 

- Most prior work on self-supervised representation learning has focused on either images (e.g. MAE, BEiT, DINO) or videos (e.g. VideoMAE, MaskedFeatV, BEVT) separately. There has been little work exploring joint self-supervised pretraining on both images and videos in a unified model.

- The concurrent work PolyViT does jointly train on images, videos and audio with a shared transformer encoder. However, PolyViT uses supervised pretraining objectives while this work relies only on unsupervised masked autoencoding.

- Another concurrent work, BEVT, also explores joint masked modeling of images and videos using a Swin transformer. However, it requires a separate tokenizer trained on images first. This work uses a simpler vanilla Vision Transformer architecture without any specialized inductive biases. 

- Compared to methods like Omnivore and PolyViT that require multi-head architectures and supervised pretraining, this work presents a simpler approach for unified modeling of visual modalities using only unsupervised masked reconstruction.

- The extreme masking ratios used in this work (90% images, 95% videos) enables much more efficient training compared to prior MAE works. This allows scaling up to larger models like ViT-Huge.

- This work shows that the simple vanilla Vision Transformer can work surprisingly well for both images and videos with proper pretraining. This is in contrast to other works like Swin Transformers and MViT that use specialized architectures for visual recognition tasks.

In summary, this work explores a new direction of unified self-supervised modeling of images and videos with a simple vanilla architecture and masking-based pretraining. The approach is simpler and more efficient than many prior works while achieving strong performance on both images and videos.
