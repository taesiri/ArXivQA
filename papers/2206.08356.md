# [OmniMAE: Single Model Masked Pretraining on Images and Videos](https://arxiv.org/abs/2206.08356)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the main research question seems to be:

Can a single, unified transformer model be trained to perform well on both image and video recognition tasks? 

The authors hypothesize that by using masked autoencoding as the pretraining objective, they can train a vanilla Vision Transformer (ViT) model jointly on images and videos that will learn good general visual representations. This is in contrast to prior work that trains specialized models separately for images vs videos, or requires modified transformer architectures tailored for computer vision.

The key ideas explored are:

- Using masked autoencoding for self-supervised pretraining, where a percentage of image/video patches are masked out and the model must reconstruct them. This allows pretraining without human annotations.

- Showing that a simple ViT model can work effectively for both images and videos when trained this way, despite having no built-in inductive biases for computer vision like some other architectures.

- Demonstrating that joint pretraining on images and videos improves transfer performance on downstream tasks compared to models trained on a single modality.

- Using very high masking ratios during pretraining (e.g. 90% images, 95% video) to substantially reduce training compute and enable scaling to larger models.

So in summary, the main hypothesis is that masked pretraining enables a single vanilla ViT model to achieve strong performance on both images and videos, which they verify through extensive experiments.


## What is the main contribution of this paper?

 Based on the abstract and introduction, the main contributions of this paper seem to be:

- Proposing OmniMAE, a single Vision Transformer model for images and videos trained with masked autoencoding. Previous work has focused on separate models for each modality. 

- Showing that masked autoencoding can effectively pretrain a unified model on multiple visual modalities. Prior attempts at joint modeling have typically required specialized architectures or resulted in worse performance.

- Demonstrating that OmniMAE achieves strong performance on both image and video recognition benchmarks, comparable or better than specialized single-modality models, using just a simple Vision Transformer architecture.

- Enabling training of large OmniMAE models by using extremely high masking ratios of 90% on images and 95% on videos to reduce computation and memory usage substantially.

- Analyzing design choices like masking strategies, decoder architecture, dataset ratios, etc. to understand what works best for joint masked pretraining across modalities.

In summary, the main contribution seems to be proposing and analyzing OmniMAE, a single masked autoencoder model that achieves strong performance on both images and videos using just a simple Transformer architecture. The joint training enables unified modeling across visual modalities.
