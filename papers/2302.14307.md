# [GradMA: A Gradient-Memory-based Accelerated Federated Learning with   Alleviated Catastrophic Forgetting](https://arxiv.org/abs/2302.14307)

## What is the central research question or hypothesis that this paper addresses?

 This paper appears to propose a new federated learning approach called GradMA to alleviate catastrophic forgetting caused by data heterogeneity and stragglers (inactive workers) in federated learning. The key ideas seem to be:

1) On the worker side, it uses quadratic programming to correct the gradient direction of each local model by taking into account the previous gradient, global model gradient, and difference between the current local model and global model. This aims to mitigate forgetting caused by data heterogeneity. 

2) On the server side, it maintains a memory buffer to store past accumulated updates from all workers. It then uses this memory buffer in quadratic programming to correct the update direction of the global model. This aims to mitigate forgetting caused by stragglers.

3) It provides a memory reduction strategy to make the approach practical for large numbers of workers. 

4) It analyzes the convergence theoretically and shows the convergence rate can achieve linear speedup with more workers.

So in summary, the central hypothesis appears to be that by correcting both the worker-side and server-side update directions using quadratic programming and gradients memory, catastrophic forgetting in federated learning can be alleviated, leading to better accuracy and efficiency. Theoretical analysis and experiments are provided to validate this hypothesis.


## What is the main contribution of this paper?

 This paper proposes a new federated learning approach called GradMA to alleviate catastrophic forgetting caused by data heterogeneity and stragglers. The key contributions are:

1. GradMA utilizes quadratic programming and memorized updates to simultaneously correct the update directions on both the server side and worker side. This takes inspiration from continual learning methods.

2. On the worker side, GradMA uses the gradients of the local and global models, and the parameter difference, as constraints in quadratic programming to correct the local update direction. 

3. On the server side, GradMA maintains a memory state to store accumulated updates from all workers. This memory state is used as a constraint in quadratic programming to augment the momentum of the global model.

4. A memory reduction strategy is proposed to reduce the size of quadratic programming and memory cost, enabling GradMA to scale to large numbers of workers. 

5. Theoretical analysis shows GradMA achieves a linear speedup in convergence rate as the number of active workers increases, under smooth non-convex settings.

6. Extensive experiments demonstrate GradMA outperforms state-of-the-art baselines significantly on image classification tasks under heterogeneous data distributions and varying degrees of stragglers.

In summary, the main contribution is a new federated learning method that leverages quadratic programming and memory to correct update directions on both server and workers, achieving faster convergence and higher accuracy compared to prior art. The memory reduction technique also allows it to scale effectively.
