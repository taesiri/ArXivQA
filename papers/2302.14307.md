# [GradMA: A Gradient-Memory-based Accelerated Federated Learning with   Alleviated Catastrophic Forgetting](https://arxiv.org/abs/2302.14307)

## What is the central research question or hypothesis that this paper addresses?

 This paper appears to propose a new federated learning approach called GradMA to alleviate catastrophic forgetting caused by data heterogeneity and stragglers (inactive workers) in federated learning. The key ideas seem to be:

1) On the worker side, it uses quadratic programming to correct the gradient direction of each local model by taking into account the previous gradient, global model gradient, and difference between the current local model and global model. This aims to mitigate forgetting caused by data heterogeneity. 

2) On the server side, it maintains a memory buffer to store past accumulated updates from all workers. It then uses this memory buffer in quadratic programming to correct the update direction of the global model. This aims to mitigate forgetting caused by stragglers.

3) It provides a memory reduction strategy to make the approach practical for large numbers of workers. 

4) It analyzes the convergence theoretically and shows the convergence rate can achieve linear speedup with more workers.

So in summary, the central hypothesis appears to be that by correcting both the worker-side and server-side update directions using quadratic programming and gradients memory, catastrophic forgetting in federated learning can be alleviated, leading to better accuracy and efficiency. Theoretical analysis and experiments are provided to validate this hypothesis.
