# [Artifacts or Abduction: How Do LLMs Answer Multiple-Choice Questions   Without the Question?](https://arxiv.org/abs/2402.12483)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Multiple-choice questions (MCQs) are commonly used to evaluate large language models (LLMs). It is important to verify that MCQ accuracy actually reflects the abilities we intend to measure in LLMs. 
- The paper investigates whether LLMs can exploit "artifacts" (biases or patterns) in MCQ dataset choices to answer questions correctly without needing the accompanying question text. If so, MCQ evaluations may overestimate LLM capabilities.

Method: 
- The paper tests "choices-only" prompts with 4 LLMs (LLaMA, Falcon, Mixtral, Phi) on 3 MCQ datasets (ARC, MMLU, HellaSwag). These prompts only provide the answer choices as input to see if choices alone contain enough signal.
- The choices-only accuracy is compared to majority class baselines. Accuracies substantially above the baseline suggest potential artifact exploitation. 
- To explain the artifact exploitation, the paper tests hypotheses around: 1) Memorization 2) Choice priors and dynamics 3) Abductive question inference.

Key Findings:
- In 11/12 cases, choices-only prompts significantly beat baseline, with over 0.3 accuracy gains. This suggests LLMs may use artifacts.
- There is little evidence memorization drives the high choices-only accuracy.
- LLMs seem to exploit interdependencies between choices, not just cues from individual choices.
- LLMs show an impressive partial ability to infer the original question from just the choices.  

Contributions:
- First paper to show LLMs can get high MCQ accuracy from just choices using few-shot prompting.
- Release an analysis toolkit to enable transparent MCQ evaluations.
- Find evidence that LLMs can both exploit artifacts but also show complex reasoning abilities when answering from limited context.


## Summarize the paper in one sentence.

 Here is a one-sentence summary of the paper:

The paper investigates whether large language models can exploit multiple-choice question answering dataset artifacts by achieving high accuracy when only provided the answer choices as input, and through analysis, finds evidence that while some accuracy may be attributed to surface-level shortcuts, the models also demonstrate more complex reasoning abilities like inferring relevant questions solely from the choices.


## What is the main contribution of this paper?

 The main contribution of this paper is threefold:

1) The authors assess "choices-only" prompts with four LLMs on three MCQA datasets, showing for the first time that LLMs can achieve high partial-input accuracy in few-shot settings with minimal exemplars.

2) They test several hypotheses, such as memorization, choice priors, and abductive question inference, to help explain the high choices-only accuracy exhibited by LLMs.

3) They release an in-depth black-box evaluation suite based on their proposed partial-input prompts to facilitate transparent LLM evaluations.

In summary, the paper highlights the potential for LLMs to exploit artifacts in MCQA evaluations, while also providing insights into the decision-making abilities of LLMs when only provided with partial inputs. The analysis framework and code release aim to enable more rigorous benchmarking of LLMs.


## What are the keywords or key terms associated with this paper?

 Based on my review of the paper, some of the key terms and concepts associated with it include:

- Multiple-choice question answering (MCQA)
- Large language models (LLMs) 
- Dataset artifacts 
- Partial-input models
- Choices-only prompts
- Memorization 
- Choice dynamics
- Individual priors
- Group dynamics
- Meta-strategies
- Abductive question inference (AQI)

The paper examines whether large language models are truly comprehending and reasoning when answering multiple-choice questions, or whether they are exploiting artifacts and shortcuts in the dataset. It introduces the concept of "choices-only" prompts to test whether models can answer questions when provided with only the answer choices. The paper then investigates potential explanations for the high performance on choices-only prompts, including memorization, choice dynamics, and abductive question inference. Overall, it aims to bring more transparency to MCQA evaluations of large language models and shed light on their decision-making processes.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in this paper:

1. How does the choices-only prompt allow the authors to probe whether LLMs are relying on artifacts rather than comprehending the passage and reasoning between the question and choices? What assumptions did the authors make about the capabilities of LLMs in designing this prompt?

2. The authors test three hypotheses to explain the high performance of LLMs on the choices-only prompt. Can you summarize each hypothesis and the prompts designed to test them? What are some limitations or alternative explanations that could be explored? 

3. When testing for memorization, what are some ways the prompts could be improved to more conclusively rule out memorization? Could the authors design prompts to test different types or degrees of memorization?  

4. In analyzing choice dynamics and priors, how did the authors quantify and compare the impact of individual choice priors versus group dynamics and meta-strategies? Can you explain their scoring methodology? What are its limitations?

5. Abductive question inference relies on the LLM's ability to generate a relevant question from the choices. How might the quality of generated questions be evaluated beyond just answer accuracy? What types of questions might an LLM struggle to infer accurately?

6. The agreement between the choices-only prompt and abductive question inference was only moderate. What might this imply about the strategies used by LLMs in the choices-only setting? How could the interplay between strategies be studied further?

7. In the qualitative analysis, what explanations were explored for cases when abductive question inference succeeded versus failed? Could further annotations examine the reasoning process in more detail? 

8. The authors find cases where the inferred question matches the meaning of the original question. What does this suggest about the shortcut exploitation versus reasoning abilities of LLMs? How could additional experiments be designed to study this?

9. This study focuses on analyzing LLM behavior in a black-box manner. What are some of the benefits and limitations of this approach? How could the study be extended by accessing model internals? 

10. The authors offer several suggestions for improving transparency in MCQA evaluations and limiting dataset artifacts. Can you summarize their main recommendations? What other solutions could help promote more robust benchmarking?
