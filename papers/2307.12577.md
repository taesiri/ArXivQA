# [PRIOR: Prototype Representation Joint Learning from Medical Images and   Reports](https://arxiv.org/abs/2307.12577)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the main research goal is to develop a joint representation learning framework that can effectively align medical images and reports at both global and local levels. The key hypotheses are:- Learning joint representations for medical images and text reports can transfer useful linguistic knowledge to the visual domain and improve performance on medical image analysis tasks. - Incorporating both global and local alignment between images and text can capture fine-grained relationships and improve localization tasks like segmentation and detection.- Representing text sentences as categorical prototypes can help the model focus on high-level clinical concepts rather than syntactic details. - Adding a cross-modality reconstruction task can further enforce mutual information sharing and recover low-level details lost during contrastive learning.In summary, the central hypothesis is that an approach combining global and local alignment, discrete categorical sentence representations, and cross-modal reconstruction will learn improved representations for medical images and text that transfer well to various analysis tasks. The experiments aim to demonstrate the value of this joint representation learning framework.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contributions appear to be:- Proposing a novel prototype representation learning framework to align medical images and reports via both global and local feature learning. This includes introducing a sentence-wise prototype memory bank to cluster similar sentences and enable discrete sentence embedding.- Designing a cross-modality alignment module to align global and local representations between images and reports. This uses both contrastive learning for global alignment and a novel sigmoid-based attention mechanism for local alignment. - Leveraging a cross-modality conditional reconstruction module to further capture fine-grained and structural representation by reconstructing masked images and generating sentence prototypes.- Demonstrating state-of-the-art performance on multiple datasets across five downstream tasks - supervised classification, zero-shot classification, image-to-text retrieval, semantic segmentation, and object detection.In summary, the main contribution seems to be proposing a new prototype representation learning approach for jointly pre-training medical images and reports. This framework effectively combines global and local alignment with discrete sentence embeddings and cross-modality reconstruction to learn improved representations for various medical image analysis tasks.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper proposes a new framework called PRIOR for learning joint representations of medical images and reports through global and local alignment as well as cross-modality conditional reconstruction.
