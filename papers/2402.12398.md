# [Primary and Secondary Factor Consistency as Domain Knowledge to Guide   Happiness Computing in Online Assessment](https://arxiv.org/abs/2402.12398)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Happiness computing models based on online data are being used to understand factors that contribute to people's happiness. However, different models can produce different explanations for happiness predictions, making it hard to know which factors are truly most important.  
- Two key challenges: (1) How to represent domain knowledge like the primary and secondary relationships between happiness factors; (2) How to use this knowledge to improve model training.

Proposed Solution:
- Empirically demonstrate the explanation inconsistency among popular happiness computing models on the same datasets.
- Prove multiple models with additive factor attributions like Shapley values have consistent primary/secondary factor relationships.
- Represent the factor relationships as importance distributions to encode domain knowledge.
- Use Kullback-Leibler divergence loss between model attributions and domain knowledge distributions to regularize training.

Main Contributions:
- Provide evidence that happiness computing models produce inconsistent explanations. 
- Theoretically prove consistency of primary/secondary factors among additive models.
- Novel method to encode factor relationships as distributions and use them to improve training.
- Experiments on real-world datasets demonstrate improvements in accuracy and explanation consistency when using the domain knowledge, allowing models to better predict happiness for the right reasons.

In summary, the paper tackles the problem of inconsistent explanations from happiness computing models, proposes representing domain knowledge as factor importance distributions, and uses this to improve both prediction accuracy and explanation faithfulness. The results have implications for supporting decision-making based on model predictions.
