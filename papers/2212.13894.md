# LAMBADA: Backward Chaining for Automated Reasoning in Natural Language

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the main research question is: Can integrating backward chaining with large language models lead to improved performance on automated reasoning tasks compared to existing approaches based on forward chaining or end-to-end training? The key hypothesis is that combining backward chaining (a classical AI automated reasoning technique) with the natural language understanding capacity of large language models will enable more effective automated reasoning on tasks where the inputs and inference chains are expressed in natural language. The paper proposes a new model called LAMBADA that integrates backward chaining with large language model modules to perform automated reasoning. It compares LAMBADA experimentally to existing approaches like Chain-of-Thought and Selection-Inference on challenging deductive reasoning datasets. The main result is that LAMBADA substantially outperforms these baselines, especially on longer reasoning chains, demonstrating the benefits of incorporating backward chaining strategies into language model based reasoning.In summary, the central research question is whether combining backward chaining with language models can improve automated reasoning with natural language, which the paper aims to validate through the proposed LAMBADA model and comparative experiments on reasoning datasets.


## What is the main contribution of this paper?

The main contribution of this paper is developing LAMBADA, a backward chaining algorithm for automated reasoning in natural language using language models. The key ideas are:- Using backward chaining rather than forward chaining for proof search, as backward chaining is more efficient and goal-directed. The authors argue and show experimentally that backward chaining is better suited for text-based deductive reasoning compared to popular forward chaining approaches.- Decomposing the reasoning into four modular components - Fact Check, Rule Selection, Goal Decomposition, and Sign Agreement - each implemented by an LM module through few-shot prompting. This allows combining the strength of LMs for natural language understanding with symbolic backward chaining for robust reasoning. - Showing significant improvements in accuracy over state-of-the-art methods like Chain-of-Thought and Selection-Inference on challenging deductive reasoning datasets, especially on examples requiring longer proof chains. The gains are particularly large on a dataset with more naturalistic text.- Demonstrating the query efficiency, generalizability to longer chains, and lexical robustness of LAMBADA.In summary, the key contribution is developing and evaluating a hybrid neuro-symbolic reasoning algorithm LAMBADA that integrates goal-directed backward chaining with modular LM components for automated reasoning over natural text. The results strongly demonstrate the advantages of incorporating backward chaining into language model based reasoning.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper develops a new technique called LAMBADA that combines large language models with backward chaining to significantly improve performance on challenging logical reasoning tasks requiring multi-step inference chains.


## How does this paper compare to other research in the same field?

Here are some key ways this paper compares to other research on automated reasoning and logical deduction with natural language:- Backward chaining algorithm: This paper proposes a novel backward chaining algorithm called LAMBADA for automated reasoning. Most prior work has focused on forward chaining or end-to-end reasoning. Backward chaining has been shown to be more efficient for automated reasoning in symbolic systems, but hasn't been widely explored for reasoning with natural language and large language models until this work.- Modular reasoning: The paper proposes decomposing reasoning into separate modules handled by an LM, which contrasts with end-to-end or monolithic reasoning approaches. The modular design allows combining the strengths of symbolic methods like backward chaining with LMs. Related modular reasoning ideas have been explored in some prior work.- Reasoning with natural language: A key focus is reasoning over natural language statements and rules, rather than purely formal logical representations. The proposed approach is evaluated on challenging natural language reasoning datasets. Most prior work has focused more on formal logical reasoning.- Scaling laws: The paper provides analysis showing backward chaining combined with LM modules mitigates some scaling limitations of forward chaining, like combinatorial search spaces. This contributes empirical evidence about reasoning strategies for LMs.- Accuracy improvements: Substantial accuracy improvements are demonstrated over state-of-the-art baselines relying on forward chaining or end-to-end reasoning. The gains are especially significant for problems requiring longer reasoning chains. This shows the promise of backward chaining for multi-hop reasoning.In summary, the proposed backward chaining approach, modular reasoning design, and gains on natural language reasoning differentiate this work from prior research and contribute important insights on automated reasoning strategies with LMs. The empirical analysis also reveals limitations of prevailing approaches.
