# [Using Self-Supervised Learning Can Improve Model Robustness and   Uncertainty](https://arxiv.org/abs/1906.12340)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the key research questions and hypotheses of this paper appear to be:1) Can self-supervised learning improve model robustness and uncertainty estimates beyond what is possible with purely supervised training? The paper hypothesizes that adding self-supervised auxiliary tasks during training can improve model robustness to adversarial examples, label noise/corruption, common input corruptions, and enhance out-of-distribution detection. 2) Can self-supervised learning surpass fully supervised methods for out-of-distribution detection, particularly on near-distribution anomalies?The paper hypothesizes that the representations learned via self-supervision may better capture the structure of in-distribution data, allowing for superior OOD detection compared to supervised methods alone.3) Should robustness and uncertainty estimation be considered as additional evaluation axes for self-supervised learning? The paper proposes that future self-supervised methods could be judged based on their ability to improve model robustness and uncertainty estimates, not just standard accuracy metrics.In summary, the key hypotheses are that self-supervision can benefit robustness and uncertainty in ways that supervised learning alone cannot, and these benefits are often masked when evaluating only on clean accuracy. The paper aims to demonstrate these gains across several robustness tasks and OOD detection.


## What is the main contribution of this paper?

The main contribution of this paper is showing that self-supervised learning can improve model robustness and uncertainty estimation, even when used in conjunction with fully supervised training on a large labeled dataset like ImageNet. Specifically, the paper demonstrates that adding an auxiliary rotation prediction task during training improves robustness to adversarial examples, label corruption, and common input corruptions. It also greatly improves out-of-distribution detection, allowing self-supervised methods to surpass fully supervised techniques on CIFAR and ImageNet experiments. The key insight is that while self-supervision may not boost standard accuracy much when combined with full supervision, it provides regularization that makes models more robust. The gains are masked when evaluating only on clean test accuracy.Overall, this suggests robustness and uncertainty tasks as new evaluation dimensions for self-supervised learning research beyond just standard accuracy. The results show the utility of self-supervision even with access to large labeled datasets, and demonstrate it can improve robustness without requiring more data or model capacity.
