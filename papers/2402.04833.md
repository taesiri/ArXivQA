# [Long Is More for Alignment: A Simple but Tough-to-Beat Baseline for   Instruction Fine-Tuning](https://arxiv.org/abs/2402.04833)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
Recent works have shown that instruction fine-tuning (IFT) of large language models (LLMs) benefits from using fewer but higher-quality demonstration examples. However, curating such datasets requires expensive solutions like leveraging very large proprietary models or significant human effort. This raises the question - what defines high-quality instructions and how can we obtain them efficiently?

Method:
This paper investigates length of the response as a heuristic to select high-quality training examples from existing datasets. Surprisingly, they show that simply picking the 1,000 examples with the longest responses from Alpaca or Evol-Instruct leads to better instruction-following performance than more sophisticated methods like AlpaGasus and LIMA according to LLMs like GPT-4. 

To further push the limits, they refine the longest 1k Alpaca instructions using GPT-3.5 Turbo and combine it with a recent data augmentation method NEFTune. This allows a LLaMA-2 model fine-tuned on only 1,000 instructions to match or exceed the performance of models trained on orders of magnitude more data.

Contributions:
- Show that length is an effective heuristic to extract high-quality IFT examples, consistently outperforming complex manual or automated curation.
- Demonstrate SFT on just 1,000 instructions can compete with alignment schemes using hundreds of thousands of examples and millions of preference pairs. 
- Introduce an inexpensive refinement procedure via GPT-3.5 Turbo to further improve quality.
- Extensive analysis ruling out that improvements stem solely from length bias. Evaluate impact on factual knowledge using Open LLM benchmarks.
- Conclude that 1k longest instructions form a strong baseline for instruction tuning research.
