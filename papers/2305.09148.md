# [Dual-Alignment Pre-training for Cross-lingual Sentence Embedding](https://arxiv.org/abs/2305.09148)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be:How can we improve cross-lingual sentence embedding by enabling both sentence-level and token-level alignment during pre-training? The key hypothesis appears to be:Incorporating token-level alignment objectives in addition to sentence-level alignment will result in better cross-lingual sentence embeddings compared to using sentence-level alignment alone.In particular, the authors propose a new pre-training framework called "dual-alignment pre-training" (DAP) that combines sentence-level alignment using a translation ranking task with a new proposed token-level alignment task called "representation translation learning" (RTL). The goal is to leverage both alignment signals during pre-training in order to learn cross-lingual sentence embeddings that are effective for downstream cross-lingual natural language processing tasks like bitext retrieval, bitext mining, and cross-lingual inference.So in summary, the central research question is how to improve cross-lingual sentence embeddings using both sentence and token alignment, with the key hypothesis that dual alignment pre-training will outperform sentence-level alignment alone. The DAP framework with RTL is proposed to test this hypothesis.
