# [PDiT: Interleaving Perception and Decision-making Transformers for Deep   Reinforcement Learning](https://arxiv.org/abs/2312.15863)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem: 
Designing better deep networks and reinforcement learning (RL) algorithms are both important for deep RL. Existing methods either fuse the roles of environmental perception and decision-making in one model, which is suboptimal, or use a combination of different modules like CNN, RNN, Transformer which complicates the model design. The goal is to have a simple yet powerful model that specializes in perception and decision-making for general deep RL settings with multi-modal observations.

Proposed Solution:
The paper proposes the Perception and Decision-making Interleaving Transformer (PDiT) network which interleaves a perceiving Transformer that focuses on environmental perception by processing observations and a deciding Transformer that focuses on decision-making by conditioning on history of returns, perceiver outputs and actions. 

Main Contributions:

1) Proposes Vanilla-PDiT which naively stacks the deciding Transformer on the perceiving Transformer and full PDiT which interleaves perceiving and deciding Transformer blocks in a PDiT block.

2) Shows PDiT is generally applicable for a variety of RL settings: online/offline RL algorithms, policy gradient/Q learning methods, environments with image/proprioception/image-language observations without changing network architecture.

3) Achieves superior performance compared to strong baselines like Decision Transformer, ResNet, CoBERL etc. in Atari, MuJoCo and BabyAI environments trained using PPO, CQL and RvS based methods.

4) Visualizations and ablation studies provide better understanding about PDiT's representations and design choices.

In summary, the key insight is to specialize perception and decision-making into two Transformers in a compact framework while still being generally applicable across diverse RL settings. The interleaving of Transformer blocks is vital for information interaction and performance.
