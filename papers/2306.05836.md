# [Can Large Language Models Infer Causation from Correlation?](https://arxiv.org/abs/2306.05836)

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper addresses is: Can large language models infer causation from correlation?Specifically, the paper proposes a novel task called "correlation-to-causation inference" (Corr2Cause) to probe and benchmark the pure causal inference abilities of large language models. The key hypothesis is that current LLMs may not perform well on inferring causality purely from correlational statements, without relying on empirical knowledge.To test this hypothesis, the authors:- Formulate the Corr2Cause task, which takes correlational statements as input and determines the causal relationship between variables. - Create a dataset of over 400K examples following principles from causal discovery research.- Evaluate 17 existing LLMs on this dataset and find they perform poorly, close to random guess levels.- Explore finetuning LLMs on this dataset, showing improved performance but lack of out-of-distribution generalization.In summary, the main research question is whether LLMs can perform pure causal inference given only correlational statements. The key finding is that current LLMs lack this reasoning skill, motivating further research into improving LLMs' abilities for causal reasoning. The Corr2Cause dataset provides a valuable benchmark for this line of research.


## What is the main contribution of this paper?

The main contribution of this paper is proposing a new task and dataset to test the pure causal inference abilities of large language models (LLMs). Specifically:- The paper formulates a novel task called "correlation-to-causation inference" (Corr2Cause), which involves inferring causality purely from correlational statements, without relying on empirical knowledge. - The authors compose a large-scale dataset with over 400K examples for this task, using insights from causal discovery research to systematically generate correlation statements and causal hypotheses.- The paper evaluates 17 existing LLMs on this dataset and shows they perform poorly, close to random guess levels. This demonstrates a key limitation of current LLMs' reasoning abilities.- The authors further explore whether LLMs can learn this skill by finetuning on the dataset. Finetuned models achieve high accuracy on the original test set but fail to generalize to out-of-distribution perturbations of the test set.- This is the first benchmark dataset and set of experiments probing the pure causal inference abilities of LLMs. The authors argue this is an important reasoning skill for LLMs to have and that their dataset can motivate further research to improve LLMs in this aspect.In summary, the key contribution is proposing a new task and dataset to measure and improve LLMs' skills at inferring causality purely from correlational data/statements, without relying on empirical knowledge. This tests an important aspect of reasoning that is currently limited in LLMs.


## How does this paper compare to other research in the same field?

Here are a few key ways this paper compares to other related work in causal inference and reasoning in NLP:- It proposes a new task, Corr2Cause, that focuses specifically on pure causal reasoning from correlation statements. Most prior work has focused on inferring causality from empirical knowledge rather than formal reasoning principles. This is the first dataset to directly test models' correlation-to-causation inference abilities.- The dataset is large-scale, with over 400K samples generated in a principled way based on concepts from causal discovery research. Many existing causal inference datasets in NLP tend to be small-scale or built in a less systematic manner.- The paper finds that current LLMs perform very poorly on this pure causal inference task, close to random chance levels. This suggests existing models have limited reasoning abilities despite their strong performance on many NLP benchmarks. - The robustness experiments reveal LLMs fail to generalize their causal skills to out-of-distribution data, even after finetuning. This highlights issues with spurious pattern matching rather than robust causal learning.- The paper focuses on causal graphs with a small number of variables. Scaling up the complexity could be an interesting direction for future work.Overall, this paper makes a strong contribution in identifying and quantifying an important missing capability in current LLMs - the ability to do pure causal reasoning. The task and dataset enable targeted investigation of this skill. The poor performance highlights important limitations of current models and the need for advances in causal learning and robust reasoning.


## What future research directions do the authors suggest?

Based on my reading of the paper, some of the main future research directions suggested by the authors include:- Exploring larger graphs with more than 6 nodes. The authors limited their causal graphs to 2-6 nodes, but suggest future work could generate datasets with larger graphs.- Generating datasets that assume hidden confounders. The authors did not include hidden confounders in their graphs. They suggest future work could create more challenging datasets that require inferring the existence of hidden confounders, similar to the FCI algorithm. - Connecting the dataset to real-world examples of false beliefs from confusing correlation and causation. The authors were motivated by invalid reasoning patterns that lead to false beliefs. They suggest future work could connect this benchmark dataset to more real-world examples of people making incorrect causal inferences.- Studying the effect of in-context learning when querying the models. The authors suggest exploring if providing examples in the context for the models improves performance.- Adding more signals to the context to guide the models, such as explicitly stating rules of causal inference.- Analyzing example chains of reasoning that models use on these questions to better understand their inference process.In summary, key future directions include expanding the scope and complexity of the graphs and datasets, connecting the task closer to real-world reasoning, and analyzing model performance more deeply through in-context learning, adding signals to guide the models, and inspecting full reasoning chains. The authors propose this as a first step and encourage future work to build on this causal inference benchmark.
