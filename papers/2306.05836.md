# [Can Large Language Models Infer Causation from Correlation?](https://arxiv.org/abs/2306.05836)

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper addresses is: Can large language models infer causation from correlation?Specifically, the paper proposes a novel task called "correlation-to-causation inference" (Corr2Cause) to probe and benchmark the pure causal inference abilities of large language models. The key hypothesis is that current LLMs may not perform well on inferring causality purely from correlational statements, without relying on empirical knowledge.To test this hypothesis, the authors:- Formulate the Corr2Cause task, which takes correlational statements as input and determines the causal relationship between variables. - Create a dataset of over 400K examples following principles from causal discovery research.- Evaluate 17 existing LLMs on this dataset and find they perform poorly, close to random guess levels.- Explore finetuning LLMs on this dataset, showing improved performance but lack of out-of-distribution generalization.In summary, the main research question is whether LLMs can perform pure causal inference given only correlational statements. The key finding is that current LLMs lack this reasoning skill, motivating further research into improving LLMs' abilities for causal reasoning. The Corr2Cause dataset provides a valuable benchmark for this line of research.


## What is the main contribution of this paper?

The main contribution of this paper is proposing a new task and dataset to test the pure causal inference abilities of large language models (LLMs). Specifically:- The paper formulates a novel task called "correlation-to-causation inference" (Corr2Cause), which involves inferring causality purely from correlational statements, without relying on empirical knowledge. - The authors compose a large-scale dataset with over 400K examples for this task, using insights from causal discovery research to systematically generate correlation statements and causal hypotheses.- The paper evaluates 17 existing LLMs on this dataset and shows they perform poorly, close to random guess levels. This demonstrates a key limitation of current LLMs' reasoning abilities.- The authors further explore whether LLMs can learn this skill by finetuning on the dataset. Finetuned models achieve high accuracy on the original test set but fail to generalize to out-of-distribution perturbations of the test set.- This is the first benchmark dataset and set of experiments probing the pure causal inference abilities of LLMs. The authors argue this is an important reasoning skill for LLMs to have and that their dataset can motivate further research to improve LLMs in this aspect.In summary, the key contribution is proposing a new task and dataset to measure and improve LLMs' skills at inferring causality purely from correlational data/statements, without relying on empirical knowledge. This tests an important aspect of reasoning that is currently limited in LLMs.


## How does this paper compare to other research in the same field?

Here are a few key ways this paper compares to other related work in causal inference and reasoning in NLP:- It proposes a new task, Corr2Cause, that focuses specifically on pure causal reasoning from correlation statements. Most prior work has focused on inferring causality from empirical knowledge rather than formal reasoning principles. This is the first dataset to directly test models' correlation-to-causation inference abilities.- The dataset is large-scale, with over 400K samples generated in a principled way based on concepts from causal discovery research. Many existing causal inference datasets in NLP tend to be small-scale or built in a less systematic manner.- The paper finds that current LLMs perform very poorly on this pure causal inference task, close to random chance levels. This suggests existing models have limited reasoning abilities despite their strong performance on many NLP benchmarks. - The robustness experiments reveal LLMs fail to generalize their causal skills to out-of-distribution data, even after finetuning. This highlights issues with spurious pattern matching rather than robust causal learning.- The paper focuses on causal graphs with a small number of variables. Scaling up the complexity could be an interesting direction for future work.Overall, this paper makes a strong contribution in identifying and quantifying an important missing capability in current LLMs - the ability to do pure causal reasoning. The task and dataset enable targeted investigation of this skill. The poor performance highlights important limitations of current models and the need for advances in causal learning and robust reasoning.
