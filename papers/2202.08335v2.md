# [Task-Agnostic Graph Explanations](https://arxiv.org/abs/2202.08335v2)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question seems to be: 

How can we develop graph neural network (GNN) explainers that are task-agnostic, i.e., able to provide explanations without being tied to a specific prediction task?

The key hypotheses appear to be:

1) By decomposing the prediction model into an embedding model and lightweight downstream models, separate explainers can be designed for each component.

2) A self-supervised training framework based on conditioned contrastive learning can be used to train an embedding explainer without knowledge of downstream tasks. 

3) The proposed framework, Task-Agnostic GNN Explainer (TAGE), will enable explaining GNN models more efficiently in cases where downstream tasks are unknown or multitask predictions need to be explained.

In summary, the paper introduces the task-agnostic explanation problem for GNNs and proposes the TAGE framework to address the limitations of existing task-specific explainers. The central hypothesis is that the proposed approach will allow explaining GNNs with unknown or multiple downstream tasks using a single embedding explainer.


## What is the main contribution of this paper?

 Based on the LaTeX code provided, this appears to be a paper submission with the following key contributions:

1. It proposes a task-agnostic GNN explanation framework called TAGE that can explain GNN models trained in a two-stage fashion without requiring knowledge of downstream tasks. 

2. TAGE decomposes the explanation into an embedding explainer and lightweight downstream explainers. The embedding explainer is trained with a self-supervised conditioned contrastive learning objective to maximize mutual information between original and subgraph embeddings.

3. Experiments on molecular, protein, and e-commerce graph datasets demonstrate TAGE's ability to efficiently provide high quality explanations for multiple downstream tasks using a single embedding explainer. 

4. Comparisons show TAGE achieves higher fidelity and sparsity scores than task-specific explainers like GNNExplainer and PGExplainer. It also significantly reduces computational cost for explaining multiple tasks.

5. Visualizations provide qualitative results showing TAGE identifies meaningful substructures while other methods highlight scattered unimportant edges. Explanations for embedding dimensions demonstrate the method's ability for task-agnostic explanations.

In summary, the key contribution is proposing a framework for task-agnostic explanation of GNNs that enables efficient multitask explanation and explaining models trained without downstream task knowledge. The self-supervised conditioned contrastive training objective and decomposition into embedding/downstream explainers are novel components. Experiments demonstrate efficiency, quality, and flexibility of the proposed TAGE framework.
