# [Beyond Transduction: A Survey on Inductive, Few Shot, and Zero Shot Link   Prediction in Knowledge Graphs](https://arxiv.org/abs/2312.04997)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
The paper discusses the tasks of inductive link prediction (ILP), few-shot link prediction (FSLP) and zero-shot link prediction (ZSLP) in knowledge graphs, which go beyond the traditional transductive link prediction setting. The key problems highlighted are:

1) Lack of commonly accepted benchmark datasets, leading to results not being comparable across papers. 

2) Confusing terminology used to define ILP, with different papers referring to the same experimental setup using different names, and vice versa. This makes it very difficult to understand exactly what task and setting is being evaluated.

3) Need for clearly delineating ILP, FSLP and ZSLP from each other. Currently there is overlap in how some papers define and use these terms.

Proposed Solution:

1) The paper first systematically surveys 129 papers on ILP, FSLP and ZSLP, analyzing trends, models and datasets used.

2) It then deeply analyzes the terminology issues, inconsistent definitions and lack of clarity around these 3 tasks. 

3) Finally, it proposes a simple and consistent anchor-based nomenclature to unambiguously define test setups in terms of which elements - head, relation or tail - are seen vs unseen. This nomenclature clearly maps existing work to 8 possible test triple configurations.

Main Contributions:

1) First broad survey focused specifically on ILP, FSLP and ZSLP for link prediction in knowledge graphs

2) Identifies and highlights the serious issue of confusing terminology in defining these tasks

3) Proposes a simple anchor-based naming scheme to consistently define test setups in terms of seen vs unseen elements

4) Maps all surveyed papers to the proposed taxonomy, revealing trends and gaps

The paper makes an important contribution towards standardizing terminology and evaluation of ILP, FSLP and ZSLP tasks, enabling better understanding and fairer comparison of techniques. The proposed taxonomy also reveals under-explored settings for future work.


## Summarize the paper in one sentence.

 This paper systematically reviews the literature on inductive, few-shot, and zero-shot link prediction in knowledge graphs, analyzes inconsistencies in terminology and evaluation, and proposes a unified nomenclature based on seen vs unseen elements in test triples to unambiguously characterize different settings.


## What is the main contribution of this paper?

 According to the paper, the main contributions are:

1. It is the first systematic review of existing literature on inductive, few-shot, and zero-shot link prediction (LP) in knowledge graphs. It provides an overview of trends in model designs, datasets, and general research directions. 

2. It conducts a thorough analysis that clearly shows the lack of consistent terminology and task definitions for the aforementioned LP settings across different papers. This limits the possibility of comparing approaches.

3. It attempts to provide a unified understanding of each setting by dissecting their intrinsic characteristics. It proposes a consistent naming nomenclature to refer to the settings unambiguously, avoiding overlap between them. This is based on the notion of "anchors", i.e. seen vs unseen elements in test triples.

In summary, the key contribution is identifying and addressing the terminological inconsistencies in inductive, few-shot and zero-shot LP through a systematic analysis, and proposing a unified nomenclature to define these tasks clearly. This provides a foundation for better framing and comparing research in knowledge-scarce LP scenarios.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and keywords associated with this paper include:

- Link prediction (LP)
- Knowledge graphs (KGs) 
- Knowledge graph embeddings (KGEMs)
- Few-shot learning (FSL)
- Zero-shot learning (ZSL)
- Inductive link prediction (ILP)
- Few-shot link prediction (FSLP)
- Zero-shot link prediction (ZSLP)
- Transductive setting
- Semi-inductive setting
- Fully-inductive setting 
- Metric-based models
- Optimization-based models
- Benchmark datasets
- Terminology
- Nomenclature
- Anchors (seen vs unseen elements)

The paper conducts a systematic literature review of research related to link prediction in knowledge graphs under low-resource or unseen settings like few-shot, zero-shot, and inductive. It analyzes trends, datasets, task definitions, terminology, and proposes a unified nomenclature for referring to these settings consistently based on seen vs unseen "anchor" elements in test triples. The key focus areas are few-shot LP, zero-shot LP, inductive LP, and analysis of terminology, definitions, and benchmarks around these settings.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in this paper:

1. The paper proposes an anchor-based nomenclature to refer to different link prediction settings in a consistent manner. What are the key benefits of having such a unified naming convention? How does it help advance research in this area?

2. The proposed nomenclature uses a binary indexing approach to characterize the "signature" of a test triple, denoting whether each element (head, relation, tail) is seen or unseen. What is the intuition behind capturing the test triples in this fine-grained, element-wise manner? 

3. The paper maps several existing link prediction settings proposed in previous works, such as "semi-inductive", "fully inductive" etc. to the newly proposed anchor-based naming convention. What inconsistencies or contradictions are revealed through this mapping?

4. The survey analysis reveals that the ILP setting has diverging definitions across papers. How does this impact fair comparison of models? How can the proposed unified nomenclature alleviate this issue?  

5. The paper argues that Few-Shot Link Prediction is closer to the transductive setting than Zero-Shot or Inductive Link Prediction. What is the rationale behind this argument? Do you agree?

6. The survey reveals that the $I_{000}$ anchor-based setting (test triples with unseen head, relation and tail) is currently unexplored in link prediction research. What are the challenges associated with this setting? Do you think models should aim to tackle this setting as well?

7. The terminology used to refer to emerging entities and relations is very diverse across papers (out-of-graph, out-of-knowledge-base etc.). Does this variety of terms provide any additional useful information? Should authors converge on a smaller set of terms?

8. The paper suggests that an inductive model could be few-shot or zero-shot, based on the availability of support triples. However, keeping these settings disjoint based on how the support triples are used (training vs test-time) is also reasonable. Comment on this debate. 

9. The survey advocates the need for widely accepted benchmark datasets in link prediction. What steps can be taken by the research community to define standardized splits and evaluation protocols?  

10. The paper reflects on future research directions such as tackling unseen entities and relations jointly in the ILP setting. What other promising and under-explored avenues exist in advancing knowledge-scarce link prediction?
