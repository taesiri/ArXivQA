# [On the Effectiveness of Distillation in Mitigating Backdoors in   Pre-trained Encoder](https://arxiv.org/abs/2403.03846)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem: 
Self-supervised learning (SSL) has become popular for pre-training encoders on large unlabeled datasets and releasing them for downstream tasks to fine-tune. However, SSL faces serious security threats as adversaries can inject backdoors into pre-trained encoders, which then get inherited by downstream models. Defending against such backdoored encoders is an open challenge.

Proposed Solution:
This paper studies using knowledge distillation to mitigate backdoors in poisoned pre-trained encoders. The key idea is to distill benign knowledge from a poisoned encoder (teacher) into a clean encoder (student), filtering out malicious backdoor knowledge. They adapt distillation techniques from supervised learning and conduct an empirical study to analyze:

1) Effectiveness of distillation against backdoored encoders 
2) Impact of different components like teacher net, student net, distillation loss
3) Potential improvements via iterative training, epochs, data ratio
4) Robustness against trigger sizes and architectures

Main Contributions:

- First work studying distillation for defending backdoored pre-trained encoders
- Reduce attack success rate from 80.87% to 27.51% with 6.35% accuracy drop 
- Find fine-tuned teachers, warm-up students and attention loss work best
- More epochs and clean data improve performance, iterative training does not
- Robustness across trigger sizes and ResNet architectures

The results provide guiding principles for designing distillation defenses against backdoored encoders regarding component selection, performance improvements, robustness.
