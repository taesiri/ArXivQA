# [MEND: Meta dEmonstratioN Distillation for Efficient and Effective   In-Context Learning](https://arxiv.org/abs/2403.06914)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
Large language models (LLMs) have shown impressive capabilities for in-context learning (ICL), where the model makes predictions based on a few input-output demonstrations provided in the context. However, incorporating lengthy demonstrations leads to a quadratic increase in computation due to the self-attention mechanism in LLMs. Existing solutions attempt to distill demonstrations into compact vectors but often require task-specific retraining or compromise the LLM's ICL performance.

Proposed Solution: 
The paper proposes Meta Demonstration Distillation (MEND), where a language model learns to distill any lengthy demonstrations into vectors without needing retraining for new downstream tasks. MEND exploits knowledge distillation to enhance alignment between itself and the LLM to achieve both efficiency and effectiveness. It has a two-stage training process - meta-distillation pretraining on text corpora and finetuning on ICL tasks. This provides MEND the meta-knowledge to distill demonstrations and generalize to unseen ones.

Key Contributions:
1) Introduction of MEND to improve LLM's ICL efficiency without compromising effectiveness
2) Exploration of using knowledge distillation to align the demonstration distillation model with the LLM
3) Comprehensive quantitative and qualitative analysis showing MEND's prowess in matching or exceeding vanilla ICL's performance while requiring fewer computations across diverse LLMs and task partitions

In summary, the paper proposes an innovative distillation technique called MEND that can generate high-quality vector representations for lengthy demonstrations to enhance LLM's efficiency for ICL, without sacrificing performance. The key innovation is using knowledge distillation to equip the distillation model with meta-knowledge for demonstration compression and alignment with the LLM.
