# [Generalization to New Sequential Decision Making Tasks with In-Context   Learning](https://arxiv.org/abs/2312.03801)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem Statement
The paper focuses on the challenge of learning new sequential decision making tasks from only a handful of demonstrations. This capability is important for applications like robotics or virtual assistants where collecting large amounts of training data can be expensive or dangerous. Current methods based on meta-learning can adapt to new tasks but require more data and fine-tuning. In contrast, transformers trained on large datasets have shown the ability to perform in-context learning (ICL) - solving new tasks simply by conditioning on a few examples without any weight updates. However, it has not been studied if ICL works for sequential decision making problems which are more sensitive to errors since bad actions can lead to unrecoverable states.

Proposed Solution 
The paper first shows via an experiment that simply applying transformers (as in NLP) does not enable ICL for sequential decision tasks when trained on single trajectory sequences. They propose instead to train on sequences of multiple trajectories with certain distributional properties like trajectory burstiness. Their key insight is that in supervised learning, the context only needs a few examples, but in sequential decision making, it is crucial to include full trajectories covering a wide range of states. 

The authors construct a dataset adhering to these principles using expert demonstrations from MiniHack and Procgen. They train causal transformers on sequences of trajectories and evaluate on completely novel and unseen tasks from both benchmarks requiring generalization to new states, dynamics and rewards.

They also perform an extensive analysis on factors influencing ICL including model size, dataset size, task diversity, environment stochasticity and trajectory burstiness.

Main Results
1) The proposed approach allows ICL on new MiniHack and Procgen tasks from 1-7 demonstrations without any weight updates, significantly outperforming baselines.

2) Increasing model size, dataset diversity, environment stochasticity and trajectory burstiness improves ICL.

3) There are diminishing returns - performance plateaus after model size of 30M parameters and dataset size of 30K episodes.

4) The ability to ICL depends on the task difficulty and similarity between train/test tasks. Analysis of failure modes reveals different learning phenomena.

To the best of the authors' knowledge, this is the first work to demonstrate generalization to new procedurally generated MiniHack and Procgen tasks requiring handling new states, dynamics and rewards from only a handful of demonstrations without any parameter updates. Their extensive analysis also provides useful insights on factors influencing emergence of ICL for sequential decision making problems.
