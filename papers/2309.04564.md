# [When Less is More: Investigating Data Pruning for Pretraining LLMs at   Scale](https://arxiv.org/abs/2309.04564)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, it seems the central research question is:Can we remove the least impactful examples from a pretraining dataset and achieve similar or better performance?The authors investigate different methods for pruning a large pretraining dataset down to a higher quality subset in order to train better language models. They compare different metrics for estimating data quality/impact, like perplexity, EL2N, and memorization, and evaluate models trained on subsets of the data determined by these metrics. Their goal is to see if pruning the dataset (removing low quality/impact examples) can improve model performance compared to using the full dataset.So in summary, the main research question is whether pruning a pretraining dataset to keep only the most impactful/high quality examples can lead to better language model performance compared to using the entire dataset. The authors test different methods for identifying those impactful examples to see which works best.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contribution seems to be the rigorous benchmarking and analysis of different data pruning methods for pretraining large language models (LLMs). Specifically:- The paper provides an extensive comparison of perplexity, Error L2-Norm (EL2N), and memorization factor as methods for scoring and pruning pretraining data for LLMs. It tests these methods systematically across different model sizes, datasets, and amounts of data pruned.- The results show that the simple perplexity method consistently outperforms the more complex EL2N and memorization techniques. Surprisingly, training on just 30-50% of the data pruned by perplexity matches or improves performance compared to using the full dataset.- The analysis points to larger reference models and training on cleaner data (e.g. Wikipedia vs web crawl data) producing better perplexity rankings for pruning. It also finds that early training checkpoints can provide effective pruning signals.- Experiments demonstrate the robustness of perplexity-based pruning as model scale increases, with consistent improvements compared to random pruning from 124M to 1.5B parameters.- Downstream task evaluation on GLUE shows models pretrained on pruned data can achieve higher performance compared to no pruning, confirming the generalization of the approach.In summary, the key contribution is showing perplexity is an effective data pruning method for pretraining LLMs, outperforming more complex techniques, while also providing analysis into the factors that impact the quality of the data pruning. The work helps establish best practices for pruning large web crawled datasets to improve LLM performance.


## How does this paper compare to other research in the same field?

Based on my review, here are some ways this paper compares to other research in the field:- This paper focuses specifically on data pruning strategies for pretraining large language models (LLMs). Much prior work on data pruning has focused on computer vision or fine-tuning tasks, so this provides a novel investigation in the LLM pretraining setting.- The paper provides a broad and rigorous comparison of various pruning techniques, including simple methods like perplexity as well as more complex techniques like memorization. This level of analysis is quite comprehensive compared to prior works that often evaluate just one or two techniques. - The scale of the experiments, with models up to 1.5B parameters trained on billion-token datasets, is much larger than most prior data pruning research. This demonstrates the applicability of these techniques to modern large-scale LLM training.- The paper introduces a general framework for treating different data subsets based on the distribution of pruning scores. This formalism seems more systematic than heuristic subset selection in some past work.- The finding that a simple perplexity pruning method outperforms more complex techniques goes against the intuition from some past research that favored sophisticated metrics. This sheds new light on what signals may be most indicative of data quality.- The experiments on downstream task performance provide useful analysis of how pretraining data pruning impacts generalization capabilities. Many prior works focus only on metrics during pretraining.Overall, the scale, rigor, and novelty of the techniques evaluated seem to push forward the state of the art in data pruning research, especially for large language models. The paper provides one of the most thorough investigations of this area to date in the LLM pretraining literature.


## What future research directions do the authors suggest?

The authors suggest several future research directions in the conclusion section of the paper:1. Further investigation into the impact of pretraining dataset pruning on downstream task performance, especially for larger transformer models that can overfit more easily. They suggest evaluating models on a wider variety of tasks beyond GLUE.2. Exploring adaptive pruning methods that prune the dataset dynamically during pretraining, rather than statically before training begins. This could potentially yield higher quality subsets.3. Developing better theoretical understanding of what makes a "high quality" example for pretraining and what metrics best identify those examples. The authors note there is still limited theoretical grounding.4. Applying iterative dataset distillation approaches to pretraining data pruning. The authors suggest distilling knowledge from a model trained on the full dataset into a smaller pruned dataset may further enhance performance.5. Extending perplexity-based pruning to use several reference models rather than just one. The authors suggest this ensemble approach may improve results. 6. Evaluating additional data scoring metrics beyond the ones explored here. Identifying new signals correlated with data quality could lead to better pruning.In summary, the main directions are: further empirical analysis on more tasks, adaptive pruning methods, better theoretical grounding, dataset distillation, ensembling reference models, and exploring new scoring metrics. The authors lay out several promising avenues for improving data pruning techniques in future work.


## Summarize the paper in one paragraph.

Here is a one paragraph summary of the paper:The paper investigates data pruning methods for pretraining large language models (LLMs) at scale. The authors explore different methods for scoring the quality of examples in large web-scraped pretraining datasets, including perplexity, Error L2-Norm (EL2N), and memorization factor. These metrics are used to rank the training data and prune low-quality examples. The pruned datasets are used to pretrain LLMs with up to 1.5 billion parameters. Surprisingly, the authors find that the simple technique of ranking examples by perplexity outperforms the more sophisticated EL2N and memorization metrics. Models pretrained on just 30-50% of the original dataset, pruned by perplexity, achieve similar or better performance compared to baselines trained on the full dataset. The authors demonstrate the effectiveness and scalability of perplexity-based data pruning, showing it can remove the majority of web pretraining data while retaining model performance. Their work provides a framework for automatic data pruning and suggests focusing on higher quality subsets rather than simply bigger datasets.


## Summarize the paper in two paragraphs.

Here is a two paragraph summary of the paper:The paper investigates data pruning techniques for pretraining large language models (LLMs) at scale. The authors note that large-scale datasets used for pretraining, such as those scraped from the web, contain a substantial amount of noisy and low quality text. To improve data quality, the paper explores using perplexity, Error L2-Norm (EL2N), and memorization to automatically score and rank training examples. These metrics are used to prune the training set by removing low-scoring instances. The authors perform extensive experiments training 124M to 1.5B parameter Transformer models on various percentages of the original dataset after pruning based on these metrics. Surprisingly, they find that the simple technique of ranking examples by perplexity outperforms the more complex scoring methods of EL2N and memorization. In fact, models trained on only 50% of the dataset pruned by perplexity achieve over 1% better test perplexity compared to training on the full dataset. The benefits of perplexity-based pruning are shown to be consistent even when scaling up to 1.5B parameter models. The paper provides a rigorous framework for automatically pruning noisy pretraining data and suggests the majority of web scraped data may not be necessary for achieving strong LLM performance.


## Summarize the main method used in the paper in one paragraph.

Here is a one paragraph summary of the main method used in the paper:The paper explores different methods for pruning large pretraining datasets to improve the performance of large language models (LLMs). The main method is to score each training example using metrics like perplexity, Error L2-Norm (EL2N), and memorization factor computed using reference models. These scores are then used to rank the examples and prune the bottom, middle, or top percentile subsets of the dataset before pretraining the LLM. Several reference model variations are tested, including different sizes and checkpoints. The pruned subsets are used to train new LLMs, and their performance is evaluated on a test set and compared to baselines trained on the full dataset and random subsets. The simple perplexity metric computed using a large reference model performs the best, achieving improved performance while using as little as 30% of the original training data.


## What problem or question is the paper addressing?

Based on my reading of the paper, it seems the main problem or question being addressed is:How can we systematically prune down large pretraining datasets for large language models (LLMs) to a higher quality subset, in order to improve model performance? The paper notes that current pretraining datasets are very large and scraped from the internet, containing a lot of noisy or low quality data. However, existing methods for filtering this data rely mostly on hand-crafted heuristics and rules. The paper explores using scalable estimates of data quality, based on model outputs like perplexity, Error L2 Norm, and memorization, to automatically rank and prune pretraining datasets. It compares LLMs trained on these pruned datasets to evaluate the impact.Specifically, some key questions examined are:- Can removing the least impactful examples from a dataset lead to similar or improved performance compared to using the full dataset? - Do simple techniques for estimating data quality (like perplexity) outperform more complex approaches?- What aspects of training dynamics and model outputs best signal data quality for the purposes of pruning?So in summary, the main focus is developing and evaluating automatic methods of data pruning for pretraining datasets, in order to improve LLM performance compared to using the full raw datasets. The key questions center around what signals best indicate data quality for pruning and the impact on model training.
