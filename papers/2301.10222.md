# [RangeViT: Towards Vision Transformers for 3D Semantic Segmentation in   Autonomous Driving](https://arxiv.org/abs/2301.10222)

## What is the central research question or hypothesis that this paper addresses?

 The central research question this paper addresses is whether vision transformer (ViT) architectures, which have shown great success on image tasks, can be effectively adapted and leveraged to improve 3D semantic segmentation performance on LiDAR point clouds. 

The key hypotheses are:

1) The strong representation learning capacity of ViTs can be exploited for LiDAR semantic segmentation by adapting them with minimal changes to process projected range images.

2) ViT architectures can be unified between modalities (images & LiDAR) so advances in one domain benefit the other. 

3) ViTs pre-trained on large image datasets can be transferred to improve LiDAR segmentation, despite the domain gap.

In summary, the paper questions if the latest advances with ViTs in computer vision can be leveraged to push state-of-the-art in projection-based methods for LiDAR semantic segmentation. It hypothesizes this is possible by adaptating ViTs with a tailored tokenization process and decoder while exploiting their pre-training on images.


## What is the main contribution of this paper?

 This paper proposes RangeViT, a new method for 3D semantic segmentation of LiDAR point clouds based on vision transformers (ViTs). The key contributions are:

1. Adapting ViT architectures for processing sparse LiDAR point clouds by using range projection to create a 2D representation, and adding a convolutional stem and decoder around the ViT encoder.

2. Showing that ViTs pre-trained on large image datasets can be effectively transferred to the LiDAR segmentation task through fine-tuning, despite the domain gap between images and range projections. 

3. Achieving state-of-the-art results among projection-based methods on the nuScenes and SemanticKITTI datasets. 

4. Providing a simple but effective way to leverage ViTs for point cloud segmentation, that could benefit from future advances in ViT models and image pre-training strategies.

5. Demonstrating the feasibility of using a common backbone architecture like ViT across modalities (images and point clouds), enabling transfer learning between the two.

In summary, the main contribution is successfully adapting ViT models to point cloud segmentation through innovative use of range projection, a tailored convolutional stem/decoder, and image pre-training, outperforming prior projection-based methods. The paper shows ViTs are a promising backbone for point cloud perception tasks.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the main contributions of this paper:

The paper proposes RangeViT, a new approach for LiDAR point cloud semantic segmentation that adapts vision transformer (ViT) architectures to work on sparse 3D point clouds represented as 2D range images, and shows this approach can effectively leverage image-pretrained ViTs and achieve state-of-the-art results among projection-based methods.


## How does this paper compare to other research in the same field?

 Here are a few key points comparing this paper to other research in LiDAR point cloud semantic segmentation:

- The main novelty is using a vision transformer (ViT) architecture for LiDAR segmentation in a projection-based method. This is the first work to explore ViTs for outdoor autonomous driving datasets like nuScenes and SemanticKITTI. Most prior work on ViTs for point clouds focused on indoor datasets. 

- The method exploits ViTs pre-trained on large image datasets like ImageNet and shows they can transfer well to the LiDAR domain despite the gap between images and range projections. This demonstrates the generalization capacity of ViTs and is an interesting finding.

- To adapt ViTs, the paper proposes several modifications like a convolutional stem and decoder to add some inductive biases suited for the LiDAR data. Many other ViT works also modify or extend ViT to better handle different data modalities.

- The RangeViT model achieves state-of-the-art results compared to other projection-based methods on nuScenes and SemanticKITTI. However, some voxel-based methods like Cylinder3D still outperform it. 

- The model is overall simple and does not require complex point cloud preprocessing or augmentation. Many other point cloud segmentation models are more complex or require expensive augmentations.

- The computational cost and number of parameters of RangeViT is reasonable compared to other LiDAR segmentation models, though not the absolute lowest.

In summary, the key novelty is successfully applying ViTs to outdoor LiDAR segmentation through an adapted projection-based approach. The results are very promising and demonstrate ViTs should be further explored for point cloud understanding tasks.
