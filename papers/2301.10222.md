# [RangeViT: Towards Vision Transformers for 3D Semantic Segmentation in   Autonomous Driving](https://arxiv.org/abs/2301.10222)

## What is the central research question or hypothesis that this paper addresses?

 The central research question this paper addresses is whether vision transformer (ViT) architectures, which have shown great success on image tasks, can be effectively adapted and leveraged to improve 3D semantic segmentation performance on LiDAR point clouds. 

The key hypotheses are:

1) The strong representation learning capacity of ViTs can be exploited for LiDAR semantic segmentation by adapting them with minimal changes to process projected range images.

2) ViT architectures can be unified between modalities (images & LiDAR) so advances in one domain benefit the other. 

3) ViTs pre-trained on large image datasets can be transferred to improve LiDAR segmentation, despite the domain gap.

In summary, the paper questions if the latest advances with ViTs in computer vision can be leveraged to push state-of-the-art in projection-based methods for LiDAR semantic segmentation. It hypothesizes this is possible by adaptating ViTs with a tailored tokenization process and decoder while exploiting their pre-training on images.


## What is the main contribution of this paper?

 This paper proposes RangeViT, a new method for 3D semantic segmentation of LiDAR point clouds based on vision transformers (ViTs). The key contributions are:

1. Adapting ViT architectures for processing sparse LiDAR point clouds by using range projection to create a 2D representation, and adding a convolutional stem and decoder around the ViT encoder.

2. Showing that ViTs pre-trained on large image datasets can be effectively transferred to the LiDAR segmentation task through fine-tuning, despite the domain gap between images and range projections. 

3. Achieving state-of-the-art results among projection-based methods on the nuScenes and SemanticKITTI datasets. 

4. Providing a simple but effective way to leverage ViTs for point cloud segmentation, that could benefit from future advances in ViT models and image pre-training strategies.

5. Demonstrating the feasibility of using a common backbone architecture like ViT across modalities (images and point clouds), enabling transfer learning between the two.

In summary, the main contribution is successfully adapting ViT models to point cloud segmentation through innovative use of range projection, a tailored convolutional stem/decoder, and image pre-training, outperforming prior projection-based methods. The paper shows ViTs are a promising backbone for point cloud perception tasks.
