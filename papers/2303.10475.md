# Is Prompt All You Need? No. A Comprehensive and Broader View of   Instruction Learning

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research questions and hypotheses seem to be:- What is task instruction, and what instruction types exist? The paper categorizes task instructions into three types: entailment-oriented, PLM-oriented, and human-oriented. It provides examples and comparisons between these instruction types.- How to model instructions?The paper discusses three main strategies for modeling instructions: semantic parser-based, tuning-based, and hypernetwork-based. It explains how each strategy encodes and utilizes the instructions.- What factors (e.g. model size, task numbers) impact the instruction-driven systems' performance, and how to design better instructions?The paper analyzes several factors that influence the performance of instruction learning, including instruction tuning, consistency, model/task scale, diversity, taxonomies, and conforming to models' preferences. It provides suggestions for designing effective instructions.- What applications can instruction learning contribute?The paper highlights applications in human-computer interaction, data/feature augmentation, and building generalist language models. It argues instruction learning can assist in these areas.- What challenges exist in instruction learning and what are future directions?The paper discusses several challenges like handling negated instructions, improving explainability, moving to more explicit objectives beyond tuning, and developing better evaluation paradigms. It proposes some potential solutions.In summary, the central hypothesis seems to be that instruction learning is a promising paradigm for rapidly adapting language models to new tasks in a low-resource setting. The paper aims to provide a comprehensive overview of this emerging field and directions for future work.


## What is the main contribution of this paper?

This paper provides a comprehensive survey on textual instruction learning for natural language processing (NLP) tasks. The main contributions are:1. It summarizes and categorizes different types of textual instructions used in NLP, including entailment-oriented instructions, PLM-oriented instructions (e.g. prompts), and human-oriented instructions. 2. It reviews different modeling strategies for encoding instructions, including semantic parser-based, tuning-based, and hypernetwork-based methods.3. It discusses important factors that impact the performance of instruction learning, such as instruction tuning, instruction consistency, model and task scale, instruction diversity, etc. 4. It highlights applications of instruction learning in areas like human-computer interaction, data augmentation, and building generalist language models.5. It outlines key challenges and future directions, including negated instruction learning, explainable instruction learning, explicit instruction learning objectives, and scalable oversight evaluation paradigms.In summary, this is a comprehensive survey that organizes and connects different research areas related to textual instruction learning. It provides useful taxonomies, analyses, and insights that can inform and guide future research on this important topic. The discussion of challenges and future trends is particularly valuable for the community.
