# [Scale Mixtures of Neural Network Gaussian Processes](https://arxiv.org/abs/2107.01408)

## What is the central research question or hypothesis that this paper addresses?

This paper addresses the question of how to extend neural network Gaussian processes (NNGPs) to model more expressive stochastic processes beyond Gaussian processes. The key hypothesis is that introducing a prior distribution on the scale parameter of the last layer weights in an infinitely wide neural network can turn the network into a richer class of stochastic processes with useful properties like heavy tails. Specifically, the main contributions and results are:- Proposing the scale mixtures of NNGPS, which are NNGPS extended with a prior distribution on the last layer weight scales. This leads to a flexible class of stochastic processes.- Showing that with certain priors like inverse gamma, the scale mixtures become heavy-tailed processes like Student's t processes.- Analyzing the limiting distribution of wide neural networks initialized under the proposed scheme and trained with gradient descent. The results are similar to standard NNGPS.- Presenting an efficient posterior inference algorithm for the proposed scale mixtures of NNGPS.- Empirically demonstrating on real-world data that the heavy-tailed processes from the proposed framework are more robust to outliers compared to regular NNGPS.So in summary, the central hypothesis is that a simple modification to NNGPS by adding a scale prior leads to more flexible stochastic process models with useful robustness properties. Theoretical analysis and experiments validate this hypothesis.
