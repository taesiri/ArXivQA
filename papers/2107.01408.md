# [Scale Mixtures of Neural Network Gaussian Processes](https://arxiv.org/abs/2107.01408)

## What is the central research question or hypothesis that this paper addresses?

This paper addresses the question of how to extend neural network Gaussian processes (NNGPs) to model more expressive stochastic processes beyond Gaussian processes. The key hypothesis is that introducing a prior distribution on the scale parameter of the last layer weights in an infinitely wide neural network can turn the network into a richer class of stochastic processes with useful properties like heavy tails. Specifically, the main contributions and results are:- Proposing the scale mixtures of NNGPS, which are NNGPS extended with a prior distribution on the last layer weight scales. This leads to a flexible class of stochastic processes.- Showing that with certain priors like inverse gamma, the scale mixtures become heavy-tailed processes like Student's t processes.- Analyzing the limiting distribution of wide neural networks initialized under the proposed scheme and trained with gradient descent. The results are similar to standard NNGPS.- Presenting an efficient posterior inference algorithm for the proposed scale mixtures of NNGPS.- Empirically demonstrating on real-world data that the heavy-tailed processes from the proposed framework are more robust to outliers compared to regular NNGPS.So in summary, the central hypothesis is that a simple modification to NNGPS by adding a scale prior leads to more flexible stochastic process models with useful robustness properties. Theoretical analysis and experiments validate this hypothesis.


## What is the main contribution of this paper?

Here is a summary of the main contributions of the paper "Scale Mixtures of Neural Network Gaussian Processes":- The paper proposes a simple extension of Neural Network Gaussian Processes (NNGPs) by introducing a prior distribution on the scale parameter of the last layer weights. This results in a class of stochastic processes called "scale mixtures of NNGPs". - By putting a prior on the scale, the paper is able to obtain heavy-tailed stochastic processes from wide neural networks, including Student's t processes when using an inverse gamma prior. This increases flexibility compared to regular NNGPs.- The paper shows that introducing the scale prior only on the last layer weights is sufficient to get these more flexible stochastic processes corresponding to infinitely wide neural networks. This result relies on the master theorem for tensor programs representing neural network computations.- For the inverse gamma prior leading to Student's t processes, the paper demonstrates that kernels and exact posterior inference are still tractable, similar to regular NNGPs. For generic scale priors, an efficient approximate inference method based on importance sampling is presented.- The paper analyzes the distribution of neural networks initialized with the proposed scale prior and trained with gradient descent, extending existing results on the training dynamics of infinitely wide networks.- Empirically, the scale mixtures of NNGPs are shown to be more robust compared to NNGPs for out-of-distribution data, while maintaining similar performance on normal data.In summary, the main contribution is proposing the scale mixtures of NNGPs as a simple yet flexible extension of NNGPs that results in useful heavy-tailed stochastic processes corresponding to wide neural networks. Both theoretical properties and empirical benefits are demonstrated.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper proposes a simple extension of Neural Network Gaussian Processes (NNGPs) by introducing a scale prior on the last layer weight parameters, turning them into a more flexible class of stochastic processes including heavy-tailed ones like Student's t processes, while still allowing efficient posterior inference.In slightly more detail:- The paper introduces "scale mixtures of NNGPS" where a prior distribution is placed on the scale/variance of the last layer weights. - This allows constructing heavy-tailed processes like Student's t processes, increasing flexibility over regular NNGPs.- It is shown this often doesn't increase inference difficulty much. Closed-form posteriors are still available in some cases like the inverse gamma prior.- Efficient approximate posterior inference methods are presented for more general priors.- Empirical results demonstrate the benefits of the increased flexibility, like better uncertainty quantification and robustness to outliers.So in summary, it's a simple way to make NNGPS more flexible while retaining many of their appealing properties. The scale mixtures lead to heavy-tailed processes that can be more robust.


## How does this paper compare to other research in the same field?

Here are a few thoughts on how this paper compares to other research in the field:- This paper builds on the growing body of work showing connections between deep neural networks and Gaussian processes in the infinite width limit. It extends previous results by introducing scale mixtures of NNGPs to obtain more flexible stochastic process models like Student's t processes.- Other related works have tried to go beyond NNGPs to obtain heavy-tailed stochastic processes using different methods. For example, Favaro et al. (2020) showed wide fully-connected networks converge to stable processes under an alternative prior. This paper's approach of mixing the scale seems simpler as it retains more of the original NNGP framework.- A key contribution is providing theoretical results characterizing the distribution of neural nets initialized and trained according to the proposed scale mixture framework. The authors show results analogous to existing findings on how NNGPs relate to gradient descent training.- For inference, the paper presents approximations using importance sampling and stochastic variational methods. This helps maintain computational efficiency compared to plain NNGPs. Other works have proposed different inference methods for related models.- The empirical evaluation shows the potential of the proposed scale mixtures on regression and classification tasks. Performance is generally comparable to NNGPs, but with improved robustness and calibration on out-of-distribution data. This demonstrates the usefulness beyond just being a theoretical extension.Overall, the paper makes a nice contribution in extending the flexibility of processes derived from wide neural nets. It builds cleanly on the NNGP literature while proposing a simpler way to obtain heavy-tailed behavior. The theory and inference methods enable practical application. The scale mixture perspective could potentially inspire other directions for modifying NNGPs.


## What future research directions do the authors suggest?

The paper suggests several potential future research directions in the conclusion:1. Improving the posterior inference algorithm for non-Gaussian likelihoods. The importance sampling method presented works well for Gaussian likelihoods, but struggles with categorical likelihoods. Developing more efficient inference algorithms for broader likelihoods would be useful.2. Evaluating the model on larger datasets and with deeper neural networks. The experiments in the paper were restricted to smaller datasets due to computational limitations. Testing the methods on larger datasets and more complex neural network architectures could provide more insight.3. Further analyzing the theoretical properties of the scale mixtures of NNGPs. While some theoretical results were presented, further characterizing the properties of these models could lead to a deeper understanding. 4. Exploring other mechanisms for introducing heavy tails beyond a prior on the last layer. The paper introduced heavy tails by putting a prior on the scale of the last layer, but there may be alternative ways to achieve a similar effect.5. Applying the framework to other domains beyond regression and classification. The methods were demonstrated on standard supervised learning tasks, but could potentially be useful in other problem settings as well.6. Comparing to other approaches for achieving robustness. The scale mixtures were shown to be robust to outliers, but could be compared to other techniques like adversarial training. In summary, the main directions are: improving inference, scaling to larger problems, further theoretical analysis, exploring architectural variants, applying to new domains, and comparative analysis. Overall, the authors lay out several interesting avenues for future work based on their proposed scale mixtures framework.


## Summarize the paper in one paragraph.

Here is a one paragraph summary of the paper "Scale Mixtures of Neural Network Gaussian Processes":The paper proposes a simple extension of neural network Gaussian processes (NNGPs) by introducing a prior distribution on the scale parameter of the weights in the last layer of an infinitely wide neural network. This results in a more flexible class of stochastic processes called scale mixtures of NNGPS, which includes heavy-tailed processes like Student's t processes. The authors show that introducing a scale prior only on the last layer weights is sufficient to get these more expressive processes, and with an inverse gamma prior the limiting process is Student's t. Similar results are derived relating the training dynamics of wide networks initialized under their construction to scale mixtures of processes related to NNGPs and NTKs. Efficient posterior inference techniques are presented, and experiments demonstrate the robustness of the heavy-tailed processes to out-of-distribution data in regression and classification tasks. Overall, the paper provides a simple way to obtain more flexible processes from wide neural nets while retaining tractable inference.


## Summarize the paper in two paragraphs.

Here is a two paragraph summary of the paper "Scale Mixtures of Neural Network Gaussian Processes":This paper proposes a new class of stochastic processes called scale mixtures of Neural Network Gaussian Processes (NNGPs). The key idea is to introduce a prior distribution over the scale parameter of the last layer weights in an infinitely wide neural network. This turns the limiting prior over functions into a scale mixture of Gaussians rather than just a Gaussian process. With certain priors like the inverse gamma distribution, this results in heavy-tailed processes like the Student's t process which are more robust. The paper shows that this simple modification results in a rich new class of processes while retaining many nice properties of NNGPS. In particular, for the Student's t case, kernels and posteriors can be computed in closed form. For more general priors, efficient approximate inference algorithms are presented. Experiments demonstrate that the heavy-tailed processes are indeed more robust to outliers while maintaining similar performance on clean data. Theoretical results are provided linking these processes to the training dynamics of wide neural networks. Overall, the scale mixtures of NNGPS provide a principled way to obtain more flexible processes while building on the NNGP framework.


## Summarize the main method used in the paper in one paragraph.

Here is a one paragraph summary of the main method used in the paper "Scale Mixtures of Neural Network Gaussian Processes":The paper proposes extending neural network Gaussian processes (NNGPs) by introducing a prior distribution on the scale parameter of the weights in the last layer of a neural network. This results in a more flexible class of stochastic processes called scale mixtures of NNGPs. In particular, placing an inverse gamma prior on the scale leads to Student's t processes, which are heavy-tailed and more robust. Theoretically, the paper shows that infinitely wide neural networks constructed this way correspond to scale mixtures of NNGPs. Also, when trained with gradient descent, these networks converge to scale mixtures of Gaussian processes. For inference, exact posterior computation is possible with inverse gamma priors, leading to Student's t processes. For generic priors, the paper presents an efficient importance sampling method. Experiments on regression and classification tasks demonstrate the benefits of the proposed scale mixtures of NNGPs compared to standard NNGPs.In summary, the main method is introducing scale priors on the last layer of wide neural networks, which results in more flexible stochastic processes that can be heavy-tailed while still allowing efficient inference. This is shown to improve robustness empirically while maintaining comparable performance on standard tasks.


## What problem or question is the paper addressing?

The paper "Scale Mixtures of Neural Network Gaussian Processes" addresses the problem of extending the class of stochastic processes that infinitely-wide neural networks correspond to beyond just Gaussian processes (GPs). Recent work has shown the equivalence between infinitely-wide neural networks and GPs. However, this restricts the stochastic processes corresponding to neural networks to only GPs, which have some limitations. The key contribution of this paper is proposing a simple way to extend neural network GP equivalences to richer classes of stochastic processes, including heavy-tailed ones like Student's t processes. This is done by introducing a prior distribution on the scale parameter of the last layer of a neural network.Some key aspects addressed in the paper:- Showing that a scale mixture of neural network GPs results from putting a prior on last layer scale. With inverse gamma prior, this yields a Student's t process.- Analyzing gradient descent training dynamics and characterizing the distribution of trained wide networks. Similar results as standard neural network GP work.- Developing efficient posterior inference for the proposed scale mixtures of neural network GPs.- Empirical evaluation showing the robustness of heavy-tailed processes from the proposed method, especially on out-of-distribution data.In summary, the paper aims to enrich the class of stochastic processes linked to infinitely-wide neural networks using ideas of scale mixtures. This results in more flexible processes while retaining many nice properties.
