# [Scale Mixtures of Neural Network Gaussian Processes](https://arxiv.org/abs/2107.01408)

## What is the central research question or hypothesis that this paper addresses?

This paper addresses the question of how to extend neural network Gaussian processes (NNGPs) to model more expressive stochastic processes beyond Gaussian processes. The key hypothesis is that introducing a prior distribution on the scale parameter of the last layer weights in an infinitely wide neural network can turn the network into a richer class of stochastic processes with useful properties like heavy tails. Specifically, the main contributions and results are:- Proposing the scale mixtures of NNGPS, which are NNGPS extended with a prior distribution on the last layer weight scales. This leads to a flexible class of stochastic processes.- Showing that with certain priors like inverse gamma, the scale mixtures become heavy-tailed processes like Student's t processes.- Analyzing the limiting distribution of wide neural networks initialized under the proposed scheme and trained with gradient descent. The results are similar to standard NNGPS.- Presenting an efficient posterior inference algorithm for the proposed scale mixtures of NNGPS.- Empirically demonstrating on real-world data that the heavy-tailed processes from the proposed framework are more robust to outliers compared to regular NNGPS.So in summary, the central hypothesis is that a simple modification to NNGPS by adding a scale prior leads to more flexible stochastic process models with useful robustness properties. Theoretical analysis and experiments validate this hypothesis.


## What is the main contribution of this paper?

Here is a summary of the main contributions of the paper "Scale Mixtures of Neural Network Gaussian Processes":- The paper proposes a simple extension of Neural Network Gaussian Processes (NNGPs) by introducing a prior distribution on the scale parameter of the last layer weights. This results in a class of stochastic processes called "scale mixtures of NNGPs". - By putting a prior on the scale, the paper is able to obtain heavy-tailed stochastic processes from wide neural networks, including Student's t processes when using an inverse gamma prior. This increases flexibility compared to regular NNGPs.- The paper shows that introducing the scale prior only on the last layer weights is sufficient to get these more flexible stochastic processes corresponding to infinitely wide neural networks. This result relies on the master theorem for tensor programs representing neural network computations.- For the inverse gamma prior leading to Student's t processes, the paper demonstrates that kernels and exact posterior inference are still tractable, similar to regular NNGPs. For generic scale priors, an efficient approximate inference method based on importance sampling is presented.- The paper analyzes the distribution of neural networks initialized with the proposed scale prior and trained with gradient descent, extending existing results on the training dynamics of infinitely wide networks.- Empirically, the scale mixtures of NNGPs are shown to be more robust compared to NNGPs for out-of-distribution data, while maintaining similar performance on normal data.In summary, the main contribution is proposing the scale mixtures of NNGPs as a simple yet flexible extension of NNGPs that results in useful heavy-tailed stochastic processes corresponding to wide neural networks. Both theoretical properties and empirical benefits are demonstrated.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper proposes a simple extension of Neural Network Gaussian Processes (NNGPs) by introducing a scale prior on the last layer weight parameters, turning them into a more flexible class of stochastic processes including heavy-tailed ones like Student's t processes, while still allowing efficient posterior inference.In slightly more detail:- The paper introduces "scale mixtures of NNGPS" where a prior distribution is placed on the scale/variance of the last layer weights. - This allows constructing heavy-tailed processes like Student's t processes, increasing flexibility over regular NNGPs.- It is shown this often doesn't increase inference difficulty much. Closed-form posteriors are still available in some cases like the inverse gamma prior.- Efficient approximate posterior inference methods are presented for more general priors.- Empirical results demonstrate the benefits of the increased flexibility, like better uncertainty quantification and robustness to outliers.So in summary, it's a simple way to make NNGPS more flexible while retaining many of their appealing properties. The scale mixtures lead to heavy-tailed processes that can be more robust.


## How does this paper compare to other research in the same field?

Here are a few thoughts on how this paper compares to other research in the field:- This paper builds on the growing body of work showing connections between deep neural networks and Gaussian processes in the infinite width limit. It extends previous results by introducing scale mixtures of NNGPs to obtain more flexible stochastic process models like Student's t processes.- Other related works have tried to go beyond NNGPs to obtain heavy-tailed stochastic processes using different methods. For example, Favaro et al. (2020) showed wide fully-connected networks converge to stable processes under an alternative prior. This paper's approach of mixing the scale seems simpler as it retains more of the original NNGP framework.- A key contribution is providing theoretical results characterizing the distribution of neural nets initialized and trained according to the proposed scale mixture framework. The authors show results analogous to existing findings on how NNGPs relate to gradient descent training.- For inference, the paper presents approximations using importance sampling and stochastic variational methods. This helps maintain computational efficiency compared to plain NNGPs. Other works have proposed different inference methods for related models.- The empirical evaluation shows the potential of the proposed scale mixtures on regression and classification tasks. Performance is generally comparable to NNGPs, but with improved robustness and calibration on out-of-distribution data. This demonstrates the usefulness beyond just being a theoretical extension.Overall, the paper makes a nice contribution in extending the flexibility of processes derived from wide neural nets. It builds cleanly on the NNGP literature while proposing a simpler way to obtain heavy-tailed behavior. The theory and inference methods enable practical application. The scale mixture perspective could potentially inspire other directions for modifying NNGPs.


## What future research directions do the authors suggest?

The paper suggests several potential future research directions in the conclusion:1. Improving the posterior inference algorithm for non-Gaussian likelihoods. The importance sampling method presented works well for Gaussian likelihoods, but struggles with categorical likelihoods. Developing more efficient inference algorithms for broader likelihoods would be useful.2. Evaluating the model on larger datasets and with deeper neural networks. The experiments in the paper were restricted to smaller datasets due to computational limitations. Testing the methods on larger datasets and more complex neural network architectures could provide more insight.3. Further analyzing the theoretical properties of the scale mixtures of NNGPs. While some theoretical results were presented, further characterizing the properties of these models could lead to a deeper understanding. 4. Exploring other mechanisms for introducing heavy tails beyond a prior on the last layer. The paper introduced heavy tails by putting a prior on the scale of the last layer, but there may be alternative ways to achieve a similar effect.5. Applying the framework to other domains beyond regression and classification. The methods were demonstrated on standard supervised learning tasks, but could potentially be useful in other problem settings as well.6. Comparing to other approaches for achieving robustness. The scale mixtures were shown to be robust to outliers, but could be compared to other techniques like adversarial training. In summary, the main directions are: improving inference, scaling to larger problems, further theoretical analysis, exploring architectural variants, applying to new domains, and comparative analysis. Overall, the authors lay out several interesting avenues for future work based on their proposed scale mixtures framework.


## Summarize the paper in one paragraph.

Here is a one paragraph summary of the paper "Scale Mixtures of Neural Network Gaussian Processes":The paper proposes a simple extension of neural network Gaussian processes (NNGPs) by introducing a prior distribution on the scale parameter of the weights in the last layer of an infinitely wide neural network. This results in a more flexible class of stochastic processes called scale mixtures of NNGPS, which includes heavy-tailed processes like Student's t processes. The authors show that introducing a scale prior only on the last layer weights is sufficient to get these more expressive processes, and with an inverse gamma prior the limiting process is Student's t. Similar results are derived relating the training dynamics of wide networks initialized under their construction to scale mixtures of processes related to NNGPs and NTKs. Efficient posterior inference techniques are presented, and experiments demonstrate the robustness of the heavy-tailed processes to out-of-distribution data in regression and classification tasks. Overall, the paper provides a simple way to obtain more flexible processes from wide neural nets while retaining tractable inference.
