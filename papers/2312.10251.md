# [Advancing Surgical VQA with Scene Graph Knowledge](https://arxiv.org/abs/2312.10251)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Surgical visual question answering (VQA) is an emerging field to provide natural language interaction for surgical data science systems. However, current surgical VQA datasets and models have limitations:
    - Datasets have question-condition bias and lack complex, geometrically grounded questions covering detailed surgical scene understanding.  
    - Models lack explicit scene-aware reasoning, instead using global image features. This limits performance on fine-grained questions needing localized understanding.

Proposed Solution:
- Introduce a new surgical scene graph-based VQA dataset called SSG-QA:
    - Uses segmentation and detection on public datasets to build surgical scene graphs capturing objects, attributes, and spatial/action relationships. 
    - Employs a surgery-specific question engine with templates to generate diverse, unbiased QA pairs of varying complexity requiring different reasoning steps.
- Propose a multi-modality VQA model called SSG-QA Net:
    - Extracts object-level visual features using a trained detector and ROI pooling.
    - Introduces a Scene-embedded Interaction Module (SIM) which models text-aware scene graph embeddings using cross-attention and refines them via self-attention before fusing with other features.

Main Contributions:
- SSG-QA dataset provides complex, geometrically grounded QA pairs covering detailed surgical scene understanding without question-condition bias.
- SSG-QA Net model explicitly incorporates surgical scene knowledge and achieves state-of-the-art surgical VQA performance, especially on fine-grained questions needing localized understanding.
