# [LookupFFN: Making Transformers Compute-lite for CPU inference](https://arxiv.org/abs/2403.07221)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Feed-forward networks (FFNs) are compute-intensive modules used extensively in deep neural networks. They rely heavily on GEMM operations which are extremely FLOP-intensive. 
- Reducing FLOP count of FFNs to make them more efficient, especially for CPU inference, is an important area of research.

Prior Work:
- Methods like Slide and Mongoose use locality-sensitive hashing (LSH) to sparsify FFNs and reduce FLOPs. However, they have issues like the need for frequent rehashing during training, skewed hash bucket distributions, and difficulty of adoption.

Proposed Solution: LookupFFN
- Proposes a formulation to replace GEMM in FFNs with memory lookups and accumulations.
- Learns hash functions and hash tables end-to-end via backpropagation to avoid rehashing. 
- Uses an efficient Hadamard transform based projection method (BH4) for generating hash codes.
- Decouples hash table computation from hash functions to eliminate issues due to skewed distributions.

Main Contributions:
- Achieves 6x FLOP reduction compared to vanilla FFN in RoBERTa model with similar accuracy.
- Completely eliminates need for rehashing during training.
- Makes workload independent of hash bucket sizes for easier parallelization.
- Reduces energy consumption by making majority of memory accesses hit cache instead of DRAM.
- Achieves 2.5x speedup compared to vanilla FFN on CPUs with further room for optimization.
- Framework easily integrates into existing deep learning models and workflows.

In summary, the paper proposes an efficient and adoptable method to replace computationally heavy GEMM operation in FFNs with memory lookups while preserving accuracy. This makes FFNs extremely lite in terms of FLOPs leading to faster inference on CPUs.


## Summarize the paper in one sentence.

 Here is a one-sentence summary of the paper:

The paper proposes LookupFFN, a reformulation of feed-forward networks in Transformers to use memory lookup operations instead of matrix multiplication to dramatically reduce compute requirements while maintaining accuracy, enabling efficient CPU inference.


## What is the main contribution of this paper?

 The main contribution of this paper is proposing LookupFFN, a novel formulation of feed-forward networks (FFNs) that replaces most computation with memory lookups. Specifically:

- It introduces a differentiable formulation to directly learn the hash functions and hash tables used for lookup, eliminating the need for rehashing during training. This allows end-to-end training via backpropagation.

- It proposes using a specialized projection called BH4 that is efficient yet expressive for computing the hash codes. 

- Experiments show LookupFFN can achieve 6x or higher FLOP reduction compared to standard FFNs, with similar accuracy on tasks like RoBERTa pretraining.

- Analysis indicates LookupFFN is more efficient on CPUs compared to standard FFNs, achieving 2.5x lower latency, due to better utilizing high memory bandwidth of CPUs through the lookup formulation.

In summary, the paper proposes a way to make feed-forward networks extremely compute-lite by reformulating them based primarily on memory lookups rather than FLOPs. This makes them more efficient on CPUs compared to standard FFNs widely used in models like Transformers.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, here are some of the key terms and concepts:

- LookupFFN - The proposed feedforward network formulation that replaces matrix multiplications with memory lookups to reduce compute intensity.

- FLOP-lite - The goal of making neural network modules less compute-intensive and more memory access friendly.

- Locality sensitive hashing (LSH) - A technique used in prior works like Slide and Mongoose to sparsify feedforward networks. The paper builds on these ideas.

- Differentiable lookup - Transforming the hash functions and lookups to be differentiable so the whole module can be trained with backpropagation. 

- BH4 - The efficient Hadamard transform based projection method proposed to reduce complexity of generating hash codes.

- Rehashing overhead - The need to periodically update hash functions in prior LSH-based methods, which this work eliminates.

- Cache utilization - Analyzing cache hit rates and memory bandwidth saturation to understand runtime performance.

- FLOP reduction - The paper aims to dramatically reduce floating point operations while maintaining accuracy.

- Transformer models - The LookupFFN is analyzed in the context of Transformer language models like RoBERTa.

- CPU inference - The goal of enabling more efficient inference on CPU-based servers without reliance on GPUs.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 potential in-depth questions about the method proposed in the paper:

1. The paper proposes a new formulation called LookupFFN to replace the standard GEMM-based feedforward network (FFN) in Transformers. What is the key motivation behind proposing this new formulation? What limitations of prior work on sparse FFNs does it aim to address?

2. How does LookupFFN reformulate the FFN to avoid the issues of rehashing and skewed bucket distribution that exist in prior hash-based approximations of FFN? Explain the main ideas behind the proposed formulation.  

3. The paper claims LookupFFN is end-to-end differentiable. Walk through the mathematical derivation and explain how the authors made the overall formulation differentiable. What is the role of the structured matrix S used here?

4. Explain the efficient softmax approximation used in LookupFFN to avoid the exponential runtime complexity. How does the structure of matrix S enable an easy non-uniform sampling scheme here?

5. The projection step in LookupFFN uses a new efficient projection called BH4. What is BH4 and how is it different from prior work like ACDC? Why is it more suitable for LookupFFN?

6. The paper evaluates LookupFFN on RoBERTa pre-training. Walk through the key results and discuss the tradeoffs observed between model accuracy and FLOPs. How does LookupFFN compare to baseline methods?

7. Explain the hardware-level profiling conducted for LookupFFN in the paper. What are the key observations regarding latency, throughput and potential future improvements?

8. The hardware-managed caches in CPUs seem well-suited for LookupFFN. Why is that the case? How can optimized software help further improve the cache efficiency?  

9. How suitable do you think LookupFFN is for inference on mobile or edge devices compared to server-based inference? Explain the pros and cons.

10. The paper claims LookupFFN relies more on memory access rather than compute. Do you foresee any scalability issues for large-scale models having billions of parameters in the long run?
