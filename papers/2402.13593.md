# [Knowledge Graph Enhanced Large Language Model Editing](https://arxiv.org/abs/2402.13593)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
Large language models (LLMs) have shown impressive capabilities in various natural language processing tasks. However, the knowledge embedded in LLMs can be factually incorrect or outdated, which limits their performance. Existing model editing methods that aim to update the knowledge in LLMs mainly focus on editing target knowledge pieces, but fail to capture the changes in associated knowledge that are also impacted by the edits. This limits the generalization ability of post-edit LLMs in reasoning with edited knowledge. 

Proposed Solution: 
The paper proposes a novel model editing framework called GLAME that leverages knowledge graphs to enhance LLM editing. It has two main components:

1) Knowledge Graph Augmentation (KGA) Module: Constructs a subgraph centered around the edited entity to uncover knowledge associations altered due to the edit. It initializes subgraph nodes with hidden vectors from the LLM.

2) Graph-based Knowledge Edit (GKE) Module: Employs a relational graph neural network to incorporate the subgraph encoding into the rank-one editing framework to update the parameters. This allows changes in associated knowledge to be reflected in the edited parameters.

Main Contributions:
- Emphasizes the importance of capturing changes in associated knowledge when editing LLMs.
- Proposes a graph-enhanced model editing method that structures knowledge changes and incorporates them into LLM parameters. 
- Comprehensive experiments on GPT-J and GPT-2 XL show superior performance over state-of-the-art methods in utilizing edited knowledge.

In summary, the key innovation is the integration of knowledge graphs to uncover and encode updates to associated knowledge for more effective LLM editing. Experiments demonstrate enhanced generalization capabilities of post-edit LLMs in employing edited knowledge.
