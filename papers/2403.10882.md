# [Optimizing Language Augmentation for Multilingual Large Language Models:   A Case Study on Korean](https://arxiv.org/abs/2403.10882)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Large language models (LLMs) require massive compute resources for training, making it difficult for smaller research groups to utilize them. 
- Multilingual LLMs (MLLMs) often overlook less-resourced languages (LRLs) like Korean during training. As a result, they have limited vocabulary and knowledge for LRLs.

Proposed Solution:
- Enhance Korean language capabilities of the Llama2 MLLM using 3 strategies:
  1) Expand Korean vocabulary by merging with KoBERT dictionary
  2) Enrich knowledge by pretraining on Korean-English Wikipedia data
  3) Improve usability by constructing high-quality Korean LIMA dataset and performing instruction tuning

Main Contributions:
- Proposed methods to expand vocabulary, reinforce knowledge, and enhance usability of LRLs in MLLMs
- Constructed Korean version of LIMA dataset by translating and modifying English LIMA data to fit Korean cultural context 
- Created \texttt{Bllossom} model by applying all 3 enhancement strategies to Llama2, and made model/data publicly available
- Showed superior performance of \texttt{Bllossom} over other Korean LLMs in quantitative benchmarks and qualitative human/\texttt{GPT4} evaluations

In summary, the paper introduces practical techniques to improve multilingual LLMs for less-resourced languages like Korean, with strong experimental results from the constructed \texttt{Bllossom} model. The public release of the model, data and services will support further research.


## Summarize the paper in one sentence.

 This paper proposes three methods to enhance the performance of less-resourced languages in multilingual language models: expanding vocabularies, enriching knowledge through bilingual pretraining, and improving usability via instruction tuning with a high-quality dataset.


## What is the main contribution of this paper?

 The main contributions of this paper are:

1. Proposing a method to enhance the vocabulary, knowledge, and usability of less-resourced languages (LRLs) using multilingual large language models (MLLMs). Specifically, this involves vocabulary expansion, bilingual pretraining, and instruction tuning using a high-quality small-scale dataset. 

2. Presenting a method for constructing instruction data based on language-specific features, demonstrated by constructing a Korean version of the LIMA dataset.

3. Making the data, models, and services used to construct and evaluate the enhanced Korean language model publicly available to facilitate easy utilization.

The paper focuses on enhancing the Korean language capabilities of the Llama2 MLLM as a case study using the proposed methods. The enhanced model, called Bllossom, outperforms previous Korean monolingual models of the same size in both quantitative evaluations on benchmarks and qualitative human and GPT-4 assessments.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and keywords associated with this paper include:

- Large language models (LLMs)
- Multilingual large language models (MLLMs) 
- Less-resourced languages (LRLs)
- Vocabulary expansion
- Knowledge enrichment 
- Instruction tuning
- Supervised fine tuning (SFT) 
- LIMA dataset
- Bilingual pretraining
- Qualitative evaluation
- GPT4 evaluation
- Bllossom model

The paper proposes methods to improve the capabilities of less-resourced languages like Korean in multilingual large language models. The three main methods are:

1) Vocabulary expansion to enhance expressiveness
2) Bilingual pretraining to align high- and low-resourced languages  
3) Construction of high-quality instruction dataset and instruction tuning to improve applicability

The Bllossom model applies these three methods to the Llama2 model for Korean. Both quantitative and qualitative evaluations are performed to validate the methods, using benchmarks tasks and assessments by humans and GPT4.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in this paper:

1. The paper proposes expanding the vocabulary of Llama2 with the KoBERT vocabulary. What considerations went into choosing KoBERT specifically for the vocabulary expansion over other Korean language models? How much does the choice of vocabulary source impact performance?

2. The paper uses a Korean-English parallel corpus for further pretraining after vocabulary expansion. What is the rationale behind using a bilingual corpus over a monolingual Korean corpus? How does the relative size of Korean vs English data in the bilingual corpus impact model performance? 

3. The construction of the Korean LIMA dataset involves manual modification of an initial machine translation to better reflect Korean linguistic style and cultural context. Can you elaborate on some of the specific modifications made? How necessary is this manual modification step?

4. The paper uses a variant of LoRA training that only updates certain parameters of Llama2. What is the motivation behind using LoRA here rather than finetuning the entire model? How does the choice of which parameters to update impact overall performance?  

5. Both human evaluation and evaluation by GPT4 find the proposed Bllossom model to outperform other models. What are the relative strengths and weaknesses of each evaluation approach? In what ways do the results complement each other?

6. The qualitative evaluations find more significant gains from the proposed methods than the quantitative benchmarks. Why might this be the case? What implications does this have for LLM evaluation?

7. Bilingual supervised finetuning is found to provide gains over monolingual finetuning in the qualitative evaluations. What factors contribute to this result? Would we expect similar gains for other language pairs?

8. The English proficiency of models finetuned on Korean is observed to decrease. Is there a way to maintain English performance while enhancing Korean capabilities? Is this a necessary tradeoff? 

9. The model incorporates vocabulary expansion, bilingual pretraining, and bilingual finetuning. What is the relative importance of each component? Are there diminishing returns from combining multiple strategies?

10. The method is shown effective for improving Korean in Llama2. How difficult would it be to adapt the approach to improve other less resourced languages in multilingual models? What language-specific customization would be required?
