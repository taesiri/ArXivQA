# [Optimizing Language Augmentation for Multilingual Large Language Models:   A Case Study on Korean](https://arxiv.org/abs/2403.10882)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Large language models (LLMs) require massive compute resources for training, making it difficult for smaller research groups to utilize them. 
- Multilingual LLMs (MLLMs) often overlook less-resourced languages (LRLs) like Korean during training. As a result, they have limited vocabulary and knowledge for LRLs.

Proposed Solution:
- Enhance Korean language capabilities of the Llama2 MLLM using 3 strategies:
  1) Expand Korean vocabulary by merging with KoBERT dictionary
  2) Enrich knowledge by pretraining on Korean-English Wikipedia data
  3) Improve usability by constructing high-quality Korean LIMA dataset and performing instruction tuning

Main Contributions:
- Proposed methods to expand vocabulary, reinforce knowledge, and enhance usability of LRLs in MLLMs
- Constructed Korean version of LIMA dataset by translating and modifying English LIMA data to fit Korean cultural context 
- Created \texttt{Bllossom} model by applying all 3 enhancement strategies to Llama2, and made model/data publicly available
- Showed superior performance of \texttt{Bllossom} over other Korean LLMs in quantitative benchmarks and qualitative human/\texttt{GPT4} evaluations

In summary, the paper introduces practical techniques to improve multilingual LLMs for less-resourced languages like Korean, with strong experimental results from the constructed \texttt{Bllossom} model. The public release of the model, data and services will support further research.
