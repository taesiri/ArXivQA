# [Improving Model's Interpretability and Reliability using Biomarkers](https://arxiv.org/abs/2402.12394)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Accurate and interpretable AI models are critical for safe clinical diagnosis, especially in medicine. However, most AI models are black boxes lacking interpretability.
- There is a need for reliable and interpretable AI techniques to ensure clinicians can understand and verify the reasoning behind model predictions before acting upon them.

Proposed Solution: 
- The authors propose an interpretable lung ultrasound diagnostic pipeline consisting of:
  1) A feature encoder that extracts established lung ultrasound biomarkers that clinicians are familiar with. This allows clinicians to easily verify the encoder's outputs.
  2) Downstream tree-based classifiers built on top of these biomarkers that are inherently interpretable as they operate on known explainable features.
- This yields a fully interpretable diagnostic model. 

Objective and Methods:
- The authors conduct a user study to evaluate whether the proposed decision tree explanations improve the ability of clinicians to identify inaccurate model predictions, compared to conventional saliency maps. 
- 30 lung ultrasound videos were analyzed by the model and clinicians judged the correctness of predictions using either saliency maps, decision trees, both or none.

Key Results:
- Decision tree explanations help clinicians reliably detect false positives by revealing the decision logic and evidence used by the model. This can overcome limitations of saliency maps.
- Saliency maps are better for detecting true positives. Using both analysis tools together provides the most benefit.
- The proposed interpretable pipeline with decision tree explanations enhances clinicians' ability to verify model predictions before acting on them. This improves reliability and safety.

Main Contributions:
- Demonstrating that decision tree classifiers built on top of clinically established biomarkers can enhance the interpretability and reliability of ultrasound diagnostic models.
- Conducting a user study to formally evaluate the interpretability of the proposed interpretable pipeline.
- Showing that decision tree explanations help clinicians detect model mispredictions, especially false positives. This can avoid unsafe clinical actions.
