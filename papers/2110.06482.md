# [Parallel Deep Neural Networks Have Zero Duality Gap](https://arxiv.org/abs/2110.06482)

## What is the central research question or hypothesis that this paper addresses?

This paper focuses on exploring the convex duality for training deep neural networks. The central questions it aims to address are:1. Does strong duality hold for deep neural networks? In other words, is there a duality gap (difference between the primal and dual optimal values) when optimizing deep neural networks using convex duality techniques?2. Can we characterize the duality gap for deep neural networks? 3. Is there a neural network architecture for which strong duality holds regardless of the depth?The key hypothesis is that standard deep neural networks may have a non-zero duality gap, meaning the convex dual does not perfectly characterize the non-convex training problem. However, a modified parallel architecture may achieve strong duality. Specifically, the paper proves:- For standard deep linear networks, the duality gap is non-zero when the depth is 3 or more.- For standard 3-layer ReLU networks, the duality gap is also non-zero.- For parallel deep linear and ReLU networks with certain regularization, the duality gap is zero regardless of depth.So in summary, the paper aims to characterize and prove whether strong duality holds for different neural network architectures, which determines if an equivalent convex formulation exists for training them. The key finding is that parallel networks can achieve zero duality gap while standard networks may not.
