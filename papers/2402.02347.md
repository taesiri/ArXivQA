# [Riemannian Preconditioned LoRA for Fine-Tuning Foundation Models](https://arxiv.org/abs/2402.02347)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Fine-tuning large foundation models like LLMs and diffusion models using full parameter fine-tuning is very costly in terms of storage and compute. 
- Recent methods like LoRA perform low-rank adaptation by adding low-rank matrices to existing weights and only training those. However, optimization of LoRA parameters using SGD or AdamW can be further improved.

Proposed Solution: 
- The paper proposes using a Riemannian preconditioner by scaling the LoRA parameter gradients using the other parameter's outer product. For example, the gradient w.r.t. A is scaled by (B^T B + Î´I)^(-1).
- This is motivated by interpreting LoRA optimization as optimizing over the manifold of low-rank matrices. The proposed preconditioner has been studied for low-rank matrix optimization problems.  

- The preconditioned SGD method is called scaled GD and preconditioned AdamW method is called scaled AdamW in the paper.

Contributions:
- Introduces Riemannian preconditioning to accelerate and stabilize LoRA fine-tuning of large foundation models, inspired by optimization over low-rank matrix manifolds.

- Shows improved performance of scaled GD and scaled AdamW over SGD and AdamW for LoRA fine-tuning of LLMs like GPT-2, Mistral 7B and diffusion models on various datasets.

- Demonstrates increased robustness to hyperparameters like learning rate for the proposed methods.

- Provides convergence guarantee for fine-tuning two-layer neural network with reparameterization and proposed preconditioner, with rate independent of data condition number.

- Overall, applies optimization techniques studied for low-rank matrices to accelerate and stabilize an important technique (LoRA) for fine-tuning large foundation models.
