# [Class Incremental Learning for Adversarial Robustness](https://arxiv.org/abs/2312.03289)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a summary paragraph of the key points from the paper:

This paper proposes a new method called Adversarially Robust Class Incremental Learning (ARCIL) to improve the adversarial robustness of deep learning models in an incremental learning setting. The authors identify a problem they term "flatness forgetting", where models lose the flatness of their loss landscape on past tasks when learning new tasks incrementally. This flatness is important for adversarial robustness. To address this, they propose a Flatness Preserving Distillation (FPD) loss that aligns the difference in outputs between clean and adversarial examples from the current model to a previous model, helping to transfer flatness. They also introduce a Logit Adjustment Distillation (LAD) loss that adapts the previous model's knowledge to new tasks by modifying its logits on new data based on feature robustness. Experiments across datasets demonstrate state-of-the-art performance, with substantially higher robust accuracy and reduced catastrophic forgetting compared to baselines. The method provides an effective approach for incremental learning with adversarial robustness and introduces the novel concept of flatness forgetting in this setting.


## Summarize the paper in one sentence.

 This paper proposes two novel losses, Flatness Preserving Distillation and Logit Adjustment Distillation, to address the problem of catastrophic forgetting and lack of adaptability when combining adversarial training with class incremental learning.


## What is the main contribution of this paper?

 According to the paper, the main contributions are:

1) Addressing the less studied Adversarially Robust Class Incremental Learning (ARCIL) task, which involves adversarial training on sequential data.

2) Identifying the "flatness forgetting" problem when applying adversarial training to class incremental learning, and proposing a novel solution called Flatness Preserving Distillation (FPD) to mitigate this issue. 

3) Introducing Logit Adjustment Distillation (LAD) which slightly modifies the outputs of previous tasks to enhance the model's adaptability to new tasks.

4) Assessing the performance of current class incremental learning methods in the ARCIL setting and outperforming their results with the proposed method, achieving superior performance in both adversarial robustness and clean accuracy.

So in summary, the main contributions are identifying the flatness forgetting issue in ARCIL, proposing solutions to address it as well as enhance adaptability, and demonstrating state-of-the-art performance on this task.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and concepts are:

- Adversarially Robust Class Incremental Learning (ARCIL) - The paper focuses on studying adversarial robustness in a class incremental learning setting, which they term ARCIL.

- Flatness forgetting - A phenomenon the authors identify when applying adversarial training to class incremental learning, where the flatness of the loss landscape that is characteristic of adversarial training is lost when learning new tasks. 

- Flatness Preserving Distillation (FPD) - A loss proposed by the authors to address flatness forgetting by preserving the subtraction output differences on clean and adversarial examples between model versions on different tasks.

- Logit Adjustment Distillation (LAD) - A distillation approach proposed that adjusts the logits of the previous model on new task data before distilling to the current model, enhancing adaptability.

- Catastrophic forgetting - The common problem in incremental learning where knowledge of previous tasks is forgotten rapidly as new tasks are learned.

So in summary, key terms cover the incremental learning setup, adversarial training concepts, the specific problems and solutions identified in the paper related to flatness and distillation, and catastrophic forgetting.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions I would ask about the method proposed in this paper:

1) The paper identifies a "flatness forgetting" problem when applying adversarial training to incremental learning methods. Can you explain in more detail why this loss of flatness occurs and how it impacts model robustness against adversarial attacks? 

2) The proposed Flatness Preserving Distillation (FPD) loss aims to mitigate the flatness forgetting problem. How exactly does distilling the difference between clean and adversarial image outputs help preserve flatness and gradient/curvature information from the previous task's model?

3) For the FPD loss, you chose to use the KL divergence as the surrogate loss function D. What was the rationale behind selecting KL divergence over other losses like L1 or L2? How sensitive is model performance to the choice of surrogate loss?

4) The Logit Adjustment Distillation (LAD) loss modifies the previous model's logits for unseen data before distilling knowledge. What motivated this adjustment and how does it improve adaptation to new tasks? Could an ablation study quantify this impact?

5) You showed FPD helps even when using random noise perturbations, but works best with adversarial perturbations. Does FPD also work for naturally corrupted images or is there something unique about adversarial perturbations? 

6) The paper evaluates model robustness using PGD and AutoAttack. How does performance compare using other attacks like FGSM or CW? Are the conclusions still valid?

7) You incorporated PGD adversarial training though methods like TRADES and MART tend to achieve higher robustness. Could FPD and LAD be combined with those methods instead? How might that impact incremental learning performance?

8) The rehearsal memory buffer stores exemplars from previous tasks. How sensitive is the method to the buffer size and buffer update strategy? Could a smarter sampling approach improve results?

9) How efficiently can the proposed method scale to larger, more complex datasets like ImageNet? Would curriculum learning schedules be useful for larger incremental task sequences? 

10) Beyond image classification, could this method apply to incremental learning scenarios in other domains like object detection, semantic segmentation, reinforcement learning etc? What modifications might be needed?


## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
The paper addresses the problem of adversarially robust class incremental learning (ARCIL). While adversarial training methods have shown success in improving model robustness against adversarial attacks, they have mainly been studied in fixed dataset settings. However, real-world scenarios involve continuously accumulating new data over time. Therefore, it is important to study adversarial robustness in incremental learning settings where models need to learn new classes over time while maintaining robustness on old classes.  

The paper reveals an important issue that arises when simply integrating adversarial training into incremental learning - the loss of flatness on old tasks, which they term as "flatness forgetting". Specifically, the gradient and curvature of loss for old tasks increase substantially as new tasks are added, indicating greater sensitivity to input changes and loss of robustness.

Proposed Solution:
To address the flatness forgetting issue, the paper proposes a Flatness Preserving Distillation (FPD) loss that aligns the difference in outputs between clean and adversarial examples from the old model to the new model. This effectively transfers the flatness properties and preserves robustness for old classes.

Additionally, a Logit Adjustment Distillation (LAD) loss is introduced to enhance model adaptability for new tasks. It modifies the logits of the old model for new data points before distilling knowledge to the new model. This increases plasticity by reducing overconfidence on unseen data.

Main Contributions:

- Identifies the problem of "flatness forgetting" that arises when applying adversarial training naively in incremental learning 

- Proposes a novel FPD loss to mitigate flatness forgetting by distilling flatness properties from old to new model using output differences on clean and adversarial examples

- Introduces a LAD loss to adjust old model's logits on new data and enable better knowledge transfer and adaptation

- Extensive experiments demonstrate state-of-the-art performance on ARCIL, significantly outperforming baseline incremental learning methods combined with adversarial training

- Provides new perspective on importance of flatness and output differences for preserving robustness in adversarial training
