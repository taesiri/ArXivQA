# [Image2Sentence based Asymmetrical Zero-shot Composed Image Retrieval](https://arxiv.org/abs/2403.01431)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Existing composed image retrieval (CIR) methods rely on labeled image-text-image triplets for training, which are expensive to collect. Recently proposed zero-shot CIR (ZSCIR) methods adopt symmetric retrieval with heavy models on both query and database sides, lacking flexibility for practical deployment.

Method: 
- The paper proposes an asymmetric zero-shot CIR framework named Image2Sentence based Asymmetric zero-shot composed image retrieval (ISA).
- A lightweight CNN or transformer model is used on the query side to extract visual features. An adaptive token learner is proposed to map the visual features to a descriptive sentence in the word embedding space of a frozen large vision-language (VL) model. 
- The mapped sentence captures salient visual concepts and interacts with the text modifier. It is concatenated with text modifier as the composed query.
- The large VL model is deployed on the database side to extract image and text features. Global contrastive distillation and local alignment regularization are used to align the lightweight and VL models.

Main Contributions:
- Proposes the first asymmetric framework for zero-shot CIR, enabling efficient deployment.
- Designs an adaptive token learner to map images to informative sentences instead of single words, enhancing discrimination.
- Achieves new state-of-the-art results on CIRR, FashionIQ and CIRCO datasets, demonstrating effectiveness. 
- Shows trade-off between efficiency and accuracy by using different lightweight models, proving flexibility.

In summary, the paper presents an asymmetric ZSCIR framework with an adaptive token learner that only relies on unlabeled images for training. It achieves superior accuracy and efficiency compared to previous symmetric ZSCIR methods. The framework is flexible to deploy and strikes a balance between efficiency and accuracy.


## Summarize the paper in one sentence.

 This paper proposes an asymmetric zero-shot composed image retrieval framework called ISA that uses an adaptive token learner to map images to sentences in the word embedding space of a foundation model for more efficient and flexible deployment.


## What is the main contribution of this paper?

 This paper proposes an asymmetric zero-shot composed image retrieval framework called Image2Sentence based Asymmetric zero-shot composed image retrieval (ISA). The main contributions are:

1) It proposes an adaptive token learner to map images to sentences in the word embedding space of a visual-language model. This helps extract more distinctive visual information compared to mapping images to single words.

2) It develops an asymmetric retrieval framework with a lightweight model on the query side and a large foundation model on the gallery side. This allows for more flexible and efficient deployment compared to symmetric retrieval. 

3) It aligns the lightweight and foundation models using global contrastive distillation and local alignment regularization. This helps transfer knowledge from the foundation model to the lightweight model.

4) Experiments on three benchmarks show the proposed method achieves better accuracy and efficiency compared to prior arts, demonstrating the effectiveness of the asymmetric framework and adaptive token learner.

In summary, the main contributions are the asymmetric framework, adaptive token learner, and alignment techniques, which together enable more accurate and flexible zero-shot composed image retrieval.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and concepts are:

- Composed image retrieval (CIR) - The main task that the paper focuses on, retrieving images based on a query image and modifier text.

- Zero-shot composed image retrieval (ZSCIR) - A variant of CIR that does not require labeled training triplets, only image-text pairs.

- Asymmetric retrieval - Using different models for the query and gallery sides, with a lighter model on the query side and heavier model on the gallery side. Enables flexibility.

- Adaptive token learner - Key component proposed that maps visual features to a sentence in the word embedding space, acting as a "semantic filter" to preserve discriminative information.

- Global contrastive distillation (GCD) - Loss used to align the lightweight query encoder output with the visual features from the frozen foundation model.

- Local alignment regularization (LAR) - Additional loss used to make the mapped sentence close to a real caption describing the image.

- Deployment flexibility - A goal of the paper is to enable deployment of CIR systems in resource-constrained scenarios like mobile devices by using a lighter query-side model.

Does this summary help capture the key ideas and terminology? Let me know if you need any clarification or have additional questions!


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 potential in-depth questions about the method proposed in this paper:

1. The paper proposes an asymmetric framework for composed image retrieval where the query side uses a lightweight model and the gallery side uses a large vision-language model. What are the advantages and disadvantages of this asymmetric approach compared to a symmetric approach?

2. The adaptive token learner maps image features to a sentence in the word embedding space of the vision-language model. How does representing the image as a sentence rather than a single word help improve performance? What are the limitations?

3. The paper uses global contrastive distillation (GCD) to align the lightweight query model with the larger gallery model. What other techniques could be used to improve alignment between mismatched models in an asymmetric framework?

4. What impact does the choice of lightweight query model have on overall retrieval performance? How could the adaptive token learner be tailored to different query models? 

5. The local alignment regularization (LAR) loss helps the mapped sentence tokens retain more faithful visual information. What other auxiliary losses could help regularize or provide useful training signals for the adaptive token learner?

6. How well does the learned pseudo sentence representation capture attributes like color, texture, shape etc. compared to the original visual features? What visual concepts is it still struggling with?

7. Could the proposed asymmetric framework and adaptive token learner extend to other cross-modal retrieval tasks like text-to-image retrieval? What adaptations would be needed?

8. How does the performance compare when using ground-truth image captions vs LAR for regularization? What are the tradeoffs?

9. The token length $L$ impacts performance - what is the sweet spot and why do very short and very long sentences degrade results?

10. What failure casespersist in the proposed model - where does it still struggle with composed retrieval compared to simply using visual features?
