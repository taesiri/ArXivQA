# [Investigating the Robustness of Modelling Decisions for Few-Shot   Cross-Topic Stance Detection: A Preregistered Study](https://arxiv.org/abs/2404.03987)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Stance detection models often fail to generalize across topics. This limits their usefulness for applications like viewpoint diverse news recommenders which require detecting stance differences on new topics.
- Several modeling choices likely impact cross-topic stance detection but their effects are not well studied: task definition (pro/con vs same/different side), model architecture (bi- vs cross- encoding), adding natural language inference (NLI) knowledge.

Methodology:
- The authors preregistered a hypothesis-driven study to test effects of these choices on 7 stance detection datasets.
- They compared pro/con and same side stance definitions, bi-encoder (SBERT) and cross-encoder (RoBERTa) architectures, with and without NLI knowledge added via intermediate pretraining.
- Experiments were in a few-shot setting, training on 100 examples per dataset. Both cross-topic and in-topic evaluation was done.

Results:
- Same side stance was more stable across datasets but did not clearly outperform pro/con overall. Performance depends on dataset and other choices.
- No clear correlation was found between number of training topics and cross-topic performance.  
- Cross-encoders tended to outperform bi-encoders, especially for same side stance, but some datasets showed the opposite.
- Adding NLI knowledge considerably improved performance on some but not all datasets.

Conclusions:
- No single optimal modeling approach emerged across different datasets, indicating the variability in modelling the concept of "stance". 
- Systematic experiments on multiple datasets are essential for finding robust cross-topic stance detection.
- The study demonstrates the value of preregistration for transparency and responsible use of compute in NLP.

Main Contributions:
- First comprehensive study on the effects and interactions of core modelling choices for cross-topic stance detection.
- New benchmark dataset adapted for same side stance.
- First preregistered hypothesis testing in computational argumentation, serving as a model for responsible NLP research.
