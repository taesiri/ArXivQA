# Multimodal Analogical Reasoning over Knowledge Graphs

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be:How can we develop artificial neural network models that are capable of multimodal analogical reasoning, and how does providing multimodal input compare to single-modal input for analogical reasoning?The key points are:- The paper introduces a new task of multimodal analogical reasoning, requiring models to make analogies across vision and language modalities. - Prior work has focused mainly on single-modal analogical reasoning, either in vision or language separately. This paper argues that multimodal reasoning is more akin to human cognition.- The paper constructs a new dataset and knowledge graph to study multimodal analogical reasoning.- Experiments compare various neural baselines on the new dataset. The proposed model called MarT outperforms others, showing the benefit of incorporating multimodal knowledge and an analogical reasoning framework.- Overall, the central hypothesis seems to be that neural models with multimodal knowledge inputs and some inductive biases for analogical reasoning will perform better at multimodal analogical reasoning compared to single-modal models without such specialized mechanisms. The paper aims to demonstrate this through the new dataset and models.In summary, the key research question is how to develop artificial neural network models for the new task of multimodal analogical reasoning, with a central hypothesis that multimodal models will outperform single-modal ones. The paper introduces data and models to test this hypothesis.
