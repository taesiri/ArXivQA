# [Large Language Models are Contrastive Reasoners](https://arxiv.org/abs/2403.08211)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Prompting methods like chain-of-thought (CoT) are crucial for enhancing reasoning capabilities of large language models (LLMs). However, zero-shot CoT may generate incorrect reasoning steps while few-shot CoT requires expensive human labeling of reasoning examples.  

Proposed Solution: 
- The paper proposes "contrastive prompting" which instructs LLMs to generate both correct and incorrect answers when solving problems. This triggers LLMs' self-awareness of potential errors without needing human-labeled examples.

Method:
- Apply a simple trigger prompt like "Let's give a correct and a wrong answer" before the LLM generates answers. 
- First extract the reasoning text from the LLM using the trigger. Then extract the final answer using the reasoning text.
- Can combine with CoT by prompting "Let's think step-by-step and give both a correct and wrong answer".

Experiments:
- Evaluated on 12 datasets spanning arithmetic, commonsense, symbolic and logical reasoning tasks. 
- Used GPT-3.5 and GPT-4 as base models.
- Contrastive prompting improves over zero-shot CoT on most tasks and achieves comparable or better performance than few-shot CoT.
- Combining contrastive prompting and CoT yields best results, surpassing state-of-the-art on some datasets.

Main Contributions:
- Proposed a simple yet effective contrastive prompting approach to elicit self-awareness of errors in LLMs without human labeling.
- Achieves new state-of-the-art results on arithmetic and commonsense reasoning datasets.
- Demonstrated LLMs have decent contrastive reasoning capabilities that can be unlocked via prompting.
