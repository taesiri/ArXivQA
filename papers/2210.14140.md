# Contrastive Search Is What You Need For Neural Text Generation

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be:Are autoregressive language models (LMs) intrinsically anisotropic, and therefore require additional training like SimCTG to enable effective use of the contrastive search decoding method?The key hypotheses appear to be:1. Previous work has claimed that autoregressive LMs like GPT-2 are anisotropic, meaning token representations reside in a narrow subset of the entire representation space. 2. This anisotropy is an issue for contrastive search, which relies on an isotropic representation space to work effectively. 3. The SimCTG method was proposed to calibrate LMs to be more isotropic through additional training.4. However, the authors hypothesize that many autoregressive LMs may actually be naturally isotropic, without needing additional training.To test these hypotheses, the authors evaluate the isotropy of a wide range of autoregressive LMs across 16 languages. They find that only GPT-2 small and medium models are anisotropic, while most other LMs are intrinsically isotropic.They then show contrastive search works very well on these isotropic LMs without any calibration training, challenging the need for methods like SimCTG.In summary, the key question is whether autoregressive LMs need additional training to enable contrastive search, or if most models are naturally isotropic. The findings suggest most are intrinsically isotropic, challenging conventional wisdom.


## What is the main contribution of this paper?

This paper makes two key contributions:1. It investigates the isotropy of autoregressive language models (LMs) across 16 languages. Previous works have assumed that autoregressive LMs are anisotropic, but this paper finds that most existing LMs are actually isotropic, with the exception of some smaller GPT-2 models. 2. It shows that the contrastive search decoding method significantly outperforms previous decoding methods like beam search and nucleus sampling when using off-the-shelf isotropic LMs, without needing additional contrastive training like SimCTG. Experiments across 4 text generation tasks in 16 languages demonstrate the effectiveness of contrastive search.In summary, the main contributions are:- Showing most existing autoregressive LMs are naturally isotropic, contrary to previous assumptions.- Demonstrating the effectiveness of contrastive search with isotropic off-the-shelf LMs, without needing extra training.The key insight is that contrastive search works very well on existing isotropic LMs, simplifying its application compared to prior work requiring anisotropic LMs to be retrained with contrastive learning objectives like SimCTG.
