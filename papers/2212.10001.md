# Towards Understanding Chain-of-Thought Prompting: An Empirical Study of   What Matters

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central research question this paper aims to address is: What are the key aspects of Chain-of-Thought (CoT) prompting rationales that make the technique effective in improving the reasoning capabilities of large language models?In particular, the paper investigates:1) How important is the validity of reasoning in the CoT prompting examples for achieving good performance? 2) If valid reasoning is not crucial, then what other aspects of the demonstrated reasoning steps contribute most to the effectiveness of CoT prompting?To summarize, the paper seeks to gain a deeper understanding of what makes CoT prompting work by examining the importance of different components of the CoT rationales through a series of ablation studies. The key research questions revolve around unraveling the role of reasoning validity and identifying other key factors that determine the success of CoT prompting.


## What is the main contribution of this paper?

Based on my reading, the main contributions of this paper are:1. Conducting extensive ablation studies to understand the key aspects that make Chain-of-Thought (CoT) prompting effective for improving the reasoning ability of large language models (LLMs). 2. Finding that the validity of the reasoning steps in CoT demonstrations matters surprisingly little - providing invalid reasoning steps leads to only a small drop in model performance.3. Identifying relevance to the input query and coherence in ordering of steps as the most crucial factors for effective CoT prompting.4. Proposing new ways to quantify the prior knowledge LLMs have on reasoning tasks, by intentionally perturbing the CoT demonstrations. 5. Raising important discussions on how LLMs learn (or fail to learn) from CoT prompts, their reliance on prior knowledge, and reflections on benchmarking few-shot reasoning.In summary, this paper provides novel analysis to reveal insights into the inner workings of CoT prompting, an influential technique for few-shot reasoning. The findings challenge common intuitions and lead to new perspectives on evaluating and interpreting LLM reasoning.


## How does this paper compare to other research in the same field?

Here are a few thoughts on how this paper compares to other related work:- This paper focuses specifically on understanding and analyzing Chain-of-Thought (CoT) prompting for large language models. Other recent work has proposed variants or extensions of CoT, like exploring different decoding strategies or automating the construction of prompts. But this paper focuses directly on understanding the key factors that make the original CoT method effective through ablation studies. - There is some related work that also aims to analyze or understand CoT prompting. Concurrent work by Madaan et al. makes CoT prompts counterfactual to study component importance. Ye et al. explores corrupting CoT demonstrations. Saparov et al. parses CoT rationales into logical forms. However, this paper shows more drastically that even invalid reasoning in CoT prompts has little impact on performance, and identifies relevance and coherence as key through controlled settings.- More broadly, there is a line of work trying to understand in-context learning. Min et al. showed ground truth mappings often don't matter much. Webson & Pavlick found instruction following is robust to irrelevant instructions. This paper contributes an analysis targeted at CoT prompting for multi-step reasoning tasks. The findings suggest models may rely more on prior knowledge rather than few-shot learning during CoT prompting.- Overall, a key contribution of this paper is the extensive ablation study to reveal model behaviors during CoT prompting. The findings open new questions around how reasoning abilities demonstrated during CoT prompting relate to prior knowledge versus few-shot learning. The paper makes important steps toward better understanding prompting-based evaluation of reasoning.In summary, this paper provides novel in-depth analysis and insights into CoT prompting through controlled experiments, contributing to the growing literature on understanding prompt-based evaluation. The findings highlight interesting areas for future work on assessing reasoning abilities.


## What future research directions do the authors suggest?

Based on my reading of the paper, some of the main future research directions suggested by the authors are:- Developing better ways to intrinsically evaluate the quality of generated rationales, beyond just checking for correctness of key intermediate results. The authors mention that their current evaluation based on bridging objects does not fully capture coherence and reasoning dependencies. New methods are needed for more comprehensive intrinsic evaluation without requiring ground truth annotations.- Extending the ablation study to other types of reasoning tasks beyond arithmetic and factual QA. The authors mention extending their designs to more "reduced" symbolic reasoning tasks that are highly template-based could be an interesting direction.- Giving a more systematic treatment to constructing invalid reasoning for the demonstrations, such as following categorizations of common informal logical fallacies.- Identifying or constructing alternative benchmarks where language models have less prior knowledge, to more faithfully evaluate few-shot learning of reasoning skills. The authors suggest their work reveals the models' existing reasoning abilities, so evaluating on novel tasks is needed.- Further investigation into when and how language models can learn skills like reasoning purely in-context, complementary to the findings here. The authors mention some recent work has shown in-context learning is possible in certain settings.- Exploring the impacts of different pretraining objectives, data, and model architectures on chain of thought prompting. The authors observe some inconsistencies between different models tested.In summary, the main directions are developing better evaluation methods for rationales, extending the analysis to new reasoning tasks, constructing better benchmarks, further probing in-context learning, and understanding how model properties affect chain of thought prompting.
