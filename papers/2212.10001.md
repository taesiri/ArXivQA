# Towards Understanding Chain-of-Thought Prompting: An Empirical Study of   What Matters

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central research question this paper aims to address is: What are the key aspects of Chain-of-Thought (CoT) prompting rationales that make the technique effective in improving the reasoning capabilities of large language models?In particular, the paper investigates:1) How important is the validity of reasoning in the CoT prompting examples for achieving good performance? 2) If valid reasoning is not crucial, then what other aspects of the demonstrated reasoning steps contribute most to the effectiveness of CoT prompting?To summarize, the paper seeks to gain a deeper understanding of what makes CoT prompting work by examining the importance of different components of the CoT rationales through a series of ablation studies. The key research questions revolve around unraveling the role of reasoning validity and identifying other key factors that determine the success of CoT prompting.


## What is the main contribution of this paper?

Based on my reading, the main contributions of this paper are:1. Conducting extensive ablation studies to understand the key aspects that make Chain-of-Thought (CoT) prompting effective for improving the reasoning ability of large language models (LLMs). 2. Finding that the validity of the reasoning steps in CoT demonstrations matters surprisingly little - providing invalid reasoning steps leads to only a small drop in model performance.3. Identifying relevance to the input query and coherence in ordering of steps as the most crucial factors for effective CoT prompting.4. Proposing new ways to quantify the prior knowledge LLMs have on reasoning tasks, by intentionally perturbing the CoT demonstrations. 5. Raising important discussions on how LLMs learn (or fail to learn) from CoT prompts, their reliance on prior knowledge, and reflections on benchmarking few-shot reasoning.In summary, this paper provides novel analysis to reveal insights into the inner workings of CoT prompting, an influential technique for few-shot reasoning. The findings challenge common intuitions and lead to new perspectives on evaluating and interpreting LLM reasoning.
