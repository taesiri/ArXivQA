# [Global Rewards in Multi-Agent Deep Reinforcement Learning for Autonomous   Mobility on Demand Systems](https://arxiv.org/abs/2312.08884)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
The paper studies the problem of vehicle dispatching in autonomous mobility-on-demand (AMoD) systems, where a central operator controls a fleet of autonomous vehicles to serve transportation requests from customers. The goal is to maximize the total profit of the system by deciding which requests to serve and assigning vehicles accordingly. Recent approaches using multi-agent deep reinforcement learning (MADRL) train agents based on local per-vehicle rewards, which can distort the global profit signal and lead to suboptimal policies.

Proposed Solution: 
The paper proposes a novel MADRL algorithm that trains agents via global system-level rewards to align their objectives with maximizing the global profit. It assigns rewards to agents using a counterfactual baseline that estimates each agent's contribution to the global reward. Specifically, it extends the Counterfactual Multi-Agent Policy Gradient (COMA) credit assignment approach into a Soft Actor-Critic (SAC) framework for discrete actions. To scale the approach, the paper also proposes a scheduled algorithm that combines local and global rewards.

Main Contributions:
- A new credit assignment method for MADRL that embeds COMA into SAC to estimate per-agent advantages using a counterfactual baseline
- A scheduled algorithm called COMA^scd that combines local and global rewards to scale the approach to large instances
- Evaluations on real-world taxi data showing COMA^scd outperforms state-of-the-art local-rewards MADRL by up to 2% on average and 6% on individual test dates  
- Structural analysis indicating global rewards improve implicit vehicle balancing and demand forecasting abilities compared to local rewards

The paper sets a new state-of-the-art for vehicle dispatching in AMoD systems using MADRL. The proposed combination of SAC and COMA is also generally applicable for multi-agent learning problems requiring credit assignment.


## Summarize the paper in one sentence.

 Here is a one-sentence summary of the paper:

The paper proposes a new multi-agent deep reinforcement learning algorithm for vehicle dispatching in autonomous mobility-on-demand systems that aligns agent rewards with the global objective of profit maximization by using credit assignment based on a counterfactual baseline, achieving improved performance over prior methods.


## What is the main contribution of this paper?

 According to the paper, the main contribution is:

The development of the first scalable multi-agent deep reinforcement learning (MADRL) algorithm for vehicle dispatching in profit-maximizing autonomous mobility-on-demand (AMoD) systems that trains agents via global rewards. Specifically, the paper proposes a novel algorithm that combines the benefits of low learning variance and sample efficiency of soft actor-critic (SAC) with the benefits of credit assignment via a counterfactual baseline based on difference rewards. This resolves goal conflicts between the trained agents and the operator's objective of maximizing overall system profit. The algorithm is shown to improve performance over state-of-the-art methods on real-world taxi data while maintaining scalability.

In summary, the key contribution is a new, scalable MADRL algorithm for vehicle dispatching in AMoD systems that uses global rewards and a novel credit assignment approach to align agent and system-level objectives.


## What are the keywords or key terms associated with this paper?

 Based on reviewing the paper, the keywords or key terms associated with it are:

multi-agent learning, credit assignment, deep reinforcement learning, autonomous mobility on demand


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 potential in-depth questions about the method proposed in the paper:

1. The paper proposes a novel way to train agents with global rewards instead of local rewards. How does the use of global rewards help align the agents' goals better with the overall system objective of maximizing profit? What are some challenges introduced by using global rewards?

2. The paper embeds reward marginalization into a Soft Actor-Critic (SAC) framework to assign credit to individual agents for contributing to the global rewards. Why is the naive combination of SAC and Counterfactual Multi-Agent Policy Gradients (COMA) insufficient? What adjustment did the authors make to successfully integrate COMA into SAC?

3. The paper introduces a new advantage function that combines an equally-weighted advantage baseline and a target actor network baseline. What are the rationales behind using these two different baselines? Why does using them together in the adjusted COMA algorithm (COMA^adj) work better than using either one alone?

4. The paper proposes a scheduled algorithm COMA^scd that combines local and global rewards to improve scalability. Why does using a dynamic weighting between local and global rewards help address scalability issues compared to using a static mix? What schedules for increasing the global reward weight were tested?

5. What structural differences were observed between the policies learned using local versus global rewards? How did the use of global rewards lead to better implicit balancing of vehicles across zones and better demand forecasting abilities?

6. The performance improvement from using global rewards appears small in percentage terms but is stated to be substantial in the AMoD setting. Why is this the case? What monetary impact could these percentage improvements have on a real AMoD operator? 

7. The paper benchmarks the algorithm on a hexagonal grid discretization of Manhattan. What considerations went into designing the experimental setup this way? How were the neighbor distances and travel times between zones determined?

8. What hardware was used for training the models? How many GPUs and what types? How long did it take to train the largest model with 38 zones and 100 vehicles?

9. The paper states that future work could focus on stabilizing purely global-rewards-based algorithms. What instability issues were observed from the ablation study results when increasing the number of agents? How might these issues be addressed?

10. Could the proposed methodology be applied to other problem domains beyond AMoD vehicle dispatching? What other applications might benefit from scalable multi-agent deep reinforcement learning with global rewards and credit assignment?
