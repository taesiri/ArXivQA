# [A Process for Topic Modelling Via Word Embeddings](https://arxiv.org/abs/2312.03705)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
The paper addresses the problem of automatically extracting topics from a set of unclassified text documents, known as topic modeling. Topic modeling is useful for organizing large collections of texts but most existing methods have limitations like fixing the number of topics beforehand or not capturing relationships between words well.  

Proposed Solution:
The paper proposes a multi-step process for topic modeling that utilizes neural word embeddings, dimensionality reduction, and clustering. Specifically:

1. Word embeddings are generated for each document by using a pretrained multilingual BERT model. This encodes semantic relationships between words.

2. The high dimensionality of the BERT embeddings is reduced to 2D using UMAP, which preserves local and global structure. 

3. K-means clustering is applied on the UMAP embeddings to group similar documents into topics.

Main Contributions:

- Combines neural embeddings that capture word relationships with dimensionality reduction and clustering to achieve unsupervised topic modeling without setting the topic number beforehand.

- Evaluates the coherence and diversity of extracted topics using metrics like Topic Coherence and Topic Diversity. The method achieves scores comparable to recent techniques.

- Visualizes and interprets the topic models by examining top keywords per topic cluster ranked by TF-IDF. Four clear topics were extracted from a Spanish news dataset: digital finance, economic issues, climate change, and government regulations.

In summary, the paper presents an effective workflow for neural topic modeling that overcomes limitations of methods like LSA and LDA, avoiding preset number of topics and better capturingcontextual word relationships. Both quantitative metrics and qualitative inspection of topic keywords demonstrate the promise of this approach.
