# [Non-Convex Stochastic Composite Optimization with Polyak Momentum](https://arxiv.org/abs/2403.02967)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
The paper studies the stochastic composite optimization problem of minimizing an objective function $F(x)=f(x)+\psi(x)$, where $f$ is a smooth function and $\psi$ is a simple but possibly non-smooth function. It focuses on the non-convex setting where the stochastic gradient oracle for $f$ has high variance. The issue with vanilla stochastic proximal gradient methods in this setting is that they fail to converge beyond the variance of the stochastic gradient noise. Therefore, mega-batches of size $\Omega(\epsilon^{-1})$ are required to converge to an $\epsilon$-neighborhood of a stationary point, which is impractical.

Proposed Solution:
The paper proposes using stochastic proximal gradient with Polyak momentum to address the mega-batch issue. The key update is:
\begin{align*}
m_k &= (1-\gamma) m_{k-1} + \gamma g_k \\  
x_{k+1} &= \arg\min_{x} \langle m_k, x \rangle + \psi(x) + \frac{M}{2}\|x-x_k\|^2  
\end{align*}
Where $g_k$ is a stochastic gradient of $f$ and $m_k$ aggregates the gradients with a momentum coefficient $\gamma$.

Main Contributions:
- Establishes a lower bound for vanilla stochastic proximal gradient, showing it cannot converge beyond the stochastic gradient noise level without mega-batches.

- Proves that with Polyak momentum, the method achieves the optimal convergence rate of $O(1/K)$ for non-convex composite optimization without any batch size requirement. Rigorously analyzes the variance reduction effect of momentum.

- Extends the analysis to the case of inexact proximal steps, with the same convergence guarantees. This is important since exactly solving the proximal step may not be tractable. 

- Validates the theoretical findings numerically and demonstrates the practical effectiveness of the momentum method, including for a statistical preconditioning application from deep learning.

In summary, the paper provides a thorough analysis of using momentum for stochastic composite optimization, demonstrates its ability to alleviate the mega-batch issue, and shows strong theoretical and empirical evidence for its effectiveness.


## Summarize the paper in one sentence.

 This paper studies the stochastic proximal gradient method with Polyak momentum for non-convex composite optimization, proves its optimal convergence rate regardless of batch size, analytically quantifies the variance reduction effect of momentum, and shows the method works with inexact proximal steps.


## What is the main contribution of this paper?

 The main contributions of this paper are:

1) It establishes a lower bound result showing that the vanilla stochastic proximal gradient method cannot converge beyond the variance of the gradient noise. 

2) It analyzes the effect of incorporating momentum into the stochastic proximal gradient method and proves its optimal convergence rate without needing error-dependent mega-batches. The paper also rigorously studies the variance reduction effect of momentum.

3) It further extends the analysis to the case where the proximal steps are solved inexactly and shows similar convergence guarantees, demonstrating the robustness of the momentum method.

4) It provides numerical experiments to validate the theoretical findings and demonstrate the practical effectiveness of the stochastic proximal gradient method with Polyak momentum.

In summary, the paper shows both theoretically and empirically that using Polyak momentum allows the stochastic proximal gradient method to attain an optimal convergence rate in non-convex settings, regardless of batch size, overcoming limitations of the vanilla method. The analysis precisely quantifies the variance reduction effect due to momentum as well.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper's abstract and introduction, some of the key terms and concepts associated with this paper include:

- Stochastic proximal gradient method
- Non-convex optimization
- Composite optimization
- Polyak momentum
- Variance reduction
- Convergence rates
- Stationary points
- Proximal steps
- Inexact proximal steps

The paper focuses on analyzing the stochastic proximal gradient method with Polyak momentum for solving non-convex composite optimization problems. It provides convergence guarantees and analyzes the variance reduction effect of momentum, particularly in the small batch regime. Key theoretical concepts examined include convergence rates, proximal steps, inexact proximal steps, and getting to the stationary points of the non-convex objectives. Overall, the paper aims to demonstrate theoretically and empirically the superiority of using momentum compared to vanilla stochastic methods in the non-convex setting.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 potential in-depth questions about the method proposed in this paper:

1. The paper proves an optimal convergence rate for the stochastic proximal gradient method with Polyak momentum. Can you explain intuitively why momentum helps stabilize the method and achieve this optimal rate?

2. The analysis shows that Polyak momentum reduces the variance of the gradient noise. What is the mechanism behind this variance reduction effect? How does it precisely reduce the variance over iterations?

3. The paper considers inexact proximal steps. What kind of algorithms can be used to compute such inexact proximal steps? What are some practical approaches to ensure the inexactness criterion is satisfied? 

4. How does the composite setting make analyzing and initializing the method more tricky compared to the non-composite case? Can you summarize the key differences?

5. One of the assumptions is on the monotonicity of the proximal step. What is the intuition behind this assumption? When is it reasonable/unreasonable? How does it simplify the analysis?

6. The convergence guarantee involves a quantity Φ0 that depends on the initialization. What are the tradeoffs in different initialization schemes discussed in the paper? How do they affect the final convergence rate?

7. The paper argues that convergence in terms of the gradient norm is more natural than the proximal gradient mapping. Can you expand on this argument? What are some pros and cons of each convergence notion?

8. The momentum parameter is set to be Θ(L/M) where M is the proximal regularization parameter. How does this choice ensure convergence? What happens if momentum is set too small or too large?

9. The analysis assumes stochastic gradients of the smooth term f. What changes if f itself was a finite-sum instead? Would the method and analysis still apply in the deterministic setting?

10. The paper focuses on non-convex objectives. What changes would be needed to specialize the analysis to the (strongly) convex setting? Would we still need momentum for optimal convergence?
