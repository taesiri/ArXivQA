# [Self-Supervision is All You Need for Solving Rubik's Cube](https://arxiv.org/abs/2106.03157)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question seems to be whether a deep neural network can learn to efficiently solve goal-oriented combinatorial problems like Rubik's Cube through a simple self-supervised training approach on random goal-based scrambles. 

The key hypothesis appears to be that the inherent bias of random scrambles originating from the goal state can enable a DNN to statistically infer near-optimal reverse move sequences to unscramble states back to the goal. So the main question is whether this simple method of self-supervision on goal-based random scrambles can train a neural network to efficiently find high quality solutions for problems like Rubik's Cube.

In summary, the central research question/hypothesis is whether a DNN can learn to efficiently solve goal-oriented combinatorial problems through a straightforward self-supervised approach of training on random goal-based scrambles, by capitalizing on the inherent optimality bias of such scrambles.


## What is the main contribution of this paper?

 The main contribution of this paper is proposing a novel self-supervised learning method for solving combinatorial problems with a predefined goal, such as Rubik's Cube. The key ideas are:

- Reframing the task as "unscrambling" - training a neural network to sequentially reverse scramble moves that lead back to the predefined goal state. 

- Leveraging the inherent bias of random scrambles toward optimality. The shorter a scramble path, the more likely it is to occur randomly.

- Demonstrating that this simple approach of training on random scrambles outperforms prior methods like DeepCubeA on Rubik's Cube and other problems, achieving more optimal solutions with fewer node expansions and less training data.

The paper shows empirically that the probability distribution of random scrambles correlates with solution optimality. This enables the method to efficiently find near-optimal solutions by tracing back high-probability move sequences predicted by the trained neural network.

In summary, the main contribution is introducing a straightforward yet effective self-supervised deep learning approach for solving goal-based combinatorial problems, requiring only random scrambles as training data.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence TL;DR summary of the paper:

The paper proposes a new self-supervised deep learning method for solving combinatorial problems like Rubik's Cube that trains a neural network on random scrambles from the goal state and shows it can efficiently find near-optimal solutions.


## How does this paper compare to other research in the same field?

 Here are a few key points about how this paper compares to other research on solving Rubik's Cube and similar combinatorial problems:

- It proposes a new self-supervised learning approach that is much simpler than previous methods like DeepCubeA. Rather than using complex reinforcement learning, it just trains a neural network to predict the last move in a random scramble sequence. 

- It shows this simple approach can match or exceed the performance of DeepCubeA on Rubik's Cube, 15 Puzzle, and 7x7 Lights Out, while using 80-99% less training data. This demonstrates the efficiency and generalizability of the method.

- The results show a smooth tradeoff between solution optimality and computation time. As beam search width increases, solutions become more optimal at the expense of more node expansions. This allows flexibility based on use case.

- The scaling experiments characterize how performance improves with more model parameters and training data. This provides insights into how to scale up the approach. The derived scaling law enables estimating the optimal allocation for a given compute budget.

- Compared to optimal solvers like IDA* that rely on problem-specific heuristics and large precomputed tables, this method uses a generic neural network architecture and self-supervised training process. This makes it more adaptable to new problems.

- A limitation is that it only applies to problems with a predefined goal state. Methods like DeepCubeA and reinforcement learning may be better suited for optimization problems without a single solution.

Overall, this work pushes the boundaries of using deep learning for combinatorial search and planning problems in a simple yet effective manner. The self-supervised approach and analysis of scaling laws are novel contributions to the field.


## What future research directions do the authors suggest?

 Based on my reading of the paper, some of the main future research directions suggested by the authors include:

- Exploring the applicability of the proposed method to other combinatorial problems with predefined goals, beyond Rubik's Cube, 15 Puzzle, and 7x7 Lights Out. The authors state the method may be generalizable but more evaluation is needed.

- Estimating an optimal scaling law specifically for temporal efficiency rather than just loss/accuracy. The authors note that increasing model size provided diminishing returns for solving time, so optimizing compute budget for speed is an area for future work. 

- Adapting the method for combinatorial optimization problems without a predefined goal state, like the Traveling Salesman Problem. New techniques may be needed to handle problems where the goal itself is to find an optimal combination.

- Evaluating the approach on problems with stochastic/non-deterministic transitions between states. The reversibility assumption may not hold in such cases.

- Further analysis of the correlation between path probabilities and optimality. While the method relies on this correlation, more investigation could strengthen this theoretical basis.

- Extensions to problems defined on continuous state spaces, which may require adaptations like discretization.

So in summary, the main future directions are studying how broadly the method applies, optimizing its efficiency, modifying it for new problem classes, strengthening the theory, and expanding to continuous domains. The paper provides a strong proof-of-concept but leaves many avenues open for future work.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the paper:

This paper introduces a new self-supervised learning method for solving combinatorial problems that have a predefined goal state, such as Rubik's Cube. The key idea is to train a deep neural network (DNN) to predict the last move in a random scramble sequence originating from the goal state, based only on the resulting scrambled state. By sequentially reversing the moves predicted by the DNN for a given scrambled state, the method can efficiently find a solution path back to the goal. Experiments on Rubik's Cube, 15 Puzzle, and 7x7 Lights Out show the method achieves shorter, more optimal solutions than prior state-of-the-art, despite using far less training data. The paper also analyzes the scaling behavior of the method, and shows improved performance with larger models and more training data. Overall, the paper demonstrates a simple yet powerful approach of using self-supervision on random scrambles to effectively solve goal-oriented combinatorial problems.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the paper:

This paper introduces a new deep learning method for solving combinatorial problems that have a predefined goal state, such as Rubik's Cube. The key idea is to train a neural network model using a self-supervised approach on random scramble sequences originating from the goal state. Specifically, the model is trained to predict the last move in a scramble sequence based on the resulting scrambled state. At inference time, the model can then be used to sequentially reverse the predicted moves in order to "unscramble" a given state back to the goal. 

The authors demonstrate the effectiveness of this approach on Rubik's Cube, 15 Puzzle, and 7x7 Lights Out. In experiments on Rubik's Cube, the method finds shorter, more optimal solutions while expanding fewer nodes during search compared to the previous state-of-the-art method DeepCubeA. It achieves this despite using only 20% as much training data. Additional analyses reveal the method also scales well with increased model size and training data. Overall, the work provides a simple yet powerful deep learning approach for solving goal-based combinatorial problems, capitalizing on the inherent optimality bias of random scrambles originating from the goal state.


## Summarize the main method used in the paper in one paragraph.

 Here is a one paragraph summary of the main method used in the paper:

The paper proposes a self-supervised learning method to solve combinatorial problems that have a predefined goal, such as Rubik's Cube. The key idea is to train a deep neural network (DNN) to predict the last move in a sequence of random scrambles applied to the goal state. At each step during training, a random move is applied to the current state to generate a new scrambled state, and the DNN learns to predict this last move based on the scrambled state. This allows the DNN to learn the statistical tendencies of random scrambles applied to the goal state. To find a solution for a new scrambled state, the trained DNN sequentially predicts moves to apply in reverse order, tracing back to the goal state. The authors show this method can efficiently solve Rubik's Cube and related problems better than prior methods.


## What problem or question is the paper addressing?

 The paper is addressing the problem of solving combinatorial search problems with a predefined goal, such as Rubik's Cube, 15 Puzzle, and Lights Out. These problems have very large search spaces and finding optimal solutions can be computationally challenging. 

The main question the paper tries to answer is: can a simple deep learning method that trains on random scrambles generalize to efficiently find near-optimal solutions for these kinds of problems?

In summary, the key problem and question are:

- Solving combinatorial search problems with predefined goals that have very large search spaces (e.g. Rubik's Cube)

- Whether a simple deep learning method trained on random scrambles can generalize to find near-optimal solutions efficiently for these problems.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key keywords and terms are:

- Combinatorial search
- Goal-predefined problems
- Rubik's Cube
- 15 Puzzle
- Lights Out
- Iterative Deepening A* (IDA*) 
- DeepCubeA
- Deep reinforcement learning
- Self-supervised learning
- Beam search
- Scaling law
- Model size
- Training data volume

The paper introduces a self-supervised learning method to solve combinatorial problems that have a predefined goal, like Rubik's Cube. It trains a neural network on random scrambles branching from the goal state. At each step, the network learns to predict the last move applied based on the resulting state pattern. To solve a scrambled state, it sequentially reverses the predicted moves back to the goal. 

The method is evaluated on Rubik's Cube, 15 Puzzle, and Lights Out. It outperforms the previous DeepCubeA method in terms of solution optimality and efficiency, despite using less training data. The scaling law with respect to model size and training data is also analyzed.

Overall, the key focus is on using deep learning and self-supervision from random scrambles to efficiently solve goal-predefined combinatorial problems like Rubik's Cube.
