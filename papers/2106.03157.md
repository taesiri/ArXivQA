# [Self-Supervision is All You Need for Solving Rubik's Cube](https://arxiv.org/abs/2106.03157)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be whether a deep neural network can learn to efficiently solve goal-oriented combinatorial problems like Rubik's Cube through a simple self-supervised training approach on random goal-based scrambles. The key hypothesis appears to be that the inherent bias of random scrambles originating from the goal state can enable a DNN to statistically infer near-optimal reverse move sequences to unscramble states back to the goal. So the main question is whether this simple method of self-supervision on goal-based random scrambles can train a neural network to efficiently find high quality solutions for problems like Rubik's Cube.In summary, the central research question/hypothesis is whether a DNN can learn to efficiently solve goal-oriented combinatorial problems through a straightforward self-supervised approach of training on random goal-based scrambles, by capitalizing on the inherent optimality bias of such scrambles.


## What is the main contribution of this paper?

The main contribution of this paper is proposing a novel self-supervised learning method for solving combinatorial problems with a predefined goal, such as Rubik's Cube. The key ideas are:- Reframing the task as "unscrambling" - training a neural network to sequentially reverse scramble moves that lead back to the predefined goal state. - Leveraging the inherent bias of random scrambles toward optimality. The shorter a scramble path, the more likely it is to occur randomly.- Demonstrating that this simple approach of training on random scrambles outperforms prior methods like DeepCubeA on Rubik's Cube and other problems, achieving more optimal solutions with fewer node expansions and less training data.The paper shows empirically that the probability distribution of random scrambles correlates with solution optimality. This enables the method to efficiently find near-optimal solutions by tracing back high-probability move sequences predicted by the trained neural network.In summary, the main contribution is introducing a straightforward yet effective self-supervised deep learning approach for solving goal-based combinatorial problems, requiring only random scrambles as training data.
