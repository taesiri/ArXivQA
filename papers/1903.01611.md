# [Stabilizing the Lottery Ticket Hypothesis](https://arxiv.org/abs/1903.01611)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the main research questions/hypotheses appear to be:

1) The lottery ticket hypothesis conjectures that typical neural networks contain small subnetworks that can train to similar accuracy in a commensurate number of steps. However, prior work has struggled to find such subnetworks, or "winning tickets", in larger and deeper networks. This paper investigates why this is the case and how the lottery ticket hypothesis can be adapted for deeper networks. 

2) The paper hypothesizes that prior efforts have struggled on deeper networks because they focus on pruning precisely at initialization (iteration 0). The paper proposes modifying the iterative magnitude pruning (IMP) method to search for winning tickets that could have been obtained by pruning early in training rather than exactly at initialization.

3) The paper introduces the concepts of subnetwork "stability to pruning" and "stability to data order" to characterize how robust subnetworks are to noise and how consistently they can reach a good solution. It hypothesizes that IMP struggles at iteration 0 because the subnetworks lack stability, and that delaying pruning improves stability and thereby enables winning tickets to be found.

In summary, the main hypothesis is that by pruning slightly later in training rather than exactly at initialization, more stable winning tickets can be found in deeper networks, providing evidence for the lottery ticket hypothesis in larger models. The paper aims to test this hypothesis and provide insights into why rewinding the pruning iteration improves subnetwork stability and accuracy.


## What is the main contribution of this paper?

 Based on my reading of the paper, the main contributions are:

- The authors modify the iterative magnitude pruning (IMP) algorithm to "rewind" pruned weights to earlier points in training rather than resetting them to initialization. They show this allows IMP to find winning tickets (highly sparse subnetworks that can train to match the full network accuracy) in deeper neural networks where it fails with resetting to initialization.

- They demonstrate that rewinding to early points in training (0.1-7% through) allows finding winning tickets with high sparsity (50-99% weights pruned) in deep networks for ImageNet, which has not been shown before. 

- They analyze the stability of the IMP subnetworks and show that stability to pruning and data order improve rapidly as the rewinding point moves past initialization. They hypothesize this improved stability explains why rewinding enables finding winning tickets.

- Based on these findings, they revise the lottery ticket hypothesis to consider rewinding to early training iterations rather than precisely initialization. Overall, the results provide new insights into the opportunity to prune large neural networks early in training.

In summary, the main contribution is showing that rewinding pruned weights to small offsets from initialization enables finding highly sparse subnetworks that match full network accuracy on larger and deeper models, which expands our understanding of the lottery ticket hypothesis. The stability analysis also provides new perspective on why rewinding helps and how subnetworks evolve early in training.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the key points from the paper:

The paper proposes modifying the iterative magnitude pruning (IMP) method to search for sparse subnetworks early in training rather than precisely at initialization, finding this allows IMP to identify small, trainable subnetworks in larger vision models where it previously failed, which they argue reveals opportunities to successfully prune much earlier than previously thought if techniques can exploit rapid initial improvements in subnetwork stability.


## How does this paper compare to other research in the same field?

 This paper builds on prior work on network pruning and the lottery ticket hypothesis. Here are some key points of comparison to related work:

- The lottery ticket hypothesis (LTH) proposed that dense neural networks contain small subnetworks ("winning tickets") that can train to the same accuracy as the original network. This paper argues that rather than existing strictly at initialization, these winning tickets emerge shortly after the start of training. This modifies the original LTH.

- Prior work on the LTH and pruning focused on small vision tasks. This paper scales up the analysis to larger networks and datasets like ImageNet and demonstrates the utility of rewinding/late resetting in that setting.

- Techniques like gradual pruning and dynamic sparse training also aim to find compact subnetworks, but they maintain and train large portions of the network before pruning. This paper prunes much earlier while still matching accuracy.

- Methods like SNIP and network slimming prune before training, but they reset weights to initialization and do not consider later reset points. The rewinding approach here outperforms those techniques.

- Overall, this paper expands our understanding of when and how pruning can successfully produce trainable subnetworks. The late resetting modification enables pruning earlier in training and to higher sparsity levels than prior approaches.

In summary, this work shifts the perspective on when winning tickets emerge and highlights the value of stability in identifying trainable subnetworks early on large-scale tasks. It demonstrates opportunities to find sparse architectures more efficiently than prior techniques allowed.


## What future research directions do the authors suggest?

 Based on my reading of the paper, some key future research directions suggested by the authors include:

- Developing more efficient methods for finding the sparse, trainable subnetworks (winning lottery tickets) early in training. The iterative magnitude pruning (IMP) method used in this paper requires training the full network multiple times, which is computationally expensive. The authors suggest investigating techniques like SNIP that can identify winning tickets after only seeing a small portion of the data.

- Further exploring the connection between subnetwork stability and accuracy. The results in this paper suggest a link between higher stability and improved accuracy, but more work is needed to precisely characterize this relationship. This could lead to new techniques that directly optimize for stability when pruning. 

- Studying the opportunity to prune large-scale networks later than initialization but still early in training. The authors' rewinding experiments reveal some potential to prune productively shortly after initialization. More work could uncover better moments or techniques for very early pruning.

- Investigating whether the insights apply to other neural network architectures and domains beyond image classification. This paper focuses on convolutional networks for vision, but the lottery ticket hypothesis may generalize more broadly.

- Developing methods to maintain sparse networks dynamically during training. The current work looks at identifying a fixed sparse structure, but allowing the network topology to evolve during training could lead to further gains.

In summary, the main future directions are around developing better techniques for early pruning based on the insights around stability, timing, and the lottery ticket hypothesis revealed in this work. Applying and extending these ideas to other models and domains is also suggested.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the paper:

This paper modifies the iterative magnitude pruning (IMP) technique to find sparse, trainable subnetworks of neural networks. The authors find that IMP struggles to find winning tickets (sparse subnetworks that can train to match the original network's accuracy) in deeper networks when pruning precisely at initialization. To address this, they modify IMP to search for subnetworks that could be obtained by pruning early in training rather than at iteration 0. With this change, IMP can find winning tickets with high sparsity (e.g. 80%) on large networks like ResNet-50 trained on ImageNet. The authors introduce the concept of subnetwork stability to explain why pruning slightly later yields better subnetworks - pruning later results in subnetworks that reach more similar weights to the original network and do so more consistently. Overall, this work demonstrates the potential to prune large networks early in training while maintaining accuracy, and provides new insights into the underlying mechanics through stability analysis.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the key points from the paper:

This paper proposes modifications to the iterative magnitude pruning (IMP) method to find sparse, trainable subnetworks of neural networks. IMP involves iteratively pruning the lowest magnitude weights from a trained network and resetting the remaining weights to their initial values from before training. The authors find that IMP struggles to find small subnetworks capable of matching the original network's accuracy when applied at initialization for deeper networks. 

To address this, the authors propose rewinding pruned weights to earlier points in training (e.g. after 0.1% to 7% of training) rather than resetting to initialization. They show this rewinding approach allows IMP to find highly sparse subnetworks of deeper networks like Resnet-50 and Inception-v3 that match the full networks' accuracy on ImageNet. The authors argue rewinding works because it finds subnetworks during points in training where they have greater stability, meaning they reach similar optima to the full network and do so consistently despite noise. Overall, this work provides new insights into effectively pruning networks early in training.


## Summarize the main method used in the paper in one paragraph.

 The paper proposes modifying iterative magnitude pruning (IMP) to find sparse, trainable subnetworks in large vision networks. IMP typically prunes at initialization time and resets the remaining weights to their initial values, but struggles to find good subnetworks in large networks. The authors modify IMP to rewind the weights to an earlier point in training (e.g. 10 iterations in) rather than all the way back to initialization before retraining the subnetwork. They find that this allows IMP to identify much sparser trainable subnetworks in large networks like ResNet-50 and Inception-v3. The key insight is that networks contain trainable subnetworks just a small way into training, even if they do not contain them at initialization. Rewinding weights to these early points allows IMP to leverage them.


## What problem or question is the paper addressing?

 Based on my reading, the key points of this paper are:

- The paper is investigating the "lottery ticket hypothesis", which suggests that typical large neural networks contain small subnetworks ("winning tickets") that can be trained in isolation to match the full network's accuracy. 

- Prior work has shown this hypothesis holds for small vision models, but struggles to find winning tickets in larger/deeper networks. 

- This paper argues that prior efforts have failed on deeper networks because they prune precisely at initialization (iteration 0). Instead, the authors find that pruning after a short period of training (e.g. 0.1-7% through training) reveals winning tickets in deeper networks.

- The paper introduces the concept of "stability" - both stability to pruning noise and data order noise - to study why pruning later helps find winning tickets. Rewinding later in training improves both stability metrics, which correlates with finding higher accuracy subnetworks.

- Experiments show this "rewinding" approach reveals winning tickets with high sparsity (e.g. 80%) in deep networks like ResNet-50 and Inception-v3 on ImageNet, which prior work could not do by pruning only at initialization.

- The key conclusion is that opportunities exist to prune large models early in training (though not exactly at iteration 0) while maintaining accuracy, reducing training costs. The concept of stability offers insights into why pruning later helps find these subnetworks.

In summary, the key question addressed is why the lottery ticket hypothesis holds for small networks but not deeper ones, and the answer proposed is that pruning a bit later in training reveals winning tickets in deeper networks too. The paper expands our understanding of opportunities to prune early in training.
