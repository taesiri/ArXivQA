# [Stabilizing the Lottery Ticket Hypothesis](https://arxiv.org/abs/1903.01611)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the main research questions/hypotheses appear to be:1) The lottery ticket hypothesis conjectures that typical neural networks contain small subnetworks that can train to similar accuracy in a commensurate number of steps. However, prior work has struggled to find such subnetworks, or "winning tickets", in larger and deeper networks. This paper investigates why this is the case and how the lottery ticket hypothesis can be adapted for deeper networks. 2) The paper hypothesizes that prior efforts have struggled on deeper networks because they focus on pruning precisely at initialization (iteration 0). The paper proposes modifying the iterative magnitude pruning (IMP) method to search for winning tickets that could have been obtained by pruning early in training rather than exactly at initialization.3) The paper introduces the concepts of subnetwork "stability to pruning" and "stability to data order" to characterize how robust subnetworks are to noise and how consistently they can reach a good solution. It hypothesizes that IMP struggles at iteration 0 because the subnetworks lack stability, and that delaying pruning improves stability and thereby enables winning tickets to be found.In summary, the main hypothesis is that by pruning slightly later in training rather than exactly at initialization, more stable winning tickets can be found in deeper networks, providing evidence for the lottery ticket hypothesis in larger models. The paper aims to test this hypothesis and provide insights into why rewinding the pruning iteration improves subnetwork stability and accuracy.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contributions are:- The authors modify the iterative magnitude pruning (IMP) algorithm to "rewind" pruned weights to earlier points in training rather than resetting them to initialization. They show this allows IMP to find winning tickets (highly sparse subnetworks that can train to match the full network accuracy) in deeper neural networks where it fails with resetting to initialization.- They demonstrate that rewinding to early points in training (0.1-7% through) allows finding winning tickets with high sparsity (50-99% weights pruned) in deep networks for ImageNet, which has not been shown before. - They analyze the stability of the IMP subnetworks and show that stability to pruning and data order improve rapidly as the rewinding point moves past initialization. They hypothesize this improved stability explains why rewinding enables finding winning tickets.- Based on these findings, they revise the lottery ticket hypothesis to consider rewinding to early training iterations rather than precisely initialization. Overall, the results provide new insights into the opportunity to prune large neural networks early in training.In summary, the main contribution is showing that rewinding pruned weights to small offsets from initialization enables finding highly sparse subnetworks that match full network accuracy on larger and deeper models, which expands our understanding of the lottery ticket hypothesis. The stability analysis also provides new perspective on why rewinding helps and how subnetworks evolve early in training.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper proposes modifying the iterative magnitude pruning (IMP) method to search for sparse subnetworks early in training rather than precisely at initialization, finding this allows IMP to identify small, trainable subnetworks in larger vision models where it previously failed, which they argue reveals opportunities to successfully prune much earlier than previously thought if techniques can exploit rapid initial improvements in subnetwork stability.


## How does this paper compare to other research in the same field?

This paper builds on prior work on network pruning and the lottery ticket hypothesis. Here are some key points of comparison to related work:- The lottery ticket hypothesis (LTH) proposed that dense neural networks contain small subnetworks ("winning tickets") that can train to the same accuracy as the original network. This paper argues that rather than existing strictly at initialization, these winning tickets emerge shortly after the start of training. This modifies the original LTH.- Prior work on the LTH and pruning focused on small vision tasks. This paper scales up the analysis to larger networks and datasets like ImageNet and demonstrates the utility of rewinding/late resetting in that setting.- Techniques like gradual pruning and dynamic sparse training also aim to find compact subnetworks, but they maintain and train large portions of the network before pruning. This paper prunes much earlier while still matching accuracy.- Methods like SNIP and network slimming prune before training, but they reset weights to initialization and do not consider later reset points. The rewinding approach here outperforms those techniques.- Overall, this paper expands our understanding of when and how pruning can successfully produce trainable subnetworks. The late resetting modification enables pruning earlier in training and to higher sparsity levels than prior approaches.In summary, this work shifts the perspective on when winning tickets emerge and highlights the value of stability in identifying trainable subnetworks early on large-scale tasks. It demonstrates opportunities to find sparse architectures more efficiently than prior techniques allowed.
