# [Stabilizing the Lottery Ticket Hypothesis](https://arxiv.org/abs/1903.01611)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the main research questions/hypotheses appear to be:1) The lottery ticket hypothesis conjectures that typical neural networks contain small subnetworks that can train to similar accuracy in a commensurate number of steps. However, prior work has struggled to find such subnetworks, or "winning tickets", in larger and deeper networks. This paper investigates why this is the case and how the lottery ticket hypothesis can be adapted for deeper networks. 2) The paper hypothesizes that prior efforts have struggled on deeper networks because they focus on pruning precisely at initialization (iteration 0). The paper proposes modifying the iterative magnitude pruning (IMP) method to search for winning tickets that could have been obtained by pruning early in training rather than exactly at initialization.3) The paper introduces the concepts of subnetwork "stability to pruning" and "stability to data order" to characterize how robust subnetworks are to noise and how consistently they can reach a good solution. It hypothesizes that IMP struggles at iteration 0 because the subnetworks lack stability, and that delaying pruning improves stability and thereby enables winning tickets to be found.In summary, the main hypothesis is that by pruning slightly later in training rather than exactly at initialization, more stable winning tickets can be found in deeper networks, providing evidence for the lottery ticket hypothesis in larger models. The paper aims to test this hypothesis and provide insights into why rewinding the pruning iteration improves subnetwork stability and accuracy.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contributions are:- The authors modify the iterative magnitude pruning (IMP) algorithm to "rewind" pruned weights to earlier points in training rather than resetting them to initialization. They show this allows IMP to find winning tickets (highly sparse subnetworks that can train to match the full network accuracy) in deeper neural networks where it fails with resetting to initialization.- They demonstrate that rewinding to early points in training (0.1-7% through) allows finding winning tickets with high sparsity (50-99% weights pruned) in deep networks for ImageNet, which has not been shown before. - They analyze the stability of the IMP subnetworks and show that stability to pruning and data order improve rapidly as the rewinding point moves past initialization. They hypothesize this improved stability explains why rewinding enables finding winning tickets.- Based on these findings, they revise the lottery ticket hypothesis to consider rewinding to early training iterations rather than precisely initialization. Overall, the results provide new insights into the opportunity to prune large neural networks early in training.In summary, the main contribution is showing that rewinding pruned weights to small offsets from initialization enables finding highly sparse subnetworks that match full network accuracy on larger and deeper models, which expands our understanding of the lottery ticket hypothesis. The stability analysis also provides new perspective on why rewinding helps and how subnetworks evolve early in training.
