# [Can LLMs Generate Architectural Design Decisions? -An Exploratory   Empirical study](https://arxiv.org/abs/2403.01709)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Architectural Decision Records (ADRs) are important for documenting design decisions in software projects, but adoption has been slow. Generating ADRs manually is time-consuming and disruptive to architects' workflow. 

- Recent advances in large language models (LLMs) present an opportunity to automate parts of the ADR generation process, but their effectiveness for this task is unexplored.

Methodology:
- The authors conduct an empirical study to evaluate whether LLMs can generate the "Decision" component of ADRs based on the provided "Context." 

- They employ multiple well-known LLMs (GPT, T5) of varying sizes and utilize zero-shot prompting, few-shot learning, and fine-tuning approaches.

- 95 real-world ADR sample pairs of Context and Decision are gathered from 5 GitHub repositories as experimental data.

Results:
- Large LLMs like GPT-4 can generate pertinent Decisions in 0-shot setting, but still fall short of human performance. 

- Smaller, more affordable LLMs like text-davinci achieve comparable few-shot results.  

- Fine-tuned smaller models like Flan-T5 demonstrate strong capability after specialization, despite requiring minimal infrastructure.

Conclusions:
- Study suggests LLMs can assist architects in ADR generation, but more research needed to attain human-level automated generation.

- Fine-tuned compact models emerge as promising substitutes for large generic LLMs in specialized use cases like ADR generation.

Main Contributions:
- First empirical study evaluating LLMs for generating Design Decisions in ADRs
- Demonstrates capability of LLMs in assisting ADR documentation
- Identifies smaller fine-tuned models as alternatives to extensive LLMs


## Summarize the paper in one sentence.

 This paper performs an exploratory study on using large language models with zero-shot, few-shot, and fine-tuning approaches to generate the design decisions section of architectural decision records, finding that while the models show promise, further research is needed to reach human-level performance.


## What is the main contribution of this paper?

 The main contribution of this paper is an exploratory empirical study investigating the feasibility of using large language models (LLMs) to generate the "Design Decisions" component of Architecture Decision Records (ADRs), given the decision "Context". 

Specifically, the paper:

- Explores generating Design Decisions using 0-shot, few-shot, and fine-tuning approaches with various GPT and T5-based LLMs
- Finds that models like GPT-3.5, GPT-4, and T0 can effectively generate relevant Design Decisions in 0-shot settings, although they fall short of human-level quality
- Shows smaller models like text-davinci-003 can achieve comparable performance to larger models through few-shot learning
- Demonstrates that smaller fine-tuned models like Flan-T5 can produce results on par with extensive cloud-based LLMs
- Concludes that while not fully reliable, LLMs can assist architects in documenting and formulating Design Decisions for ADRs

The paper provides an initial investigation of leveraging LLMs for automated ADR generation. It suggests promise in this application but indicates further research is needed to reach human-level generative capabilities. The sharing of scripts also enables further experiments by the community.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the main keywords or key terms associated with it are:

- ADR (Architecture Decision Record)
- LLM (Large Language Model) 
- Zero-shot learning
- Few-shot learning
- Fine-tuning
- Text generation
- Architecture Design Decision (ADD) 
- Architectural Knowledge Management (AKM)
- GPT (Generative Pre-trained Transformer)
- T5
- BERTScore
- Flan-T5

The paper explores using different types of large language models (LLMs) like GPT and T5 to automatically generate the design decisions section of architectural decision records (ADRs). It compares zero-shot, few-shot, and fine-tuning approaches for enabling the LLMs to generate text. Metrics like BERTScore are used to evaluate the quality of the generated text. So these are some of the key terms that summarize what the paper is about.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in this paper:

1. What was the motivation behind exploring the use of LLMs for generating architectural design decisions? Why is there a need for automating this process?

2. What were the key criteria used for selecting the LLMs evaluated in this study? How representative is the chosen set of models of the broader landscape of available LLMs?

3. The study relies primarily on the BERTScore metric for evaluation. What are the advantages and potential limitations of using BERTScore over other metrics? Could the choice of metrics impact the conclusions?

4. What were some of the challenges faced in gathering real-world ADR data for model training and evaluation? Could the limited data have impacted model performance or generalizability of findings? 

5. For the few-shot experiments, what guidelines were followed in selecting the gold sample ADRs used for in-context learning? Could the choice of samples have biased the models?

6. What regularization, optimization, and hyperparameter tuning strategies were employed during the fine-tuning experiments? Were multiple configurations evaluated?

7. The study concludes that small fine-tuned models can substitute larger models in certain scenarios. What criteria should architects consider when deciding between these models?

8. What custom prompts were designed for the different models? What insights were gained into the impact of prompting techniques on output quality?

9. How sensitive are the findings to the specific formatting conventions used for the ADR markdown? Could adherence to structure outweigh content accuracy?

10. What guidelines would you propose for architects aiming to employ LLMs for generating design decisions based on the findings? Which models and approaches seem most promising?
