# Improving alignment of dialogue agents via targeted human judgements

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the main research question appears to be: How can targeted human judgements be used to improve the alignment of dialogue agents via reinforcement learning from human feedback?More specifically, the central hypotheses seem to be:1) Breaking down requirements for good dialogue behavior into specific natural language rules will allow for more targeted human judgements of agent behavior. This can enable more efficient training of rule-conditional reward models. 2) Having the agent provide supporting evidence alongside its statements will improve the correctness and verifiability of its responses. 3) Combining targeted rule judgements and overall response preferences with reinforcement learning can yield a dialogue agent that is preferred to baselines while also being more resilient to adversarial human probing.So in summary, the central research questions revolve around using targeted human feedback and inline evidence to improve the alignment of dialogue agents via multi-objective reinforcement learning. The key hypotheses are that breaking down rules, providing evidence, and combining preferences with rules in RL can yield improved alignment.


## What is the main contribution of this paper?

The main contribution of this paper seems to be developing a dialogue agent called Sparrow that is trained via reinforcement learning from human feedback to be more helpful, correct, and harmless compared to baseline language models. Some key aspects are:- Using targeted human judgements on specific rules (e.g. do not threaten, do not give medical advice) to get more fine-grained feedback and train rule-based classifiers. This allows more efficient training compared to just using a generic "safe/unsafe" label.- Incorporating evidence from web searches to make factual claims verifiable and improve correctness. When providing evidence, Sparrow's responses are found to be supported 78% of the time.- Training via a multi-objective RL scheme that optimizes for human preferences (helpfulness) while minimizing rule violations (harmlessness). This results in improved preference rates while reducing rule violations compared to baselines.- Detailed analysis of the impact of the training methods on distributional fairness issues like stereotyping, finding that instance-level rules do not fully resolve these.So in summary, the main contribution seems to be developing a dialogue agent that can engage in information-seeking conversations, while being optimized for helpfulness, correctness, and safety through targeted human judgements and web evidence. The analysis also highlights both the successes and limitations of this approach.
