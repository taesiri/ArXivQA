# [DPPA: Pruning Method for Large Language Model to Model Merging](https://arxiv.org/abs/2403.02799)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Model merging aims to combine multiple fine-tuned models from different domains into one model to enhance its capabilities across domains. However, parameter conflicts during merging causes performance degradation. 
- Existing methods resolve this issue during merging, but latest works focus on mitigating it during pruning. 
- Recent DARE method works for simple fine-tuned models, but struggles for complex models with significant parameter deviations from base model.

Proposed Solution:
- Proposes a two-stage DPPA method to tackle merging of complex fine-tuned models.

1) Dynamically Pruning (DP): 
- Improved magnitude pruning approach to boost performance at higher pruning rates.  
- Dynamically adjusts pruning rate based on importance of different linear layers rather than whole layers.

2) Dynamically Partition Amplification (DPA):
- Rescaling strategy that dynamically amplifies partitions of parameters based on significance.  
- Tackles issue of DARE not working well for largely deviated parameters.

Main Contributions:
- Retains only 20% domain-specific parameters yet achieves comparable performance to methods retaining 90% parameters.
- Displays over 20% improvement in model merging performance compared to prior SOTA method.
- Generalizable rescaling strategy in DPA boosts performance when applied to DARE.
- Provides analysis and insights into abnormal behaviour in law domain and effectiveness of solutions.

In summary, it proposes an effective two-stage pruning method to tackle the challenging problem of merging complex fine-tuned models from multiple domains, outperforming prior state-of-the-art.
