# [NIRVANA: Neural Implicit Representations of Videos with Adaptive   Networks and Autoregressive Patch-wise Modeling](https://arxiv.org/abs/2212.14593)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is: How can we develop an implicit neural representation (INR) method for video compression that is more efficient, flexible, and scalable than prior approaches? The key hypotheses appear to be:1) An autoregressive, patch-wise prediction model will be more efficient by exploiting spatio-temporal redundancies in videos, compared to pixel-wise or frame-wise prediction models. 2) Modeling videos as groups of frames with separate networks, initialized from previous groups, will allow the method to scale to longer videos without degradation.3) Performing quantization during training will enable adaptive compression rates tailored to each video's complexity, avoiding slow post-hoc steps.4) The proposed method, NIRVANA, will achieve improved encoding speeds, decoding speeds, scalability, and adaptive compression while maintaining competitive rate-distortion performance compared to prior video INR methods like SIREN and NeRV.In summary, the central goal is developing a flexible and efficient video INR framework through technical contributions like autoregressive modeling, patch predictions, and integrated quantization. The key hypothesis is that this approach will outperform prior video INRs in various aspects like speed and adaptability while achieving good compression rates and visual quality.


## What is the main contribution of this paper?

The main contribution of this paper is proposing NIRVANA, a neural implicit representation method for video compression. The key ideas are:- Using patch-wise prediction instead of pixel-wise or full frame prediction. This allows exploiting spatial redundancy while being flexible to different resolutions. - Using an autoregressive training approach where separate networks are fit to groups of frames (clips) in a video, with each network initialized from the previous one. This exploits temporal redundancy and allows scaling to arbitrary video lengths.- Quantizing the network weights during training itself using an entropy loss, avoiding need for post-hoc pruning/quantization. This adapts the compression rate to the complexity of each video.- The method achieves 12x faster encoding than prior work NeRV with comparable quality and bitrate on benchmark datasets. It also scales better to higher resolutions and longer videos.In summary, the key novelty is the autoregressive patch-wise modeling approach combined with in-training quantization, which allows spatially/temporally flexible and efficient video compression using implicit neural representations.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper proposes a new neural network-based approach called NIRVANA for efficiently compressing videos by modeling them with separate networks for groups of frames in an autoregressive and patch-wise manner to exploit spatio-temporal redundancy.
