# [DiffTAD: Temporal Action Detection with Proposal Denoising Diffusion](https://arxiv.org/abs/2303.14863)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question seems to be how to formulate temporal action detection (TAD) as a diffusion-based generative modeling task. Specifically, the paper proposes a new TAD model called DiffTAD that treats TAD as a conditional denoising diffusion process that generates accurate action proposals from random/noisy proposals. 

The key ideas and contributions appear to be:

- Formulating TAD as diffusion-based generative modeling, which is a new perspective compared to prior TAD methods based on discriminative learning.

- Designing DiffTAD using a transformer decoder as the denoiser within a single-stage detection framework like DETR. This allows bypassing issues with two-stage pipelines like R-CNN.

- Introducing a temporal location query design and cross-step selective conditioning to enable efficient diffusion-based inference for TAD.

- Demonstrating state-of-the-art TAD performance on ActivityNet and THUMOS benchmarks compared to both generative and discriminative baselines.

So in summary, the central hypothesis is that reformulating TAD as a conditional denoising diffusion task can achieve strong results by reversing the diffusion process to generate accurate proposals. The key novelty is the diffusion perspective and how it is adapted to TAD using the proposed DiffTAD model.


## What is the main contribution of this paper?

 Based on my reading, the main contributions of this paper appear to be:

- Proposes a new framework called DiffTAD (Diffusion Temporal Action Detection) for temporal action detection in videos. DiffTAD formulates TAD as a denoising diffusion process that generates accurate action proposals from noisy/random proposals in a generative manner. 

- Integrates denoising diffusion with a transformer decoder architecture (e.g. DETR) for TAD. Using the decoder as a denoiser helps solve the typical slow convergence issue with Transformer decoders.

- Introduces a cross-timestep selective conditioning mechanism during inference that minimizes redundancy and regulates the diffusion direction by selectively conditioning the next step based on similarity and overlap with the reference segments. This improves efficiency.

- Achieves state-of-the-art performance on ActivityNet and THUMOS benchmarks compared to previous TAD methods. Demonstrates the potential of diffusion models and generative learning for temporal action detection.

- Shows properties like faster convergence, flexible proposal sizes, and progressive refinement enabled by the diffusion formulation. The model can be used for various speed-accuracy trade-offs without retraining.

Overall, the key novelty seems to be formulating TAD as a diffusion-based generative task and integrating it effectively with a Transformer decoder architecture to achieve superior performance and nice properties compared to prior discriminative TAD methods.
