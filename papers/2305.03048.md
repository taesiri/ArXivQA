# [Personalize Segment Anything Model with One Shot](https://arxiv.org/abs/2305.03048)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the key research focus of this paper appears to be developing efficient methods to personalize the Segment Anything Model (SAM) for segmentation of specific visual concepts using minimal training data. Specifically, the paper introduces two approaches:1) PerSAM - A training-free personalization approach that customizes SAM using only a single reference image and mask indicating the target visual concept. It calculates a location prior to guide SAM's attention, incorporates target semantics into the prompts, and refines the segmentation in a cascaded manner.2) PerSAM-F - A fine-tuning variant that adapts SAM by learning relative weights for multi-scale mask outputs using only 2 trainable parameters. This allows selecting the optimal mask scale for different concepts. The central hypothesis seems to be that the proposed PerSAM and PerSAM-F can efficiently specialize the generalist SAM model for personalized segmentation using very limited data and computation resources. The paper aims to demonstrate that competitive personalized segmentation can be achieved without extensive fine-tuning.In summary, the key research focus is on efficiently personalizing a pre-trained segmentation foundation model with minimal training data and parameters. The two proposed approaches PerSAM and PerSAM-F aim to validate the hypothesis that this is achievable.
