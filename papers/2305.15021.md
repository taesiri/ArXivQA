# EmbodiedGPT: Vision-Language Pre-Training via Embodied Chain of Thought

## What is the main contribution of this paper?

Based on my reading, the main contributions of this paper appear to be:1. The authors build an end-to-end multi-modal foundation model called EmbodiedGPT for embodied AI, which can perform tasks like embodied planning, embodied VQA, and embodied control. 2. They develop two new datasets - EgoCOT and EgoVQA - consisting of 200M annotated videos from Ego4D with corresponding detailed planning instructions and VQA data. The datasets go through machine generation, semantics-based filtering, and human verification.3. They introduce a cost-effective training approach and a paradigm for extracting task-relevant features from the LLM-generated planning queries. This forms a closed loop between high-level planning and low-level control.4. Through extensive experiments, they demonstrate EmbodiedGPT's effectiveness on various embodied tasks like embodied control, planning, video captioning, and VQA. Significantly, it improves the success rate on the Franka Kitchen and Meta-World control benchmarks compared to prior methods.In summary, the main contribution appears to be proposing the EmbodiedGPT model along with associated datasets and training techniques to enable stronger performance on embodied AI tasks spanning high-level planning to low-level control. The closed-loop approach and ability to extract task-relevant features seem to be key innovations.
