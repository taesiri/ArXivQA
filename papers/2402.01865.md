# [What Will My Model Forget? Forecasting Forgotten Examples in Language   Model Refinement](https://arxiv.org/abs/2402.01865)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Language models deployed in the real world make errors. Fixing errors by updating the models leads to catastrophic forgetting - the updated models make mistakes on examples they previously learned correctly during pre-training or instruction tuning.
- Randomly replaying upstream data to mitigate forgetting has limitations like high variance and poor controllability. 

Objectives: 
- Forecast which upstream examples will be forgotten when fixing an error to improve controllability and interpretability of the replay process. 
- Understand what interactions between an online learned example and upstream example contribute to forgetting.

Methods:
- Propose a partially interpretable logit-based method: Changes in logit scores of online learned examples proportionally transfer to upstream examples, causing forgetting. The method learns to predict degree of logit change transfer.
- Propose a blackbox representation-based method: Learns representations of example pairs and classification layer to forecast forgetting. Incorporates frequency prior of examples being forgotten.

Key Results:
- Logit-based method effective for BART but not for T5. Blackbox representation-based method consistently effective.
- Replaying forecasted examples reduces catastrophic forgetting, achieving better performance than alternative selection strategies.
- Forecasting methods computationally efficient - do not require repetitive inference on upstream data.

Main Contributions:
- Formalize and evaluate forecasting of examples forgotten during language model refinement.
- Propose interpretable and blackbox methods to forecast forgetting. 
- Demonstrate utility of forecasting by replaying forecasted examples to reduce catastrophic forgetting.


## Summarize the paper in one sentence.

 This paper proposes methods to forecast which upstream examples will be forgotten when fixing errors in pretrained language models, in order to improve model refinement through targeted data replay.


## What is the main contribution of this paper?

 According to the paper, the main contributions are threefold:

(1) It formulates a novel problem of forecasting forgotten examples in language model refinement, where the goal is to predict which upstream pretraining examples will be forgotten when fixing errors in the language model. 

(2) It proposes both a partially interpretable logit-based forecasting method and a black-box representation-based forecasting method for predicting forgotten examples. The logit-based method models the transfer of logit changes from the online learned example to the pretraining example.

(3) It demonstrates the practical utility of forecasting forgotten examples by showing reduced catastrophic forgetting when replaying the forecasted examples during model refinement, compared to replaying random examples. The forecasting models are also shown to generalize to continually fixing multiple errors.

In summary, the key contribution is defining and providing solutions to the new problem of forecasting the examples that will be forgotten during language model refinement, in order to improve the model refinement process. The proposed forecasting models are shown to be effective for reducing forgetting.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper abstract and introduction, some of the key terms and concepts associated with this paper include:

- Language model refinement
- Catastrophic forgetting
- Model updates
- Forecasting forgotten examples
- Logit-change transfer
- Partially interpretable forecasting model
- Representation-based forecasting model
- Reduced catastrophic forgetting
- Computational efficiency

The paper focuses on forecasting examples that will be forgotten when updating or refining pretrained language models to fix errors, in order to improve the controllability and interpretability of the model refinement process. Key ideas include modeling the "logit-change transfer" between an online learned example and a pretraining example to forecast forgetting, as well as a representation-based forecasting model. Experiments demonstrate reduced catastrophic forgetting by replaying forecasted forgotten examples.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 potential in-depth questions about the method proposed in this paper:

1. The paper proposes both a partially interpretable logit-based method and a blackbox representation-based method for forecasting forgetting. What are the key differences in formulation between these two methods and what are the relative tradeoffs?

2. The logit-based method seems to work well for BART but not as well for T5 models. What factors might contribute to this difference in performance across model architectures? 

3. The paper argues the logit-based method is "partially interpretable" in explaining how logit changes are transferred from one example to another. But how interpretable is this method really to a human without deep knowledge of neural networks?

4. For the representation-based forecasting method, what is the intuition behind using the inner product between example representations as the basis for prediction? How might the model leverage this to capture latent interactions?

5. The frequency prior is shown to improve performance of the representation-based method. But doesn't this detract from the model's ability to capture interactions between examples? Why or why not?

6. The forecasting models are static while the language models being updated are continually changing. How might the shifting language model dynamics affect forecasting performance over time? 

7. What assumptions does the logit-based forecasting method make about the learning dynamics within the base language model? When might these assumptions fail to hold?

8. Could the forecasting models be recursively updated alongside the base language model during continual refinement? What challenges would this introduce?

9. How sensitive are the forecasting methods to hyperparameters used during language model updating, such as learning rate or number of update steps?

10. What other approaches could be explored for forecasting examples likely to be forgotten during language model refinement?
