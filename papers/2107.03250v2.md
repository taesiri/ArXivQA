# [Understanding Intrinsic Robustness Using Label Uncertainty](https://arxiv.org/abs/2107.03250v2)

## What is the central research question or hypothesis that this paper addresses?

 After reading through the paper, it seems the main research question it addresses is:

How can we better estimate the intrinsic robustness limit for image classification tasks by incorporating label information? 

The key points are:

- Prior work has used concentration of measure to estimate intrinsic robustness limits. However, this ignores label information, which is crucial for supervised learning tasks. 

- The paper argues standard concentration measures fail to provide a realistic intrinsic robustness limit. They are defined only based on the input distribution, without considering learnability of the classification problem.

- The authors propose incorporating label uncertainty into concentration measures. They introduce a notion of label uncertainty for an input region. 

- They provide an algorithm to empirically estimate a label uncertainty constrained concentration measure from data samples.

- Experiments on CIFAR-10 datasets demonstrate their proposed approach provides tighter and more realistic intrinsic robustness limits compared to prior methods.

- The results suggest intrinsic label uncertainty, rather than just concentration of the input space, may explain fundamental limits on adversarial robustness.

In summary, the key hypothesis is that incorporating label information into concentration of measure will enable more accurate estimation of intrinsic robustness limits for classification problems. The experiments support this claim.


## What is the main contribution of this paper?

 Based on my reading, the main contributions of this paper are:

1. It identifies an insufficiency with the standard notion of concentration of measure for characterizing the intrinsic robustness limit of a classification problem. In particular, it argues that standard concentration ignores the label information, which is essential for supervised learning tasks.

2. It proposes a new notion of "label uncertainty" to capture the average uncertainty in the label assignments for a subset of inputs. 

3. It incorporates this label uncertainty into the standard concentration measure to get a more realistic characterization of intrinsic robustness. It does this by constraining the search space in the standard concentration problem to only subsets that have high label uncertainty. 

4. It provides an empirical estimator for the proposed label uncertainty constrained concentration measure, and theoretically analyzes its asymptotic convergence. 

5. It demonstrates through experiments on CIFAR-10 that error regions of state-of-the-art classifiers have higher label uncertainty than random subsets. This supports the need to account for label uncertainty.

6. The proposed method produces lower and likely more accurate intrinsic robustness estimates for CIFAR-10 compared to prior work. The gap between these estimates and the robustness of state-of-the-art classifiers suggests label uncertainty may explain adversarial vulnerability more than concentration.

7. It shows abstaining on high label uncertainty inputs can improve classifier accuracy and robustness, suggesting this as a promising direction.

In summary, the key novelty is in incorporating label uncertainty into the notion of concentration to get better intrinsic robustness estimates. The paper makes both theoretical and empirical contributions demonstrating the importance of considering labels for understanding robustness limits.
