# [MSViT: Dynamic Mixed-Scale Tokenization for Vision Transformers](https://arxiv.org/abs/2307.02321)

## What is the central research question or hypothesis that this paper addresses?

This paper proposes a dynamic mixed-scale tokenization scheme for Vision Transformers (ViTs) called MSViT. The central hypothesis is that encoding uniform background regions of an image with coarse tokens and more cluttered/content-rich regions with fine tokens can improve the accuracy-complexity trade-off of ViTs. The key research questions addressed are:- How to design an efficient conditional gating mechanism to select the optimal token scale for each image region?- How to train this gating module jointly with the ViT backbone in an end-to-end manner?- How to control the learned distribution of token scales during training to avoid trivial solutions? - How to reduce the training overhead incurred by handling tokens at multiple scales?So in summary, the central goal is developing a dynamic mixed-scale tokenization scheme that can flexibly adapt the number of tokens per image based on its content, to improve efficiency of ViTs. The paper explores techniques to achieve this via a lightweight gating module, a generalized batch shaping loss, and adaptive trimming during training.
