# [Good Teachers Explain: Explanation-Enhanced Knowledge Distillation](https://arxiv.org/abs/2402.03119)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper "Good Teachers Explain: Explanation-Enhanced Knowledge Distillation":

Problem:
Knowledge distillation (KD) is a technique to compress large "teacher" models into smaller "student" models that can match the teacher's accuracy. However, recent work has shown that despite matching accuracy, students may not learn the same function or features as the teacher. This lack of "fidelity" can negatively impact users when models are updated, cause students to rely on spurious features instead of the "right reasons", and prevent students from maintaining the interpretability properties of teachers. 

Proposed Solution: 
This paper proposes "explanation-enhanced knowledge distillation" (e2KD) which adds a term to the KD loss that matches "explanations" between the teacher and student. Explanations refer to interpretation methods like Grad-CAM that highlight important features in the model's decision process. By explicitly matching explanations, e2KD aims to increase fidelity - making students right for the right reasons, focused on proper features, and maintaining interpretability.

Contributions:
1) Proposes the simple, model-agnostic e2KD method to improve KD fidelity by matching teacher-student explanations.

2) Shows e2KD significantly boosts accuracy and agreement between teacher and student, especially with limited training data. Helps students be right for the right reasons under distribution shift.

3) Demonstrates e2KD makes students learn similar explanations as teachers, exhibiting the same degree of interpretability. Works robustly across CNN and Vision Transformer models.

4) Introduces the idea of "frozen" explanations to reduce computational cost. Shows e2KD works even with approximate explanations computed only once.

5) Defines metrics to measure KD fidelity - accuracy, agreement, relying on proper features, maintaining interpretability. Evaluates e2KD across diverse settings like distribution shift and model architectures.

In summary, e2KD is a simple yet effective technique to create more faithful student models in knowledge distillation across accuracy, features, and interpretability - by encouraging students to learn from the teacher's explanations.
