# Towards Reasoning in Large Language Models: A Survey

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be: To what extent are large language models capable of reasoning, and how can we improve and evaluate their reasoning abilities?The paper provides a comprehensive review of the current state of research on reasoning abilities in large language models (LLMs). It summarizes techniques for eliciting and enhancing reasoning in LLMs, methods for evaluating reasoning capabilities, key findings from previous studies, and implications and open questions around this topic. The overarching focus seems to be on synthesizing the knowledge around reasoning in LLMs, reflecting on the extent of their capabilities, and outlining directions for future work.While the paper does not state an explicit central hypothesis, the underlying hypothesis appears to be:LLMs exhibit some emergent reasoning abilities when they reach sufficient scale, but the extent of their reasoning skills is still unclear. Further research is needed to analyze, improve, and evaluate reasoning in LLMs.The authors review work providing evidence both for and against LLMs' reasoning capabilities. They ultimately conclude that more research is needed to reach definitive conclusions, highlighting this as an important open problem in the field. The paper seems aimed at synthesizing current knowledge to guide future work on understanding and advancing reasoning in LLMs.


## What is the main contribution of this paper?

The main contributions of this paper are:1. It provides a comprehensive overview of the current state of knowledge on reasoning abilities in large language models (LLMs). The paper covers techniques for improving and eliciting reasoning in LLMs, methods and benchmarks for evaluating reasoning, key findings and implications from previous research, and discussion on limitations and future directions. 2. The paper engages in an insightful discussion about the extent to which LLMs are capable of reasoning. It reflects on whether LLMs are actually able to reason or are just generating responses that appear reasoning-like. The authors conclude that more research is needed to fully understand the reasoning capabilities of LLMs.3. The paper summarizes and synthesizes a broad range of studies on reasoning in LLMs. By providing a detailed literature review, it enables readers to gain an up-to-date understanding of this rapidly evolving research area. 4. The paper stimulates meaningful discussion on open questions related to reasoning in LLMs, such as the need for more realistic benchmarks and applications, improving reasoning capabilities, and properly evaluating whether models can truly reason.5. The paper identifies limitations of existing research, such as the focus on simple artificial tasks and the lack of in-depth analysis of reasoning process in LLMs. It provides suggestions for future work to advance knowledge in this field.In summary, the main contribution is providing a comprehensive up-to-date review of research on reasoning in LLMs, summarizing current knowledge, engaging in insightful discussion, and identifying open questions to stimulate future work on this important topic. The paper serves as a useful reference for researchers interested in reasoning abilities of large language models.
