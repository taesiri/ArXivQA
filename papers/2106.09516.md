# [Transductive Few-Shot Learning: Clustering is All You Need?](https://arxiv.org/abs/2106.09516)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the main contributions of this paper are:1. It proposes a general formulation for clustering and transductive few-shot learning that integrates prototype-based objectives (e.g. K-means, K-modes), Laplacian regularization, and supervision constraints. 2. It develops a concave-convex relaxation of the problem and derives an efficient block-coordinate bound optimization algorithm called SLK with convergence guarantees. The algorithm performs parallel updates of the assignment variables.3. It adapts the formulation for transductive few-shot learning tasks, where the goal is to classify query samples given a support set with a few labeled examples per class.4. It provides a thorough convergence analysis of SLK based on point-to-set maps and Cauchy sequences. 5. It evaluates SLK extensively on clustering and few-shot learning tasks, showing it is competitive in terms of accuracy and optimization quality while scaling to large problems.6. Surprisingly, it finds that basic clustering methods like K-means perform competitively to state-of-the-art transductive few-shot learning techniques on standard benchmarks, indicating potential limitations of current evaluations.In summary, the central hypothesis is that formulating few-shot learning as a constrained clustering problem, relaxing it appropriately, and optimizing it with a scalable block-coordinate approach can yield an effective and efficient solution for transductive few-shot tasks. The experiments aim to validate the viability of this approach against the state-of-the-art.


## What is the main contribution of this paper?

This paper proposes a general formulation and solution for transductive few-shot learning based on clustering. The main contributions are:- They formulate transductive few-shot learning as a constrained clustering problem with a Laplacian regularization term and supervision constraints from the support set. - They propose a concave-convex relaxation for the formulation and derive an efficient block-coordinate bound optimization algorithm called SLK to solve it. The algorithm has parallel updates and convergence guarantees.- They adapt the formulation for two clustering objectives - K-means (SLK-Means) and K-modes (SLK-MS).- They provide a thorough convergence analysis using point-to-set maps and prove the algorithm converges to a stationary point.- They evaluate the clustering methods extensively on several datasets and show they achieve competitive accuracy and optimization quality compared to baselines. - They apply SLK-Means and SLK-MS to few-shot learning tasks on miniImageNet, tieredImageNet and CUB datasets. Without any meta-learning, they show the methods outperform many recent meta-learning techniques by significant margins.- Surprisingly, they find even basic K-means achieves competitive accuracy, questioning the usefulness of complex few-shot learning methods. Their results indicate simple clustering could be an effective baseline for few-shot learning.In summary, the main contribution is a general clustering formulation for transductive few-shot learning that has an efficient optimization algorithm and achieves excellent empirical performance compared to recent few-shot learning techniques. The simplicity and effectiveness questions the need for complex meta-learning based techniques.
