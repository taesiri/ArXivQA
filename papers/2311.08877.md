# [Llamas Know What GPTs Don't Show: Surrogate Models for Confidence   Estimation](https://arxiv.org/abs/2311.08877)

## Summarize the paper in one sentence.

 The paper proposes using probabilities from surrogate language models to estimate confidence for large language models like GPT-4 and Claude that don't expose probabilities.


## Summarize the paper in one paragraphs.

 Here is a one paragraph summary of the paper:

The paper studies confidence estimation for large language models (LLMs) like GPT-4 and Claude-v1.3 that currently do not provide access to softmax probabilities. First, the authors examine eliciting confidences linguistically by prompting the model to assess its confidence, finding it works reasonably but leaves room for improvement. They then explore using a separate "surrogate" model where probabilities are available to evaluate the original model's confidence, finding this surprisingly outperforms linguistic confidences even from a weaker model. Combining linguistic confidences and surrogate model probabilities leads to the best results, outperforming prior work. The analysis suggests linguistic confidences are limited in resolution, but surrogate models can transfer uncertainties on examples. Even mixing a small fraction of surrogate probabilities helps separate identical linguistic confidences. Overall, the paper demonstrates that model probabilities enable better confidence estimates and that uncertainties can surprisingly transfer between very different models.


## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a high-quality paragraph summarizing the key points of the paper:

The paper studies confidence estimation for large language models (LLMs) like GPT-4 and Claude-v1.3, which currently do not provide access to softmax probabilities. First, the authors examine eliciting confidence linguistically by prompting the model to assign confidence scores, finding this gives reasonable but imperfect performance (80.5% AUC). Next, they surprisingly show that using a separate "surrogate" model like Llama 2, where probabilities are available, as a source of confidence scores for the target model  improves performance (82.1% AUC) over linguistic elicitation. Further gains come from mixing the linguistic and surrogate confidences (83.4% AUC), which also outperforms a concurrent self-consistency technique. Analysis indicates linguistic confidences are limited by coarse values, while surrogate models transfer well by identifying similar "difficult" examples. Overall, the work provides an effective method to estimate confidences for state-of-the-art LLMs using surrogate models and linguistic elicitation.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 The paper proposes methods to estimate confidence for large language models like GPT-4 that don't provide access to internal probabilities, using linguistic prompting and probabilities from surrogate models. The main findings are that surrogate model probabilities can provide better confidence estimates than a model's own linguistic confidence scores, and combining linguistic and surrogate confidences gives the best results.


## What is the central research question or hypothesis that this paper addresses?

 The central research question this paper addresses is how to produce good confidence estimates for large language models (LLMs) like GPT-4 and Claude-v1.3, which currently do not provide access to internal model probabilities. 

The key hypotheses tested are:

1) Linguistically eliciting confidence estimates from the models directly can provide reasonable but imperfect confidence estimates.

2) Using a separate "surrogate" model that does provide probabilities, even if it is weaker, can surprisingly provide better confidence estimates for the target model than its own linguistic confidences. 

3) Combining the linguistic confidences and surrogate model probabilities leads to the best confidence estimates, outperforming either alone.

The paper systematically tests these hypotheses across a range of models, datasets, and methods. The central goal is developing techniques to estimate confidence for LLMs that do not expose internal probabilities, with the key findings being that surrogate models and mixtures can improve over linguistic elicitation.


## What is the main contribution of this paper?

 The main contribution of this paper is proposing methods to estimate the confidence of large language models like GPT-4 and Claude-v1.3, which currently do not provide access to internal model probabilities. The key ideas proposed are:

1) Eliciting linguistic confidence scores by prompting the model to assess its own confidence. This provides reasonable confidence estimates but leaves room for improvement.

2) Using a separate "surrogate" model like Llama 2, where probabilities are available, to estimate the confidence of the original black-box model like GPT-4. Surprisingly, this gives better confidence estimates than the model's own linguistic confidences. 

3) Combining the linguistic confidences and surrogate model probabilities in a mixture model, which gives the best confidence estimates overall.

4) Analysis providing intuitions for why surrogate model probabilities can effectively estimate confidences, even for very different models. 

In summary, the main contribution is developing and analyzing methods to estimate confidences for large language models without access to probabilities, using linguistic elicitation and clever use of surrogate models. The proposed methods advance the state-of-the-art in confidence estimation for these black-box models.


## How does this paper compare to other research in the same field?

 Here are a few key ways this paper compares and contributes to other research on confidence estimation for large language models:

- Focus on Selective Classification/Confidence Estimation for Models Without Access to Probabilities: This paper focuses specifically on estimating confidence for large language models like GPT-4 and Claude that currently do not provide access to internal model probabilities. Many prior works rely on using model probabilities for confidence estimation, so this paper targets selective classification for models where that is not possible.

- Proposes and Evaluates Linguistic Confidence Elicitation: The paper systematically evaluates eliciting confidences linguistically by prompting the model, finding it works reasonably well but has limitations. Prior work has looked at linguistic elicitation of rationales or other info, but this paper specifically focuses on eliciting numerical confidence scores.

- Surrogate Model Approach Outperforms Linguistic Confidences: The paper shows that surprisingly, using a separate surrogate model for confidence estimation actually outperforms prompting the model itself for confidences. This is a novel approach not explored in prior work.

- Analyzes Complementarity of Linguistic and Surrogate Confidence Estimates: The paper demonstrates that linguistic and surrogate confidences provide complementary signals, and composing them further improves performance. Analysis provides insights into why this complementarity exists.

- Achieves SOTA Performance for Black Box Confidence Estimation: The proposed methods achieve state-of-the-art selective classification results for models like GPT-4 where probabilities are not available, advancing research on confidence estimation.

- Evaluates Extensively on Diverse Datasets: The paper tests on 12 datasets spanning different domains to show the general applicability of their confidence estimation techniques.

Overall, the paper makes solid contributions to confidence estimation for large language models by targeting black box settings, proposing effective techniques like surrogate modeling, and advancing SOTA through compositional confidence modeling. The analysis also provides useful insights into model uncertainties.


## What future research directions do the authors suggest?

 The authors suggest several promising future research directions based on the findings and analysis in their paper:

- Further study the transferability of model probabilities and representations between models, and leverage this transferability more to understand uncertainties and confidences of black-box models using white-box models.

- Develop more sophisticated methods to compose confidence signals from multiple models, since they find combining confidences from two models can improve confidence estimates. Extending this to more models could lead to further gains. 

- Improve linguistic confidences, which are currently limited in their variance and ability to distinguish between correct and incorrect answers. For example, by increasing the diversity of generated confidence scores.

- Apply similar confidence estimation techniques to other tasks beyond question answering where black-box model confidences need to be assessed, such as summarization, translation, and dialogue systems.

- Study whether fine-tuning models specifically for good confidence estimation and calibration could complement these methods and lead to further improvements, as suggested by some prior work.

- Examine if insights from this work could help make black-box model behaviors more interpretable and explainable in general.

The key high-level directions are better understanding and exploiting model transferability, combining signals from multiple models, improving linguistic confidences, and applying these ideas more broadly beyond question answering.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and concepts are:

- Large language models (LLMs) - The paper focuses on estimating confidence for large neural network language models like GPT-4 and Claude-v1.3.

- Confidence estimation - The main problem studied is how to estimate the confidence of LLMs on different inputs, especially when model probabilities are not available. 

- Linguistic confidence scores - Eliciting confidence scores by prompting the model to assess its certainty in its predictions.

- Surrogate models - Using another model that exposes probabilities, like Llama-2, to estimate confidence scores for models like GPT-4. 

- Mixture of models - Combining linguistic confidence scores and surrogate model probabilities leads to better confidence estimation.

- AUC, AUROC - Key metrics used to evaluate the quality of confidence scores, based on accuracy-coverage curves and true/false positive rates.

- Selective classification - The problem of deciding when a model should make a prediction versus abstain, based on estimated confidence.

So in summary, the key terms cover confidence estimation, linguistic prompting, surrogate models, combining confidence signals, and metrics for evaluating confidence quality. The overall goal is selective prediction for LLMs.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in the paper:

1. The paper proposes using surrogate model probabilities as a way to estimate confidence for models like GPT-4 that don't provide access to internal probabilities. Why do you think the probabilities from a separate, potentially weaker model can provide better confidence estimates than prompting the model itself for confidence linguistically? What might this suggest about the transferability of uncertainty signals between models?

2. The paper finds that combining linguistic confidences and surrogate model probabilities leads to better performance than either alone. Why might these two signals be complementary for confidence estimation? What are the potential limitations or weaknesses of each method that combining them might help address?

3. The analysis suggests that linguistic confidences are limited in their variability and ability to distinguish between examples. How might future work address this limitation to elicit more nuanced linguistic confidences? For example, could different prompting strategies yield more varied confidence scores? 

4. The paper hypothesizes that surrogate models can estimate confidence well for a target model when the two models struggle with semantically similar concepts. How exactly could you test whether this hypothesis explains the effectiveness of surrogate models? What kind of analysis could you do on the incorrect predictions to validate this claim?

5. The method of breaking ties by mixing in a small amount of surrogate probability performs remarkably well. Why might adding such a small signal from another model enhance discrimination ability so much? What does this suggest about the expressiveness and resolution of linguistic confidences?

6. The paper focuses on zero-shot evaluation of confidences without any training or fine-tuning. How might you adapt or extend these methods to allow some amount of training to further improve confidence modeling? What are the potential benefits and drawbacks of introducing training?

7. The proposed methods achieve good performance across a diverse set of 12 question answering datasets. How could you further analyze model performance to understand if there are particular types of questions or domains where these confidence modeling techniques are more or less effective?

8. The paper studies confidences for question answering tasks. How might you extend or adapt these methods for confidence estimation in other applications of large language models like summarization, translation, or dialog? What changes would need to be made?

9. The analysis studies correlation between models, but are there other ways to quantify or predict which models will make good surrogates beyond just correlation? What other analysis could give insight into which models have uncertainty estimates that transfer well?

10. The method composes confidence estimates from multiple models. What are other ways this idea could be extended, for example incorporating confidence estimates from >2 models? How could you effectively combine and weigh estimates from multiple sources?
