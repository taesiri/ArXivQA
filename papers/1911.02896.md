# [Contextualized Sparse Representations for Real-Time Open-Domain Question   Answering](https://arxiv.org/abs/1911.02896)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the key points of this paper are:- It proposes a contextualized sparse phrase representation called SPARC to improve phrase retrieval for open-domain question answering. - The main hypothesis is that augmenting existing dense phrase embeddings with a learned sparse representation can help better capture lexical information and disambiguate between similar entities.- SPARC learns sparse vectors for each phrase by using rectified self-attention on neighboring n-grams. This allows encoding lexical information with high dimensionality (n-gram vocabulary space) compared to prior work.- A kernel trick is used during training for efficiency, avoiding explicit mapping to the high-dimensional sparse space.- Experiments show SPARC improves the Dense Phrase Retrieval (DenSPI) method on SQuAD-Open and CuratedTREC by over 4% in accuracy, achieving state-of-the-art results.So in summary, the key hypothesis is that a contextualized sparse representation can enhance phrase embeddings for open-domain QA to better capture lexical semantics. The method proposed is SPARC, which learns high-dimensional sparse vectors using self-attention and a kernel trick. Experiments verify this hypothesis, showing improved accuracy compared to prior methods.


## What is the main contribution of this paper?

Based on my reading, the main contribution of this paper is proposing a method to learn contextualized sparse phrase representations for open-domain question answering. Specifically:- They introduce a novel method called Contextualized Sparse Representation (CoSPAR) to encode sparse phrase representations that dynamically capture important lexical information depending on the context. - They propose an efficient training strategy that uses a kernel trick to avoid explicitly mapping to a very high-dimensional sparse space. This allows them to scale up to n-gram vocabulary space.- They demonstrate the effectiveness of CoSPAR by augmenting a previous dense phrase retrieval model called DenSPI. Adding CoSPAR improves performance by over 4% on SQuAD-Open and CuratedTREC datasets compared to using just DenSPI.- Their model achieves new SOTA results on CuratedTREC compared to prior retrieve-and-read approaches, while being much faster. On SQuAD-Open they also outperform BERT-based pipeline models with over 100x speedup.In summary, the key contribution is developing an interpretable contextualized sparse phrase representation that encodes precise lexical cues and significantly boosts performance of real-time open-domain QA compared to prior methods. The proposed training strategy also allows scaling up sparse representations to n-gram vocabulary space.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper proposes a contextualized sparse phrase representation method called SPARC that improves a dense phrase retrieval model for open-domain question answering by augmenting phrase embeddings with rectified self-attention-based sparse vectors that focus on semantically related n-grams.


## How does this paper compare to other research in the same field?

Here are a few key ways this paper compares to other research in open-domain question answering:- The paper focuses on improving phrase retrieval for open-domain QA, while much previous work has focused on retrieve-and-read pipelines that first retrieve documents then read/comprehend them. This is a meaningful comparison since phrase retrieval aims to avoid the pipeline limitations.- To improve phrase retrieval, the paper proposes learning contextualized sparse phrase representations. This contrasts with prior work on sparse representations that use static methods like tf-idf or learn only low-dimensional sparse vectors. The contextualization and high dimensionality are novel.- For learning representations, the paper uses an efficient kernelized training method. This compares to other representation learning methods that map to sparse spaces directly. The kernelization enables handling the high dimensionality.- The experiments compare performance on open QA datasets to previous state-of-the-art pipeline and end-to-end methods. The proposed model achieves new SOTA results, demonstrating advantages over prior approaches.- The inference speed is compared and shown to be much faster than competing retrieve-and-read pipelines, which is an important advantage of the phrase retrieval approach.Overall, the comparisons highlight the novelty of the contextualized sparse phrase representations for improving phrase retrieval in open-domain QA, and the results demonstrate these representations outperform prior methods in both accuracy and speed. The paper makes good contributions to the field of open-domain QA research.


## What future research directions do the authors suggest?

Based on my reading of the paper, some of the main future research directions suggested by the authors include:- Extending the contextualized sparse representations to other challenging QA settings such as multi-hop reasoning. The current approach focuses on single-hop factoid questions, so extending it to handle more complex reasoning would be an interesting direction.- Building a full inverted index of the learned sparse representations. The authors suggest this could allow for more powerful Sparse-First Search (SFS), rather than just using the sparse representations as a re-ranking step.- Applying contextualized sparse representations to other domains beyond QA. The authors suggest it could be effective for tasks where precisely encoding similar but distinct features based on context is important, such as modeling structural similarities of chemical compounds.- Further closing the "decomposability gap" between the joint and decomposed representations on standard QA datasets like SQuAD. There is still a gap between the query-agnostic phrase representations and jointly encoding the passage and question.- Improving training efficiency and scalability. The kernelized training currently requires computing an NxM kernel matrix which can be expensive. Exploring ways to scale this up would enable training on even larger corpora.In summary, the main suggestions are around extending the representations to new tasks and settings, scaling up the training and indexing, and continuing to close the gap with joint representations to make the query-agnostic phrase representations even more effective.


## Summarize the paper in one paragraph.

Here is a one paragraph summary of the key points from the paper:The paper introduces a method to learn Contextualized Sparse Representations (CoSPR) for encoding phrases in open-domain question answering. The sparse representations dynamically compute the importance of each n-gram based on context, unlike standard tf-idf vectors which are static. The method leverages rectified self-attention weights on neighboring n-grams to encode rich lexical information in high-dimensional sparse vectors. To enable training on large sparse vector spaces, it uses a kernel trick to avoid explicit mapping. The CoSPR representations are used to augment the phrase embeddings in the existing DenSPI model for open-domain QA. Experiments on SQuAD-Open and CuratedTREC datasets show over 4% gains compared to DenSPI baselines by encoding precise lexical cues. The CuratedTREC results achieve new SOTA for end-to-end models, even outperforming retrieve-and-read models while being much faster. The contributions are: (1) learning contextualized sparse phrase representations via self-attention and kernels; (2) improving open-domain QA retrieval by augmenting existing dense vectors; (3) achieving SOTA accuracy with low latency.


## Summarize the paper in two paragraphs.

Here is a 2-paragraph summary of the paper:The paper introduces a method to learn contextualized sparse representations (\ours) for phrases to improve open-domain question answering under a phrase retrieval setup. Previous methods like DenSPI used static sparse vectors like tf-idf that were identical across a document. Instead, this method uses rectified self-attention on neighboring n-grams to generate dynamic sparse vectors that focus on the most semantically relevant words to each phrase. It allows representing phrases in a vector space of n-gram vocabulary size rather than just a few thousand dimensions like prior work. To enable training without explicitly mapping to this huge space, a kernel trick is used to compute the inner product between sparse vectors efficiently.  The method is applied in the DenSPI framework, augmenting its phrase embeddings with \ours instead of static tf-idf vectors. Experiments on SQuAD-Open and CuratedTREC show over 4% gain over DenSPI and state-of-the-art results compared to retrieve-and-read models, with at least 45x faster inference. The results demonstrate the benefit of contextualized sparse representations for encoding interpretable phrase vectors with rich lexical information for improved open-domain QA performance. Key advantages are the ability to handle n-gram features without hand-engineering and scalability to much higher dimensionality than prior sparse methods.
