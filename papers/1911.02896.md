# [Contextualized Sparse Representations for Real-Time Open-Domain Question   Answering](https://arxiv.org/abs/1911.02896)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the key points of this paper are:- It proposes a contextualized sparse phrase representation called SPARC to improve phrase retrieval for open-domain question answering. - The main hypothesis is that augmenting existing dense phrase embeddings with a learned sparse representation can help better capture lexical information and disambiguate between similar entities.- SPARC learns sparse vectors for each phrase by using rectified self-attention on neighboring n-grams. This allows encoding lexical information with high dimensionality (n-gram vocabulary space) compared to prior work.- A kernel trick is used during training for efficiency, avoiding explicit mapping to the high-dimensional sparse space.- Experiments show SPARC improves the Dense Phrase Retrieval (DenSPI) method on SQuAD-Open and CuratedTREC by over 4% in accuracy, achieving state-of-the-art results.So in summary, the key hypothesis is that a contextualized sparse representation can enhance phrase embeddings for open-domain QA to better capture lexical semantics. The method proposed is SPARC, which learns high-dimensional sparse vectors using self-attention and a kernel trick. Experiments verify this hypothesis, showing improved accuracy compared to prior methods.


## What is the main contribution of this paper?

Based on my reading, the main contribution of this paper is proposing a method to learn contextualized sparse phrase representations for open-domain question answering. Specifically:- They introduce a novel method called Contextualized Sparse Representation (CoSPAR) to encode sparse phrase representations that dynamically capture important lexical information depending on the context. - They propose an efficient training strategy that uses a kernel trick to avoid explicitly mapping to a very high-dimensional sparse space. This allows them to scale up to n-gram vocabulary space.- They demonstrate the effectiveness of CoSPAR by augmenting a previous dense phrase retrieval model called DenSPI. Adding CoSPAR improves performance by over 4% on SQuAD-Open and CuratedTREC datasets compared to using just DenSPI.- Their model achieves new SOTA results on CuratedTREC compared to prior retrieve-and-read approaches, while being much faster. On SQuAD-Open they also outperform BERT-based pipeline models with over 100x speedup.In summary, the key contribution is developing an interpretable contextualized sparse phrase representation that encodes precise lexical cues and significantly boosts performance of real-time open-domain QA compared to prior methods. The proposed training strategy also allows scaling up sparse representations to n-gram vocabulary space.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper proposes a contextualized sparse phrase representation method called SPARC that improves a dense phrase retrieval model for open-domain question answering by augmenting phrase embeddings with rectified self-attention-based sparse vectors that focus on semantically related n-grams.


## How does this paper compare to other research in the same field?

Here are a few key ways this paper compares to other research in open-domain question answering:- The paper focuses on improving phrase retrieval for open-domain QA, while much previous work has focused on retrieve-and-read pipelines that first retrieve documents then read/comprehend them. This is a meaningful comparison since phrase retrieval aims to avoid the pipeline limitations.- To improve phrase retrieval, the paper proposes learning contextualized sparse phrase representations. This contrasts with prior work on sparse representations that use static methods like tf-idf or learn only low-dimensional sparse vectors. The contextualization and high dimensionality are novel.- For learning representations, the paper uses an efficient kernelized training method. This compares to other representation learning methods that map to sparse spaces directly. The kernelization enables handling the high dimensionality.- The experiments compare performance on open QA datasets to previous state-of-the-art pipeline and end-to-end methods. The proposed model achieves new SOTA results, demonstrating advantages over prior approaches.- The inference speed is compared and shown to be much faster than competing retrieve-and-read pipelines, which is an important advantage of the phrase retrieval approach.Overall, the comparisons highlight the novelty of the contextualized sparse phrase representations for improving phrase retrieval in open-domain QA, and the results demonstrate these representations outperform prior methods in both accuracy and speed. The paper makes good contributions to the field of open-domain QA research.
