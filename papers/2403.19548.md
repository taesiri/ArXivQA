# [WaterJudge: Quality-Detection Trade-off when Watermarking Large Language   Models](https://arxiv.org/abs/2403.19548)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Watermarking generative AI systems like large language models (LLMs) is important to track their usage and prevent misuse. 
- Current LLM watermarking methods cause a tradeoff between watermark detectability and quality of generated text.
- There is little analysis on quantifying the quality degradation caused by existing watermarking schemes.

Proposed Solution:
- The paper proposes WaterJudge, a framework to analyze the quality-detection tradeoff when watermarking LLMs. 
- It uses comparative assessment by a judge LLM to measure quality degradation from watermarking compared to unwatermarked text.
- This enables visualizing operating points showing detectability vs quality.

Experiments and Results:
- Experimented with watermarking summarization (BART, Zephyr) and translation (mBART) systems.
- Clear tradeoff shown between watermark strength and output quality.
- Comparative assessment correlates highly to other metrics like UniEval and COMET.  
- Shows promise for transferring optimal settings between models and tasks.

Main Contributions:
- WaterJudge framework to visually analyze quality vs detectability of LLM watermarking schemes
- Using comparative assessment as an effective automatic evaluation metric
- Analysis and insights on the impact of watermarking on various LLMs and tasks
- Showing ability to potentially transfer optimal watermark settings across models and tasks
