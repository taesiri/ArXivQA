# ChatGPT Asks, BLIP-2 Answers: Automatic Questioning Towards Enriched   Visual Descriptions

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be: Can an automatic questioning system utilizing ChatGPT as the questioner and BLIP-2 as the visual question answerer generate more informative and enriched image captions compared to just using BLIP-2 alone?The key hypothesis appears to be that by prompting ChatGPT to ask a series of targeted questions about an image and having BLIP-2 provide visual information in response, the conversation can extract additional details about the image content. This in turn allows ChatGPT to produce a more comprehensive summary of the image in the final caption compared to BLIP-2's caption alone.So in summary, the central research question is whether an automatic questioning system can enhance image captioning by acquiring more visual knowledge through an interactive question-asking process. The hypothesis is that this approach can generate captions that are rated by humans as more informative than standard image captioning models like BLIP-2. The experiments aim to test this hypothesis.


## What is the main contribution of this paper?

Based on my reading, the main contributions of this paper are:1. It introduces ChatCaptioner, a novel automatic questioning method for generating enriched image captions. ChatCaptioner uses ChatGPT to ask a series of informative questions about an image to BLIP-2, a visual question answering model. 2. It demonstrates through human evaluations that ChatCaptioner can generate significantly more informative image captions compared to just using BLIP-2 alone. On average, ChatCaptioner's captions receive 3-4 times more votes from human evaluators for providing the richest image information.3. It shows that ChatCaptioner can identify 53% more objects in images compared to BLIP-2 captions. This indicates the questions from ChatGPT help extract additional visual information from BLIP-2.4. It reveals the capability of large language models like ChatGPT to serve as effective automatic questioners when properly prompted, despite having no visual perception.5. It highlights the benefits of asking good questions to acquire more knowledge from existing AI models, and draws attention to the potential of automatic questioning systems in AI research.In summary, the main contribution is the proposal of ChatCaptioner, an automatic questioning approach that can ask informative questions to extract more visual knowledge and generate significantly enriched image descriptions. The results verify the value of automatic questioning and the questioning capability of prompted language models.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper introduces ChatCaptioner, a novel automatic questioning method using ChatGPT and BLIP-2 that generates more informative and enriched image captions by prompting ChatGPT to ask a series of questions about an image and summarize BLIP-2's answers into a detailed description.


## How does this paper compare to other research in the same field?

Here are a few key points comparing this paper to other related research in question generation and captioning:- This paper presents a novel approach of using ChatGPT as an automatic question generator to obtain richer image captions. Most prior work on question generation relies on training neural models, while this leverages the zero-shot ability of ChatGPT.- For image captioning, this paper uniquely combines an automatic question asker (ChatGPT) with a visual question answerer (BLIP-2). It demonstrates how asking good questions can improve caption quality by extracting more visual details. Other image captioning methods focus only on improving the caption generator itself. - The idea of using dialog and conversational questioning to improve image understanding has been explored before in some works like Visual Dialog. However, this paper shows specifically how a pre-trained conversational model like ChatGPT can be effectively prompted for the questioning task, without needing to train a separate dialog model.- For evaluation, they highlight issues with current captioning metrics and use human evaluation to properly assess informativeness and correctness. Many prior captioning papers rely solely on automated metrics like CIDEr which may not capture caption improvements well.- The analysis of the diversity and quality of ChatGPT's questions is quite extensive compared to related question generation papers. And the ability to leverage other LLMs like InstructGPT demonstrates the generalizability of the prompting approach.In summary, the key novelties are using ChatGPT for zero-shot automatic questioning in captioning, the combined questioning-answering loop with BLIP-2, and the prompting strategies and analyses around ChatGPT's questioning abilities. The paper makes good connections to related areas while highlighting the unique aspects of this approach.
