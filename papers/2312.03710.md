# [Don't Overlook the Grammatical Gender: Bias Evaluation for Hindi-English   Machine Translation](https://arxiv.org/abs/2312.03710)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Neural machine translation (NMT) models often exhibit gender bias, by making stereotypical or biased translations. 
- Prior work on evaluating gender bias in NMT has focused on English as the source language.
- Evaluation methods used for English don't extend well to other languages, especially ones with grammatical gender like Hindi.
- Using gender-neutral sentences to evaluate bias (e.g. with TGBI score) doesn't expose the bias present in NMT models.

Proposed Solution:
- Create two new test sets for evaluating gender bias in Hindi-English NMT:
  - OTSC-Hindi: Sentences with occupational terms and grammatical gender cues about the speaker and their friend.
  - WinoMT-Hindi: 704 sentences with contextualized grammatical gender markers.
- Use these test sets to evaluate several Hindi-English NMT models.

Key Results:
- Evaluation shows most models exhibit heavy bias against the female gender, preferring masculine default translations.
- Models perform no better than random guessing on WinoMT-Hindi in discerning gender from context.
- TGBI score fails to expose gender bias present in NMT models.
- Google Translate performs best overall in utilizing grammatical gender cues for translation.

Main Contributions:
- Show importance of contextualizing bias evaluation for non-English source languages with grammatical gender markers. 
- Create two new test sets tailored for Hindi-English NMT bias evaluation.
- Demonstrate clear gender bias in several Hindi-English NMT models using new test sets.
- Establish that gender-neutral evaluation fails to expose NMT gender bias compared to gender-specified context.

The paper highlights the need for bias evaluation methods that account for properties of the source language, rather than just adapting approaches used for English. The new Hindi test sets reveal substantial gender bias in modern NMT models.


## Summarize the paper in one sentence.

 This paper proposes creating gender-specific test sets in Hindi to better evaluate gender bias in Hindi-English machine translation systems, emphasizing the need to account for grammatical gender cues in the source language.


## What is the main contribution of this paper?

 Based on my reading of the paper, the main contribution is:

The paper proposes the creation of two new test sets, OTSC-Hindi and WinoMT-Hindi, to evaluate gender bias in Hindi-English neural machine translation systems. These test sets are designed to account for the grammatical gender cues present in Hindi, such as gender-inflected words. The authors argue that existing methods of evaluating gender bias, which often rely on gender-neutral sentences, are not effective for languages like Hindi. By creating test sets with grammatical gender cues, they can better evaluate whether NMT systems utilize these cues or rely more on biased associations for disambiguation. The test sets are used to evaluate several popular Hindi-English NMT systems, with results showing heavy gender bias towards males in most models. The work highlights the importance of tailoring bias evaluation methods to the grammatical properties of the source language.

In summary, the key contribution is creating more effective test sets for evaluating gender bias in Hindi-English NMT that account for Hindi's grammatical gender. This allows better assessment of bias originating from the models rather than the test data itself.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and keywords associated with it are:

- Gender bias in machine translation
- Hindi-English machine translation (HI-EN MT) 
- Grammatical gender cues
- Bias evaluation benchmarks
- OTSC-Hindi
- WinoMT-Hindi
- TGBI score
- Gender-inflected words (adjectives, verbs, etc.)
- Masculine default translation
- Coreference resolution
- Fairness in MT

The paper focuses on evaluating gender bias in Hindi-English neural machine translation by creating new test sets that account for grammatical gender cues in Hindi. Key terms include the names of the new test sets (OTSC-Hindi and WinoMT-Hindi), metrics like TGBI, and concepts related to how gender is expressed in Hindi through inflected words. The goal is to better expose and evaluate biased gender translations.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in this paper:

1. The paper proposes creating two new test sets, OTSC-Hindi and WinoMT-Hindi, to evaluate gender bias in Hindi-English machine translation. Why did the authors feel the need to create new test sets instead of using existing ones? What limitations did previous test sets have for this language pair?

2. The OTSC-Hindi test set is based on a template that specifies the gender of both the speaker and their friend using grammatical gender cues in Hindi. How does controlling for grammatical gender in the source language allow for better evaluation of gender bias compared to using gender-neutral source sentences?

3. For the WinoMT-Hindi test set, the authors manually created Hindi versions of the existing WinoBias sentences that contain grammatical gender markers. What was the process for creating these sentences? What types of grammatical gender cues were incorporated and why were they important?  

4. How exactly does the inclusion of grammatical gender markers in the source sentences enable the measurement of whether models can discern gender accurately versus relying on biased stereotypical associations in the training data?

5. The results show Google Translate performs significantly better than other models in terms of accuracy on WinoMT-Hindi. Why might this be the case? Does Google Translate handle grammatical gender cues differently?

6. The authors find that evaluation using the TGBI metric does not effectively expose gender bias for these models. Why might TGBI be insufficient when the source language contains rich grammatical gender markings?

7. What further analyses could be done using the WinoMT-Hindi test set to better understand differences in how models handle various types of grammatical gender cues (e.g. inflected verbs, adjectives, etc.)?

8. How difficult or easy was the process of manually creating contextualized Hindi test sets with grammatical gender markings? What are some challenges or limitations to creating effective test sets this way?

9. The results show the models still rely heavily on masculine default translations. How might the test sets proposed here be used concretely to diagnose the underlying reasons behind this bias and come up with mitigation strategies?

10. The authors propose extending evaluation to accommodate all gender identities. What considerations would need to be made in incorporating notions of non-binary gender into grammatical gender marked source test sets?
