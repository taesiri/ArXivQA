# [Learning Transferable Spatiotemporal Representations from Natural Script   Knowledge](https://arxiv.org/abs/2209.15280)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the key research focus of this paper is using natural language semantics from automatic speech recognition (ASR) transcripts to improve the learning of transferable spatiotemporal video representations. Specifically, the paper argues that existing video representation learning methods rely only on visual information and lack explicit semantics, which limits their transferability and applicability to real-world uncurated video data. To address this, the paper proposes a new pretext task called "Turning to Video for Transcript Sorting" (TVTS). The key idea is to predict the order of shuffled ASR transcripts by attending to encoded video representations. This forces the model to contextualize events over time and perform temporal reasoning to match transcripts, improving spatiotemporal representations.The central hypothesis is that by exploiting natural language semantics from ASR transcripts, the model can learn video representations that are more transferable and applicable to uncurated real-world video data compared to methods that use only visual information. The pretext task TVTS is designed to enable such learning.In summary, the paper introduces TVTS pretext task to learn transferable spatiotemporal video representations by leveraging rich semantics from ASR transcript knowledge along with the video data itself.


## What is the main contribution of this paper?

Based on my understanding, the main contributions of this paper are:1. The paper proposes a new pretext task called Turning to Video for Transcript Sorting (TVTS) for video pre-training. The key idea is to sort shuffled automatic speech recognition (ASR) transcripts by attending to the corresponding video representations. This forces the model to contextualize what is happening in the video over time in order to figure out the correct order of the transcripts. 2. The paper shows that using natural language supervision from ASR transcripts of the videos themselves is an effective way to learn transferable spatiotemporal video representations. This avoids the need for descriptive video captions which are difficult to obtain at scale. The ASR transcripts provide useful semantics about events happening in the video over time.3. Extensive experiments demonstrate the benefits of the proposed TVTS pretext task. The pre-trained model achieves state-of-the-art results on downstream action recognition tasks, especially on Something-Something V2 which requires fine-grained temporal modeling. It also shows strong zero-shot transferability via video retrieval and linear classification.In summary, the key novelty is using self-supervision from natural ASR transcripts to learn better spatiotemporal video representations via a transcript sorting pretext task. This facilitates complex temporal understanding and outperforms methods that rely only on visual cues or descriptive captions.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper proposes a new method called Turning to Video for Transcript Sorting (TVTS) to learn transferable spatiotemporal video representations by leveraging the rich semantics from natural language scripts that come with videos, achieving strong out-of-the-box performance on action recognition tasks.


## How does this paper compare to other research in the same field?

This paper introduces a new method for learning transferable spatiotemporal video representations using natural language supervision from video transcripts. Here are some key comparisons to other related works:- Compared to other video representation learning methods like VideoMAE, this paper shows superior transferability (e.g. +13.6% gain on SSV2 via linear probing). It argues previous methods rely only on visual signals like pixel reconstruction and lack semantic understanding. - Compared to video-text alignment methods like Frozen, this paper focuses on learning spatiotemporal video representations rather than just cross-modal alignment. It shows stronger results on action recognition which requires temporal modeling.- Compared to multimodal methods like MERLOT, this paper uses language to supervise the video encoder rather than fuse representations. It shows ordering frames as in MERLOT is counterproductive for learning video representations.- Compared to image-text methods like CLIP, this paper introduces a way to utilize language for temporal understanding, not just spatial concepts. It uses natural video transcripts rather than human captions.In summary, the key novelties are using inherent noisy transcripts for supervision, the pretext task of transcript ordering for temporal reasoning, and demonstrating improved transferable video representations. This direction of using language semantics to enhance video understanding is relatively less explored previously.
