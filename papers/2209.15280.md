# [Learning Transferable Spatiotemporal Representations from Natural Script   Knowledge](https://arxiv.org/abs/2209.15280)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the key research focus of this paper is using natural language semantics from automatic speech recognition (ASR) transcripts to improve the learning of transferable spatiotemporal video representations. Specifically, the paper argues that existing video representation learning methods rely only on visual information and lack explicit semantics, which limits their transferability and applicability to real-world uncurated video data. To address this, the paper proposes a new pretext task called "Turning to Video for Transcript Sorting" (TVTS). The key idea is to predict the order of shuffled ASR transcripts by attending to encoded video representations. This forces the model to contextualize events over time and perform temporal reasoning to match transcripts, improving spatiotemporal representations.The central hypothesis is that by exploiting natural language semantics from ASR transcripts, the model can learn video representations that are more transferable and applicable to uncurated real-world video data compared to methods that use only visual information. The pretext task TVTS is designed to enable such learning.In summary, the paper introduces TVTS pretext task to learn transferable spatiotemporal video representations by leveraging rich semantics from ASR transcript knowledge along with the video data itself.


## What is the main contribution of this paper?

Based on my understanding, the main contributions of this paper are:1. The paper proposes a new pretext task called Turning to Video for Transcript Sorting (TVTS) for video pre-training. The key idea is to sort shuffled automatic speech recognition (ASR) transcripts by attending to the corresponding video representations. This forces the model to contextualize what is happening in the video over time in order to figure out the correct order of the transcripts. 2. The paper shows that using natural language supervision from ASR transcripts of the videos themselves is an effective way to learn transferable spatiotemporal video representations. This avoids the need for descriptive video captions which are difficult to obtain at scale. The ASR transcripts provide useful semantics about events happening in the video over time.3. Extensive experiments demonstrate the benefits of the proposed TVTS pretext task. The pre-trained model achieves state-of-the-art results on downstream action recognition tasks, especially on Something-Something V2 which requires fine-grained temporal modeling. It also shows strong zero-shot transferability via video retrieval and linear classification.In summary, the key novelty is using self-supervision from natural ASR transcripts to learn better spatiotemporal video representations via a transcript sorting pretext task. This facilitates complex temporal understanding and outperforms methods that rely only on visual cues or descriptive captions.
