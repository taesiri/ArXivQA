# [Federated Learning with Partial Model Personalization](https://arxiv.org/abs/2204.03809v2)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading, the key research questions/hypotheses addressed in this paper are:

1. Can partial model personalization achieve most of the benefits of full model personalization while using only a fraction of the parameters?

2. How do two optimization algorithms for partial model personalization, namely FedAlt (alternating updates) and FedSim (simultaneous updates), compare in terms of convergence guarantees and empirical performance? 

3. Is partial personalization robust and does it consistently help across a diverse set of tasks and datasets? Or does it hurt performance on some devices/tasks?

To summarize, the central goals of this work are:

- To propose and analyze two algorithms, FedAlt and FedSim, for training partially personalized models in federated learning. 

- To demonstrate through theory and experiments that partial personalization can achieve comparable or better performance than full personalization using much fewer personalized parameters.

- To conduct extensive experiments across image, text and speech tasks to assess the robustness of partial personalization and compare FedAlt and FedSim.

- To reveal and discuss the phenomenon that personalization, while improving average performance, can hurt performance on some devices.

So in essence, the paper aims to propose and thoroughly evaluate partial model personalization for federated learning as an effective and practical alternative to full model personalization. The convergence guarantees, comparative assessment of FedAlt and FedSim, and experimental findings on multiple datasets are key contributions.


## What is the main contribution of this paper?

 Based on my reading, the main contributions of this paper are:

1. It provides convergence guarantees for two federated learning algorithms (FedAlt and FedSim) for training partially personalized models in the nonconvex setting with partial participation. Prior work had analyzed these algorithms only under more restrictive assumptions like convexity or full participation. 

2. It presents an extensive empirical study on real-world image, text, and speech datasets, evaluating different strategies for partial model personalization. The key findings are:

- Partial personalization can achieve most of the benefits of full model personalization with only a small fraction of personalized parameters. For example, personalizing just the output layer captures 90% of the accuracy boost from full personalization on StackOverflow.

- The alternating update algorithm FedAlt consistently outperforms the simultaneous update FedSim, although the gap is small. This aligns with the theoretical finding that FedAlt has better convergence guarantees when the local stochastic gradients have low variance.

- Personalization can hurt the test accuracy of some devices, especially those with limited local data. Typical regularization techniques like weight decay and dropout do not resolve this issue.

3. It introduces a new technique called "virtual full participation" to handle the dependent random variables that arise in analyzing FedAlt with partial participation. This could be of broader interest for nonconvex optimization with coupled variables.

4. The convergence rates proved for FedAlt and FedSim are competitive or better than existing analyses of full personalization methods like pFedMe, even though they apply to the more general setting of partial personalization.

In summary, this paper provides both theory and experiments to demonstrate the practical utility of partial personalization, while also highlighting some of the emerging challenges like potential negative effects on some devices.
