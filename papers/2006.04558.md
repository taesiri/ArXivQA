# [FastSpeech 2: Fast and High-Quality End-to-End Text to Speech](https://arxiv.org/abs/2006.04558)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question is how to develop a fast, high-quality, and fully end-to-end text-to-speech (TTS) system. Specifically, the authors aim to address limitations of previous non-autoregressive TTS models like FastSpeech, which rely on a complicated teacher-student training pipeline, inaccurate duration prediction, and information loss during knowledge distillation. 

The main hypothesis is that by:

1) Simplifying the training pipeline and using ground-truth targets instead of distilled outputs. 

2) Providing more accurate duration information and additional variance information like pitch and energy as inputs.

3) Modeling pitch variations using continuous wavelet transform.

4) Extending the model to directly generate waveforms instead of mel-spectrograms.

They can develop a faster TTS model (FastSpeech 2) that achieves higher voice quality than FastSpeech and other autoregressive models, while still enjoying the benefits of non-autoregressive models like fast and robust synthesis. They further hypothesize that extending this model (FastSpeech 2s) to directly generate waveforms can simplify the pipeline towards fully end-to-end TTS with minimal performance drop.

In summary, the main research question is how to develop a fast, high-quality, and end-to-end TTS system, with the central hypothesis that modeling additional variance information and simplifying the training process can achieve this goal.


## What is the main contribution of this paper?

 The main contributions of this paper are:

- Proposing FastSpeech 2, which improves upon FastSpeech (a previous non-autoregressive TTS model) to achieve higher voice quality and simpler training. Specifically, FastSpeech 2 removes the teacher-student distillation pipeline in FastSpeech, and instead trains directly on ground-truth spectrograms. It also uses more accurate duration estimation and adds pitch/energy predictors to provide more variation information and ease the one-to-many mapping problem in TTS.

- Developing FastSpeech 2s, an extension of FastSpeech 2 that enables fully end-to-end text-to-waveform synthesis, without intermediate mel-spectrogram generation. FastSpeech 2s simplifies the inference pipeline and achieves faster synthesis speed. 

- Experiments showing that FastSpeech 2 has 3x faster training than FastSpeech, while achieving better voice quality that can surpass autoregressive models. FastSpeech 2s further speeds up inference and maintains high voice quality.

In summary, the main contribution is proposing FastSpeech 2 and 2s models that achieve faster, simpler and higher-quality non-autoregressive end-to-end TTS compared to previous methods. The key ideas are simplifying the training pipeline, providing more variation information as input, and pushing towards fully end-to-end waveform synthesis.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the paper:

The paper proposes FastSpeech 2 and FastSpeech 2s, non-autoregressive text-to-speech models that achieve faster training and inference speed than previous methods while generating higher quality and more controllable speech by predicting additional variance information like pitch, energy, and more accurate durations.


## How does this paper compare to other research in the same field?

 Here are some key points comparing this paper to other research in non-autoregressive text-to-speech (TTS):

- This paper proposes FastSpeech 2 and FastSpeech 2s, which aim to improve upon FastSpeech, one of the most successful non-autoregressive TTS models. The key differences from FastSpeech are:

1) Simpler training pipeline without teacher-student distillation, avoiding information loss. 

2) Adding pitch, energy and more accurate duration as conditional inputs to help with the one-to-many mapping problem in TTS.

3) For pitch, using continuous wavelet transform to model variations better. 

4) FastSpeech 2s generates waveform directly from text, enabling fully end-to-end TTS.

- Compared to other non-autoregressive models like AlignTTS, FastPitch, and Glow-TTS, a key difference is the use of additional conditional inputs like pitch and energy to provide more variation information. The pitch modeling with CWT is also novel.

- Relative to autoregressive models like Tacotron and Transformer TTS, FastSpeech 2 achieves comparable quality while being much faster. FastSpeech 2s is the first fully non-autoregressive end-to-end TTS.

- Compared to end-to-end models like ClariNet and EATS that also go directly from text to waveform, FastSpeech 2s maintains high quality without an autoregressive component by leveraging adversarial training and the mel-spectrogram decoder.

Overall, FastSpeech 2 and 2s move the state-of-the-art forward in non-autoregressive TTS by improving voice quality and training efficiency compared to prior work. The additional variance inputs and waveform modeling are innovative ways to tackle the one-to-many mapping problem.


## What future research directions do the authors suggest?

 Based on my reading of the paper, some future research directions the authors suggest are:

- Implementing more variance information in the variance adaptor, such as emotion, style, and speaker identity. The current adaptor only includes duration, pitch, and energy, but adding other variance factors could further ease the one-to-many mapping problem in TTS. 

- Developing a fully end-to-end TTS system without any external alignment models or tools. Currently, FastSpeech 2 relies on an external alignment tool (Montreal Forced Aligner) for phoneme alignment. Removing this dependency would simplify the system.

- Further improving voice quality and reducing model size. They suggest exploring other techniques like denoising diffusion probabilistic models. Reducing model size could also improve inference speed.

- Extending to character-level inputs instead of phoneme inputs. This could simplify the preprocessing pipeline.

- Removing reliance on handcrafted linguistic features and instead learning representations directly from text. This could make the model more end-to-end.

- Incorporating unsupervised and semi-supervised learning to take advantage of unlabelled data. This could improve robustness and generalization.

- Exploring other lightweight vocoders to further accelerate inference speed while maintaining high quality.

In summary, the main future directions are developing a fully end-to-end system without external dependencies, improving voice quality and inference speed, and increasing the model's flexibility by adding more variance factors. The authors propose this could be done through techniques like denoising diffusion models, lightweight vocoders, semi-supervised learning, and character-level modeling.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the paper:

This paper proposes FastSpeech 2, an improved non-autoregressive text-to-speech model that addresses limitations of the previous FastSpeech model. FastSpeech 2 removes the need for a separate autoregressive teacher model and distilled training targets by directly learning to predict ground truth mel-spectrograms from text. It also incorporates additional condition inputs - predicted pitch, energy, and more accurate duration information - to help address the one-to-many mapping problem in TTS. Continuous wavelet transform is used to model pitch in the frequency domain for better accuracy. An extension called FastSpeech 2s is introduced to enable direct waveform generation from text. Experiments show FastSpeech 2 has simpler training, faster inference, and better voice quality than FastSpeech, even surpassing autoregressive models. FastSpeech 2s further simplifies the pipeline with fully end-to-end text-to-waveform generation while maintaining high quality.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the paper:

This paper proposes FastSpeech 2, an improved non-autoregressive text-to-speech model that achieves faster training speed and higher voice quality compared to the original FastSpeech model. FastSpeech 2 simplifies the training pipeline by removing the teacher-student distillation process used in FastSpeech. It instead trains directly on ground-truth spectrograms as targets. To reduce the information gap between inputs and outputs, FastSpeech 2 introduces pitch, energy, and more accurate duration as conditioning variables. It uses force alignment rather than attention to extract more precise phoneme durations. It also converts pitch contours to pitch spectrograms using continuous wavelet transform for improved pitch prediction. 

The paper further proposes FastSpeech 2s which skips mel-spectrogram generation and directly outputs waveforms from text. This simplifies the TTS pipeline to be fully end-to-end. FastSpeech 2s employs adversarial training to help the model recover phase information in waveforms. Experiments show FastSpeech 2 trains 3x faster than FastSpeech while achieving better voice quality. FastSpeech 2s further speeds up inference by generating waveforms directly. Both models also enable control over pitch, duration, and energy in synthesis. FastSpeech 2 obtains higher quality than autoregressive baselines. This demonstrates the effectiveness of the proposed techniques in improving non-autoregressive TTS.
