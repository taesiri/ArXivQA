# [Big Self-Supervised Models are Strong Semi-Supervised Learners](https://arxiv.org/abs/2006.10029)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central research question this paper addresses is: How effective is the paradigm of "unsupervised pretraining followed by supervised fine-tuning" for semi-supervised learning on large-scale image classification tasks like ImageNet? The authors propose that with the right techniques, this paradigm can achieve very strong performance for semi-supervised learning in computer vision, rivaling or surpassing previous state-of-the-art techniques that directly leverage unlabeled data during supervised training. Specifically, the paper investigates:- The importance of using big models during pretraining and fine-tuning, showing that bigger models are more label efficient. - The impact of the projection head design used in contrastive pretraining methods like SimCLR, finding that deeper projection heads improve representation quality. - The benefits of distilling the fine-tuned model into a smaller model using unlabeled data to improve efficiency.The central hypothesis is that with the right combination of these techniques, the "unsupervised pretrain, supervised fine-tune" paradigm can achieve excellent semi-supervised performance on ImageNet, despite using unlabeled data in a task-agnostic way. The empirical results validate this hypothesis and show substantial improvements over prior state-of-the-art semi-supervised techniques.
