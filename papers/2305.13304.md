# RecurrentGPT: Interactive Generation of (Arbitrarily) Long Text

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper abstract, it seems the central research question is: How can we enable large language models (LLMs) like ChatGPT to generate arbitrarily long coherent texts beyond the fixed-size context limitations of the Transformer architecture?The key idea proposed in the paper is to design a system called RecurrentGPT that simulates recurrent neural networks (RNNs) using natural language prompts and components. This allows leveraging the unlimited context capabilities of RNNs while still using LLMs like ChatGPT as the backbone model.Specifically, RecurrentGPT:- Replaces vectorized components in RNNs (e.g. cell/hidden states, input/output) with natural language paragraphs and summaries - Uses prompt engineering to simulate RNN computational graphs and recurrence - Stores long-term memory on disk and accesses it via semantic search- Maintains a short-term memory paragraph updated via the LLM- Allows human interaction by editing language memories and plansThe central hypothesis is that by designing prompts to simulate RNN architectures, RecurrentGPT will enable LLMs to generate much longer coherent text than normally possible with the fixed-size Transformer context. The experiments aim to evaluate this hypothesis across different text generation tasks and genres.In summary, the paper introduces and evaluates a novel prompting approach to get around context limitations of LLMs for long text generation.


## What is the main contribution of this paper?

Based on my reading of the abstract and introduction, the main contributions of this paper appear to be:1. Proposing a model called RecurrentGPT, which uses natural language to simulate the recurrence mechanism in RNNs and enables large language models like ChatGPT to generate arbitrarily long text beyond their fixed context size limitation. 2. Demonstrating that RecurrentGPT can generate very long and coherent texts either autonomously or interactively with a human writer. It can serve as an interpretable and customizable AI writing assistant.3. Introducing a new use case of using RecurrentGPT as a personalized interactive fiction/game that directly interacts with readers, instead of just being used as a tool for writers. The authors call this "AI as Content".4. Showing the potential of borrowing ideas from cognitive science and deep learning model designs like RNNs to create better prompts for large language models. In summary, the main contribution seems to be proposing RecurrentGPT as a way to get around the context size limitation of models like ChatGPT and enable new applications like interactive fiction through prompting. The novelty lies in using natural language to simulate RNN recurrence and achieve human-AI interaction.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Based on my reading of the paper, here is a one sentence summary: The paper introduces RecurrentGPT, a method that uses natural language prompts to simulate recurrent neural networks with large language models like ChatGPT, enabling them to generate arbitrarily long coherent texts.


## How does this paper compare to other research in the same field?

Here are my thoughts on how this paper compares to other related research:- This paper introduces RecurrentGPT, a novel method for enabling large language models (LLMs) like ChatGPT to generate arbitrarily long coherent text. This addresses a key limitation of LLMs, which can only generate text within a fixed context window. - Most prior work on generating long text with LLMs has taken a hierarchical approach - first generating an outline or summary, then generating the full text conditioned on that high-level plan. Examples are RE^3, DOC, and various story generation methods. RecurrentGPT is fundamentally different in that it simulates RNN recurrence to allow iterative, interactive generation of unbounded lengths.- Some recent work has tried to modify the Transformer architecture itself to add recurrence, like Transformer-XL, Compressive Transformers, etc. However, this requires changing the model architecture, while RecurrentGPT works purely through prompting any unmodified LLM. This is a big advantage since it can leverage state-of-the-art LLMs like ChatGPT as-is.- For controllable long text generation, RecurrentGPT contrasts with most existing AI writing assistants focused on local phrase/sentence suggestions. It reduces human effort through paragraph-level generation and interactivity. The interpretability via natural language for memories and plans is also novel.- Demonstrating the use of RecurrentGPT for interactive fiction represents an innovative application of LLMs - using them not just as creative tools but directly as personalized interactive content. The "AI as Content" paradigm is largely unexplored.- Stepping back, RecurrentGPT shows the promise of using concepts from RNNs, cognitive science, etc. to design prompts that expand capabilities of LLMs. Much prompt engineering research focuses narrowly on task performance; this looks at broader architectural principles.In summary, RecurrentGPT introduces a novel prompting approach to equip LLMs with recurrence for long text generation. The applications for controllable, interactive story generation and playable fiction highlight unique capabilities. The work ties together several important themes around prompting, interpretability, and imaginative uses of LLMs.
