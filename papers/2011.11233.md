# [ROME: Robustifying Memory-Efficient NAS via Topology Disentanglement and   Gradient Accumulation](https://arxiv.org/abs/2011.11233)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the main contributions seem to be:

1. Identifying and analyzing the performance collapse issue in single-path differentiable NAS methods like GDAS. The paper shows both empirically and theoretically that these methods suffer from similar instability issues as the original DARTS, where the architectures tend to be dominated by parameter-free operations like skip connections. 

2. Proposing a new robust NAS algorithm called ROME (Robustifying Memory-Efficient NAS) to address the instability. The key ideas are:

(a) Disentangling the search for topology and operations to make the search and evaluation stages more consistent. 

(b) Using gradient accumulation techniques to reduce the variance and enable more fair training of the operations.

3. Extensive experiments showing ROME achieves state-of-the-art performance across 15 NAS benchmarks, demonstrating its effectiveness and robustness compared to prior NAS methods.

So in summary, the central hypothesis is that single-path NAS methods also suffer from instability/collapse issues, which the authors diagnose as being due to topology inconsistency and insufficient sampling of operations. The proposed ROME algorithm aims to address these issues, leading to more robust and better performing architectures.


## What is the main contribution of this paper?

 Based on my reading of the paper, the main contributions appear to be:

1. Identifying a performance collapse issue in single-path differentiable neural architecture search (NAS) methods, where parameterless operations like skip connections tend to dominate the searched architecture. 

2. Proposing a new NAS algorithm called ROME (Robustifying Memory-Efficient NAS) to address this issue. The key ideas are:

- Disentangling the search for topology and operations to make the search and evaluation stages more consistent. This is done by using separate architectural weights for topology and operations.

- Using Gumbel-Top2 reparameterization to allow differentiable sampling while satisfying topology constraints.

- Accumulating gradients across multiple sampled subnetworks during training to reduce variance and improve training of the candidates. 

3. Demonstrating the effectiveness of ROME across 15 NAS benchmarks, including reduced search spaces from recent works. ROME achieves state-of-the-art results while maintaining low memory cost.

4. Providing ablation studies to validate the contributions of the proposed techniques in ROME. Experiments show disentangling topology and using gradient accumulation help resolve the instability issue.

In summary, the main contribution is proposing and validating a new approach called ROME that makes single-path NAS more robust and achieves strong performance across a range of benchmarks while being memory-efficient.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the paper:

The paper proposes a robust neural architecture search method called ROME that disentangles topology search from operation selection and uses gradient accumulation techniques to address performance collapse issues in memory-efficient NAS approaches like GDAS.
