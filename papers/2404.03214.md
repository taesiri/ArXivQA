# [LeGrad: An Explainability Method for Vision Transformers via Feature   Formation Sensitivity](https://arxiv.org/abs/2404.03214)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
Vision transformers (ViTs) have become a popular architecture for computer vision tasks due to their ability to model long-range dependencies. However, the interpretability of ViTs remains a challenge as methods designed for CNNs rely on convolutional layers or specific propagation rules that do not apply to ViTs. While some ViT-specific interpretation techniques exist, the interpretability of large-scale ViTs is still lacking.  

Proposed Solution: 
The authors propose LeGrad, an interpretation method designed specifically for ViTs that computes the gradient with respect to the attention maps. For each layer, LeGrad calculates the activation score and gradient w.r.t the attention maps. Negative contributions are eliminated with a ReLU and the gradients are averaged and resized to produce a 2D explainability map per layer. The maps from all layers are then averaged to produce the final explainability map highlighting image regions most influential to the model's prediction.

Main Contributions:
- Proposes LeGrad, a simple and versatile gradient-based interpretation method for ViTs that leverages attention maps 
- Scales to large ViTs like ViT-BigG and is applicable to various feature aggregation methods
- Evaluates LeGrad on segmentation, open-vocabulary detection, and perturbation tasks, showing improvement over other methods
- Demonstrates LeGrad's superior spatial fidelity and robustness to perturbations across model sizes, ranging from ViT-B/16 to ViT-BigG/14
- Makes ViTs more transparent and interpretable, paving the way for more responsible foundation models

In summary, LeGrad is a novel and high-performing method for interpreting ViT models by generating visual explanations based on the sensitivity of attention maps and feature formation. Its versatility, scalability and strong empirical performance help improve the interpretability of vision transformers.
