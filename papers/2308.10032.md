# [GameEval: Evaluating LLMs on Conversational Games](https://arxiv.org/abs/2308.10032)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the main research question seems to be: How can we develop a more comprehensive and effective approach to evaluating the integrated capabilities of large language models (LLMs) to solve complex, real-world problems?The key hypotheses appear to be:1. Existing methods for evaluating LLMs have limitations, such as the need for human labeling or preference judgments, testing only individual skills, and averaging across disconnected tasks. This makes it difficult to accurately assess models' ability to apply multiple skills simultaneously to solve multifaceted problems.2. Engaging LLMs in goal-driven conversational games that require the integrated application of diverse capabilities (e.g. cooperation, reasoning, planning) could provide a more holistic evaluation of their competencies for complex tasks. 3. Properly designed games with cooperative or adversarial objectives can effectively differentiate LLMs and highlight gaps in their competencies, without human intervention or reference answers.4. Game-based evaluation methods can complement existing approaches to provide a more comprehensive assessment of LLM performance on real-world intelligence tasks.In summary, the main research question is how to develop better evaluation paradigms for LLMs that test their integrated reasoning and problem-solving abilities, which this paper proposes to address through goal-driven conversational games. The hypotheses focus on the limitations of current methods and the potential for games to evaluate LLMs in a more holistic yet human-independent manner.
