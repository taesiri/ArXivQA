# [GameEval: Evaluating LLMs on Conversational Games](https://arxiv.org/abs/2308.10032)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the main research question seems to be: How can we develop a more comprehensive and effective approach to evaluating the integrated capabilities of large language models (LLMs) to solve complex, real-world problems?The key hypotheses appear to be:1. Existing methods for evaluating LLMs have limitations, such as the need for human labeling or preference judgments, testing only individual skills, and averaging across disconnected tasks. This makes it difficult to accurately assess models' ability to apply multiple skills simultaneously to solve multifaceted problems.2. Engaging LLMs in goal-driven conversational games that require the integrated application of diverse capabilities (e.g. cooperation, reasoning, planning) could provide a more holistic evaluation of their competencies for complex tasks. 3. Properly designed games with cooperative or adversarial objectives can effectively differentiate LLMs and highlight gaps in their competencies, without human intervention or reference answers.4. Game-based evaluation methods can complement existing approaches to provide a more comprehensive assessment of LLM performance on real-world intelligence tasks.In summary, the main research question is how to develop better evaluation paradigms for LLMs that test their integrated reasoning and problem-solving abilities, which this paper proposes to address through goal-driven conversational games. The hypotheses focus on the limitations of current methods and the potential for games to evaluate LLMs in a more holistic yet human-independent manner.


## What is the main contribution of this paper?

The main contribution of this paper seems to be proposing GameEval, a new evaluation paradigm for large language models (LLMs) based on goal-driven conversational games. Some key points:- GameEval engages LLMs in conversational games with different goals and roles to comprehensively evaluate their capabilities. This is different from existing evaluation methods that rely on references or human/model preferences.- The paper designs three sample games (Ask-Guess, SpyFall, TofuKingdom) that require LLMs to demonstrate cooperative strategies, reasoning, planning, etc. to succeed. Metrics are proposed to quantify performance in these games.- Experiments on ChatGPT, GPT-3, and GPT-4 show GameEval can effectively differentiate between models, revealing gaps not apparent with existing methods.- GameEval provides a more realistic way to assess how well LLMs can apply their skills in an integrated manner to solve complex problems, as compared to individual task-based evaluations.In summary, the key contribution is proposing game-based evaluation as a new paradigm for LLMs that overcomes limitations of prior approaches and provides a comprehensive assessment of capabilities. The designed games and experiments demonstrate the promise of GameEval.
