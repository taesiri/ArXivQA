# [The Impact of Syntactic and Semantic Proximity on Machine Translation   with Back-Translation](https://arxiv.org/abs/2403.18031)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Back-translation is used successfully for unsupervised neural machine translation (UNMT), but theoretically it should not work in general without some alignment signal between languages. So why does it work empirically?

- The paper investigates what properties of natural languages make back-translation effective for UNMT, when in theory it is not guaranteed to succeed on its own.

Methodology: 
- Use artificial languages where lexical, syntactic and semantic properties can be controlled. Train UNMT between pairs of these languages with back-translation.

- Manipulate factors like grammar, vocabulary, word frequencies, partial supervision, and coarse semantic information to analyze their effect on back-translation performance.

Findings:
- Languages with more similar grammars are easier to align with back-translation. But grammar alone is not enough when vocabularies differ across languages.

- Factors often assumed to help like shared vocabulary, parallel word frequencies do not help much. Some supervision signal is critical for back-translation to work across distinct vocabularies.

- Crude semantic information in the form of lexical fields (sets of semantically related words) does provide a useful signal for back-translation to find better alignment.

Conclusions:
- Pure back-translation objectives can fail even on very similar languages. Its empirical success on real languages is likely due to complex semantic similarities across languages that anchor the mapping.

- Neither lexical nor simple syntactic similarity is enough to explain back-translation's effectiveness. More research needed into what semantic properties allow models to align languages in an unsupervised way.

Main Contributions:
- Analyze limitations of back-translation objectives through counterexamples 
- Use controlled artificial languages to systematically measure factors affecting back-translation
- Show grammar itself does not suffice across vocabularies; supervision signal is key
- Demonstrate even coarse semantic information helps anchor alignment
