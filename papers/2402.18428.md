# [Leveraging Diverse Modeling Contexts with Collaborating Learning for   Neural Machine Translation](https://arxiv.org/abs/2402.18428)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper "Leveraging Diverse Modeling Contexts with Collaborative Learning for Neural Machine Translation":

Problem:
- Autoregressive (AR) and non-autoregressive (NAR) models have complementary strengths and weaknesses for neural machine translation. AR models can effectively model translation distributions but have slow inference. NAR models enable fast decoding but suffer from performance issues like the multi-modality problem.

- Previous work has tried to improve one model type using the other as a teacher. However, this limits further improvement due to the static teacher. Some recent work has tried to unify AR and NAR models, but does not fully leverage their diverse contexts.

Proposed Solution: 
- Proposes a collaborative learning framework called Diverse Context Modelling with Collaborative Learning (DCMCL) where AR and NAR models learn from each other as collaborators instead of a static teacher-student setup.

- Uses a shared encoder, and separate AR and NAR decoders. Applies multi-task learning to optimize both models.

- Introduces token-level mutual learning between AR and NAR on the NAR model's masked tokens to leverage diverse contextual dependencies.

- Further employs sequence-level contrastive learning to ensure semantic consistency between AR and NAR outputs using them as positive pairs.

Main Contributions:
- Proposes a generic collaborative learning approach to simultaneously improve AR and NAR models by hierarchically leveraging their diverse contextual information.

- Achieves significant BLEU score improvements over strong baselines on WMT and IWSLT datasets using two standard iterative NAR models CMLM and DisCo.

- Analysis shows collaborative learning helps resolve issues like multi-modality for NAR and length bias for AR, and brings hidden representations closer.

- Demonstrates generalizability to multiple model types and potential for applications beyond machine translation.
