# [Scaling Behavior of Machine Translation with Large Language Models under   Prompt Injection Attacks](https://arxiv.org/abs/2403.09832)

## What is the main contribution of this paper?

 Based on my reading of the paper, the main contributions are:

1) The authors investigate the scaling behavior of large language models (LLMs) on machine translation under prompt injection attacks. Specifically, they evaluate how model performance changes as model size increases on a machine translation task, both with and without adversarial prompts designed to manipulate the model.

2) They introduce a new parallel test set of questions in English and translations to German, French, Romanian, and Russian. The test set contains both "clean" examples as well as adversarial examples constructed using prompt injection attacks.

3) They evaluate multiple LLM families (GPT-3, InstructGPT, T5, FLAN, LLaMA2) of varying sizes on this test set. They find instances of inverse scaling, where larger models become more susceptible to successful prompt injection attacks.

4) To the best of their knowledge, this is the first work investigating non-monotonic scaling and prompt injection attacks in a multi-lingual setting. The analysis provides further evidence that larger neural network models can become more vulnerable to manipulation in certain scenarios.

In summary, the key contribution is an analysis and new benchmark focused on understanding scaling laws and vulnerabilities of LLMs for machine translation under adversarial attacks.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in this paper:

1. The paper introduces a new parallel test set of questions for evaluating prompt-based machine translation under prompt injection attacks. How was this test set created and what considerations went into the data collection and preprocessing?

2. The paper evaluates performance using a "question mark accuracy" metric to determine if the model output is a successful translation or a failed one that answers the question instead. What are some potential limitations or caveats of using this simple heuristic, compared to more standard machine translation evaluation metrics like BLEU?  

3. For the few-shot prompting experiments, the paper uses a single parallel example in the prompt. How might the results differ if more in-context examples were provided, for instance 5-shot prompting? Would we expect more or less susceptibility to prompt injection attacks?

4. The paper found inverse scaling trends under certain model series and zero-shot scenarios. What might explain why zero-shot prompting leads more often to non-monotonic scaling compared to few-shot prompting? 

5. Could the trends found, particularly the inverse scaling, be an artifact of the specific prompt injection attack and test set used? How could the methodology be strengthened to increase confidence that similar phenomena would manifest with other attacks or test data?

6. The English distractors were found to be more effective than distractors in other languages. To what extent could this be explained by factors like the training data size, domain, or other attributes of the English portion of the training corpus?

7. For the model families and sizes showing inverse scaling, at what point does performance peak before decreasing? What might explain why that particular model size is optimal?

8. The paper hypothesizes that techniques like instruction tuning could mitigate some of the vulnerabilities to prompt injection attacks. However, results were mixed across model families. What factors may determine whether instruction tuning helps or hurts robustness?

9. If robustness to prompt injection is a priority, what other techniques besides scaling up model size could help? For example, training procedures, different prompting approaches, or auxiliary tasks.

10. How amenable are the phenomena found, especially the non-monotonic scaling, to being characterized precisely as scaling laws? What additional experiments could help strengthen or refine such laws around susceptibility to attacks?
