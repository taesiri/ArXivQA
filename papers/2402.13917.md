# [What Linguistic Features and Languages are Important in LLM Translation?](https://arxiv.org/abs/2402.13917)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem Statement
The paper evaluates the machine translation capabilities of Llama2, a recently released large language model, with a focus on analyzing performance across languages seen and unseen during Llama2's training. The authors note that assessing proprietary models like GPT is challenging due to lack of transparency into the training data. In contrast, Llama2 explicitly reports languages and amounts of training data used. 

The paper also aims to study which linguistic features and training languages have the biggest impact on Llama2's translation quality. Prior work has shown including related languages helps for low-resource translation, so the linguistic proximity of languages likely influences performance.

Methodology 
The authors test Llama2 on 41 languages - 26 seen during training ("inllama") and 15 unseen ("outllama"). Translation quality is measured by BLEU and COMET scores on the FLORES benchmark (excluding English to mitigate data leakage). The impact of model scale, instruction tuning, and few-shot learning is analyzed for the 15 "outllama" languages. 

Linguistic distances between languages are extracted from the URIEL database and correlated with translation scores. This analysis focuses on genetic, geographical, inventory, phonological and syntactic distances.

Key Results
- Llama2 can translate all "inllama" languages with BLEU >10 but struggles on some "outllama" languages.
- Increasing model scale boosts translation of unseen languages more than instruction tuning or few-shot learning.
- Syntactic similarity does not always have the highest correlation with translation quality.
- Despite much less training data, some languages show correlations comparable to English in determining translation performance.

Main Contributions
1) Comprehensive Llama2 translation evaluation on 41 languages 
2) Model scale more impactful than instruction tuning/few-shot learning
3) Syntactic proximity not always most important linguistic factor
4) Non-English languages can show equally strong or stronger correlations with translation quality

The results provide new perspective on potential limitations of English-centric models and viability of models centered around other languages.


## Summarize the paper in one sentence.

 This paper evaluates Llama2's machine translation capabilities across 41 languages, explores which linguistic features and languages in its training data are most important for translation quality, and finds that syntactic similarity and English data are not always the primary factors determining performance.


## What is the main contribution of this paper?

 Based on my reading of the paper, the main contributions are:

1. Comprehensive evaluation of Llama2's machine translation capabilities across 41 languages, including 15 languages not seen during its training.

2. Analysis showing that increasing model scale (parameters) is more effective at improving translation quality than instruction tuning or adding few-shot examples, although the extent varies across languages. 

3. Demonstration that syntactic similarity is not always the most important linguistic factor determining translation quality. English features do not always have the strongest correlation either, despite English having the most training data.

4. Findings suggesting English-centric models may not be optimal, and multilingual models centered around other languages could potentially be more effective foundations.

In summary, the key contributions are a thorough analysis of Llama2's machine translation abilities, an evaluation of techniques to improve quality, and an exploration of which languages and linguistic features have the biggest influence on translation performance. The discoveries provide new perspectives on current limitations of LLMs and how future models could be improved.


## What are the keywords or key terms associated with this paper?

 Based on my review of the paper, the key terms and keywords associated with it are:

Llama2, machine translation, linguistic distances, multilingual models, language features, syntactic similarity, model scale, instruction tuning, few-shot learning, URIEL typological database, genetic distance, geographical distance, inventory distance, phonological distance

The paper presents an evaluation of the Llama2 language model on machine translation tasks, with a focus on analyzing the impact of different languages and linguistic features in its training data. It examines translation quality for languages both seen and unseen during Llama2's training, across different model scales, chat/non-chat versions, and few-shot scenarios. A key component is correlating translation scores with typological distances between languages calculated using the URIEL database. The analysis explores which linguistic features and training languages demonstrate the strongest links with translation performance.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in the paper:

1. The paper examines both languages seen during Llama2's training (inllama) and unseen languages (outllama). What motivates the inclusion of unseen languages in their analysis, and what key insights did this provide about Llama2's translation capabilities?

2. The authors explore the effects of model scale, using chat versions, and adding shot counts. What were the key differences observed between these techniques in enhancing translation of unseen languages? Were there any languages that did not see much improvement from any of these techniques?

3. The paper calculates linguistic distances between translation languages and Llama2's training languages using the URIEL database. Why is using this typological database advantageous compared to just using raw training data quantities? What new perspectives did this analysis reveal?  

4. The analysis reveals syntactic similarity is not always the most important factor in determining translation quality. What other linguistic features emerged as more strongly correlated, and for translations of what language groups specifically?

5. Despite having the most training data, English syntactic similarity did not always rank the highest in terms of correlation with translation scores. What explanations are provided for this, and what languages instead showed stronger correlations?  

6. The paper demonstrates the feasibility of translating unseen languages. However, success varied greatly by language. What language traits or training data factors might explain this variability in translatability?

7. How reliable and unbiased are the evaluations? What potential issues could arise from relying on the URIEL database, COMET evaluations, or the parsing of Llama2's chat responses?

8. Flores and Wikipedia are used for evaluation despite risks of train/test contamination. What reassurances are provided and what alternative test sets could be used? How might performance differ?

9. The analysis is constrained by limited computational resources. What additional model variations, shot counts, languages etc would be beneficial to explore in future work?  

10. The paper suggests English-centric models may not be optimal for multilingual translation, posing multicentric models instead. What evidence motivates this proposal? How specifically might such alternative models be structured?
