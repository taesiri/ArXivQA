# [What Linguistic Features and Languages are Important in LLM Translation?](https://arxiv.org/abs/2402.13917)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem Statement
The paper evaluates the machine translation capabilities of Llama2, a recently released large language model, with a focus on analyzing performance across languages seen and unseen during Llama2's training. The authors note that assessing proprietary models like GPT is challenging due to lack of transparency into the training data. In contrast, Llama2 explicitly reports languages and amounts of training data used. 

The paper also aims to study which linguistic features and training languages have the biggest impact on Llama2's translation quality. Prior work has shown including related languages helps for low-resource translation, so the linguistic proximity of languages likely influences performance.

Methodology 
The authors test Llama2 on 41 languages - 26 seen during training ("inllama") and 15 unseen ("outllama"). Translation quality is measured by BLEU and COMET scores on the FLORES benchmark (excluding English to mitigate data leakage). The impact of model scale, instruction tuning, and few-shot learning is analyzed for the 15 "outllama" languages. 

Linguistic distances between languages are extracted from the URIEL database and correlated with translation scores. This analysis focuses on genetic, geographical, inventory, phonological and syntactic distances.

Key Results
- Llama2 can translate all "inllama" languages with BLEU >10 but struggles on some "outllama" languages.
- Increasing model scale boosts translation of unseen languages more than instruction tuning or few-shot learning.
- Syntactic similarity does not always have the highest correlation with translation quality.
- Despite much less training data, some languages show correlations comparable to English in determining translation performance.

Main Contributions
1) Comprehensive Llama2 translation evaluation on 41 languages 
2) Model scale more impactful than instruction tuning/few-shot learning
3) Syntactic proximity not always most important linguistic factor
4) Non-English languages can show equally strong or stronger correlations with translation quality

The results provide new perspective on potential limitations of English-centric models and viability of models centered around other languages.
