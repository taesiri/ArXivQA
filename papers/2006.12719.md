# Unsupervised Evaluation of Interactive Dialog with DialoGPT

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is: How can we develop an automatic evaluation metric for open-domain dialog systems that 1) does not rely on a ground-truth response, 2) measures multiple fine-grained dialog qualities, and 3) does not require training data?The key hypothesis is that a large pre-trained dialog model like DialoGPT has implicitly captured some notions of dialog quality through its training, and can therefore be used to evaluate dialog without explicit supervision. Specifically, the authors hypothesize that by having DialoGPT generate follow-up utterances in response to a dialog, they can estimate qualities like interestingness, relevance, etc. based on the likelihood of positive vs negative responses.So in summary, the paper is exploring how to leverage the knowledge captured in large pre-trained models like DialoGPT to develop better automatic evaluation metrics for dialog that do not suffer from the limitations of requiring ground-truth responses or training data. The key hypothesis is that DialoGPT has learned something about dialog quality that can be extracted in an unsupervised manner.


## What is the main contribution of this paper?

The main contribution of this paper is the introduction of the FED metric, a new automatic evaluation metric for dialog systems. Key points:- The FED metric measures the quality of a dialog system by looking at how likely the dialog model DialoGPT is to respond in certain ways, without needing a ground truth response. - It measures 18 different fine-grained qualities of dialog, at both the individual response level and whole dialog level.- The FED metric attains moderate to strong correlation with human judgments, despite requiring no training data or ground truth responses. - This demonstrates that dialog models like DialoGPT have implicitly captured an understanding of dialog quality through pre-training.- The paper also introduces the FED dataset, a collection of human annotations of dialog quality for 124 conversations, which can be used to evaluate automatic metrics.In summary, the main contribution is a new unsupervised automatic dialog evaluation method that correlates well with humans and provides interpretable fine-grained quality scores. This is enabled by leveraging the implicit knowledge of pre-trained models like DialoGPT.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper introduces a new automatic dialog evaluation metric called FED that leverages a pre-trained dialog model to assess the quality of conversational agents without needing reference responses or training data.
