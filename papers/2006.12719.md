# [Unsupervised Evaluation of Interactive Dialog with DialoGPT](https://arxiv.org/abs/2006.12719)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is: How can we develop an automatic evaluation metric for open-domain dialog systems that 1) does not rely on a ground-truth response, 2) measures multiple fine-grained dialog qualities, and 3) does not require training data?The key hypothesis is that a large pre-trained dialog model like DialoGPT has implicitly captured some notions of dialog quality through its training, and can therefore be used to evaluate dialog without explicit supervision. Specifically, the authors hypothesize that by having DialoGPT generate follow-up utterances in response to a dialog, they can estimate qualities like interestingness, relevance, etc. based on the likelihood of positive vs negative responses.So in summary, the paper is exploring how to leverage the knowledge captured in large pre-trained models like DialoGPT to develop better automatic evaluation metrics for dialog that do not suffer from the limitations of requiring ground-truth responses or training data. The key hypothesis is that DialoGPT has learned something about dialog quality that can be extracted in an unsupervised manner.


## What is the main contribution of this paper?

The main contribution of this paper is the introduction of the FED metric, a new automatic evaluation metric for dialog systems. Key points:- The FED metric measures the quality of a dialog system by looking at how likely the dialog model DialoGPT is to respond in certain ways, without needing a ground truth response. - It measures 18 different fine-grained qualities of dialog, at both the individual response level and whole dialog level.- The FED metric attains moderate to strong correlation with human judgments, despite requiring no training data or ground truth responses. - This demonstrates that dialog models like DialoGPT have implicitly captured an understanding of dialog quality through pre-training.- The paper also introduces the FED dataset, a collection of human annotations of dialog quality for 124 conversations, which can be used to evaluate automatic metrics.In summary, the main contribution is a new unsupervised automatic dialog evaluation method that correlates well with humans and provides interpretable fine-grained quality scores. This is enabled by leveraging the implicit knowledge of pre-trained models like DialoGPT.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper introduces a new automatic dialog evaluation metric called FED that leverages a pre-trained dialog model to assess the quality of conversational agents without needing reference responses or training data.


## How does this paper compare to other research in the same field?

Here are a few key points about how this paper compares to other research in dialog evaluation:- It introduces a new dataset (FED) for evaluating dialog systems on multiple fine-grained qualities. Most prior work has focused on only 1-2 qualities like relevance or fluency. The FED dataset has annotations for 18 different qualities, allowing for more comprehensive evaluation.- It proposes a new automatic metric (FED metric) that correlates decently with human judgments without requiring any training data or ground truth responses. Most prior automatic metrics rely on comparisons to ground truth responses and/or require training on human annotations. The unsupervised nature of the FED metric makes it more flexible.- The FED metric leverages recent advances in pre-trained dialog models like DialoGPT. It demonstrates these models have implicitly learned notions of dialog quality that can be extracted. Prior work has not really explored using pre-trained models in this way.- Results are demonstrated for open-domain conversational agents. Most prior work has focused on goal-oriented dialog systems. Evaluating open-domain chit-chat is inherently more challenging due to the lack of clear goals/ground truths.- Correlations with human judgments are decent but imperfect. Performance is weaker on some qualities than others. This highlights room for improvement in future work leveraging pre-trained models or other approaches.Overall, the paper makes solid contributions in terms of the dataset, unsupervised metric, and analysis around pre-trained models. But there is certainly ample opportunity for future work to build on these ideas and improve dialog evaluation further. The results are a step forward but not definitive.


## What future research directions do the authors suggest?

Based on my reading of the paper, the authors suggest the following future research directions:1. The FED dataset can be used to benchmark automatic evaluation metrics on eighteen fine-grained dialog qualities. 2. Building on this paper, future work could identify mechanisms that further leverage pre-trained models for dialog evaluation.3. Future work can explore strategies for extending the FED metric beyond open-domain chit-chat conversations to goal oriented dialog. 4. The FED metric can be used to evaluate, analyze and improve dialog systems.5. Future work can better leverage pre-trained models for dialog evaluation and devise better pre-trained models specifically for dialog evaluation to mitigate the shortcomings of DialoGPT on certain dialog qualities.In summary, the main future directions are: benchmarking metrics using the FED dataset, improving pre-trained models for dialog evaluation, extending the FED metric to goal-oriented dialog, and using FED to evaluate and improve dialog systems. The authors suggest leveraging the resources they have developed (FED dataset and metric) as well as improving upon their approach to advance dialog evaluation research.


## Summarize the paper in one paragraph.

Here is a one paragraph summary of the paper:This paper introduces the FED dataset and FED metric for evaluating interactive dialog systems. The FED dataset consists of human annotations of conversations with dialog systems, scoring them on 18 fine-grained dialog qualities at both the turn and dialog levels. The FED metric leverages the pre-trained DialoGPT model to measure dialog quality without needing a ground-truth response or training data. It has DialoGPT give likelihoods for various possible follow-up utterances to a system response, indicating its quality. Experiments show the FED metric attains moderate to strong correlation with human judgments for both turn and dialog level qualities. The paper demonstrates that pre-trained models like DialoGPT have implicitly captured notions of dialog quality that can be extracted without supervision. The FED dataset and metric enable analyzing dialog system strengths/weaknesses and tuning them on fine-grained qualities.


## Summarize the paper in two paragraphs.

Here is a two paragraph summary of the key points in the paper:The paper introduces the FED dataset and metric for evaluating open-domain dialog systems. The FED dataset consists of human annotations of 124 conversations from the Meena, Mitsuku, and human dataset. Conversations were annotated with 18 fine-grained dialog qualities at both the turn and dialog levels, resulting in over 4700 quality judgements. Analysis of the dataset provides insights into the comparative strengths of the systems and the qualities that contribute most to overall human judgements of dialog. The paper also presents the FED metric which leverages the pre-trained DialoGPT model to assess dialog quality without relying on a reference response or training data. For each dialog quality, positive and negative utterances are generated that DialoGPT would hypothetically say in response to a good or bad dialog act. By comparing the likelihood of positive and negative responses, the FED metric attains moderate to strong correlation with human judgements for many dialog qualities. This demonstrates DialoGPT's implicit understanding of qualities like interestingness and relevance. The FED metric and dataset enable more fine-grained unsupervised evaluation to advance open-domain dialog research.


## Summarize the main method used in the paper in one paragraph.

The main method presented in the paper is the FED (fine-grained evaluation of dialog) metric, which is an automatic evaluation metric for dialog systems. The key ideas behind FED are:- It leverages the pre-trained dialog model DialoGPT to assess the quality of a system response by looking at how likely DialoGPT is to produce certain follow-up utterances. For example, if DialoGPT is more likely to respond "That's really interesting!" than "That's boring", the system response is deemed interesting.- It measures 18 different fine-grained dialog qualities (e.g. interesting, coherent, error recovery) at both the turn and dialog levels, without needing ground truth responses. - It requires no training data or supervision. The follow-up utterances were handwritten and minimally tuned on a small subset of the dataset. - By having DialoGPT take the role of the user and assign probabilities to positive and negative follow-up utterances, it extracts information about dialog quality in an unsupervised manner.In summary, the FED metric leverages the capabilities of the pre-trained DialoGPT model to assess fine-grained qualities of a dialog system's responses and overall dialogs without ground truth responses or training data. This is achieved by using DialoGPT to estimate the likelihood of handwritten positive and negative follow-up utterances that provide feedback on the quality of the dialog.


## What problem or question is the paper addressing?

Based on my reading of the paper, the key problem the authors are trying to address is developing automatic evaluation metrics for dialog systems that are meaningful, interpretable, and can be applied in interactive settings without relying on a ground truth response or training data. The paper highlights that standard language generation metrics like BLEU perform poorly for evaluating dialog systems. This is due to the one-to-many nature of dialog and the fact that dialog quality is multi-faceted. The authors argue that new automatic metrics are needed that can capture multiple fine-grained qualities of dialog without needing a ground truth response. They introduce a new metric called FED (Fine-grained Evaluation of Dialog) that aims to address these challenges.Specifically, the key questions/problems the paper seems to be tackling are:- How to automatically evaluate dialog systems on multiple fine-grained qualities without relying on a ground truth response?- How to develop an automatic metric that works in interactive settings where there may not be a static dialog corpus? - How to leverage large pre-trained dialog models like DialoGPT for automatic dialog evaluation without any additional training data or supervision?- How well does such a model-based metric correlate with human judgments on fine-grained dialog qualities?So in summary, the paper is focused on developing more effective automatic dialog evaluation metrics that capture nuanced qualities of dialog, do not require ground truth responses, and can be applied in interactive settings.
