# [Sketching the Future (STF): Applying Conditional Control Techniques to   Text-to-Video Models](https://arxiv.org/abs/2305.05845)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question or hypothesis appears to be: Can a novel approach that combines zero-shot text-to-video generation with conditional control techniques improve the output quality and controllability of text-to-video models?The key points are:- Text-to-video generation is challenging due to lack of training data and difficulty controlling output with just text prompts. - The authors propose a new method called "Sketching the Future" (STF) that takes sketched frames as additional input to better guide video generation.- STF combines zero-shot text-to-video (Text2Video Zero) with conditional control (ControlNet).- The hypothesis is that by adding sketched frames as conditional input, STF can improve the quality and control over text-to-video generation compared to Text2Video Zero alone.- Experiments aim to demonstrate STF's ability to produce videos that more accurately match the desired motion specified through the sketched frames, which Text2Video Zero struggles with using just text prompts.In summary, the central research question is whether the proposed STF approach can enhance text-to-video generation by incorporating sketched frames as conditional control input. The hypothesis is that STF will improve output quality and controllability compared to baseline Text2Video Zero.


## What is the main contribution of this paper?

The main contribution of this paper is proposing a novel method called STF (Sketching the Future) that combines zero-shot text-to-video generation with conditional control techniques to improve the output of text-to-video models. Specifically, the key contributions are:- Proposing an approach to take multiple sketched frames as input along with a text prompt, and generate a video output that matches the flow and motion conveyed across the sketched frames.- Building upon the Text-to-Video Zero architecture and incorporating ControlNet to enable conditional control based on the input sketched frames. - Interpolating additional frames between the input sketches and using them along with the text prompt in Text-to-Video Zero conditioned on the sketched frames via ControlNet.- Demonstrating through experiments that this approach can generate more accurate and high-quality video content compared to just using Text-to-Video Zero with text prompts.- Providing a comprehensive set of resources including a demo video, project website, GitHub repo, and Colab for users to explore the method.In summary, the key contribution is proposing and demonstrating an effective way to combine zero-shot text-to-video generation with sketch-based conditional control to achieve better video generation results. The method provides more control over the output to match user's intended vision.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper proposes a novel method called Sketching the Future (STF) that combines zero-shot text-to-video generation using the Text-to-Video Zero architecture with conditional control techniques from ControlNet to enable sketch-guided video generation from textual prompts.
