# [Towards Deeper Graph Neural Networks](https://arxiv.org/abs/2007.09296v1)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question is: 

How can we enable deeper graph neural networks without suffering from performance deterioration?

Specifically, the paper investigates the issue of performance degradation in deep graph neural networks, which has been attributed to the "over-smoothing" problem where node representations become indistinguishable after multiple layers. 

The key hypothesis is that the entanglement of feature transformation and propagation in current graph convolution operations is the main factor compromising performance in deeper models. 

The paper aims to demonstrate that by decoupling these two operations, deeper graph neural networks can leverage larger receptive fields and incorporate more contextual information without over-smoothing. This allows them to learn better node representations without suffering from performance deterioration as typically happens when simply stacking multiple graph convolution layers.

In summary, the central question is how to unlock the benefits of larger receptive fields in deeper graph neural networks while avoiding the over-smoothing problem, with the key hypothesis being that decoupling feature transformation from propagation can achieve this.
