# [Towards Joint Optimization for DNN Architecture and Configuration for   Compute-In-Memory Hardware](https://arxiv.org/abs/2402.11780)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Compute-in-memory (CiM) architectures are promising to alleviate bandwidth bottlenecks in Von-Neumann architectures for large deep neural networks (DNNs). However, constructing optimal CiM hardware is challenging as the memory hierarchy configuration (cache sizes, bandwidths etc.) may not match well with a DNN's attributes. 

- Neural architecture search (NAS) has been effective in finding efficient DNN sub-networks for a target hardware metric budget. But it assumes fixed hardware configuration, often yielding sub-optimal sub-networks.

- There is a need for joint optimization of DNN architecture and CiM hardware configurations to create Pareto optimal options balancing accuracy and hardware metrics like latency.

Proposed Solution:
- The paper proposes CiMNet, a framework to jointly search for optimal DNN sub-networks and CiM hardware configurations. It can comprehend the complex interplay between sub-network performance and choices like bandwidth, processing element size, memory size.

- It uses a multi-objective evolutionary search to evaluate accuracy and cycles for various sub-networks and hardware configurations, aided by predictors instead of real evaluations.

- The search space includes elastic parameters for both model (kernel size, width, depth etc.) and hardware (bandwidths, memory sizes etc.). Overall compute and memory capacity is kept constant.

Main Contributions:
- Novel joint optimization paradigm and framework for co-design of near optimal DNN algorithms and hardware without frozen hardware assumption.

- Infrastructure for DNN compiler and accurate simulator to project end-to-end latency for any hardware configuration and sub-network.

- Experiments show joint search can yield 5.4x fewer cycles for similar ImageNet accuracy compared to 2.1x reduction with only model architecture search.

In summary, the paper makes significant contributions in enabling joint search over elastic DNN architectures and hardware configurations to achieve synergistic and optimized combinations for efficiency.
