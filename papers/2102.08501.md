# [DEUP: Direct Epistemic Uncertainty Prediction](https://arxiv.org/abs/2102.08501)

## What is the central research question or hypothesis that this paper addresses?

The central hypothesis of this paper is that model uncertainty/bias should be accounted for when estimating epistemic uncertainty, in addition to approximation uncertainty. The key claims made are:1) Discrepancy-based measures of epistemic uncertainty (e.g. variance of the Bayesian posterior predictive) only capture approximation uncertainty and miss model uncertainty/bias. This can lead to poor uncertainty estimates and decisions when the model is misspecified.2) The excess risk, defined as the gap between the generalization error and the Bayes error, captures both approximation and model uncertainty. It is proposed as a more robust measure of epistemic uncertainty.3) A framework called DEUP is proposed to directly estimate the excess risk by training a separate model to predict the generalization error and subtracting an estimate of aleatoric uncertainty.4) DEUP can improve optimization and exploration in interactive learning settings like sequential model optimization and RL compared to variance-based uncertainty measures.5) The error predictor in DEUP can generalize its uncertainty estimates to out-of-distribution data by using additional "stationarizing" features related to density and model variance.So in summary, the central hypothesis is that epistemic uncertainty estimates should account for model uncertainty/bias and not just approximation uncertainty. DEUP is proposed as a way to achieve this.


## What is the main contribution of this paper?

The main contributions of this paper are:1. It provides a theoretical analysis of the sources of epistemic uncertainty and argues that model bias/misspecification should be accounted for, in addition to approximation uncertainty captured by variance. 2. It proposes the DEUP framework which trains an error predictor to estimate the generalization error (excess risk) directly. Subtracting an estimate of aleatoric uncertainty gives an estimate of epistemic uncertainty.3. It describes how DEUP can be adapted for interactive learning settings like active learning, sequential model optimization, and reinforcement learning. It proposes using stationarizing features like density, variance estimates, and an in-sample bit as inputs to the error predictor to make it robust to non-stationarities.4. It provides experimental validation that DEUP's uncertainty estimates lead to improved performance on downstream tasks like SMO, exploration in RL, and OOD detection compared to variance-based Bayesian approaches. It also shows DEUP can estimate uncertainty well in a real-world drug synergy prediction task.In summary, the key idea is to directly predict the generalization error using out-of-sample errors to get an estimate of epistemic uncertainty that captures model bias, unlike just using variance. The method is model-agnostic and shown to be useful across different interactive learning settings.
