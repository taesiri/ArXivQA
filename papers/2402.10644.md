# [Linear Transformers with Learnable Kernel Functions are Better   In-Context Models](https://arxiv.org/abs/2402.10644)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Traditional Transformer models scale quadratically with sequence length, making them impractical for very long sequences. Alternative approaches like Linear Transformers and State Space Models (SSMs) reduce this complexity but perform worse on tasks requiring associative recall from long contexts.

- The 'Based' hybrid architecture combines linear attention with a novel kernel function inspired by exponential functions to improve long-range associative recall. However, it still struggles with very long contexts relative to standard Transformer models. 

Method:
- The authors propose 'ReBased', which refines the Based kernel function to better handle long contexts. Specifically:
  - They make the kernel parameters learnable to adapt to actual query/key values during training. 
  - They add layer normalization before the kernel computation.
  
- ReBased is evaluated on the Multi-Query Associative Recall (MQAR) task requiring retrieving multiple tokens from context. It outperforms Based across varying context lengths and model sizes.

- ReBased is also tested by pretraining a language model on the Pile dataset. It shows improved perplexity, especially on tokens requiring associative recall from context.

Main Contributions:
- A novel ReBased architecture that refines the Based kernel function with learnable parameters and normalization to better handle long contexts.

- Experiments showing ReBased outperforms Based on associative recall tasks, especially with very long (2k+ token) contexts and smaller models.

- Analysis of attention patterns suggests ReBased is better at focusing on relevant tokens but still lags behind standard Transformer attention.

- Results highlight the remaining gap between attention models and alternatives like ReBased for critical capabilities like associative recall over long contexts.
