# [Gradient Norm Aware Minimization Seeks First-Order Flatness and Improves   Generalization](https://arxiv.org/abs/2303.03108)

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper addresses is how to define and optimize a stronger measure of flatness of the loss landscape to improve generalization of deep neural networks. The key points are:- The commonly used "zeroth-order" flatness that measures the maximal loss within a perturbation radius can be insufficient to discriminate good minima with low generalization error. - This paper proposes "first-order" flatness, which measures the maximal gradient norm within a radius. It shows first-order flatness is a stronger measure than zeroth-order flatness, as it bounds the maximal eigenvalue of the Hessian and the zeroth-order flatness.- The paper develops an optimization method called Gradient Norm Aware Minimization (GAM) to optimize first-order flatness along with the prediction loss. - Experiments show GAM consistently improves generalization over SGD, Adam, and SAM (a zeroth-order flatness optimization method) across various datasets and network architectures.In summary, the central hypothesis is that optimizing for first-order flatness, a stronger measure of flatness than previously used, will lead to minima that generalize better, which is supported by the empirical results.
