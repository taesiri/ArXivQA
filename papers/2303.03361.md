# [Nerflets: Local Radiance Fields for Efficient Structure-Aware 3D Scene   Representation from 2D Supervision](https://arxiv.org/abs/2303.03361)

## What is the central research question or hypothesis that this paper addresses?

 The central research question of this paper is how to efficiently learn a structure-aware 3D scene representation from only 2D supervision. 

The key points are:

- The goal is to produce a structured 3D scene representation that captures semantics, instances, and appearance for tasks like novel view synthesis, segmentation, and editing. 

- The representation should be learned from only 2D images, without 3D supervision or input geometry like meshes or point clouds. This makes it more widely applicable since 2D data is more abundant.

- The representation should be efficient and compact compared to global neural radiance fields like NeRF. This is achieved through a decomposition into local neural fields called "nerflets".

- The nerflet representation consists of small MLPs that each store pose, shape, appearance, semantic, and instance information. Together they form a decomposed representation of the scene.

- The nerflets are optimized jointly end-to-end with 2D photometric and semantic losses. This encourages them to move to cover the appropriate regions and objects in the scene.

- After optimization, nerflets provide an explicit panoptic 3D decomposition useful for tasks like novel view synthesis, segmentation, and editing.

In summary, the key hypothesis is that a structured decomposition into local neural fields can produce an efficient, interpretable 3D scene representation from abundant 2D supervision alone. Experiments on real-world datasets support this.
