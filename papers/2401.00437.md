# [BatchEval: Towards Human-like Text Evaluation](https://arxiv.org/abs/2401.00437)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem: 
Current LLM-based text evaluators suffer from issues of sensitivity to prompt design, poor robustness to noise, and inferior ensemble performance. These stem from their sole reliance on the criterion prompt as reference for scoring, unlike human evaluators who leverage both criterion and sample comparison.

Method - BatchEval:
The paper proposes BatchEval, an iterative batch-wise text evaluation paradigm. Key ideas:

1) Evaluates samples batched together to introduce in-batch comparison as additional reference alongside criterion. Allows more discriminative scoring.  

2) Iteratively changes batch composition to provide diverse references across rounds. Enhances ensemble diversity.

3) Explores variants - two stage procedure to first analyze then score works best. Heterogeneous batches outperform homogeneous/random. Decimal scoring captures nuances.

Contributions:  

1) Identifies limitations of existing LLM evaluators relative to human evaluators, theoretically and empirically.

2) Proposes BatchEval paradigm to alleviate said limitations. Shows 10.5% correlation gain over prior art on 4 text tasks while using only 64% of the API cost.

3) Validates improved robustness to prompt design/noise and analyzes workings of BatchEval via attention visualization. 

4) Explores variants to discover optimal settings - two stage procedure with heterogeneous batches and decimal scoring works best.

In summary, the paper introduces BatchEval, an iterative batch-wise evaluation paradigm that simulates human evaluation more closely and achieves significantly improved consistency, robustness and cost-efficiency.
