# [AM^2-EmoJE: Adaptive Missing-Modality Emotion Recognition in   Conversation via Joint Embedding Learning](https://arxiv.org/abs/2402.10921)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Human emotion recognition in conversations is challenging due to heterogeneity within and across modalities (text, audio, video) and their complex interplay. 
- Existing multimodal methods assume complete modalities during testing, which may not hold due to privacy, device or security constraints.
- The relative contribution of each modality in exhibiting emotions is not uniform. For example, facial expressions may have limited information compared to body language.
- Missing modality information during inference and its influence on learning multimodal interaction patterns is understudied.

Proposed Solution - AM^2-EmoJE:
- Query Adaptive Fusion (QAF) - Dynamically learns relative importance of cross-attended mode-specific representations in a query-specific manner. Aims to prioritize mode-invariant spatial query patterns while retaining mode-exclusive aspects.

- Multimodal Joint Embedding - Addresses missing modalities during inference by emphasizing correlated cross-modal patterns. Aligns mode-specific descriptors pairwise in a joint embedding space to compensate for missing modalities.  

Main Contributions:
- QAF allows a flexible fusion mechanism adapted per query to handle missing modalities and data heterogeneity.

- Joint embedding learning enables effective "mode switching" to deliver competitive performance despite missing modalities during inference.

- Achieves state-of-the-art on MELD and IEMOCAP datasets. Shows 2-5% performance gain over baselines in various missing modality scenarios.

- Preserves privacy by relying more on body language than facial expressions with only 2% performance drop.

In summary, the paper presents an adaptive multimodal framework for emotion recognition in conversations that can handle missing modalities and data heterogeneity via flexible fusion and joint embedding learning.


## Summarize the paper in one sentence.

 Here is a one-sentence summary of the paper:

The paper proposes a multimodal emotion recognition model called AM^2-EmoJE that adaptively fuses modalities, preserves privacy by relying more on body language, and handles missing modalities at test time via a joint embedding scheme.


## What is the main contribution of this paper?

 Based on my reading of the paper, the main contributions of this work are:

1) A Query Adaptive Fusion (QAF) mechanism that can automatically learn the relative importance of different modalities (text, audio, video) for a given query in an adaptive way, prioritizing more reliable modes while retaining mode-specific details. 

2) A multimodal joint embedding learning framework that can effectively handle missing modalities during test time by learning correlated patterns across modalities and aligning them in a shared embedding space. This allows the model to compensate when some modalities are missing.

3) Extensive experiments on two datasets - MELD and IEMOCAP - demonstrating state-of-the-art performance. The model shows 2-4% improvement in weighted F1 score over previous methods in the complete multimodal setting. It also exhibits strong performance in various missing modality scenarios thanks to the joint embedding learning, with gains of 2-8% weighted F1 compared to without joint embedding learning.

In summary, the main contributions are the query adaptive fusion and multimodal joint embedding learning, which enable the model to handle heterogeneous, incomplete multimodal emotion recognition in conversations more effectively. The experiments validate these contributions.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and keywords associated with this paper include:

- Multimodal emotion recognition
- Joint embedding learning
- Adaptive fusion
- Missing modality
- Text, audio, and video modalities
- Query adaptive fusion (QAF)
- Multimodal joint embedding
- Guided noise contrastive estimation (NCE) loss
- Focal loss
- Cross-modal attention
- MELD and IEMOCAP datasets

The paper proposes a model called AM^2-EmoJE for adaptive missing-modality emotion recognition in conversations via joint embedding learning. The key ideas include an adaptive fusion technique to handle missing modalities, a joint embedding method to align representations across modalities, and losses like guided NCE and focal loss to help learn better multimodal representations. Experiments on standard datasets demonstrate state-of-the-art performance, including in missing modality settings.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in this paper:

1. The paper proposes a Query Adaptive Fusion (QAF) mechanism. How does this mechanism determine the relative importance of each modality (text, audio, video) for a given query? What are the benefits of having a query-specific fusion approach?

2. The Adaptive Cross-attended Network (ACN) uses a masking mechanism during training to simulate missing modalities. Explain how this masking mechanism works and how it helps the model handle missing modalities during inference.

3. What is the purpose of having separate face and body sequences for the video modality? How are these two visual sequences modeled and what role does the self-attention layer play in combining them? 

4. Explain the working of the proposed Guided NCE loss function. How does it help in learning better aligned representations across modalities? What advantages does it offer over using only the Focal loss?

5. The multimodal joint embedding module learns invertible linear mappings to align representations from different modalities. Walk through the details of the Jensen-Shannon divergence based loss function used for this module.  

6. The model leverages both dialogue context and speaker context by using two separate BiLSTM streams. Explain the difference between these two contexts and how they are helpful in capturing emotion dynamics.

7. The weighted fusion assigns higher weights to more reliable modalities based on the query. Propose some heuristic strategies that can be used to determine reliability of each modality.  

8. How robust is the performance of the model in missing modality scenarios compared to previous state-of-the-art methods? Explain if and why the drop in performance is less.

9. The model analyzes facial expressions and body language separately. Do you think jointly modeling them could be more optimal? Justify your view.

10. The paper evaluates only on two datasets - MELD and IEMOCAP. What other conversational emotion recognition datasets could have been used for evaluation? Would the trends observed on MELD and IEMOCAP generalize to other datasets as well?
