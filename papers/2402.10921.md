# [AM^2-EmoJE: Adaptive Missing-Modality Emotion Recognition in   Conversation via Joint Embedding Learning](https://arxiv.org/abs/2402.10921)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Human emotion recognition in conversations is challenging due to heterogeneity within and across modalities (text, audio, video) and their complex interplay. 
- Existing multimodal methods assume complete modalities during testing, which may not hold due to privacy, device or security constraints.
- The relative contribution of each modality in exhibiting emotions is not uniform. For example, facial expressions may have limited information compared to body language.
- Missing modality information during inference and its influence on learning multimodal interaction patterns is understudied.

Proposed Solution - AM^2-EmoJE:
- Query Adaptive Fusion (QAF) - Dynamically learns relative importance of cross-attended mode-specific representations in a query-specific manner. Aims to prioritize mode-invariant spatial query patterns while retaining mode-exclusive aspects.

- Multimodal Joint Embedding - Addresses missing modalities during inference by emphasizing correlated cross-modal patterns. Aligns mode-specific descriptors pairwise in a joint embedding space to compensate for missing modalities.  

Main Contributions:
- QAF allows a flexible fusion mechanism adapted per query to handle missing modalities and data heterogeneity.

- Joint embedding learning enables effective "mode switching" to deliver competitive performance despite missing modalities during inference.

- Achieves state-of-the-art on MELD and IEMOCAP datasets. Shows 2-5% performance gain over baselines in various missing modality scenarios.

- Preserves privacy by relying more on body language than facial expressions with only 2% performance drop.

In summary, the paper presents an adaptive multimodal framework for emotion recognition in conversations that can handle missing modalities and data heterogeneity via flexible fusion and joint embedding learning.
