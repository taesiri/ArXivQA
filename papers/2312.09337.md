# [Promptable Behaviors: Personalizing Multi-Objective Rewards from Human   Preferences](https://arxiv.org/abs/2312.09337)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper "Promptable Behaviors: Personalizing Multi-Objective Rewards from Human Preferences":

Problem: 
Customizing robotic behaviors to align with diverse human preferences is challenging. Common practices use hand-designed rewards, which is difficult for non-experts. Recent methods require re-training agents for each new preference. This paper aims to personalize robotic agents to diverse preferences without needing to re-train the agent.

Proposed Solution:
The paper proposes "Promptable Behaviors", a framework with two key components:
1) Train a multi-objective reinforcement learning (MORL) policy conditioned on a reward weight vector across multiple navigation objectives like efficiency, exploration and safety.
2) Infer the reward weight vector representing the user's preferences through interactions like demonstrations, preference feedback on trajectory comparisons, or language instructions.  

The framework takes a modular approach - train once then customize. The policy is trained only once using MORL. Then users can provide various interactions to infer reward weights for personalization without additional policy training.

Main Contributions:
1) A novel personalization framework that aligns agents to diverse preferences without re-training, using MORL and reward weight inference.
2) Three distinct interaction methods for preference inference: demonstrations, preference feedback, language. Each offers unique advantages.
3) Demonstrated effectiveness in two long-horizon personalized navigation tasks - object-goal navigation and flee navigation, in ProcTHOR and RoboTHOR environments.
4) Evaluations show the framework can effectively prompt agent behaviors via reward weight adjustments to satisfy human preferences in different scenarios. Preference feedback performs best overall.

In summary, the paper presents a new paradigm for efficiently personalizing embodied AI agents using multi-objective learning and minimally invasive human interactions, with demonstrations across complex navigation tasks.
