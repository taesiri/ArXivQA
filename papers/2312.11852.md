# [Predicting Human Translation Difficulty with Neural Machine Translation](https://arxiv.org/abs/2312.11852)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Translating text from one language to another varies in difficulty for human translators. Some words and phrases require more time and effort to process cognitively. 
- Prior work has proposed surprisal and attention features from neural machine translation (NMT) models as potential predictors of this translation difficulty, but evaluations have been limited.

Proposed Solution:
- Use reading times and production durations from the CRITT Translation Process Research Database as measures of translation difficulty. Data covers 312 translators working on 13 language pairs. 
- Evaluate surprisal from a multilingual language model and an NMT model as predictors of human translation difficulty. Also extract 3 types of attention features from the NMT model - encoder, decoder, and cross-attention.
- Compare predictive performance of surprisal and attention features using log-likelihood tests. Also analyze the relationships through regression coefficients.

Main Contributions:
- Surprisal is found to be the best single predictor of human translation duration. Translation surprisal outperforms monolingual surprisal.
- Attention features provide supplementary predictive power when combined with surprisal measures. 
- Analysis of coefficients shows interpretable relationships between attention features and translation difficulty.
- Comprehensive investigation of predicting human translation difficulty using state-of-the-art NLP models and the largest dataset of translators to date.

In summary, the paper demonstrates surprisal's efficacy in predicting translation difficulty across languages. The analysis also provides novel findings into how attention patterns captured by NMT relate to processing challenges faced by human translators.
