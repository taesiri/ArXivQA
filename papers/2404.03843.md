# [Scaling Motion Forecasting Models with Ensemble Distillation](https://arxiv.org/abs/2404.03843)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Motion forecasting is critical for autonomous vehicles to plan safe routes, but model accuracy is limited by onboard compute budgets. 
- Ensembling models can improve accuracy but increases compute costs.
- There is a need for methods to improve forecasting accuracy without exceeding compute budgets.

Proposed Solution:
- Develop ensemble distillation framework to transfer knowledge from a high-performing ensemble teacher into a smaller student model.
- Create ensemble teachers by training multiple copies of baseline forecasting models.
- Propose method to aggregate outputs across heterogeneous ensemble members.  
- Formulate distillation losses for matching full trajectory distributions.
- Use efficient sampling and mapping between teacher and student modes.

Contributions:
- Achieve state-of-the-art results on WOMD and Argoverse datasets with ensemble model.
- Propose general framework for ensemble distillation for motion forecasting.
- Demonstrate distilled student matches ensemble performance on WOMD with 20x fewer FLOPs.
- Analysis of ensemble scaling trends and effect of distillation hyperparameters.
- Show improved performance from distillation on Argoverse dataset.

In summary, this paper makes key contributions in developing methods for ensemble distillation to improve motion forecasting accuracy for autonomous vehicles while remaining feasible to deploy within onboard compute constraints. The proposed techniques are demonstrated to achieve state-of-the-art results on two key datasets.
