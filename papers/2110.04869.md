# [Global Vision Transformer Pruning with Hessian-Aware Saliency](https://arxiv.org/abs/2110.04869)

## What is the central research question or hypothesis that this paper addresses?

The central research question addressed in this paper is how to improve the efficiency of vision transformer (ViT) models by better redistributing parameters within and across ViT blocks. The key hypothesis is that the commonly used design of assigning uniform dimensions to all ViT blocks is suboptimal, and a better parameter distribution can lead to enhanced efficiency and accuracy tradeoff.Specifically, the paper challenges the conventional wisdom of using the same dimension settings across all layers in a ViT model, and hypothesizes that redistributing parameters non-uniformly can allow the model to utilize its capacity more efficiently. It aims to systematically explore the potential for parameter redistribution in ViTs via a global structural pruning approach.The main hypothesis is that global pruning can provide insights on the relative importance and redundancy of different components and layers in ViT models, guiding more efficient parameter allocation. By analyzing the pruned model architectures, the paper hopes to discover new design principles and heuristics for constructing more efficient vision transformers that outperform conventional uniform design.In summary, the central hypothesis is that the parameter distribution in ViT models is suboptimal and can be improved by leveraging the insights from global pruning, leading to enhanced efficiency and performance tradeoffs compared to the prevailing practice of using uniform dimensions. The paper aims to provide both empirical evidence through pruned models and analysis to shed light on efficient ViT design.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contributions appear to be:- Proposing NViT, a novel hardware-friendly global structural pruning algorithm for Vision Transformers (ViTs). The pruning method uses a latency-aware, Hessian-based importance criteria to reduce parameters and FLOPs in a ViT model. - Performing a systematic analysis on the prunable components in a ViT model, including the embedding dimension, number of heads, MLP hidden dimension, etc. The paper shows these components have distinct sensitivity to pruning.- Exploring hardware-friendly parameter redistribution in ViTs through the global pruning process. The paper discovers trends like high prunability of ViT models, unique parameter distribution across stacked ViT blocks, etc. - Achieving efficient ViT models named NViT through the pruning process that outperform prior work like DeiT, SWIN, and AutoFormer models in terms of accuracy under similar FLOPs and latency constraints.- Providing insights on the differences in learning dynamics and architecture design principles between CNNs vs ViTs based on the global pruning study.- Proposing a simple yet effective heuristic for redistributing ViT parameters based on the pruning insights, which leads to improved accuracy over DeiT models of similar sizes.In summary, the main contribution appears to be the proposal of a novel ViT pruning method that provides insights on efficient ViT design and leads to more accurate yet hardware-friendly ViT models called NViT. The work also compares ViT vs CNN models and proposes simple heuristics for improved ViT architecture.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper proposes a new method for vision transformer compression and parameter redistribution, where they utilize global structural pruning with a novel Hessian-based importance criteria to achieve near lossless compression on DeiT models, discover insights on parameter redistribution, and design more efficient vision transformer architectures that outperform prior art.


## How does this paper compare to other research in the same field?

Here are a few key points comparing this paper to other related research:- This paper focuses specifically on vision transformers (ViTs), while much previous work on transformer pruning and efficiency has focused on natural language processing models like BERT. The authors argue that pruning ViTs poses unique challenges due to their more complex architecture.- Prior work on ViT pruning like Zhu et al. and Chen et al. applied uniform sparsity ratios across all components and did not optimize directly for latency reduction. This paper introduces a global pruning approach tailored to ViTs that allows more flexibility in compressing different components and incorporates latency estimates.- Compared to NAS-based methods like AutoFormer, this paper's approach explores the full design space continuously via iterative pruning rather than searching over a limited set of choices. The single pruning process is also lower cost than NAS supernet training.- For model distillation, this paper distills from both a CNN teacher (as in DeiT) and the original unpruned ViT. Most prior work uses either a teacher or the original model.- This paper provides detailed analysis into the trends discovered in effective ViT pruning, such as the importance of different components and how pruning affects parameter distribution across layers. This offers new architectural insights compared to past ViT compression methods.- The proposed approach achieves significantly higher compression rates and speedups compared to previous state-of-the-art. For example, near lossless 2.6x FLOPs compression on DeiT-Base versus 2x for CNNs, and 1.9x speedup versus 1.6x.Overall, this paper makes several novel contributions tailored to the ViT architecture and demonstrates notably improved efficiency over past work in ViT compression and design. The architectural insights from pruning could help guide more efficient ViT designs in the future.
