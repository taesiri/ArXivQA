# [Global Vision Transformer Pruning with Hessian-Aware Saliency](https://arxiv.org/abs/2110.04869)

## What is the central research question or hypothesis that this paper addresses?

The central research question addressed in this paper is how to improve the efficiency of vision transformer (ViT) models by better redistributing parameters within and across ViT blocks. The key hypothesis is that the commonly used design of assigning uniform dimensions to all ViT blocks is suboptimal, and a better parameter distribution can lead to enhanced efficiency and accuracy tradeoff.Specifically, the paper challenges the conventional wisdom of using the same dimension settings across all layers in a ViT model, and hypothesizes that redistributing parameters non-uniformly can allow the model to utilize its capacity more efficiently. It aims to systematically explore the potential for parameter redistribution in ViTs via a global structural pruning approach.The main hypothesis is that global pruning can provide insights on the relative importance and redundancy of different components and layers in ViT models, guiding more efficient parameter allocation. By analyzing the pruned model architectures, the paper hopes to discover new design principles and heuristics for constructing more efficient vision transformers that outperform conventional uniform design.In summary, the central hypothesis is that the parameter distribution in ViT models is suboptimal and can be improved by leveraging the insights from global pruning, leading to enhanced efficiency and performance tradeoffs compared to the prevailing practice of using uniform dimensions. The paper aims to provide both empirical evidence through pruned models and analysis to shed light on efficient ViT design.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contributions appear to be:- Proposing NViT, a novel hardware-friendly global structural pruning algorithm for Vision Transformers (ViTs). The pruning method uses a latency-aware, Hessian-based importance criteria to reduce parameters and FLOPs in a ViT model. - Performing a systematic analysis on the prunable components in a ViT model, including the embedding dimension, number of heads, MLP hidden dimension, etc. The paper shows these components have distinct sensitivity to pruning.- Exploring hardware-friendly parameter redistribution in ViTs through the global pruning process. The paper discovers trends like high prunability of ViT models, unique parameter distribution across stacked ViT blocks, etc. - Achieving efficient ViT models named NViT through the pruning process that outperform prior work like DeiT, SWIN, and AutoFormer models in terms of accuracy under similar FLOPs and latency constraints.- Providing insights on the differences in learning dynamics and architecture design principles between CNNs vs ViTs based on the global pruning study.- Proposing a simple yet effective heuristic for redistributing ViT parameters based on the pruning insights, which leads to improved accuracy over DeiT models of similar sizes.In summary, the main contribution appears to be the proposal of a novel ViT pruning method that provides insights on efficient ViT design and leads to more accurate yet hardware-friendly ViT models called NViT. The work also compares ViT vs CNN models and proposes simple heuristics for improved ViT architecture.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper proposes a new method for vision transformer compression and parameter redistribution, where they utilize global structural pruning with a novel Hessian-based importance criteria to achieve near lossless compression on DeiT models, discover insights on parameter redistribution, and design more efficient vision transformer architectures that outperform prior art.
