# [Open Vocabulary Semantic Segmentation with Patch Aligned Contrastive   Learning](https://arxiv.org/abs/2212.04994)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question addressed is: How can we enable open vocabulary semantic segmentation using only image-text data, without requiring any segmentation annotations or masks during training? 

The key hypothesis is that by training an alignment between the patch tokens of a vision encoder and the text embeddings, a model can identify image regions corresponding to text concepts. This alignment can then be leveraged to perform zero-shot transfer to semantic segmentation in an open vocabulary setting.

Specifically, the paper proposes Patch Aligned Contrastive Learning (PACL) to train such an alignment by modifying the contrastive loss in CLIP. The main hypothesis is that with PACL, a CLIP-like model trained only on image-text data can achieve strong performance on open vocabulary semantic segmentation without needing segmentation supervision.

In summary, the central research question is how to do open vocabulary semantic segmentation with only image-text supervision, and the key hypothesis is that training an alignment between vision and text patches with PACL can enable this zero-shot transfer capability.


## What is the main contribution of this paper?

 The main contribution of this paper is proposing a modified contrastive learning objective called Patch Aligned Contrastive Learning (PACL) to train an alignment between the patch tokens from a vision encoder and the CLS token from a text encoder. The key ideas are:

- They observe that standard CLIP training aligns the CLS tokens from the vision and text encoders, but lacks alignment at the patch level. This is a problem for transferring CLIP to dense prediction tasks like semantic segmentation. 

- They propose a new compatibility function for contrastive loss that computes patch-level similarity between vision tokens and the text CLS token. This similarity is used to take a weighted sum of vision tokens. The dot product between this weighted sum and the CLS text token gives the new compatibility score.

- Training with this objective gives vision-text alignment at the patch level. This allows segmentation in a zero-shot manner by finding image regions corresponding to textual descriptors.

- They show state-of-the-art results on zero-shot segmentation on Pascal VOC, Pascal Context, COCO Stuff and ADE20K datasets, outperforming recent methods including some that use segmentation masks/annotations.

- The approach also gives improved zero-shot classification over vanilla CLIP on 12 classification datasets, showing it learns better global alignment too.

In summary, the key contribution is a simple but effective contrastive learning technique to get vision-text patch alignment for transferring CLIP to zero-shot dense prediction tasks like segmentation.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the key points from the paper:

The paper proposes Patch Aligned Contrastive Learning (PACL), a modified contrastive loss function for CLIP models that trains an alignment between image patch tokens and text tokens, enabling zero-shot transfer to open vocabulary semantic segmentation without requiring segmentation annotations during training.
