# [PolyFormer: Referring Image Segmentation as Sequential Polygon   Generation](https://arxiv.org/abs/2302.07387)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central research question this paper addresses is: How can we formulate referring image segmentation (RIS) as a sequence-to-sequence prediction problem to improve performance?The key points are:1. The paper proposes to represent the segmentation mask as a sequence of polygon vertices rather than predicting the dense pixel masks directly. 2. It introduces a new sequence-to-sequence model called PolyFormer to generate the polygon vertices sequentially.3. A regression-based decoder is designed to output continuous floating point coordinates directly without quantization errors.4. This allows the paper to unify RIS and referring expression comprehension (REC) tasks into one seq2seq framework by outputting both polygon vertices and bounding box coordinates.5. Experiments show this approach outperforms previous state-of-the-art methods on major RIS and REC benchmarks by a significant margin.In summary, the central hypothesis is that formulating RIS as sequential polygon generation within a seq2seq framework can lead to better performance compared to existing methods. The paper provides substantial experiments to validate this hypothesis.


## What is the main contribution of this paper?

The main contributions of this paper are:1. It proposes PolyFormer, a new sequence-to-sequence framework for referring image segmentation (RIS) and referring expression comprehension (REC). PolyFormer takes a sequence of image patches and text query tokens as input, and autoregressively outputs a sequence of polygon vertices to represent the segmentation mask. 2. It introduces a regression-based decoder that directly predicts the continuous 2D coordinates of polygons without quantization. This is more accurate for localization compared to previous approaches that formulate it as a classification task over discrete bins.3. It shows that the polygon-based PolyFormer outperforms previous mask-based RIS methods by a large margin across three major benchmarks. It also generalizes well to the referring video segmentation task in a zero-shot manner.4. The simple seq2seq formulation of PolyFormer provides a unified framework for both RIS and REC tasks. The text and image features can be naturally fused as the input sequence, and the output sequence contains both polygon vertices and bounding box coordinates.5. The polygon representation is more structured and interpretable compared to pixel-level masks. The paper shows that generating polygons sequentially in an autoregressive manner enables modeling complex shapes and occlusions.In summary, the main contribution is proposing PolyFormer, a novel seq2seq model that formulates RIS and REC as polygon generation, and demonstrates superior performance and generalization ability compared to prior arts. The regression-based decoder is also a key contribution for accurate localization in this framework.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is my one sentence summary of the key points in the paper:The paper proposes a new framework called PolyFormer that formulates referring image segmentation as a sequence-to-sequence prediction problem, where the model sequentially generates polygon vertices outlining the referred object for more accurate segmentation compared to prior mask-based approaches.


## How does this paper compare to other research in the same field?

Here are some key ways this paper compares to other research in referring image segmentation and referring expression comprehension:1. Most prior work predicts pixel-level segmentation masks directly. This paper instead formulates referring segmentation as sequential polygon generation. Polygon representations are more structured and enable a simple seq2seq formulation. 2. The paper proposes a novel regression-based decoder to predict continuous polygon vertex coordinates in the seq2seq framework. This is different from other recent seq2seq vision models that formulate all tasks, including localization, as classification over discretized outputs. The regression decoder improves localization accuracy.3. The paper shows for the first time that a polygon-based method outperforms mask-based approaches by a large margin on major referring segmentation benchmarks. It also shows the polygon method generalizes well to unseen scenarios including video and synthetic data.4. The paper presents a unified model for both referring image segmentation (polygon prediction) and referring expression comprehension (bounding box prediction) in one simple seq2seq framework. Some prior works require task-specific model components or show performance degrades in multi-task learning.5. The method sets new state-of-the-art results on RefCOCO, RefCOCO+ and RefCOCOg referring segmentation datasets, with gains of 2.5%-5.4% absolute mIoU over previous best results. It also achieves strong performance on referring expression datasets.In summary, the key innovations are the polygon-based formulation, regression decoder, simplicity and unification of the seq2seq framework, strong empirical results surpassing prior art, and good generalization ability. The paper makes significant advances over existing research in referring image segmentation and comprehension.


## What future research directions do the authors suggest?

After reviewing the paper, some potential future research directions suggested by the authors include:- Exploring ways to reduce the dependence on accurate bounding box and polygon annotations for training PolyFormer, such as using weakly supervised data.- Analyzing the broader impacts of the data and model, including fairness, social bias, and potential misuse.- Extending the PolyFormer framework beyond referring image segmentation and comprehension tasks to other vision-language tasks.- Improving PolyFormer's ability to handle complex cases like fragmented objects and occlusion.- Applying PolyFormer to additional datasets beyond the standard referring image segmentation benchmarks.- Evaluating PolyFormer on referring video object segmentation without fine-tuning to further demonstrate generalization ability. - Comparing PolyFormer with other recent vision-language pretrained models.- Developing extensions to handle 3D inputs.- Exploring the regression-based decoder design for other vision-language tasks involving coordinate prediction.In summary, the authors point out promising directions like reducing annotation dependence, analyzing broader impacts, extending the framework to new tasks/datasets, improving generalization, and further developing the regression-based decoder.


## Summarize the paper in one paragraph.

Here is a one paragraph summary of the paper:The paper proposes a new method called PolyFormer for referring image segmentation (RIS) and referring expression comprehension (REC). Instead of predicting pixel-level segmentation masks directly, PolyFormer formulates RIS as a sequence prediction problem, where it generates a sequence of polygon vertices outlining the target object. This allows PolyFormer to handle complex shapes and occlusion. For REC, bounding boxes can also be represented as a corner point sequence. Thus, PolyFormer provides a unified framework for both tasks. It consists of a visual encoder, text encoder, multi-modal transformer encoder, and a regression-based decoder to output continuous coordinate values without quantization error. Experiments show PolyFormer outperforms previous state-of-the-art methods on RIS and REC benchmarks by a large margin. It also generalizes well to referring video segmentation in a zero-shot setting. The simple and flexible sequence-to-sequence formulation is the main contribution, which avoids complex feature fusion and supports multi-task learning naturally.
