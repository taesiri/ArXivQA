# [Diffusion-SDF: Conditional Generative Modeling of Signed Distance   Functions](https://arxiv.org/abs/2211.13757)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question is:

How can we develop a generative model for 3D shape completion, single-view reconstruction, and reconstruction of real-scanned point clouds using diffusion models and neural signed distance functions (SDFs)?

The key hypotheses appear to be:

1) Neural SDFs can be used as a unified 3D representation to parameterize the geometry described by various input signals like point clouds and 2D images. 

2) Diffusion models can be trained on latent vectors representing SDF embeddings to generate diverse shape completions conditioned on partial inputs. 

3) A modulation scheme can be developed to create compressed SDF representations as input to the diffusion model.

4) Providing geometric guidance through end-to-end training and conditioning mechanisms will allow the diffusion model to generate shapes consistent with input geometry.

In summary, the central goal is to develop a generative model for shape completion and reconstruction tasks using diffusion models and neural SDFs, which requires addressing the challenges of representing and diffusing SDFs and conditioning the model on partial geometric inputs.


## What is the main contribution of this paper?

 The main contributions of this paper are:

- Proposing a probabilistic generative model based on diffusion models that can create clean and diverse 3D meshes. 

- Solving the problem of diffusing the weights of implicit neural functions (signed distance functions/SDFs in this case) while providing geometric guidance through a custom modulation module.

- Demonstrating the method's ability to reconstruct plausible 3D outputs from various imperfect observations such as sparse, partial point clouds, single images, and real-scanned noisy point clouds.

- Showing through experiments that the proposed method achieves strong performance in shape generation and completion tasks compared to existing methods.

In summary, the key contribution is presenting a generative model that leverages diffusion models and neural SDFs to generate high-quality 3D shapes in an unconditional manner or conditioned on various input types. The method expands the domain of diffusion models from 2D image synthesis to 3D shape modeling using implicit neural representations.
