# [Diffusion-SDF: Conditional Generative Modeling of Signed Distance   Functions](https://arxiv.org/abs/2211.13757)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question is:

How can we develop a generative model for 3D shape completion, single-view reconstruction, and reconstruction of real-scanned point clouds using diffusion models and neural signed distance functions (SDFs)?

The key hypotheses appear to be:

1) Neural SDFs can be used as a unified 3D representation to parameterize the geometry described by various input signals like point clouds and 2D images. 

2) Diffusion models can be trained on latent vectors representing SDF embeddings to generate diverse shape completions conditioned on partial inputs. 

3) A modulation scheme can be developed to create compressed SDF representations as input to the diffusion model.

4) Providing geometric guidance through end-to-end training and conditioning mechanisms will allow the diffusion model to generate shapes consistent with input geometry.

In summary, the central goal is to develop a generative model for shape completion and reconstruction tasks using diffusion models and neural SDFs, which requires addressing the challenges of representing and diffusing SDFs and conditioning the model on partial geometric inputs.
