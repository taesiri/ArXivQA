# [BGE M3-Embedding: Multi-Lingual, Multi-Functionality, Multi-Granularity   Text Embeddings Through Self-Knowledge Distillation](https://arxiv.org/abs/2402.03216)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
Existing text embedding models have limitations in versatility for supporting multiple languages, handling inputs of different lengths, and unifying different retrieval functionalities like dense, sparse and multi-vector retrieval. Most models only work for English and short texts, while real-world applications require a versatile model that works across languages, text lengths, and retrieval methods.

Proposed Solution:
The paper proposes M3-Embedding, a new embedding model with strong versatility in:

- Multi-Linguality: Supports over 100 languages and achieves state-of-the-art results for multi-lingual and cross-lingual retrieval tasks. Learns a unified semantic space for different languages.

- Multi-Functionality: Simultaneously performs dense, sparse and multi-vector retrieval in a unified model, enabling hybrid retrieval workflows.

- Multi-Granularity: Handles inputs from short sentences to long documents with up to 8,192 tokens.

The training incorporates self-knowledge distillation across retrieval functions, optimized batching for high throughput, and comprehensive multi-lingual data curation.

Main Contributions:

- Proposes M3-Embedding as the first highly versatile text embedding model for languages, text lengths and retrieval functionalities

- Introduces self-knowledge distillation to integrate signals from different retrieval functions as teacher signal

- Optimizes batching strategy to enable large batch size and high training throughput

- Performs large-scale, diverse, multi-lingual data curation from variety of sources

- Achieves new state-of-the-art results on multi-lingual, cross-lingual and long document retrieval

- Releasing model, code and data to provide useful resources for text embeddings

The summary covers the key aspects of the problem being addressed, the proposed M3-Embedding solution and its versatility, the technical innovations for effective training, and the main contributions made in the paper regarding state-of-the-art results, released resources, and advancing text embeddings.


## Summarize the paper in one sentence.

 This paper presents M3-Embedding, a versatile text embedding model with multi-linguality, multi-functionality, and multi-granularity capabilities, achieved through self-knowledge distillation and optimized training strategies.


## What is the main contribution of this paper?

 This paper presents M3-Embedding, a versatile text embedding model with multi-linguality, multi-functionality, and multi-granularity. The main contributions are:

1) Proposing M3-Embedding, the first embedding model that supports over 100 languages, unifies dense, sparse, and multi-vector retrieval functionalities, and handles input sequences up to 8192 tokens.

2) Introducing a self-knowledge distillation framework to jointly train multiple retrieval functions and use their relevance scores to enhance each other. 

3) Optimizing the batching strategy to achieve large batch size and high throughput for training long sequence data.

4) Performing high-quality curation of training data from three complementary sources: massive unlabeled corpora, related labeled datasets, and synthesized data.

5) Achieving new state-of-the-art results on popular multi-lingual and cross-lingual benchmarks like MIRACL and MKQA. The model, code, and data will be publicly released.

In summary, the main contribution is proposing the highly versatile M3-Embedding model and an effective training framework involving self-distillation and efficient batching to realize its multi-linguality, multi-functionality, and multi-granularity.


## What are the keywords or key terms associated with this paper?

 Based on my review of the paper, some of the key terms and concepts associated with it are:

- M3-Embedding: The name of the proposed versatile text embedding model, with "M3" standing for Multi-Linguality, Multi-Functionality, and Multi-Granularity.

- Multi-linguality: Support for a wide variety of languages (over 100) and performing well on multi-lingual and cross-lingual retrieval tasks.  

- Multi-functionality: Unifying multiple common retrieval functions of text embeddings like dense retrieval, sparse/lexical retrieval, and multi-vector retrieval.

- Multi-granularity: Ability to handle inputs of different lengths, from short sentences to long documents (up to 8192 tokens).

- Self-knowledge distillation: A proposed training approach to integrate relevance scores from different retrieval functions as teacher signal.

- Efficient batching: Optimized batching strategy to achieve high throughput training and large batch sizes for discriminative embeddings.  

- Multi-stage training: Pre-training on unlabeled data, fine-tuning with supervised data, training with self-knowledge distillation.

- State-of-the-art performance: Superior results on benchmarks like MIRACL, MKQA, narrativeQA for multi-lingual and cross-lingual retrieval across various languages.

In summary, the key focus is on developing a versatile and well-performing text embedding model with strengths in supporting multiple languages, tasks, and input lengths.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in this paper:

1. What is the motivation behind proposing a versatile embedding model with multi-linguality, multi-functionality and multi-granularity support? Why are existing methods limited in versatility?

2. How does M3-Embedding support multiple languages and build a unified cross-lingual semantic space? What techniques allow it to transfer knowledge between languages? 

3. What are the three retrieval functionalities unified by M3-Embedding? How does it integrate dense, sparse, and multi-vector retrieval in a hybrid process?

4. What is the proposed self-knowledge distillation method and how does it facilitate joint optimization of different retrieval objectives? How is the teacher signal constructed?

5. How does the efficient batching strategy optimize GPU memory usage and increase batch size? How does gradient checkpointing and split-batch method work? 

6. What are the three main sources of training data and how do they complement each other? How is the synthetic long document data generated?

7. What is the multi-stage training workflow? What does pre-training with RetroMAE bring to the table? How are ANCE hard negatives introduced?

8. Why is the sparse retrieval exceptionally effective for long document retrieval? How does the proposed MCLS method enhance long-text capability?

9. What explains M3-Embedding's stable performance across languages compared to baselines? How does massive diverse pre-training data help?

10. What findings can be made from the ablation study? How does self-knowledge distillation affect different retrieval functionalities?
