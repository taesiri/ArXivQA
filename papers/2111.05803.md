# [Gradients are Not All You Need](https://arxiv.org/abs/2111.05803)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the main research question seems to be:How do gradient-based optimization methods perform when applied to chaotic dynamical systems, and what techniques can be used to overcome potential issues that arise?The key points related to this question appear to be:- Chaotic dynamical systems, where small changes in initial conditions lead to diverging trajectories, can pose challenges for gradient-based optimization methods like backpropagation. The gradients can "explode" or become unusable.- This exploding gradient problem arises from the Jacobian matrices that appear when differentiating through long dynamical system trajectories. Eigenvalues greater than 1 in these Jacobian products lead to exponential divergence.- The authors demonstrate this exploding gradient phenomenon concretely in various domains like rigid-body physics simulators, recurrent neural networks, and learned optimization.- Potential solutions discussed include modifying the dynamical systems, using truncated backpropagation, gradient clipping, black-box gradient estimates, and exploiting ergodicity. Each has tradeoffs.- Overall, the key takeaway seems to be that blindly applying gradient-based methods to chaotic systems can fail catastrophically, and more care needs to be taken, either by changing the system or using more robust optimization techniques.In summary, the core question is understanding and overcoming challenges of combining gradient-based optimization with chaotic dynamics across different domains.
