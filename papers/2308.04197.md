# [D3G: Exploring Gaussian Prior for Temporal Sentence Grounding with   Glance Annotation](https://arxiv.org/abs/2308.04197)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the main goal of this paper is to explore the use of glance annotations for temporal sentence grounding (TSG). The glance annotations only provide a single timestamp within the target moment, rather than precise start/end timestamps. The key research questions addressed are:- How can the information from a single glance annotation be effectively utilized to localize the full extent of the target moment for a sentence query?- Can a model trained with only glance annotations approach the performance of models trained with more expensive full supervision?To address these questions, the paper proposes a framework called D3G that consists of two main components:1) A Semantic Alignment Group Contrastive Learning (SA-GCL) module that samples positive moments based on the glance annotation and semantic similarity to the query. It contrasts groups of these positive moments against the sentence query.2) A Dynamic Gaussian Prior Adjustment (DGA) module that uses multiple Gaussian distributions to dynamically model the target moment distribution and alleviate annotation bias.The central hypothesis is that by effectively exploiting the glance annotations in this way, the D3G framework can substantially improve performance over standard weakly supervised approaches and approach the accuracy of fully supervised methods. Experiments on multiple TSG benchmarks support this hypothesis.In summary, the key research contribution is an effective way to utilize low-cost glance annotations for TSG, narrowing the gap to full supervision. The proposed D3G framework addresses the challenges of modeling the full target moment extent and handling annotation bias.


## What is the main contribution of this paper?

This paper proposes a new framework called D3G for temporal sentence grounding (TSG) with low-cost glance annotations. The main contributions are:1. The paper investigates TSG with glance annotation, which only requires annotating a single frame per sentence query instead of precise start/end timestamps. This reduces annotation cost compared to fully supervised TSG. 2. A Semantic Alignment Group Contrastive Learning (SA-GCL) module is proposed to align positive sentence-moment pairs by sampling a group of positive moments based on Gaussian priors and semantic consistency.3. A Dynamic Gaussian Prior Adjustment (DGA) module is proposed to dynamically adjust the Gaussian priors to alleviate annotation bias and model complex multi-event queries.4. Experiments show D3G outperforms previous glance supervised and weakly supervised methods by a large margin and achieves comparable performance to some fully supervised methods, demonstrating the effectiveness of exploiting glance annotations.5. The proposed framework provides a simple yet effective way to develop TSG with lower annotation cost while maintaining strong performance.In summary, the main contribution is proposing the D3G framework to effectively exploit glance annotations for achieving high-quality TSG with lower annotation cost compared to full supervision. The SA-GCL and DGA modules are key novel components for aligning positive moments and adjusting priors dynamically.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper proposes a Dynamic Gaussian prior based Grounding framework with Glance annotation (D3G) for temporal sentence grounding that utilizes a Semantic Alignment Group Contrastive Learning module and a Dynamic Gaussian prior Adjustment module to effectively exploit glance annotations and achieve competitive performance compared to fully supervised methods.
