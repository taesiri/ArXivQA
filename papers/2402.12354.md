# [LoRA+: Efficient Low Rank Adaptation of Large Models](https://arxiv.org/abs/2402.12354)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
The paper studies efficient finetuning of large pretrained language models using Low Rank Adaptation (LoRA). LoRA is a popular method that inserts low-rank "adapter" matrices into the pretrained model and only trains these adapter matrices, keeping the original pretrained weights mostly fixed. This greatly reduces compute and memory requirements for finetuning. 

The authors observe that standard LoRA finetuning can be inefficient when model width (embedding dimension) is large, which is common for state-of-the-art models. Using a scaling argument, they show theoretically that the learning rates for the two adapter matrices in LoRA should scale differently with width for efficient learning. However, standard LoRA uses the same learning rate for both.  

Proposed Solution:
The authors propose LoRA+, which sets the learning rate ratio between the two adapter matrices to a fixed constant that scales with width. Concretely, if ηA, ηB are learning rates for the two matrices, LoRA+ sets ηB = ληA where λ is a constant much greater than 1. 

Through extensive experiments on models like GPT-2, RoBERTa and LLama, the authors validate the theory and show LoRA+ leads to faster and better finetuning than standard LoRA. For example, on GLUE tasks, LoRA+ gets 1-2% higher accuracy than standard LoRA.

Main Contributions:
- Theoretically show that standard LoRA can be inefficient for large width models, provide theory to derive optimal learning rate ratio
- Propose LoRA+ with fixed learning rate ratio between adapter matrices that scales as a power of width
- Extensive empirical validation showing LoRA+ improves finetuning speed and accuracy over standard LoRA
- Provide practical guidelines for setting the learning rate ratio in LoRA+ based on model and task

In summary, the paper provides theory-driven guidelines for efficiently finetuning large models with LoRA, leading to a simple but improved method LoRA+ that works better than standard LoRA.
