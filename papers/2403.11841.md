# [Pessimistic Causal Reinforcement Learning with Mediators for Confounded   Offline Data](https://arxiv.org/abs/2403.11841)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem: 
Most existing offline reinforcement learning (RL) algorithms rely on two key assumptions - unconfoundedness (no unmeasured confounders) and positivity (all actions sufficiently explored). However, observational datasets often violate these assumptions, leading to suboptimal policies. This paper studies offline RL with both unmeasured confounding and distributional shift.

Proposed Solution:
The paper proposes a novel framework called "confounded mediated Markov decision process (M2DP)" which features an auxiliary mediator variable satisfying the front-door criterion. This allows removing confounding bias via front-door adjustment. To address distributional shift, the paper develops a Pessimistic Causal Learning (PESCAL) algorithm that penalizes the estimated mediator probability. 

Key Ideas:
- Introduce mediated Q-function and show its optimal policy is greedy w.r.t. a weighted average of this Q-function 
- Propose Causal Learning (CAL) algorithm that integrates fitted Q-iteration with front-door adjustment
- Penalize mediator probability estimator in PESCAL to avoid overestimating less explored actions  
- Provide theoretical guarantees on regret bounds for both algorithms
- PESCAL only requires pointwise uncertainty quantification for mediator probability, instead of uniform bound

Main Contributions:
- First paper to study offline RL under both unconfoundedness and positivity violations
- Propose CAL and PESCAL algorithms with theoretical convergence guarantees
- PESCAL enhances practicality by only requiring pointwise uncertainty quantification
- Demonstrate superior empirical performance over state-of-the-art algorithms on both synthetic and real-world ridesharing datasets

In summary, this paper makes significant contributions in offline RL by tackling key challenges of unmeasured confounding and distributional shift via a mediated Q-learning approach enhanced with pessimism. The algorithms show great promise both theoretically and empirically.
