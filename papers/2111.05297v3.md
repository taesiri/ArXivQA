# [Sliced Recursive Transformer](https://arxiv.org/abs/2111.05297v3)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading, the central research question this paper tries to address is:

How can we improve the parameter efficiency and utilization of vision transformers without increasing model size or complexity? 

The key hypotheses the authors explore are:

1) Sharing weights recursively across transformer layers can help extract stronger representations without adding parameters.

2) Approximating global self-attention via multiple sliced group self-attentions can reduce computational cost while maintaining accuracy. 

3) Their proposed "Sliced Recursive Transformer" (SReT) architecture can achieve better accuracy and efficiency through these techniques.

Specifically, the authors aim to design a transformer model that achieves competitive or better accuracy than state-of-the-art methods while using significantly fewer parameters and FLOPs. Their core innovations revolve around recursive weight sharing and sliced self-attention to improve parameter and computational efficiency. The experiments aim to validate whether SReT can outperform other vision transformers on image classification with smaller model size.

In summary, the main research question is how to build more efficient vision transformers. The key hypotheses are weight sharing through recursion and sliced self-attention approximations can improve parameter utilization and efficiency without sacrificing accuracy. The experiments test if their proposed SReT model achieves better accuracy and efficiency compared to other ViT architectures.


## What is the main contribution of this paper?

 The main contributions of this paper are:

- It proposes a sliced recursive transformer (SReT) architecture that improves parameter utilization and representation ability in vision transformers without increasing model size. 

- It introduces an approximating method through multiple sliced group self-attentions across recursive layers. This reduces the computational cost caused by recursion while maintaining accuracy.

- It provides design principles and comparisons of different SReT variants on factors like computational complexity, distillation strategies, etc. 

- It verifies the approach across vision and language tasks, outperforming state-of-the-art methods with fewer parameters. The flexible scalability also enables building extremely deep transformers.

- The proposed weight sharing mechanism allows constructing over 100 shared layers easily while keeping the model compact, avoiding optimization difficulties with very large models.

In summary, the key contribution is introducing the sliced recursive operation to vision transformers for improved parameter efficiency and representation ability. The approximations via group self-attention also balance accuracy and efficiency. The approach generalizes across tasks and the weight sharing provides flexibility to scale model depth.
