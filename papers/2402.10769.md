# [Distillation Enhanced Generative Retrieval](https://arxiv.org/abs/2402.10769)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper "Distillation Enhanced Generative Retrieval":

Problem:
- Generative retrieval is an emerging paradigm for text retrieval that generates identifier strings of relevant passages as targets. However, it still lags behind advanced dense retrieval methods.  
- Current generative retrieval relies on incomplete query-passage relevance judgments during training, treating target passages as positive and all others as equally negative. This introduces noise and limits performance.

Proposed Solution:
- Propose a distillation enhanced generative retrieval (DGR) framework to address the limitations.
- Uses a powerful ranking model (e.g. cross-encoder) as teacher to provide improved passage rankings instead of binary labels.
- Novel distilled RankNet loss to effectively transfer ranking knowledge into the generative retrieval model for optimization.

Key Contributions:
- First work to introduce distillation into generative retrieval, identifying it as a promising direction.
- Formulates the distilled RankNet loss that leverages passage rankings from teacher to train student generative retriever.
- Achieves new state-of-the-art for generative retrieval on 4 datasets without affecting inference.
- Shows exceptional robustness and generalizability with various teacher models and losses.

In summary, the paper identifies limitations of current generative retrieval methods and proposes a distillation framework called DGR to address them. By distilling passage rankings from powerful teacher models, DGR significantly pushes generative retrieval to new levels of performance across multiple benchmarks. The feasibility of distillation for advancing generative retrieval is demonstrated.
