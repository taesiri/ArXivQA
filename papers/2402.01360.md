# [What Makes Medical Claims (Un)Verifiable? Analyzing Entity and Relation   Properties for Fact Verification](https://arxiv.org/abs/2402.01360)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem: 
- Biomedical claim verification often fails when no evidence can be discovered, leaving claims "unverifiable".  
- Little is known about what properties of biomedical claims impact their verifiability. Specifically, the role of entities (e.g. medical conditions) and relations (e.g. "causes") is not understood.

Proposed Solution:
- Create a new dataset called BEAR-Fact with 1,448 Twitter claims annotated with:
  - Fact checking verdicts (supported, refuted, unverifiable etc.)
  - Evidence documents from PubMed
  - Structured entity and relation information
- Use this dataset to analyze if entities and relations impact fact verifiability.
- Compare in-house annotations to crowdsourcing.
- Train classifier to predict if claims are verifiable.

Key Findings:
- Claims about negative relations (e.g. "does not prevent") are much harder to verify.
- Annotators reformulate entities to canonical names when searching for evidence. 
- Domain expertise does not substantially impact annotation quality.
- Possible to reliably identify verifiable claims (.82 F1) but harder to identify unverifiable ones (.27 F1).

Main Contributions:
- First corpus of scientific claims with both fact checking annotations and structured entity/relation information
- Analysis connecting entity/relation properties to fact verifiability
- Comparison of in-house vs crowd annotations
- Modeling verifiability prediction

The paper demonstrates that entity and relation properties provide insights into what makes claims difficult to verify, with both data analysis and modeling experiments.


## Summarize the paper in one sentence.

 Here is a one-sentence summary of the paper:

The paper introduces a new biomedical fact checking dataset annotated with entities, relations, evidence documents, and veracity labels, analyzes properties that impact claim verifiability, compares annotation quality across different settings, and shows that models can reliably predict if claims are verifiable or unverifiable from the text.


## What is the main contribution of this paper?

 Based on my reading of the paper, the main contribution is:

The paper contributes a new dataset called BEAR-Fact, which consists of 1,448 fact-checked biomedical claims extracted from tweets, along with structured entity and relation information and evidence documents used to verify the claims. The key features of this dataset are:

1) It enables analyzing which properties of biomedical claims, specifically entities and relations, make claims (un)verifiable. 

2) It allows observing how annotators refine entity-based search queries to discover evidence from PubMed to fact-check claims.

3) It facilitates comparing in-house annotations to crowdsourcing for biomedical claim verification.

4) It provides data to train and evaluate models for predicting whether a biomedical claim will be verifiable or not solely based on the claim text.

In summary, the main contribution is the creation of a novel dataset that supports analyzing verifiability of biomedical claims and modeling biomedical fact verification in various settings.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and keywords associated with it are:

- Biomedical claim verification
- Fact checking
- Entity-relation patterns
- Verifiability
- Evidence retrieval
- Annotation study
- Crowdsourcing
- Domain expertise
- Social media
- Twitter
- PubMed
- Relation extraction
- Entity linking

The paper presents an annotation study for biomedical fact checking, where annotators search PubMed to verify claims extracted from Twitter posts. It analyzes the connection between entity-relation patterns and claim verifiability. The study also compares in-house annotations to crowdsourcing, evaluating the impact of domain knowledge. Additionally, the paper investigates modeling verifiability prediction from claims. The key terms reflect this focus on biomedical fact checking and evidence retrieval, while covering the study setup with crowdsourcing and modeling experiments.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in the paper:

1. The paper introduces a new dataset called BEAR-Fact. What is the purpose of creating this dataset and what annotations does it contain? 

2. The paper extracts medical claims from tweets and then has annotators fact-check them. What was the process for extracting the claims from tweets? What guidelines were given to annotators for fact-checking the claims?

3. The paper analyzes different properties of claims, such as entities, relations, and verifiability. Why is it important to understand how these properties relate to the verifiability of claims? How could this help improve fact-checking systems?

4. The paper finds that claims with negated relations tend to be more difficult to verify. Why might this be the case? What strategies could help address this challenge? 

5. The paper introduces a task of predicting whether a claim will be verifiable or not from the text alone. Why is this an important capability for a fact-checking system? What performance does the RoBERTa model achieve on this task?

6. The paper compares in-house annotations to crowdsourced annotations. What differences were observed between domain experts and non-experts in the crowdsourcing study? What might account for these differences?

7. The paper analyzes how annotators refine entity-based search queries to find evidence. What types of query refinement strategies were most common? Why might entity normalization be important for evidence retrieval?

8. How reliable were the verdicts obtained from crowdworkers compared to trained in-house annotators? What metrics were used to evaluate agreement? What factors could account for differences?

9. What ethical considerations should be kept in mind when working with a dataset of fact-checked medical claims? How should the verdicts be interpreted and utilized?

10. The paper identifies several limitations and opportunities for future work. What are 2-3 of the main limitations? What kinds of follow-up studies could help address these limitations or build on this work?
