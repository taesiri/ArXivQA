# [OneBit: Towards Extremely Low-bit Large Language Models](https://arxiv.org/abs/2402.11295)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
Existing methods for quantizing large language models (LLMs) to extremely low bitwidths like 1-bit suffer from severe performance degradation. Methods like post-training quantization and existing quantization-aware training frameworks struggle when compressing models down to 1-bit, failing to maintain effectiveness. This is mainly due to the drastic precision loss causing substantial inaccuracies in the core linear projection operations within LLMs.

Proposed Solution:
This paper proposes a novel 1-bit model architecture for LLMs to improve time and space efficiency during inference. The key ideas include:

1) A new linear layer design using a 1-bit sign matrix for weights along with two small FP16 value vectors to compensate for precision loss. This representation better quantizes weights while adding little overhead.

2) A sign-value independent decomposition (SVID) method to initialize the 1-bit model by decomposing the original FP32/FP16 weights into the proposed format. SVID provides an effective starting point for further training.

3) Quantization-aware knowledge distillation to transfer capabilities from the original full-precision teacher model to the proposed 1-bit student model. Custom loss functions based on output logits and internal representations are used.

Main Contributions:

- A highly efficient 1-bit model architecture specially designed for quantizing LLMs that is more amenable to training and knowledge transfer compared to prior works.

- The SVID technique for weight matrix decomposition that produces superior initialization for the 1-bit student model, improving performance and convergence speed. 

- Extensive experiments on major LLM families (OPT, LLaMA, LLaMA2) from 1.3B to 13B parameters showcasing stable training and state-of-the-art results compared to strong baselines. The method achieves at least 83% of full-precision performance with over 90% compression.

- Analysis providing guidance for future research into extremely low-bit LLMs in terms of efficiency, robustness and problem-solving abilities.
