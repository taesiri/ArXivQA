# [Towards Fair and Firm Real-Time Scheduling in DNN Multi-Tenant   Multi-Accelerator Systems via Reinforcement Learning](https://arxiv.org/abs/2403.00766)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Cloud services need to provide consistent Quality of Service (QoS) to users/tenants, who have unique quality expectations based on their expenditure. 
- Literature often defines QoS using system-level aggregate metrics like average response time. However, these can hide unfairness at the tenant level.  
- Scheduling Deep Neural Networks (DNNs) for inference-as-a-service across multi-tenant, multi-accelerator cloud platforms is challenging to meet tenant-specific Service Level Objectives (SLOs).

Proposed Solution:
- Use Deep Reinforcement Learning (DRL) based online scheduling algorithm that operates at the granularity of DNN layers.
- Scheduling decisions consist of priority and accelerator allocation for each layer.
- State encoding captures system-level features (accelerator utilization) and request-level features (DNN type, layer info, deadlines).
- Unique tenant-specific current and target SLI values are added to request features.
- Reward function promotes meeting tenant-wise SLOs over aggregate system SLO.  

Key Contributions:
- First online DNN scheduling technique providing tenant-specific QoS levels in multi-tenant multi-accelerator settings.
- State encoding and reward formulation using current and target SLI values enables meeting individually negotiated SLOs.
- Evaluated for fairness across tenants and firm real-time constraints showing improved tenant-specific SLO achievement over baselines.
- Low overhead modification to baseline DRL scheduler. Easy addition of new tenants without impacting learning.

In summary, the paper presents a DRL based DNN scheduling technique for multi-tenant cloud platforms that can provide tenant-specific QoS targets defined using SLIs/SLOs. The main highlights are support for customized SLOs and improved fairness across tenants.


## Summarize the paper in one sentence.

 This paper proposes a novel online scheduling technique using deep reinforcement learning to provide tenant-specific quality of service guarantees in terms of deadline hit rates for deep neural network inferences on multi-tenant multi-accelerator systems in cloud environments.


## What is the main contribution of this paper?

 The main contribution of this paper is proposing a novel online scheduling technique using deep reinforcement learning to provide tenant-specific quality of service (QoS) guarantees in multi-tenant, multi-accelerator systems for executing deep neural network (DNN) inferences. Specifically:

1) The paper introduces a scheduler that aims to meet tenant-wise, model-specific service level objectives (SLOs) rather than overall system-level metrics. This allows for fairer QoS across tenants. 

2) The scheduler incorporates the current and target service level indicator (SLI) values on a per-tenant, per-model basis into the state representation seen by the reinforcement learning agent. This allows the agent to learn to prioritize jobs based on their proximity to SLO violations.

3) Experiments demonstrate that the proposed technique provides more consistent QoS guarantees across tenants compared to heuristics and SLO-agnostic reinforcement learning schedulers. The scheduler is also shown to be capable of meeting firm real-time constraints specified in service level agreements.

4) The overhead of the proposed scheduler is low - only 0.39% energy overhead is added versus a baseline SLO-agnostic scheduler.

In summary, the key innovation is using deep reinforcement learning for fine-grained, tenant-specific QoS management during online scheduling in multi-tenant accelerator-based systems.


## What are the keywords or key terms associated with this paper?

 Based on my review of the paper, some of the key terms and concepts associated with it include:

- Quality of Service (QoS)
- Service Level Indicators (SLIs) 
- Service Level Objectives (SLOs)
- Service Level Agreements (SLAs)
- Multi-tenant systems
- Multi-accelerator systems
- Deep Neural Networks (DNNs)
- Deep Reinforcement Learning (DRL)
- Online scheduling 
- Firm real-time scheduling
- Deadline constraints
- Tenant-specific QoS
- Fairness

The paper proposes a novel online scheduling technique using DRL for executing DNN inferences in a multi-tenant, multi-accelerator cloud system. It focuses on providing tenant-wise, model-specific QoS levels based on deadline constraints, while also considering fairness and firm real-time scheduling criteria. The key innovation is managing QoS at an individual tenant and model level rather than using a global system metric.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 potential in-depth questions about the method proposed in this paper:

1. The paper introduces a novel State encoding mechanism that includes both system-level and request-level features. Can you explain in more detail how these state features are defined and what key information they aim to capture? 

2. The Action encoding determines both a priority and a sub-accelerator allocation for each sub-job. What factors guide how the policy network learns to make these interdependent scheduling decisions?

3. The paper argues that providing the policy network with the current and target SLI for each request is crucial for managing tenant-specific QoS. Can you elaborate on why this tenant-wise information is more effective than a system-level metric?

4. The reward function balances various objectives like deadline hits, misses, and tenant satisfaction. How are these components weighted and calibrated in the overall reward signal seen by the policy network during training?

5. Evaluations compare the method against several heuristic and state-of-the-art approaches. Can you summarize the key limitations of these other techniques that motivated the need for a learning-based scheduler?  

6. In the first use case, results show lower variance in SLO achievement across tenants compared to baselines. What specifically in the proposed technique accounts for this improved fairness?

7. For the firm real-time scenario, how does the method adapt its scheduling logic when tenants require different target SLO rates rather than just best-effort? 

8. Fig. 3 shows the proposed method is better able to meet stipulated SLO targets for most tenants. To what extent can guarantees still be made if extreme or unfamiliar workloads arise?

9. What are some ways the State representation or reward formulation could be enhanced to further improve the scheduling decisions and tenant-level QoS? 

10. The paper focuses on deadline-based QoS for DNN workloads. How amenable is this approach to managing other SLI metrics or emerging accelerator architectures?
