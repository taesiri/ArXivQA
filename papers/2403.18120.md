# [Don't Trust: Verify -- Grounding LLM Quantitative Reasoning with   Autoformalization](https://arxiv.org/abs/2403.18120)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
Large language models (LLMs) like Google's Minerva and OpenAI's GPT families have become very capable at solving quantitative reasoning problems. However, they still make logical errors and incorrect computations in their reasoning steps and final answers. 

Proposed Solution: 
This paper proposes a method called "Don't Trust: Verify" (DTV) that leverages the autoformalization capability of LLMs to translate informal mathematical statements into formal logic expressions. These formal statements can then be automatically verified for logical consistency using theorem proving systems like Isabelle.

The key ideas are:

1) Use LLM few-shot prompting to generate formal Isabelle translations of the informal problem statement. Apply filters to detect incorrect translations.

2) Generate informal solutions with the LLM. Translate solutions into Isabelle formal proofs. Use Isabelle's automated tools to verify if proofs are logically valid.

3) Aggregate only verified solutions through majority voting to predict the final answer.

Main Contributions:

- First work to show feasibility of using autoformalization to automatically verify correctness of LLM quantitative reasoning solutions.

- Introduces techniques like vacuous statement filtering and self-critiquing to detect incorrect formal translations.

- Evaluated on GSM8K, MATH and MultiArith datasets. DTV outperforms majority voting baseline by over 12% on GSM8K. Shows consistent gains across models and problem categories.

- Case studies demonstrate DTV identifying correct solutions and catching logical errors in LLM reasoning.

- Discusses limitations around formalization capabilities and future work directions.

In summary, the paper presents a novel approach to leverage formal verification tools to improve reliability of LLM quantitative reasoning, enabled by recent advances in few-shot autoformalization. Both empirical and qualitative results demonstrate the promise of this method.
