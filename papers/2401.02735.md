# [Shared active subspace for multivariate vector-valued functions](https://arxiv.org/abs/2401.02735)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Existing methods for identifying the most important directions of variability (active subspaces) for vector-valued functions either rely on specific data distributions or perform analysis separately on each objective. 
- The goal is to develop gradient-based methods to identify a shared active subspace that captures the dominant directions of variability common across multiple objectives, without constraints on the data distribution.

Methodology:
- Propose methods that reduce the vector-valued problem to a scalar one by manipulating the gradients or symmetric positive definite (SPD) matrices computed from gradients of each objective.
- Gradient-level methods compute a representative vector for component gradients (average, convex hull, linear projection). 
- SPD-level methods combine component SPD matrices into one (sum, joint diagonalization, stepwise eigenvector estimation) and decompose it.

Contributions:
- Introduced several new baselines for shared active subspace computation that are distribution-free.
- Showed SPD-level methods consistently outperform gradient-level methods and even match state-of-the-art methods that require normality.
- Demonstrated that simply summing individual SPD matrices provides very effective shared subspace directions in practice. 
- Provided an R package implementing the proposed methods to enable further research.
- Analyzed methods on 1 synthetic and 4 real-world optimization problems with multiple objectives.

In summary, the key innovation is developing and empirically demonstrating distribution-free methods for shared active subspace computation for vector-valued functions, with a surprisingly simple matrix summation baseline proving very effective.


## Summarize the paper in one sentence.

 Here is a one sentence summary of the paper:

The paper proposes several baseline methods to compute a shared active subspace for multivariate vector-valued functions by manipulating the gradients or symmetric positive (semi-)definite matrices to minimize the deviation between function evaluations on the original space and the reconstructed lower-dimensional space.


## What is the main contribution of this paper?

 This paper proposes several new methods to compute a shared active subspace for multivariate vector-valued functions. The key contributions are:

1) New approaches that reduce the vector-valued problem to a scalar setting either at the gradient level or at the level of symmetric positive definite (SPD) matrices computed from the gradients. This allows applying existing active subspace methods for scalar functions.

2) The proposed methods can be applied to any data distribution, unlike existing vector-valued active subspace methods that rely on specific distributions (e.g. normal).

3) Experimental evaluation on synthetic and real-world problems shows that the SPD-level methods generally outperform gradient-level methods and are comparable to vector-valued methods that assume a normal distribution. Notably, simply taking the sum of the SPD matrices works very well in identifying a shared active subspace.

4) The methods are implemented in an R package to make them easily usable.

In summary, the main contribution is a set of new techniques to identify a low-dimensional shared active subspace for multiple vector-valued functions, with experimentally demonstrated effectiveness. The methods are distribution-agnostic and easy to apply in practice.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and keywords associated with this paper include:

- Dimension reduction
- Active subspace
- Vector-valued functions 
- Symmetric positive definite (SPD) matrices
- Common principal component analysis
- Gradient manipulation
- Multi-objective optimization

The paper proposes several approaches to compute a shared active subspace for multivariate vector-valued functions, in order to minimize the deviation between the function evaluations on the original space and the reconstructed space. The methods work by manipulating either the gradients or the SPD matrices computed from the gradients of each component function, to obtain a single structure common to all components. The approaches are tested on optimization problems with multiple objectives.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the methods proposed in this paper:

1) The paper proposes several approaches to compute a shared active subspace for multivariate vector-valued functions. How do these approaches differ from existing methods like the one proposed by Zahm et al. (2020) which relies on distributions satisfying Poincar√©-type inequalities?

2) The paper manipulates the gradients or SPD matrices at two different levels - the gradient level and the SPD level. What is the key difference between these two levels of manipulation? Which one works better in general based on the experimental results?

3) The linear projection of gradients method projects the derivatives of each objective function onto a one-dimensional subspace. How is this one-dimensional subspace computed? What are the limitations of summarizing the derivatives into a single vector in this manner?

4) Explain the minimum norm element approach for finding a common descent direction from the gradients. Why does this not perform as well as other SPD-level methods experimentally?

5) The sum of SPD matrices is proposed as a simple approach for combining the component-wise SPD matrices. Intuitively, why does this work better than manipulating the raw gradients? How does it relate to the method by Zahm et al.?  

6) What is joint diagonalization of matrices and how is it achieved using the Flury-Gautschi (FG) algorithm? What is the shortcoming of using the FG algorithm eigenvectors directly for dimensionality reduction?

7) Explain the concept of stepwise estimation of eigenvectors proposed by Trendafilov (2010) to induce a common ordering and decay structure across groups. Why is this preferred over the FG algorithm for shared subspace estimation?

8) The depth functions are proposed as a way to identify central parts of the output space to give more preference to. How can using depths in the copula space alleviate issues compared to directly using objective-wise depths? 

9) What are some ways the shared subspace methods can be extended for applications requiring preference of certain parts of the output space? What are other potential limitations not addressed in this work?

10) A shared active subspace seeks to balance accuracy across multiple objectives. But in practice, often some objectives are more important than others. How can the methods be refined to allow incorporating unequal preferences or priorities for different objectives?
