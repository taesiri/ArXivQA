# Large Language Models are Better Reasoners with Self-Verification

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper abstract and introduction, the main research question seems to be:Can large language models demonstrate improved reasoning and inference abilities when equipped with a self-verification capability to check their own conclusions?Specifically, the authors propose that large language models like GPT-3 can learn to self-verify their reasoning chains and conclusions, rather than relying solely on an initial chain of thought. Their hypothesis is that by prompting the model to verify its own outputs in a backward pass, the overall reasoning accuracy can be improved compared to just doing forward chained reasoning. The self-verification acts as a consistency check on the model's conclusions.The paper introduces a self-verification method that does not require additional training or fine-tuning of the language model. Instead, it relies only on carefully designed prompts that induce the model to verify its reasoning steps in a backward direction. The core hypothesis is that language models possess an inherent self-verification capability that can be unlocked through proper prompting.In summary, the central question is whether large language models can reason better when equipped with the ability to self-verify, avoiding some errors that can accumulate during multi-step forward reasoning. The paper aims to demonstrate this capability and its benefits empirically across several reasoning datasets.


## What is the main contribution of this paper?

The main contribution of this paper is proposing and demonstrating that large language models (LLMs) have the ability to self-verify their own prediction results, without needing additional training of verifier models. Specifically, the key contributions are:1. Proposing a self-verification method for LLMs that involves generating multiple candidate answers using chain of thought prompting, then verifying them by masking and re-predicting the original conditions. This allows calculating a verification score to select the best answer.2. Showing that this approach improves reasoning performance across multiple datasets - arithmetic, commonsense, and logical reasoning - achieving new state-of-the-art results on many.3. Demonstrating that the self-verification ability emerges in larger LLMs and benefits from more verification conditions.4. Proving LLMs can self-verify without extra training, unlike prior work on answer verification that requires fine-tuning verifier models. This makes the approach more flexible.5. Providing an interpretable verification score compared to prior verification methods based on neural network outputs.Overall, the key contribution is introducing and validating the idea that large language models can act as reasoners with built-in self-verification abilities, avoiding the need for additional specialized training. The proposed prompting-based method realizes this ability and achieves strong reasoning results across diverse tasks.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Based on my reading of the paper, here is a one sentence summary:The paper proposes and demonstrates that large language models like GPT-3 have the ability to self-verify their reasoning by generating multiple candidate answers using chain of thought, then evaluating and selecting the best answer through a prompt-based verification process without additional training.
