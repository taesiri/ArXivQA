# [Conceptual Engineering Using Large Language Models](https://arxiv.org/abs/2312.03749)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Conceptual engineering (CE) aims to assess and improve concepts, but lacks empirical, data-driven methods to evaluate different concept definitions.
- Knowledge graphs like Wikidata impact society but may lack alignment between concept definitions and representation of concept extensions.

Proposed Solution: 
- Implements classification procedures from Nado's CE theory using large language models (LLMs). 
- Applies LLMs to generate rationales that classify entities as instances or non-instances of a concept based on its definition.  
- Evaluates concept definitions by sampling instances from Wikidata, having the LLM classify them, and computing performance metrics.

Main Contributions:
- Provides a method to empirically evaluate different definitions of a concept by applying them as computational classification procedures.
- Shows this method can identify issues with concept definitions and areas for improvement.
- Discusses using this to improve alignment of definitions and representations in knowledge graphs. 
- Proposes LLMs could support philosophers conducting CE projects.
- Views this as an initial step in investigating utility of LLMs for conceptual engineering.

In summary, the paper puts forward an empirical, LLM-based approach to implementing and evaluating conceptual engineering targets, demonstrating its potential to assist philosophers in conceptual analysis while also improving alignment of definitions and representations in knowledge graphs.


## Summarize the paper in one sentence.

 This paper proposes a method to implement conceptual engineering classification procedures using large language models, and evaluates it by classifying entities from Wikidata according to definitions from conceptual engineering projects on planet and woman.


## What is the main contribution of this paper?

 The main contribution of this paper is a method for implementing classification procedures as computational artifacts using large language models. Specifically:

- The paper proposes representing conceptual engineering targets (i.e. classification procedures) as zero-shot chain-of-thought classifiers built using large language models. 

- It demonstrates this method by implementing classification procedures for different definitions of the concepts PLANET and WOMAN using the GPT-4 large language model. 

- These classification procedures are evaluated empirically on samples of entities from the Wikidata knowledge graph, providing a data-driven approach to assessing different concept definitions.

- The paper argues this provides a way for conceptual engineering projects to incorporate empirical, data-driven analysis at scale to evaluate alternative definitions. 

- It also suggests this method could support the refinement of knowledge graphs to align better with conceptual analyses.

So in summary, the main contribution is a computational method for building classification procedures to support empirical analysis in conceptual engineering, demonstrated through an evaluation using knowledge graph data.


## What are the keywords or key terms associated with this paper?

 Based on the \keywords section in the paper, the keywords associated with this paper are:

"Conceptual engineering, natural language processing, large language models"


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in the paper:

1. The paper proposes implementing classification procedures as zero-shot chain-of-thought classifiers using large language models. What are the advantages and disadvantages of this approach compared to more traditional machine learning methods for building classifiers?

2. The method relies on generating rationales along with classifications. What role do these rationales play? How could analysis of the rationales further improve the classification procedures? 

3. Knowledge graphs are used as a source of entities to evaluate the classification procedures. What properties should an ideal knowledge graph have for evaluating definitions of concepts in a conceptual engineering project?

4. The paper evaluates classifications of entities using confusion matrices and metrics like Cohen's kappa. What other quantitative evaluation approaches could complement or improve upon this analysis?

5. Could the proposed method scale to evaluating a large number of alternative definitions for a concept, or a large number of concepts, in an automated fashion? What challenges might arise in scaling up the approach?

6. The GPT-4 model is used in the experiments. How might performance differ with other state-of-the-art models? What model properties are most important for this application?

7. The method relies heavily on prompt engineering. What prompt design principles and strategies are best suited for rendering classification procedures as prompts? 

8. What role could interactive question answering play in refining definitions and classification procedures implemented using this method?

9. How robust is the approach to issues like spurious correlations and annotation artifacts which can affect machine learning systems? What safeguards could make the method more reliable?

10. The paper speculates about AI assistants supporting conceptual engineering. What functionality beyond implementing and evaluating definitions could AI provide to conceptual engineering projects and methodologies?
