# [Grounding Everything: Emerging Localization Properties in   Vision-Language Transformers](https://arxiv.org/abs/2312.00878)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Vision-language (VL) models like CLIP show remarkable capabilities in zero-shot image classification but struggle in zero-shot localization of objects based on referential expressions. They often show an inverse relation between image patches and text.
- Existing methods require localization supervision during training or fine-tuning, limiting their vocabulary and generalization. 

Proposed Solution:
- The paper proposes the Grounding Everything Module (GEM), which leverages the latent localization capabilities of VL models without needing extra supervision. 
- GEM uses a generalized self-self attention block instead of the standard self-attention. This acts as a form of clustering, increasing similarity of tokens representing the same object.
- GEM employs normalization and an adaptive temperature in the self-attention to control the cluster formation. It further uses multiple self-self attention iterations and ensembles over different projections.

Main Contributions:
- Introduction of GEM, a training-free module to enable open vocabulary localization based on pretrained VL models, using self-self attention and cluster inducing regularizations.
- Comprehensive analysis showing GEM's improvements in visual distinctiveness through token grouping and vision-language alignment over baselines.
- Extensive experiments proving state-of-the-art results on semantic segmentation over current training-free and many trained methods, especially on large-scale datasets.

In summary, the paper proposes GEM to unlock the zero-shot localization capabilities of VL models without needing extra supervision. Through self-self attention and analysis-driven designs, GEM shows top results on multiple semantic segmentation benchmarks.
