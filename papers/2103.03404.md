# [Attention is Not All You Need: Pure Attention Loses Rank Doubly   Exponentially with Depth](https://arxiv.org/abs/2103.03404)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central hypothesis of this paper is that pure self-attention networks (transformers without skip connections or MLPs) suffer from a rapid "rank collapse" where the output converges doubly exponentially to a rank-1 matrix, making all tokens identical. The key research questions addressed are:1) Why does rank collapse occur in pure self-attention networks?2) What architectural components of the transformer (skip connections, MLPs) counteract this rank collapse?3) How can we analyze and understand this phenomenon theoretically?Specifically, the authors introduce a "path decomposition" framework to analyze transformers as ensembles of paths, where each path is a sequence of attention heads. Using this, they show theoretically that each path rapidly loses rank, leading to rank collapse. They further analyze how skip connections and MLPs mitigate this. Overall, the goal is to understand the inductive biases of transformers and the roles of different components.
