# Rethinking and Refining the Distinct Metric

## What is the central research question or hypothesis that this paper addresses?

This paper is focused on refining and improving the Distinct-n metric for evaluating diversity in text generation. The key research questions/hypotheses addressed in the paper are:1. The original Distinct metric proposed by Li et al. (2016) has some biases and unfairly penalizes longer sequences. Specifically, as the length of generated text increases, the Distinct score tends to decrease even if diversity is held constant. 2. The authors hypothesize that a better way to calculate Distinct is to scale the number of distinct tokens by the expectation of distinct tokens rather than just the total number of tokens. They formally derive an "Expectation-Adjusted Distinct" (EAD) formula based on this idea.3. The authors hypothesize that their proposed EAD metric will correlate better with human judgments compared to the original Distinct when evaluating diversity of generated text. They test this hypothesis through crowdsourced evaluation.In summary, the key hypothesis is that scaling Distinct by the expectation rather than the total number of tokens will remove biases and allow for fairer assessment of diversity across varying sequence lengths. The paper aims to demonstrate problems with the original Distinct metric and show that the proposed EAD variant better matches human judgments.


## What is the main contribution of this paper?

The main contributions of this paper are:1. It investigates the performance of the original Distinct metric and demonstrates that it has biases that tend to assign higher penalties to longer sequences, making it not sufficiently fair for evaluating text diversity. 2. It proposes a new metric called Expectation-Adjusted Distinct (EAD) that improves on the original Distinct metric by scaling the number of distinct tokens based on their expectations. This helps remove the biases that exist in the original Distinct metric.3. It provides empirical evidence from experiments on two dialog datasets showing that the proposed EAD metric correlates better with human judgments in evaluating response diversity compared to the original Distinct metric.4. It discusses the drawbacks of the EAD metric, such as its reliance on idealized assumptions, and suggests feasible applications for using it in practice. 5. It highlights risks of using the original Distinct metric for evaluating response diversity and proposes an improved metric. Overall, it aims to refine the commonly used Distinct metric to enable more accurate evaluation of text diversity in dialog systems and other text generation tasks.In summary, the main contribution is the proposal and evaluation of an improved metric called Expectation-Adjusted Distinct (EAD) to overcome biases in the commonly used original Distinct metric for evaluating text diversity. The refined EAD metric better correlates with human judgments.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper proposes an improved metric called Expectation-Adjusted Distinct (EAD) to evaluate diversity in text generation by adjusting the scaling factor in the original Distinct metric, providing empirical and theoretical evidence that EAD correlates better with human judgments and removes biases that penalize longer texts.


## How does this paper compare to other research in the same field?

Here are a few key points on how this paper compares to other related research on evaluating diversity metrics for text generation:- The paper clearly identifies an issue with the commonly used distinct metric - that it unfairly penalizes longer sequences - which has been overlooked in many prior works that use distinct. The authors provide empirical evidence and mathematical derivations to demonstrate this bias. - To address this issue, the paper proposes an improved metric called Expectation-Adjusted Distinct (EAD) that adjusts the scaling factor based on token expectations. The new metric is shown to correlate better with human judgments compared to distinct.- The paper builds on psycholinguistic research showing issues with distinct for analyzing child language. It makes a connection between those prior findings and issues arising in using distinct for evaluating neural text generation systems. - Many prior works have proposed or analyzed other diversity metrics like Self-BLEU, entropy etc. But this paper focuses specifically on refining the distinct metric due to its widespread use. The analyses are thorough and the improved EAD metric seems promising.- The scale of analysis on multiple datasets, with human eval and correlations, is quite extensive compared to some other analyses of diversity metrics. The theoretical derivations and discussions around desired properties of a good metric are also more rigorous.Overall, this paper makes an important contribution by thoroughly analyzing and refining a commonly used metric. The improved metric EAD seems to address some clear limitations of distinct. The analyses are comprehensive and the results demonstrate the benefits of the new metric. This helps advance the measurement of an important aspect - diversity - for text generation systems.


## What future research directions do the authors suggest?

The authors suggest a few potential future research directions:- Further investigating different scaling methods for the distinct metric beyond what they explored in this work. They propose that the expectation calculated under the uniform distribution assumption may not be optimal. - Evaluating the performance of the proposed EAD metric on other text generation tasks beyond dialogue, such as story generation, where diversity is also an important factor.- Exploring adjustments to EAD to make it more suitable for specialized datasets like Twitter, where the authors found EAD scores decline for longer texts. - Combining EAD with other metrics like Self-BLEU to create a more comprehensive automatic evaluation of diversity.- Developing better methods for human evaluation of diversity that can produce more absolute scores to serve as improved ground truth. The authors note limitations in their crowdsourcing process for diversity scoring.- Further analysis of the relationship between distinct metrics and human perception of diversity. The authors propose some correlations but more work could be done here.In summary, the main future directions are improving the EAD metric itself, combining it with other metrics, evaluating it on more tasks, and better understanding its relationship to human judgments of diversity through more rigorous human annotation studies. The authors lay out a research agenda for better automatic evaluation of text diversity.
