# Rethinking and Refining the Distinct Metric

## What is the central research question or hypothesis that this paper addresses?

This paper is focused on refining and improving the Distinct-n metric for evaluating diversity in text generation. The key research questions/hypotheses addressed in the paper are:1. The original Distinct metric proposed by Li et al. (2016) has some biases and unfairly penalizes longer sequences. Specifically, as the length of generated text increases, the Distinct score tends to decrease even if diversity is held constant. 2. The authors hypothesize that a better way to calculate Distinct is to scale the number of distinct tokens by the expectation of distinct tokens rather than just the total number of tokens. They formally derive an "Expectation-Adjusted Distinct" (EAD) formula based on this idea.3. The authors hypothesize that their proposed EAD metric will correlate better with human judgments compared to the original Distinct when evaluating diversity of generated text. They test this hypothesis through crowdsourced evaluation.In summary, the key hypothesis is that scaling Distinct by the expectation rather than the total number of tokens will remove biases and allow for fairer assessment of diversity across varying sequence lengths. The paper aims to demonstrate problems with the original Distinct metric and show that the proposed EAD variant better matches human judgments.


## What is the main contribution of this paper?

The main contributions of this paper are:1. It investigates the performance of the original Distinct metric and demonstrates that it has biases that tend to assign higher penalties to longer sequences, making it not sufficiently fair for evaluating text diversity. 2. It proposes a new metric called Expectation-Adjusted Distinct (EAD) that improves on the original Distinct metric by scaling the number of distinct tokens based on their expectations. This helps remove the biases that exist in the original Distinct metric.3. It provides empirical evidence from experiments on two dialog datasets showing that the proposed EAD metric correlates better with human judgments in evaluating response diversity compared to the original Distinct metric.4. It discusses the drawbacks of the EAD metric, such as its reliance on idealized assumptions, and suggests feasible applications for using it in practice. 5. It highlights risks of using the original Distinct metric for evaluating response diversity and proposes an improved metric. Overall, it aims to refine the commonly used Distinct metric to enable more accurate evaluation of text diversity in dialog systems and other text generation tasks.In summary, the main contribution is the proposal and evaluation of an improved metric called Expectation-Adjusted Distinct (EAD) to overcome biases in the commonly used original Distinct metric for evaluating text diversity. The refined EAD metric better correlates with human judgments.
