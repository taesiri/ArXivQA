# [GRAWA: Gradient-based Weighted Averaging for Distributed Training of   Deep Learning Models](https://arxiv.org/abs/2403.04206)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
The paper focuses on distributed training of deep learning models across multiple devices/workers. Such distributed training is crucial for handling large models and datasets, but it presents optimization challenges. Specifically, existing methods like Elastic Averaging SGD (EASGD) and Local SGD (LSGD) suffer from issues like the "curse of symmetry" where workers get stuck between local minima or the leader worker gets stuck in a narrow minimum. There is a need for distributed algorithms that encourage convergence to flatter, wider minima that generalize better.

Proposed Solution:
The paper proposes a new family of asynchronous distributed algorithms called Gradient-based Weighted Averaging (GRAWA). The key ideas are:

1) Periodically compute a center variable by taking a weighted average of the worker models. The weights are based on the inverse gradient norms, so that workers in flatter regions get higher weight. This allows seeking wider minima.

2) Two variants are proposed - Model-level GRAWA (MGRAWA) which computes weights at the whole model level, and Layer-level GRAWA (LGRAWA) which computes separate weights for each layer.

3) A proximity search mechanism is added to pull workers towards the center variable during local steps. This prevents loss of flatness seeking behavior between synchronizations.

4) Momentum on gradient norm averages helps stabilize weight estimates.

Contributions:

- The idea of using gradient norms to estimate flatness for distributed weight averaging is novel and first of its kind.

- MGRAWA and LGRAWA algorithms provide more reliable flatness-aware updates compared to prior approaches. Convergence guarantees are proved.

- Experiments demonstrate faster convergence and lower error than EASGD/LSGD, while finding wider minima. Works well even as number of workers is scaled up.

- Communication overhead is lower than competitors since flatness estimation uses local quantities.

In summary, the paper makes significant contributions around distributed training that targets flatter optima for better generalization, using simple and efficient gradient-based weighted averaging.
