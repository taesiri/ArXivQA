# [Lyrics: Boosting Fine-grained Language-Vision Alignment and   Comprehension via Semantic-aware Visual Objects](https://arxiv.org/abs/2312.05278)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Existing large vision-language models (LVLMs) rely on generic visual features from a frozen image encoder, lacking fine-grained visual object detection capabilities. This leads to visual hallucinations and factual errors when responding to instructions requiring detailed image understanding. 

Proposed Solution: 
- The paper proposes Lyrics, a novel pre-training and fine-tuning framework to align vision and language representations via semantic-aware visual objects. 
- A visual refiner module with image tagging, object detection and segmentation is introduced to extract local visual features and spatial representations of objects.
- A Multi-scale Querying Transformer (MQ-Former) aligns the local and global visual features with textual features via multi-task pre-training objectives. 
- The pre-trained MQ-Former provides informative soft visual prompts to a large language model (LLM) during instruction fine-tuning to boost vision-language alignment.

Main Contributions:
- Proposes a general two-stage framework to bridge visual encoders and LLMs via semantic-aware visual objects for precise image understanding.
- Introduces MQ-Former to align multi-scale visual features with language representations.
- Achieves new state-of-the-art results across vision-language tasks like image captioning, VQA and referring expression comprehension. 
- Demonstrates detailed visual understanding capabilities and mitigation of visual hallucinations in real-world dialogue scenarios.

In summary, the paper presents Lyrics, a novel LVLM pre-training and fine-tuning approach to align vision and language via fine-grained visual objects. A multi-scale transformer aligns local and global visual features with language, providing informative signals to the LLM to boost visual understanding, leading to state-of-the-art performance on various vision-language tasks.
