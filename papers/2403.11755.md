# [Meta-Prompting for Automating Zero-shot Visual Recognition with LLMs](https://arxiv.org/abs/2403.11755)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Vision-language models (VLMs) like CLIP can perform zero-shot image classification by embedding image and class name text into a shared space and measuring similarity. However, performance is sensitive to the composition of the text used to represent classes. 
- Manually designing diverse, class-specific text prompts is time-consuming and does not scale. Recent methods automate prompt generation using a large language model (LLM), but still require manual engineering of prompts for the LLM, limiting diversity.

Proposed Solution: 
- The paper proposes MPVR, a method to automatically generate diverse, class-specific visual prompts for VLMs using meta-prompting of LLMs. 
- A meta-prompt with high-level instructions, examples and task description is fed to an LLM to elicit task-specific but class-agnostic prompt templates. These templates are populated with class names and used to prompt the LLM again, yielding class-specific visual prompts.
- Two-stage prompting produces prompts infused with both task and class visual knowledge from the LLM, with no specific manual prompt engineering.

Main Contributions:
- Fully automated prompt generation for VLMs by meta-prompting LLMs, with minimal human input
- Two-stage strategy shown to produce more visually diverse and task/class-specific prompts
- Significantly boosts zero-shot classification over CLIP and other prompt engineering methods on 20 diverse datasets 
- Generalizes to multiple LLMs (GPT, Mixtral) and VLMs (CLIP, MetaCLIP)
- Releases dataset of 2.5M visual class descriptions produced by approach, spanning visual knowledge of LLMs
