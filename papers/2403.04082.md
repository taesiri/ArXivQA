# [Inference via Interpolation: Contrastive Representations Provably Enable   Planning and Inference](https://arxiv.org/abs/2403.04082)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper "Inference via Interpolation: Contrastive Representations Provably Enable Planning and Inference":

Problem:
The paper studies the problem of probabilistic inference (e.g. prediction, planning) over high-dimensional time series data. Specifically, it aims to answer questions like "what will happen in the future?" or "how did we get here?". These inference questions are challenging for high-dimensional data. The paper explores whether discriminative representation learning methods like contrastive learning can be used to perform such inference.

Proposed Solution: 
The key idea is to apply a variant of contrastive learning to time series data to obtain representations that are distributed according to a Gauss-Markov chain. Under this model, the marginal distribution over representations is Gaussian and the conditional distributions can be obtained by inverting a low-dimensional matrix. This allows prediction by mapping current representations to future ones, and planning by mapping initial and goal representations to intermediate ones.

Main Contributions:
- Provides theory to show regularized temporal contrastive learning makes the marginal distribution over learned representations Gaussian (Assumption 2 + Lemma 1).
- Builds on prior work to show the representations encode the probability ratio between positive and negative pairs (Assumption 1).
- Combines the above to prove the joint distribution over representations is Gaussian Markov, enabling easy probabilistic inference. 
    - Lemma 2: Distribution over future representations is Gaussian with linear mapping from current ones. Enables prediction.
    - Lemma 3: Distribution over intermediate representations is Gaussian conditional on initial and final representations. Enables planning.
- Validates theory on 2D spiral dataset, showing it captures nonlinear structure. Also shows usefulness for control on higher-dimensional robotic simulation tasks.

In summary, the paper provides a theoretical analysis to show how contrastive representation learning enables efficient probabilistic inference on high-dimensional time series data via simple geometric operations like linear interpolation.
