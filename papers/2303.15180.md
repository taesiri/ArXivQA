# [Detecting Backdoors in Pre-trained Encoders](https://arxiv.org/abs/2303.15180)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question seems to be: How can we detect backdoors in self-supervised learning image encoders without access to downstream task information or labels? 

The key ideas and contributions seem to be:

- Existing backdoor detection methods for supervised learning models cannot be directly applied to detect backdoors in self-supervised learning encoders, especially without access to downstream task data/labels. This is a gap the authors aim to address.

- The authors propose a new backdoor detection method called DECREE that directly scans encoders to identify backdoors. It does not require downstream task data or labels.

- DECREE inverts minimal triggers that cause high embedding similarity and uses the trigger size to identify backdoors. Backdoored encoders need smaller triggers to make clean samples have similar embeddings. 

- Extensive experiments on 444 encoders show DECREE effectively detects backdoors in common SSL attack settings like image-on-image, image-on-pair, and text-on-pair.

- DECREE is efficient and works even with no access to the encoder's pretraining data. It generalizes across different datasets, architectures, and attack settings.

So in summary, the key hypothesis is that backdoored SSL encoders can be detected by directly scanning for minimal triggers that cause high embedding similarity, without needing downstream labels or task data. The results support this hypothesis and demonstrate the effectiveness of the proposed DECREE method.


## What is the main contribution of this paper?

 Based on my reading of the paper, the main contribution seems to be proposing DECREE, the first backdoor detection approach for pre-trained encoders in self-supervised learning (SSL). The key ideas are:

- Directly scan encoders for backdoors without needing downstream task knowledge or classifiers. This overcomes limitations of prior detection methods like Neural Cleanse and ABS that rely on downstream classifiers.

- Introduce a new metric called Proportionate-L1 Norm (PL^1) to quantify the normalized size of inverted triggers from encoders. Triggers inverted from backdoored encoders tend to be smaller than those from clean encoders, allowing detection. 

- Formulate self-supervised trigger inversion as a constrained optimization problem to find a minimal trigger that causes high embedding similarity. This allows approximating the dense area in embedding space induced by backdoors.

- Evaluate on over 400 encoders under 3 SSL attack types (image-on-image, image-on-pair, text-on-pair). Show high detection accuracy even with limited or no access to pre-training data.

In summary, the main contribution is proposing the first direct backdoor detection method for SSL encoders, overcoming limitations of prior work tied to downstream tasks/classifiers. The introduced techniques like constrained trigger inversion and PL^1 metric allow effective scanning of encoders.
