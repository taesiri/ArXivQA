# [Empowering Dual-Level Graph Self-Supervised Pretraining with Motif   Discovery](https://arxiv.org/abs/2312.11927)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem: 
Applying self-supervised learning techniques to graph data still faces challenges of limited topology learning (most tasks are based on neighborhood structures, unable to capture higher-order patterns), human knowledge dependency (existing methods rely on predefined motifs, limiting generalizability), and incompetent multi-level interactions (simple aggregation of node and graph representations).

Proposed Solution:
The paper proposes a novel framework called Dual-level Graph self-supervised Pretraining with Motif discovery (DGPM). The key ideas are:

1) Introduce a dual-level pretraining architecture with node-level and subgraph-level tasks:
    - Node-level: Reconstruct node features to capture local information 
    - Subgraph-level: Discover motifs (significant high-order patterns) autonomously through a novel edge pooling module. Enforce similarity of learned motifs to match graph kernel-based similarity.

2) Establish cross-level matching between nodes and motifs to enable interaction between the two levels.

3) The overall framework orchestrates node, subgraph pretraining and cross-level matching to learn informative dual-level representations without human intervention.

Main Contributions:

1) Propose the first graph pretraining framework that utilizes motif auto-discovery to capture vital graph structures for self-supervised learning.

2) Introduce a novel pretext task to autonomously uncover crucial high-order patterns as motifs, enhancing generalization across domains.

3) Design a cross-matching task to enable intrinsic multi-level interaction between node and subgraph encoders.

4) Extensive experiments show superiority over state-of-the-art methods in unsupervised representation learning and transfer learning on 15 datasets. Analyses also demonstrate the learned motifs are interpretable.

In summary, the paper presents a novel dual-level pretraining framework with automated motif discovery that outperforms previous methods and enhances model interpretability. The design addresses key limitations of existing graph self-supervised learning approaches.
