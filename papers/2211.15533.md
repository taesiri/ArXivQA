# [The Stack: 3 TB of permissively licensed source code](https://arxiv.org/abs/2211.15533)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the main research question it addresses is:How can we create a large, high-quality dataset of permissively licensed source code to enable open and responsible research on large language models for code?The key goals outlined in the introduction are to:- Release a large dataset of permissively licensed source code to increase accessibility, reproducibility, and transparency of research on code LLMs.- Investigate whether it is possible to train competitive code LLMs using only permissively licensed data. - Start experimenting with giving developers control over whether their data is included, as not all developers may want their code used for LLM training.To address these goals, the authors introduce The Stack, a 3.1 TB dataset of permissively licensed source code in 30 programming languages collected from GitHub. They present their methodology for data collection, filtering, and deduplication. The paper then analyzes The Stack to gain insights into its composition and shows that training 350M parameter transformers on The Stack subsets can achieve strong performance on text2code benchmarks, matching or exceeding prior work. Finally, the authors acknowledge limitations of the current dataset and outline next steps for data governance such as giving developers the ability to opt-out of inclusion.In summary, the central research question is focused on creating, analyzing and demonstrating the usefulness of a large-scale permissively licensed code dataset to promote openness and responsibility in code LLM research. The authors take steps towards this goal with the release and benchmarking of The Stack.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contributions are:1. Introduction of The Stack, a large dataset of 3.1 TB of permissively licensed source code in 30 programming languages. This is one of the largest openly released datasets for pretraining code language models. 2. Analysis showing promising results on text-to-code benchmarks when training 350M parameter transformers on different Python subsets of the data. Key findings:- Near-deduplicating the data significantly improves performance across all experiments. This aligns with findings on natural language datasets.- It is possible to match performance of previous work like Codex and CodeGen on text-to-code using only permissively licensed data.  3. Release of the dataset at https://hf.co/BigCode along with tools for developers to search for their code and request removal. This enables more open and responsible research on code language models.4. Discussion of data governance, including risks around PII, malicious code, and licensing. The authors acknowledge limitations of the current dataset and plan to work on improvements in future iterations.5. Overall, the paper introduces a valuable new resource to the community and shows promising initial benchmark results. The public release and governance plan represent a step towards more transparent research on code LLMs.


## How does this paper compare to other research in the same field?

Based on the paper, here are some key ways it compares to other research in training transformers on source code:- Dataset Size: At 3.1TB, The Stack dataset used for pretraining is much larger than many other code datasets like CodeParrot (872GB), AlphaCode (715GB), and CodeGen (314GB). Having a large diverse dataset is crucial for training capable code models.- Licensing: The paper places an emphasis on using only permissively licensed data. Many prior works like Codex do not disclose licensing details. Using only permissively licensed data helps promote openness. - Deduplication: The paper shows the importance of near-deduplication, which significantly improves results over just exact deduplication. Many prior works like CodeGen only apply exact deduplication. Accounting for near-duplicates in code is an important data processing step.- Reproducibility: The authors release the permissively licensed dataset to promote reproducibility. Other works like Codex do not release training data. Releasing the dataset enables more rigorous comparisons.- Architecture: The paper uses a standard decoder-only transformer architecture, similar to other works like CodeGen and PolyCoder. The model quality primarily comes from pretraining data rather than novel model architecture.- Benchmarks: The paper evaluates on HumanEval and MBPP benchmarks that are gaining popularity in the field. Using common benchmarks facilitates comparisons between different code models.Overall, the emphasis on curating a high-quality permissively licensed dataset, rigorously evaluating different data preprocessing methods like deduplication, and promoting reproducibility through data/code release are the major contributions compared to related work. The results demonstrate it is possible to train capable code models using only permissively licensed data.


## What future research directions do the authors suggest?

Based on my reading of the paper, some of the main future research directions suggested by the authors include:- Developing more powerful and scalable models for code generation. The authors note that while their 350M parameter decoder model shows promising results, there is still room for improvement by training even larger models. They suggest exploring models with billions or trillions of parameters.- Extending the training data to more programming languages beyond Python. The authors mainly focused on Python subsets of the dataset, but note that expanding training to other languages is an important area for future work.- Improving data preprocessing techniques like deduplication. The authors find near-deduplication helps model performance significantly. They suggest further research on optimized data preprocessing pipelines. - Addressing data contamination issues. The authors point out that some evaluation examples were present in the training data. Methods to prevent and detect such contamination should be researched.- Developing better benchmarks and evaluation protocols. The authors rely on HumanEval and MBPP but note more comprehensive benchmarks are needed to fully assess code generation abilities.- Researching applications of code LLMs beyond text-to-code generation. The authors focus on decoding text to code, but other applications like code search, code summarization, and code translation could be explored.- Continuing work on data governance and developer rights. The authors propose giving developers control over their data's use. More research is needed on how to responsibly manage code datasets.- Studying societal impacts and limitations. The authors acknowledge potential risks around security, bias, and effects on the software industry. Further investigation of the technology's broader implications is suggested.In summary, the main future directions mentioned are developing more powerful models, expanding the data to more languages, improving data preprocessing and benchmarks, researching new applications, and conducting studies on social impact and ethical issues. The authors lay out an extensive research agenda around developing code LLMs responsibly.


## Summarize the paper in one paragraph.

The paper presents a new large-scale dataset called The Stack for pre-training language models on source code. The dataset contains over 3 TB of permissively licensed code across 30 programming languages collected from GitHub. The authors describe how they build the dataset, focusing on extracting permissively licensed repositories and applying near-deduplication. They train decoder-only Transformer models on different subsets of the data and evaluate on code generation benchmarks like HumanEval and MBPP. The key findings are: (1) near-deduplication significantly improves performance, (2) it is possible to match state-of-the-art results on these benchmarks using only permissively licensed data from The Stack, and (3) training on the full dataset leads to even better results. The paper discusses limitations around licensing, biases, and model performance. It also presents a data governance plan to give developers control over their data. Overall, The Stack enables open and responsible research on pre-trained models for code.
