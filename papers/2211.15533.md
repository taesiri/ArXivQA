# [The Stack: 3 TB of permissively licensed source code](https://arxiv.org/abs/2211.15533)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the main research question it addresses is:How can we create a large, high-quality dataset of permissively licensed source code to enable open and responsible research on large language models for code?The key goals outlined in the introduction are to:- Release a large dataset of permissively licensed source code to increase accessibility, reproducibility, and transparency of research on code LLMs.- Investigate whether it is possible to train competitive code LLMs using only permissively licensed data. - Start experimenting with giving developers control over whether their data is included, as not all developers may want their code used for LLM training.To address these goals, the authors introduce The Stack, a 3.1 TB dataset of permissively licensed source code in 30 programming languages collected from GitHub. They present their methodology for data collection, filtering, and deduplication. The paper then analyzes The Stack to gain insights into its composition and shows that training 350M parameter transformers on The Stack subsets can achieve strong performance on text2code benchmarks, matching or exceeding prior work. Finally, the authors acknowledge limitations of the current dataset and outline next steps for data governance such as giving developers the ability to opt-out of inclusion.In summary, the central research question is focused on creating, analyzing and demonstrating the usefulness of a large-scale permissively licensed code dataset to promote openness and responsibility in code LLM research. The authors take steps towards this goal with the release and benchmarking of The Stack.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contributions are:1. Introduction of The Stack, a large dataset of 3.1 TB of permissively licensed source code in 30 programming languages. This is one of the largest openly released datasets for pretraining code language models. 2. Analysis showing promising results on text-to-code benchmarks when training 350M parameter transformers on different Python subsets of the data. Key findings:- Near-deduplicating the data significantly improves performance across all experiments. This aligns with findings on natural language datasets.- It is possible to match performance of previous work like Codex and CodeGen on text-to-code using only permissively licensed data.  3. Release of the dataset at https://hf.co/BigCode along with tools for developers to search for their code and request removal. This enables more open and responsible research on code language models.4. Discussion of data governance, including risks around PII, malicious code, and licensing. The authors acknowledge limitations of the current dataset and plan to work on improvements in future iterations.5. Overall, the paper introduces a valuable new resource to the community and shows promising initial benchmark results. The public release and governance plan represent a step towards more transparent research on code LLMs.
