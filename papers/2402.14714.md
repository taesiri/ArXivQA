# [Efficient and Effective Vocabulary Expansion Towards Multilingual Large   Language Models](https://arxiv.org/abs/2402.14714)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Recent advances in large language models (LLMs) have shown remarkable capabilities, but predominantly favor English. Non-English languages like Korean require more tokens for equivalent semantic content, negatively impacting computational efficiency and user experience.
- Expanding tokenizer vocabulary to include frequently used long words as tokens is important for non-English users, but training new embeddings requires trillions of tokens. 

Proposed Solution: 
- The paper introduces a novel Efficient and Effective Vocabulary Expansion (EEVE) method to enhance non-English, specifically Korean, proficiency of English-centric LLMs.
- EEVE utilizes subword-based embedding initialization and a 7-stage training methodology with meticulous parameter freezing to effectively incorporate new linguistic tokens.

Key Contributions:
- EEVE allows transferring advanced capabilities from English to Korean more efficiently than standalone Korean pre-training.  
- Introduces Korean LLM family - EEVE-Korean-10.8B and EEVE-Korean-2.8B built on top of SOLAR-10.7B and Phi-2 respectively.
- Shows improved performance on Korean tasks and preserved English capabilities. EEVE-Korean-10.8B outperforms prior Korean pre-trained LLMs.
- Achieves significance performance boost using just 2 billion pre-training tokens, demonstrating efficiency of proposed training strategy.
- Models and training methodology open-sourced to empower research community for developing inclusive language technologies.

In summary, the paper makes notable contributions in efficiently adapting English-centric LLMs to other languages by proposing the EEVE training methodology and showcasing state-of-the-art capabilities for the open Korean LLM space.
