# [Efficient and Effective Vocabulary Expansion Towards Multilingual Large   Language Models](https://arxiv.org/abs/2402.14714)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Recent advances in large language models (LLMs) have shown remarkable capabilities, but predominantly favor English. Non-English languages like Korean require more tokens for equivalent semantic content, negatively impacting computational efficiency and user experience.
- Expanding tokenizer vocabulary to include frequently used long words as tokens is important for non-English users, but training new embeddings requires trillions of tokens. 

Proposed Solution: 
- The paper introduces a novel Efficient and Effective Vocabulary Expansion (EEVE) method to enhance non-English, specifically Korean, proficiency of English-centric LLMs.
- EEVE utilizes subword-based embedding initialization and a 7-stage training methodology with meticulous parameter freezing to effectively incorporate new linguistic tokens.

Key Contributions:
- EEVE allows transferring advanced capabilities from English to Korean more efficiently than standalone Korean pre-training.  
- Introduces Korean LLM family - EEVE-Korean-10.8B and EEVE-Korean-2.8B built on top of SOLAR-10.7B and Phi-2 respectively.
- Shows improved performance on Korean tasks and preserved English capabilities. EEVE-Korean-10.8B outperforms prior Korean pre-trained LLMs.
- Achieves significance performance boost using just 2 billion pre-training tokens, demonstrating efficiency of proposed training strategy.
- Models and training methodology open-sourced to empower research community for developing inclusive language technologies.

In summary, the paper makes notable contributions in efficiently adapting English-centric LLMs to other languages by proposing the EEVE training methodology and showcasing state-of-the-art capabilities for the open Korean LLM space.


## Summarize the paper in one sentence.

 This paper introduces EEVE-Korean, a method for efficiently expanding the vocabulary of English-centric language models to better support Korean language tasks, without compromising performance on English.


## What is the main contribution of this paper?

 Based on the paper, the main contribution is the proposal of an efficient and effective vocabulary expansion (EEVE) method for adapting large language models trained on English to also handle Korean language data. Specifically:

- The EEVE method combines parameter freezing and subword-based embedding initialization to effectively incorporate new linguistic tokens from the Korean language into an English-centric language model. 

- A structured 7-stage training process is introduced to carefully integrate the new tokens into the model in a way that preserves existing capabilities while enhancing Korean language proficiency.

- The EEVE method allows building Korean-adapted models like EEVE-Korean-10.8B-v1.0 that outperform other Korean pre-trained models, while maintaining strong performance on English tasks. This demonstrates improved efficiency and effectiveness compared to prior work.

- The EEVE training strategy enables strong Korean performance with fewer training tokens than comparable models. For example, EEVE-Korean-10.8B-v1.0 was trained on just 2 billion tokens but outperforms the 10.7B parameter OPEN-SOLAR-KO model.

In summary, the main contribution is an efficient and effective approach to vocabulary expansion that can adapt English-centric LLMs to new languages like Korean while preserving most of their capabilities.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and keywords associated with it are:

- Efficient and Effective Vocabulary Expansion (EEVE): The main method introduced in the paper for expanding the vocabulary of large language models to better support non-English languages like Korean. Involves techniques like parameter freezing and subword initialization.

- Large language models (LLMs): The models that the research aims to improve, like SOLAR-10.7B and Phi-2, by making them more multilingual.

- Korean: The non-English language that the researchers focus on supporting better through vocabulary expansion.

- Subword initialization: One of the techniques used in EEVE to initialize the embeddings of new vocabulary tokens based on subword embeddings.

- Parameter freezing: The other key technique in EEVE that freezes different parameters at different stages of training to better integrate the new vocabulary. 

- Multilingual: An important capability of models that can process and understand multiple languages like English and Korean. A goal of the vocabulary expansion.

- Tokenization: Relevant to the more efficient Korean tokenization achieved after vocabulary expansion.

- Evaluation benchmarks: Used to test the models in both English and Korean, like BoolQ, COPA, HellaSwag.

Does this summary cover the major keywords and terminology from the paper? Let me know if you need any clarification or have additional questions.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the EEVE method proposed in the paper:

1. The paper mentions using subword-based embedding initialization for the newly added tokens. Can you explain in more detail how this initialization works and why it is useful? 

2. The training methodology involves 7 stages with meticulous freezing/unfreezing of parameters. What is the motivation behind this multi-stage approach? Why not train all parameters together from the start?

3. Stage 1 focuses on only training the input embeddings of new tokens. Why is training just the input embeddings important at this stage? How does the subword initialization help here?

4. At later stages, the methodology trains output embeddings of both old and new tokens. Why is it important to also update old token embeddings in thisvocabulary expansion scenario? 

5. The paper claims significant efficiency benefits from the optimized tokenizer. Can you analyze the effects of the improved tokenizer on sequence length, computational complexity and overall training efficiency?

6. For fine-tuning, the paper uses translated versions of English instruction datasets like Orca. What are the key challenges in creating a high-quality translated dataset for this purpose? 

7. The results show performance gains in Korean without losses in English. What aspects of the EEVE method contribute to preserving English capabilities despite the focus on Korean?

8. The method is evaluated on several Korean and English benchmarks. Are there any other evaluation scenarios or tasks you would suggest to better analyze the model's reasoning and generative abilities?

9. The conclusion discusses applying EEVE to more languages. Do you foresee any challenges in scaling this approach to significantly more languages? 

10. The paper focuses primarily on vocabulary expansion. Could the EEVE method work for more extensive model architecture modifications? What are the limitations?
