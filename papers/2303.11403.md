# eP-ALM: Efficient Perceptual Augmentation of Language Models

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper addresses is how to efficiently adapt unimodal pretrained models (e.g. large language models and visual encoders) to multimodal tasks like visual question answering and image/video/audio captioning without large amounts of multimodal pretraining data or fine-tuning many parameters. Specifically, the paper investigates augmenting language models with perception by freezing most parameters and only training a small number of adaptation parameters (e.g. a linear projection layer and soft prompt).The key hypothesis is that it is possible to devise very efficient approaches, in terms of number of trainable parameters, training data, and compute, to adapt existing unimodal models for multimodal tasks by better aligning the visual and language representations.


## What is the main contribution of this paper?

The main contributions of this paper are:- Proposes eP-ALM, an efficient approach to adapt large language models (LLMs) for multimodal tasks by augmenting them with perception. - Introduces a new challenging setup for adapting pretrained unimodal models to multimodal tasks that favors training and inference efficiency. The setup avoids multimodal pretraining, uses only unimodal encoders, freezes most parameters, and keeps fast inference.- Shows that by freezing over 99% of parameters and training only a linear projection layer and soft prompt, eP-ALM can significantly outperform other baselines on image, video and audio-language tasks.- Demonstrates that eP-ALM is parameter efficient, data efficient, generalizes better out-of-distribution, and scales well with larger language and vision models, reaching 54.5% on VQA v2 while training only 0.06% parameters.- Validates the effectiveness across multiple modalities (image, video, audio) and tasks like VQA, captioning, visual reasoning, and audio captioning.In summary, the main contribution is an extremely parameter-efficient approach to adapt large pretrained language models for multimodal tasks by augmenting them with perception from frozen unimodal encoders. This is done through a minimal setup that enables efficient training and inference.
