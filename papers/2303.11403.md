# eP-ALM: Efficient Perceptual Augmentation of Language Models

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper addresses is how to efficiently adapt unimodal pretrained models (e.g. large language models and visual encoders) to multimodal tasks like visual question answering and image/video/audio captioning without large amounts of multimodal pretraining data or fine-tuning many parameters. Specifically, the paper investigates augmenting language models with perception by freezing most parameters and only training a small number of adaptation parameters (e.g. a linear projection layer and soft prompt).The key hypothesis is that it is possible to devise very efficient approaches, in terms of number of trainable parameters, training data, and compute, to adapt existing unimodal models for multimodal tasks by better aligning the visual and language representations.


## What is the main contribution of this paper?

The main contributions of this paper are:- Proposes eP-ALM, an efficient approach to adapt large language models (LLMs) for multimodal tasks by augmenting them with perception. - Introduces a new challenging setup for adapting pretrained unimodal models to multimodal tasks that favors training and inference efficiency. The setup avoids multimodal pretraining, uses only unimodal encoders, freezes most parameters, and keeps fast inference.- Shows that by freezing over 99% of parameters and training only a linear projection layer and soft prompt, eP-ALM can significantly outperform other baselines on image, video and audio-language tasks.- Demonstrates that eP-ALM is parameter efficient, data efficient, generalizes better out-of-distribution, and scales well with larger language and vision models, reaching 54.5% on VQA v2 while training only 0.06% parameters.- Validates the effectiveness across multiple modalities (image, video, audio) and tasks like VQA, captioning, visual reasoning, and audio captioning.In summary, the main contribution is an extremely parameter-efficient approach to adapt large pretrained language models for multimodal tasks by augmenting them with perception from frozen unimodal encoders. This is done through a minimal setup that enables efficient training and inference.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper proposes an efficient approach called eP-ALM to adapt pretrained language models for multimodal tasks like VQA and captioning by augmenting them with perceptual context from frozen unimodal vision encoders, demonstrating strong performance while training only a minimal number of parameters.


## How does this paper compare to other research in the same field?

This paper proposes a new method called eP-ALM for efficiently adapting large unimodal language models for multimodal tasks like visual question answering and image/video/audio captioning. Here are some key ways this work compares to other related research:- Focuses on directly fine-tuning pretrained models on downstream tasks rather than pretraining on large multimodal datasets. This makes the approach more efficient in terms of training data, compute, and time compared to many existing vision-language models that require expensive pretraining.- Freezes nearly all parameters of the vision and language models (over 99%) and only trains a small linear connection layer and/or soft prompt. This is far more parameter-efficient than methods that fine-tune all parameters or use large adapter modules. - Relies solely on unimodal vision models like ViT rather than multimodal models like CLIP that require training on huge image-text datasets. Avoiding this expensive multimodal pretraining improves efficiency.- Tests image, video, and audio modalities. Most prior work on adapting LMs has focused on images, so exploring multiple modalities is a useful contribution.- Achieves strong performance on VQA, captioning, etc. with very minimal training. Outperforms other efficient approaches for adapting LMs like PromptFuse and MAGMA.- Provides useful analysis of model scaling, showing performance gains as language and vision model size increases. This helps demonstrate the potential of the approach when combined with very large models.Overall, this work pushes towards extremely efficient exploitation of pretrained unimodal models for multimodal tasks. The focus on minimizing trainable parameters, avoiding expensive pretraining, and using only unimodal encoders sets it apart from much prior research and provides a useful data point towards maximizing efficiency.
