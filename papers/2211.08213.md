# [Is Style All You Need? Dependencies Between Emotion and GST-based   Speaker Recognition](https://arxiv.org/abs/2211.08213)

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper addresses is whether speaker identity embeddings extracted from speech samples can be used for detection and classification of emotion. Specifically, the authors hypothesize that vocal expression changes speaker-dependent attributes, consequently affecting the extracted speaker embeddings. They test this by using speaker identity embeddings from a pre-trained DeepTalk model for emotion classification and detection tasks.The key hypothesis is that a speaker sounds less like themselves when their emotion changes, so the emotion state should be detectable from the speaker identity embeddings that capture vocal style.


## What is the main contribution of this paper?

The main contributions of this paper are:1. A novel use of Global Style Tokens (GSTs) from a speaker recognition model for speech emotion recognition. The authors show that GST embeddings trained on speaker identity contain useful information for detecting emotions.2. Successful speech emotion detection and classification via transfer learning from a speaker recognition model. The authors are able to classify emotions in acted (CREMA-D), semi-natural (IEMOCAP) and natural (MSP-Podcast) speech datasets by using GST embeddings and SVMs.3. A novel hierarchical classifier for disambiguating between similar emotion classes. The authors propose a two-stage SVM classifier that first separates out the 'sad' class before classifying between other emotions. This improves performance. 4. Analysis of the effect of emotions on speaker recognition performance. The authors hypothesize and provide evidence that emotions change speaker identity characteristics. This could motivate future work on emotion-invariant speaker recognition.In summary, the main contribution is showing that speaker identity embeddings trained only with speaker labels contain useful information for emotion recognition, enabling transfer learning from speaker recognition to emotion classification. The proposed methods demonstrate competitive performance on standard emotion datasets.
