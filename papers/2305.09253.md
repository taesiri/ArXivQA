# [Online Continual Learning Without the Storage Constraint](https://arxiv.org/abs/2305.09253)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is:How can we perform online continual learning without storage constraints, emphasizing economical computation budgets instead?The key hypothesis is that by removing storage constraints and storing all past data, a simple nearest neighbor-based approach can outperform existing online continual learning methods that operate under tighter storage budgets. The paper argues that with dropping storage costs, storing all past data is feasible as long as computational costs are controlled. It proposes an adaptive memory system based on k-nearest neighbors that can store all past data and operate under a logarithmic computational budget. The hypothesis is that this simple approach will enable rapid adaptation and avoid catastrophic forgetting, outperforming existing online continual learning methods.In summary, the main research question is how to do online continual learning without storage constraints, and the hypothesis is that a nearest neighbor-based system can work better than existing approaches by exploiting the ability to store all past data. The experiments aim to test this hypothesis.


## What is the main contribution of this paper?

The main contribution of this paper is proposing an online continual learning approach that relaxes storage constraints and emphasizes fixed, limited economical budget. The key ideas are:- Storing the entirety of the incoming data stream is feasible under a logarithmic computational budget by using efficient approximate nearest neighbor algorithms. - Simple k-nearest neighbor classifiers with universal pre-trained feature extractors can effectively utilize all the stored data for online learning.- This approach provides attractive properties like fast adaptation to new examples and not forgetting past data.- Experiments on large-scale datasets CLOC and CGLM show the proposed method significantly outperforms existing online continual learning methods in terms of forward transfer, backward transfer, and computational efficiency.In summary, the paper argues for removing storage constraints in online continual learning settings where storage is inexpensive, and shows a simple but effective approach to leverage unlimited storage for online learning under a logarithmic computational budget. The main contribution is a new perspective on online continual learning that emphasizes economy and computations over storage.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper proposes an online continual learning approach that stores all past data using efficient nearest neighbor search and adapts rapidly via one-sample learning, outperforming methods with far larger computational budgets on large-scale benchmarks.


## How does this paper compare to other research in the same field?

Here are some key ways this paper compares to other research in online continual learning:- Problem formulation: This paper follows the latest generation of formulation from Cai et al. 2021 that evaluates rapid adaptation and information retention on real-world data streams without task boundaries. Many prior methods assumed task boundaries or used simpler stationary datasets. - Storage constraints: Unlike most prior work that imposed fixed limited storage, this paper removes storage constraints entirely. It argues storing all data is feasible and focuses on controlling computational complexity instead.- Approach: This paper proposes a simple kNN-based approach using pretrained features, unlike most prior deep learning methods based on experience replay and SGD. The consistency and fast adaptation properties are notable. - Experiments: Evaluation is on large-scale and complex image classification datasets orders of magnitude bigger than commonly used benchmarks. Performance significantly exceeds prior algorithms despite far lower compute.- Key advantage: The ability to learn from single examples is a major advantage compared to prior experience replay methods that require batches. This enables truly online and rapid adaptation.Overall, this paper makes a case for rethinking assumptions in online continual learning using a simple approach tailored to modern hardware capabilities. The strong empirical results challenge the need for complex constrained optimization-based algorithms. It also highlights issues with deep learning techniques in this setting.
