# [Online Continual Learning Without the Storage Constraint](https://arxiv.org/abs/2305.09253)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is:How can we perform online continual learning without storage constraints, emphasizing economical computation budgets instead?The key hypothesis is that by removing storage constraints and storing all past data, a simple nearest neighbor-based approach can outperform existing online continual learning methods that operate under tighter storage budgets. The paper argues that with dropping storage costs, storing all past data is feasible as long as computational costs are controlled. It proposes an adaptive memory system based on k-nearest neighbors that can store all past data and operate under a logarithmic computational budget. The hypothesis is that this simple approach will enable rapid adaptation and avoid catastrophic forgetting, outperforming existing online continual learning methods.In summary, the main research question is how to do online continual learning without storage constraints, and the hypothesis is that a nearest neighbor-based system can work better than existing approaches by exploiting the ability to store all past data. The experiments aim to test this hypothesis.


## What is the main contribution of this paper?

The main contribution of this paper is proposing an online continual learning approach that relaxes storage constraints and emphasizes fixed, limited economical budget. The key ideas are:- Storing the entirety of the incoming data stream is feasible under a logarithmic computational budget by using efficient approximate nearest neighbor algorithms. - Simple k-nearest neighbor classifiers with universal pre-trained feature extractors can effectively utilize all the stored data for online learning.- This approach provides attractive properties like fast adaptation to new examples and not forgetting past data.- Experiments on large-scale datasets CLOC and CGLM show the proposed method significantly outperforms existing online continual learning methods in terms of forward transfer, backward transfer, and computational efficiency.In summary, the paper argues for removing storage constraints in online continual learning settings where storage is inexpensive, and shows a simple but effective approach to leverage unlimited storage for online learning under a logarithmic computational budget. The main contribution is a new perspective on online continual learning that emphasizes economy and computations over storage.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper proposes an online continual learning approach that stores all past data using efficient nearest neighbor search and adapts rapidly via one-sample learning, outperforming methods with far larger computational budgets on large-scale benchmarks.
