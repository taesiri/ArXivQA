# [Meta-Learned Attribute Self-Gating for Continual Generalized Zero-Shot   Learning](https://arxiv.org/abs/2102.11856)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Zero-shot learning (ZSL) methods aim to recognize new unseen classes not encountered during training, using auxiliary information like attributes. However, most methods assume a one-time adaptation and fail in continual learning settings where new data keeps arriving sequentially. 
- Generative model based approaches to tackle ZSL are slow and assume unseen classes are known apriori during training.

Proposed Solution: 
- The paper proposes a meta-continual zero-shot learning (MCZSL) approach that works in both generalized ZSL and continual learning settings.

Key Components:
- Self-Gating on Attributes: Learns to filter noisy attribute dimensions and provide robust class-specific vectors satisfying intra-class variability.  
- Scaled Layer Normalization: Helps mitigate seen class bias, important for ZSL.
- Reservoir Sampling: Replays samples from memory to mitigate catastrophic forgetting.
- Meta-Learning: Enables efficient few-shot adaptation critical due to the limited memory budget.

Experiments:
- Evaluated on CUB, AwA, aPY and SUN datasets on generalized ZSL, fixed continual ZSL and dynamic continual ZSL protocols.
- Achieves new state-of-the-art results on all datasets and protocols.
- Ablation studies validate the contribution of each component.
- Training is >100x faster than generative approaches.

Key Contributions:
- Novel method for meta-continual zero-shot learning that works in both generalized and continual settings.
- Self-gating on attributes and scaled normalization that obviates expensive generative models.
- State-of-the-art results on multiple datasets and evaluation protocols.
- Significantly faster training than generative approaches.
