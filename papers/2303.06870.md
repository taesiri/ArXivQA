# [Three Guidelines You Should Know for Universally Slimmable   Self-Supervised Learning](https://arxiv.org/abs/2303.06870)

## What is the central research question or hypothesis that this paper addresses?

 The central research question this paper addresses is how to train a self-supervised model that can run efficiently at arbitrary widths, requiring only a single training process and set of weights. 

The key hypothesis is that ensuring temporal consistency in the guidance provided to the sub-networks during training is critical to making universal slimmability work for self-supervised models. The paper proposes three guidelines for loss design to achieve this:

1. Use a relative distance based loss for the base model to produce temporally consistent outputs. 

2. Use a relative distance based loss for distillation to provide consistent guidance to sub-networks.

3. Use a momentum teacher to produce stable guidance for sub-networks.

The paper shows experimentally that satisfying at least one of these guidelines enables successful training of a universally slimmable self-supervised model, with a single training run producing a model that achieves strong accuracy-efficiency trade-offs across a range of widths.
