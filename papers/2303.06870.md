# [Three Guidelines You Should Know for Universally Slimmable   Self-Supervised Learning](https://arxiv.org/abs/2303.06870)

## What is the central research question or hypothesis that this paper addresses?

 The central research question this paper addresses is how to train a self-supervised model that can run efficiently at arbitrary widths, requiring only a single training process and set of weights. 

The key hypothesis is that ensuring temporal consistency in the guidance provided to the sub-networks during training is critical to making universal slimmability work for self-supervised models. The paper proposes three guidelines for loss design to achieve this:

1. Use a relative distance based loss for the base model to produce temporally consistent outputs. 

2. Use a relative distance based loss for distillation to provide consistent guidance to sub-networks.

3. Use a momentum teacher to produce stable guidance for sub-networks.

The paper shows experimentally that satisfying at least one of these guidelines enables successful training of a universally slimmable self-supervised model, with a single training run producing a model that achieves strong accuracy-efficiency trade-offs across a range of widths.


## What is the main contribution of this paper?

 The main contribution of this paper is proposing a method called Universally Slimmable Self-Supervised Learning (US3L) to train a single self-supervised model that can run at arbitrary width with high accuracy and efficiency. The key highlights are:

- They discover that directly adapting self-supervised learning to universally slimmable networks fails, and identify that temporal consistency of guidance is critical for the success of US-Net training. 

- Based on theoretical analysis, they propose three guidelines for the loss design to ensure temporal consistency from a unified gradient perspective.

- They propose dynamic sampling and group regularization strategies to reduce training overhead and improve accuracy of US3L.

- Extensive experiments on CNNs and ViTs validate that their method exceeds individually trained models and is comparable to knowledge distillation, with only once training and a single model.

- Their US3L model achieves superior accuracy-efficiency trade-offs when transferred to various downstream tasks like recognition, detection and segmentation.

In summary, the main contribution is proposing the US3L framework to train a single slimmable self-supervised model that can adaptively run at any width with high performance, which saves significant training and deployment overhead compared to existing methods. The theoretical analysis and practical techniques are also valuable.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the key points from the paper:

The paper proposes three guidelines for training universally slimmable self-supervised models to achieve better accuracy-efficiency trade-offs, using techniques like dynamic sampling, group regularization, and ensuring temporal consistency in the loss functions.
