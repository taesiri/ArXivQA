# [Bridging the Gap between Model Explanations in Partially Annotated   Multi-label Classification](https://arxiv.org/abs/2304.01804)

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper aims to address is: How do false negative labels affect the model explanation in partially annotated multi-label image classification, and how can we mitigate the effects? Specifically, the paper investigates how assuming unobserved labels as negative labels (i.e. false negatives) impacts the class activation maps (CAMs) that explain the model's predictions. The key findings are:- The overall spatial structure/shape of the CAMs is preserved, but the attribution scores are lowered, especially for positive labels. - This drop in attribution scores for positive labels leads to lower prediction scores.Based on these observations, the paper proposes a simple method called BoostReLU to compensate for the damaged attribution scores by boosting the scores in highlighted CAM regions. This helps bridge the gap between the explanation of models trained with full vs. partial labels.In summary, the central hypothesis is that boosting attribution scores in CAMs can mitigate the negative effects of false negative labels on model explanations in partially labeled multi-label classification. The proposed BoostReLU method is designed to test this hypothesis.


## What is the main contribution of this paper?

The main contribution of this paper is proposing a method called BoostReLU to improve the performance of partially annotated multi-label image classification. The key ideas are:- Analyzing how false negative labels (unannotated positive labels that are incorrectly assumed negative) affect the class activation maps (CAM) of models trained with partial vs full labels. They find the overall CAM structure is similar but attribution scores drop for the partial label model. - Proposing BoostReLU, a simple function that boosts high attribution scores in the CAM to compensate for the score drop caused by false negatives. This makes the partial label model CAM more similar to the full label model CAM.- Applying BoostReLU during inference improves performance on the partial label baseline model without retraining. Combining it with methods that correct false negatives during training leads to further gains.- Achieving state-of-the-art results on PASCAL VOC, MS COCO, NUSWIDE, and OpenImages datasets by applying BoostReLU with recent false negative correction methods.In summary, the key contribution is a simple and effective technique to bridge the gap between model explanations under partial vs full supervision by boosting damaged CAM scores, which also improves multi-label classification performance.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

The paper proposes a method to bridge the gap between model explanations in partially annotated multi-label classification. The key idea is to boost the attribution scores of a model trained with partial labels to resemble those of a model trained with full labels, which improves performance. The main contribution is a simple function called BoostLU that compensates for score damage caused by false negative labels in the partial label setting.


## How does this paper compare to other research in the same field?

This paper makes several contributions to multi-label image classification with partially annotated training data:- It analyzes how false negative label noise affects model explanations generated by class activation mapping (CAM). The key finding is that while the overall spatial structure of the CAM is preserved, the attribution scores are lowered, especially for positive labels. - Motivated by this analysis, the authors propose a simple but effective BoostLU function to compensate for the reduced CAM scores by boosting the scores above a threshold. - They demonstrate three usage scenarios for BoostLU: inference only, training+inference, and in combination with recent methods that detect/modify likely false negatives during training. The last approach performs best.- Experiments on four standard datasets show that adding BoostLU to state-of-the-art methods for handling partial labels boosts performance significantly. The combined approach achieves new state-of-the-art results.The key differences from related work are:- The CAM analysis provides new insights into how false negatives affect model explanations in multi-label classification. This is a different finding than in prior work on CAM for noisy single-label classification [1].- BoostLU is simpler than prior methods for handling false negatives like loss correction/rejection [2], asymmetric loss [3], or entropy maximization [4]. It improves performance at very little extra cost.- Using BoostLU in combination with recent false negative detection methods [2] is novel and achieves better results than either technique alone.Overall, this paper makes both empirical and methodological contributions for multi-label classification with missing labels. The CAM analysis and effective BoostLU approach are the key novel elements compared to related work.[1] Zhang et al. Learn from Failure: Dealing with Noisy Labels by Learning from Explanations. CVPR 2022.[2] Kim et al. Large Loss Modification for Robust Noisy Multi-Label Classification. ECCV 2022.  [3] Ben-Baruch et al. The Annoyance of the Mean: Revisiting Class Activation Mappings for Local Explanations. ICCV 2021.[4] Zhou et al. Acknowledging Unlabeled Data in Partially Labeled Multi-Label Classification. CVPR 2022.
