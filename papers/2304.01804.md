# [Bridging the Gap between Model Explanations in Partially Annotated   Multi-label Classification](https://arxiv.org/abs/2304.01804)

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper aims to address is: How do false negative labels affect the model explanation in partially annotated multi-label image classification, and how can we mitigate the effects? Specifically, the paper investigates how assuming unobserved labels as negative labels (i.e. false negatives) impacts the class activation maps (CAMs) that explain the model's predictions. The key findings are:- The overall spatial structure/shape of the CAMs is preserved, but the attribution scores are lowered, especially for positive labels. - This drop in attribution scores for positive labels leads to lower prediction scores.Based on these observations, the paper proposes a simple method called BoostReLU to compensate for the damaged attribution scores by boosting the scores in highlighted CAM regions. This helps bridge the gap between the explanation of models trained with full vs. partial labels.In summary, the central hypothesis is that boosting attribution scores in CAMs can mitigate the negative effects of false negative labels on model explanations in partially labeled multi-label classification. The proposed BoostReLU method is designed to test this hypothesis.


## What is the main contribution of this paper?

The main contribution of this paper is proposing a method called BoostReLU to improve the performance of partially annotated multi-label image classification. The key ideas are:- Analyzing how false negative labels (unannotated positive labels that are incorrectly assumed negative) affect the class activation maps (CAM) of models trained with partial vs full labels. They find the overall CAM structure is similar but attribution scores drop for the partial label model. - Proposing BoostReLU, a simple function that boosts high attribution scores in the CAM to compensate for the score drop caused by false negatives. This makes the partial label model CAM more similar to the full label model CAM.- Applying BoostReLU during inference improves performance on the partial label baseline model without retraining. Combining it with methods that correct false negatives during training leads to further gains.- Achieving state-of-the-art results on PASCAL VOC, MS COCO, NUSWIDE, and OpenImages datasets by applying BoostReLU with recent false negative correction methods.In summary, the key contribution is a simple and effective technique to bridge the gap between model explanations under partial vs full supervision by boosting damaged CAM scores, which also improves multi-label classification performance.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

The paper proposes a method to bridge the gap between model explanations in partially annotated multi-label classification. The key idea is to boost the attribution scores of a model trained with partial labels to resemble those of a model trained with full labels, which improves performance. The main contribution is a simple function called BoostLU that compensates for score damage caused by false negative labels in the partial label setting.
