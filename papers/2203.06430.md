# [Categories of Differentiable Polynomial Circuits for Machine Learning](https://arxiv.org/abs/2203.06430)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the abstract and introduction, this paper appears to focus on studying categories of polynomial circuits as a semantic framework for machine learning models. More specifically, it aims to:1) Provide an algebraic description of polynomial circuits and their reverse derivative structure, by building a presentation of these categories using operations and equations. 2) Show that a certain presentation of polynomial circuits is functionally complete, meaning it can represent any function over a finite semiring. This allows the category to have enough expressiveness for machine learning applications.3) Discuss specific choices of semiring and the implications they have on using polynomial circuits for machine learning. For example, using saturating arithmetic semirings to avoid problems with modular arithmetic. 4) Highlight opportunities to use these polynomial circuit categories as the basis for practical ML tools, as well as connections to concepts like model quantization.So in summary, the central research focus seems to be on formally defining and studying algebraic categories of polynomial circuits suitable for giving semantics to machine learning models and algorithms. The key hypothesis is that these categories can provide a useful foundation for building ML tools and studying new models/learning algorithms.


## What is the main contribution of this paper?

Based on the abstract, it seems the main contribution of this paper is defining a class of "polynomial circuits" as a setting for machine learning models, showing they admit a reverse derivative structure, and proving they are functionally complete. Specifically:- They define polynomial circuits as categories presented by generators (operations) and equations. Polynomial circuits generalize boolean circuits by having wires carry values from an arbitrary semiring rather than just {0,1}.- They show these categories admit a reverse derivative structure, making them suitable as a class of machine learning models that can be trained with gradient descent. - They prove a functional completeness result, showing polynomial circuits are expressive enough to represent any function between finite sets. This is important for a machine learning model class.- They discuss implications of using different semirings, relating it to existing work on neural networks and quantized networks. They propose semirings like "saturating arithmetic" as alternatives that may have advantages.So in summary, they introduce polynomial circuits as a new model class for machine learning, show this class has desirable theoretical properties like reverse derivatives and functional completeness, and relate it to existing literature on neural network models. The main contribution seems to be providing both a theoretical foundation and expressiveness guarantees for polynomial circuits as a learning model.


## How does this paper compare to other research in the same field?

Based on my reading, here is a brief comparison of this paper to other related work:- The paper presents a categorical framework for studying machine learning models and algorithms. Using the formalism of reverse derivative categories (RDCs), it focuses on defining specific model classes rather than just the learning algorithms. This is a novel approach compared to most categorical machine learning work like RDA which treats the model and its category abstractly.- The model class proposed is "polynomial circuits", a generalization of the boolean circuits studied in RDA. While RDA uses circuits over Z2, this allows arbitrary semirings. The functional completeness result also seems new compared to RDA. - The focus on alternate semirings for model design also seems unique. Most work on efficient models uses standard real-valued networks and then quantizes. Here, alternate arithmetic itself provides efficiency gains. Related work like finite tropical semirings exist but aren't connected to ML.- The algebraic presentations via generators and equations is a common technique in categorical ML, but hasn't been much applied to model classes before. Using the extension theorem to build RDCs is also novel.- The discussion of potential tools is similar to work like Applied Categorical Structures. But the emphasis on alternate arithmetic and model classes is new. Overall the combination of RDC semantics, algebraic presentations, and non-standard arithmetic seems unique compared to related categorical ML research.In summary, this paper carves out a novel niche in the categorical study of machine learning, with its focus on model classes and alternate arithmetic. The technical tools are established ones like RDC and algebraic presentations, but their application to define model classes rather than just algorithms is new.
