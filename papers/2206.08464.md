# [PRANC: Pseudo RAndom Networks for Compacting deep models](https://arxiv.org/abs/2206.08464)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading, the key research questions and hypotheses addressed in this paper are:- Can we reparametrize a deep neural network model as a linear combination of several randomly initialized and frozen models?The authors hypothesize that it is possible to represent a trained deep model as a linear combination of multiple "basis" models that are randomly initialized and kept frozen. - Can we find good solutions that reside in low-dimensional random subspaces of the weight space?The authors examine whether it is possible to find good solutions that lie in very low-dimensional random subspaces defined by the frozen random basis models. This would suggest that good solutions can be found in small random slices of the high-dimensional weight space of overparametrized networks.- Can we significantly compress deep models by storing only the seed and coefficients? The key hypothesis is that by constraining the model to be a linear combination of random basis models, the model can be compressed by a large factor by only storing the seed used to generate the random models and the learned linear combination coefficients.- How does this method compare to prior compression techniques on accuracy and efficiency?The authors evaluate whether this compression scheme can outperform prior techniques like knowledge distillation, pruning, and core-set/data distillation in terms of accuracy vs model size. The goal is to achieve much higher compression rates without compromising accuracy.In summary, the core research questions revolve around whether deep models can be reparametrized and compressed effectively using random frozen basis models, and how this compares to prior model compression techniques. The authors hypothesize substantial compression is possible without sacrificing accuracy or model architecture.
