# [Efficient Multimodal Semantic Segmentation via Dual-Prompt Learning](https://arxiv.org/abs/2312.00360)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a high-quality paragraph summarizing the key points of the paper:

This paper proposes DPLNet, a surprisingly simple yet effective framework for training-efficient multimodal semantic segmentation. The key innovation is a dual-prompt learning paradigm that adapts a frozen pre-trained RGB model to multimodal semantic segmentation by introducing only a small number of additional trainable parameters. Specifically, the method consists of two components: 1) A multimodal prompt generator (MPG) that fuses features from different modalities in a compact manner to generate multilevel multimodal prompts, which are injected into the frozen backbone network; 2) A multimodal feature adapter (MFA) module with learnable tokens that interacts with multimodal features via cross-attention to enhance feature adaption. Experiments on multiple RGB-D and RGB-T datasets demonstrate state-of-the-art performance for semantic segmentation while using only 4.4% of the backbone parameters during training. Additional experiments on other tasks like salient object detection further showcase the generalizability of the approach. By enabling training-efficient adaptation of powerful pre-trained models, DPLNet provides an effective and unified paradigm for multimodal dense prediction tasks.
