# [Deep Multimodal Fusion for Surgical Feedback Classification](https://arxiv.org/abs/2312.03231)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a high-quality paragraph summarizing the key points of the paper:

This paper explores automated classification of real-world live surgical feedback using multi-modal inputs of text, audio, and video. The feedback is classified into 5 clinically validated categories adapted from Wong et al. (2023). The authors systematically investigate different deep learning model architectures for fusing the modalities, such as voting fusion, ensemble fusion, and feature fusion. More importantly, they also explore different training strategies, including individual modality training, joint training, and staged training. They find that a staged training approach, where modalities are first trained separately and then fine-tuned jointly, works best, outperforming other strategies. This highlights the importance of training procedures for effective fusion. With staged training and ensemble fusion, they achieve AUC scores between 71.5-77.6 for the 5 feedback types. The use of manual transcriptions further improves performance. Overall, this pioneering work demonstrates feasibility of automating multi-modal classification of live surgical feedback using deep learning, opening up possibilities for large-scale analysis to improve surgical training.
