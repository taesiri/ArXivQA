# [FLIQS: One-Shot Mixed-Precision Floating-Point and Integer Quantization   Search](https://arxiv.org/abs/2308.03290)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper abstract, it seems the central research question is how to automate mixed-precision floating-point and integer quantization to optimize deep neural network models for high accuracy and low cost/complexity on diverse hardware platforms. Specifically, the authors propose a new method called FLIQS (Floating-point and Integer Quantization Search) to automatically search for optimal per-layer numerical formats (float vs integer, and precision) to minimize model size and computational requirements while maximizing accuracy. This allows models to be efficiently deployed on hardware with varying numerical capabilities. The key ideas/contributions seem to be:- FLIQS is the first one-shot quantization search method that avoids separate retraining trials for each configuration. This reduces overall search time and cost.- It uses reinforcement learning for the quantization search, avoiding the high memory costs of methods that branch model weights for each option.- FLIQS introduces a new cosine entropy regularization schedule that diminishes the search noise during training, enabling deployment without retraining.- Results show FLIQS outperforms prior quantization search methods on accuracy and efficiency for convolutional and transformer networks using both integer and floating point formats.- Largest comparison to date of mixed-precision integer vs floating point models.- First automated search for mixed-precision floating point quantization.- Extends method to jointly optimize quantization and neural architecture search.In summary, the main research question is how to efficiently automate the search for optimal mixed numerical formats to maximize accuracy and hardware efficiency. FLIQS introduces a new one-shot quantization search method to address this.


## What is the main contribution of this paper?

Based on the abstract, the main contributions of this paper appear to be:1. Introducing FLIQS, the first one-shot quantization and neural architecture search framework without retraining for searching mixed-precision integer and floating point formats.2. Demonstrating state-of-the-art results for integer and low-precision floating-point quantization search across convolutional and transformer networks. 3. Performing the largest comparison of integer and floating-point mixed-precision networks.4. Conducting the first study of quantization and neural architecture search on low-precision floating-point networks and establishing recommendations for allocating compute across bitwidth and neural architectural dimensions.The key ideas seem to be using reinforcement learning for a one-shot quantization search during training to find optimal mixed precision configurations without needing to retrain models. This is done for both integer and floating point formats. The method is evaluated on convolutional and transformer models and also extended to quantization-aware neural architecture search.


## How does this paper compare to other research in the same field?

Here are a few key ways this paper compares to other research in the field of quantization and neural architecture search:- It proposes FLIQS, a novel one-shot mixed-precision quantization search method that avoids the need for retraining after the search. This differs from many prior quantization search methods that require additional retraining steps after the search is completed.- It explores both integer and floating point quantization spaces, including conducting what seems to be the first large-scale mixed-precision floating point quantization search. Most prior work has focused solely on integer quantization. - It demonstrates state-of-the-art quantization results across several CNNs and vision transformer models, outperforming recent works like HAWQ-V3 for integer search and FP8 methods for floating point.- The work introduces a cosine entropy regularization schedule to reduce the quantization search interference with model training. This allows higher accuracy without retraining.- It proposes joint quantization and architecture search, showing benefits compared to separate NAS and quantization optimization. Prior works like APQ have also looked at joint QNAS but use different search methods.- The study provides useful comparisons and insights between floating point vs integer quantization, which have mostly been explored separately before. It also examines compute allocation tradeoffs between bitwidth vs architectural dimensions.- The one-shot RL search method is lower complexity and memory than methods involving super-networks or differentiable NAS. But it may require more tuning to stabilize training compared to differentiable approaches.Overall, this paper pushes forward the state-of-the-art in automated mixed-precision quantization search and provides useful analysis to guide joint QNAS and hardware-software co-design. The proposed techniques seem promising for optimizing models for diverse hardware backends.
