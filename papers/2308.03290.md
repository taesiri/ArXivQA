# [FLIQS: One-Shot Mixed-Precision Floating-Point and Integer Quantization   Search](https://arxiv.org/abs/2308.03290)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper abstract, it seems the central research question is how to automate mixed-precision floating-point and integer quantization to optimize deep neural network models for high accuracy and low cost/complexity on diverse hardware platforms. Specifically, the authors propose a new method called FLIQS (Floating-point and Integer Quantization Search) to automatically search for optimal per-layer numerical formats (float vs integer, and precision) to minimize model size and computational requirements while maximizing accuracy. This allows models to be efficiently deployed on hardware with varying numerical capabilities. The key ideas/contributions seem to be:- FLIQS is the first one-shot quantization search method that avoids separate retraining trials for each configuration. This reduces overall search time and cost.- It uses reinforcement learning for the quantization search, avoiding the high memory costs of methods that branch model weights for each option.- FLIQS introduces a new cosine entropy regularization schedule that diminishes the search noise during training, enabling deployment without retraining.- Results show FLIQS outperforms prior quantization search methods on accuracy and efficiency for convolutional and transformer networks using both integer and floating point formats.- Largest comparison to date of mixed-precision integer vs floating point models.- First automated search for mixed-precision floating point quantization.- Extends method to jointly optimize quantization and neural architecture search.In summary, the main research question is how to efficiently automate the search for optimal mixed numerical formats to maximize accuracy and hardware efficiency. FLIQS introduces a new one-shot quantization search method to address this.
