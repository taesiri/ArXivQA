# [[CLS] Token is All You Need for Zero-Shot Semantic Segmentation](https://arxiv.org/abs/2304.06212)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is:How can we leverage pre-trained vision-language models like CLIP to perform zero-shot semantic segmentation, without requiring any annotations for the unseen classes?The key hypotheses are:1) The global text tokens ([CLS] tokens) from CLIP's text encoder provide a strong semantic representation of category information.2) Replacing the [CLS] tokens in the visual encoder with these text [CLS] tokens can guide the model to focus on relevant image regions for a given category. 3) This "one-way [CLS] token navigation" from text to visual encoder allows transferring CLIP's zero-shot classification abilities to the dense prediction task of semantic segmentation.4) Further localizing objects with region proposals before segmentation can help focus on tiny objects and improve performance.The proposed methods ClsCLIP and ClsCLIP+ aim to test these hypotheses for zero-shot semantic segmentation using CLIP. The key novelty is the one-way navigation of global text tokens to guide the visual encoder, extending CLIP's abilities beyond classification.
