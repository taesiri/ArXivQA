# [[CLS] Token is All You Need for Zero-Shot Semantic Segmentation](https://arxiv.org/abs/2304.06212)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is:How can we leverage pre-trained vision-language models like CLIP to perform zero-shot semantic segmentation, without requiring any annotations for the unseen classes?The key hypotheses are:1) The global text tokens ([CLS] tokens) from CLIP's text encoder provide a strong semantic representation of category information.2) Replacing the [CLS] tokens in the visual encoder with these text [CLS] tokens can guide the model to focus on relevant image regions for a given category. 3) This "one-way [CLS] token navigation" from text to visual encoder allows transferring CLIP's zero-shot classification abilities to the dense prediction task of semantic segmentation.4) Further localizing objects with region proposals before segmentation can help focus on tiny objects and improve performance.The proposed methods ClsCLIP and ClsCLIP+ aim to test these hypotheses for zero-shot semantic segmentation using CLIP. The key novelty is the one-way navigation of global text tokens to guide the visual encoder, extending CLIP's abilities beyond classification.


## What is the main contribution of this paper?

This paper proposes a new method for zero-shot semantic segmentation based on the CLIP model. The main contributions are:1. They propose a simple yet effective approach called ClsCLIP that extends CLIP from image-level classification to pixel-level segmentation. The key idea is to replace the [CLS] tokens in the shallow layers of the CLIP visual encoder with the [CLS] tokens from the text encoder, which provides semantic guidance. 2. They show that the text [CLS] token in CLIP encodes strong semantic information about the category, and replacing visual [CLS] tokens with text [CLS] tokens allows guiding the visual encoder to focus on relevant image regions.3. They further improve ClsCLIP by incorporating region proposals to deal with small objects. This enhanced model ClsCLIP+ first generates region proposals and then applies ClsCLIP to each proposal for segmentation. 4. Experiments on PASCAL-5i and COCO-20i benchmarks demonstrate state-of-the-art results for ClsCLIP and ClsCLIP+ under the zero-shot setting. ClsCLIP+ even outperforms previous 1-shot methods, showing the effectiveness of their approach.In summary, the key novelty is using CLIP's text [CLS] tokens to guide the visual encoder for zero-shot semantic segmentation, achieving strong performance in a simple and elegant way. The region proposal enhancement also effectively handles small objects.
