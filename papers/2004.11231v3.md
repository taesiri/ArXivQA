# [Federated Stochastic Gradient Langevin Dynamics](https://arxiv.org/abs/2004.11231v3)

## What is the central research question or hypothesis that this paper addresses?

 This paper proposes a new method called federated stochastic gradient Langevin dynamics (FSGLD) for Bayesian posterior sampling in distributed environments with non-IID data. The key ideas and contributions are:

- It proposes using "conducive gradients", which are approximations of the log-likelihood contributions from each data shard, to reduce the variance of stochastic gradient estimates in distributed stochastic gradient MCMC algorithms like DSGLD. 

- It analytically shows that adding these conducive gradients results in an unbiased estimator of the full log-likelihood gradient, and provides convergence bounds for the proposed FSGLD algorithm.

- It demonstrates through experiments on metric learning and Bayesian neural networks that FSGLD outperforms DSGLD in non-IID federated settings, and converges correctly even with infrequent communication while DSGLD fails.

So in summary, the central hypothesis is that using these conducive gradients to incorporate approximate likelihood information from all shards can improve distributed/federated stochastic MCMC, especially for non-IID data. The paper provides theoretical analysis and empirical validation of this idea through the proposed FSGLD algorithm.


## What is the main contribution of this paper?

 This paper proposes a new method called federated stochastic gradient Langevin dynamics (FSGLD) for performing Bayesian inference on federated non-IID data. 

The key ideas and contributions are:

- They propose "conducive gradients", which are auxiliary gradients computed from tractable approximations of each client's likelihood. These help reduce the variance of stochastic gradients in the federated non-IID setting.

- They incorporate conducive gradients into distributed stochastic gradient Langevin dynamics (DSGLD) to obtain the novel FSGLD method. 

- FSGLD is tailored for federated learning and handles non-IID data much better than vanilla DSGLD. It allows delayed communication without the Markov chains diverging.

- They provide theoretical analysis bounding the convergence of both DSGLD and FSGLD. The bound for FSGLD provides insight into how to choose good likelihood approximations to minimize the bound.

- They demonstrate the benefits of FSGLD over DSGLD experimentally on problems like metric learning and Bayesian neural nets under federated non-IID data.

So in summary, the main contribution is proposing FSGLD, a variant of distributed stochastic gradient MCMC that leverages conducive gradients to enable Bayesian inference on federated non-IID data. Theoretical analysis and experiments back the advantages of this method.
