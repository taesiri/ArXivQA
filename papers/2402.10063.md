# [Balancing the Causal Effects in Class-Incremental Learning](https://arxiv.org/abs/2402.10063)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Recent studies show contradictory results on the catastrophic forgetting of pretrained models (PTMs) in class-incremental learning (CIL). Some studies show PTMs are resistant to forgetting while others show they still suffer from substantial forgetting.
- This paper aims to analyze the reason behind this phenomenon and propose a solution to mitigate catastrophic forgetting for PTMs in CIL. 

Analysis:
- Conducted pilot study based on linear probing of PTMs on CIL tasks. Find that while PTMs retain knowledge, the overall model still forgets due to the trainable classifier on top.
- Tracked feature-embedding distance between PTMs and classifiers. Discovered that new classes align well between PTMs and classifiers while old classes get pushed away, indicating conflicting adaptation between new and old classes.  
- Framed CIL process into causal graphs. Revealed imbalanced causal effects - new data encourages adapting to new classes while hindering old classes and vice versa. This causal imbalance causes catastrophic forgetting.

Proposed Solution - BaCE:
- Balances the causal effects (BaCE) between new and old data when adapting model to new and old classes.
- Defines two objectives - Effect_new and Effect_old that build balanced causal paths from both new and old data to adaptation of new and old classes respectively.
- Effect_new relates a sample's prediction to its KNearestNeighbors' predictions based on feature distance in the teacher model.
- Effect_old enforces prediction consistency between teacher and student model on new data.
- Balancing the causal effects helps model learn from new and old data collaboratively, mitigating forgetting.

Contributions:
- Revealed that causal imbalance between new and old data leads to catastrophic forgetting in CIL.
- Proposed BaCE to balance the causal effects by building positive causal paths from both new and old data to adapt to each class. 
- Validated BaCE on continual image classification, text classification and NER tasks, outperforming existing CIL methods.


## Summarize the paper in one sentence.

 Here is a one-sentence summary of the paper:

The paper reveals that the catastrophic forgetting in class-incremental learning with pre-trained models is caused by imbalanced causal effects between adapting to new versus old classes, and proposes a method called Balancing Causal Effects (BaCE) to mitigate this issue by encouraging models to learn new and old data in a mutually beneficial way.


## What is the main contribution of this paper?

 The main contributions of this paper are threefold:

1. It reveals that pretrained models (PTMs) are resistant to forgetting, but the causal imbalance problem causes the forgetting of the whole model. Specifically, there are conflicting causal effects between adapting to new classes and old classes that lead to catastrophic forgetting.

2. It delves into the causalities in continual learning and discovers that the causal effects between new and old data are imbalanced under the experience replay setting. Only new or old data has causal effects on adapting to new or old classes.  

3. It proposes BaCE (Balancing the Causal Effects) to address the causal imbalance problem by balancing the causal effects between new and old data when adapting to each class. Extensive experiments show that BaCE outperforms a series of continual learning methods on different tasks and settings.

In summary, this paper makes significant contributions in revealing, analyzing and addressing the causal imbalance problem in continual learning with pretrained models through the lens of causal inference. The proposed BaCE method achieves new state-of-the-art performance by balancing the adaptation of new and old classes.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and concepts include:

- Class-Incremental Learning (CIL): The paper focuses on this challenging and practical scenario of incremental learning where the model needs to classify all classes seen so far without task indexes.

- Pre-Trained Models (PTMs): The paper investigates the causal relationship behind catastrophic forgetting in PTMs like BERT and ViT when applied to CIL.

- Causal Inference: The paper uses causal graphs and causal analysis to reveal the imbalanced causal effects between adapting to new and old classes in CIL. 

- Balancing Causal Effects (BaCE): The proposed method to mitigate the issue of causal imbalance by encouraging adaptation to new and old classes with balanced causal effects from both new and old data.

- Effect_new and Effect_old: The two objectives proposed in BaCE to build positive and balanced causal paths from both new and old data to the predictions of new and old classes.

- Conflicting Causal Effects: The paper reveals these effects that hinder the adaptation process between new and old classes in CIL from a causal perspective.

- Class Imbalance Problem: The well-known issue in CIL that BaCE connects to the discovered causal imbalance issue.

In summary, the key ideas revolve around using causal analysis to reveal and address the imbalance issues in adapting new and old knowledge in Class-Incremental Learning.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in this paper:

1. The paper proposes balancing the causal effects between new and old data in class-incremental learning. What is the intuition behind framing this as a causal inference problem? How does causal inference help provide insight into the forgetting phenomenon?

2. The paper introduces two metrics - positive causal effects and conflicting causal effects. Explain what these metrics represent and how they are defined in the context of adapting to new versus old classes. 

3. The central proposal of the paper is the BaCE objective function which contains two components - Effect_new and Effect_old. Walk through the mathematical formulations and intuitions behind each of these components. What role does each one play?

4. Effect_new involves selecting the K nearest neighbors in the feature space of the teacher model. Explain the rationale behind using the KNN mechanism here. How does it capture the causal effect of old data when learning new classes?

5. The paper visualizes some examples of the KNNs chosen by Effect_new. Analyze some of these examples - do the neighbors seem reasonable? Could the choice of neighbors be further improved?

6. Effect_old uses KL divergence between teacher and student model predictions. Explain why this particular objective is reasonable for encoding the causal effect of new data when learning old classes.

7. The method outperforms a range of prior class-incremental learning techniques. What is lacking in other methods that the proposed Balancing Causal Effects approach is able to address?

8. The paper also combines the method with bias correction techniques. Why would reducing the prediction bias towards new classes be helpful? How does the confusion matrix change after bias correction?

9. One limitation mentioned is longer training time compared to experience replay. Suggest methods to potentially reduce the training time overhead of the BaCE approach.

10. The core ideas of balancing causal effects could apply to incremental learning scenarios beyond class-incremental learning. Propose other incremental learning settings where a causal perspective may be beneficial.
