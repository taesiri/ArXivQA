# [CodeTalker: Speech-Driven 3D Facial Animation with Discrete Motion Prior](https://arxiv.org/abs/2301.02379)

## What is the central research question or hypothesis that this paper addresses?

 The central research question this paper addresses is how to generate high-quality 3D facial animations from speech using discrete motion priors. The key hypotheses are:

1. Modeling the facial motion space with discrete motion primitives can reduce the ambiguity and uncertainty in mapping speech to facial motions. 

2. Learning a generic and reusable codebook of facial motion primitives allows capturing realistic and subtle motions.

3. Formulating speech-driven facial animation as a code query task in this discrete proxy space can promote synthesis quality and vividness against cross-modal uncertainty.

4. A temporal autoregressive model over the discrete motion space with speech as input can produce accurate lip sync as well as plausible facial expressions.

The main goal is to develop a speech-driven facial animation model that overcomes limitations of previous approaches, such as over-smoothing and lack of vividness, by utilizing a learned discrete motion prior. The method, called CodeTalker, aims to achieve superior performance in terms of realistic motions, accurate lip sync, and natural facial expressions. The code query formulation and discrete motion space are the key innovations proposed to address the ambiguity in mapping speech to motions.
