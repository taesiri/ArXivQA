# [Rethinking Video ViTs: Sparse Video Tubes for Joint Image and Video   Learning](https://arxiv.org/abs/2212.03229)

## What is the central research question or hypothesis that this paper addresses?

 This paper proposes a new method called TubeViT that aims to enable Vision Transformer (ViT) models to efficiently handle both image and video inputs. The key ideas are:

- Using sparse video tubes to generate tokens from videos instead of dense sampling. This reduces the number of tokens and computational cost compared to prior video ViT methods. 

- Sharing weights between the image and video pathways, rather than having separate specialized networks. This allows joint training and transfer learning between images and videos.

- Using fixed positional embeddings based on tube coordinates rather than learned embeddings. This better represents the global spatio-temporal location of the sparse tubes.

- Scaling up models by adapting pretrained image ViTs using the sparse tube approach, avoiding expensive full finetuning.

The main hypothesis is that these techniques will allow ViT models to work well on both images and videos in an efficient and unified manner, outperforming prior specialized video ViT models. The experiments aim to validate this hypothesis across several video datasets.
