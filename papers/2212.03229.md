# [Rethinking Video ViTs: Sparse Video Tubes for Joint Image and Video   Learning](https://arxiv.org/abs/2212.03229)

## What is the central research question or hypothesis that this paper addresses?

 This paper proposes a new method called TubeViT that aims to enable Vision Transformer (ViT) models to efficiently handle both image and video inputs. The key ideas are:

- Using sparse video tubes to generate tokens from videos instead of dense sampling. This reduces the number of tokens and computational cost compared to prior video ViT methods. 

- Sharing weights between the image and video pathways, rather than having separate specialized networks. This allows joint training and transfer learning between images and videos.

- Using fixed positional embeddings based on tube coordinates rather than learned embeddings. This better represents the global spatio-temporal location of the sparse tubes.

- Scaling up models by adapting pretrained image ViTs using the sparse tube approach, avoiding expensive full finetuning.

The main hypothesis is that these techniques will allow ViT models to work well on both images and videos in an efficient and unified manner, outperforming prior specialized video ViT models. The experiments aim to validate this hypothesis across several video datasets.


## What is the main contribution of this paper?

 The main contribution of this paper is proposing a simple and effective method called TubeViT to enable standard Vision Transformer (ViT) models to work seamlessly with both image and video inputs. The key ideas are:

- Introducing Sparse Video Tubes, obtained by sparsely sampling the input video with 3D convolutional kernels of different shapes and strides. This allows creating a small set of tokens to represent the video, keeping the overall number of tokens low. 

- The sparse sampling means the model can easily handle either images or videos as input using the same parameters and backbone network.

- The sparse video tubes capture both spatial and temporal information from videos, while keeping compute costs manageable.

- The model achieves state-of-the-art results on multiple video classification benchmarks including Kinetics, Something-Something and Charades, outperforming previous specialized video architectures.

- The approach enables easy joint training on image and video datasets, which improves performance on both compared to training them separately. 

- It provides an efficient way to scale up ViT models for video by adapting large image-pretrained models using the sparse tubes, without needing full fine-tuning.

In summary, the key contribution is proposing Sparse Video Tubes to enable standard ViTs to work seamlessly and efficiently on both images and videos, outperforming prior specialized video models while being simple and easy to implement. The sparse sampling is critical to efficiently handle videos with transformers while allowing joint training on multiple modalities.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the key points from the paper:

The paper proposes a simple and efficient approach called TubeViT that enables Vision Transformer (ViT) models to handle both image and video inputs by sparsely sampling the video using multi-scale spatio-temporal tubes, achieving state-of-the-art performance on video classification benchmarks while seamlessly integrating image and video data.
