# [Rethinking Video ViTs: Sparse Video Tubes for Joint Image and Video   Learning](https://arxiv.org/abs/2212.03229)

## What is the central research question or hypothesis that this paper addresses?

 This paper proposes a new method called TubeViT that aims to enable Vision Transformer (ViT) models to efficiently handle both image and video inputs. The key ideas are:

- Using sparse video tubes to generate tokens from videos instead of dense sampling. This reduces the number of tokens and computational cost compared to prior video ViT methods. 

- Sharing weights between the image and video pathways, rather than having separate specialized networks. This allows joint training and transfer learning between images and videos.

- Using fixed positional embeddings based on tube coordinates rather than learned embeddings. This better represents the global spatio-temporal location of the sparse tubes.

- Scaling up models by adapting pretrained image ViTs using the sparse tube approach, avoiding expensive full finetuning.

The main hypothesis is that these techniques will allow ViT models to work well on both images and videos in an efficient and unified manner, outperforming prior specialized video ViT models. The experiments aim to validate this hypothesis across several video datasets.


## What is the main contribution of this paper?

 The main contribution of this paper is proposing a simple and effective method called TubeViT to enable standard Vision Transformer (ViT) models to work seamlessly with both image and video inputs. The key ideas are:

- Introducing Sparse Video Tubes, obtained by sparsely sampling the input video with 3D convolutional kernels of different shapes and strides. This allows creating a small set of tokens to represent the video, keeping the overall number of tokens low. 

- The sparse sampling means the model can easily handle either images or videos as input using the same parameters and backbone network.

- The sparse video tubes capture both spatial and temporal information from videos, while keeping compute costs manageable.

- The model achieves state-of-the-art results on multiple video classification benchmarks including Kinetics, Something-Something and Charades, outperforming previous specialized video architectures.

- The approach enables easy joint training on image and video datasets, which improves performance on both compared to training them separately. 

- It provides an efficient way to scale up ViT models for video by adapting large image-pretrained models using the sparse tubes, without needing full fine-tuning.

In summary, the key contribution is proposing Sparse Video Tubes to enable standard ViTs to work seamlessly and efficiently on both images and videos, outperforming prior specialized video models while being simple and easy to implement. The sparse sampling is critical to efficiently handle videos with transformers while allowing joint training on multiple modalities.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the key points from the paper:

The paper proposes a simple and efficient approach called TubeViT that enables Vision Transformer (ViT) models to handle both image and video inputs by sparsely sampling the video using multi-scale spatio-temporal tubes, achieving state-of-the-art performance on video classification benchmarks while seamlessly integrating image and video data.


## How does this paper compare to other research in the same field?

 Here are a few key ways this paper compares to other video understanding research:

- The paper proposes a new technique called "Sparse Video Tubes" to enable Vision Transformers (ViTs) to handle both image and video inputs. This contrasts with many previous works that have developed specialized video architectures like 3D CNNs or spatio-temporal transformers. The Sparse Video Tubes allow a standard ViT to work on videos without major modifications.

- The method achieves state-of-the-art results on major video datasets like Kinetics-400/600/700 and Something-Something V2. It outperforms prior works that use techniques like inflating 2D kernels to 3D, adding temporal specific layers, or using dense space-time sampling. This shows the Sparse Video Tubes are an efficient and effective approach. 

- The model is able to jointly leverage both image and video data seamlessly during training. This allows it to utilize diverse datasets and improves performance over models trained on either images or videos alone. Many prior works have separate image and video models or complex co-training procedures.

- The approach enables easy scaling up of models by adapting pretrained image ViTs with lightweight fine-tuning. This is more efficient than full video pretraining and matches or exceeds specialized video pretraining methods.

- The sparse sampling is shown to be important for performance and efficiency. Too many tokens degraded results, especially when training only on video data. This indicates current datasets may not support very long sequences and sparse sampling is a better strategy.

Overall, the paper shows competitive results can be achieved with a simple and unified architecture for both images and videos. The Sparse Video Tubes seem to be an effective technique for adapting vision transformers to video in an efficient and scalable manner.
