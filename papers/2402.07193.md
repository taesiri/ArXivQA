# [The Implicit Bias of Gradient Noise: A Symmetry Perspective](https://arxiv.org/abs/2402.07193)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem Statement
- The paper studies the dynamics of stochastic gradient descent (SGD) compared to gradient descent (GD) when there is continuous symmetry in the loss function. 
- It is known that SGD often converges to better solutions than GD in training neural networks, even when the noise level is small. This indicates the gradient noise can substantially bias the dynamics. 
- The goal is to characterize how the symmetry in the loss landscape affects the dynamics of SGD compared to GD, in order to explain why SGD generalizes better.

Main Contributions
- The paper defines a broad class of symmetries called "exponential symmetries", which includes common symmetries like rescaling and scaling. 
- It is shown that for every exponential symmetry, SGD has a unique, attractive fixed point along the degenerate (symmetry) direction. This fixed point balances and aligns the gradient noises in different subspaces.
- The dynamics of SGD are decomposed into two timescales - the empirical risk minimization timescale and the timescale to reach equilibrium along the degenerate directions. 
- When symmetry exists in the loss, SGD explores the degenerate directions. When it approximately exists, SGD solutions still get systematically biased.
- The theory offers insights into techniques like minibatching, warmup, normalization and matrix factorization. It also explains phenomenons like progressive sharpening/flattening.

Proposed Solution  
- The paper derives a "Noether flow" equation to characterize how SGD moves along degenerate directions due to gradient noise.
- A fixed point theorem is proved, showing every exponential symmetry has a unique fixed point that balances gradient noise. 
- The interplay between symmetries and weight decay is also characterized.
- These results are applied to problems like normalization, matrix factorization and residual connections to highlight the bias of SGD.

In summary, the paper provides a theoretical framework, based on symmetry and conservation laws, to precisely characterize how SGD dynamics differ from GD due to implicit gradient noise bias. This offers insights into many practices in deep learning.
