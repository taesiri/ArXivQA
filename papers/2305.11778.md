# [Cross-Lingual Supervision improves Large Language Models Pre-training](https://arxiv.org/abs/2305.11778)

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper addresses is: Is cross-lingual supervised data beneficial when pre-training large language models? In particular, are there gains both on open and closed generation when using the in-context learning paradigm for evaluation?The paper investigates whether including parallel/aligned data during pre-training of large language models, in addition to unlabeled monolingual data, can improve the models' abilities for cross-lingual and monolingual tasks when evaluated using few-shot in-context learning. The authors are interested in evaluating both "closed generation" tasks performed in one language, and "open generation" tasks performed across two languages.So in summary, the key research question is whether adding cross-lingual supervised data during pre-training improves language models' few-shot in-context learning performance on both monolingual and cross-lingual tasks.
