# [Cross-Lingual Supervision improves Large Language Models Pre-training](https://arxiv.org/abs/2305.11778)

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper addresses is: Is cross-lingual supervised data beneficial when pre-training large language models? In particular, are there gains both on open and closed generation when using the in-context learning paradigm for evaluation?The paper investigates whether including parallel/aligned data during pre-training of large language models, in addition to unlabeled monolingual data, can improve the models' abilities for cross-lingual and monolingual tasks when evaluated using few-shot in-context learning. The authors are interested in evaluating both "closed generation" tasks performed in one language, and "open generation" tasks performed across two languages.So in summary, the key research question is whether adding cross-lingual supervised data during pre-training improves language models' few-shot in-context learning performance on both monolingual and cross-lingual tasks.


## What is the main contribution of this paper?

The main contribution of this paper is demonstrating that including cross-lingual supervision during pre-training of large language models improves their ability to perform tasks like question answering and machine translation in a few-shot setting. Specifically, the key findings are:- Including some parallel data during pre-training improves performance on question answering and machine translation when evaluated using in-context learning, compared to models pre-trained on monolingual data only. This holds for both closed-generation tasks in a single language and open-generation tasks across languages.- Using automated curriculum learning to schedule the two pre-training objectives (language modeling and machine translation) works better than fixed mixing ratios and does not require multiple training runs.- The learned dynamic mixing ratio outperforms heuristics like gradually increasing the amount of parallel data.- Gains are shown on multiple model sizes (1.2B and 3.8B parameters) and across various languages.So in summary, the key contribution demonstrated here is that cross-lingual supervision is beneficial during pre-training of large multi-task language models, and automated curriculum learning is an effective way to balance the two objectives. The gains are shown on question answering, machine translation and summarization across diverse languages.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper demonstrates that including cross-lingual supervision by adding parallel data during pre-training of large encoder-decoder language models improves their ability to perform question answering, summarization, and machine translation through in-context learning across languages, and using automated curriculum learning to determine the data mixing ratio outperforms fixed ratio baselines.
