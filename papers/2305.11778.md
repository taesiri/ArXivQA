# [Cross-Lingual Supervision improves Large Language Models Pre-training](https://arxiv.org/abs/2305.11778)

## What is the central research question or hypothesis that this paper addresses?

 The central research question this paper addresses is: 

Is cross-lingual supervised data beneficial when pre-training large language models? In particular, are there gains both on open and closed generation when using the in-context learning paradigm for evaluation?

The paper investigates whether including parallel/aligned data during pre-training of large language models, in addition to unlabeled monolingual data, can improve the models' abilities for cross-lingual and monolingual tasks when evaluated using few-shot in-context learning. The authors are interested in evaluating both "closed generation" tasks performed in one language, and "open generation" tasks performed across two languages.

So in summary, the key research question is whether adding cross-lingual supervised data during pre-training improves language models' few-shot in-context learning performance on both monolingual and cross-lingual tasks.


## What is the main contribution of this paper?

 The main contribution of this paper is demonstrating that including cross-lingual supervision during pre-training of large language models improves their ability to perform tasks like question answering and machine translation in a few-shot setting. 

Specifically, the key findings are:

- Including some parallel data during pre-training improves performance on question answering and machine translation when evaluated using in-context learning, compared to models pre-trained on monolingual data only. This holds for both closed-generation tasks in a single language and open-generation tasks across languages.

- Using automated curriculum learning to schedule the two pre-training objectives (language modeling and machine translation) works better than fixed mixing ratios and does not require multiple training runs.

- The learned dynamic mixing ratio outperforms heuristics like gradually increasing the amount of parallel data.

- Gains are shown on multiple model sizes (1.2B and 3.8B parameters) and across various languages.

So in summary, the key contribution demonstrated here is that cross-lingual supervision is beneficial during pre-training of large multi-task language models, and automated curriculum learning is an effective way to balance the two objectives. The gains are shown on question answering, machine translation and summarization across diverse languages.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the key points from the paper:

The paper demonstrates that including cross-lingual supervision by adding parallel data during pre-training of large encoder-decoder language models improves their ability to perform question answering, summarization, and machine translation through in-context learning across languages, and using automated curriculum learning to determine the data mixing ratio outperforms fixed ratio baselines.


## How does this paper compare to other research in the same field?

 Here is a summary of how this paper compares to other related work in training large language models using cross-lingual supervision:

- The key contribution of this paper is showing that including parallel data during pre-training improves performance of large encoder-decoder language models when evaluated using few-shot in-context learning across multiple tasks. 

- Prior work like PARADISE (Reid & Artetxe, 2022) and mT6 (Chi et al., 2021) also explored using parallel data in pre-training, but evaluated performance after full fine-tuning rather than few-shot in-context learning.

- NMT5 (Kale et al., 2021) used parallel data in an intermediate training phase between pre-training and fine-tuning, again evaluating after full task-specific fine-tuning. 

- This paper differs in directly evaluating few-shot in-context learning abilities after pre-training with parallel data, without any full fine-tuning.

- It also introduces a dynamic curriculum learning method to automatically balance the self-supervised LM objective with the parallel MT objective during pre-training.

- The results demonstrate gains on question answering, summarization, and translation compared to pre-training on LM data alone, especially for non-English languages.

- Compared to other methods that use parallel data during pre-training or intermediate training, this paper provides a simpler and more direct way to incorporate cross-lingual supervision that improves in-context few-shot learning.

In summary, the key novelty is the focus on few-shot in-context evaluation after pre-training, the dynamic curriculum learning for mixing LM and MT data, and demonstrations of gains, especially for cross-lingual abilities, from incorporating parallel data during pre-training in this manner.


## What future research directions do the authors suggest?

 Based on my reading of the paper, here are some of the main future research directions suggested by the authors:

- Train and evaluate larger models (beyond 8B parameters) to better quantify the gains from adding parallel data during pre-training. The results on summarization indicate that larger models may benefit more from the added parallel data.

- Experiment with more sophisticated automated curriculum learning techniques for scheduling the mixing between language modeling and machine translation objectives during pre-training. The simple bandit-based approaches explored already outperformed static mixing ratios, but more advanced methods may further improve results.

- Extend the experiments to decoder-only model architectures like GPT in addition to the encoder-decoder models studied in the paper. The findings may generalize to decoder-only models as well.

- Evaluate on a broader range of downstream tasks beyond question answering, summarization and translation. The benefits of added parallel data during pre-training may extend to other tasks.

- Analyze the learned representations to understand in more detail how the added parallel data changes the representations compared to pre-training on language modeling alone.

- Explore whether gains can be achieved by adding parallel data sequentially during pre-training, rather than all at once.

- Consider combining the parallel data techniques with other methods like multi-task learning to pre-train on a diverse set of objectives.

In summary, the main directions are scaling up experiments, exploring more sophisticated curriculum learning, extending to other architectures and tasks, and analyzing learned representations and training dynamics in more detail. But the results so far indicate that added parallel data during pre-training is a promising research direction.


## Summarize the paper in one paragraph.

 The paper demonstrates that including cross-lingual supervised data during the pre-training of large language models improves performance when evaluating the models with few-shot learning. Specifically, the authors train Encoder-Decoder models from scratch with a mixture of a self-supervised language modeling objective and a supervised machine translation objective. They show gains on question answering, machine translation, and summarization tasks when evaluating the models with one-shot in-context learning, compared to models trained on the language modeling objective alone. A key contribution is using automated curriculum learning to learn the optimal mixing proportion between the two objectives during training, avoiding an expensive hyperparameter search. The learned curriculum outperforms static mixing policies. Overall, the work indicates that cross-lingual supervision is still beneficial when pre-training large models, improving abilities for both monolingual and cross-lingual generation when using in-context learning.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the paper:

This paper explores whether using cross-lingual supervised data during pre-training of large language models (LLMs) can improve their ability to perform tasks through in-context learning. The authors train Encoder-Decoder LLMs from scratch with a mixture of a language modeling objective and a machine translation (MT) objective on parallel corpora. They show that including some parallel data during pre-training improves performance on question answering, machine translation, and summarization when evaluated in a one-shot setting. The gains are especially pronounced for translation between English and low-resource languages. 

Since determining the optimal amount of parallel data to use is prohibitively expensive through grid search, the authors employ automated curriculum learning techniques to learn a good mixing ratio during training. Their proposed "FAIR" bandit algorithm outperforms static mixing ratios and a baseline EXP3 bandit. The learned dynamic scheduling uses less parallel data towards the end of training. Overall, this work demonstrates both the utility of cross-lingual supervision during LLM pre-training and the promise of automated curriculum methods for finding optimal training schedules.


## Summarize the main method used in the paper in one paragraph.

 This paper proposes a method to improve large language model (LLM) pre-training by incorporating cross-lingual supervised machine translation (MT) data alongside the self-supervised monolingual language modeling (LM) data typically used. The key findings are:

- Adding a small amount of parallel MT data (10-50%) during pre-training improves performance on question answering, summarization, and machine translation compared to using only LM data. Gains are seen for both monolingual and cross-lingual settings.

- Determining the optimal mix of LM vs MT data is challenging due to the high compute cost of pre-training. The authors propose using automated curriculum learning with a multi-armed bandit algorithm to learn the mixing ratio dynamically during training. This outperforms fixed mixing ratios. 

- The curriculum method balances between an MT objective on parallel sentences and a standard LM objective. It uses the intrinsic rewards from each task, measured as the loss reduction on held-out data, to adjust the sampling probabilities over time.

- Results show this automated scheduling during pre-training is more effective than static mixing ratios. It allows optimizing the trade-off between LM and MT objectives without needing multiple training runs.


## What problem or question is the paper addressing?

 The paper is addressing the question of whether including cross-lingual supervised data during pre-training of large language models is beneficial for improving their abilities for multilingual natural language processing tasks. 

Specifically, the key questions the paper investigates are:

1. Is cross-lingual supervised data beneficial when pre-training large language models? Does it improve their abilities for both closed generation (single language) tasks and open generation (cross-lingual) tasks when evaluated using in-context learning?

2. When using a mix of unsupervised language modeling data and supervised parallel data for pre-training, what is the best way to determine the optimal mix ratio? Is it feasible to do a hyperparameter search, or can this ratio be learned automatically during training?

3. How do models pre-trained with a mix of unsupervised and supervised data compare to baselines like specialized machine translation models and other large language models pre-trained only on unsupervised data?

So in summary, the paper is exploring whether and how cross-lingual supervised data can enhance the multilingual abilities of large pre-trained language models, especially for few-shot in-context learning scenarios. It focuses on finding effective ways to mix supervised and unsupervised data for pre-training.
