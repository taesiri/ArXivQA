# [Cross-Lingual Supervision improves Large Language Models Pre-training](https://arxiv.org/abs/2305.11778)

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper addresses is: Is cross-lingual supervised data beneficial when pre-training large language models? In particular, are there gains both on open and closed generation when using the in-context learning paradigm for evaluation?The paper investigates whether including parallel/aligned data during pre-training of large language models, in addition to unlabeled monolingual data, can improve the models' abilities for cross-lingual and monolingual tasks when evaluated using few-shot in-context learning. The authors are interested in evaluating both "closed generation" tasks performed in one language, and "open generation" tasks performed across two languages.So in summary, the key research question is whether adding cross-lingual supervised data during pre-training improves language models' few-shot in-context learning performance on both monolingual and cross-lingual tasks.


## What is the main contribution of this paper?

The main contribution of this paper is demonstrating that including cross-lingual supervision during pre-training of large language models improves their ability to perform tasks like question answering and machine translation in a few-shot setting. Specifically, the key findings are:- Including some parallel data during pre-training improves performance on question answering and machine translation when evaluated using in-context learning, compared to models pre-trained on monolingual data only. This holds for both closed-generation tasks in a single language and open-generation tasks across languages.- Using automated curriculum learning to schedule the two pre-training objectives (language modeling and machine translation) works better than fixed mixing ratios and does not require multiple training runs.- The learned dynamic mixing ratio outperforms heuristics like gradually increasing the amount of parallel data.- Gains are shown on multiple model sizes (1.2B and 3.8B parameters) and across various languages.So in summary, the key contribution demonstrated here is that cross-lingual supervision is beneficial during pre-training of large multi-task language models, and automated curriculum learning is an effective way to balance the two objectives. The gains are shown on question answering, machine translation and summarization across diverse languages.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper demonstrates that including cross-lingual supervision by adding parallel data during pre-training of large encoder-decoder language models improves their ability to perform question answering, summarization, and machine translation through in-context learning across languages, and using automated curriculum learning to determine the data mixing ratio outperforms fixed ratio baselines.


## How does this paper compare to other research in the same field?

Here is a summary of how this paper compares to other related work in training large language models using cross-lingual supervision:- The key contribution of this paper is showing that including parallel data during pre-training improves performance of large encoder-decoder language models when evaluated using few-shot in-context learning across multiple tasks. - Prior work like PARADISE (Reid & Artetxe, 2022) and mT6 (Chi et al., 2021) also explored using parallel data in pre-training, but evaluated performance after full fine-tuning rather than few-shot in-context learning.- NMT5 (Kale et al., 2021) used parallel data in an intermediate training phase between pre-training and fine-tuning, again evaluating after full task-specific fine-tuning. - This paper differs in directly evaluating few-shot in-context learning abilities after pre-training with parallel data, without any full fine-tuning.- It also introduces a dynamic curriculum learning method to automatically balance the self-supervised LM objective with the parallel MT objective during pre-training.- The results demonstrate gains on question answering, summarization, and translation compared to pre-training on LM data alone, especially for non-English languages.- Compared to other methods that use parallel data during pre-training or intermediate training, this paper provides a simpler and more direct way to incorporate cross-lingual supervision that improves in-context few-shot learning.In summary, the key novelty is the focus on few-shot in-context evaluation after pre-training, the dynamic curriculum learning for mixing LM and MT data, and demonstrations of gains, especially for cross-lingual abilities, from incorporating parallel data during pre-training in this manner.


## What future research directions do the authors suggest?

Based on my reading of the paper, here are some of the main future research directions suggested by the authors:- Train and evaluate larger models (beyond 8B parameters) to better quantify the gains from adding parallel data during pre-training. The results on summarization indicate that larger models may benefit more from the added parallel data.- Experiment with more sophisticated automated curriculum learning techniques for scheduling the mixing between language modeling and machine translation objectives during pre-training. The simple bandit-based approaches explored already outperformed static mixing ratios, but more advanced methods may further improve results.- Extend the experiments to decoder-only model architectures like GPT in addition to the encoder-decoder models studied in the paper. The findings may generalize to decoder-only models as well.- Evaluate on a broader range of downstream tasks beyond question answering, summarization and translation. The benefits of added parallel data during pre-training may extend to other tasks.- Analyze the learned representations to understand in more detail how the added parallel data changes the representations compared to pre-training on language modeling alone.- Explore whether gains can be achieved by adding parallel data sequentially during pre-training, rather than all at once.- Consider combining the parallel data techniques with other methods like multi-task learning to pre-train on a diverse set of objectives.In summary, the main directions are scaling up experiments, exploring more sophisticated curriculum learning, extending to other architectures and tasks, and analyzing learned representations and training dynamics in more detail. But the results so far indicate that added parallel data during pre-training is a promising research direction.


## Summarize the paper in one paragraph.

The paper demonstrates that including cross-lingual supervised data during the pre-training of large language models improves performance when evaluating the models with few-shot learning. Specifically, the authors train Encoder-Decoder models from scratch with a mixture of a self-supervised language modeling objective and a supervised machine translation objective. They show gains on question answering, machine translation, and summarization tasks when evaluating the models with one-shot in-context learning, compared to models trained on the language modeling objective alone. A key contribution is using automated curriculum learning to learn the optimal mixing proportion between the two objectives during training, avoiding an expensive hyperparameter search. The learned curriculum outperforms static mixing policies. Overall, the work indicates that cross-lingual supervision is still beneficial when pre-training large models, improving abilities for both monolingual and cross-lingual generation when using in-context learning.
