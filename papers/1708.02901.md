# [Transitive Invariance for Self-supervised Visual Representation Learning](https://arxiv.org/abs/1708.02901)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central research question this paper tries to address is: How can we learn visual representations with both inter-instance invariance (different instances should have similar features) and intra-instance invariance (to variations like viewpoint, pose, etc. of the same instance) in a completely self-supervised manner without any human annotations?The key ideas and contributions are:- Proposes a method to construct a graph with two types of edges - inter-instance edges between different instances and intra-instance edges between different views of the same instance. - Apply transitive relations on this graph to obtain richer image pairs exhibiting complex invariance. - Train a siamese network with a ranking loss using image pairs from the graph to learninvariant representations.- Achieve 63.2% mAP on PASCAL VOC 07 detection using Fast R-CNN, competitive with 67.3% using ImageNet pretraining.- Achieve 23.5% AP on COCO using Faster R-CNN, closest reported accuracy compared to 24.4% with ImageNet pretraining.- Outperform ImageNet pretraining on surface normal estimation task on NYUv2.So in summary, the key hypothesis is that combining two types of invariance via transitive relations can help learn rich self-supervised representations for various vision tasks. The results demonstrate the effectiveness of the proposed approach.


## What is the main contribution of this paper?

This paper proposes a method for self-supervised visual representation learning by exploiting different types of invariance and applying transitive relations on them. The main contributions are:- They construct a graph with two types of edges - "inter-instance" edges that connect different instances with similar appearance, and "intra-instance" edges that connect different views of the same instance. - They generate new invariant relationships between images by applying transitivity on this graph. For example, if image A and B have an inter-instance edge, and A and A' have an intra-instance edge, then by transitivity A' and B can be considered invariant.- They use these transitive invariant pairs to train a triplet siamese network to learn invariant representations. The network is trained on image pairs that are transitively invariant and dissimilar to a third distractor image.- Their method achieves 63.2% mAP on PASCAL VOC 2007 detection using Fast R-CNN, which is the closest performance to ImageNet supervised pre-training (67.3% mAP) using self-supervision.- They also report competitive results (23.5% AP) on the challenging COCO dataset using Faster R-CNN, again very close to ImageNet supervised pretraining (24.4% AP).So in summary, the key contribution is a way to generate richer invariant relationships from different self-supervised approaches via transitivity, and use it to learn visually invariant representations without manual supervision. The representations transfer well to object detection.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Unfortunately I cannot provide a full summary or TL;DR of the paper without reading it in its entirety. However, based on the abstract and introduction, it seems the main point is that the authors propose a method to learn visual representations that capture both inter-instance invariance (different objects of the same category should have similar features) and intra-instance invariance (the same object should have similar features under viewpoint, pose, deformation changes, etc). They do this by constructing a graph relating different object instances and viewpoints, and applying transitive relations on the graph to obtain richer invariance. The learned representations transfer well to object detection tasks.In one sentence, I would summarize it as: The authors propose a graph-based method to learn visual representations exhibiting both inter- and intra-instance invariance, which transfer well to object detection.
