# [Transitive Invariance for Self-supervised Visual Representation Learning](https://arxiv.org/abs/1708.02901)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central research question this paper tries to address is: How can we learn visual representations with both inter-instance invariance (different instances should have similar features) and intra-instance invariance (to variations like viewpoint, pose, etc. of the same instance) in a completely self-supervised manner without any human annotations?The key ideas and contributions are:- Proposes a method to construct a graph with two types of edges - inter-instance edges between different instances and intra-instance edges between different views of the same instance. - Apply transitive relations on this graph to obtain richer image pairs exhibiting complex invariance. - Train a siamese network with a ranking loss using image pairs from the graph to learninvariant representations.- Achieve 63.2% mAP on PASCAL VOC 07 detection using Fast R-CNN, competitive with 67.3% using ImageNet pretraining.- Achieve 23.5% AP on COCO using Faster R-CNN, closest reported accuracy compared to 24.4% with ImageNet pretraining.- Outperform ImageNet pretraining on surface normal estimation task on NYUv2.So in summary, the key hypothesis is that combining two types of invariance via transitive relations can help learn rich self-supervised representations for various vision tasks. The results demonstrate the effectiveness of the proposed approach.


## What is the main contribution of this paper?

This paper proposes a method for self-supervised visual representation learning by exploiting different types of invariance and applying transitive relations on them. The main contributions are:- They construct a graph with two types of edges - "inter-instance" edges that connect different instances with similar appearance, and "intra-instance" edges that connect different views of the same instance. - They generate new invariant relationships between images by applying transitivity on this graph. For example, if image A and B have an inter-instance edge, and A and A' have an intra-instance edge, then by transitivity A' and B can be considered invariant.- They use these transitive invariant pairs to train a triplet siamese network to learn invariant representations. The network is trained on image pairs that are transitively invariant and dissimilar to a third distractor image.- Their method achieves 63.2% mAP on PASCAL VOC 2007 detection using Fast R-CNN, which is the closest performance to ImageNet supervised pre-training (67.3% mAP) using self-supervision.- They also report competitive results (23.5% AP) on the challenging COCO dataset using Faster R-CNN, again very close to ImageNet supervised pretraining (24.4% AP).So in summary, the key contribution is a way to generate richer invariant relationships from different self-supervised approaches via transitivity, and use it to learn visually invariant representations without manual supervision. The representations transfer well to object detection.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Unfortunately I cannot provide a full summary or TL;DR of the paper without reading it in its entirety. However, based on the abstract and introduction, it seems the main point is that the authors propose a method to learn visual representations that capture both inter-instance invariance (different objects of the same category should have similar features) and intra-instance invariance (the same object should have similar features under viewpoint, pose, deformation changes, etc). They do this by constructing a graph relating different object instances and viewpoints, and applying transitive relations on the graph to obtain richer invariance. The learned representations transfer well to object detection tasks.In one sentence, I would summarize it as: The authors propose a graph-based method to learn visual representations exhibiting both inter- and intra-instance invariance, which transfer well to object detection.


## How does this paper compare to other research in the same field?

Here are a few key ways this paper compares to other research in self-supervised visual representation learning:- It proposes a novel method of combining intra-instance invariance (tracking objects in videos) and inter-instance invariance (context prediction on still images) to obtain richer invariance. Many prior works focus on just one of these sources. The combination is done not via multi-task learning but by reasoning about transitive relations on a graph.- The paper presents extensive experimental analysis and ablation studies to demonstrate the effectiveness of the proposed approach on PASCAL VOC object detection using both Fast R-CNN and Faster R-CNN frameworks. It achieves 63.2% and 65.0% mAP respectively on VOC 2007, outperforming prior self-supervised methods by a large margin and getting fairly close to supervised ImageNet pretraining.- This is the first work to report results with a self-supervised pre-trained model on the challenging COCO object detection dataset. The achieved 23.5% AP is very close (within 1%) to ImageNet pretraining, which is a significant result.- In addition to object detection, the paper also shows strong performance on surface normal estimation, significantly outperforming ImageNet pretraining. This suggests the learned features capture different invariances useful for different tasks. - Compared to concurrent self-supervised works, this paper presents a unique approach of combining multiple self-supervision losses via graph-based reasoning. The achieved performance onVOC and COCO object detection surpasses other methods from that time by a large gap.Overall, this paper makes significant contributions through its novel transitive approach, extensive analysis, strong detection results competitive with ImageNet pretraining, and generalization to other tasks like surface normal estimation. It demonstrates the importance of learning rich invariance in representations.
