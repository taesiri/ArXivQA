# [Transitive Invariance for Self-supervised Visual Representation Learning](https://arxiv.org/abs/1708.02901)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central research question this paper tries to address is: How can we learn visual representations with both inter-instance invariance (different instances should have similar features) and intra-instance invariance (to variations like viewpoint, pose, etc. of the same instance) in a completely self-supervised manner without any human annotations?The key ideas and contributions are:- Proposes a method to construct a graph with two types of edges - inter-instance edges between different instances and intra-instance edges between different views of the same instance. - Apply transitive relations on this graph to obtain richer image pairs exhibiting complex invariance. - Train a siamese network with a ranking loss using image pairs from the graph to learninvariant representations.- Achieve 63.2% mAP on PASCAL VOC 07 detection using Fast R-CNN, competitive with 67.3% using ImageNet pretraining.- Achieve 23.5% AP on COCO using Faster R-CNN, closest reported accuracy compared to 24.4% with ImageNet pretraining.- Outperform ImageNet pretraining on surface normal estimation task on NYUv2.So in summary, the key hypothesis is that combining two types of invariance via transitive relations can help learn rich self-supervised representations for various vision tasks. The results demonstrate the effectiveness of the proposed approach.
