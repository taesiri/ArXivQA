# [SCCA: Shifted Cross Chunk Attention for long contextual semantic   expansion](https://arxiv.org/abs/2312.07305)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a summary paragraph of the key points from the paper:

This paper proposes Shifted Cross Chunk Attention (SCCA) and Shifted Dilated Attention (SDA), two novel sparse attention patterns that enable extending the context length of large language models (LLMs) during efficient fine-tuning. SCCA shifts the key-value vectors in different heads to enable queries to attend outside their local window, accumulating results to approximate full attention. Two SCCA variants are presented: SCCA_fixed shifts half the heads, while SCCA_flow shifts each head differently to enable global coverage. SDA combines dilated attention and neighborhood attention, selecting tokens in a dilated pattern across the full context. Experiments using LLaMA-7B show SCCA variants combined with positional interpolation can extend the model to 8k context with lower perplexity than prior sparse attention methods. A LongMixed combination of SCCA and SDA achieves the best perplexity. The proposed attention patterns enable efficient LLM context extension as plug-and-play fine-tuning techniques, without changing model architectures. They accumulate local attention results to approximate global context, balancing efficiency and global modeling.
