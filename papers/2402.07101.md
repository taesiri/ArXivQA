# [On the Complexity of First-Order Methods in Stochastic Bilevel   Optimization](https://arxiv.org/abs/2402.07101)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- The paper studies the problem of finding stationary points in Bilevel optimization when the lower-level problem is unconstrained and strongly convex. 
- The key challenges are to keep track of the changing lower-level solutions $y^*(x)$ in response to changes in the upper-level variable $x$, and to estimate the gradients and Hessians needed to compute the hypergradient $\grad F(x)$.

Proposed Solution and Contributions:

1. The paper proposes the concept of a "$y^*$-aware" oracle, which returns an estimate $\hat{y}(x)$ of the lower-level solution $y^*(x)$ within an $O(\epsilon)$ ball, in addition to locally unbiased stochastic gradient estimators. 

2. With this oracle, the paper gives an algorithm that achieves an $\tilde{O}(\epsilon^{-6})$ rate to find an $\epsilon$-stationary point, without stochastic smoothness assumptions. This improves the best known $\tilde{O}(\epsilon^{-7})$ rate with standard oracles.  

3. With an additional stochastic smoothness assumption, the paper achieves an $\tilde{O}(\epsilon^{-4})$ rate, matching the best-known rate for methods using second-order oracles. This shows first-order methods can match second-order methods complexity.

4. The paper gives matching lower bounds of $\Omega(\epsilon^{-6})$ and $\Omega(\epsilon^{-4})$ for the two settings above, showing the algorithm and analysis are tight.

5. The upper bounds hold even for the standard oracle model (infinite radius $y^*$-awareness), thus improving existing upper bounds. The lower bounds apply for any algorithm able to simulate a $y^*$-aware oracle.

In summary, the key contribution is providing tight complexity bounds for first-order bilevel optimization in the cases with and without stochastic smoothness assumptions. The results provide insight on the limits of what is possible with first-order versus second-order information.
