# [Pre-training Vision Transformers with Very Limited Synthesized Images](https://arxiv.org/abs/2307.14710)

## What is the central research question or hypothesis that this paper addresses?

The central hypothesis of this paper is that the process of generating different instances for each category in previous formula-driven supervised learning (FDSL) datasets can be replaced with data augmentation techniques. Specifically, the authors propose that instead of having multiple synthesized images for each category, a single representative image per category along with data augmentation is sufficient for effective vision transformer (ViT) pre-training. To validate this, the authors create two minimal FDSL datasets called 2D-OFDB and 3D-OFDB with only 1 image per category. Despite having only 1,000 total images, 2D-OFDB and 3D-OFDB are able to match or exceed the performance of previous FDSL datasets with orders of magnitude more images on various downstream tasks.In summary, the central hypothesis is that data augmentation can replace explicit instance generation in FDSL, allowing effective ViT pre-training with only a single image per category. The authors validate this through the performance of their proposed minimal OFDB datasets.


## What is the main contribution of this paper?

The main contributions of this paper are:1. Proposing two synthetic image datasets for pre-training Vision Transformers (ViTs) - 2D-OFDB and 3D-OFDB. These consist of only 1 representative image per category, so for example 2D-OFDB-1k has just 1,000 total images. 2. Showing that pre-training ViT models on these tiny datasets with only 1 instance per class can match or exceed the performance of models pre-trained on much larger datasets like ImageNet-1k, FractalDB-1k, etc. For example, 2D-OFDB-1k gets 84.0% on CIFAR-100 compared to FractalDB-1k's 81.6% despite using 1,000 images vs 1 million.3. Proposing two data augmentation techniques specifically for fractal images - random patch augmentation and random texture augmentation. These boost the performance of models pre-trained on the proposed OFDB datasets.4. Demonstrating that OFDB-21k with just 21,000 images matches or exceeds ImageNet-21k with 14 million images on ImageNet-1k fine-tuning, while being 3.3x faster to pre-train.5. Showing that OFDB-1k outperforms state-of-the-art methods like IDMM for pre-training ViTs on small datasets, achieving 80.8% average accuracy on 7 datasets compared to IDMM's 77.4%.In summary, the key contribution is showing that ViTs can be effectively pre-trained on extremely small synthetic datasets with just 1 instance per class, if good data augmentation is used. This makes pre-training much more efficient.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence TL;DR summary of the paper:The paper proposes a very efficient way to pre-train vision transformers using only a few synthesized images per category, showing it can match the performance of models pre-trained on datasets with millions of images like ImageNet.
