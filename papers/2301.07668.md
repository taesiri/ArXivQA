# [Behind the Scenes: Density Fields for Single View Reconstruction](https://arxiv.org/abs/2301.07668)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading, the central research question this paper addresses is: 

How can we infer a meaningful 3D geometric representation of a scene from a single image, including geometry of areas occluded in the input image?

The authors note that traditional approaches like depth map prediction can only reason about visible areas. Neural radiance fields (NeRFs) can capture full 3D geometry including color, but require many images of a scene. 

Their key hypothesis is that by predicting an implicit density field and directly sampling color from input views (instead of also predicting it like NeRF), the complexity is reduced enough to enable predicting it from a single image through a novel self-supervised training approach.

The experiments then aim to evaluate whether their proposed method can indeed capture occluded geometry, as well as how it performs on depth prediction and novel view synthesis compared to other state-of-the-art approaches.
