# [All You Need is "Love": Evading Hate-speech Detection](https://arxiv.org/abs/1808.09115)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the main research questions and hypotheses addressed in this paper are:1. How do different state-of-the-art hate speech detection models compare in terms of performance when tested on the same datasets?The authors hypothesize that model architecture may not have a major impact on performance for hate speech detection. They test this by replicating 7 models from prior work and evaluating them on 4 datasets.2. How well do hate speech detection models transfer across different datasets? The authors hypothesize that hate speech indicators do not transfer well across datasets due to differences in text types and labeling criteria. They test this by applying pre-trained models to datasets they were not trained on.3. How well can current models distinguish between hate speech and offensive but non-hateful speech?The authors hypothesize that models may conflate the two, classifying offensive non-hate speech as hate speech. They test this by evaluating model performance on offensive non-hate examples. 4. How robust are current models to simple evasion attacks that modify input text?The authors hypothesize that models rely too much on surface-level features and may be brittle against such attacks. They test this through 6 types of modifications like typos, leetspeak, whitespace changes, and word appending.In summary, the main goals are to systematically compare current models, evaluate their transferability, test their ability to distinguish hate from offensive speech, and assess their robustness to evasion attacks. The overarching hypothesis is that current models are limited in these aspects.


## What is the main contribution of this paper?

Based on my reading, the main contributions of this paper are:1. The first experimental comparative analysis of state-of-the-art hate speech detection models and datasets from prior research literature. The authors replicated 7 models on 4 datasets and systematically tested their performance.2. Demonstration of several evasion attacks against all the models, categorized into - word changes, word boundary changes, and word appending. They showed that simple text modifications can significantly deteriorate the performance of most classifiers. 3. Presentation of a very simple but highly effective "love attack" that combines whitespace removal and appending "love" to the text. This attack completely broke all the word-based models and also impacted the character-based models.4. Analysis of the limitations of current hate speech detection methods - lack of transferability across datasets, conflation of hate speech and offensive speech leading to false positives, and susceptibility to evasion attacks. 5. Recommendations for future work - focus on datasets instead of models, use character-based models, detect hate speech as anomaly instead of classification, use data augmentation.In summary, the paper systematically analyzed the current state-of-the-art in hate speech detection through comparative experiments, demonstrated their weaknesses through attacks, and provided recommendations for improving robustness and effectiveness in future research.
