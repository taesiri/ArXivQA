# [Distilling Large Language Models for Text-Attributed Graph Learning](https://arxiv.org/abs/2402.12022)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Text-attributed graphs (TAGs) combine both text content and graph structure, allowing for powerful analysis. However, training graph models on TAGs requires large amounts of human-annotated node labels, which are expensive and sometimes unavailable.  
- Large language models (LLMs) like GPT-3 show promising capability for zero-shot and few-shot node classification on TAGs. But they have issues with scalability, cost, and privacy.
- Distilling LLMs into smaller graph models can mitigate the issues with LLMs. However, distilling language knowledge into graph models is challenging due to the differences in their inputs/outputs.

Proposed Solution:
- Propose a framework to distill LLM knowledge into a graph model by using an interpreter model.
- Leverage LLM's expressive power to provide rationales and pseudo-supervision to train the interpreter. Convert textual rationales into enhanced text, structure and message-level features.
- Propose a semantics and structure-aware model alignment method to transfer knowledge from the interpreter to a student model. Align text embeddings and graph structure embeddings between models.

Main Contributions:
- A new framework for distilling LLM knowledge to graph models for TAG learning using an interpreter model.
- Methods to convert LLM textual rationales into enhanced features to train the interpreter.
- A semantics and structure-aware model alignment technique to transfer interpreter's knowledge to a student model.
- Experiments showing the framework outperforms baselines on multiple TAG datasets, keeping high accuracy without needing the LLM at test time.
