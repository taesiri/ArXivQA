# Self-Refine: Iterative Refinement with Self-Feedback

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be how large language models like GPT-3 can be improved at test time by having the model provide iterative feedback and refinement on its own outputs, without requiring additional training data or reinforcement learning. The key hypothesis appears to be that by using the same underlying language model to generate an initial output, provide introspective feedback on that output, and then refine the output accordingly, the model can achieve better results on a diverse set of natural language generation tasks compared to just doing standard one-step generation.The paper introduces an approach called Self-Refine which implements this iterative self-feedback and refinement loop within a single language model, relying only on prompt engineering rather than extra training. The core hypothesis seems to be that this approach can consistently enhance the performance of language models like GPT-3 and GPT-4 across different tasks by allowing them to refine their own outputs.The experiments aim to test if Self-Refine leads to improved performance over direct one-step generation from the same base language models on tasks like dialogue generation, code optimization, math reasoning, and more. The key result is that across all the evaluated tasks, Self-Refine generates outputs that are preferred by humans and score higher on automatic metrics, showing around a 20% average absolute improvement versus direct generation.So in summary, the central research question appears to be whether iterative self-feedback and refinement within a single language model can improve its performance at test time, which the results generally confirm across diverse tasks. The paper presents this Self-Refine approach as a way to enhance large language models without needing extra training data.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contribution seems to be proposing an iterative self-refinement approach called Self-Refine that allows large language models (LLMs) like GPT-3 to improve their own outputs through a feedback and refinement loop, without requiring additional training data or reinforcement learning. Specifically, the key ideas presented are:- Self-Refine operates by first generating an initial output with an LLM. Then the same LLM provides feedback on its own output, and uses that feedback to refine/improve the output in an iterative loop.- The feedback and refinement steps use prompt engineering to guide the LLM, rather than training separate models.- Self-Refine is evaluated on a diverse set of 7 text generation tasks, including dialogue, code optimization, reasoning, etc.- Experiments across tasks and models like GPT-3, Codex and GPT-4 show Self-Refine can boost performance over standard one-step inference by 15-20% on average based on automated and human evaluations.- The simplicity of Self-Refine means it can plug into any existing LLM without modifications, while improving quality by leveraging the model's own feedback.In summary, the core contribution is presenting and evaluating an iterative self-refinement approach using prompt engineering that can enhance existing LLMs like GPT-3 without extra training. The gains across diverse tasks highlight the benefits of iterative refinement guided by the model's own feedback.
