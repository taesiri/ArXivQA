# [Learning General Policies for Classical Planning Domains: Getting Beyond   C$_2$](https://arxiv.org/abs/2403.11734)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Existing GNN-based approaches for learning general policies for planning domains are limited to the expressive power of $C_2$ logic (first-order logic with two variables and counting). This limits their ability to learn policies requiring more complex logical features. 

- Going to higher order GNNs like 3-GNNs can overcome this limitation but has impractical computational requirements (quartic message passing time, cubic space for embeddings).

Proposed Solution:
- Introduce a parameterized extended version of Relational GNNs (R-GNNs), called R-GNN[t], where t controls the number of recursive compositions of relations.

- R-GNN[t] takes a transformed set of atoms A_t(S) as input instead of just the atoms S. A_t(S) recursively composes relations up to t times using a new ternary predicate.

- For t>=1, R-GNN[t] can capture features requiring $C_3$ logic with only quadratic message passing and space complexity, approximating 3-GNNs practically.

- The architecture mirrors standard R-GNNs, with the transformation A_t(S) only affecting the inputs. So existing R-GNN code needs minimal changes.

Contributions:
- Introduce a practical way to enhance R-GNN expressive power beyond $C_2$ using the parameter t and automatic recursive composition of relations.

- Show experimentally that even small t values allow learning policies requiring $C_3$ features, with significantly improved coverage and plan quality compared to R-GNNs and Edge Transformers.

- Provide a detailed analysis attributing performance gains over baseline methods to the ability of R-GNN[t] to compute key logical features efficiently.

In summary, the paper makes R-GNNs more practically expressive using recursive relation composition, while keeping computational complexity quadratic. This lets them learn complex policies not possible with standard R-GNNs.


## Summarize the paper in one sentence.

 This paper introduces a parameterized extension of relational graph neural networks, R-GNN[t], that approximates 3-GNNs using quadratic space to achieve greater expressive power for learning general policies in classical planning domains.


## What is the main contribution of this paper?

 The main contribution of this paper is introducing a parameterized version of Relational Graph Neural Networks (R-GNNs), called R-GNN[t], that extends their expressive power beyond the limitations of $C_2$ logic while still being practical. Specifically:

- R-GNN[t] approximates 3-GNNs, which have the expressive power of $C_3$ logic, using only quadratic space for embeddings instead of cubic space. This makes R-GNN[t] more practical and scalable than 3-GNNs. 

- By controlling the parameter t, R-GNN[t] can balance expressive power with computational effort. Even small values of t (e.g. t=1 or t=2) are often sufficient to capture the key $C_3$ features needed in several planning domains, while avoiding the overhead of higher values of t or full 3-GNNs.

- R-GNN[t] achieves this extended expressivity by transforming the input planning states fed into a regular R-GNN architecture. So it leverages existing R-GNN machinery without alteration.

- Experiments demonstrate clear performance gains of R-GNN[t] over plain R-GNNs and edge transformers across planning domains requiring greater than $C_2$ expressivity. R-GNN[t] improves coverage and plan quality while maintaining performance on $C_2$ domains.

In summary, the key contribution is an practical and scalable method for extending the expressive power of relational GNNs beyond $C_2$ through a parameterized input transformation.


## What are the keywords or key terms associated with this paper?

 Based on my review of the paper, some of the key terms and concepts associated with it include:

- Relational Graph Neural Networks (R-GNNs)
- Expressive power of GNNs
- $C_2$ and $C_3$ logics
- Generalized planning
- Classical planning 
- k-GNNs
- Message passing
- Object embeddings
- Triplet embeddings  
- Extended R-GNN architecture (R-GNN[t])
- Composition atoms
- Parameterized message passing
- Approximating 3-GNNs
- Baselines like plain R-GNNs and Edge Transformers
- Experimental evaluation on planning domains
- Analysis of results on $C_2$ and $C_3$ domains

In a nutshell, the paper introduces a parameterized and extended version of Relational GNNs called R-GNN[t] that aims to increase their expressive power beyond the limitations of $C_2$ logic by approximating 3-GNNs in a practical and scalable manner. This is achieved by transforming the input atoms through composition atoms and controlled message passing based on the parameter t. The approach is evaluated on a range of planning domains and shown to improve performance on $C_3$ domains while maintaining competitiveness on $C_2$ domains.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in this paper:

1. The proposed R-GNN[t] method aims to extend the expressive power of Relational GNNs beyond C_2 while still maintaining efficiency. How exactly does the addition of the composition atoms in Î”_t(S) allow the method to capture higher-order logic features like those in C_3?

2. The parameter t controls the maximum number of sequential compositions captured in R-GNN[t]. What is the intuition behind incrementing t to get higher expressive power? What are the tradeoffs involved as t gets larger? 

3. How does the message passing mechanism in R-GNN[t] emulate 2-FWL for appropriate values of t? Explain the similarities and differences.

4. Explain why directly using 3-GNNs would not be as practical as the proposed R-GNN[t] method, even though they can capture C_3 logic. What are the specific advantages of R-GNN[t]?

5. The edge transformer baseline also aims to approximate 3-GNNs. What is the key difference in its triangulation mechanism compared to R-GNN[t]? Why does this difference explain its poorer performance?

6. Take one of the C3 domains like Grid or Logistics and analyze the types of features needed to solve it optimally. Then explain how R-GNN[t] is able to capture those features. 

7. For the Grid domain, even the best coverage achieved is 75%. Provide a detailed analysis explaining why optimal coverage was not achieved even though R-GNN[1] could capture the necessary C3 features.

8. Explain why directly storing distances in the embeddings for each robot is problematic in the Vacuum domain for the baseline. How does R-GNN[t] get around this issue?

9. The performance difference between the baseline and R-GNN[t] models is much larger for Visitall-xy compared to Visitall. Why is this the case? What property of Visitall-xy makes C3 features more critical?

10. What ideas do the authors propose for further improving performance in complex C3 domains like Grid and Rovers where optimal plans require longer reasoning? Critically analyze these ideas.
