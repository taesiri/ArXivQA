# [The AL$\ell_0$CORE Tensor Decomposition for Sparse Count Data](https://arxiv.org/abs/2403.06153)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Tensor decomposition methods like Canonical Polyadic (CP) and Tucker decomposition are used to find low-rank representations of multi-way data. 
- CP decomposition is simple and fast but lacks the representational power of Tucker decomposition. 
- Tucker decomposition can learn richer latent structures but suffers from an exponential blowup in computational complexity as the number of latent factors increases. This makes it infeasible for large tensors.

Proposed Solution: 
- The paper proposes a new decomposition called AL$\ell_0$CORE that combines the benefits of CP and Tucker. 
- It is a sparse Tucker decomposition where the core tensor has a budget constraint of at most Q non-zero entries.
- The locations and values of the non-zeros are treated as latent variables and inferred from data.
- This allows exploring an exponentially large latent space like Tucker but with low computational complexity that scales only with Q.

Key Contributions:
- Introduces the concept of an allocated $\ell_0$-constrained core tensor to restrict Tucker to be sparse but retain its representational power.
- Provides a general recipe for inference using sampling to iteratively explore locations of non-zeros in the core tensor.
- Gives a complete example called Bayesian Poisson AL$\ell_0$CORE tailored to sparse count tensors with a Poisson likelihood. 
- Empirically demonstrates on real-world dynamic network data that AL$\ell_0$CORE can match the performance of Tucker at a fraction of the cost and is better than CP decomposition.
- Shows AL$\ell_0$CORE can infer rich interpretable latent structures with many communities and their interactions which was previously infeasible.

In summary, the key idea is restricting the core tensor in Tucker decomposition to be sparse while keeping the latent dimensions high. This helps avoid the exponential blowup in complexity and makes large latent spaces tractable. The paper provides a general approach for this idea and shows its utility for modeling large sparse tensors.
