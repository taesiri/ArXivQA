# [Safe Latent Diffusion: Mitigating Inappropriate Degeneration in   Diffusion Models](https://arxiv.org/abs/2211.05105)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading, the central research question of this paper is: How can we mitigate inappropriate degeneration in diffusion models trained on large unfiltered datasets? 

The key points are:

- Diffusion models trained on large unfiltered datasets like LAION-5B can generate inappropriate and biased content. This is demonstrated through an analysis of ethnic bias.

- To systematically measure the risk of inappropriate degeneration, the authors introduce a new benchmark called I2P (Inappropriate Image Prompts) containing real-world text prompts that are likely to generate inappropriate images.

- They show that Stable Diffusion generates inappropriate content even when conditioned on I2P prompts that seem harmless. 

- To address this, they propose a method called Safe Latent Diffusion (SLD) to suppress inappropriate content during the diffusion process by manipulating the latent space.

- SLD does not require any additional training or classifiers. It relies only on the model's acquired knowledge and representations.

- Experiments on I2P and biased prompts show SLD can significantly reduce inappropriate generations while maintaining image quality and text alignment.

So in summary, the central research question is how to mitigate inappropriate content in diffusion models, which they address by proposing and evaluating SLD. The key hypothesis seems to be that SLD can leverage the model's own knowledge to safely guide image generation without extra supervision.
