# [Edge Private Graph Neural Networks with Singular Value Perturbation](https://arxiv.org/abs/2403.10995)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper "Edge Private Graph Neural Networks with Singular Value Perturbation":

Problem:
- Graph neural networks (GNNs) are vulnerable to privacy attacks where an adversary can query the GNN model to infer private information about the graph used to train the model, such as sensitive edges representing social or financial relationships.
- Existing defense methods based on differential privacy (DP) perturb the entire adjacency matrix or a compact representation, altering the graph structure substantially and reducing model utility/accuracy.

Proposed Solution: 
- The paper proposes Eclipse, a new DP-based training methodology for GNNs that leverages the low-rank structure of real-world graph adjacency matrices.
- Eclipse decomposes the adjacency matrix into low-rank and sparse components via singular value decomposition (SVD). Only the top few singular values capture most information about the graph.
- Eclipse trains the GNN on the low-rank approximation, which preserves primary graph topology while removing secondary edges, providing empirical privacy. 
- Eclipse adds DP noise only to the top singular values, protecting privacy with less perturbation compared to existing methods.
- Eclipse works as a plug-in to existing GNN architectures without modifying model architecture.

Main Contributions:
- Formal privacy analysis showing Eclipse guarantees edge-level DP. Noise is only added to low-rank singular values instead of the entire adjacency matrix.
- Extensive evaluation on multiple datasets demonstrating Eclipse achieves significantly better privacy-utility tradeoff than prior DP-based methods, especially under high privacy constraints. Eclipse improves GNN accuracy by up to 46% for a given privacy budget.
- Analysis of Eclipse's resilience to privacy attacks like LPA and LINKTELLER. Eclipse provides stronger protection compared to baselines, lowering attack AUC by up to 5% for the same model accuracy.
- Analysis and insights into Eclipse's protection on nodes with different degree distribution. Low-rank approximation in Eclipse causes more changes in degree for low-degree nodes, providing them stronger privacy.

In summary, the key innovation in Eclipse is exploiting low-rank structure to apply DP noise in lower dimensions for better utility, while still protecting edge privacy. Eclipse advances state-of-the-art in private GNN training.


## Summarize the paper in one sentence.

 This paper proposes a new privacy-preserving graph neural network training method called Eclipse that utilizes low-rank approximation and differential privacy on the singular values to protect sensitive graph edges while maintaining high model utility.


## What is the main contribution of this paper?

 Based on my understanding, the main contribution of this paper is proposing a new privacy-preserving graph neural network (GNN) training algorithm called Eclipse that provides strong edge-level differential privacy guarantees while maintaining good model utility. Specifically:

1) Eclipse perturbs the low-rank singular values obtained via SVD on the adjacency matrix rather than adding noise to the entire adjacency matrix. This reduces the dimension for differential privacy and preserves more useful graph structure.

2) Eclipse decorrelates the trained GNN from the residual edges discarded during low-rank approximation, providing empirical privacy protection. 

3) Eclipse achieves significantly better privacy-utility tradeoff compared to prior state-of-the-art methods like DPGCN and LPGNet, especially under high privacy constraints (small epsilon). For example, Eclipse improves model utility by up to 46% for epsilon<4.

4) Eclipse demonstrates better resilience against common edge attacks like LPA and LINKTELLER, lowering the attack AUC by up to 5% compared to other methods.

In summary, the main contribution is a novel differentially private GNN training approach that perturbs the low-rank singular values of the adjacency matrix to provide formal DP guarantees on edges while achieving much better model utility and attack resilience.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and concepts include:

- Graph neural networks (GNNs)
- Differential privacy (DP) 
- Edge privacy
- Low-rank graph approximation
- Singular value decomposition (SVD)
- Privacy attacks (e.g. LPA, LINKTELLER)
- Privacy-utility tradeoff
- Perturbation mechanisms
- Gaussian mechanism
- Model utility (accuracy, F1 score)

The paper proposes a new privacy-preserving GNN training methodology called Eclipse that utilizes low-rank graph approximation via SVD. It adds DP noise to the singular values to protect edge privacy while maintaining model utility. The method is evaluated on accuracy and resilience against common privacy attacks like LPA and LINKTELLER under different DP budgets. The key ideas revolve around exploiting low-rank structure to improve privacy-utility tradeoff compared to prior DP methods.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 potential in-depth questions about the method proposed in the paper:

1. The paper proposes using singular value decomposition (SVD) to decompose the adjacency matrix into a low-rank approximation before adding noise. Why is operating on the low-rank format more effective for preserving utility compared to adding noise directly to the original adjacency matrix?

2. When performing SVD, the paper assumes the principal bases (U and V matrices) stay nearly invariant when edges are added/removed but the singular values change. What are the implications on privacy if this assumption does not hold perfectly in a dataset? Can you quantify or bound the deviation?

3. The paper claims the low-rank reconstruction removes substantial edges, especially for low-degree nodes. Can you analytically compute or estimate the expected number or percentage of edges removed for nodes with degree less than k? 

4. How does the noise added to the low-rank singular values provide privacy for the edges not included in the low-rank approximation? Is there still potential leakage and can it be quantified?

5. The privacy analysis relies on the singular values capturing the difference between adjacent graphs. Are there graph structures or changes where this would not hold well? How can the method be adapted?

6. The paper evaluates against link stealing attacks. Are there other types of attacks or threat models the method is more or less robust to? Why?

7. The rank selection controls the tradeoff between utility and privacy. Can you design an adaptive scheme to automatically select the rank rather than setting it manually?

8. The method trains models on the low-rank graph. Does this introduce any model bias issues compared to training on the original graph? How does bias manifest?

9. Can the concept of low-rank perturbation be extended to protect attributed graphs where node features also need privacy, not just structure? What are the challenges?

10. The method has better utility than baselines under high privacy regimes. What are the limits in terms of minimum epsilon where gains are seen over naively training without the graph or other methods?
