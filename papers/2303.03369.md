# [Multimodal Prompting with Missing Modalities for Visual Recognition](https://arxiv.org/abs/2303.03369)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question seems to be: 

How can we make multimodal transformers robust to missing modalities during training and testing, while avoiding the need to finetune the entire heavy model?

The key points related to this question appear to be:

- Multimodal transformers require complete multimodal data during both training and testing for optimal performance. However, in real-world scenarios, some modalities may be missing due to various constraints. 

- Finetuning the entire heavy pretrained transformer model on new datasets is computationally expensive and often infeasible.

- The paper proposes using "missing-aware prompts" that can be plugged into a pretrained multimodal transformer to make it robust to missing modalities, without needing to finetune the entire model.

- The prompts can provide conditioning for different missing modality cases and require training only a very small fraction of the model's parameters.

- Experiments show the proposed prompting method improves performance on tasks with missing modalities, while requiring less than 1% of the model parameters to be trained.

So in summary, the central hypothesis is that missing-aware prompts can efficiently adapt multimodal transformers to handle missing modalities during training and inference, avoiding costly finetuning of the full model.
