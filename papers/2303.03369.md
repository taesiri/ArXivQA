# [Multimodal Prompting with Missing Modalities for Visual Recognition](https://arxiv.org/abs/2303.03369)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question seems to be: 

How can we make multimodal transformers robust to missing modalities during training and testing, while avoiding the need to finetune the entire heavy model?

The key points related to this question appear to be:

- Multimodal transformers require complete multimodal data during both training and testing for optimal performance. However, in real-world scenarios, some modalities may be missing due to various constraints. 

- Finetuning the entire heavy pretrained transformer model on new datasets is computationally expensive and often infeasible.

- The paper proposes using "missing-aware prompts" that can be plugged into a pretrained multimodal transformer to make it robust to missing modalities, without needing to finetune the entire model.

- The prompts can provide conditioning for different missing modality cases and require training only a very small fraction of the model's parameters.

- Experiments show the proposed prompting method improves performance on tasks with missing modalities, while requiring less than 1% of the model parameters to be trained.

So in summary, the central hypothesis is that missing-aware prompts can efficiently adapt multimodal transformers to handle missing modalities during training and inference, avoiding costly finetuning of the full model.


## What is the main contribution of this paper?

 The main contributions of this paper are:

- It proposes a framework to handle two key challenges in multimodal learning: dealing with missing modalities during training and/or testing, and avoiding the need to finetune the entire heavyweight pre-trained model. 

- It introduces the concept of "missing-aware prompts" that can be plugged into pretrained multimodal transformers like ViLT to make them robust to missing modalities. Only the prompts need to be trained, not the entire model, making this approach very efficient.

- It explores two designs for integrating the missing-aware prompts: input-level and attention-level prompting. It analyzes the trade-offs between these two approaches.

- It conducts extensive experiments on three multimodal classification tasks to demonstrate the effectiveness of the proposed approach. The results show that the missing-aware prompts can substantially improve performance under various missing modality conditions, while only requiring less than 1% of the full model's parameters.

In summary, the key contribution is a simple yet effective prompting-based method to make multimodal transformers robust to missing modalities during training or inference, without the computational burden of full model fine-tuning. The concept of conditional missing-aware prompts is novel.
