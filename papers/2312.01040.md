# [From Beginner to Expert: Modeling Medical Knowledge into General LLMs](https://arxiv.org/abs/2312.01040)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem: 
- Large language models (LLMs) have difficulty with sensitive applications like reasoning over medical knowledge and answering medical questions.  
- Prior studies have tried to address this by increasing model size (>100B parameters) to learn more medical knowledge, but there is still room for improvement in smaller LLMs (<100B parameters).

Proposed Solution:  
The authors present a methodology to fine-tune a 10B parameter general pre-trained LLM (AntGLM) into a medical expert model (AntGLM-Med-10B) using a 3-stage optimization procedure:

1. General Medical Knowledge Injection: Continual pre-training on large-scale medical textbooks, knowledge graphs, exam questions and articles to inject basic medical knowledge.  

2. Medical Domain Instruction Tuning: Fine-tuning on Chinese medical exam questions, medical QA datasets and multiple choice questions to learn diverse medical task types.

3. Specific Medical Task Adaptation: Tailoring to a specific clinical task (multi-choice question answering) using PubMedQA dataset. A novel Verification-of-Choice prompting strategy is introduced to enhance reasoning.

Main Contributions:

- Investigating adapting general pre-trained LLMs into the medical domain for specific medical tasks through a 3-stage optimization procedure.

- Collecting and constructing large-scale medical datasets for each optimization stage, encompassing diverse data types (QA, reasoning, conversations) and tasks.  

- Introducing a Verification-of-Choice prompt engineering technique for multi-choice medical questions to improve LLM reasoning.

- Achieving strong performance on PubMedQA medical QA task, even outperforming larger LLMs. Demonstrates optimizing smaller LLMs effectively for medical domain.

The paper provides a methodology grounded in multi-stage optimization strategies and medical data curation to transform general language models into specialized medical expert models for better clinical reasoning. The performance demonstrates this is a promising approach even without extensively scaled models.
