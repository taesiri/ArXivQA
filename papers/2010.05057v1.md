# [Fairness-aware Agnostic Federated Learning](https://arxiv.org/abs/2010.05057v1)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question is:

How to achieve fairness in federated learning when the testing data distribution is unknown or different from the training data distribution?

The key points are:

- In federated learning, the training data is distributed across multiple clients and may have different distributions. 

- Most prior work on federated learning focuses on privacy protection and communication efficiency, but achieving fairness is under-explored.

- Simply adding fairness constraints on the global model trained on client data cannot guarantee fairness on unknown testing data.

- The authors propose a fairness-aware agnostic federated learning framework (AgnosticFair) to deal with unknown testing distribution. 

- AgnosticFair uses kernel reweighting functions to assign values to training samples in the loss and fairness constraints. This allows the global model to achieve fairness even with distribution shift.

- Experiments on two datasets demonstrate AgnosticFair can achieve higher accuracy and fairness under data distribution shift compared to baselines.

In summary, the key hypothesis is that using kernel reweighting in the loss and fairness constraints will allow achieving fairness in federated learning when the testing distribution is unknown or different from training. The paper proposes and evaluates the AgnosticFair framework to address this problem.


## What is the main contribution of this paper?

 The main contributions of this paper are:

1. It proposes a fairness-aware agnostic federated learning framework (AgnosticFair) to deal with the challenge of unknown testing data distribution in federated learning. 

2. It uses kernel reweighing functions to assign weights to each training sample in both the loss function and fairness constraint. This allows the model to achieve high accuracy and fairness guarantee on unknown testing data.

3. It formulates the problem as a two-player adversarial minimax game between the learner and the adversary. The adversary aims to generate any possible unknown testing distribution to maximize the classifier loss, while the learner tries to find parameters to minimize the worst case loss.

4. It develops an efficient approach to optimize the agnostic loss function under the agnostic fairness constraints between the server and clients, without exposing any raw data. 

5. It conducts experiments on two real datasets to demonstrate the effectiveness of the proposed approach in achieving fairness under data distribution shifts while maintaining high accuracy.

In summary, the key contribution is proposing a new federated learning framework that can achieve both accuracy and fairness on unknown testing data distributions, by formulating it as an adversarial minimax game and using kernel reweighing functions. The framework provides an efficient optimization approach involving only the exchange of parameters between server and clients.
