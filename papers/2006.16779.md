# PLATO-2: Towards Building an Open-Domain Chatbot via Curriculum Learning

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is: How can we build a high-quality open-domain chatbot via curriculum learning? The key points are:- Open-domain chatbots need to be able to model the one-to-many mapping in dialogues, where one context can have multiple appropriate responses. This capability is crucial for generating diverse and human-like responses.- Previous work PLATO introduced discrete latent variables to model the one-to-many relationship. However, directly scaling up PLATO encounters training instability and inefficiency. - This paper proposes PLATO-2, which is trained via curriculum learning in two stages:  - Stage 1 trains a coarse-grained model for general response generation under a simplified one-to-one mapping.  - Stage 2 trains a fine-grained model with latent variables for diverse responses, and an evaluation model for selecting the most appropriate response.- PLATO-2 leverages curriculum learning to gradually learn the complex concept of one-to-many mapping for open-domain conversations, achieving new state-of-the-art results.In summary, the central hypothesis is that curriculum learning can enable the effective training of PLATO-2 to build a high-quality open-domain chatbot that can model the one-to-many mapping relationship. The experiments verify the superiority of PLATO-2 over previous state-of-the-art methods.


## What is the main contribution of this paper?

The main contributions of this paper are:1. It introduces PLATO-2, an open-domain chatbot trained via curriculum learning. PLATO-2 has English and Chinese versions with model sizes up to 1.6 billion parameters. 2. It proposes a two-stage curriculum learning method to train PLATO-2:- Stage 1 trains a coarse-grained model for general response generation under a simplified one-to-one mapping between context and response. - Stage 2 trains a fine-grained model with latent variables for diverse response generation, and a separate evaluation model for selecting the most appropriate response.3. It shows that PLATO-2 outperforms previous state-of-the-art models like Meena, Blender, DialoGPT, etc. through comprehensive experiments and evaluations on English and Chinese datasets. PLATO-2 achieves new state-of-the-art results.4. It demonstrates that the two-stage curriculum learning approach is essential for successfully training the large PLATO-2 model. The models from the two stages are also applicable for knowledge-grounded and task-oriented conversations.5. It releases the English PLATO-2 models and source code to facilitate research in open-domain dialogue generation.In summary, the main contributions are proposing PLATO-2 trained via curriculum learning, showing its state-of-the-art performance on open-domain chitchat, and releasing the models and code to advance research in this area.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from that paper:The paper introduces PLATO-2, an open-domain chatbot trained via curriculum learning in two stages - first a coarse-grained model for general response generation, then a fine-grained model with latent variables for diverse responses and an evaluation model for selecting the most appropriate response. Comprehensive experiments on English and Chinese datasets demonstrate PLATO-2 achieves state-of-the-art results on open-domain dialogue generation.
