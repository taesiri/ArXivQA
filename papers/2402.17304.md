# [Probing Multimodal Large Language Models for Global and Local Semantic   Representation](https://arxiv.org/abs/2402.17304)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Recent works have shown that multimodal large language models (MLLMs) achieve great performance on image-to-text tasks. However, there is little investigation on whether MLLMs truly understand complete image information (global semantics) or only capture local objects (local semantics). 

Main Contributions:
- Design visual-language entailment task and object detection task to probe global and local semantic representation abilities of MLLMs. 
- Find intermediate layers encode more global semantics while top layers focus excessively on local token features, diminishing global encoding.
- Results consistent across diverse MLLMs and prompts, shedding light on pre-training and model architectures.

Methodology:
- Use 4 MLLMs (Kosmos-2, LaVIT, Emu, Qwen-VL) with 7B-14B parameters.
- Extract layer representations with different prompts for entailment and detection tasks.
- Entailment: predict if image-caption pair is semantically aligned. 
- Detection: predict presence/absence of 80 COCO objects.
- Compare cues (categories provided) vs no cues.  
- Freeze MLLM parameters, train linear classifiers.

Key Results:  
- For entailment, intermediate layers outperform top layers in encoding global semantics.  
- For detection, top layers strongly encode local features of prompted token.  
- Providing category cues significantly boosts top layers' detection ability.
- Findings consistent across models and prompts.

Conclusion:
- Top layers lose some global information and overly focus on local token features. 
- Provides insights into improving MLLM pre-training and architectures.

In summary, the key finding is that while top layers of MLLMs strongly capture local semantics, their global encoding diminishes compared to intermediate layers. The paper sheds light on this issue and calls for better MLLM designs.
