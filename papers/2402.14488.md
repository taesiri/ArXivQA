# [Does the Generator Mind its Contexts? An Analysis of Generative Model   Faithfulness under Context Transfer](https://arxiv.org/abs/2402.14488)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem Statement
- The paper investigates the faithfulness of generative question answering models in the presence of dynamic contextual knowledge, a process referred to as "context transfer".  
- Specifically, it examines the phenomenon of "memory hallucination", where models generate answers relying on outdated knowledge from the training data rather than grounding answers in up-to-date contextual knowledge provided at test time.
- Two key research questions are posed: (1) To what extent do models exhibit faithfulness under context transfer? (2) What are the underlying reasons for memory hallucinations?

Proposed Solution
- A context transfer question answering task is defined where models are trained on (question, context, answer) triples but tested on updated contexts for the same questions.
- A new metric called "Margin Failure Rate" (MFR) is proposed to quantify the extent of memory hallucinations. It measures when model generations are more similar to training data than test data.
- Experiments are conducted with various models like BART, T5, and FiD to analyze their faithfulness per the MFR metric.
- Further analysis explores the impact of factors like context scale, irrelevant noisy contexts to gain insights into reasons behind memory hallucinations.

Main Contributions  
- Identifies and formalizes the problem of evaluating model faithfulness under context transfer.
- Proposes a new metric tailored to quantifying memory hallucinations in this setup.
- Provides an empirical analysis of various state-of-the-art models using the formalized task and metric.
- Uncovers insights through ablations regarding the susceptibility of models to outdated knowledge as well as the causal role of noisy contexts.

In summary, the paper makes important contributions around evaluating and understanding model faithfulness issues that arise due to evolving contextual knowledge in question answering.
