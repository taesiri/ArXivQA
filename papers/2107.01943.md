# When and How to Fool Explainable Models (and Humans) with Adversarial   Examples

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the main research question seems to be: Can adversarial examples be effectively generated against explainable machine learning models in scenarios where humans assess not only the input sample but also the model's classification and explanation?The paper aims to analyze if and how adversarial attacks can succeed when attacking explainable ML models under human supervision. Rather than only considering attacks that fool the model's classification, the authors generalize adversarial examples to also account for attacks that produce incorrect or misleading explanations while possibly maintaining the original classification. The key contributions appear to be:- Extending the notion of adversarial examples to fit explainable ML scenarios with human assessment of the model's predictions and explanations. - Proposing a framework to identify if and how adversarial attacks can fool explainable models and humans based on factors like the explanation type, user expertise, and purpose of the explanation.- Describing attack paradigms that manipulate the model's classification, explanation, or both to be convincing under human assessment.- Illustrating the attack paradigms through examples on image classification tasks using feature-based and prototype-based explanations.Overall, the paper aims to provide a comprehensive analysis of the possibilities and limitations of adversarial attacks against explainable ML models when humans are in the loop, in order to enable more rigorous research on this topic.


## What is the main contribution of this paper?

This paper introduces a framework to study adversarial attacks against explainable machine learning models. The main contributions are:- It extends the notion of adversarial examples to fit explainable ML scenarios where humans assess not only the input but also the model's output and explanation. This allows examining different attack paradigms.- It analyzes how adversarial attacks should be designed to fool explainable models and humans, based on factors like the task, user expertise, and explanation type/impact. - It proposes a comprehensive framework that establishes whether/how adversarial attacks can succeed against explainable models under human supervision. The framework provides a roadmap for designing attacks in realistic scenarios.- It illustrates the attack paradigms using image classification tasks and feature-based/prototype-based explanations. In summary, the paper provides a systematic framework to study the possibilities and limitations of adversarial attacks against explainable ML models from both an attacker and a defender perspective. It contributes to more rigorous research on adversarial robustness for explainable ML.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper proposes a framework to analyze the possibilities and limitations of fooling explainable machine learning models and humans using adversarial examples, by extending the notion of adversarial attacks to account for scenarios where humans assess not only the input but also the model's classification and explanation.
