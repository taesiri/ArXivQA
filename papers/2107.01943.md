# [When and How to Fool Explainable Models (and Humans) with Adversarial   Examples](https://arxiv.org/abs/2107.01943)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the main research question seems to be: Can adversarial examples be effectively generated against explainable machine learning models in scenarios where humans assess not only the input sample but also the model's classification and explanation?The paper aims to analyze if and how adversarial attacks can succeed when attacking explainable ML models under human supervision. Rather than only considering attacks that fool the model's classification, the authors generalize adversarial examples to also account for attacks that produce incorrect or misleading explanations while possibly maintaining the original classification. The key contributions appear to be:- Extending the notion of adversarial examples to fit explainable ML scenarios with human assessment of the model's predictions and explanations. - Proposing a framework to identify if and how adversarial attacks can fool explainable models and humans based on factors like the explanation type, user expertise, and purpose of the explanation.- Describing attack paradigms that manipulate the model's classification, explanation, or both to be convincing under human assessment.- Illustrating the attack paradigms through examples on image classification tasks using feature-based and prototype-based explanations.Overall, the paper aims to provide a comprehensive analysis of the possibilities and limitations of adversarial attacks against explainable ML models when humans are in the loop, in order to enable more rigorous research on this topic.


## What is the main contribution of this paper?

This paper introduces a framework to study adversarial attacks against explainable machine learning models. The main contributions are:- It extends the notion of adversarial examples to fit explainable ML scenarios where humans assess not only the input but also the model's output and explanation. This allows examining different attack paradigms.- It analyzes how adversarial attacks should be designed to fool explainable models and humans, based on factors like the task, user expertise, and explanation type/impact. - It proposes a comprehensive framework that establishes whether/how adversarial attacks can succeed against explainable models under human supervision. The framework provides a roadmap for designing attacks in realistic scenarios.- It illustrates the attack paradigms using image classification tasks and feature-based/prototype-based explanations. In summary, the paper provides a systematic framework to study the possibilities and limitations of adversarial attacks against explainable ML models from both an attacker and a defender perspective. It contributes to more rigorous research on adversarial robustness for explainable ML.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper proposes a framework to analyze the possibilities and limitations of fooling explainable machine learning models and humans using adversarial examples, by extending the notion of adversarial attacks to account for scenarios where humans assess not only the input but also the model's classification and explanation.


## How does this paper compare to other research in the same field?

This paper provides a comprehensive framework for studying adversarial attacks against explainable machine learning models. Some key aspects that distinguish it from prior work:- Most prior work has focused on attacks against specific explanation methods (e.g. saliency maps) or in narrow scenarios. This paper proposes a general taxonomy that encompasses different types of attacks, explanation methods, and usage scenarios. - It thoroughly analyzes how attacks should be designed depending on factors like user expertise, explanation impact, etc. Most works overlook the role of the human in the loop.- It identifies and illustrates attacks that fool both the model and the user. Many papers only consider attacks that change the model's output. - The framework covers a wide range of scenarios, from model development to high-stakes applications. This provides a holistic view of adversarial threats in explainable ML.- It formalizes different notions of adversarial examples in explainable contexts based on changes to classification, explanation, or both. This level of rigor is missing in previous work.- The paper highlights open challenges like developing unified attack algorithms and improving explanation robustness. This helps guide future research directions.In summary, this review takes a systematic approach to analyzing adversarial threats against explainable ML, considering the interplay of factors often studied in isolation. It provides a solid foundation and an agenda for more rigorous research on this emerging problem.


## What future research directions do the authors suggest?

Based on my reading of the paper, some of the main future research directions suggested by the authors include:- Considering additional factors in the proposed framework, such as the model's confidence score, which can influence the human acceptance of the prediction. The authors suggest developing a more comprehensive framework that incorporates additional relevant factors beyond just the input, output class, and explanation.- Developing a general and unified attack algorithm capable of generating adversarial examples satisfying the requirements of different attack paradigms, scenarios, and explanation methods. The authors propose studying the development of such a flexible attack approach in future work.- Further analyzing the vulnerability of explanation methods, especially prototype-based approaches, to adversarial attacks. The authors highlight the need for more research on improving the robustness and reliability of explanations. - Conceiving new defense strategies tailored to different explanation methods in order to make them more robust against adversarial attacks. This is noted as an important direction to increase the trustworthiness of explainable AI models.In summary, the main future directions are: developing a more comprehensive analytical framework, designing a flexible attack algorithm, evaluating explanation methods' robustness, and conceiving tailored defense strategies to improve explainable AI reliability against adversarial threats. The authors propose advancing research in these areas to promote more rigorous studies on adversarial examples in explainable machine learning.


## Summarize the paper in one paragraph.

Here is a one paragraph summary of the paper:This paper proposes a comprehensive framework to study adversarial attacks against explainable machine learning models. It extends the notion of adversarial examples to scenarios where humans assess not just the input sample but also the model's output classification and explanation. The paper analyzes different attack paradigms where the adversary can fool the model by changing the classification, the explanation, or both. It discusses how attacks should be designed based on factors like the explanation type, user expertise, and objective. Attacks are illustrated on image classification tasks using feature and prototype-based explanations. Overall, the paper provides a thorough analysis of requirements for realistic adversarial attacks on explainable ML models under human supervision, contributing to more rigorous research on model vulnerabilities.


## Summarize the paper in two paragraphs.

Here is a two paragraph summary of the paper:This paper proposes a framework to study adversarial attacks against explainable machine learning models. The key idea is that regular adversarial examples, which only aim to fool the model's classification, may not be effective when humans can assess not just the input sample but also the model's output and explanation. Therefore, the notion of adversarial examples needs to be extended for explainable ML scenarios. The paper introduces a taxonomy of attacks that can target the model's classification, explanation, or both jointly. It analyzes attack strategies depending on factors like the explanation type, user expertise, and task impact. Attacks are illustrated on medical image classification and large-scale visual recognition using feature-based and prototype-based explanations. Overall, the framework provides a comprehensive roadmap for designing realistic attacks on explainable ML models under human supervision. It highlights requirements for reliable explanations and identifies promising future research directions such as developing a unified attack algorithm and improving explanation robustness.


## Summarize the main method used in the paper in one paragraph.

Here is a one paragraph summary of the main method used in the paper:This paper proposes a framework to study adversarial attacks against explainable machine learning models. The key idea is to extend the notion of adversarial examples to scenarios where humans can assess not just the input sample but also the model's prediction and explanation. This allows examining different attack paradigms, such as fooling the model's classification, explanation, or both. The framework analyzes how attacks should be designed to mislead models and humans based on factors like the explanation type, user expertise, and task impact. Attacks are illustrated on image classification tasks using feature-based and prototype-based explanations. Overall, the framework provides a comprehensive roadmap for designing realistic attacks on explainable models under human supervision, highlighting requirements for reliable explainable AI.


## What problem or question is the paper addressing?

Based on my reading, the key points of this paper are:- It focuses on the topic of adversarial attacks against explainable machine learning models. Specifically, it aims to analyze whether effective adversarial examples can be generated when humans not only observe the input sample, but also the model's classification and explanation. - The paper extends the notion of adversarial examples to account for scenarios where humans assess the full prediction process (input, output, explanation). This allows examining different types of attacks that fool the model's classification, explanation, or both jointly.- A framework is proposed to identify how attacks should be designed to successfully mislead explainable models and humans, based on factors like the explanation type, user expertise, task impact, etc. - The framework provides a comprehensive roadmap for creating realistic attacks against explainable models under human supervision. It can help adversaries design attacks, but also help developers identify requirements for more reliable explainable models.- Different attack paradigms are illustrated on image classification tasks using feature-based and prototype-based explanations. The examples demonstrate designing attacks based on the proposed framework, to fool models in plausible scenarios.In summary, the key focus is on presenting a rigorous framework to study adversarial vulnerabilities of explainable machine learning under human assessment, which provides guidance on attack design and helps advance research on more robust explainable AI.
