# When and How to Fool Explainable Models (and Humans) with Adversarial   Examples

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the main research question seems to be: Can adversarial examples be effectively generated against explainable machine learning models in scenarios where humans assess not only the input sample but also the model's classification and explanation?The paper aims to analyze if and how adversarial attacks can succeed when attacking explainable ML models under human supervision. Rather than only considering attacks that fool the model's classification, the authors generalize adversarial examples to also account for attacks that produce incorrect or misleading explanations while possibly maintaining the original classification. The key contributions appear to be:- Extending the notion of adversarial examples to fit explainable ML scenarios with human assessment of the model's predictions and explanations. - Proposing a framework to identify if and how adversarial attacks can fool explainable models and humans based on factors like the explanation type, user expertise, and purpose of the explanation.- Describing attack paradigms that manipulate the model's classification, explanation, or both to be convincing under human assessment.- Illustrating the attack paradigms through examples on image classification tasks using feature-based and prototype-based explanations.Overall, the paper aims to provide a comprehensive analysis of the possibilities and limitations of adversarial attacks against explainable ML models when humans are in the loop, in order to enable more rigorous research on this topic.


## What is the main contribution of this paper?

This paper introduces a framework to study adversarial attacks against explainable machine learning models. The main contributions are:- It extends the notion of adversarial examples to fit explainable ML scenarios where humans assess not only the input but also the model's output and explanation. This allows examining different attack paradigms.- It analyzes how adversarial attacks should be designed to fool explainable models and humans, based on factors like the task, user expertise, and explanation type/impact. - It proposes a comprehensive framework that establishes whether/how adversarial attacks can succeed against explainable models under human supervision. The framework provides a roadmap for designing attacks in realistic scenarios.- It illustrates the attack paradigms using image classification tasks and feature-based/prototype-based explanations. In summary, the paper provides a systematic framework to study the possibilities and limitations of adversarial attacks against explainable ML models from both an attacker and a defender perspective. It contributes to more rigorous research on adversarial robustness for explainable ML.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper proposes a framework to analyze the possibilities and limitations of fooling explainable machine learning models and humans using adversarial examples, by extending the notion of adversarial attacks to account for scenarios where humans assess not only the input but also the model's classification and explanation.


## How does this paper compare to other research in the same field?

This paper provides a comprehensive framework for studying adversarial attacks against explainable machine learning models. Some key aspects that distinguish it from prior work:- Most prior work has focused on attacks against specific explanation methods (e.g. saliency maps) or in narrow scenarios. This paper proposes a general taxonomy that encompasses different types of attacks, explanation methods, and usage scenarios. - It thoroughly analyzes how attacks should be designed depending on factors like user expertise, explanation impact, etc. Most works overlook the role of the human in the loop.- It identifies and illustrates attacks that fool both the model and the user. Many papers only consider attacks that change the model's output. - The framework covers a wide range of scenarios, from model development to high-stakes applications. This provides a holistic view of adversarial threats in explainable ML.- It formalizes different notions of adversarial examples in explainable contexts based on changes to classification, explanation, or both. This level of rigor is missing in previous work.- The paper highlights open challenges like developing unified attack algorithms and improving explanation robustness. This helps guide future research directions.In summary, this review takes a systematic approach to analyzing adversarial threats against explainable ML, considering the interplay of factors often studied in isolation. It provides a solid foundation and an agenda for more rigorous research on this emerging problem.
