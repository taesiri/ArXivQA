# [Stop Reasoning! When Multimodal LLMs with Chain-of-Thought Reasoning   Meets Adversarial Images](https://arxiv.org/abs/2402.14899)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Multimodal large language models (MLLMs) have shown impressive capabilities in image understanding. However, like traditional vision models, they remain vulnerable to adversarial attacks through adversarial images. 
- Chain-of-thought (CoT) reasoning has been explored to enhance MLLMs' performance and explainability by generating intermediate reasoning steps. But the impact of CoT reasoning on MLLMs' robustness against adversarial attacks is still unknown.

Proposed Solution:
- Evaluate adversarial robustness of MLLMs with and without CoT reasoning through answer attack, rationale attack (proposed), and stop-reasoning attack (proposed).
- Analyze how CoT reasoning alters when MLLMs make wrong predictions on adversarial images to provide explainability.

Main Contributions:
- Demonstrate that CoT reasoning marginally improves robustness of MLLMs against answer and rationale attacks.
- Propose stop-reasoning attack that effectively compromises the marginal robustness improvement from CoT reasoning.
- Provide examples showing how CoT reasoning changes when MLLMs incorrectly classify adversarial images, enhancing explainability.
- Perform comprehensive experiments on MiniGPT4, OpenFlamingo and LLaVA models using A-OKVQA and ScienceQA datasets to validate proposals.

In summary, the paper studies the impact of CoT reasoning on adversarial robustness of MLLMs. It shows CoT provides marginal robustness gains, but this can be easily nullified. The paper also leverages the explainability from CoT to interpret model behaviors on adversarial images.
