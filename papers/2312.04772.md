# [Remembering to Be Fair: On Non-Markovian Fairness in Sequential   DecisionMaking (Preliminary Report)](https://arxiv.org/abs/2312.04772)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a summary paragraph of the key points from the paper:

This paper introduces the notion of non-Markovian fairness for sequential decision making, where fairness depends on the history of decisions made over time, not just the current state. They define non-Markovian fair decision processes (NMFDPs) where the fairness function considers the entire trace of states and actions. They discuss different realizations of fairness functions, like Nash welfare, Rawlsian social welfare, and novel metrics like "time in first place." Different notions of fair policies are also proposed, including fair in the limit, anytime fair, periodically fair, and fair-optimal. A key insight is the role of memory in converting non-Markovian fairness functions into Markovian ones. With sufficient memory, an NMFDP can be transformed into an equivalent Markovian process, enabling standard reinforcement learning methods to find fair policies. Overall, the paper lays theoretical foundations to study history-dependent fairness in sequential settings with multiple stakeholders. Formalizing non-Markovian fairness exposes new dimensions for fair sequential decision making.


## Summarize the paper in one sentence.

 This paper introduces formal foundations for studying non-Markovian fairness in sequential decision making, including notions of long-term, anytime, periodic, and bounded fairness, explores the role of memory in converting non-Markovian to Markovian fairness measures, and discusses implications for policy verification, learning, and analysis.


## What is the main contribution of this paper?

 The main contribution of this paper is formally defining the notion of non-Markovian fairness in sequential decision making. Specifically:

- The paper introduces the concept of a Non-Markovian Fair Decision Process (NMFDP), where the fairness function depends on the entire history of states and actions rather than just the current state. This allows expressing a broader range of fairness criteria.

- The paper discusses different possible realizations of the fairness function, such as maximizing Nash welfare or Rawlsian social welfare over time.

- It defines several notions of what constitutes a "fair policy" in this non-Markovian setting, including being fair in the limit, anytime fair, periodically fair, and optimally balancing fairness and reward.

- The paper analyzes the role of memory in converting some non-Markovian fairness functions into Markovian ones. This facilitates applying standard reinforcement learning algorithms for optimizing fairness.

In summary, the main contribution is a formal foundation and analysis of non-Markovian, history-based fairness in sequential decision making problems. This is an important stepping stone towards developing algorithms for learning or verifying fair policies in this broader class of problems.


## What are the keywords or key terms associated with this paper?

 Based on my review of the paper, some of the key terms and concepts related to this work include:

- Non-Markovian fairness - The central concept of fairness depending on the history of decisions, not just the current state. 

- Sequential decision making - Multi-step decision processes over time that aim to achieve objectives.

- Stakeholders - The entities affected by sequential decisions.

- Fairness functions - Functions that quantify the fairness of a decision process. Examples include Nash welfare and Rawlsian social welfare.  

- Fair policies - Policies that aim to ensure fairness over time, such as being fair in the limit, anytime fair, periodically fair, or bounded fair.

- Memory - External storage of relevant history that can allow conversion of non-Markovian fairness functions into Markovian ones.

- Verification - Checking if a policy or trace meets fairness criteria.  

- Policy learning - Learning optimal fair policies from data.

In summary, the key focus is on formalizing temporal notions of fairness in sequential decision settings using concepts like non-Markovian fairness functions and different fair policy definitions. Memory also plays an important role.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions I would ask to better understand the proposed method for non-Markovian fairness in sequential decision-making:

1. The paper defines the notion of a "non-Markovian policy" that maps histories to actions. Can you elaborate on how policies are actually specified under this framework? Do you assume access to a policy optimizer that can learn non-Markovian policies? 

2. How does the choice of fairness function $F$ impact the efficiency and computational complexity of learning a fair policy? Are there broad classes of fairness functions that allow for more efficient optimization?

3. You define fairness “in the limit” for infinite traces. How can we provide finite-sample guarantees for critical applications like healthcare where individuals cannot wait indefinitely? 

4. Periodic fairness seems very relevant for applications with reporting requirements. How might the period length $k$ depend on environment dynamics in complex domains? Is there an analogue of the mixing time that could inform good values of $k$?

5. The role of memory is highlighted for Markovizing history-based fairness functions. Are there complexity results showing whether finite memory suffices in broad cases? Or lower bounds on the required memory?

6. You note converting to Markovian fairness facilitates application of standard RL algorithms. But what modifications need to be made to policy optimizers to actually optimize non-Markovian fairness? Do off-policy methods hold an advantage?

7. How rich can the environments be where you can still learn fair policies efficiently? Are there assumptions beyond full observability and Markov dynamics that become problematic?

8. The vaccine allocation example has global state and many resource constraints. How do we scale optimization of fairness notions to problems with very large (or continuous) state spaces? 

9. You motivate fairness over entire decision processes rather than individual decisions. However, processes often have subtasks with different fairness considerations. How can we decompose problems to better capture this?

10. From an application view, how would a domain expert even specify an appropriate non-Markovian fairness function $F$ that captures their desiderata? What interfaces could facilitate this?
