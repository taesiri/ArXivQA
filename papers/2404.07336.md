# [PEAVS: Perceptual Evaluation of Audio-Visual Synchrony Grounded in   Viewers' Opinion Scores](https://arxiv.org/abs/2404.07336)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Lack of metrics to evaluate audio-visual (AV) synchronization in "in the wild" videos to drive progress in AV generative models. Existing metrics focus on audio or video quality separately.
- Need for a metric aligned with human perception of AV sync issues across various distortion types like speed change, shuffling, flickering etc. beyond just temporal offsets.

Proposed Solution:
- Created a large-scale human annotated dataset with 100+ hours of videos representing 9 types of AV sync distortions at 10 levels each. 120K annotations with score of 1-5 on disruption.
- Proposed PEAVS, a learned perceptual metric to predict human scores of AV sync quality on a scale of 1-5 without any ground truth. 

Key Contributions:
- New Audio-Visual Synchrony human perception benchmark with 100+ hrs of distorted and annotated video content.
- PEAVS metric that correlates well (0.79) with human judgements at set-level and works in a reference free manner.
- Validation experiments showing PEAVS outperforms baseline AV extension of Frechet distance by 51% and audio-visual synchronization classifier SparseSync by 19% in accuracy.
- Analysis of PEAVS response across distortions highlights efficacy in capturing human perception of synchrony.
- Established strong baselines via proposed cross-modal transformer framework and pre-training methodology for future audio-visual sync evaluation research.

The paper makes notable contributions in form of the large-scale annotated benchmark dataset, PEAVS metric design and extensive experiments in modeling human perception of audio-visual synchronization issues.


## Summarize the paper in one sentence.

 Here is a one-sentence summary of the paper:

The paper introduces PEAVS, a new automatic metric with interpretable 5-point scoring for evaluating audio-visual synchronization in videos, trained on a large-scale human annotated dataset of synchronization issues like speed changes and shuffling; PEAVS achieves high correlation to human judgments of synchronization quality.


## What is the main contribution of this paper?

 According to the paper, the main contributions are:

1) Presenting a new Audio-Visual Synchrony human perception (AVS) benchmark dataset with over 120K annotations and over 100+ hours of content. Each video is annotated by three different annotators.

2) Proposing a new audio-visual synchrony evaluation metric called PEAVS (Perceptual Evaluation of Audio-Visual Synchrony) that is reference-free and has an interpretable scoring scale from 1 to 5. 

3) PEAVS shows a high correlation of 0.79 with human judgments on the benchmark dataset, significantly outperforming a Fréchet distance based audio-visual synchrony metric.

So in summary, the main contributions are creating a large-scale human annotated dataset for audio-visual synchrony, and developing an automatic metric called PEAVS that closely aligns with human perception of audio-visual synchronization issues.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, here are some of the key terms and concepts:

- Audio-visual synchronization - evaluating how well aligned the audio and video streams are in a video
- Audio-visual generative modeling - using models to generate synthetic audio-visual content
- Evaluation metrics - metrics used to quantitatively measure the quality of generated audio-visual content
- Human perception - modeling how humans subjectively perceive audio-visual synchronization issues
- Interpretable scoring - having a scoring system that is understandable and aligned with human judgments
- "In the wild" videos - real-world videos with natural variation, not constrained videos like those with just faces
- Distortion types - different ways audio-visual synchronization can break down, like temporal shifts, speed changes, etc.
- Perceptual guidelines - instructions given to human raters on how to annotate based on perceptual impact
- Audio-visual benchmark dataset - a new large-scale human-annotated dataset for audio-visual synchronization
- PEAVS - the proposed Perceptual Evaluation of Audio-Visual Synchrony metric aligned with human opinions
- Cross-modal transformer - using transformers to model relationships between audio and visual modalities
- Pre-training and fine-tuning - training strategy with contrastive pre-training and supervised fine-tuning

The key focus areas are around developing better metrics for evaluating audio-visual synchronization that correlate well with human perceptual judgments, using appropriate benchmark datasets.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in the paper:

1) The paper proposes a new metric called PEAVS for evaluating audio-visual synchronization in videos. What are the key components of the PEAVS metric architecture? How does it leverage features from pretrained I3D and VGGish models?

2) What is the two stage training strategy adopted for PEAVS? What is the purpose of using contrastive learning based pre-training in stage 1? How does fine-tuning happen in stage 2?

3) Explain the cross-modal transformer formulation used in PEAVS. Why is it important for capturing temporal relationships between audio and visual modalities? 

4) The paper highlights the importance of developing a perceptual, interpretable metric aligned with human judgments. How does PEAVS achieve interpretability in its scoring? What scale is used and what guidelines are provided for mapping model outputs to human perceptual scores?

5) What were the different types of audio-visual distortions introduced in the benchmark dataset to represent synchronization issues? Why was it important to go beyond just temporal shifts in audio/video streams?  

6) Analyze the differences in objective and approach between PEAVS and prior works like SparseSync and AVST for evaluating AV synchronization. Why can't these methods be directly compared to PEAVS?

7) The paper demonstrates the limitations of using Fréchet distance based metrics like FAD and FVD for effectively capturing AV synchronization issues. Analyze the interactions shown in Figure 5 to illustrate this. Why does combining audio and visual features into FAVD lead to better capturing of temporal distortions?

8) How robust is the PEAVS metric? Analyze the ablation experiments done with Cross-Modal Base and Basic Transformer to demonstrate the impact of model choices like pre-training and cross-modal layers on performance.

9) Pick one of the distortion types in Figure 6, and analyze how the PEAVS scores vary with increase in distortion levels. How aligned is it with human judgement? Where does the metric fail to capture perceptual thresholds?

10) What were some of the limitations of the study highlighted in the paper? How can the benchmark dataset be expanded in future work? How can additional pre-training help address metric limitations stemming from small dataset size?
