# Language models show human-like content effects on reasoning

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is: Do large language models show human-like content effects on reasoning tasks? The authors hypothesize that because large language models are trained on human-generated text, they will pick up on regularities and biases in how humans reason about different types of content. Therefore, the authors predict that language models will show similar biases to humans in how they perform on reasoning tasks depending on the content of the task - performing better on reasoning tasks with familiar, believable content compared to abstract or nonsensical content.To test this hypothesis, the authors evaluate large language models on three reasoning tasks that have been well-studied in cognitive science - natural language inference, assessing the validity of syllogisms, and the Wason selection task. In all three tasks, they systematically vary the content of the reasoning problems to be believable/realistic, unbelievable, or abstract/nonsensical. They find that the language models mirror patterns of human reasoning biases, performing better on problems with believable content compared to unbelievable or abstract content across all three reasoning tasks.In summary, the central hypothesis is that language models will show human-like biases in reasoning that depend on the content of the task, and the experiments confirm this hypothesis across several different reasoning paradigms. Evaluating the models in this way provides insight into the factors that affect their reasoning abilities.


## What is the main contribution of this paper?

Based on my reading of the paper abstract, it seems the main contribution is showing that large language models reflect many of the same patterns of content effects on reasoning that are observed in humans. Specifically, the paper finds that the performance of large language models on logical reasoning tasks depends on the content and context of the reasoning problems in ways that parallel known biases and limitations of human reasoning. For example, the models reason more accurately about logical rules grounded in realistic situations compared to arbitrary abstract rules, mirroring findings from classic experiments like the Wason selection task. The authors argue these findings have implications for both cognitive science and AI safety research in terms of understanding the factors that affect reasoning in intelligent systems. Overall, the key contribution appears to be using language models as a tool to gain insight into human cognitive biases and their computational basis.
