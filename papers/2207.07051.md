# Language models show human-like content effects on reasoning

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is: Do large language models show human-like content effects on reasoning tasks? The authors hypothesize that because large language models are trained on human-generated text, they will pick up on regularities and biases in how humans reason about different types of content. Therefore, the authors predict that language models will show similar biases to humans in how they perform on reasoning tasks depending on the content of the task - performing better on reasoning tasks with familiar, believable content compared to abstract or nonsensical content.To test this hypothesis, the authors evaluate large language models on three reasoning tasks that have been well-studied in cognitive science - natural language inference, assessing the validity of syllogisms, and the Wason selection task. In all three tasks, they systematically vary the content of the reasoning problems to be believable/realistic, unbelievable, or abstract/nonsensical. They find that the language models mirror patterns of human reasoning biases, performing better on problems with believable content compared to unbelievable or abstract content across all three reasoning tasks.In summary, the central hypothesis is that language models will show human-like biases in reasoning that depend on the content of the task, and the experiments confirm this hypothesis across several different reasoning paradigms. Evaluating the models in this way provides insight into the factors that affect their reasoning abilities.


## What is the main contribution of this paper?

Based on my reading of the paper abstract, it seems the main contribution is showing that large language models reflect many of the same patterns of content effects on reasoning that are observed in humans. Specifically, the paper finds that the performance of large language models on logical reasoning tasks depends on the content and context of the reasoning problems in ways that parallel known biases and limitations of human reasoning. For example, the models reason more accurately about logical rules grounded in realistic situations compared to arbitrary abstract rules, mirroring findings from classic experiments like the Wason selection task. The authors argue these findings have implications for both cognitive science and AI safety research in terms of understanding the factors that affect reasoning in intelligent systems. Overall, the key contribution appears to be using language models as a tool to gain insight into human cognitive biases and their computational basis.


## How does this paper compare to other research in the same field?

Here are a few key ways this paper compares to other research in abstract reasoning and content effects:- The use of large language models to investigate human reasoning biases is a relatively new approach. Most prior work has studied these effects in humans directly through behavioral experiments. Using language models allows testing hypotheses and generating new predictions at scale. - The paper comprehensively evaluates three different reasoning paradigms that have been extensively studied in cognitive science - natural language inference, syllogisms, and the Wason selection task. Connecting findings across these diverse tasks strengthens the case that language models mirror systematic human reasoning patterns.- The results support the view that both humans and large language models exhibit context-dependent, content-sensitive reasoning. This contrasts with some assumptions that neural networks behave in a purely heuristic or statistical manner. The paper argues that a unitary model can capture varied human-like behaviors through interaction of knowledge and context.- The paper integrates these findings with other recent work probing systematicity and abstract reasoning abilities of large language models. It emphasizes the limitations and biases in language model reasoning, while also highlighting how targeted training may improve model reasoning.- The cognitive modeling approach complements other lines of research using language models to reconstruct patterns of human neural activity during language processing. Linking behavioral and neural levels of analysis is an exciting direction for future work.In summary, this paper makes connections between several literatures on human reasoning, cognitive biases, and language model capabilities. The empirical findings advance our understanding of the mechanisms underlying human-like reasoning in large models. The work highlights opportunities to further develop and improve reasoning abilities in language models through targeted training.
