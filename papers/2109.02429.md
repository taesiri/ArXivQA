# [Learning Neural Causal Models with Active Interventions](https://arxiv.org/abs/2109.02429)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the key points of this paper are:- The paper proposes a method for learning causal models with neural networks from a combination of observational and interventional data. - The main research question is how to actively and adaptively select the best interventions to perform in order to efficiently learn the underlying causal structure.- The paper introduces an "Active Intervention Targeting" (AIT) method to select the most informative interventions during the learning process. - AIT computes a score for each possible intervention target indicating how informative intervening on that target would be. It then selects the target with the highest score to intervene on next.- The score is based on measuring the discrepancy between hypothetical post-intervention samples drawn from different candidate causal graphs. High discrepancy suggests high uncertainty about the relationships around that target.- AIT can be integrated into neural network frameworks for causal discovery like SDI and DCDI to guide the interventional experiments.- Through experiments on synthetic and real-world datasets, AIT is shown to improve sample efficiency and identification of the true causal graph compared to random intervention selection.In summary, the key hypothesis is that actively targeting interventions using a discrepancy-based score will enable more efficient learning of neural network causal models from interventional data. The paper provides empirical validation of this hypothesis.


## What is the main contribution of this paper?

Based on my reading, the main contributions of this paper are:- It proposes an active intervention targeting (AIT) method to enable quick identification of the underlying causal structure from observational and interventional data. - AIT computes a score for each possible intervention target to select the most informative target. The score captures the discrepancy between hypothetical post-interventional distributions generated under different candidate causal graph structures.- The paper introduces a two-phase DAG sampling technique to efficiently generate hypothesis DAGs from a structural belief parameterized as a soft adjacency matrix.- It demonstrates how AIT can be integrated into recent differentiable causal discovery frameworks like SDI and DCDI to guide the exploration using interventional data.- Through extensive experiments on simulated and real-world datasets, the paper shows that AIT leads to superior performance over random intervention targeting across different frameworks. AIT reduces the required number of interventions, improves sample efficiency, convergence speed, and robustness to noise.In summary, the key contribution is an active intervention targeting method that enables more efficient causal structure discovery by adaptively selecting the most informative interventions based on the current state of a learned neural causal model.


## How does this paper compare to other research in the same field?

This paper proposes a new method for learning causal models with neural networks using active interventions. Here is a brief comparison to other related work:- Most prior work on neural network-based causal discovery has focused on learning from observational data or data with fixed/random interventions. This paper introduces an adaptive intervention design technique called Active Intervention Targeting (AIT) to actively choose informative interventions.- For active causal learning, many existing approaches rely on starting from a known causal graph and selecting interventions to orient edges. In contrast, this paper starts without a known graph and uses AIT to progressively build up causal knowledge.- Bayesian active learning methods like ABC-D/ABCD use information gain to select interventions. This paper presents a frequentist score-based approach more naturally suited to the differentiable neural network frameworks.- Prior graph-theoretic active learning selects interventions based on graph properties like cliques. AIT uses a novel score based on discrepancies of generated samples across possible graphs to choose interventions.- Most active learning is focused on discrete frameworks. A strength of this paper is demonstrating benefits of active interventions in both discrete and continuous optimization frameworks.In summary, the key innovations of this paper compared to prior work are:1) Introducing adaptive, score-based intervention design for neural network causal discovery.2) Not relying on a known initial graph structure.3) Showing benefits in both discrete and continuous settings.4) Using sample discrepancies rather than graph properties or information gain to select interventions.This provides a new approach to actively learn unknown causal graphs in the emerging area of neural causal discovery. The experiments demonstrate superior performance over existing methods on a range of benchmarks.


## What future research directions do the authors suggest?

Based on my reading of the paper, some of the key future research directions suggested by the authors include:- Developing methods to learn more complex neural causal models, such as those with hidden confounders or selection variables. The authors state that extending the proposed methods to handle latent variables and selection bias would be an interesting direction.- Scaling up the methods to larger graphs. The authors note that dealing with larger variable sets poses optimization challenges, and developing techniques to handle high-dimensional graphs is an important research direction.- Incorporating additional biases and priors. The authors suggest integrating appropriate inductive biases and causal priors could guide the learning and improve generalization. - Exploring broader classes of interventions. The current methods focus on hard interventions, but extending to context-specific, conditional, and soft interventions is an interesting direction.- Developing theoretical understandings of identifiability. The authors note that characterizing theoretical identifiability guarantees for these methods is an open question for future work.- Combining neural causal models with other representations like structural equation models. Integrating neural networks with more explicit causal variable relationships is suggested as a promising direction.- Validating the methods on more real-world datasets. Testing the neural causal discovery methods on complex real-world causal learning tasks is needed.In summary, the key future directions highlighted are developing more flexible and scalable neural causal models, incorporating useful inductive biases, expanding the classes of interventions handled, establishing theoretical guarantees, integrating with other causal representations, and thorough empirical testing on real-world problems.


## Summarize the paper in one paragraph.

The paper presents an active intervention targeting (AIT) method for neural causal discovery. The key ideas are:- Neural causal discovery methods aim to learn causal structures from observational and interventional data using neural networks with differentiable parameters. Existing methods use fixed datasets of observational or random interventional data. - AIT enables adaptive acquisition of interventional data to quickly identify the underlying causal structure. It computes a score for each possible intervention target indicating how informative an intervention would be. The score measures discrepancies between hypothetical post-interventional sample distributions generated across likely graph structures. - AIT preferentially selects interventions that maximize disagreement across candidate graphs, rapidly honing in on the true structure. It significantly reduces the number of required interventions compared to random targeting.- AIT can be incorporated into any neural causal discovery method providing access to structural and functional parameters. Experiments across frameworks and datasets demonstrate superior performance over random intervention selection.- AIT enables more rapid, robust and controlled discovery of causal structures. The distribution of chosen interventions correlates with node topological properties, avoiding redundant interventions. AIT is widely applicable and improves sample efficiency.


## Summarize the paper in two paragraphs.

Here is a 2 paragraph summary of the paper:The paper presents a method called Active Intervention Targeting (AIT) for neural network-based causal discovery from a combination of observational and interventional data. Causal discovery aims to learn the causal structure, in the form of a directed acyclic graph (DAG), between variables from data. Existing neural network methods for causal discovery have focused on passive learning from fixed observational and interventional datasets. In contrast, AIT enables active causal discovery by adaptively selecting the best interventions to perform in order to quickly identify the underlying causal DAG. AIT works by computing a score for each possible intervention target measuring how informative intervening on that target would be. It generates hypothetical samples for each intervention target across multiple possible causal graphs sampled from the current model. It then computes statistics comparing these hypothetical samples, in particular the discrepancy between graphs, to score each target. AIT intervenes on the target that maximizes this discrepancy score, as targets that produce very different distributions across graphs are most informative. Experiments across simulated and real-world benchmark datasets demonstrate AIT identifies the causal DAG faster and with fewer samples than passive/random intervention selection.
