# [Interpretability at Scale: Identifying Causal Mechanisms in Alpaca](https://arxiv.org/abs/2305.08809)

## What is the central research question or hypothesis that this paper addresses?

This paper addresses the question of how to scale causal interpretability methods to large language models (LLMs) with billions of parameters, in order to uncover the interpretable algorithmic structure underlying their behavior. The central hypothesis is that the Alpaca LLM solves a simple numerical reasoning task by implementing a causal model with interpretable boolean variables, and that the alignment between these variables and Alpaca's neural representations is robust to changes in the inputs and task details.Specifically, the paper introduces a new method called Boundless Distributed Alignment Search (\newDAS) to efficiently search for causal structure in large models like Alpaca. The authors then apply \newDAS to study how Alpaca follows basic instructions on a "price tagging" task. They hypothesize causal models involving boundary checks on the input values and find evidence that Alpaca implements these simple algorithms internally. Further experiments suggest the discovered alignments generalize across variations in the task setup.In summary, the paper aims to scale causal interpretability to large models and provide initial evidence that they can solve simple reasoning tasks by implementing human-interpretable algorithms, with robust variable alignments, rather than opaque computations dependent on specific inputs. This represents an important step toward understanding and providing guarantees about the behavior of widely used LLMs.
