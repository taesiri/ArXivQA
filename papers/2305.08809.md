# [Interpretability at Scale: Identifying Causal Mechanisms in Alpaca](https://arxiv.org/abs/2305.08809)

## What is the central research question or hypothesis that this paper addresses?

 This paper addresses the question of how to scale causal interpretability methods to large language models (LLMs) with billions of parameters, in order to uncover the interpretable algorithmic structure underlying their behavior. The central hypothesis is that the Alpaca LLM solves a simple numerical reasoning task by implementing a causal model with interpretable boolean variables, and that the alignment between these variables and Alpaca's neural representations is robust to changes in the inputs and task details.Specifically, the paper introduces a new method called Boundless Distributed Alignment Search (\newDAS) to efficiently search for causal structure in large models like Alpaca. The authors then apply \newDAS to study how Alpaca follows basic instructions on a "price tagging" task. They hypothesize causal models involving boundary checks on the input values and find evidence that Alpaca implements these simple algorithms internally. Further experiments suggest the discovered alignments generalize across variations in the task setup.In summary, the paper aims to scale causal interpretability to large models and provide initial evidence that they can solve simple reasoning tasks by implementing human-interpretable algorithms, with robust variable alignments, rather than opaque computations dependent on specific inputs. This represents an important step toward understanding and providing guarantees about the behavior of widely used LLMs.


## What is the main contribution of this paper?

 The main contribution of this paper is presenting Boundless Distributed Alignment Search (\newDAS), a novel method that enables scaling causal interpretability methods to large language models with billions of parameters. Specifically, the key contributions are:- Proposing \newDAS, which replaces the remaining brute-force search aspect of prior work (DAS) with learned parameters. This enables efficient alignment search at scale.- Applying \newDAS to analyze how the 7B parameter Alpaca model solves a simple numerical reasoning task when following instructions. The analysis discovers Alpaca implements the task using a causal model with two interpretable boolean variables.- Demonstrating the alignments learned by \newDAS generalize robustly to new inputs and instructions. This provides evidence that the discovered causal mechanisms reflect inherent structure in Alpaca rather than idiosyncrasies of specific prompts.- More broadly, this work represents an initial step towards explaining the inner workings of large language models in human-interpretable terms, which is critical for issues of safety, trustworthiness, and fairness. The presented methods scale existing techniques grounded in causal abstraction theory.In summary, the key contribution is advancing the state-of-the-art in interpretable explanations of large language models by developing and applying the \newDAS method to uncover causal structure at scale. The analyses yield new insights about the internal algorithms implemented in models like Alpaca.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the paper:The paper introduces Boundless Distributed Alignment Search (Boundless DAS), a method that scales causal interpretability techniques to large language models by replacing brute-force search with learned parameters, and applies it to show that the 7B parameter Alpaca model implements a simple causal algorithm with interpretable variables to follow numerical reasoning instructions.


## How does this paper compare to other research in the same field?

 Here are a few key ways this paper compares to other related research:- It focuses on interpretability methods for large language models (LLMs), which most prior work has not tackled. Interpretability for complex neural networks is an active area, but LLMs present unique challenges due to their massive scale.- The paper grounds its approach in causal abstraction theory. This provides a principled framework for defining what it means for an interpretation to faithfully reflect a model's internal workings. Most prior interpretability methods do not have an explicit causal theory underlying them.- The proposed method, Boundless DAS, builds on Distributed Alignment Search (DAS) but makes key innovations to enable scaling up. DAS was previously only applied to small fine-tuned models. Boundless DAS introduces learned parameters to avoid brute-force search.- The experiments focus on analyzing an off-the-shelf 7B parameter LLM (Alpaca) following textual instructions. This is a practical application area where interpretability is very relevant. Most prior work looks at models trained specifically for interpretability.- The paper demonstrates that alignments found by Boundless DAS are robust across variations in inputs/instructions. Assessing generalization is crucial but often overlooked in interpretability work.- Limitations include only evaluating on a simple numerical reasoning task, and being unable to provide guarantees about finding all interpretable structure. But the approach shows promise on an important open problem.In summary, this paper makes contributions in scaling up interpretability methods to very large models and tasks where they are urgently needed. It builds on strong theoretical foundations and rigorously probes the generalization of interpretations.


## What future research directions do the authors suggest?

 Based on my reading of the paper, here are some of the main future research directions suggested by the authors:- Applying Boundless DAS to study even larger language models, such as models with hundreds of billions of parameters. The authors note that as larger LLMs are released and evaluated as having stronger reasoning abilities, Boundless DAS will be ready to analyze them.- Having humans in the loop for testing different hypotheses of high-level causal models. The authors suggest it could be interesting to have humans propose and iterate on interpretive causal models to test with Boundless DAS. - Tightening the connection between interchange intervention accuracy (IIA) and task performance by modeling errors of the language model in more detail. The authors note that where task performance is low, IIA can still find causal structure, but future work should tie these metrics together more closely.- Exploring highly non-linear representations of variables, which may currently be missed by Boundless DAS. The method still relies on linear subspaces for alignments.- Adapting Boundless DAS to search for alignments between heads and high-level concepts, enabling circuit finding in Transformers. The authors note this could be done by adding a shared rotation matrix over heads.- Developing new techniques to scale methods like Boundless DAS to layer-level alignments in huge models, which is currently intractable due to memory constraints.In summary, the main suggestions are to scale the approach to even larger models, incorporate human feedback, handle errors and non-linearity better, adapt the method for circuit finding, and develop techniques to align full layers in huge models. The authors present Boundless DAS as an initial step toward interpreting large language models, but suggest many promising avenues for future work.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the paper:The paper introduces Boundless Distributed Alignment Search (Boundless DAS), a novel method for scaling alignment search of causal structure in large language models (LLMs) with billions of parameters. The key innovation is replacing the remaining brute-force search aspects of prior work on Distributed Alignment Search (DAS) with learned parameters, enabling efficient search for interpretable causal mechanisms in LLMs. The authors apply Boundless DAS to analyze how the 7B parameter Alpaca model solves a simple numerical reasoning task when given instructions. They discover Alpaca implements the task using a causal model with two interpretable boolean variables representing boundary checks. Further experiments show this simple causal model is robustly used by Alpaca across variations in inputs and instructions. Overall, the paper makes progress toward faithfully explaining the inner workings of large, widely deployed LLMs using human-interpretable causal models.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the paper:This paper introduces Boundless Distributed Alignment Search (Boundless DAS), a new method for scaling alignment search to uncover interpretable causal structure in large language models (LLMs). Boundless DAS builds on prior work in Distributed Alignment Search (DAS) by replacing the remaining brute-force search aspects with learned parameters. This enables efficient searching for alignments between distributed neural representations in LLMs and variables in interpretable causal models. The authors apply Boundless DAS to analyze how the 7B parameter Alpaca model solves a simple numerical reasoning task when given instructions. They discover Alpaca implements the task using a causal model with two interpretable boolean variables representing boundary checks. Further experiments show Alpaca robustly uses this simple algorithm across variations in the task and instructions. Overall, this work represents an important step toward explaining the inner workings of large, widely used LLMs like Alpaca in human-interpretable terms, which is crucial for AI safety and auditing.


## Summarize the main method used in the paper in one paragraph.

 Here is a one paragraph summary of the main method used in the paper:The paper introduces Boundless Distributed Alignment Search (\newDAS), an extension of Distributed Alignment Search (DAS), to scale causal explainability methods to large language models (LLMs) with billions of parameters. \newDAS replaces the brute-force search over neural dimensionality in DAS with learned boundary parameters that dynamically identify the dimensions needed to represent abstract variables. This enables efficient gradient-based search for alignments between interpretable causal variables and distributed representations in LLMs. The authors apply \newDAS to analyze how the 7B parameter Alpaca model solves a simple numerical reasoning task when following instructions. They discover Alpaca implements a causal model with two interpretable boolean variables representing boundary checks, and show this aligns well across layers and generalizes across variations in inputs and task details. Overall, \newDAS enables searching for causal structure in very large neural networks like Alpaca in a more scalable way.
