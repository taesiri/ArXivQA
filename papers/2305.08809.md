# [Interpretability at Scale: Identifying Causal Mechanisms in Alpaca](https://arxiv.org/abs/2305.08809)

## What is the central research question or hypothesis that this paper addresses?

This paper addresses the question of how to scale causal interpretability methods to large language models (LLMs) with billions of parameters, in order to uncover the interpretable algorithmic structure underlying their behavior. The central hypothesis is that the Alpaca LLM solves a simple numerical reasoning task by implementing a causal model with interpretable boolean variables, and that the alignment between these variables and Alpaca's neural representations is robust to changes in the inputs and task details.Specifically, the paper introduces a new method called Boundless Distributed Alignment Search (\newDAS) to efficiently search for causal structure in large models like Alpaca. The authors then apply \newDAS to study how Alpaca follows basic instructions on a "price tagging" task. They hypothesize causal models involving boundary checks on the input values and find evidence that Alpaca implements these simple algorithms internally. Further experiments suggest the discovered alignments generalize across variations in the task setup.In summary, the paper aims to scale causal interpretability to large models and provide initial evidence that they can solve simple reasoning tasks by implementing human-interpretable algorithms, with robust variable alignments, rather than opaque computations dependent on specific inputs. This represents an important step toward understanding and providing guarantees about the behavior of widely used LLMs.


## What is the main contribution of this paper?

The main contribution of this paper is presenting Boundless Distributed Alignment Search (\newDAS), a novel method that enables scaling causal interpretability methods to large language models with billions of parameters. Specifically, the key contributions are:- Proposing \newDAS, which replaces the remaining brute-force search aspect of prior work (DAS) with learned parameters. This enables efficient alignment search at scale.- Applying \newDAS to analyze how the 7B parameter Alpaca model solves a simple numerical reasoning task when following instructions. The analysis discovers Alpaca implements the task using a causal model with two interpretable boolean variables.- Demonstrating the alignments learned by \newDAS generalize robustly to new inputs and instructions. This provides evidence that the discovered causal mechanisms reflect inherent structure in Alpaca rather than idiosyncrasies of specific prompts.- More broadly, this work represents an initial step towards explaining the inner workings of large language models in human-interpretable terms, which is critical for issues of safety, trustworthiness, and fairness. The presented methods scale existing techniques grounded in causal abstraction theory.In summary, the key contribution is advancing the state-of-the-art in interpretable explanations of large language models by developing and applying the \newDAS method to uncover causal structure at scale. The analyses yield new insights about the internal algorithms implemented in models like Alpaca.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper introduces Boundless Distributed Alignment Search (Boundless DAS), a method that scales causal interpretability techniques to large language models by replacing brute-force search with learned parameters, and applies it to show that the 7B parameter Alpaca model implements a simple causal algorithm with interpretable variables to follow numerical reasoning instructions.
