# On Evaluating Adversarial Robustness of Large Vision-Language Models

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is: How vulnerable are large vision-language models (VLMs) to adversarial attacks, especially in a black-box threat model with targeted adversarial goals? More specifically, the paper investigates whether adversaries can craft targeted adversarial examples against surrogate models like CLIP and then transfer them to fool other VLMs into generating specific targeted responses. The hypothesis appears to be that large VLMs have security vulnerabilities that could be exploited by such black-box transfer-based attacks and query-based attacks to produce targeted outputs. The paper quantitatively evaluates this hypothesis by attacking various VLMs like MiniGPT-4, LLaVA, UniDiffuser, BLIP-2, and Img2Prompt using a combination of transfer-based and query-based strategies. The high success rates obtained for generating targeted responses suggest these models are indeed vulnerable to such adversarial attacks.In summary, the central research question is assessing the adversarial robustness of large VLMs, especially against black-box attacks seeking targeted responses. The hypothesis is that these models are vulnerable, which is supported by the paper's experiments.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper proposes and evaluates black-box adversarial attack methods to craft targeted adversarial examples against vision-language models by first generating attacks against surrogate models like CLIP and then transferring them to other models, showing these models are vulnerable to such attacks.


## How does this paper compare to other research in the same field?

Here are a few key points comparing this paper to other related work on adversarial robustness of vision-language models:- The paper focuses specifically on evaluating large vision-language models (e.g. BLIP, MiniGPT-4) rather than smaller CNN-RNN models commonly studied in prior work. This is an important extension as large VLMs are becoming more widely deployed.- The threat model considers a realistic black-box setting where the adversary only has query access to model APIs, and seeks targeted responses rather than just untargeted errors. Many previous papers assume white-box access or less restrictive threat models.- The attacks are fully automated without any human involvement, unlike some prior work that incorporates human interaction for constructing natural language or visual adversarial examples.- Transfer-based attacks are explored using CLIP/BLIP as surrogates, as well as query-based attacks to improve transferability. The combination provides high attack success rates.- The paper provides a fairly comprehensive benchmark evaluating different model architectures (e.g. BLIP, UniDiffuser, MiniGPT) across modalities like image captioning, VQA, joint generation.- The focus is strictly on digital adversarial examples. Evaluating physical or real-world attacks remains an open challenge for future work.Overall, this paper provides an extensive benchmark and analysis of adversarial vulnerabilities of large VLMs using more realistic threat models compared to much of the prior work. The results highlight the needs for further robustness improvements before deployment of these models.
