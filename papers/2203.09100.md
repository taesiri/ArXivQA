# PLANET: Dynamic Content Planning in Autoregressive Transformers for   Long-form Text Generation

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is:How can we improve long-form text generation through more effective content planning and coherence modeling? Specifically, the paper proposes a new framework called PLANET that aims to address two key challenges in long-form text generation:1) Conducting proper content selection and ordering to form a coherent high-level logical flow. This involves deciding "what to say and when to say it".2) Appropriately reflecting the content plans in the final text generation. This involves deciding "how to say it". To address these challenges, the central hypothesis of the paper is that dynamically performing content planning and surface realization in an end-to-end fashion within large autoregressive language models like BART can lead to improved coherence and content richness in long-form text generation. The key ideas proposed to test this hypothesis are:- Introducing latent representations to represent sentence-level semantic plans that guide surface generation.- Using a bag-of-words prediction task to supervise the latent representations. - Employing a coherence-based contrastive learning objective to distinguish coherent and incoherent texts.So in summary, the central research question is how to improve long-form text generation through more dynamic content planning and coherence modeling, with the hypothesis that the proposed PLANET framework can achieve this by conducting planning and realization together in an end-to-end fashion. The core ideas are the latent planning representations and contrastive learning objective.


## What is the main contribution of this paper?

The main contributions of this paper are:- It proposes a new text generation framework called PLANET that dynamically performs content planning and surface realization in autoregressive Transformers. - It introduces latent representations for each target sentence to capture the overall semantics and guide the generation process. These latent representations are supervised with a sentence-level bag-of-words prediction task.- It presents a novel coherence-based contrastive learning method to improve the coherence of generated texts, with different strategies to construct negative samples. - Experiments on two long-form opinion generation tasks - counter-argument generation and opinion article generation - show that PLANET outperforms strong baseline methods in both automatic evaluations and human evaluations.In summary, the key contribution is a new end-to-end framework that unifies dynamic content planning and surface realization in autoregressive Transformers, guided by latent representations and trained with contrastive learning. This improves coherence and content richness for long-form opinion text generation.
