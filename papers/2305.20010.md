# [Human or Not? A Gamified Approach to the Turing Test](https://arxiv.org/abs/2305.20010)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, this white paper does not appear to have a single focused research question or hypothesis. Instead, it provides an overview and analysis of an online social experiment called "Human or Not?" which was designed as a large-scale Turing test. The key elements of the paper are:- Describing the motivation, design principles, and development of the "Human or Not?" game platform. This includes details on the conversational AI bots created to participate in chats.- Presenting initial results and analysis from over 1.5 million users who played the game over one month. This examines the strategies people used to distinguish humans from bots, and bots' abilities to mimic human behavior. - Discussing the limitations of the experiment and its results in terms of generalizability and potential biases.- Drawing insights related to the capabilities of current AI systems, human perceptions of AI, and design of human-AI interactions.So in summary, this paper does not have one focused research question, but rather uses the "Human or Not?" game as a case study to provide a broad analysis of an AI Turing test experiment and its implications. The key goal seems to be presenting the platform itself as a novel way to quantitatively track AI progress in natural language interaction.


## What is the main contribution of this paper?

Based on my reading, the main contributions of this paper are:1. It presents the design, development, and results of the "Human or Not?" experiment, which is a large-scale online Turing-like test that engaged over 1.5 million users. 2. It provides an analysis of over 10 million human-AI and human-human conversations from this experiment, examining the strategies users employed to distinguish humans from AIs and vice versa.3. The results offer insights into the current capabilities of AI language models in mimicking human conversation, as well as how humans perceive and interact with AI systems. 4. In particular, the overall 68% accuracy rate in guessing the identity of conversation partners echoes the prediction made by Alan Turing in 1950 that an "average interrogator" would have less than 70% chance of making the right identification after 5 minutes of questioning.5. The analysis of common human strategies and AI capabilities contributes to our understanding of human-AI interaction and has implications for the future development and deployment of conversational AI systems.6. The experiment methodology and dataset provide a statistically robust baseline for tracking progress in AI language models on Turing-like tests over time.In summary, this paper makes both practical and theoretical contributions through its novel gamified Turing test experiment and the insights gained into current human-AI conversational abilities. The results represent an important milestone in AI progress and human-AI interaction research.
