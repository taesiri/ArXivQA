# [Generator Born from Classifier](https://arxiv.org/abs/2312.02470)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a high-quality paragraph summarizing the key points of the paper:

This paper explores the novel task of training an image generator directly from a pre-trained classifier without using any training data. The key idea is to leverage the theory of Maximum-Margin Bias, which shows that the parameters of a neural network trained with gradient descent converge to the solution of an optimization problem that maximizes the classification margin. Based on the necessary conditions for the solution to this optimization problem, the authors devise a method to train a generator such that it produces images satisfying these conditions for the pre-trained classifier. Specifically, the loss function ensures the generator guarantees good performance of the classifier on the generated data distribution, and that the current classifier parameters are the convergence point under this distribution. Experiments on MNIST and CelebA datasets demonstrate the efficacy of the proposed strategy, with the generator able to perform conditional sampling of digits and faces despite never seeing real images. The method is also extended to incorporate multiple pre-trained classifiers into one generator. This explores a novel direction for reusing predictive models to derive generative capabilities without needing extra training data.
