# [Fluctuation-based Adaptive Structured Pruning for Large Language Models](https://arxiv.org/abs/2312.11983)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem: 
Large language models (LLMs) like GPT-3 have hundreds of billions of parameters, making them very costly to deploy and run inferences on. Existing model compression techniques like pruning are not feasible at this scale. Structured pruning reduces parameters and speeds up inference without specialized hardware, but requires new metrics to evaluate the importance of entire rows/columns. However, current LLM pruning metrics only consider individual weights, not whole structures.

Proposed Solution - FLAP:
This paper proposes a structured pruning framework called FLuctuation-based Adaptive Structured Pruning (FLAP) that meets 3 criteria:

1) Structured importance metric: FLAP proposes a "fluctuation metric" that measures variation of each input channel relative to its baseline value on calibration samples. This estimates recoverability of output if a column is removed.

2) Adaptive search for global compression: Scores are standardized across layers/modules for unified comparison. An adaptive search identifies the globally optimal compressed structure. 

3) Bias compensation to minimize performance loss: Additional bias terms using baseline values are added to recover output maps. This avoids expensive retraining.

Main Contributions:
- First retraining-free structured pruning method for LLMs 
- Introduces bias compensation to replace retraining for performance recovery
- Outperforms state-of-the-art pruning methods without any retraining
- Maintains accuracy and speeds up inference, enabling efficient deployment

The method is evaluated on the LLaMA family, outperforming techniques like LLM-Pruner and Wanda extensions while avoiding retraining. This facilitates the deployment of efficient LLM models for various applications.
