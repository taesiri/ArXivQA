# [Everybody Prune Now: Structured Pruning of LLMs with only Forward Passes](https://arxiv.org/abs/2402.05406)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
As large language models (LLMs) continue to grow in size and capability, the hardware requirements to run them are becoming inaccessible to most practitioners. Methods like pruning, distillation, and quantization have been proposed to compress larger models into smaller ones that are more usable. However, these compression techniques themselves tend to be very resource intensive, requiring large amounts of memory, compute, and data. This puts them out of reach for many potential users that could benefit from smaller derived models. Specifically, existing structured pruning methods have prohibitive memory requirements during training/pruning, needing several times more memory than just running inference on the original large model.

Proposed Solution: 
The paper proposes Bonsai, a new perturbative structured pruning technique that relies exclusively on forward passes through the model being pruned. This allows it to work under strict memory constraints where the user only has enough memory to run inference on the large parent model. 

Bonsai estimates the relevance of each module (e.g. attention heads, MLP dimensions) in the model by:
1) Sampling a manageable number of sub-models by dropping different modules according to an informative prior based on activation statistics. 
2) Evaluating the performance of each sub-model using forward passes.
3) Framing the module relevance estimation problem as an under-determined regression problem to infer relevance scores for all modules, even those not sampled.
4) Pruning the lowest relevance modules first according to the estimated scores.

By taking a holistic view of all modules and layers simultaneously, iteratively re-estimating priorities, and leveraging priors, Bonsai is able to accurately prune models using only forward passes.

Main Contributions:
- First effective structured pruning method for large language models that works exclusively with forward passes, enabling use under strict memory constraints
- Novel formulation of module relevance estimation as an under-determined regression problem based on performance of sampled sub-models
- Demonstrates strong performance compared to prior gradient-based and semi-structured methods, achieving higher speedups and accuracy
- Enables practitioners to create their own compressed LLMs customized to their hardware constraints and use cases

The method opens up structured pruning to a wider range of practitioners and hardware configurations by reducing memory overhead. It also produces models that are faster at inference time compared to common alternatives.
