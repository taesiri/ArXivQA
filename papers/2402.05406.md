# [Everybody Prune Now: Structured Pruning of LLMs with only Forward Passes](https://arxiv.org/abs/2402.05406)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
As large language models (LLMs) continue to grow in size and capability, the hardware requirements to run them are becoming inaccessible to most practitioners. Methods like pruning, distillation, and quantization have been proposed to compress larger models into smaller ones that are more usable. However, these compression techniques themselves tend to be very resource intensive, requiring large amounts of memory, compute, and data. This puts them out of reach for many potential users that could benefit from smaller derived models. Specifically, existing structured pruning methods have prohibitive memory requirements during training/pruning, needing several times more memory than just running inference on the original large model.

Proposed Solution: 
The paper proposes Bonsai, a new perturbative structured pruning technique that relies exclusively on forward passes through the model being pruned. This allows it to work under strict memory constraints where the user only has enough memory to run inference on the large parent model. 

Bonsai estimates the relevance of each module (e.g. attention heads, MLP dimensions) in the model by:
1) Sampling a manageable number of sub-models by dropping different modules according to an informative prior based on activation statistics. 
2) Evaluating the performance of each sub-model using forward passes.
3) Framing the module relevance estimation problem as an under-determined regression problem to infer relevance scores for all modules, even those not sampled.
4) Pruning the lowest relevance modules first according to the estimated scores.

By taking a holistic view of all modules and layers simultaneously, iteratively re-estimating priorities, and leveraging priors, Bonsai is able to accurately prune models using only forward passes.

Main Contributions:
- First effective structured pruning method for large language models that works exclusively with forward passes, enabling use under strict memory constraints
- Novel formulation of module relevance estimation as an under-determined regression problem based on performance of sampled sub-models
- Demonstrates strong performance compared to prior gradient-based and semi-structured methods, achieving higher speedups and accuracy
- Enables practitioners to create their own compressed LLMs customized to their hardware constraints and use cases

The method opens up structured pruning to a wider range of practitioners and hardware configurations by reducing memory overhead. It also produces models that are faster at inference time compared to common alternatives.


## Summarize the paper in one sentence.

 Here is a one sentence summary of the paper:

The paper presents Bonsai, a new gradient-free structured pruning method for large language models that can deliver small, fast, and accurate pruned models under the memory constraints typical of consumer hardware by estimating module relevance through evaluating performance on randomly sampled sub-models.


## What is the main contribution of this paper?

 Based on my understanding, the main contribution of this paper is proposing Bonsai, a new gradient-free structured pruning method for large language models (LLMs) that can prune models using only forward passes. Specifically:

- Bonsai allows pruning LLMs even when there is only enough memory to run inference on the model being pruned. This makes Bonsai usable under real-world memory constraints faced by many practitioners.

- Bonsai estimates the importance of modules in the LLM (e.g. attention heads, MLP dimensions) by evaluating the performance of randomly generated sub-models. It formulates this as an under-determined regression problem to estimate module relevance from a small number of sub-models. 

- Bonsai introduces techniques like using informative priors based on unstructured pruning metrics to sample useful sub-models, global decision making instead of greedy layer-wise pruning, and iterative pruning. These make the approach work well for large LLMs.

- Experiments show Bonsai can match or outperform gradient-based structured pruning methods, while using only forward passes. It also produces fast and accurate sub-2B models from larger models that outperform prior work.

In summary, the main contribution is proposing Bonsai, a novel perturbative and gradient-free approach to structured pruning that works under real-world memory constraints and delivers high quality pruned LLMs.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key keywords and terms associated with this work include:

- Structured pruning - The paper focuses on pruning entire modules of a language model (like attention heads, layers, etc.) rather than individual parameters. This is referred to as structured pruning.

- Gradient-free - The proposed pruning method, called Bonsai, relies only on forward passes of the model rather than computing gradients. This makes it more memory-efficient.

- Perturbative - Bonsai estimates the importance of modules in the model by generating sub-models and evaluating their performance. This perturbative approach allows relevance estimation with fewer model evaluations. 

- Under-determined regression - The problem of inferring module importances from evaluated sub-models is posed as an under-determined regression problem, enabling estimation of a large number of modules from a smaller set of sub-models.

- Iterative pruning - Pruning is performed slowly over multiple steps to get better relevance estimates and preserve model accuracy. 

- Low-rank adaptation - After pruning, the model can be further fine-tuned with methods like LoRA that are parameter-efficient to recover lost performance.

- Forward pass only - A key constraint is that the pruning process must rely only on forward passes, allowing application to very large models using limited memory.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in this paper:

1. The paper proposes a gradient-free structured pruning method called Bonsai. What are the key challenges in scaling existing gradient-free pruning methods like SHAPLEY to large language models? How does Bonsai address these challenges?

2. Bonsai treats the problem of estimating module importance from evaluated sub-models as an underdetermined regression problem. Why is this more tractable than methods that require evaluating as many sub-models as there are modules? What assumptions does this regression approach make?

3. Instead of sampling sub-models to evaluate uniformly at random, Bonsai uses informative priors derived from unstructured pruning metrics. Why is this beneficial? How much do the choice of different priors based on metrics like activation magnitude versus Wanda impact end performance?

4. Bonsai takes a more holistic pruning approach instead of the commonly used greedy layer-by-layer approach. What failure modes might the layer-by-layer approach run into that the global approach avoids? Provide a concrete illustrative example.  

5. The paper demonstrates Bonsai's efficacy by pruning large models like LLaMA and Phi-2. How do the results compare to state-of-the-art gradient-based structured pruning methods? Under what settings does Bonsai outperform these approaches and why?

6. Bonsai is shown to work well iteratively, removing a small fraction of modules per iteration. Why might this lead to better relevance estimations and end performance compared to direct one-shot pruning? What is the tradeoff?

7. The paper combines pruning with distillation during the post-pruning finetuning phase. Why might this be beneficial compared to just finetuning on the end task? Does the relative weighting between distillation versus end task objective matter?

8. What are some limitations of the Bonsai method? For instance, how might the runtime scale with the number of perturbative samples and what are potential ways to address this?

9. The paper prunes models to maximize likelihood on a language modeling corpus. How might directly optimizing to preserve performance on end tasks like reasoning improve results? What challenges would this incur?

10. The results are demonstrated on self-attention and MLP modules of transformers. How might Bonsai need to be adapted to effectively prune other model architectures like CNNs? What new challenges might arise?
