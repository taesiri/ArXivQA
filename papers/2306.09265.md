# LVLM-eHub: A Comprehensive Evaluation Benchmark for Large
  Vision-Language Models

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be: How to comprehensively evaluate the capabilities of large vision-language models (LVLMs) in a systematic way?The authors note that while LVLMs have shown impressive progress on various multimodal tasks, there lacks a holistic framework to evaluate their strengths and weaknesses across different capabilities. To address this, the paper proposes the LVLM Evaluation Hub (LVLM-eHub), which encompasses:1) Quantitative capability evaluation - Assessing LVLMs across 6 categories of capabilities (visual perception, knowledge acquisition, reasoning, commonsense, object hallucination, embodied intelligence) using 47 text-related visual benchmarks. 2) Online evaluation with LVLM Arena - Allowing randomized 1v1 battles between LVLMs with crowd-sourced human judgment in an open-world QA setting.Through both quantitative benchmarks and qualitative human evaluation, the LVLM-eHub aims to provide a comprehensive assessment of LVLMs to understand their current abilities and guide future progress. The key research question is how to design such a systematic evaluation framework covering the diverse capabilities of LVLMs.In summary, the central research question is how to holistically evaluate LVLMs across different capabilities in a quantitative and qualitative manner. The LVLM-eHub is proposed as a solution to enable comprehensive benchmarking.
