# LVLM-eHub: A Comprehensive Evaluation Benchmark for Large   Vision-Language Models

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be: How to comprehensively evaluate the capabilities of large vision-language models (LVLMs) in a systematic way?The authors note that while LVLMs have shown impressive progress on various multimodal tasks, there lacks a holistic framework to evaluate their strengths and weaknesses across different capabilities. To address this, the paper proposes the LVLM Evaluation Hub (LVLM-eHub), which encompasses:1) Quantitative capability evaluation - Assessing LVLMs across 6 categories of capabilities (visual perception, knowledge acquisition, reasoning, commonsense, object hallucination, embodied intelligence) using 47 text-related visual benchmarks. 2) Online evaluation with LVLM Arena - Allowing randomized 1v1 battles between LVLMs with crowd-sourced human judgment in an open-world QA setting.Through both quantitative benchmarks and qualitative human evaluation, the LVLM-eHub aims to provide a comprehensive assessment of LVLMs to understand their current abilities and guide future progress. The key research question is how to design such a systematic evaluation framework covering the diverse capabilities of LVLMs.In summary, the central research question is how to holistically evaluate LVLMs across different capabilities in a quantitative and qualitative manner. The LVLM-eHub is proposed as a solution to enable comprehensive benchmarking.


## What is the main contribution of this paper?

Based on my reading, the main contributions of this paper are:1. Proposes LVLM-eHub, the first comprehensive evaluation benchmark for large vision-language models, to the best of the authors' knowledge. 2. LVLM-eHub provides extensive evaluation on 6 key capabilities (visual perception, knowledge acquisition, reasoning, commonsense, hallucination robustness, embodied intelligence) of LVLMs using over 40 text-based visual datasets.3. LVLM-eHub builds an online arena platform that allows user-level evaluation of LVLMs in an open-world question answering scenario through randomized pairwise battles.4. The evaluation reveals several important findings:- Instruction-tuned LVLMs with massive in-domain data like InstructBLIP tend to overfit existing benchmarks but generalize poorly to open-world scenarios. - Moderate instruction tuning can cause object hallucination issues, making evaluation metrics like CIDEr ineffective. - Multi-turn reasoning evaluations can mitigate hallucination issues, highlighting the need for better evaluation pipelines.Overall, this paper makes significant contributions by proposing a comprehensive evaluation framework and benchmark for LVLMs, revealing insights into their capabilities and limitations through extensive experiments, and highlighting opportunities for developing more effective evaluation techniques. The benchmark and findings provide a valuable resource for future LVLM research and development.
