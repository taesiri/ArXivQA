# [Identification of Systematic Errors of Image Classifiers on Rare   Subgroups](https://arxiv.org/abs/2303.05072)

## What is the central research question or hypothesis that this paper addresses?

 The central research question addressed in this paper is: How can we systematically identify cases where image classifiers make errors on rare or underrepresented subgroups, when we do not have labeled examples from those subgroups?

The key ideas and contributions are:

- Leveraging recent advances in text-to-image models like Stable Diffusion to synthesize examples of rare subgroups specified through text prompts. This allows probing classifier performance on subgroups even if real examples are not available.

- A procedure called PromptAttack that explores a large space of subgroup prompts in a principled way using combinatorial testing. This allows identifying subgroups where classifiers show systematically worse performance.

- Demonstrating PromptAttack can uncover known issues like classifying certain images of people as apes. It also finds new subgroups for ImageNet classifiers where performance drops, requiring a conjunction of factors like color, pose, weather etc.

- Proposing a benchmark for evaluating systematic error identification methods, based on intentionally injecting errors on subgroups into zero-shot classifiers.

Overall, the key insight is that text-to-image synthesis can help probe classifier performance on rare subgroups beyond the available training/test data. The paper shows this can uncover known and previously unknown systematic errors, which is useful for improving model robustness, reliability and fairness.


## What is the main contribution of this paper?

 The main contributions of this paper are:

1. Proposing PromptAttack, a new procedure for identifying systematic errors of image classifiers based on synthetically generated images from text-to-image models conditioned on prompts encoding subgroup information. 

2. Using combinatorial testing to explore a large subset of subgroups from an operational design domain, achieving near-equable coverage of the subgroups.

3. Introducing a benchmark for evaluating systematic error identification methods based on zero-shot classifiers constructed from CLIP.

4. Applying PromptAttack to ImageNet classifiers and identifying novel systematic errors on rare subgroups, including fairness issues related to misclassifying people.

In summary, the paper introduces a novel method leveraging advances in text-to-image models to systematically test image classifiers on rare subgroups defined by combinations of semantic attributes. The quantitative and qualitative results demonstrate PromptAttack can effectively identify systematic errors missed by prior work. The main innovations are the conditional image synthesis for subgroup exploration and the use of combinatorial testing for coverage.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence TL;DR summary of the paper:

The paper proposes a method called PromptAttack that uses text-to-image models to synthesize images of rare subgroups and identify cases where image classifiers have systematically worse performance, in order to improve model safety, fairness and robustness.


## How does this paper compare to other research in the same field?

 Here are some key ways this paper compares to other research on identifying systematic errors of image classifiers on rare subgroups:

- Leverages text-to-image models like Stable Diffusion to synthetically generate images of rare subgroups defined by textual prompts. This allows testing classifier performance on rare subgroups without needing actual image samples. Other works like DOMINO and AdaVision rely on available datasets which may lack coverage of rare subgroups.  

- Focuses on rare subgroups that require multiple concurrent semantic shifts, rather than just single shifts. For example, identifying that "rear view + small size + orange color + snowy background" concurrently causes misclassifications, not just one of those factors alone. This is a key advantage over perturbation-based approaches.

- Uses combinatorial testing to efficiently search over combinations of factors defining subgroups. This allows covering a large space of potential subgroups efficiently. Other works tend to focus on generating/retrieving examples without structured subgroup search.

- Does not require a human-in-the-loop for interpreting failure cases, as in AdaVision. This enables fully automated auditing. But it means the approach may identify "false positive" subgroups that humans wouldn't consider coherent.

- Evaluated on real ImageNet classifiers, identifying novel subgroups with high misclassification rates. Demonstrates the approach can find systematic errors in real-world models.

- Proposed benchmark based on "zero-shot systematic errors" in CLIP classifiers provides ground truth for quantitative evaluation. Getting ground truth subgroups with errors is a challenge. 

Overall, leveraging text-to-image synthesis and combinatorial search over textual prompts appears to be a promising approach for efficiently discovering systematic errors on rare subgroups. Key advantages over prior work are not needing datasets with subgroup examples and identifying intersectional failures.
