# VIGC: Visual Instruction Generation and Correction

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is: How can we develop a framework that enables multimodal large language models to generate high-quality instruction-tuning data for vision-language tasks in an autonomous manner?The key hypothesis is that by utilizing the available multimodal large language models (MLLMs) and proposing a new framework consisting of Visual Instruction Generation (VIG) and Visual Instruction Correction (VIC), it is possible to generate diverse, high-quality instruction-tuning data without requiring extensive manual annotation or intervention.In summary, the paper aims to address the problem of scarce high-quality instruction-tuning data for vision-language tasks, by proposing a novel VIGC framework that can automatically generate such data leveraging existing MLLMs. The central hypothesis is that this framework with VIG and VIC components can produce high-quality and diverse data for enhancing vision-language models.


## What is the main contribution of this paper?

The main contributions of this paper are:1. It introduces the Visual Instruction Generation and Correction (VIGC) framework, which can autonomously generate high-quality image-text instruction fine-tuning datasets for multimodal models. The framework consists of two components - Visual Instruction Generation (VIG) to generate initial visual question-answer pairs, and Visual Instruction Correction (VIC) to iteratively correct any inaccuracies/hallucinations in the generated data.2. It provides the first multimodal instruction fine-tuning dataset generated fully automatically by a multimodal large language model (MLLM), without any human annotation. Specifically, it generates the VIGC-LLaVA-COCO-extra dataset with 36,781 instances and the much larger VIGC-LLaVA-Objects365 dataset with around 1.8 million instances. 3. It validates the quality of the generated datasets through extensive experiments. Fine-tuning mainstream models like LLaVA and InstructBLIP on the VIGC datasets leads to improved performance on standard multimodal benchmarks like MMBench, LLaVA-Eval, OKVQA and A-OKVQA. The generated data helps compensate for the limitations of language-only data generation methods.4. It demonstrates a new capability of multimodal models to self-enhance through iterative data generation and model training cycles. The generated data improves model performance, and the improved model can in turn generate even better quality data.In summary, the key innovation is an automated framework to create high-quality instruction tuning data for multimodal models, eliminating the need for costly manual annotation. The generated datasets enhance model performance across diverse benchmarks.
