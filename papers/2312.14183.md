# [On Early Detection of Hallucinations in Factual Question Answering](https://arxiv.org/abs/2312.14183)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Large language models (LLMs) can sometimes hallucinate incorrect facts when used for open-ended question answering. 
- Detecting these hallucinations is challenging since the generated text often looks coherent even when incorrect.

Proposal: 
- The paper proposes using different artifacts associated with the model's generation process to detect hallucinations. These artifacts span the whole generation pipeline, from the output layer, to the intermediate layers, back to the input layer.

Artifacts Studied:
1) Softmax probabilities of generated tokens 
2) Integrated Gradients (IG) token attributions 
3) Self-attention scores  
4) Activations in the fully-connected layers

The paper hypothesizes that the distributions of these artifacts differ between hallucinated and non-hallucinated generations.

Approach:  
- The artifacts are extracted and input to binary classifiers to categorize model outputs as hallucinations or not.
- Multiple classifiers are trained, each using one type of artifact.

Results:
- Qualitative analysis confirms distributions differ for hallucinated vs non-hallucinated outputs.  
- Classifiers using self-attention and fully-connected activations perform best, achieving over 0.70 AUROC across datasets/models.
- Performance is better for later Transformer layers.
- Classifiers can detect hallucinations even before the first token is generated.

Main Contributions:
- Showing model artifacts can effectively detect hallucinations before they occur
- Demonstrating classifiers built on artifacts like self-attention can identify hallucinations with significantly better than random accuracy
- Analysis showing trends persist across datasets and LLMs.
