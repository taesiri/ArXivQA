# [Novel Policy Seeking with Constrained Optimization](https://arxiv.org/abs/2005.10696)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the key points of this paper are:- It aims to enable reinforcement learning agents to discover diverse and novel solutions to a problem, rather than converging to a single optimal policy. - It proposes a new metric space to quantitatively measure policy differences, based on the Wasserstein metric. - It frames the problem of seeking novel policies as a constrained optimization problem, rather than a multi-objective optimization problem as in prior work. This avoids performance degradation on the main task.- It develops a practical algorithm called Interior Policy Differentiation (IPD) for constrained novelty-seeking. IPD uses an implicit barrier approach reminiscent of interior point methods in constrained optimization.- Experiments in MuJoCo locomotion tasks show IPD can generate varied policies without sacrificing performance. It outperforms prior multi-objective approaches like WSR and TNB on both novelty and performance.In summary, the central hypothesis is that formulating novel policy search as constrained optimization will enable better diversity while maintaining task performance, compared to prior multi-objective approaches. The paper proposes a new metric, formulation, and algorithm to demonstrate this hypothesis empirically.


## What is the main contribution of this paper?

Based on my reading of the abstract and introduction, the main contributions of this paper appear to be:1. They propose a new metric to measure the difference between policies that provides instant feedback at every timestep. This addresses limitations of prior novelty/diversity metrics that were computationally expensive or only provided episodic reward. 2. They formulate the problem of generating novel policies as a constrained optimization problem rather than multi-objective optimization. This avoids hindering performance on the primary task while still encouraging diversity. 3. They propose a practical algorithm called Interior Policy Differentiation (IPD) for constrained optimization of novel policies. This is inspired by interior point methods in constrained optimization. A key advantage is it does not depend on the scaling of the novelty reward.4. They demonstrate experimentally that IPD can generate diverse and well-performing policies on MuJoCo locomotion tasks, outperforming prior multi-objective approaches as well as constrained optimization baselines.In summary, the main contribution appears to be proposing a new formulation, metric, and algorithm for generating novel policies while maintaining primary task performance. The constrained optimization perspective and IPD method seem to be the key novel ideas proposed.
