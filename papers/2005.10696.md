# [Novel Policy Seeking with Constrained Optimization](https://arxiv.org/abs/2005.10696)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the key points of this paper are:- It aims to enable reinforcement learning agents to discover diverse and novel solutions to a problem, rather than converging to a single optimal policy. - It proposes a new metric space to quantitatively measure policy differences, based on the Wasserstein metric. - It frames the problem of seeking novel policies as a constrained optimization problem, rather than a multi-objective optimization problem as in prior work. This avoids performance degradation on the main task.- It develops a practical algorithm called Interior Policy Differentiation (IPD) for constrained novelty-seeking. IPD uses an implicit barrier approach reminiscent of interior point methods in constrained optimization.- Experiments in MuJoCo locomotion tasks show IPD can generate varied policies without sacrificing performance. It outperforms prior multi-objective approaches like WSR and TNB on both novelty and performance.In summary, the central hypothesis is that formulating novel policy search as constrained optimization will enable better diversity while maintaining task performance, compared to prior multi-objective approaches. The paper proposes a new metric, formulation, and algorithm to demonstrate this hypothesis empirically.


## What is the main contribution of this paper?

Based on my reading of the abstract and introduction, the main contributions of this paper appear to be:1. They propose a new metric to measure the difference between policies that provides instant feedback at every timestep. This addresses limitations of prior novelty/diversity metrics that were computationally expensive or only provided episodic reward. 2. They formulate the problem of generating novel policies as a constrained optimization problem rather than multi-objective optimization. This avoids hindering performance on the primary task while still encouraging diversity. 3. They propose a practical algorithm called Interior Policy Differentiation (IPD) for constrained optimization of novel policies. This is inspired by interior point methods in constrained optimization. A key advantage is it does not depend on the scaling of the novelty reward.4. They demonstrate experimentally that IPD can generate diverse and well-performing policies on MuJoCo locomotion tasks, outperforming prior multi-objective approaches as well as constrained optimization baselines.In summary, the main contribution appears to be proposing a new formulation, metric, and algorithm for generating novel policies while maintaining primary task performance. The constrained optimization perspective and IPD method seem to be the key novel ideas proposed.


## How does this paper compare to other research in the same field?

Based on my reading, here are a few thoughts on how this paper compares to other research in the field of novel policy generation in reinforcement learning:- The paper focuses on generating diverse policies that perform well on the original task, rather than just maximizing diversity. This differs from some prior work that used multi-objective optimization to balance task performance and novelty, often resulting in reduced task performance. The constrained optimization formulation in this paper aims to maintain good task performance.- The paper introduces a new metric for measuring policy differences based on the Wasserstein distance. This provides an instant, dense novelty reward signal during training. Some prior work used more complex novelty metrics based on autoencoders or measuring differences in visited states, which were more expensive to compute.- The proposed IPD algorithm avoids explicitly combining novelty and task reward gradients, unlike prior approaches like TNB. IPD uses a more implicit novelty constraint based on terminating episodes, making it less sensitive to how the novelty reward is shaped.- Experiments demonstrate that IPD can produce varied and high-performing policies on MuJoCo benchmarks, outperforming prior multi-objective and constrained optimization techniques for novelty-seeking. The performance gains suggest IPD helps avoid some local optima.- The work focuses on a setting of sequentially generating a set of novel policies. Some other work has looked at learning a single policy that exhibits diverse skills or modes of behavior. The sequential setting may face scaling challenges in producing a very large set of policies.Overall, the paper makes nice contributions in formulating novelty-seeking as constrained optimization and introducing a practical algorithm that avoids direct reward gradient conflicts. The experiments provide evidence this approach can improve over prior techniques in producing useful diversity without sacrificing task performance.
