# [Novel Policy Seeking with Constrained Optimization](https://arxiv.org/abs/2005.10696)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the key points of this paper are:- It aims to enable reinforcement learning agents to discover diverse and novel solutions to a problem, rather than converging to a single optimal policy. - It proposes a new metric space to quantitatively measure policy differences, based on the Wasserstein metric. - It frames the problem of seeking novel policies as a constrained optimization problem, rather than a multi-objective optimization problem as in prior work. This avoids performance degradation on the main task.- It develops a practical algorithm called Interior Policy Differentiation (IPD) for constrained novelty-seeking. IPD uses an implicit barrier approach reminiscent of interior point methods in constrained optimization.- Experiments in MuJoCo locomotion tasks show IPD can generate varied policies without sacrificing performance. It outperforms prior multi-objective approaches like WSR and TNB on both novelty and performance.In summary, the central hypothesis is that formulating novel policy search as constrained optimization will enable better diversity while maintaining task performance, compared to prior multi-objective approaches. The paper proposes a new metric, formulation, and algorithm to demonstrate this hypothesis empirically.


## What is the main contribution of this paper?

Based on my reading of the abstract and introduction, the main contributions of this paper appear to be:1. They propose a new metric to measure the difference between policies that provides instant feedback at every timestep. This addresses limitations of prior novelty/diversity metrics that were computationally expensive or only provided episodic reward. 2. They formulate the problem of generating novel policies as a constrained optimization problem rather than multi-objective optimization. This avoids hindering performance on the primary task while still encouraging diversity. 3. They propose a practical algorithm called Interior Policy Differentiation (IPD) for constrained optimization of novel policies. This is inspired by interior point methods in constrained optimization. A key advantage is it does not depend on the scaling of the novelty reward.4. They demonstrate experimentally that IPD can generate diverse and well-performing policies on MuJoCo locomotion tasks, outperforming prior multi-objective approaches as well as constrained optimization baselines.In summary, the main contribution appears to be proposing a new formulation, metric, and algorithm for generating novel policies while maintaining primary task performance. The constrained optimization perspective and IPD method seem to be the key novel ideas proposed.


## How does this paper compare to other research in the same field?

Based on my reading, here are a few thoughts on how this paper compares to other research in the field of novel policy generation in reinforcement learning:- The paper focuses on generating diverse policies that perform well on the original task, rather than just maximizing diversity. This differs from some prior work that used multi-objective optimization to balance task performance and novelty, often resulting in reduced task performance. The constrained optimization formulation in this paper aims to maintain good task performance.- The paper introduces a new metric for measuring policy differences based on the Wasserstein distance. This provides an instant, dense novelty reward signal during training. Some prior work used more complex novelty metrics based on autoencoders or measuring differences in visited states, which were more expensive to compute.- The proposed IPD algorithm avoids explicitly combining novelty and task reward gradients, unlike prior approaches like TNB. IPD uses a more implicit novelty constraint based on terminating episodes, making it less sensitive to how the novelty reward is shaped.- Experiments demonstrate that IPD can produce varied and high-performing policies on MuJoCo benchmarks, outperforming prior multi-objective and constrained optimization techniques for novelty-seeking. The performance gains suggest IPD helps avoid some local optima.- The work focuses on a setting of sequentially generating a set of novel policies. Some other work has looked at learning a single policy that exhibits diverse skills or modes of behavior. The sequential setting may face scaling challenges in producing a very large set of policies.Overall, the paper makes nice contributions in formulating novelty-seeking as constrained optimization and introducing a practical algorithm that avoids direct reward gradient conflicts. The experiments provide evidence this approach can improve over prior techniques in producing useful diversity without sacrificing task performance.


## What future research directions do the authors suggest?

Based on my reading of the paper, here are some of the main future research directions suggested by the authors:- Developing methods to scale diverse policy generation to very large numbers of policies (e.g. thousands). The authors note that their method of sequentially generating policies may be challenging to scale up due to the need to compute novelty against an ever-growing set of reference policies. New algorithms or approximations may be needed.- Applying the proposed constrained optimization approach to other RL algorithms besides PPO. The authors suggest their method could likely be extended to other popular RL algorithms but leave this to future work.- Exploringwhether constrained optimization novelty seeking could help tackle the problem of local optima in RL. The authors provide some preliminary evidence that their method allows hopper and half-cheetah policies to avoid getting stuck in poor local optima that PPO policies fall into. More investigation is needed.- Developing theoretical guarantees for the proposed methods. The authors provide an initial theoretical framing using metric spaces and constrained optimization but do not provide convergence guarantees or sample complexity bounds. Formal theoretical analysis is suggested as an area for future work.- Speeding up training. The authors note their method can slow down training, especially early on. Improving training efficiency is cited as an area for improvement.- Generalizing the approach to even more diverse domains and tasks. The methods are demonstrated on MuJoCo locomotion but the authors suggest they may generalize more broadly if adapted appropriately.In summary, the main suggestions are to scale up, generalize the approach to new domains, improve training speed and sample efficiency, provide theoretical guarantees, and integrate the ideas into more RL algorithms. Advancing the practicality and theory of constrained optimization for diverse policy learning seems to be the core direction.
