# [Don't Believe Everything You Read: Enhancing Summarization   Interpretability through Automatic Identification of Hallucinations in Large   Language Models](https://arxiv.org/abs/2312.14346)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

The paper focuses on detecting and reducing hallucinations (incorrect or fabricated information) generated by large language models (LLMs) when summarizing text. The lack of faithfulness and factual consistency in LLM-generated summaries is a major challenge. The main problem is that current evaluation metrics for summarization do not effectively measure faithfulness. 

The paper introduces a new linguistically-motivated taxonomy of hallucination errors in dialogue summarization. Using this taxonomy, human annotators labeled hallucinated tokens in generated summaries from the SAMSum dialogue summarization dataset. Analysis of the annotated dataset shows "missing information" is the most common type of hallucination error.

The paper investigates hallucination detection and factual consistency of summaries through three approaches: 

1) Training a "proxy" model to tag hallucinated tokens using labeled dataset. Achieves F1 score of 0.71 with GPT-4.

2) Jointly training a model for both summary generation and hallucination detection tasks simultaneously. Improves ROUGE score by 0.4 over baselines and also sometimes generates better summaries than reference.

3) Prompting techniques with LLMs like GPT-4 and LLaMa to make them consider hallucinations when generating summaries. Explicit hallucination detection prompting improves quality of generated summaries.

The key contributions are:
(1) A new fine-grained annotated dataset identifying hallucination types. 
(2) Demonstrating LLMs can improve summarization through explicit handling of hallucinations.
(3) A new joint training approach that improves faithfulness by aligning summarization and hallucination detection.

Overall, the paper significantly advances the state-of-the-art in improving faithfulness and reducing hallucinations for text summarization through interpretability, datasets, model architectures and prompting strategies.
