# [LOSS-GAT: Label Propagation and One-Class Semi-Supervised Graph   Attention Network for Fake News Detection](https://arxiv.org/abs/2402.08401)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
The rapid spread of fake news on social media is a major issue, but detecting fake news is challenging due to the limited availability of labeled datasets. Manual labeling of news data is expensive and fake news detection is an imbalanced classification problem with much more real news than fake news.

Proposed Solution: 
The paper proposes a semi-supervised one-class learning approach called LOSS-GAT that uses only a small set of labeled fake news to detect fake news. The key ideas are:

1) Represent news data as a similarity graph with each node being a news article. Only a small subset of nodes have fake news labels. 

2) Apply a two-step label propagation method on the graph to infer additional pseudo-labels for unlabeled nodes, marking them as likely fake or real news. This expands the limited labeled set.

3) Augment the graph structure by adding edges between structurally similar nodes using the Adamic-Adar similarity measure. This improves connectivity.  

4) Classify all nodes as fake or real using a Graph Attention Network (GAT) model that randomly selects a subset of neighbors during aggregation to improve robustness.

Main Contributions:

- Novel semi-supervised fake news detection using one-class learning and only a small labeled fake news dataset
- Two-step label propagation method using a GAT model to infer better pseudo-labels
- Structural augmentation of the news graph using Adamic-Adar similarity  
- Randomized neighbor selection during GAT aggregation to improve model learning

The method is evaluated on 5 datasets and achieves 10%+ better performance than baselines using one-class and binary-labeled models, while requiring much less labeled data.
