# [Scaling Spherical CNNs](https://arxiv.org/abs/2306.05420)

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper addresses is how to scale spherical CNNs to larger problems so they can be competitive with other state-of-the-art models. The key hypotheses are:- Larger spherical CNN models with adequate capacity and efficiency can achieve strong performance on real-world tasks involving spherical data and rotational symmetries.- Spherical CNNs can be competitive with graph neural networks and transformers on molecule property prediction and weather forecasting tasks.Specifically, the paper investigates:- Novel variants of common neural network components like nonlinearities, normalization, and residual connections that improve expressivity and efficiency of spherical CNNs.- An efficient implementation of spherical CNN operations optimized for TPUs.- Application-specific input representations and modeling choices for molecules and weather data that allow spherical CNNs to work well.The overarching goal is to demonstrate spherical CNNs can be scaled to much larger sizes than prior work, making them viable for complex scientific applications at a scale not previously possible. The experiments on molecular property prediction and multiple weather forecasting tasks aim to validate the hypotheses.


## What is the main contribution of this paper?

The main contribution of this paper is scaling up spherical CNNs to tackle larger problems. Specifically:- They design large spherical CNN architectures with improved layers like phase collapse nonlinearity, spectral batch normalization, and efficient residual blocks. This allows them to build models with higher capacity.- They provide an optimized implementation of spherical CNN operations on TPUs, exploiting matrix multiplications and distributed training for speed.- They introduce application-specific input representations and output heads for molecules (distance-based spherical features) and weather forecasting (leveraging multiple input variables). - They evaluate the scaled spherical CNNs on molecular property prediction on QM9 and multiple weather forecasting tasks. The models reach state-of-the-art on QM9, previously dominated by graph neural nets and transformers, and are competitive on the weather tasks.In summary, the key contribution is demonstrating that with proper modeling choices and optimizations, spherical CNNs can be scaled to tackle problems an order of magnitude larger than prior work, reaching performance on par with or better than alternative approaches like graph networks and transformers. This helps establish spherical CNNs as a viable technique for problems with spherical inputs and rotational symmetries.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper scales up spherical CNNs by improving model components like activations and batch normalization, optimizing implementations for TPUs, and designing application-specific input representations, enabling spherical CNNs to achieve state-of-the-art performance on molecular property prediction and competitive results on weather forecasting tasks.
