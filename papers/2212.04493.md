# [SDFusion: Multimodal 3D Shape Completion, Reconstruction, and Generation](https://arxiv.org/abs/2212.04493)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading, the central research question this paper aims to address is: 

How can we develop a flexible 3D shape generation framework that can synthesize high-resolution shapes conditioned on various input modalities like partial shapes, images, and text?

The key hypotheses appear to be:

- Using a signed distance function (SDF) representation along with a compressed latent space will allow generating high resolution 3D shapes more efficiently compared to other representations. 

- Applying diffusion models on this latent space will allow high-quality and diverse shape modeling.

- Using task-specific encoders and cross-attention will enable flexibly conditioning the diffusion model on various input modalities like partial shapes, images, and text.

- Allowing joint conditioning on multiple modalities with adjustable weights will provide more interactive control over shape generation.

- Combining the proposed 3D shape generation model with 2D diffusion models can enable texturing the generated shapes.

So in summary, the central research question is around developing an efficient, high-resolution, and flexibly controllable 3D shape generation framework using diffusion models, task-specific conditioning, and latent SDF representations. The key hypothesis is that this approach will outperform prior specialized models on tasks like shape completion, image-based reconstruction, and text-to-shape generation.
