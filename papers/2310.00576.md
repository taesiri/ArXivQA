# [GrowLength: Accelerating LLMs Pretraining by Progressively Growing   Training Length](https://arxiv.org/abs/2310.00576)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question addressed is:

"Can content window extension be adapted to the pretraining stage to reduce training time?"

The key hypothesis is that optimizing sentence length during pretraining can lead to more efficient models in the fine-tuning stage. Specifically, the authors hypothesize that:

1) Models trained with shorter sequence lengths can effectively predict long sequences, as shown by the success of content window extension in the fine-tuning stage.

2) Training with shorter sentences is substantially more time-efficient than training with longer sequences. 

Based on these two observations, the authors explore adapting content window extension techniques to the pretraining stage in order to reduce the overall pretraining time for large language models. Their proposed method, "GrowLength," progressively increases the training length during pretraining to optimize computational resource utilization and increase the number of tokens processed within a given training duration.

In summary, the central research question is whether content window extension can be effectively adapted to pretraining to reduce training time. The key hypothesis is that a dynamic, growing training length will enable more efficient pretraining compared to fixed-length methods.


## What is the main contribution of this paper?

 The main contribution of this paper is proposing a novel method called "GrowLength" to accelerate the pretraining of large language models (LLMs) by progressively growing the training sequence length. 

Specifically, the key contributions are:

- They extend the context window extension technique used in fine-tuning LLMs to the pretraining stage in order to reduce overall pretraining time. The method starts with shorter sequence lengths and gradually increases to longer ones over the course of pretraining.

- They provide analysis showing training with shorter sequences is faster than longer ones, and transitioning to longer sequences does not cause a loss jump. This motivates the progressive sequence length increase in GrowLength.

- They demonstrate the effectiveness of GrowLength through experiments on various state-of-the-art LLMs. Models trained with GrowLength converge faster and outperform fixed-length baselines given the same training time.

- GrowLength is orthogonal to other LLM acceleration methods and can be combined with them. It accelerates pretraining from the input sequence perspective, unlike prior work focused on model architecture or computational optimizations.

In summary, the key contribution is proposing and validating GrowLength, a simple but effective method to accelerate LLM pretraining by progressively growing the training sequence length over time. This unexplored way of speeding up pretraining could help reduce computational costs and training time for large models.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the paper:

The paper proposes a new method called "GrowLength" to accelerate the pretraining of large language models by progressively increasing the training sequence length over time, allowing the model to process more tokens efficiently.
