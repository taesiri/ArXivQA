# Long Time No See! Open-Domain Conversation with Long-Term Persona Memory

## What is the central research question or hypothesis that this paper addresses?

This paper addresses the problem of generating coherent and engaging open-domain dialogues in long-term interactions between humans and chatbots. The central hypothesis is that equipping chatbots with long-term persona memory and the ability to actively track both their own persona and the user's persona during conversations will lead to more consistent and engaging dialogues over multiple turns. Specifically, the paper proposes that current chatbots lack "long-term persona ability", which refers to the capability of understanding and modeling the user's and the chatbot's persona through long conversations. This inability to leverage long-term persona information leads to poor performance in multi-turn chats. To test this hypothesis, the paper introduces a new task called "Long-term Memory Conversation" (LeMon) that focuses on mutual persona modeling in long dialogues. It also constructs a new Chinese dataset called DuLeMon for this task. Furthermore, the paper proposes a novel framework called PLATO-LTM that adds a Long-Term Memory (LTM) module to existing chatbot models like PLATO-2. This LTM module enables real-time extraction and storage of both user and chatbot persona from dialogues.In summary, the central hypothesis is that modeling long-term mutual persona through mechanisms like the proposed LTM will improve chatbot consistency and engagingness in long conversations, which is tested through the introduction of the LeMon task, DuLeMon dataset, and PLATO-LTM model.


## What is the main contribution of this paper?

The main contributions of this paper are:1. It proposes a new long-term persona chat task LeMon and builds a corresponding Chinese dataset DuLeMon, which focuses on modeling both the user's and chatbot's persona in long-term conversations. 2. It designs a PLATO-LTM framework by adding a novel Long-Term Memory (LTM) mechanism to the state-of-the-art open-domain dialogue model PLATO. The LTM can extract and store both parties' persona information from the conversation history and retrieve relevant personas to generate responses.3. Experiments show that PLATO-LTM can significantly improve the consistency and engagingness of dialogues compared to baselines, enabling more coherent and intimate long-term conversations without relying on multi-session training data.In summary, this paper makes both conceptual and technical contributions to enhancing chatbots' ability to understand and utilize long-term persona information of both parties through real-time extraction and memory. The proposed dataset and model framework help advance research on this important but less studied problem in open-domain conversational AI.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one-sentence summary of the key points from the paper:The paper proposes a novel task of long-term memory conversation (LeMon) and builds a new Chinese dialogue dataset DuLeMon that focuses on modeling the persona of both the user and chatbot over multiple dialogue turns; it also develops a retrieval-augmented dialogue generation framework with a long-term memory module that can extract and utilize both speakers' persona information to improve long-term consistency and engagingness.


## How does this paper compare to other research in the same field?

Here are a few key ways this paper compares to other related research:- This paper focuses specifically on enabling chatbots to conduct long-term conversations by modeling the personas of both the user and the chatbot over time. Much prior work on persona-based dialogue has focused only on modeling the chatbot's persona, not the user's. Modeling the user's persona as well is novel.- The paper introduces a new dataset, DuLeMon, designed specifically for studying long-term mutual persona modeling in dialogues. The dialogue examples have annotated personas and persona groundings to support this research. This is a unique contribution compared to existing persona chat datasets.- The proposed model architecture, PLATO-LTM, incorporates a novel Long-Term Memory mechanism to extract and store personas of both speakers from the dialogue history, then retrieve relevant personas to guide response generation. This approach to explicitly modeling long-term personas is different from prior work.- The paper demonstrates strong improvements on consistency and engagingness in long dialogues by modeling mutual personas with the Long-Term Memory. Quantitative results and human evaluations validate the benefits of the model. Most prior work has not evaluated on such long multi-turn conversations.- Overall, by tackling the problem of long-term persona modeling for both users and chatbots, introducing a novel dataset, and proposing an architecture incorporating explicit Long-Term Memory for personas, this work makes important contributions compared to related prior studies on persona-based dialogue systems. The longevity of conversations and grounding is a key distinction.In summary, the focus on mutual long-term personas, new dataset, explicit persona memory mechanism, and evaluations in long conversations differentiate this work from related research in persona-based dialogues. The paper makes useful contributions towards enabling richer, more consistent bot personalities.


## What future research directions do the authors suggest?

Here are some of the key future research directions suggested by the authors:- Exploring the possibility of using reinforcement learning with human feedback signals to further improve the long-term conversational capability of the model. The authors mention that human evaluations indicate there is still room for improvement in engagingness and coherence. - Studying long-term persona modeling in a fully mutli-party conversational setting, rather than just between two speakers. The current work focuses on modeling the personas of a single user and the chatbot, but extending this to group conversations could be an interesting direction.- Validating the approach on other languages and multilingual datasets. The current work focuses solely on Chinese language conversations. Applying it to other languages could demonstrate the wider applicability.- Exploring different architectures and training techniques for the persona extractor module. The authors use a simple ERNIE+CNN architecture, but mention there may be better ways to identify persona-related utterances.- Investigating other strategies to distinguish and handle the different speaker personas within the generative model, beyond just using role embeddings/tokens.- Collecting more diverse and natural conversational data to cover an even broader range of personas, contexts, and interaction patterns.In summary, the key suggestions are around improving the core model components (e.g. persona extractor, generative model), expanding to more complex conversational scenarios, validating on broader datasets, and collecting richer training data. The persona modeling approach seems promising but still needs more research to reach its full potential.
