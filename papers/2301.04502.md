# [Pruning Compact ConvNets for Efficient Inference](https://arxiv.org/abs/2301.04502)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research questions/hypotheses appear to be:1) RQ1: Can pruning a larger FBNetV3 model (optimized with NAS) achieve higher generalization performance than a smaller FBNetV3 model, when the pruned model has the same computational budget (FLOPs)?2) RQ2: Is pruning a faster and less computationally expensive approach to obtain a high-accuracy model at a desired FLOPs level, compared to running a full neural architecture search from scratch?The key ideas seem to be:- FBNetV3 models are state-of-the-art efficient ConvNets optimized with NAS. This paper studies if they can be further improved via pruning.- The authors prune larger FBNetV3 models to the FLOPs levels of smaller models. The pruned larger models consistently outperform the smaller original models.- Pruning a larger model requires much less compute than running NAS from scratch to find a small model. So pruning is a efficient way to find compact high-accuracy models.In summary, the main hypotheses are that pruning can further optimize NAS-optimized efficient ConvNets like FBNetV3, and do so with lower compute than NAS. The paper appears to provide empirical evidence supporting both these claims.
