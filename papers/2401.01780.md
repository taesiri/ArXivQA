# [Navigating Uncertainty: Optimizing API Dependency for Hallucination   Reduction in Closed-Book Question Answering](https://arxiv.org/abs/2401.01780)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Large language models (LLMs) like GPT-3 have shown impressive performance on question answering, but still suffer from hallucination where they generate false answers. 
- Existing methods to reduce hallucination rely on external techniques like retrieving documents or perplexity thresholds to detect incorrect answers. 
- However, these external techniques introduce additional computational costs and don't fully leverage the knowledge already encoded within the LLM's parameters.

Proposed Solution:
- The authors propose training the LLM to self-estimate whether it can directly answer a question or needs to query an external knowledge source. 
- They introduce a "Hallucination Masking Mechanism" that uses the LLM's own performance on a closed-book QA dataset to generate pseudo-labels indicating whether each answer is correct or a hallucination.
- Hallucinated answers are masked and replaced with a special "search" token so the model learns to query an external tool instead.  
- The trained LLM aims to balance retaining QA abilities, reducing hallucinations and minimizing unnecessary external searches to save costs.

Main Contributions:
- Novel self-estimation based method for an LLM to discern when it should directly answer vs. defer to an external search.
- Pseudo-labeling approach using closed-book QA performance to supervise when the LLM hallucinates.  
- Parameter-efficient fine-tuning method that retains existing QA abilities while gaining uncertainty detection.
- Experiments showing the approach matches or exceeds perplexity-based methods in hallucination reduction, while preserving more correct answers and reducing search rate.


## Summarize the paper in one sentence.

 This paper proposes teaching large language models to self-assess whether they can answer a factual question correctly or should query an external knowledge source, aiming to minimize unnecessary searches to reduce computational costs.


## What is the main contribution of this paper?

 The main contribution of this paper is proposing a new large language model (LLM) that is able to self-estimate whether it can answer a question directly or needs to request help from an external tool. Specifically:

- The paper introduces a "Hallucination Masking Mechanism" to teach the LLM to generate a special "search" token when it is not confident in its answer, instead of hallucinating an incorrect response. This allows the model to query an external knowledge base only when needed.

- The authors evaluate supervised approaches to training this model on closed-book QA datasets. They show their method is able to directly answer 78.2% of known questions and opt to search for 77.2% of unknown questions on the Natural Questions dataset.

- Compared to baselines, the proposed model provides a better tradeoff between answering accurately vs. relying on external search, reducing computational costs. The key innovation is enabling the LLM to self-assess its own certainty in responding.

So in summary, the main contribution is an LLM architecture that can discern when to answer directly vs. defer to an external search tool, minimizing unnecessary API queries.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and concepts are:

- Large language models (LLMs)
- Hallucination reduction
- Closed-book question answering (CBQA)
- Knowledge grounding
- External knowledge bases/tools
- Self-assessment of answer confidence 
- Perplexity-based hallucination detection
- Parameter-efficient fine-tuning 
- Low-rank adaptation (LoRA)
- Budgeted open QA formulation
- Hallucination masking mechanism

The paper proposes an approach to teach LLMs to self-estimate their ability to answer factual questions directly or determine when they need to request information from an external tool. It aims to optimize the tradeoff between accessing external knowledge versus answering directly from the LLM's own knowledge, in order to reduce computational costs. The key idea is to use a hallucination masking mechanism with pseudo-labels to train the LLM to generate an API call instead of an incorrect answer when uncertain. Overall, it tackles the problem of hallucinations in QA while minimizing dependence on external search.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in the paper:

1. The paper proposes a "Hallucination Masking Mechanism" to generate pseudo-labels for training the model to detect its own hallucinations. How exactly does this mechanism work? What are the inputs and outputs?

2. The paper evaluates the method on Natural Questions and TriviaQA datasets. What are the key characteristics of these datasets? Why were they chosen to evaluate this method?

3. The authors use several metrics beyond standard accuracy to evaluate the models, including proportions of correct, hallucinated, and search answers. Why is it important to look beyond just accuracy for this task? What extra insights do these metrics provide?

4. The method relies on fine-tuning a pretrained QA model. What model architectures were used? Why were they chosen? How were they pretrained and what impact does this have? 

5. Low-rank adaptation (LoRA) is one of the fine-tuning techniques explored. How does LoRA work? What are its advantages over standard fine-tuning for this application?

6. Perplexity-thresholding is used as a baseline. How does it work? What are its limitations? Why might the proposed approach outperform it?

7. How is the tradeoff between search rate and hallucinations controlled? What is the effect of varying the relative importance hyperparameter lambda?

8. In-context learning models like Mistral-7B are also analyzed. How do they perform? What issues arise with few-shot prompting for this task?

9. The analysis in Table 2 shows the effect of varying search accuracy. How impactful is the external knowledge source's reliability? How could this guide real-world deployment?

10. The method relies on question distributions for training and evaluation being similar. Why? How could the approach be adapted to handle distribution shift between training and test sets?
