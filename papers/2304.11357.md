# [Learning Symbolic Representations Through Joint GEnerative and   DIscriminative Training](https://arxiv.org/abs/2304.11357)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
Current neuro-symbolic learning methods rely on costly pre-training or additional supervision at the symbolic representation level from neural networks. This can lead to the problem of representational collapse, where neural networks learn trivial/meaningless representations that satisfy constraints but do not capture the actual content. For example, mapping all images to the same symbol would allow a network to satisfy a constraint that the third image digit is the sum of the first two, without learning useful representations.

Proposed Solution: 
The authors propose a framework called GEDI that combines existing self-supervised learning objectives with likelihood-based generative models. GEDI leverages complementary properties of discriminative approaches suitable for representation learning, and generative models that capture information about the underlying data distribution. This alleviates the representation collapse problem and learns improved symbolic representations without needing additional supervision or pre-training.

Key Points:

- GEDI is based on an evidence lower bound linking the negative entropy of data to a generative cross-entropy term and discriminative self-supervised losses.

- A data augmentation strategy called DAM is introduced, which uses the learned generative model to sample plausible synthetic data points near the data manifold. This augments the training set with unlabeled but on-manifold examples.

- GEDI is trained in two phases - first pre-training the generative model, then end-to-end training of generative and self-supervised components.

- Experiments on SVHN, CIFAR and synthetic datasets demonstrate state-of-the-art clustering performance, showing GEDI representations do not collapse and capture semantic content.

- A simple neural-symbolic experiment confirms GEDI alleviates collapse by integrating logical constraints, giving improved digit classification accuracy from only triplet sum supervision.

Main Contributions:

1) A unified Bayesian perspective linking generative and self-supervised learning
2) The GEDI framework and training procedure leveraging their complementary strengths 
3) The DAM augmentation strategy exploiting learned generative models
4) Empirical demonstration of improved representations and clustering
5) Confirmation that GEDI mitigates collapse in neural-symbolic models

In summary, GEDI advances representation learning for neuro-symbolic AI without needing additional supervision. The integration of generative and self-supervised models is shown to learn semantically meaningful representations that support logical reasoning.


## Summarize the paper in one sentence.

 The paper introduces GEDI, a Bayesian framework that unifies existing self-supervised learning objectives with likelihood-based generative models to learn improved symbolic representations for integration with neuro-symbolic systems.


## What is the main contribution of this paper?

 According to the paper, the main contribution is introducing GEDI, a Bayesian framework that combines existing self-supervised learning objectives with likelihood-based generative models. Specifically:

- GEDI leverages the benefits of both generative (GE) and discriminative (DI) approaches to learn better symbolic representations compared to standalone solutions. 

- It can be easily integrated and trained jointly with existing neuro-symbolic frameworks without needing additional supervision or costly pre-training steps. 

- Experiments demonstrate that GEDI outperforms existing self-supervised learning strategies on clustering tasks by a significant margin. 

- The symbolic component also allows it to leverage knowledge in the form of logical constraints to improve performance in the small data regime, which is shown on an MNIST experiment.

In summary, the key innovation is unifying generative and discriminative self-supervised learning in a principled Bayesian framework called GEDI to achieve improved symbolic representations and integration with neuro-symbolic systems.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and concepts are:

- GEDI (GEnerative and DIscriminative) - The name of the proposed Bayesian framework that combines self-supervised learning objectives with likelihood-based generative models.

- Self-supervised learning - Learning techniques that create supervised signals from unlabeled data itself, often through pretext tasks. GEDI incorporates objectives from this area.

- Generative models - Models that learn the probability distribution of the data. GEDI incorporates likelihood-based generative models like variational autoencoders and normalizing flows.

- Representational collapse - The problem when neural networks map different inputs onto the same inappropriate outputs just to satisfy constraints/objectives.

- Symbolic representations - Abstract categorical representations of inputs that can support reasoning. GEDI aims to learn better symbolic representations.  

- Neuro-symbolic learning - Integrating neural networks with symbolic/logical representations and reasoning for perception and inference. GEDI relates to this area.

- Decorrelation and invariance - Two properties promoted by the negative-free objective in GEDI for better representations.

- Sinkhorn-Knopp algorithm - Algorithm used in GEDI's discriminative loss to avoid representations all collapsing to a single cluster.

So in summary, key terms revolve around combining generative and self-supervised discriminative learning to obtain better symbolic representations that can support reasoning, while avoiding problems like representational collapse.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 potential in-depth questions about the method proposed in the paper:

1. The paper introduces a new data augmentation strategy called DAM that leverages the underlying data manifold learned by the generative model. Can you explain in more detail how DAM works and why it is beneficial? 

2. One of the main benefits claimed is the ability to leverage logical constraints in the small data regime. Can you elaborate on how the integration with symbolic methods like DeepProbLog allows this?

3. The paper shows improved clustering performance over standalone self-supervised learning methods. What is the intuition behind why jointly training a generative and discriminative model leads to better representations?

4. How exactly is the generative model integrated into the overall GEDI framework? What modifications needed to be made to existing self-supervised objectives? 

5. The objective function combines multiple loss terms including negative entropy, invariant representations, and uniform cluster assignments. Can you discuss the motivation and tradeoffs associated with each?  

6. How does GEDI differ from other efforts like EBCLR that also combine energy-based models with contrastive learning? What advantage does the more general integration provide?

7. The two-step training procedure pre-trains the generative model first. Why is this important? What issues could arise if the models were trained jointly from the start?  

8. For the MNIST experiment, how exactly does the logical constraint provide additional supervision? Does this constrain parts of the representation space that would normally lead to poor solutions?

9. The paper mentions representational collapse as an issue in neuro-symbolic systems. How might the ideas from GEDI help address this? Could GEDI complement existing solutions?

10. The computational overhead from GEDI seems non-trivial given the need for sampling and running DAM. Could approximations be made to improve efficiency while retaining benefits? What are the most expensive components?
