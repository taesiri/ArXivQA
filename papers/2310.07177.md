# [Online Speculative Decoding](https://arxiv.org/abs/2310.07177)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading, the key research questions and hypotheses addressed in this paper are:

- How can we improve the performance of speculative decoding for large language model (LLM) inference by adapting the smaller "draft" models dynamically based on the query distribution? 

- The paper hypothesizes that continuously retraining the draft models on user query data can enhance their ability to emulate the target LLM's outputs for that query distribution. This would improve the token acceptance rate and reduce latency.

- It also hypothesizes that the abundant spare computational resources in typical LLM serving clusters can be repurposed to retrain the draft models online at low cost.

- The paper explores whether adapting multiple draft models, each tailored to a different query distribution mode (e.g. language or topic), can further improve performance.

- More broadly, the paper introduces and evaluates a framework called "online speculative decoding" that aims to reduce LLM serving latency by dynamically adapting draft models using query data and knowledge distillation. It hypothesizes this approach can effectively handle shifts in query distributions.

In summary, the key hypotheses are around using online learning and spare resources to tailor draft models to the query distribution, in order to improve speculative decoding performance and reduce LLM serving latency. The paper explores these hypotheses through algorithm development, analysis, and experiments on both synthetic and real-world data.


## What is the main contribution of this paper?

 Based on my reading, the main contributions of this paper are:

1. It introduces a new method called "online speculative decoding" to reduce the latency of large language model (LLM) inference. 

2. It proposes to continuously update/retrain smaller "draft" models on-the-fly using user query data and knowledge distillation. The key ideas are:

- Leverage the abundant spare FLOPs in a typical LLM serving cluster to retrain the draft models with negligible overhead.

- Retraining the draft models on the query distribution makes them better at predicting the target LLM's outputs for similar queries, thus improving the token acceptance rate.

- As draft models evolve online, they can adapt to shifts in the query distribution.

3. It develops a prototype system and evaluates it on both synthetic and real-world queries. The results demonstrate significant improvements in token acceptance rates, translating to 1.2-3x lower latency.

4. It explores various generalized knowledge distillation (GKD) methods for training draft models and finds the most effective variants. It suggests GKD as a superior alternative to existing finetuning techniques. 

5. Overall, the method provides a practical solution to reduce LLM serving latency by adapting draft models online using spare resources available in the serving infrastructure. The ability to handle distribution shifts and negligible overhead are notable advantages.

In summary, the core contribution is an online learning framework to dynamically construct and evolve smaller draft models that can more accurately speculate the outputs of target LLMs based on observed queries. This ultimately helps accelerate LLM inference.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence TL;DR summary of the paper:

The paper introduces an online speculative decoding method that continuously updates draft models during LLM serving by leveraging spare computing resources, in order to improve speculation accuracy and reduce latency.
