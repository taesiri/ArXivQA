# [Online Speculative Decoding](https://arxiv.org/abs/2310.07177)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading, the key research questions and hypotheses addressed in this paper are:

- How can we improve the performance of speculative decoding for large language model (LLM) inference by adapting the smaller "draft" models dynamically based on the query distribution? 

- The paper hypothesizes that continuously retraining the draft models on user query data can enhance their ability to emulate the target LLM's outputs for that query distribution. This would improve the token acceptance rate and reduce latency.

- It also hypothesizes that the abundant spare computational resources in typical LLM serving clusters can be repurposed to retrain the draft models online at low cost.

- The paper explores whether adapting multiple draft models, each tailored to a different query distribution mode (e.g. language or topic), can further improve performance.

- More broadly, the paper introduces and evaluates a framework called "online speculative decoding" that aims to reduce LLM serving latency by dynamically adapting draft models using query data and knowledge distillation. It hypothesizes this approach can effectively handle shifts in query distributions.

In summary, the key hypotheses are around using online learning and spare resources to tailor draft models to the query distribution, in order to improve speculative decoding performance and reduce LLM serving latency. The paper explores these hypotheses through algorithm development, analysis, and experiments on both synthetic and real-world data.


## What is the main contribution of this paper?

 Based on my reading, the main contributions of this paper are:

1. It introduces a new method called "online speculative decoding" to reduce the latency of large language model (LLM) inference. 

2. It proposes to continuously update/retrain smaller "draft" models on-the-fly using user query data and knowledge distillation. The key ideas are:

- Leverage the abundant spare FLOPs in a typical LLM serving cluster to retrain the draft models with negligible overhead.

- Retraining the draft models on the query distribution makes them better at predicting the target LLM's outputs for similar queries, thus improving the token acceptance rate.

- As draft models evolve online, they can adapt to shifts in the query distribution.

3. It develops a prototype system and evaluates it on both synthetic and real-world queries. The results demonstrate significant improvements in token acceptance rates, translating to 1.2-3x lower latency.

4. It explores various generalized knowledge distillation (GKD) methods for training draft models and finds the most effective variants. It suggests GKD as a superior alternative to existing finetuning techniques. 

5. Overall, the method provides a practical solution to reduce LLM serving latency by adapting draft models online using spare resources available in the serving infrastructure. The ability to handle distribution shifts and negligible overhead are notable advantages.

In summary, the core contribution is an online learning framework to dynamically construct and evolve smaller draft models that can more accurately speculate the outputs of target LLMs based on observed queries. This ultimately helps accelerate LLM inference.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence TL;DR summary of the paper:

The paper introduces an online speculative decoding method that continuously updates draft models during LLM serving by leveraging spare computing resources, in order to improve speculation accuracy and reduce latency.


## How does this paper compare to other research in the same field?

 Here are my thoughts on how this paper compares to other research in the field of language model inference optimization:

- This paper focuses specifically on speculative decoding, which is an emerging technique to accelerate inference in large language models (LLMs) by using a smaller draft model to propose possible tokens that are verified by the larger target model in parallel. Other optimizations like pipelining, pruning, knowledge distillation etc. aim to improve different aspects of LLM inference. So this paper explores a quite distinct approach.

- Within the realm of speculative decoding, most prior works like Leviathan and SpecInfer assume a static draft model after deployment. A key novelty of this paper is introducing a framework to dynamically adapt the draft model in an online setting based on observed user queries, irrespective of how the draft model was constructed. This allows the draft model to stay aligned with shifts in query distribution.

- For adapting the draft model online, the paper proposes a method based on online knowledge distillation, where the target LLM acts as the teacher and draft LLM as the student. It explores different distillation objectives like forward/reverse KL divergence to maximize accuracy. Most prior speculative decoding papers use plain fine-tuning objectives. Using distillation is more effective since it transfers richer information.

- A core motivation of the paper is leveraging spare FLOPs available in LLM serving clusters for online draft model updates. Since LLM inference is memory-bound, there is ample unused compute that can be repurposed with little overhead. The computational analysis quantifies this. 

- Results on both synthetic and real-world LLM query datasets demonstrate significant gains in token acceptance rates and lower latency from online adaptation of draft models. The gains match or surpass offline upper bounds.

In summary, introducing online adaptation of draft models in speculative decoding via online distillation is a novel contribution distinguishing this paper from prior art. The motivation of harnessing spare flops is also unique and empirically validated.
