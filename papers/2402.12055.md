# [Are LLM-based Evaluators Confusing NLG Quality Criteria?](https://arxiv.org/abs/2402.12055)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Prior work has shown that large language models (LLMs) can perform well as automatic evaluators for natural language generation (NLG) tasks when prompted appropriately. 
- However, the authors discovered two issues: 1) LLM evaluation for one aspect sometimes correlates higher with human judgement on a different aspect, and 2) LLM evaluations tend to correlate more highly across different aspects compared to human judgement.  
- This suggests LLMs may be confusing different evaluation aspects, questioning their reliability.
- The authors attribute these issues partly to inconsistent conceptualization and vague expressions in definitions of evaluation aspects themselves.

Proposed Solution:
- Summarize a clear classification system for 11 commonly used aspects, with definitions of varying detail.  
- Design 18 types of fine-grained, aspect-targeted perturbation attacks to analyze evaluation behaviors.
- Conduct experiments on GPT-3.5 and GPT-4 using different aspect definitions.
- Validate impacts of perturbations via human annotation.

Key Contributions:
- First work to explore LLM capabilities in distinguishing aspects during NLG evaluation and impacts of criteria description.
- Summarize classification system and propose 18 aspect-targeted perturbations verified by humans.  
- Reveal confusion issues in LLM-based evaluation through experiments, even for powerful models like GPT-4.
- Show level of detail in aspect definitions does not alleviate confusion much.
- Resources and data released as a benchmark for future LLM evaluation research.

Overall, the paper highlights reliability issues in LLM-based NLG evaluators, questions assumptions in prior work, and provides analysis and resources to facilitate improvements.


## Summarize the paper in one sentence.

 Here is a one-sentence summary of the key points from the paper:

The paper reveals that large language models seem to confuse different natural language generation evaluation criteria, reducing their reliability, through targeted perturbation attack experiments guided by a carefully constructed classification system and verified by human judgments.


## What is the main contribution of this paper?

 Based on my understanding, the main contributions of this paper are:

1) The authors propose a classification system summarizing 11 commonly-used aspects for NLG evaluation and construct different types of criteria descriptions for each aspect. 

2) The authors design 18 types of fine-grained, aspect-targeted perturbation attacks to test the capabilities of LLMs in distinguishing aspects during NLG evaluation. The perturbations are manually crafted and verified to be reliable through human annotations.

3) Experiments are conducted on both proprietary LLMs (GPT-3.5 and GPT-4) and specifically fine-tuned LLMs (Prometheus) using the perturbation attacks. The results reveal issues of confusion across aspects inherent in current LLMs, indicating their unreliability in NLG evaluation.

4) The resources and datasets constructed in this work, including the classification system, perturbations, prompts, and experimental results, are released to facilitate future research on improving LLM-based NLG evaluation.

In summary, this paper explores the limitations of LLMs in conducting reliable NLG evaluation across different quality aspects through targeted perturbation attacks. The revealed issues and released resources help motivate and support further improvements in this direction.


## What are the keywords or key terms associated with this paper?

 Based on my review of the paper content, some of the key terms and keywords associated with this paper include:

- Large language models (LLMs)
- Natural language generation (NLG) evaluation 
- Evaluation aspects/criteria 
- Perturbation attacks
- Behavioral testing
- Classification system
- Confusion issues
- Reliability
- Direction expectation test
- Invariance test

The paper explores using perturbation attacks inspired by behavioral testing to analyze the capabilities of LLMs in distinguishing different aspects when conducting NLG evaluation. It summarizes a classification system for commonly used evaluation aspects and designs targeted perturbations for each. Experiments reveal issues like confusion across aspects in LLM-based evaluation, highlighting their unreliability and the need for further improvements. Overall, the key focus is on testing the fine-grained evaluation behaviors of LLMs across different quality criteria/aspects for NLG tasks using perturbation analysis.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in this paper:

1. The paper proposes a classification system for commonly used aspects in NLG evaluation. What are the key considerations and challenges in developing such a system? How does the proposed system address limitations of prior work in this area?

2. The paper utilizes perturbation attacks to analyze LLM evaluation capabilities. Why is the fine-grained, aspect-targeted nature of the attacks important? What are the advantages over more general perturbation strategies? 

3. The paper finds that detailed aspect definitions yield higher human annotation consistency and match rates with expected impacts of attacks. Why might this occur, and what are the implications for developing clear NLG quality criteria?  

4. The paper observes confusion and oversensitivity issues in LLM evaluation, even for large models like GPT-4. What factors might contribute to these issues, and how might they be addressed through model architecture, training data, or other means?

5. The high correlation of LLM evaluation scores across different levels of criteria detail suggests reliance on terminology. How might this finding inform prompt and instruction design for LLM evaluators? What risks does it pose?

6. The paper identifies verbosity bias and insensitivity to coherence issues in LLMs. Do the findings suggest these stem primarily from pre-training, fine-tuning, or are they fundamental issues? What interventions could reduce them?

7. Conflicting facts and grammatical issues disproportionately impact LLM evaluation of unrelated aspects. Why might LLMs struggle to isolate their influence, and how might that capability be improved?  

8. Why does the fine-tuning method used for Prometheus fail to improve aspect distinction capabilities relative to proprietary models? What alternative approaches may better impart that capacity?

9. The paper focuses on evaluating English text generation. How might the findings and method generalize to other languages? What considerations are necessary?

10. The findings reveal reliability issues in LLM evaluation. What new research directions could build upon this work to make LLM evaluators more robust and trustworthy? What are the highest priorities?
