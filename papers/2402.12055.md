# [Are LLM-based Evaluators Confusing NLG Quality Criteria?](https://arxiv.org/abs/2402.12055)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Prior work has shown that large language models (LLMs) can perform well as automatic evaluators for natural language generation (NLG) tasks when prompted appropriately. 
- However, the authors discovered two issues: 1) LLM evaluation for one aspect sometimes correlates higher with human judgement on a different aspect, and 2) LLM evaluations tend to correlate more highly across different aspects compared to human judgement.  
- This suggests LLMs may be confusing different evaluation aspects, questioning their reliability.
- The authors attribute these issues partly to inconsistent conceptualization and vague expressions in definitions of evaluation aspects themselves.

Proposed Solution:
- Summarize a clear classification system for 11 commonly used aspects, with definitions of varying detail.  
- Design 18 types of fine-grained, aspect-targeted perturbation attacks to analyze evaluation behaviors.
- Conduct experiments on GPT-3.5 and GPT-4 using different aspect definitions.
- Validate impacts of perturbations via human annotation.

Key Contributions:
- First work to explore LLM capabilities in distinguishing aspects during NLG evaluation and impacts of criteria description.
- Summarize classification system and propose 18 aspect-targeted perturbations verified by humans.  
- Reveal confusion issues in LLM-based evaluation through experiments, even for powerful models like GPT-4.
- Show level of detail in aspect definitions does not alleviate confusion much.
- Resources and data released as a benchmark for future LLM evaluation research.

Overall, the paper highlights reliability issues in LLM-based NLG evaluators, questions assumptions in prior work, and provides analysis and resources to facilitate improvements.
