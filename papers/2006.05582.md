# [Contrastive Multi-View Representation Learning on Graphs](https://arxiv.org/abs/2006.05582)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the main research question seems to be: How can we learn effective node and graph level representations in a self-supervised manner by maximizing the mutual information between different structural views of the graphs?The key hypotheses appear to be:1) Contrasting node encodings from one structural view with graph encodings from a different structural view (and vice versa) is an effective approach for self-supervised representation learning on both node and graph classification tasks. 2) Using a graph diffusion process to generate an additional global structural view, along with the local view from the adjacency matrix, provides useful congruent and incongruent signals for contrastive learning.3) Increasing the number of views or contrasting multi-scale encodings, unlike in visual representation learning, does not improve performance on graph tasks. The best results come from contrasting the local view (adjacency matrix) and global view (graph diffusion).4) Simple graph readout functions like sum pooling work better than hierarchical pooling methods like DiffPool for this self-supervised graph learning.So in summary, the main focus is on developing and evaluating a self-supervised contrastive learning framework for graphs that relies on contrasting local and global structural views. The key hypotheses center around the design choices for effectively generating views and integrating mutual information maximization on graphs.
