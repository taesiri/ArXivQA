# [Sparse Feature Circuits: Discovering and Editing Interpretable Causal   Graphs in Language Models](https://arxiv.org/abs/2403.19647)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Interpretability methods often explain neural network behaviors using coarse-grained components like attention heads or neurons. However, these are hard to interpret and apply to downstream tasks.  
- Methods based on fine-grained units require specifying the behavior of interest upfront, limiting discovery of unanticipated mechanisms.
- There is a need for methods to explain behaviors in terms of interpretable fine-grained units, without requiring prespecified behaviors.

Proposed Solution:
- Use sparse autoencoders (SAEs) to identify interpretable directions in a language model's latent space.
- View SAE features and errors as part of the model's computation graph.  
- Approximate the causal effect of SAE features and errors on model behaviors using attribution techniques.
- Discover sparse feature circuits - subgraphs of SAE features and errors influential for a behavior.
- Aggregate effects across examples and sometimes token positions.  

Contributions:
1. Method to discover interpretable sparse feature circuits explaining model behaviors. Evaluated on subject-verb agreement tasks.

2. Sparse Human-interpretable Feature Trimming (SHIFT) to improve model generalization by surgically removing sensitivity to unintended signals, without needing disambiguating data. Demonstrated on debiased profession classification.  

3. Fully unsupervised pipeline to discover thousands of behaviors via clustering and their explanatory feature circuits. Provides insights into mechanisms like succession and induction.

The key insight is to incorporate sparse autoencoder features into the model graph to enable scalable discovery of interpretable explanations using causal attribution techniques. The discovered sparse feature circuits have downstream applications like targeted debiasing and understanding automatically discovered behaviors.
