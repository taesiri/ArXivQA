# [Unsupervised pretraining transfers well across languages](https://arxiv.org/abs/2002.02848)

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper addresses is whether unsupervised pretraining of speech representations transfers well across languages, especially for low-resource languages. The key hypotheses are:- Unsupervised pretraining of speech representations in one language (English) can improve phoneme classification when transferred to other languages, even without any fine-tuning.- The gap between unsupervised and supervised pretraining for speech representations is small when using the same pretraining data.- Scaling unsupervised pretraining to larger unlabeled datasets can further reduce or even surpass the gap with supervised pretraining.The paper tests these hypotheses by pretraining contrastive predictive coding (CPC) models on English speech from Librispeech, then transferring the learned representations to phoneme classification tasks in several low-resource languages using the Common Voice dataset. The results support the hypotheses, showing unsupervised pretraining transfers well across languages and large unlabeled datasets can help it match or exceed supervised pretraining.


## What is the main contribution of this paper?

The main contribution of this paper is showing that unsupervised pretraining of speech representations transfers well across languages and can match or exceed the performance of supervised pretraining, especially when using larger unlabeled datasets. Specifically, the authors:- Modify the Contrastive Predictive Coding (CPC) approach for unsupervised pretraining to make it more stable and produce better features.- Use CPC to pretrain on English speech from Librispeech without transcripts. - Transfer these pretrained features to low-resource languages by training a simple linear classifier, without any fine-tuning.- Show this unsupervised pretraining transfers well and outperforms training from scratch, even compared to supervised pretraining on the same English dataset.- Demonstrate that unsupervised pretraining on larger unlabeled datasets (360 hrs of Librispeech) can match or exceed supervised pretraining from 100 hrs.- Evaluate cross-lingual transfer by phoneme classification on Common Voice datasets in 11 languages. Their approach achieves state-of-the-art performance compared to other unsupervised methods.So in summary, their key contribution is showing the potential of unsupervised pretraining for low-resource speech recognition across languages, rivaling supervised pretraining given sufficient unlabeled data. This could enable speech recognition for many languages without transcribed audio.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper investigates whether unsupervised pretraining of speech representations transfers well across languages, showing that a modified contrastive predictive coding approach achieves performance on par with or better than supervised pretraining when transferred to low-resource target languages.
