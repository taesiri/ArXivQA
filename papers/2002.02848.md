# [Unsupervised pretraining transfers well across languages](https://arxiv.org/abs/2002.02848)

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper addresses is whether unsupervised pretraining of speech representations transfers well across languages, especially for low-resource languages. The key hypotheses are:- Unsupervised pretraining of speech representations in one language (English) can improve phoneme classification when transferred to other languages, even without any fine-tuning.- The gap between unsupervised and supervised pretraining for speech representations is small when using the same pretraining data.- Scaling unsupervised pretraining to larger unlabeled datasets can further reduce or even surpass the gap with supervised pretraining.The paper tests these hypotheses by pretraining contrastive predictive coding (CPC) models on English speech from Librispeech, then transferring the learned representations to phoneme classification tasks in several low-resource languages using the Common Voice dataset. The results support the hypotheses, showing unsupervised pretraining transfers well across languages and large unlabeled datasets can help it match or exceed supervised pretraining.


## What is the main contribution of this paper?

The main contribution of this paper is showing that unsupervised pretraining of speech representations transfers well across languages and can match or exceed the performance of supervised pretraining, especially when using larger unlabeled datasets. Specifically, the authors:- Modify the Contrastive Predictive Coding (CPC) approach for unsupervised pretraining to make it more stable and produce better features.- Use CPC to pretrain on English speech from Librispeech without transcripts. - Transfer these pretrained features to low-resource languages by training a simple linear classifier, without any fine-tuning.- Show this unsupervised pretraining transfers well and outperforms training from scratch, even compared to supervised pretraining on the same English dataset.- Demonstrate that unsupervised pretraining on larger unlabeled datasets (360 hrs of Librispeech) can match or exceed supervised pretraining from 100 hrs.- Evaluate cross-lingual transfer by phoneme classification on Common Voice datasets in 11 languages. Their approach achieves state-of-the-art performance compared to other unsupervised methods.So in summary, their key contribution is showing the potential of unsupervised pretraining for low-resource speech recognition across languages, rivaling supervised pretraining given sufficient unlabeled data. This could enable speech recognition for many languages without transcribed audio.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper investigates whether unsupervised pretraining of speech representations transfers well across languages, showing that a modified contrastive predictive coding approach achieves performance on par with or better than supervised pretraining when transferred to low-resource target languages.


## How does this paper compare to other research in the same field?

Here are a few key points about how this paper compares to other research on unsupervised pre-training of speech representations:- The main contribution is showing that unsupervised pre-training with Contrastive Predictive Coding (CPC) transfers well across languages. Prior work like Wav2vec and Mockingjay focused more on monolingual unsupervised pre-training. - The authors make several modifications to the original CPC approach to improve performance, like replacing batch normalization with channel-wise normalization for more stable training. Their modifications lead to improved phonetic representations compared to the original CPC.- They perform experiments on low-resource languages from the Common Voice dataset, showing their unsupervised pre-trained features match or outperform supervised pre-training. Most prior work focused only on high-resource languages like English. This demonstrates the potential value for low-resource settings.- They show unsupervised pre-training on 360 hours of Librispeech matches supervised pre-training with just 100 hours. This highlights the benefits of scaling up unsupervised methods with more data compared to supervised approaches.- They obtain competitive results on ZeroSpeech challenges for unsupervised acoustic unit discovery. Prior CPC results were not competitive on these benchmarks.- Overall, this work pushes the state-of-the-art in cross-lingual transferability of unsupervised speech representations. The gains on low-resource languages and with more data are promising developments for applying these methods to new languages.In summary, the key innovations are achieving cross-lingual transfer with unsupervised pre-training, demonstrating strong performance on low-resource languages, and showing the potential of unsupervised methods to surpass supervised results with more data. The modifications to CPC also improve over prior applications of this approach.


## What future research directions do the authors suggest?

Based on my reading of the paper, some of the main future research directions suggested by the authors include:- Testing the proposed unsupervised pre-training methods on a wider range of low-resource languages to further evaluate their effectiveness for cross-lingual transfer. The paper tested 11 languages but expanding this could provide more insights.- Exploring different techniques for finetuning the pretrained features on the target low-resource languages, beyond just the linear classifier approach used in the paper. The authors note that further finetuning could help improve performance. - Applying the unsupervised pretraining methods to other speech tasks beyond just phoneme classification, such as full speech recognition. The authors mention this as an important direction.- Training the models on even larger and more diverse multilingual unlabelled datasets to further close the gap with supervised pretraining. The paper shows this helps but more data could further improve transferability.- Combining the proposed methods with other techniques like adversarial losses or data augmentation to encourage language-independent representations.- Testing the approach on a wider range of domains beyond just speech, to understand how generally applicable unsupervised pretraining is across modalities.So in summary, the main directions are expanding the language coverage, exploring different finetuning techniques, applying to more tasks, using more data, combining with other methods like adversarial learning, and testing on other modalities besides speech.
