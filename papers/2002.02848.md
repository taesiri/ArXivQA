# [Unsupervised pretraining transfers well across languages](https://arxiv.org/abs/2002.02848)

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper addresses is whether unsupervised pretraining of speech representations transfers well across languages, especially for low-resource languages. The key hypotheses are:- Unsupervised pretraining of speech representations in one language (English) can improve phoneme classification when transferred to other languages, even without any fine-tuning.- The gap between unsupervised and supervised pretraining for speech representations is small when using the same pretraining data.- Scaling unsupervised pretraining to larger unlabeled datasets can further reduce or even surpass the gap with supervised pretraining.The paper tests these hypotheses by pretraining contrastive predictive coding (CPC) models on English speech from Librispeech, then transferring the learned representations to phoneme classification tasks in several low-resource languages using the Common Voice dataset. The results support the hypotheses, showing unsupervised pretraining transfers well across languages and large unlabeled datasets can help it match or exceed supervised pretraining.
