# [KnowLA: Enhancing Parameter-efficient Finetuning with Knowledgeable   Adaptation](https://arxiv.org/abs/2403.14950)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Large language models (LLMs) have billions of parameters, making full finetuning computationally expensive. Parameter-efficient finetuning (PEFT) methods like LoRA are used to adapt LLMs to downstream tasks without changing most parameters. 

- Prior work injected knowledge graphs (KGs) into pre-trained language models (PLMs) like BERT. But these require full finetuning and their effectiveness for recent decoder-based LLMs is unknown. 

- Questions: Can knowledge injection enhance the PEFT of LLMs? How can it be used to improve PEFT methods like LoRA?

Proposed Solution:
- Propose KnowLA, a knowledgeable adaptation method that inserts an adaptation layer into an LLM to integrate KG entity embeddings for tokens in the input text.

- The adaptation layer aligns KG and LLM spaces and fuses entity embeddings with token representations using an attention mechanism.

- KnowLA is trained on instruction data in combination with LoRA, keeping LLM and entity embeddings frozen. This allows injecting knowledge while preserving efficiency.

Contributions:
- First attempt at injecting KG knowledge into LLMs for improving PEFT methods. More effective than retrieval methods.

- Experiments on 6 QA datasets with Llama and 3 KGs show KnowLA consistently improves LoRA, using only 0.5% extra parameters. Outperforms Alpaca baseline.

- Analysis shows KnowLA can align LLM and KG spaces and also activate relevant latent knowledge in the LLM itself. Allows answering without changing parameters or prompts.

- Showed robustness of KnowLA w.r.t. different LLMs, instruction data, PEFT methods, KG embedding models etc. Can bring stable improvements.
