# [Applying Large Language Models API to Issue Classification Problem](https://arxiv.org/abs/2401.04637)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Manual classification of GitHub issue reports is laborious and lacks scalability. 
- Open source software (OSS) projects rely on substantial datasets for training automated issue classification models.
- Need a reliable automated approach for issue prioritization that works well even with smaller training datasets.

Proposed Solution: 
- Leverage Generative Pre-trained Transformer (GPT) models which can efficiently handle text classification with less training data. 
- Develop a GPT-based system to accurately label and prioritize GitHub issues without needing extensive training data.

Methodology:
- Fine-tune GPT-3.5-turbo model on issue title and body text to predict labels (bug, feature, question). 
- Clean and preprocess issues, structure as conversation for API input.
- Train individual models per GitHub repo for better performance.
- Evaluate models using precision, recall and F1-score.

Results:
- Achieved avg precision of 83.24%, recall 82.87%, F1-score 82.8% across repos.
- Best F1-score of 89.34% for bug classification on tensorflow repo.
- Question labels were most difficult to classify accurately.

Contributions:  
- Showed feasibility of using GPT for accurate GitHub issue classification with limited training data.
- Developed methodology to fine-tune and apply models for multiple repositories.
- Provided insights into differences in performance across repos and issue types.

Overall, the paper demonstrates a GPT-based approach to automate the labeling and prioritization of GitHub issues reliably without needing large training datasets. The proposed techniques reduce data requirements while maintaining accuracy.


## Summarize the paper in one sentence.

 Here is a one-sentence summary of the paper:

The paper develops a GPT-based approach to accurately label and prioritize GitHub issue reports using fine-tuning, achieving strong performance in classifying issues as bugs, features, or questions while requiring less training data than typical machine learning techniques.


## What is the main contribution of this paper?

 Based on my reading of the paper, the main contribution is:

The development and evaluation of a methodology that leverages fine-tuned Generative Pre-trained Transformer (GPT) models from OpenAI to classify GitHub issue reports into "bug", "feature", or "question" categories. Specifically, the authors fine-tuned the gpt-3.5-turbo model on datasets from 5 different GitHub repositories and demonstrated the feasibility of using this approach to classify issues, obtaining a macro average F1-score of 82.8% across repositories and categories. A key aspect is reducing reliance on large training datasets by focusing on few-shot fine-tuning of the pre-trained language model. The paper discusses the variability in performance across repositories and labels, highlighting challenges in standardizing labeling practices in GitHub. Overall, it demonstrates a practical application of large language models to an important problem in software engineering.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, the keywords or key terms associated with this paper include:

Issue Report Classification, Large Language Model, Natural Language Processing, Software Engineering, Labeling, Multi-class Classification, Empirical Study


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in the paper:

1. The paper mentions using two different data cleaning methods. What are the key differences between these two methods and what was the rationale behind trying two different approaches? 

2. The process of fine-tuning a model through OpenAI's API seems intricate with multiple steps. Can you elaborate on some of the key challenges faced during this process and how they were addressed? 

3. The choice of the gpt-3.5-turbo model is stated but not justified. What were some of the other model options considered and why was this specific model selected? 

4. The paper adjusts the number of training epochs for each fine-tuned model. What criteria was used to determine that a model needed more training epochs? How much improvement was observed by increasing epochs?

5. There seems to be significant variation in performance across repositories and issue types. What are some potential reasons contributing to this? How can the model be made more robust to these variances?  

6. The lower F1 scores for 'question' type issues are concerning. Does this indicate a problem with how questions are formulated in issues or a limitation of the model? How can this be improved?

7. The state of labeling practices and standards within GitHub projects is highlighted as an area needing attention. What are some ways this could be improved or standardized?  

8. Ablation studies analyzing the contribution of the title versus body for classification could provide more insight. Was any analysis done on using only titles or bodies?

9. The choice to use precision, recall and F1-score is sound. Were any other metrics considered or tested? Would AUC-ROC curves provide additional useful evaluation?

10. The method seems very computationally intensive. What is the feasibility of implementing this on a large scale across thousands of GitHub projects? Could optimization help?
