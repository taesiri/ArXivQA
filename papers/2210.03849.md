# [ConvFinQA: Exploring the Chain of Numerical Reasoning in Conversational   Finance Question Answering](https://arxiv.org/abs/2210.03849)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the abstract, the central research questions this paper addresses are:

1. Which is the more fundamental approach for solving complex reasoning problems - neural symbolic models or prompting-based methods using large language models?

2. How well do these two approaches perform on a new complex reasoning task involving conversational question answering and numerical reasoning over financial reports?

3. What are the strengths and weaknesses of each approach on this new task? What insights can be gained?

Specifically, the authors propose a new dataset called ConvFinQA for conversational question answering that requires complex numerical reasoning over financial reports. They conduct experiments using both neural symbolic models and prompting-based methods on this dataset. Their goal is to analyze the reasoning mechanisms and provide insights into these two types of approaches for complex real-world reasoning tasks. The central hypothesis seems to be that prompting-based methods may start to struggle or underperform on tasks like this that require reasoning on specialized domains, compared to neural symbolic approaches designed specifically for the task.


## What is the main contribution of this paper?

 Based on my reading of the abstract, the main contributions of this paper appear to be:

1. Proposing a new large-scale dataset called ConvFinQA for studying complex numerical reasoning in conversational question answering over financial reports. 

2. Using this dataset to analyze the performance of both neural-symbolic methods and prompting-based methods, providing insights into their reasoning mechanisms.

3. Showing that both kinds of approaches still lag behind human performance, indicating the challenge of modeling complex real-world reasoning chains. 

4. Demonstrating that while prompting-based methods excel at simple reasoning tasks, they struggle with new complex task paradigms like in ConvFinQA.

5. Arguing that ConvFinQA should serve as a valuable benchmark for pushing research on complex reasoning in specialized real-world domains.

In summary, the key contribution seems to be the new ConvFinQA dataset for studying challenging numerical reasoning, along with experiments using this dataset to compare reasoning approaches and analyze their limitations. The authors highlight the need for continued research on complex real-world reasoning as an important direction.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the paper:

The paper proposes a new conversational QA dataset in the finance domain, ConvFinQA, to study complex numerical reasoning chains, and experiments with neural-symbolic and prompting methods, finding both have difficulties in learning long-range reasoning dependencies.
