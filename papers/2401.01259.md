# [Do Concept Bottleneck Models Obey Locality?](https://arxiv.org/abs/2401.01259)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
The paper investigates whether Concept Bottleneck Models (CBMs), a popular family of interpretable deep learning models, correctly capture the degree of conditional independence across concepts when such concepts are spatially or semantically localized. Specifically, the authors introduce the notions of "spatial locality" and "semantic locality" to refer to concepts that are localized to a subset of input features or semantically related concepts respectively. The goal is to analyze whether CBMs learn to confine their predictions to these localized regions or if they incorrectly rely on unrelated regions, compromising the quality of their concept-based explanations.

Proposed Solution:  
The authors propose quantitative metrics to measure spatial and semantic locality in CBMs. For spatial locality, they measure the ease of changing a CBM's concept prediction by modifying features outside the concept's known localized region using gradient ascent. For semantic locality, they train CBMs on a subset of concept combinations and test on novel combinations to see if unrelated concepts impact each other.  

Experiments:
Using synthetic datasets where concepts are perfectly spatially/semantically localized by construction, the authors demonstrate that CBMs struggle to confine concepts to their localized regions. Increasing model capacity worsens spatial locality. Even in real datasets like CUB, modifications to spatially distant pixels change concept predictions. For semantic locality, CBMs struggle to generalize to novel concept combinations, simply memorizing training combinations instead. 

Main Contributions:
1) Formalized the notions of spatial and semantic locality for evaluating if CBMs learn localized concept representations
2) Demonstrated that CBMs struggle with both spatial and semantic locality, compromising the quality of their concept-based explanations
3) Showed model capacity and diversity of concept combinations impact locality, providing insights into factors influencing CBMs' explanation quality

The paper raises concerns about the robustness of CBMs' explanations. The authors plan to incorporate techniques from adversarial robustness to help CBMs better capture locality.


## Summarize the paper in one sentence.

 Here is a one sentence summary of the key points from the paper:

The paper investigates whether Concept Bottleneck Models properly learn the locality, both spatially and semantically, present in concept-annotated datasets used for training, finding they often fail to fully capture such locality leading to fragile concept representations.


## What is the main contribution of this paper?

 Based on my reading of the paper, the main contribution is:

The paper introduces the notions of "spatial locality" and "semantic locality" for concept-based models like Concept Bottleneck Models (CBMs). Spatial locality refers to whether a model learns that a concept's value depends only on a subset of input features (its "local region"). Semantic locality refers to whether a model learns that a concept's value is independent of semantically unrelated concepts. 

The paper then empirically evaluates whether CBMs correctly capture spatial and semantic locality through experiments on synthetic and real-world datasets. The key findings are:

1) CBMs fail to capture spatial locality when the concept predictor is overparameterized, with larger models exhibiting more "spatial locality leakage". 

2) CBMs also fail to properly learn semantic locality, struggling to make accurate concept predictions for novel combinations of concepts not seen during training.

Overall, the paper demonstrates limitations in CBMs' ability to learn meaningful, robust concept representations that properly respect locality, raising doubts about the quality of their concept-based explanations. The introduction and analysis of spatial/semantic locality is the main contribution.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, here are some of the key terms and concepts:

- Concept Bottleneck Models (CBMs): A class of interpretable deep learning models that predict concepts from inputs then use those concepts for downstream predictions.

- Concept locality: The notion that a concept's prediction should depend only on a subset of input features (spatial locality) or other concepts (semantic locality). 

- Spatial locality: The idea that a concept's prediction should rely only on a specific region of the input determined by the concept's "local region".

- Semantic locality: The idea that a concept's prediction should not be impacted by changes to semantically unrelated concepts. 

- Locality leakage: When a model fails to properly capture locality in its learnt concept representations. 

- Synthetic datasets: Simple datasets created with spatial and semantic locality baked in, used to analyze if models capture these localities.

- Concept accuracy: The accuracy of a model's concept predictions.

- CODA: "Concept Out-of-Distribution Accuracy", a metric measuring a model's concept accuracy on combinations of concepts not seen during training.

- Model capacity: Factors like number of parameters that impact the complexity of a model. The paper finds this impacts spatial locality.

- PCA accuracy: The separability of concepts when projecting inputs into 2D, used to understand concept complexity.

The key focus is on analyzing if CBMs accurately capture spatial and semantic locality through metrics like locality leakage and CODA. The results demonstrate CBMs struggle with this.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 potential in-depth questions about the method proposed in this paper:

1. The paper introduces the notions of spatial and semantic locality for concept-based models. Could you expand more on the precise definitions of these types of locality and how they relate to the robustness of concept-based explanations? 

2. The paper shows that concept bottleneck models (CBMs) fail to properly learn spatial and semantic locality on both synthetic and real-world datasets. What are some potential reasons why CBMs struggle to capture these types of locality?

3. One result is that overparameterization of the CBM's concept predictor leads to worse spatial locality. Why might larger capacity concept predictors lose spatial locality, even when concepts are perfectly spatially localized in the training data?

4. For spatial locality, the paper shows that CNN concept predictors are worse than MLPs. What differences between CNNs and MLPs could account for the MLPs better capturing spatial locality?

5. The paper argues that semantic locality implies concept predictions should be conditioned only on relevant concepts. What types of inductive biases or regularization could encourage this conditional independence between concepts?  

6. Figure 4 shows that concept predictions tend to stay close to combinations seen during training, rather than properly generalizing. What could explain this failure case when CBMs are presented with new combinations of concepts at test time?

7. The linear SVM results suggest some concepts are inherently more difficult to predict in a disentangled way. How could the concept learner or concept annotations be improved to alleviate this issue?

8. The paper analyzes locality by modifying features outside a concept's spatial locality and measuring changes in prediction. What are other ways spatial and semantic locality could be quantitatively measured?

9. For spatial locality, the paper shows that pruning CNNs decreases accuracy before decreasing locality leakage. Why might large pruned CNNs still fail to be spatially localized?

10. The paper argues that locality is critical for concept-based explanations. What are some scenarios where lack of spatial or semantic locality could result in untrustworthy or fragile concept-based explanations?
