# [Likelihood-based Mitigation of Evaluation Bias in Large Language Models](https://arxiv.org/abs/2402.15987)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Large language models (LLMs) are being used to evaluate natural language generation tasks. However, the likelihood score calculated by LLMs can vary substantially between sentences with the same meaning, due to differences in word order, sentence structure, etc.  
- This can lead to a "likelihood bias" where LLM evaluators overrate high likelihood sentences and underrate low likelihood ones, compared to human judgement. This likelihood bias likely impacts non-intrinsic evaluation criteria like relevance more than intrinsic criteria like fluency.

Proposed Solution:  
- Introduce a quantitative measure called "BiasScore" to determine the strength of likelihood bias, based on the correlation between likelihood score and unfairness score (difference between LLM score and human score).
- Propose a bias mitigation method that uses highly biased instances from the training set as few-shot examples to retrain the LLM evaluator.  

Key Contributions:
- Show that likelihood bias exists in LLM evaluators like GPT-3.5 and Llama2-13B when evaluating data-to-text and grammatical error correction tasks. Bias is higher for non-intrinsic criteria.  
- Demonstrate that the proposed bias quantification and mitigation method successfully reduces likelihood bias and also boosts evaluation performance.
- Overall, this work identifies and provides solutions for an important bias issue affecting LLM evaluators, with both bias analysis and performance improvements.
