# [Likelihood-based Mitigation of Evaluation Bias in Large Language Models](https://arxiv.org/abs/2402.15987)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Large language models (LLMs) are being used to evaluate natural language generation tasks. However, the likelihood score calculated by LLMs can vary substantially between sentences with the same meaning, due to differences in word order, sentence structure, etc.  
- This can lead to a "likelihood bias" where LLM evaluators overrate high likelihood sentences and underrate low likelihood ones, compared to human judgement. This likelihood bias likely impacts non-intrinsic evaluation criteria like relevance more than intrinsic criteria like fluency.

Proposed Solution:  
- Introduce a quantitative measure called "BiasScore" to determine the strength of likelihood bias, based on the correlation between likelihood score and unfairness score (difference between LLM score and human score).
- Propose a bias mitigation method that uses highly biased instances from the training set as few-shot examples to retrain the LLM evaluator.  

Key Contributions:
- Show that likelihood bias exists in LLM evaluators like GPT-3.5 and Llama2-13B when evaluating data-to-text and grammatical error correction tasks. Bias is higher for non-intrinsic criteria.  
- Demonstrate that the proposed bias quantification and mitigation method successfully reduces likelihood bias and also boosts evaluation performance.
- Overall, this work identifies and provides solutions for an important bias issue affecting LLM evaluators, with both bias analysis and performance improvements.


## Summarize the paper in one sentence.

 This paper proposes a method to quantify and mitigate likelihood bias in large language model-based evaluators, which overrate high-likelihood sentences and underrate low-likelihood ones compared to human scores.


## What is the main contribution of this paper?

 Based on my reading of the paper, the main contribution is a method to quantify and mitigate likelihood bias in large language model (LLM)-based evaluators. Specifically:

1) The paper introduces the concept of "likelihood bias", which is the phenomenon of LLM-based evaluators overrating high-likelihood sentences and underrating low-likelihood ones compared to human scores. 

2) The paper proposes a metric called "BiasScore" to quantify the degree of likelihood bias in LLMs when used as evaluators.

3) The paper presents a method to mitigate likelihood bias by using highly biased instances from the training data as few-shot examples for in-context learning. 

4) Experiments on data-to-text and grammatical error correction tasks demonstrate that several LLMs exhibit likelihood bias, and that the proposed bias mitigation method successfully reduces this bias and also improves correlation with human scores.

In summary, the main contribution is identifying, quantifying, and mitigating the issue of likelihood bias in LLM-based text evaluators. The paper introduces the bias metric and bias reduction method and shows their effectiveness empirically.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and concepts are:

- Large Language Models (LLMs)
- Likelihood bias - The phenomenon of LLMs overrating high-likelihood texts and underrating low-likelihood ones compared to human scores
- Likelihood Score (LS) - A score representing the likelihood calculated by the LLM for a text
- Unfairness Score (US) - The difference between LLM scores and human scores for a text  
- BiasScore - A proposed metric to measure the degree of likelihood bias, calculated as the correlation between LS and US
- Intrinsic vs non-intrinsic evaluation criteria - Intrinsic criteria depend only on the text itself while non-intrinsic criteria depend on the relation between input and output. Non-intrinsic criteria are more prone to likelihood bias.  
- Mitigation method - A proposed method to mitigate likelihood bias by using highly biased instances, identified by a high Relative Score (RS), as few-shot examples for in-context learning
- Data-to-text and Grammatical Error Correction (GEC) - Two tasks used to evaluate likelihood bias and the proposed mitigation method

The key goals are to quantify, analyze and mitigate the effect of likelihood bias when using LLMs as automated evaluators of text generation.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in this paper:

1. The paper proposes a likelihood-based mitigation method for evaluation bias in large language models. Could you explain in more detail how the likelihood score (LS) is calculated and what it represents? 

2. The unfairness score (US) aims to quantify the difference between human and model scores. What are some limitations of using US as a measure of model unfairness? How could it be improved?

3. The bias score combines LS and US to measure the tendency of overrating high likelihood sentences. What are some alternative ways to quantify this bias? Could you propose another metric? 

4. The paper identifies intrinsic versus non-intrinsic evaluation criteria as an important factor influencing likelihood bias. Why do you think non-intrinsic criteria are more prone to bias? Can you elaborate?

5. The mitigation method utilizes highly biased instances for in-context learning. Why are these instances selected? What is the intuition behind using them to reduce bias? 

6. Only 8 biased instances are used for mitigation. How was this number determined? How would results change with more or fewer instances? What is the tradeoff?

7. For reproducibility, deterministic hyperparameters were used for GPT-3.5. How might non-deterministic sampling impact bias measurement and mitigation? Would results generalize?

8. The evaluation metrics focus on correlation with human scores. What are other relevant metrics for evaluating the bias mitigation method and its impact?

9. The method relies on likelihood approximation from Llama2-13B for GPT-3.5. How accurate do you expect this approximation to be? What is the effect on bias quantification and mitigation? 

10. The paper studies likelihood bias primarily in the context of data-to-text and GEC tasks. How would you expect the presence and extent of bias to manifest in other NLP tasks? Why?
