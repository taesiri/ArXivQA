# [ESPN: Memory-Efficient Multi-Vector Information Retrieval](https://arxiv.org/abs/2312.05417)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Multi-vector neural IR models like ColBERT achieve higher retrieval quality but have massive index sizes, 26-34x larger than lexical models like BM25. This poses scalability challenges.
- Storing indices in memory provides fast retrieval but is expensive to scale. Using mmap or swap space for larger datasets introduces software overheads and high query latencies.

Proposed Solution: 
- The authors propose ESPN, which offloads the entire re-ranking embedding tables to SSDs, reducing memory requirements by 5-16x. 
- They use GPUDirect Storage for direct SSD-to-GPU transfers.
- A novel approximate nearest neighbor (ANN) based prefetcher is designed which achieves >90% hit rates. It overlaps SSD reads with ANN search computation.  
- An early re-ranking stage ranks prefetched embeddings to further reduce query latency.

Main Contributions:
1. ESPN reduces memory footprint of multi-vector models to be only 1.6x that of BM25, improving scalability.
2. The prefetcher improves SSD retrieval latency by 3.9-6.4x and maintains near memory-level query latencies.
3. Bandwidth efficient partial re-ranking enables ESPN to handle large query batch sizes with marginal 0.3-0.7% quality loss.
4. ESPN achieves 3.1-3.9x faster query latency than mmap near memory capacity and is scalable to larger datasets.

In summary, ESPN enables memory-efficient and scalable inference for multi-vector neural IR models by offloading indices to SSDs and using novel prefetching strategies to accelerate storage access.
