# [Modality Plug-and-Play: Elastic Modality Adaptation in Multimodal LLMs   for Embodied AI](https://arxiv.org/abs/2312.07886)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a high-quality paragraph summarizing the key points of the paper:

This paper presents a new technique called Modality Plug-and-Play in multimodal Large Language Models (mPnP-LLM) to enable elastic, automated, and prompt runtime modality adaptation for embodied AI applications. mPnP-LLM connects unimodal encoders to a flexible set of last decoder blocks in LLMs via trainable latent connections. This allows adapting between modalities like RGB camera views and LiDAR based on environmental context, while minimizing training costs. Experiments on a nuScenes QA dataset demonstrate mPnP-LLM reduces FLOPs by up to 3.7x and GPU memory usage by 30\% compared to baselines, while retaining on-par accuracy. Under the same compute budget, mPnP-LLM improves QA accuracy by up to 4\%. The design focuses on widely used decoder-only LLMs and inserts multimodal tokens as new key-value pairs into LLMs' multi-head attention using interfaces from popular frameworks like HuggingFace. This avoids manual programming efforts for modality adaptation at runtime.
