# [Let's Focus on Neuron: Neuron-Level Supervised Fine-tuning for Large   Language Model](https://arxiv.org/abs/2403.11621)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Large language models (LLMs) contain neurons that exhibit varying behaviors and roles. As models scale up, the neurons become increasingly diversified. 
- Recent studies show that not all neurons are actively used across datasets, and this sparsity correlates with task-specific abilities.
- Traditional fine-tuning methods update all LLM parameters, which is computationally expensive. Parameter-efficient fine-tuning (PEFT) methods operate at a macro level (e.g. layer-level).

Proposed Solution:
- Introduce Neuron-Level Fine-Tuning (NeFT), a novel approach that fine-tunes parameters at the neuron-level granularity. This allows more precise and efficient model updates.
- Identify sensitive neurons by evaluating cosine similarity of each neuron between the original and fine-tuned model. Neurons with low similarity are treated as sensitive.
- Only update the gradients of the sensitive neurons during fine-tuning.

Main Contributions:
- NeFT outperforms full-parameter fine-tuning and other PEFT methods in most translation and summarization tasks.
- Analysis shows neurons exhibit varying sensitivity during fine-tuning. Neurons strongly affected by fine-tuning have significant parameter utilization changes. 
- Important neurons for one task tend to be relevant for other similar tasks, implying potential for transfer learning.
- Categorize neurons into strongly affected, suppressed and indirectly affected. Contrasting selection strategies induce greater volatility in neuron utilization.
- The approach is flexible and does not require additional model parameters or structures.

In summary, the paper introduces Neuron-Level Fine-Tuning to efficiently update only the most sensitive neurons in large language models. This granular approach outperforms traditional methods and provides insights into neuron behaviors.
