# Learning to Reason and Memorize with Self-Notes

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is: How can we enable language models to perform explicit multi-step reasoning and state tracking by allowing them to take "self-notes" that interleave with the input context?The key ideas and hypotheses behind this research question are:- Vanilla language models struggle with multi-step reasoning and state tracking due to their fixed per-token computation and lack of explicit memory. - Allowing models to take "self-notes" - to deviate from the input context and write notes to themselves - can act as both explicit reasoning steps and memory for state tracking.- Interleaving these self-notes with the context allows reasoning while reading, keeping inferences closer to relevant context. It also acts as a recurrent memory, overcoming transformers' limitations as feedforward networks.- This approach can improve reasoning, state tracking, and generalization to longer sequences, in contrast to prior work like scratchpads that postpone reasoning until after reading the full context.So in summary, the central hypothesis is that allowing language models to take inline self-notes will improve their reasoning and memory capabilities compared to models without any explicit note-taking. The paper aims to test this hypothesis across several reasoning and state tracking tasks.


## What is the main contribution of this paper?

The main contribution of this paper is proposing a method called "Self-Notes" that allows language models to take notes and do reasoning while reading a context, rather than only after reading the full context like in prior work. The Self-Notes method allows the model to deviate from the input context at any time to generate explicit reasoning tokens, which can act as both intermediate reasoning steps and memory for state tracking. This approach is shown through experiments to help the model generalize better to longer and more complicated test instances than seen during training.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper proposes a method called "Self-Notes" that allows language models to take notes while reading text, enabling multi-step reasoning and overcoming limited context memory.


## How does this paper compare to other research in the same field?

Here are a few key ways this paper compares to other related work:- The idea of allowing models to take "self-notes" is similar to prior work on rationales, scratchpads, and chain-of-thought prompting. However, this paper proposes an "online" variant that allows note-taking interleaved with the context, rather than only at the end.- The paper argues that taking notes inline allows reasoning steps to stay closer to the relevant context, and also acts as a memory since notes get fed back into the model. This differentiates the approach from post-context scratchpads/rationales.- The paper focuses on studying the properties and abilities of the transformer architecture itself, rather than relying on very large models where chain-of-thought is an emergent capability.- The experiments aim to test multi-step reasoning and state tracking abilities. Related work has studied length extrapolation, but often in a language modeling setting. The tasks here are more reasoning-focused.- For training, the paper proposes supervised, semi-supervised, and unsupervised methods. Related work on rationales and scratchpads relies more on supervision.- The unsupervised method trains the model to ask questions about its own samples. This is a novel approach compared to prior unsupervised rationale work.Overall, the key novelty seems to be allowing the model to take notes interleaved with the context, rather than separated. The paper argues this allows better reasoning, memory, and length generalization. The experiments aim to demonstrate these benefits over baseline transformers and scratchpad approaches.
