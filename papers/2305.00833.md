# [Learning to Reason and Memorize with Self-Notes](https://arxiv.org/abs/2305.00833)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is: How can we enable language models to perform explicit multi-step reasoning and state tracking by allowing them to take "self-notes" that interleave with the input context?The key ideas and hypotheses behind this research question are:- Vanilla language models struggle with multi-step reasoning and state tracking due to their fixed per-token computation and lack of explicit memory. - Allowing models to take "self-notes" - to deviate from the input context and write notes to themselves - can act as both explicit reasoning steps and memory for state tracking.- Interleaving these self-notes with the context allows reasoning while reading, keeping inferences closer to relevant context. It also acts as a recurrent memory, overcoming transformers' limitations as feedforward networks.- This approach can improve reasoning, state tracking, and generalization to longer sequences, in contrast to prior work like scratchpads that postpone reasoning until after reading the full context.So in summary, the central hypothesis is that allowing language models to take inline self-notes will improve their reasoning and memory capabilities compared to models without any explicit note-taking. The paper aims to test this hypothesis across several reasoning and state tracking tasks.


## What is the main contribution of this paper?

The main contribution of this paper is proposing a method called "Self-Notes" that allows language models to take notes and do reasoning while reading a context, rather than only after reading the full context like in prior work. The Self-Notes method allows the model to deviate from the input context at any time to generate explicit reasoning tokens, which can act as both intermediate reasoning steps and memory for state tracking. This approach is shown through experiments to help the model generalize better to longer and more complicated test instances than seen during training.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper proposes a method called "Self-Notes" that allows language models to take notes while reading text, enabling multi-step reasoning and overcoming limited context memory.


## How does this paper compare to other research in the same field?

Here are a few key ways this paper compares to other related work:- The idea of allowing models to take "self-notes" is similar to prior work on rationales, scratchpads, and chain-of-thought prompting. However, this paper proposes an "online" variant that allows note-taking interleaved with the context, rather than only at the end.- The paper argues that taking notes inline allows reasoning steps to stay closer to the relevant context, and also acts as a memory since notes get fed back into the model. This differentiates the approach from post-context scratchpads/rationales.- The paper focuses on studying the properties and abilities of the transformer architecture itself, rather than relying on very large models where chain-of-thought is an emergent capability.- The experiments aim to test multi-step reasoning and state tracking abilities. Related work has studied length extrapolation, but often in a language modeling setting. The tasks here are more reasoning-focused.- For training, the paper proposes supervised, semi-supervised, and unsupervised methods. Related work on rationales and scratchpads relies more on supervision.- The unsupervised method trains the model to ask questions about its own samples. This is a novel approach compared to prior unsupervised rationale work.Overall, the key novelty seems to be allowing the model to take notes interleaved with the context, rather than separated. The paper argues this allows better reasoning, memory, and length generalization. The experiments aim to demonstrate these benefits over baseline transformers and scratchpad approaches.


## What future research directions do the authors suggest?

Based on my reading of the paper, here are some of the future research directions the authors suggest:- Using reinforcement learning to discover optimal self-notes instead of relying on supervised labels. This would reduce the amount of supervision needed. - Testing whether very large models can learn to ask good self-note questions without any supervision, due to their emergent capabilities.- Combining the self-notes method with a scratchpad, to get the benefits of both inline reasoning while reading the context as well as backward reasoning from the question.- Training larger models with self-notes. The experiments in the paper were done with the 124M parameter GPT-2 model. The authors suggest training larger models as future work.- Testing the approach on more tasks, including dialog tasks where online reasoning and memory are critical. The tasks explored in the paper were primarily synthetic reasoning tasks.- Exploring different prompting techniques for eliciting good self-notes from the model.In summary, the main future directions mentioned are around reducing supervision, testing the scales at which the method works, combining it with other techniques like scratchpads, and evaluating on more diverse tasks requiring online reasoning and memory.


## Summarize the paper in one paragraph.

Here is a one paragraph summary of the paper:The paper proposes a method called "Self-Notes" that allows language models to take notes while reading text, in order to improve their ability to do multi-step reasoning and track state over long contexts. Unlike prior work like scratchpads that allow models to reason after reading the full context, Self-Notes lets models generate explicit reasoning tokens that interleave with the input context. This allows the model to recall information and perform reasoning on the fly as it processes the context. The model is trained to generate notes by providing ground truth notes in the training data. At test time, the model can deviate from the input context to take notes when needed. Experiments on reasoning and state tracking tasks show Self-Notes can generalize better to longer test instances than both vanilla language models and scratchpad models. The paper also explores semi-supervised and unsupervised training methods for Self-Notes. Overall, the interleaved nature of Self-Notes allows better multi-step reasoning and state tracking compared to postponing reasoning until after the full context.


## Summarize the paper in two paragraphs.

Here is a two paragraph summary of the paper:The paper proposes a new method called "Self-Notes" that allows language models to take notes while reading a context, in order to improve their ability to do multi-step reasoning and state tracking. The key idea is that the model can deviate from the input context at any time to write explicit reasoning steps or track state. This is different from prior methods like scratchpads or chain-of-thought prompting, where reasoning happens after the full context is processed. The Self-Notes method was tested on several synthetic and real-world datasets that require multi-step reasoning or state tracking, including a new proposed Toy Story task, algorithmic tasks, boolean logic tasks, and chess games. Across these datasets, Self-Notes outperformed baseline transformer models without explicit note-taking, as well as models with scratchpads. The paper also proposes semi-supervised and unsupervised methods for training Self-Notes. Overall, the results validate that allowing models to interleave reasoning tokens with the input context enables better multi-step reasoning and state tracking compared to doing reasoning after the full context is seen.


## Summarize the main method used in the paper in one paragraph.

Here is a one paragraph summary of the main method used in the paper:The paper proposes a method called "Self-Notes" that allows language models to take explicit notes while processing an input context, in order to improve multi-step reasoning and state tracking. During training, the model is provided with ground truth notes inserted within the context passages. At test time, the model can generate a special token to start writing a note, generate the note content, and then generate an end token to continue with the original context. This allows the model to write down intermediate reasoning steps and state changes on-the-fly as it processes the context. The notes act as both explicit reasoning chains and memory that gets fed back into the model. Experiments on multi-step reasoning tasks and program evaluation tasks demonstrate that Self-Notes improves performance compared to vanilla transformers and scratchpad methods, especially on longer test instances and for state tracking. The method is validated on a 124M parameter GPT-2 model.
