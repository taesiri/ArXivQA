# [DynIBaR: Neural Dynamic Image-Based Rendering](https://arxiv.org/abs/2211.11082)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is:How can we synthesize high-quality novel views from long, in-the-wild monocular videos depicting complex dynamic scenes with uncontrolled camera motion and object motion? The key hypothesis is that by adopting an image-based rendering approach within a volumetric framework, the system can better handle complex object and camera motions in long videos compared to prior dynamic NeRF methods that encode the entire scene within MLP weights.In summary, the paper aims to develop a novel view synthesis approach that:1) Can handle long input videos (vs just 1-2 seconds for prior methods)2) Works for complex, in-the-wild videos with uncontrolled camera motion and scene motion 3) Achieves significantly higher rendering quality on dynamic regions compared to state-of-the-art dynamic NeRF methodsThe central hypothesis is that an image-based rendering approach will be better able to meet these goals compared to canonical space methods like HyperNeRF or scene flow based methods like NSFF.


## What is the main contribution of this paper?

The main contribution of this paper is presenting a new approach for synthesizing novel views of complex dynamic scenes from a monocular video input. The key ideas are:- Adopting an image-based rendering framework that aggregates features from nearby views to render novel views, rather than encoding the full dynamic scene in a canonical MLP model like recent dynamic NeRF methods. This makes the approach more scalable to long, complex videos.- Adjusting feature aggregation based on estimated scene motion trajectories, which allows correctly reasoning about correspondences in dynamic scenes where epipolar constraints are violated. - Enforcing temporal consistency in the dynamic scene reconstruction through cross-time rendering losses.- Improving rendering quality by factoring the scene into separate static and dynamic models, using motion segmentation masks derived from a lightweight pretrained model.- Demonstrating significant improvements in rendering quality over prior state-of-the-art methods on benchmark dynamic scene datasets, as well as the ability to generate high-quality novel views on challenging in-the-wild videos.In summary, the main contribution is a new dynamic scene representation that combines ideas from image-based rendering and recent neural volumetric modeling techniques to achieve higher quality view synthesis on complex, real-world dynamic videos compared to prior work.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper presents a new approach for synthesizing novel views of dynamic scenes from monocular video that represents the scene using a volumetric image-based rendering framework which aggregates features from nearby views in a scene motion-aware manner to handle complex camera and object motions in long, unconstrained videos.
