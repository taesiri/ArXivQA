# ART: Automatic multi-step reasoning and tool-use for large language   models

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central hypothesis appears to be:The \sys framework can automatically generate high-quality multi-step reasoning decompositions and select appropriate tools for new tasks, without requiring additional training or fine-tuning of the language model. The key claims seem to be:1) By retrieving demonstrations of related tasks from a task library and formatting them using a structured query language, \sys can prompt a frozen LM to decompose new tasks in a generalizable way, transferring reasoning steps and tool usage from the demonstrations.2) The interpretable program format makes it easy to parse tool calls, execute them, and integrate the outputs back into the reasoning chain. This allows effective use of tools like search engines and code execution without specialized prompting.3) The framework is extensible - humans can provide feedback by editing programs or adding new tools, which can further improve performance on specific tasks with minimal additional supervision.4) Experiments show \sys outperforms baselines like few-shot prompting and automatic CoT generation on unseen BigBench and MMLU tasks. With human feedback, it matches or exceeds prior specialized approaches that use decomposition or tool supervision.In summary, the main hypothesis seems to be that the combination of structured demonstrations and flexible tool integration enables strong multi-step reasoning and tool use by frozen LMs, without task-specific training or prompting. The paper aims to demonstrate this through experiments on diverse reasoning tasks.


## What is the main contribution of this paper?

Based on my reading, the main contribution of this paper is presenting a framework called Automatic Reasoning and Tool-use (ART) that uses a frozen large language model (LLM) to automatically generate multi-step reasoning decompositions and select appropriate tools to use for new tasks, without requiring additional training or supervision. Specifically, the key aspects of the ART framework include:- A task library containing demonstrations of multi-step reasoning decompositions for related seed tasks, following a structured format defined by a parsing expression grammar. This allows the LLM to learn how to decompose new tasks.- A tool library containing useful external tools like search engines, code execution, etc. that can be called during decomposition steps to improve reasoning.- At test time, the LLM generates a decomposition for a new task, and ART parses it to identify tool calls, executes those tools, and integrates their output back into the program before resuming LLM generation.- Minimal supervision is required - just a description and examples of the new task. The LLM generalizes from the task library demonstrations.- The decompositions are interpretable and can be edited by humans to improve reasoning and extend tool use if needed.The authors evaluate ART on a variety of reasoning tasks from the BigBench and MMLU benchmarks. The results show it consistently matches or improves over few-shot prompting, automatic CoT generation, and expert-crafted CoT prompts for most tasks. The tool use in particular provides significant gains. With additional human feedback, ART outperforms the best prior GPT-3 results on 12 tasks by over 20 percentage points on average.In summary, the key contribution is an automated framework for interpretable multi-step reasoning and flexible tool use, without task-specific training, that also allows easy human improvements. This advances the capabilities of LLMs for complex reasoning in a low-resource setting.


## How does this paper compare to other research in the same field?

Here is a critical analysis of how this paper compares to other research in the same field:The paper presents a framework called NLProgrammer for automatically generating multi-step reasoning and tool use for large language models (LLMs) in a few-shot setting. The key ideas include:- Using a flexible query language to represent chained reasoning as "programs" with tool calls. This builds on prior work like decomposed prompting by Khot et al. - Providing few-shot demonstrations of programs for related tasks, so the LLM can generalize tools and substeps. This is similar to the idea behind AutoCOT by Zhang et al. of eliciting chaining from LLMs.- Supporting human-in-the-loop refinement of programs and tools. Related ideas have been explored in systems like Prompt Chainer.Compared to AutoCOT and other automatic chaining methods, a key distinction is the structured program representation that allows parsing tool calls. This improves accuracy of intermediate steps. The programs also enable easier human feedback.Compared to tool use methods like ToolFormer, a key difference is NLProgrammer is gradient-free. It does not require specialized training for tools. This provides flexibility - new LLMs and tools can be dropped in. However, a learned approach may have better zero-shot performance.Overall, NLProgrammer combines the strengths of automatic chaining and tool use in a flexible framework. The program representation and human feedback cycle are novel ideas compared to prior art. The paper shows strong empirical results on Big Bench and other datasets.In summary, this paper integrates several ideas from prior work in a novel framework that pushes forward the state of the art in few-shot reasoning for LLMs. The results are compelling and the approach is flexible and extensible.
