# [Continual Learning on Graphs: A Survey](https://arxiv.org/abs/2402.06330)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper "Continual Learning on Graphs: A Survey":

Problem:
- Graph data is ubiquitous and dynamic in the real world. Graph neural networks (GNNs) have shown great capabilities for graph data analysis but suffer from catastrophic forgetting when trained on non-stationary graph data distributions. 
- Continual learning aims to enable models to learn continuously from dynamic data distributions without forgetting previously learned knowledge. However, current continual learning methods mainly focus on non-graph data and do not consider the connections between data samples.
- There is a need for continual learning methods tailored to graph data, known as continual graph learning (CGL), to overcome catastrophic forgetting in GNNs and achieve continuous performance improvement.

Solutions:
- The paper categorizes existing CGL methods into four groups: replay-based, regularization-based, architecture-based and representation-based.
- Replay-based methods store parts of old graph data to replay when learning new tasks to avoid forgetting. Regularization-based methods add constraints to the optimization process to limit changes to important parameters.
- Architecture-based methods assign separate model components to old and new tasks. Representation-based methods selectively update graph embeddings to accumulate knowledge.  

Contributions:
- Proposes a new taxonomy to categorize CGL methods based on how they can achieve continuous performance improvement.
- Provides a comprehensive survey for each category of CGL methods, analyzes the key challenges, summarizes current solutions, and discusses how they can achieve continuous performance improvement.
- Outlines open issues related to achieving continuous performance improvement in CGL, including convergence, scalability, robustness, privacy preservation, unsupervised learning, explainability and continual learning for large graph models.
- The first survey to focus on continuous performance improvement in continual graph learning. Provides useful insights into future research directions in this emerging field.

In summary, the paper provides a new taxonomy and thorough literature review of continual graph learning methods, with an emphasis on discussing how current methods can overcome catastrophic forgetting and achieve the more ambitious goal of continuous performance improvement on dynamic graph data.
