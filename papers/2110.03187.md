# [On the Optimal Memorization Power of ReLU Neural Networks](https://arxiv.org/abs/2110.03187)

## What is the central research question or hypothesis that this paper addresses?

The central research question addressed by this paper is: What is the minimal number of parameters required for feedforward ReLU neural networks to perfectly memorize a dataset of $N$ labeled points? The key contributions and results of the paper are:- They provide a construction of a ReLU network that can memorize any dataset of $N$ separated samples using $\tilde{O}(\sqrt{N})$ parameters. This matches known VC dimension lower bounds up to logarithmic factors, showing the construction is nearly optimal.- They generalize the construction to bounded depth networks, showing that with depth $L$ the number of parameters required is $\tilde{O}(N/L)$. This is again nearly optimal compared to VC dimension lower bounds. - They analyze the bit complexity of weights needed for memorization, proving that to memorize $N$ points with $N^{1-\epsilon}$ parameters requires weight bit complexity $\tilde{\Theta}(N^{\epsilon})$. This shows large bit complexity is necessary and sufficient to achieve memorization with sub-linear parameters.- Overall, the paper provides nearly tight upper and lower bounds on the number of parameters, depth, and weight bit complexity required for neural network memorization. A key contribution is designing a network that approaches the VC dimension lower bounds through clever weight encoding schemes.In summary, the main hypothesis is that the number of parameters required for memorization can be significantly less than $N$, which they confirm through an optimally efficient construction. The memorization capacity of neural networks is characterized thoroughly across different measures.


## What is the main contribution of this paper?

This paper studies the memorization power of feedforward ReLU neural networks. The main contributions are:- It provides an optimal construction for memorizing N separated data points using a network with O(sqrt(N)) parameters. This matches known VC dimension lower bounds up to logarithmic factors.- It shows that networks with depth L can memorize N points using O(N/L) parameters. This is also optimal compared to VC dimension bounds for bounded depth networks. - It proves that having large weight bit complexity (omega(sqrt(N)) bits) is both necessary and sufficient to get the optimal memorization results.- Overall, the paper shows that deep networks can memorize any dataset using only sqrt(N) parameters, which is significantly better than previous constructions. This resolves the gap between upper and lower bounds on the memorization power of neural networks.In summary, the main contribution is providing optimal constructions and lower bounds that precisely characterize the memorization capacity of neural networks in terms of number of parameters, depth, and weight bit complexity. This resolves open questions on the power of depth for memorization and shows that deep networks can memorize any dataset using substantially fewer parameters than shallow ones.
