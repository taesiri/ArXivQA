# [Guiding LLMs The Right Way: Fast, Non-Invasive Constrained Generation](https://arxiv.org/abs/2403.06988)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Large language models (LLMs) like GPT-3 generate free-form text, which makes it hard to constrain their outputs to adhere to specific syntactic formats. 
- Existing methods for constrained decoding suffer from token misalignment issues between the LLM's subword vocabulary and the external syntactic constraints. This leads to distorted outputs.
- Some methods also incur significant computational overhead during inference.

Proposed Solution:
- The paper proposes a fast, minimally-invasive constrained decoding algorithm called DOMINO.
- It uses a character-level scanner and context-free grammar (CFG) parser. 
- The scanner handles low-level token constraints based on regular expressions.
- The parser enforces high-level CFG production rules.
- To address token misalignment, DOMINO precomputes subterminal trees aligned to the LLM's vocabulary. 
- At inference time, these trees are pruned dynamically based on parser state to identify valid continuations.
- Further optimizations like opportunistic masking and speculative decoding minimize overhead.

Main Contributions:
- Identifies the core challenges of constrained decoding, especially regarding token misalignment.
- Proposes the notion of minimally-invasive constrained decoding to intervene as little as possible.  
- Introduces DOMINO - an efficient decoding algorithm that handles misalignment and accelerates decoding using precomputation and speculation.
- Extensive evaluation shows DOMINO enables high-accuracy and non-invasive constraining with lower overhead than existing approaches.

In summary, the paper addresses an important challenge in guiding large language model outputs, while retaining high throughput. DOMINO paves the way for reliably integrating LLMs into downstream tasks needing structured outputs.
