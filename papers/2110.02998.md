# [Federated Learning via Plurality Vote](https://arxiv.org/abs/2110.02998)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is: How can we design a communication-efficient federated learning algorithm that jointly optimizes communication overhead, learning reliability, and deployment efficiency? The key points are:- Existing federated learning methods like FedAvg face challenges in communication efficiency, Byzantine resilience, and model deployment on edge devices. - The authors propose a new federated learning algorithm called FedVote, which has the following key features:1) Clients train a quantized neural network model locally and send binary/ternary weights to the server. This reduces communication overhead.2) The server aggregates models via weighted voting, which enhances resilience against Byzantine attacks. 3) The quantized model can be efficiently deployed on edge devices. - Theoretically and empirically, the authors demonstrate that FedVote achieves better trade-offs between communication efficiency, Byzantine resilience, and deployment efficiency compared to existing algorithms.- Specifically, FedVote requires much lower communication overhead than gradient quantization methods while achieving higher test accuracy. It also shows stronger Byzantine resilience than algorithms based on coordinate-wise median or similarity scores.In summary, the key hypothesis is that by combining model quantization, voting-based aggregation, and reputation management, the proposed FedVote algorithm can achieve good performance in communication efficiency, Byzantine resilience, and deployment efficiency simultaneously in federated learning. The theoretical analysis and experimental results support this hypothesis.
