# [Found in the Middle: How Language Models Use Long Contexts Better via   Plug-and-Play Positional Encoding](https://arxiv.org/abs/2403.04797)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Recent large language models (LLMs) face challenges in effectively utilizing long input contexts, a phenomenon referred to as "lost in the middle". Specifically, LLMs tend to overlook important information situated amidst lengthy contexts. 

- This arises from two factors: (1) attention bias - LLMs disproportionately focus on initial tokens, (2) positional encoding (e.g. RoPE) introduces a long-term decay effect, diminishing attention to distant tokens.

- As a result, crucial information in the middle of contexts is often neglected, compromising reliability.

Proposed Solution:
- Introduce Multi-scale Positional Encoding (Ms-PoE), a simple plug-and-play approach to enhance LLMs' long context reasoning without additional training. 

- Ms-PoE relieves the long-term decay effect by re-scaling position indices to shorter ranges. 

- It assigns distinct scaling ratios to different attention heads based on positional awareness, fusing multi-scale context from short to long distance.

Main Contributions:
- Demonstrate positional re-scaling can directly improve context utilization of LLMs. Careful choice of ~1.5-2x ratio balances performance.

- Identify and leverage "position-aware" attention heads which consistently focus on relevant tokens across positions.

- Ms-PoE outperforms competitive methods on ZeroSCROLLS benchmarks and long context reasoning tasks, improving performance by up to 13.4 points.

- Simple implementation as a "plug-and-play" module, no extra training or tuning needed. Enhances diverse LLMs from 7B to 16B parameters.

In summary, Ms-PoE offers an effective strategy to address the "lost in the middle" phenomenon in LLMs, enhancing their ability to leverage long input contexts across diverse models and tasks.
