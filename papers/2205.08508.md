# [A CLIP-Hitchhiker's Guide to Long Video Retrieval](https://arxiv.org/abs/2205.08508)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central research question this paper addresses is: How can we effectively adapt image-text models like CLIP to long video retrieval tasks?The key points are:- Recent works have shown promising results by using CLIP for video retrieval, essentially "hitchhiking" on the image-text representation learned by CLIP. However, simply mean pooling CLIP embeddings per frame performs comparably or better than more complex temporal aggregation methods tried. - For long videos, mean pooling is suboptimal as it weights all frames equally rather than focusing on the most relevant parts. - This paper proposes a simple weighted mean approach to aggregate CLIP embeddings based on query-relevance scoring. This outperforms prior complex aggregation methods and sets a new state-of-the-art on long video retrieval benchmarks.- They provide analysis into why this simple weighted mean works so well, including the effect of dataset scale, and improvements to single-frame representations.In summary, the main hypothesis is that a simple weighted mean aggregation of CLIP embeddings based on query-scoring can outperform more complex temporal modeling attempts for long video retrieval. The experiments validate this hypothesis across several datasets.


## What is the main contribution of this paper?

The main contribution of this paper is proposing a simple yet effective baseline method for adapting image-text models like CLIP to long video retrieval tasks. The key points are:- They show that a weighted average of CLIP image embeddings, where the weights are based on query-scoring, outperforms more complex temporal aggregation methods like transformers. This simple method achieves state-of-the-art on several long video retrieval benchmarks.- They demonstrate the effectiveness of constraining the temporal aggregation to only linearly weight the original image embeddings, rather than learning entirely new representations.- They provide analysis and insights into why this simple weighted average method works well, including: not enough training data for complex temporal modeling, weighted average embeddings capture distinct information, and scoring helps learn better image representations. - Overall, they provide a strong simple baseline for long video retrieval using CLIP, outperforming prior works, and suggest that complex temporal modeling may not be better than basic weighted averaging given current data constraints. The weighted average method also acts as an improved baseline for future methods to compare against.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper proposes using simple query-scoring to weight the relevance of individual frames when aggregating CLIP image embeddings for long video retrieval and classification, finding this outperforms more complex temporal modeling approaches and sets a new state-of-the-art on several benchmarks.
