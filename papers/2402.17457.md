# [Why do Learning Rates Transfer? Reconciling Optimization and Scaling   Limits for Deep Learning](https://arxiv.org/abs/2402.17457)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem Statement:
- Recently, scaling up neural network width and depth while preserving optimal hyperparameters like learning rate reduces the cost of hyperparameter tuning. However, from an optimization theory perspective, it is puzzling why the loss landscape remains consistent and enables learning rate transfer as the model size changes drastically. 

- The paper investigates this phenomenon by comparing the sharpness (largest Hessian eigenvalue) dynamics under different scaling limits like neural tangent kernel (NTK) and μP. The sharpness controls stability and optimal learning rate in optimization theory.

Key Contributions:
- Shows empirically that under μP and its depth extension, the sharpness remains remarkably independent of width and depth during a sizable fraction of training. This explains learning rate transfer under μP.

- In contrast, under NTK, the progressive lack of feature learning causes the sharpness to diminish with width, preventing transfer.

- Through the relationship between sharpness and NTK eigenvalues, argues that persistent feature learning under μP enables progressive sharpening to reach a width/depth-independent value, while lack of feature learning in NTK prevents this.

- Validates the width/depth-independent sharpness and resulting hyperparameter transfer extensively on convolutional networks, Vision Transformers, GPT-2 etc. on image and text datasets.

- Provides a theoretical case study on a 2-layer linear network that formalizes the connection between feature learning and width-independent optimization under μP.

In summary, the paper clearly explains the relationship between scaling limits, feature learning, sharpness evolution and hyperparameter transfer in neural networks through empirical evidence and theory.
