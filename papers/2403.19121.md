# [Code Comparison Tuning for Code Large Language Models](https://arxiv.org/abs/2403.19121)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Code large language models (Code LLMs) struggle to handle subtle code errors when fixing bugs based on human instructions. They often regenerate erroneous code or introduce new bugs.

- Existing solutions like specialized code fixing datasets or integrating code interpreters have limitations in coverage or logic error detection. 

Proposed Solution:
- The paper proposes Code Comparison Tuning (CCT), which integrates code comparison into the instruction tuning process to make models more sensitive to small code changes. 

- CCT operates at both token and sequence levels. At the token level, it introduces a preference loss to compare an original code snippet and its erroneous version to teach the model to discern token-level differences. 

- At the sequence level, it transforms comparative code snippet pairs into new instructional examples to enhance the model's bug fixing capabilities.

Main Contributions:
- Proposes CCT, a simple and effective tuning method that heightens Code LLMs' sensitivity to subtle code differences via comparison mechanisms at both token and sequence levels.

- Achieves over 4 points improvement in pass@1 scores on the HumanEvalFix benchmark compared to standard instruction tuning across diverse Code LLMs.

- Demonstrates CCT's effectiveness through extensive analysis - it maintains strong performance even with only 20% of the instructing dataset, showing its data efficiency.

In summary, CCT significantly boosts Code LLMs' bug fixing abilities by integrating code comparison into instruction tuning to make models discern even minute code differences. Experiments validate its effectiveness and efficiency.
