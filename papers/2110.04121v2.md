# [On the Limitations of Multimodal VAEs](https://arxiv.org/abs/2110.04121v2)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading, the central research question this paper seeks to address is: 

Why do multimodal variational autoencoders (VAEs) consistently underperform unimodal VAEs in terms of generative quality, despite the advantage of weak supervision from multiple modalities?

The key hypothesis proposed is that there is a fundamental limitation with the family of "mixture-based" multimodal VAEs that leads to an undesirable upper bound on the approximation of the joint distribution. Specifically, the sub-sampling of modalities during training enforces this limitation.

In more detail:

- The paper focuses on a family of "mixture-based" multimodal VAEs that includes models like the MMVAE, MoPoE-VAE, and a version of MVAE without ELBO subsampling. 

- These models subsample modalities during training as a way to learn inference networks for different subsets efficiently. 

- However, the paper theorizes that this subsampling fundamentally limits the tightness of the approximation of the joint distribution, due to the inability to completely recover modality-specific information that is unobserved for some subsets.

- They formalize this limitation mathematically through an information-theoretic perspective, showing the subsampling leads to an irreducible error term in the evidence lower bound that depends on the amount of modality-specific variation.

- Empirically, they demonstrate this generative quality gap on synthetic and real multimodal datasets, providing support for their hypothesis about the limitations of modality subsampling.

In summary, the central question is why multimodal VAEs underperform, with the core hypothesis being that modality subsampling fundamentally limits these "mixture-based" models - formally and empirically.


## What is the main contribution of this paper?

 Based on my reading, the main contributions of this paper are:

1. It identifies and formalizes a fundamental limitation of a large family of mixture-based multimodal VAEs, including the MMVAE, MoPoE-VAE, and a special case of the MVAE. Specifically, it proves that modality sub-sampling enforces an undesirable upper bound on the multimodal ELBO that depends on the amount of modality-specific variation in the data. This prevents a tight approximation of the joint distribution.

2. It provides extensive empirical validation of this theoretical limitation on both synthetic and real multimodal datasets. The experiments demonstrate a clear generative quality gap between unimodal VAEs and mixture-based multimodal VAEs that increases with the number of modalities. 

3. It reveals issues with generative coherence of existing multimodal VAEs when applied to more complex datasets than previously considered. The models fail to generate coherent and realistic samples when shared information cannot be reliably predicted across modalities.

4. It discusses the implications of these limitations, especially for real-world applications where there is significant modality-specific variation. The results indicate none of the existing VAE-based approaches fulfill all desired criteria of an effective multimodal generative model.

5. It suggests potential solutions like using modality-specific latent variables or replacing cross-modal reconstruction objectives. But more work is needed to develop multimodal VAEs without the identified limitations.

In summary, this paper makes an important theoretical and empirical contribution towards understanding the limitations of existing multimodal VAEs, especially in terms of modeling modality-specific variation and generating coherent multimodal samples.
