# [Rationalization: A Neural Machine Translation Approach to Generating   Natural Language Explanations](https://arxiv.org/abs/1702.07826)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the main research questions/hypotheses appear to be:1) Can encoder-decoder neural networks accurately generate rationalizations that fit the current situational context in a virtual learning environment? The paper proposes using neural machine translation to generate natural language rationalizations that explain the behavior of an autonomous agent. The first hypothesis tests whether this technique can produce rationalizations that accurately reflect the agent's state and actions in a game environment.2) Will humans find rationalizations more satisfying than other forms of explainability for sequential decision making problems?The second hypothesis tests if humans prefer the natural language rationalizations over other types of explanations like just stating the action or showing numeric values. The idea is that rationalizations will be more intuitive and relatable.So in summary, the two main hypotheses aim to validate both the accuracy of generating rationalizations with neural translation, and also the human preference for rationalizations compared to other explanation methods. The Frogger game environment is used to test these hypotheses.


## What is the main contribution of this paper?

The main contribution of this paper is introducing the concept of AI rationalization for explainable AI. The paper proposes using neural machine translation to generate natural language rationalizations that explain the behavior of an autonomous agent. The key ideas are:- AI rationalization generates explanations for agent behavior as if a human performed the behavior, rather than trying to faithfully represent the actual decision process. This aims to make the explanations more intuitive and relatable. - The authors treat rationalization generation as a translation task, using crowdsourcing and grammar generation to create a corpus mapping from internal state/action representations to natural language rationalizations. - An encoder-decoder neural network is trained on this corpus to translate from internal states/actions to natural language rationalizations.- Experiments in the Frogger game environment demonstrate the technique can accurately generate rationalizations fitting the context. - A human study shows rationalizations are more satisfying explanations than just declaring actions or outputting utility values.So in summary, the main contribution is proposing AI rationalization as a new approach to explainable AI, and demonstrating a neural machine translation technique to generate rationalizations. The technique is shown to produce accurate and human-satisfying explanations on a sample task.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points in the paper:The paper introduces the concept of AI rationalization, which involves generating natural language explanations for AI behavior as if a human had performed the actions; it proposes using neural machine translation to generate rationalizations from internal state representations, demonstrates this approach in Frogger, and shows through user studies that rationalizations are more satisfying explanations than alternatives like declaring actions or outputting numeric values.


## How does this paper compare to other research in the same field?

This paper introduces a novel approach to generating explanations for AI systems called "AI rationalization." The key ideas are:- AI rationalization involves generating explanations for AI behavior "as if a human had performed the behavior." This differs from other explainable AI methods that aim to accurately describe the AI's internal decision process. - Rationalizations do not need to perfectly reflect the AI's actual reasoning. The goal is to produce explanations that are natural and intuitive for humans, even non-experts.- The authors propose using neural machine translation to "translate" from the AI's internal representations to natural language rationalizations. This allows generating rationalizations without needing to design explainability into the original AI system.- The authors evaluate their approach in the Frogger game environment. They train an agent to play Frogger using reinforcement learning, without considering explainability. Then they apply their rationalization method on top to generate explanations.- Experiments show the rationalizations are accurate and people find them more satisfying than other explanation types like just declaring the action or showing numeric Q-values.The idea of rationalization and using machine translation for explanation is novel. Prior XAI work has focused more on accurately explaining the internal logic. The authors argue rationalization may be better for non-expert users. This paper demonstrates the feasibility of the approach, setting the stage for future work on evaluating effects on trust, rapport, etc. Overall it introduces a creative new direction to explore for explainable AI.


## What future research directions do the authors suggest?

Based on my reading of the paper, the main future research directions suggested by the authors are:1. Conducting experiments to evaluate how different types of rationalizations influence human perceptions of AI agents, such as confidence, perceived intelligence, and tolerance to failure. The authors suggest adapting their experimental methodology to inject increasingly more error into the rationalizations to understand how this affects human preferences.2. Exploring how inaccurate rationalizations can become before significantly impacting feelings of confidence and rapport with the agent. 3. Investigating how thinking aloud during execution versus providing rationalizations after unexpected events impacts trust and rapport. The authors suggest devising interventions so failures happen or reward functions deviate from operator expectations.4. Applying the rationalization technique to more complex environments and tasks beyond Frogger, such as using deep reinforcement learning and convolutional neural networks for state representations.5. Studying the effects of individual differences in operators, such as their familiarity with AI, and how this influences their perceptions of different types of explanations.In summary, the main suggested future work is conducting additional human subjects experiments to further evaluate the effects of rationalization on perceptions of trust, rapport, confidence, and tolerance of failure, including in more complex environments beyond Frogger.


## Summarize the paper in one paragraph.

Here is a one paragraph summary of the key points from the paper:The paper introduces the concept of AI rationalization, which is a technique for generating natural language explanations for the behavior of autonomous agents that resemble explanations a human might give. The authors propose using neural machine translation to translate internal state representations into natural language rationalizations. They test this approach in the Frogger game environment, first by creating a corpus of human gameplay with think-aloud rationalizations. They then train an encoder-decoder network to translate game states into rationalizations. Experiments show the network can accurately generate rationalizations fitting the game context compared to baselines. A second study finds people rank rationalizations as more satisfying explanations than alternatives like stating actions or utility values. The paper proposes rationalization as an explainable AI approach that can increase rapport and confidence in autonomous systems.


## Summarize the paper in two paragraphs.

Here is a two paragraph summary of the paper:This paper introduces a new approach to explainable AI called AI rationalization. AI rationalization involves generating explanations for an autonomous system's behavior that resemble how a human would explain similar behavior. The authors propose using neural machine translation to translate internal state representations in an autonomous agent to natural language rationalizations. They test this technique in the Frogger game environment by first collecting a corpus of natural language rationalizations from humans playing the game. They then train an encoder-decoder network to translate game state representations to natural language using this corpus. Experiments show the network can accurately generate rationalizations fitting the game context. Additional experiments have people rank rationalizations and other explanation types, finding rationalizations more satisfying.The key ideas are: 1) AI rationalization involves generating human-like explanations rather than fully accurate explanations, 2) neural machine translation can be used to translate internal states to natural language rationalizations, 3) experiments in Frogger show the translation technique works well, and 4) people find rationalizations more satisfying compared to other explanation types like just declaring the action or showing numeric values. The authors propose future work studying how different types of rationalizations influence perceptions of autonomous systems.
