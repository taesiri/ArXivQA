# [Defending Against Indirect Prompt Injection Attacks With Spotlighting](https://arxiv.org/abs/2403.14720)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points in the paper:

Problem:
- Large language models (LLMs) process input prompts without distinguishing between valid instructions vs invalid ones. This makes them vulnerable to "prompt injection attacks" (PIAs) where adversaries embed malicious instructions into prompts.
- A particularly concerning form is "indirect prompt injection" (XPIA) where the attack text comes from an external untrusted data source, unbeknownst to an innocent user. The model then executes those instructions on behalf of the user.

Proposed Solution - Spotlighting:  
- Introduces techniques to help models distinguish safe vs unsafe blocks of text in the prompt
- Delimiting: Use special tokens to demarcate boundaries of untrusted text blocks 
- Datamarking: Interleave a special token throughout untrusted text to signal its presence
- Encoding: Transform untrusted text with well-known encodings (base64, ROT13 etc.) which models can self-decode

Results:
- Across models and tasks, spotlighting reduces attack success rate from >50% down to <2% 
- Datamarking and encoding have negligible detrimental impact on underlying NLP task performance
- Encoding only works well for highest capacity models like GPT-4. Datamarking works more broadly.

Main Contributions:
- Identified and analyzed the indirect prompt injection problem facing LLM systems
- Introduced spotlighting, a novel prompt engineering approach to help models distinguish token blocks  
- Demonstrated the effectiveness of spotlighting, especially datamarking and encoding, across models and tasks
- Showed that spotlighting defenses can work with minimal detrimental impact on NLP efficacy
