# Subgoal Discovery for Hierarchical Dialogue Policy Learning

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper addresses is how to automatically discover useful subgoals for training dialogue agents on complex tasks. Specifically, the paper proposes a method to:- Automatically segment dialogue sessions into subtasks/subgoals without human labeling. This is done using an unsupervised sequence segmentation model called the Subgoal Discovery Network (SDN).- Use the discovered subgoals to train a hierarchical reinforcement learning (HRL) agent for dialogue policy optimization. The HRL agent has a high-level policy for selecting subgoals and low-level policies for executing actions to complete subgoals. The key hypothesis is that with automatically discovered subgoals, the HRL agent can learn dialogue policies for complex tasks more efficiently than flat RL agents trained from scratch. Subgoals help mitigate sparse rewards and exploration challenges in long conversations for complex tasks.The paper validates this hypothesis through simulated and human evaluations on a composite travel planning task. Results show the HRL agent with discovered subgoals performs competitively to one with human-defined subgoals, and better than a flat RL agent without subgoal discovery. The discovered subgoals are also found to be human interpretable.In summary, the paper focuses on automatic subgoal discovery to enable more effective hierarchical RL for training dialogue agents on complex tasks, without requiring manual subgoal annotations.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contribution is a method to automatically discover useful subgoals for hierarchical reinforcement learning of dialogue policies, called the Subgoal Discovery Network (SDN). Specifically, the key ideas are:- The SDN takes as input successful example dialogues, and identifies "hub" states that tend to occur frequently on successful paths but not unsuccessful ones. These hub states are identified as subgoals.- Given the discovered subgoals, hierarchical RL can be used to learn a policy with two levels: a top-level policy that selects among subgoals, and a lower-level policy that executes actions to achieve the selected subgoal. - They demonstrate this approach on a travel planning dialogue task, showing that an RL agent using automatically discovered subgoals can perform competitively to one using human-defined subgoals, and much better than an agent without subgoals.In summary, the main contribution is developing a method to automatically identify useful subgoals from example dialogues, which enables using hierarchical RL to learn good dialogue policies without requiring manual subgoal specification. This could make it much easier to apply hierarchical RL successfully to complex dialogue tasks.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper proposes a method to automatically discover useful subgoals from example dialogues and use them to learn hierarchical dialogue policies more efficiently using reinforcement learning.
