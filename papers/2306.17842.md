# SPAE: Semantic Pyramid AutoEncoder for Multimodal Generation with Frozen
  LLMs

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be whether frozen large language models (LLMs) can be enabled to perform multimodal understanding and generation tasks involving images and videos, without requiring explicit training on image or video data. The key hypothesis appears to be that by learning a vector quantizer that can convert between raw image/video pixels and the lexical tokens in the LLM's vocabulary space, the visual inputs can be "translated" into a language that the LLM can comprehend. This would then allow leveraging the generative capabilities of frozen LLMs for conditional image/video generation tasks, as well as the reasoning abilities of LLMs for visual understanding tasks, without needing to update the LLM parameters.In summary, the central research question is whether frozen LLMs can be empowered to perform well on multimodal tasks spanning both understanding and generation across visual modalities like images and videos, purely via the text interface, without any training on explicit image-text paired data. The proposed vector quantization method that converts visual data to and from the LLM's lexical space is the key technique explored to enable this capability.
