# SPAE: Semantic Pyramid AutoEncoder for Multimodal Generation with Frozen
  LLMs

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be whether frozen large language models (LLMs) can be enabled to perform multimodal understanding and generation tasks involving images and videos, without requiring explicit training on image or video data. The key hypothesis appears to be that by learning a vector quantizer that can convert between raw image/video pixels and the lexical tokens in the LLM's vocabulary space, the visual inputs can be "translated" into a language that the LLM can comprehend. This would then allow leveraging the generative capabilities of frozen LLMs for conditional image/video generation tasks, as well as the reasoning abilities of LLMs for visual understanding tasks, without needing to update the LLM parameters.In summary, the central research question is whether frozen LLMs can be empowered to perform well on multimodal tasks spanning both understanding and generation across visual modalities like images and videos, purely via the text interface, without any training on explicit image-text paired data. The proposed vector quantization method that converts visual data to and from the LLM's lexical space is the key technique explored to enable this capability.


## What is the main contribution of this paper?

The main contribution of this paper seems to be a method called Semantic Pyramid AutoEncoder (SPAE) which enables frozen large language models (LLMs) to perform multimodal understanding and generation tasks involving images and videos, without requiring the LLMs to be explicitly trained on those modalities. Some key aspects of their method:- SPAE converts between raw image/video pixels and interpretable lexical tokens from the LLM vocabulary. The tokens capture both semantic meaning and fine-grained details needed for reconstruction. This effectively "translates" the visual content into a language the LLM can comprehend.- The tokens are arranged in a pyramid structure, with upper layers capturing high-level semantic concepts and lower layers capturing fine appearance details needed for reconstruction. This allows flexibility in using fewer tokens for understanding tasks vs more tokens for generation.- They introduce techniques like a semantic loss and streaming average quantization to encourage semantically meaningful tokens that can reconstruct the original signal. - For generation tasks, they propose a progressive in-context denoising technique to facilitate sampling long cross-modal sequences.The method is evaluated on a diverse set of visual understanding (e.g. classification, captioning) and generation tasks using frozen PaLM and GPT models in an in-context learning setup, without any parameter updates to the LLMs.In summary, the key contribution is introducing a method that enables frozen LLMs to perform multimodal tasks in vision domains which they were not explicitly trained on, by "translating" images/videos into the LLM's language and leveraging capabilities like in-context learning. This unveils the potential of LLMs for multimodal understanding and generation.
