# [One-Shot Face Video Re-enactment using Hybrid Latent Spaces of StyleGAN2](https://arxiv.org/abs/2302.07848)

## What is the central research question or hypothesis that this paper addresses?

The central research question addressed in this paper is: Can we learn a general model to facilitate face identity, attributes, and motion edits exploiting the latent spaces of StyleGAN2 without reliance on explicit 2D/3D facial structure models while improving the performance of generating realistic, high-quality, and temporally consistent one-shot face videos?The key points are:- The authors aim to develop a model for high-fidelity one-shot face video re-enactment that can control face identity, attributes, and motions purely based on StyleGAN2's latent spaces. - They want to avoid relying on explicit 2D or 3D facial structure priors (e.g. landmarks, parameterizations) which have limitations.- The goal is to improve the realism, quality, and temporal consistency of generated face videos compared to prior work. - This will be achieved by learning to encode identity and facial deformations into StyleGAN2's latent spaces in a way that exploits their properties like editability and disentanglement.So in summary, the core research question is whether they can exploit StyleGAN2's latent spaces to achieve state-of-the-art one-shot face video re-enactment without needing explicit facial structure priors.
