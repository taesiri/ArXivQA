# [Prune Spatio-temporal Tokens by Semantic-aware Temporal Accumulation](https://arxiv.org/abs/2308.04549)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the key points of this paper are:- The goal is to develop an efficient token pruning strategy for video transformers to optimize the speed-accuracy trade-off without requiring model retraining. - The key idea is to prune tokens with high temporal redundancy while retaining semantically meaningful tokens. This is done by proposing a Semantic-aware Temporal Accumulation (STA) score.- The STA score considers two factors: temporal redundancy and semantic importance. It captures temporal redundancy by evaluating if a token is a new occurrence or a seen entity. It captures semantic importance using an activation-based attention map.- Tokens with higher STA scores have higher temporal redundancy and lower semantics, so they are pruned. This pruning is done progressively in multiple stages.- The approach is applied to off-the-shelf ViT and VideoSwin transformers. Results on Kinetics-400 and Something-Something V2 show significant computation reduction (30-50%) with minimal accuracy drop (0.1-0.2%).In summary, the key hypothesis is that pruning tokens in videos based on temporal redundancy and semantic importance can optimize video transformers efficiently without retraining. The STA score is proposed to enable such optimization in a plug-and-play manner.


## What is the main contribution of this paper?

The main contribution of this paper is proposing a new token pruning strategy called Semantic-aware Temporal Accumulation (STA) to accelerate video Transformers. The key ideas are:- Considering two factors for pruning tokens: temporal redundancy and semantic importance. Tokens that are repetitive across frames or have low semantic relevance are pruned.- Modeling temporal redundancy via a Markov chain that accumulates token similarity scores across frames. This captures redundancy along the temporal dimension. - Using an activation-based attention map to measure semantic importance of each token. Important tokens are retained even if temporally redundant.- Applying STA hierarchically at multiple Transformer blocks without extra parameters or retraining. This progressively prunes redundant and insignificant tokens.- Evaluating STA on various ViT and VideoSwin backbones for video recognition. It reduces FLOPs by 30-50% with minimal accuracy drop, outperforming prior arts.In summary, the main contribution is proposing an effective yet simple strategy to accelerate video Transformers by pruning redundant and semantically unimportant tokens in a hierarchical manner. The temporal aggregation and semantic awareness makes it well-suited for video.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper proposes a Semantic-aware Temporal Accumulation (STA) method to prune redundant spatio-temporal tokens from video Transformers, which considers both temporal redundancy and semantic importance to accelerate inference speed with minimal impact on accuracy.
