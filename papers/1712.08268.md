# Beyond saliency: understanding convolutional neural networks from   saliency prediction on layer-wise relevance propagation

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be: How can we better understand and visualize how convolutional neural networks (CNNs) comprehend images for classification tasks? Specifically, the authors propose a new two-step method to generate "Salient Relevance (SR) Maps" that aim to reveal the areas of input images, referred to as "attention areas", that deep CNN models focus on to recognize objects and make predictions. The two main steps are:1) Using layer-wise relevance propagation (LRP) to generate a pixel-wise relevance map for a given input image. 2) Applying context-aware saliency detection on the LRP relevance map to filter out irrelevant regions and reveal the true attention areas. The overall goal is to develop a visualization technique that provides insights into how CNN models perceive and understand images, going beyond just predicting classification labels. The SR maps are intended to highlight the areas the models pay attention to and learn features from when recognizing objects.So in summary, the central hypothesis is that by combining LRP and visual saliency, the proposed two-step method can effectively visualize the attention areas and internal comprehension of CNNs for image classification tasks. The experiments aim to demonstrate and validate this hypothesis.


## What is the main contribution of this paper?

Based on my reading, the main contributions of this paper are:1. Proposing a new two-step algorithm to generate a "Salient Relevance (SR) Map" to understand and interpret how deep convolutional neural networks (CNNs) recognize images. The SR map reveals the visual attention areas that contribute most to the network's classification. 2. The proposed method combines layer-wise relevance propagation (LRP) with context-aware saliency detection. LRP propagates the prediction probability backwards to get a pixel-wise relevance map. Context-aware saliency detection then filters this map to reveal the true attention areas.3. Applying the proposed method to analyze and compare different CNN models (AlexNet, VGG-16, VGG-Face) on the ImageNet dataset. The visualizations show which areas the models focus on for classification, revealing their strengths and weaknesses. 4. Demonstrating that the proposed SR map effectively reveals the models' visual attention, simulating how humans tend to focus on salient areas rather than processing the whole image uniformly. The method provides interpretable visualization of how the models recognize images.5. This is the first work, to the authors' knowledge, that incorporates attention and saliency for understanding and interpreting CNN models. The experimental results validate the effectiveness of the proposed approach.In summary, the main contribution is proposing an interpretable visualization technique to understand CNN image classification models by revealing their internal attention mechanisms using saliency-based relevance mapping.


## How does this paper compare to other research in the same field?

This paper presents a novel method for understanding and visualizing convolutional neural networks (CNNs) using salient relevance maps. Here are some key ways it relates to other research in this field:- It builds on the layer-wise relevance propagation (LRP) method, which propagates a model's predictions backwards to determine the relevance of each input pixel. The paper enhances LRP by incorporating context-aware saliency detection to identify attention areas rather than just individual pixels. - Most prior visualization methods for CNNs focused on feature visualization via activation maximization or network inversion. These often produce abstract or hallucinated images that are hard to interpret. This paper takes a different approach of propagating from model predictions to highlight relevant regions in the original input image.- Attention maps are a popular way to understand CNN focus, but are often derived from gradients or guided backpropagation. This paper uses saliency detection on the LRP relevance map instead to identify attention areas.- The evaluation uses established CNN models like AlexNet and VGG on a standard dataset (ImageNet). Most prior work evaluated novel visualization methods on smaller custom models. The experiments here demonstrate the utility on real-world complex models.- The comparisons to Deep SHAP and between different models on the same inputs provide insights into model differences. Most methods just visualize features of one model. Overall, this paper makes good advances in interpreting CNNs by enhancing LRP with saliency detection and demonstrating how the resulting salient relevance maps provide superior visualization and understanding of model predictions on real-world data and models compared to prior approaches. The method appears quite novel in identifying attention areas to explain model reasoning.
