# Beyond saliency: understanding convolutional neural networks from   saliency prediction on layer-wise relevance propagation

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be: How can we better understand and visualize how convolutional neural networks (CNNs) comprehend images for classification tasks? Specifically, the authors propose a new two-step method to generate "Salient Relevance (SR) Maps" that aim to reveal the areas of input images, referred to as "attention areas", that deep CNN models focus on to recognize objects and make predictions. The two main steps are:1) Using layer-wise relevance propagation (LRP) to generate a pixel-wise relevance map for a given input image. 2) Applying context-aware saliency detection on the LRP relevance map to filter out irrelevant regions and reveal the true attention areas. The overall goal is to develop a visualization technique that provides insights into how CNN models perceive and understand images, going beyond just predicting classification labels. The SR maps are intended to highlight the areas the models pay attention to and learn features from when recognizing objects.So in summary, the central hypothesis is that by combining LRP and visual saliency, the proposed two-step method can effectively visualize the attention areas and internal comprehension of CNNs for image classification tasks. The experiments aim to demonstrate and validate this hypothesis.


## What is the main contribution of this paper?

Based on my reading, the main contributions of this paper are:1. Proposing a new two-step algorithm to generate a "Salient Relevance (SR) Map" to understand and interpret how deep convolutional neural networks (CNNs) recognize images. The SR map reveals the visual attention areas that contribute most to the network's classification. 2. The proposed method combines layer-wise relevance propagation (LRP) with context-aware saliency detection. LRP propagates the prediction probability backwards to get a pixel-wise relevance map. Context-aware saliency detection then filters this map to reveal the true attention areas.3. Applying the proposed method to analyze and compare different CNN models (AlexNet, VGG-16, VGG-Face) on the ImageNet dataset. The visualizations show which areas the models focus on for classification, revealing their strengths and weaknesses. 4. Demonstrating that the proposed SR map effectively reveals the models' visual attention, simulating how humans tend to focus on salient areas rather than processing the whole image uniformly. The method provides interpretable visualization of how the models recognize images.5. This is the first work, to the authors' knowledge, that incorporates attention and saliency for understanding and interpreting CNN models. The experimental results validate the effectiveness of the proposed approach.In summary, the main contribution is proposing an interpretable visualization technique to understand CNN image classification models by revealing their internal attention mechanisms using saliency-based relevance mapping.
