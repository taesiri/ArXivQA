# [ClinicalMamba: A Generative Clinical Language Model on Longitudinal   Clinical Notes](https://arxiv.org/abs/2403.05795)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Clinical notes contain complex information across long sequences that needs to be integrated to understand patient conditions and make diagnoses over time. But most clinical language models were pretrained with limited context (less than 2k tokens), which is insufficient to capture long-range dependencies in medical narratives.  

Proposed Solution: 
- Introduce ClinicalMamba, a specialized version of the selective state space model Mamba, pretrained on a large corpus (MIMIC-III) of longitudinal clinical notes to handle extended context.
- Distribute training of a 2.8 billion parameter ClinicalMamba model in under 60 hours using 4 GPUs. It has a 16k max token length to cover over 98% of MIMIC visits.
- Show prompt-based finetuning of ClinicalMamba outperforms Mamba, GPT-4 and other clinical LMs on medical information extraction tasks requiring long context, like cohort screening and ICD coding.

Main Contributions:
- Public release of ClinicalMamba models (130M and 2.8B parameters) trained on longitudinal MIMIC-III notes.
- ClinicalMamba-2.8B is the first clinical autoregressive LM with 16k max context length, effectively modeling clinical language over extended narratives.  
- Superior performance over prior clinical LMs in few-shot long-range information extraction tasks while achieving high throughput, demonstrating effectiveness of selective state space models.

In summary, this paper presents ClinicalMamba, a specialized long-context language model for clinical NLP based on the selective state space Mamba architecture. It demonstrates strong performance in modeling clinical language and extracting information from lengthy patient histories with limited supervision. The public release of these models can facilitate advanced clinical NLP.
