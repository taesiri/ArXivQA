# [Local Self-Attention over Long Text for Efficient Document Retrieval](https://arxiv.org/abs/2005.04908)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research questions/hypotheses appear to be:RQ1: How does the proposed TKL model compare to the TK model and other state-of-the-art retrieval methods in terms of effectiveness and efficiency? RQ2: Does retrieval quality improve when TKL considers longer portions of documents compared to just the first 200 words?RQ3: Is TKL more likely to retrieve longer documents compared to TK?RQ4: What is the effect of using a learned saturation function in TKL compared to fixed saturation functions?RQ5: How often does TKL attend to different parts of the document versus just focusing on the beginning?The key ideas explored are using local self-attention over document windows rather than full self-attention, a learned saturation function, and a two-stage pooling strategy to identify important regions. The central hypothesis seems to be that these modifications will allow efficiently eliciting relevance from long documents while avoiding bias against longer documents. The experiments aim to evaluate the effectiveness and efficiency of TKL versus baselines, and analyze the model's behavior.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contributions appear to be:- Proposing a local self-attention mechanism for Transformer models that considers a moving window over document terms. Each term only attends to other terms in the same window, reducing compute and memory costs compared to full attention.- A two-stage pooling strategy to identify important regions in documents - first local aggregation within windows using a learned saturation function, then global selection of top distinct regions. - Incorporating these ideas into a Transformer-Kernel pooling model (TKL) that can handle long documents (thousands of tokens) more efficiently and effectively.- Experiments on TREC document ranking datasets showing TKL improves over prior Transformer-Kernel model, retrieves longer documents better, and takes advantage of longer input document lengths.In summary, the key contribution seems to be more efficient Transformer architectures for neural document ranking that can leverage long document text through local attention and multi-stage pooling. This allows the model to avoid biases against long documents faced by prior work.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper proposes a Transformer-Kernel model called TKL for efficient document retrieval that uses local attention within sliding windows over long documents and a learned saturation function with two-stage pooling to identify and aggregate evidence from the most relevant regions.


## How does this paper compare to other research in the same field?

Here are my thoughts on how this paper compares to other research in the field of neural ranking models for document retrieval:- The main contribution of this paper is proposing a local self-attention mechanism and two-staged aggregation for the Transformer-Kernel (TK) pooling architecture to enable effective ranking of long documents. This builds on prior work on TK models such as Hofst√§tter et al. 2020, but makes key innovations to handle long text.- The paper compares against several state-of-the-art neural ranking baselines on the TREC Deep Learning 2019 document ranking dataset. The results show that the proposed TKL model outperforms prior TK models and other neural baselines, and achieves effectiveness comparable to BERT reranking models while being more efficient.- The idea of assessing document relevance based on key relevant passages/segments has been explored before in classical IR works like Salton et al. 1993. This paper continues that line of work using neural models. - The local self-attention idea is related to efficient Transformer techniques in models like Reformer (Kitaev et al. 2020), but this paper specifically targets document ranking. The two-stage aggregation strategy is a novel technique proposed in this paper.- Unlike MacAvaney et al. 2019 and Yan et al. 2020 which rely on passage-level scores, the TKL model is trained end-to-end with document-level supervision only.- The analysis of the model's behavior provides insights into the benefits of modeling longer text and the model's ability to identify relevant regions throughout a document. This helps explain the improvements over baseline TK.In summary, this paper makes nice contributions in adapting Transformer-Kernel models to handle long text for document ranking, with both empirical improvements and useful analysis. The innovations over prior work like local attention and two-stage aggregation seem meaningful for this task.
