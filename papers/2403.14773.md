# [StreamingT2V: Consistent, Dynamic, and Extendable Long Video Generation   from Text](https://arxiv.org/abs/2403.14773)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
Existing text-to-video diffusion models can generate high quality short videos (typically 16 frames) that follow text instructions, but they cannot generate longer, temporally consistent videos. Naively training them on longer videos is infeasible due to computational constraints. Autoregressive approaches that generate videos chunk-by-chunk run into issues like inconsistent scene transitions between chunks, video stagnation, and degradation in quality over time.  

Proposed Solution - StreamingT2V:
This paper proposes StreamingT2V, an autoregressive text-to-long video generator. It has three key components:

1) Conditional Attention Module (CAM): Extracts features from the previous video chunk and conditions the generation of the next chunk on those features using an attentional mechanism. This leads to smooth transitions between chunks.

2) Appearance Preservation Module (APM): Extracts high-level scene and object features from an anchor frame in the first chunk and injects that into all subsequent chunks. This helps maintain identities and prevent degradation over time.  

3) Randomized blending: Splits the generated low-res long video into overlapping chunks, enhances each chunk independently using a video enhancer model, and blends the enhanced chunks smoothly in the overlapping regions using probabilistic mixing.

Together, these components enable StreamingT2V to generate videos spanning 80, 240, 600 or 1200 frames without stagnation while maintaining quality, consistency and alignment with the text prompt.

Main Contributions:
1) Proposes CAM and APM to generate temporally coherent long videos with high motion and preservation of identities/scenes
2) Introduces randomized blending for seamless autoregressive enhancement 
3) State-of-the-art quantitative and qualitative performance on text-to-long video generation

The method is shown to significantly outperform recent image-to-video and text-to-video models like DynamiCrafter-XL, SVD, FreeNoise etc. in metrics measuring consistency, motion dynamics and text alignment.
