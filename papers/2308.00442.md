# [FLatten Transformer: Vision Transformer using Focused Linear Attention](https://arxiv.org/abs/2308.00442)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading, the key research question this paper seeks to address is: 

How can we design an efficient yet expressive linear attention module to replace the quadratic complexity softmax attention in vision transformers?

The central hypothesis appears to be:

By enhancing linear attention along two dimensions - focus ability and feature diversity - it is possible to achieve comparable or better performance than softmax attention while maintaining the benefits of linear computational complexity. 

Specifically, the paper proposes a "Focused Linear Attention" module that:

1) Uses a focused mapping function to help concentrate attention on important regions, restoring the sharp distribution of softmax attention. 

2) Employs an efficient depthwise convolution for rank restoration to maintain feature diversity, overcoming limitations in prior linear attention designs.

The overall goal is to create a high-efficiency, high-expressiveness linear attention module that can serve as a drop-in replacement for softmax attention in a variety of vision transformer architectures. The paper validates this module through extensive experiments on image classification, segmentation, and detection tasks.


## What is the main contribution of this paper?

 The main contribution of this paper is proposing a novel Focused Linear Attention module for vision Transformers. The key ideas are:

1. Analyzing the limitations of previous linear attention methods from two aspects - lack of focus ability and feature diversity collapse. 

2. Proposing solutions to address these limitations:

- A simple yet effective mapping function called Focused Function to enhance the focus ability of linear attention. It helps drive similar query-key pairs closer while pushing dissimilar pairs away.

- Adding a lightweight depthwise convolution to the attention matrix to restore feature diversity and increase the rank of the attention matrix.

3. The proposed Focused Linear Attention module achieves both high efficiency and expressiveness:

- It reduces the quadratic computation complexity of softmax attention to linear.

- With the focused function and depthwise convolution, it has comparable or better expressive capability than softmax attention.

4. Extensive experiments show the module is applicable to various vision Transformer models and achieves improved performance on image classification, semantic segmentation and object detection tasks.

In summary, the key contribution is designing an effective linear attention module that maintains high expressiveness while being efficient, through novel techniques to enhance focus ability and feature diversity. This helps overcome limitations of prior linear attention methods.
