# [Text-only Domain Adaptation using Unified Speech-Text Representation in   Transducer](https://arxiv.org/abs/2306.04076)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question or hypothesis appears to be: How can we enable fast domain adaptation of end-to-end speech recognition models using only text data from the target domain, without needing to synthesize speech or modify the model architecture?The key points are:- End-to-end speech recognition models like RNN-Transducers have difficulty adapting to new domains using only text data, unlike traditional hybrid models. - Synthesizing speech for the target domain text via TTS is resource intensive. Modifying the model architecture (e.g. adding an external LM) increases complexity. - The authors propose a method to learn a unified speech-text representation within the RNN-Transducer, by adding a text encoder branch that can be removed after adaptation.- This allows adapting the model to a new domain using only unlabeled text data, without speech synthesis or architecture changes.So in summary, the central hypothesis is that learning a joint speech-text representation will enable efficient text-only domain adaptation for end-to-end models, overcoming limitations of existing methods. The paper then presents this proposed method and experiments validating it.


## What is the main contribution of this paper?

The main contribution of this paper is proposing a method to learn Unified Speech-Text Representation in Conformer Transducer (USTR-CT) to enable fast domain adaptation using only a text corpus. The key points are:- An extra text encoder is introduced to learn text representations. This allows incorporating unpaired text data during training. The text encoder can be removed during inference so no modifications are needed for deployment.- Both multi-step and single-step adaptations are explored. In multi-step, the model is first trained on paired speech-text data, then adapted on a mix of paired and unpaired text data. In single-step, the model is trained from scratch on a mix of paired and unpaired data.- Experiments show the proposed USTR-CT method reduces WER on the target domain (SPGISpeech) by 44% relative compared to a baseline Conformer Transducer. It also outperforms adaptation methods using TTS and textogram. - The method can be combined with Internal Language Model Estimation (ILME) to further improve performance. The best result is obtained by multi-step USTR-CT + ILME.- Using phoneme representation for text features works better than graphemes or subwords. Repeating the phoneme features 3-5 times simulates speech duration.So in summary, the key contribution is developing a modified Conformer Transducer architecture and training process to enable effective text-only domain adaptation, without needing to synthesize speech or change the model for deployment.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

The paper proposes a method called Unified Speech-Text Representation in Conformer Transducer (USTR-CT) to enable fast domain adaptation for end-to-end speech recognition using only text data from the target domain. The key idea is to introduce an extra text encoder during training that is removed during inference, allowing the model to learn a shared representation for speech and text without needing to modify the model for deployment. Experiments show this approach reduces word error rate on a target domain by 44% relative, outperforming prior text-only adaptation methods.


## How does this paper compare to other research in the same field?

Here are a few key points about how this paper compares to other related work in text-only domain adaptation for end-to-end speech recognition:- The main contribution is proposing a unified speech-text representation in the Conformer Transducer model (USTR-CT) to enable text-only adaptation. This is similar in spirit to some other recent works like MAESTRO and JOIST that also align speech and text representations through multi-modal training.- Compared to MAESTRO, this paper focuses more specifically on domain adaptation rather than pre-training. The text encoder branch can be removed during inference so no modifications are needed for deployment, unlike MAESTRO which requires a duration model.- Compared to JOIST, this paper adapts to a new target domain rather than evaluating on rare/unseen words in a multi-domain setting. The adaptation approach is also more direct by fine-tuning with text-only data rather than joint training on both paired and unpaired data.- The text features explored include graphemes, phonemes, and subwords. Phonemes work best, which is reasonable as they are more aligned to speech. Other papers like textogram only evaluate graphemes.- Both single-step and multi-step adaptation strategies are explored. Single-step tuning of the full model works better, while other approaches like textogram only update the output layers.- Combining with ILME gives further gains, showing the approach is complementary to other adaptation methods involving external LMs.Overall, the proposed USTR-CT approach is shown to be more effective for direct text-only domain adaptation compared to other recent methods like textogram, TTS adaptation, and ILME alone. The alignment of speech and text representations is the key novelty.
