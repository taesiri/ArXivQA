# [HiPPO: Recurrent Memory with Optimal Polynomial Projections](https://arxiv.org/abs/2008.07669)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Modeling and learning from sequential data (e.g. text, audio, video) requires effectively capturing long-term dependencies, which relies on memory representations. However, existing recurrent neural network (RNN) models struggle with retaining long-term memory due to issues like vanishing gradients.

- Prior RNN models incorporate heuristics like gating mechanisms (LSTM, GRU) or use orthogonal polynomial bases (LMU), but lack a unified framework to understand different memory mechanisms. They also require hyperparameters that depend on the timescale of the data.

- The goal is to develop a memory representation that can (1) unify and explain prior approaches, (2) remove dependence on timescale hyperparameters, (3) come with theoretical guarantees on approximation capabilities.

Proposed Solution - HiPPO Framework:
- Propose representing memory as an online function approximation problem - summarize the history of an input function f(t) by projecting it onto a polynomial subspace that minimizes approximation error according to a measure μ over time. 

- Derive a general framework (HiPPO) to obtain closed-form ordinary differential equation (ODE) updates for the projection coefficients onto orthogonal polynomial bases, for different choices of μ. Allows online updating.

- Instantiate HiPPO with different measures:
    - Recover gating RNNs and LMU updates as special cases
    - Derive novel update (HiPPO-LegS) using a timescale-invariant scaled measure, with desirable theoretical properties

- Incorporate recurrences into RNN architecture by projecting hidden state h(t) onto polynomial basis to summarize history of h.

Contributions:
- Unified view of memory mechanisms in RNNs through online function approximation
- Principled framework (HiPPO) to derive memory updates for different bases/measures
- Novel memory mechanism (HiPPO-LegS) that is invariant to timescale, efficient, has bounded gradients, adapts to any history length
- State-of-the-art results on long-term dependency benchmarks like permuted MNIST
- Demonstrates robustness to distribution shift in timescales, generalizes well unlike RNNs/ODEs
