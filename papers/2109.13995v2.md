# [IGLU: Efficient GCN Training via Lazy Updates](https://arxiv.org/abs/2109.13995v2)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question seems to be:

How can we efficiently train multi-layer Graph Convolutional Networks (GCNs) at scale?

The key points are:

- Training GCNs using standard SGD techniques scales poorly, as each gradient update propagates to a large neighborhood of nodes due to the message passing nature of GCNs.

- Recent attempts try to address this by sampling or clustering, but introduce bias and variance.

- This paper proposes a new method called IGLU that uses "lazy updates" to cached intermediate computations to reduce the computational cost of gradient descent. 

- IGLU avoids the issues of sampling methods and provides convergence guarantees to a first-order stationary point under standard assumptions.

So in summary, the central hypothesis is that a lazy update strategy based on caching intermediate computations can enable efficient and provably convergent training of multi-layer GCNs on large graphs. The paper seems to experimentally validate this hypothesis on several graph datasets.
