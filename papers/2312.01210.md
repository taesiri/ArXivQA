# [When accurate prediction models yield harmful self-fulfilling prophecies](https://arxiv.org/abs/2312.01210)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Outcome prediction models (OPMs) are commonly used in medicine to guide treatment decisions based on a patient's predicted outcome. For example, patients with a high predicted risk of heart attack may receive more aggressive treatment.
- Implementing an OPM changes the distribution of outcomes since it directly influences treatment decisions. Therefore, an OPM's accuracy on historical validation data does not necessarily mean it will improve outcomes when deployed.  
- The paper shows that some accurate OPMs yield "harmful self-fulfilling prophecies" - they harm a subgroup of patients when deployed, yet they retain good discrimination so this harm goes undetected.

Proposed Solution:
- The paper provides a formal characterization of the set of OPMs that are harmful self-fulfilling prophecies. 
- They examine the simple setting with binary treatment, outcome and a single feature. Even in this simple case, a non-trivial subset of OPMs cause harm while appearing accurate post-deployment.
- The harmful OPMs have good "calibration" before deployment but perfect pre- and post-deployment calibration implies the OPM did not change the data distribution, so it is useless for decision-making.

Main Contributions:
- First formal characterization of settings where implementing an accurate OPM for treatment decisions causes harm to patients.
- Theoretical results that good discrimination or calibration after deployment does NOT imply an OPM improved outcomes. 
- Show that requiring an OPM to have good calibration before AND after deployment renders it useless for decision making.
- Established need to revise practices around developing, validating and monitoring OPMs used for treatment decisions. For example, monitoring discrimination after deployment does not determine if a model is causing harm.

Let me know if you would like me to expand or clarify any part of this summary further. Please also feel free to ask any other questions!
