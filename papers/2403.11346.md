# [CantonMT: Cantonese to English NMT Platform with Fine-Tuned Models Using   Synthetic Back-Translation Data](https://arxiv.org/abs/2403.11346)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem: 
- Neural machine translation (NMT) for low-resource languages like Cantonese is challenging. There has been little prior work on Cantonese-English NMT.

Methodology:
- Collected over 200k monolingual Cantonese sentences from online forums. Cleaned and preprocessed the data.
- Selected 3 base NMT models - smaller OpusMT and larger mBART & NLLB models pretrained on 200+ languages including Cantonese.
- Fine-tuned models on 38k parallel Cantonese-English sentences. Used these to generate 200k synthetic parallel sentences via backtranslation.
- Further fine-tuned models on mix of real parallel data and synthetic parallel data. Tested varying ratios. Also tested model switching by using one model to generate synthetic data and different model for fine-tuning.
- Evaluated models automatically using BLEU, BERTScore, COMET and other metrics. Compared scores across different models and training regimes.

Key Results:
- All fine-tuned models outperformed baseline (non-fine-tuned) by a large margin (~50% better BLEU)
- Fine-tuning with 1:1 ratio of real:synthetic data works best for the models
- Model switching helps - using mBART to generate synthetic data for NLLB fine-tuning gave best scores
- Best fine-tuned models achieve similar/better scores than existing commercial systems Baidu, Bing, GPT4

Contributions:
- First rigorous investigation of neural Cantonese-English MT using backtranslation and fine-tuning
- Creation of usable parallel data and models which outperform commercial systems
- Open-source web platform CantonMT for accessing the models and translations

The paper makes good progress towards addressing the challenging problem of low-resource Cantonese-English NMT via data augmentation and model fine-tuning techniques. The publicly available platform is also a valuable asset for future research.


## Summarize the paper in one sentence.

 Here is a one-sentence summary of the paper:

The paper presents CantonMT, a Cantonese to English neural machine translation platform with fine-tuned models using synthetic back-translation data and a user-friendly interface to facilitate research.


## What is the main contribution of this paper?

 Based on my reading of the paper, the main contributions are:

1) The authors investigate a standard data augmentation methodology (back-translation) for neural machine translation (NMT) on a new low-resource language pair - Cantonese-to-English.

2) They select and fine-tune several NMT models of different sizes (OpusMT, NLLB, mBART) using limited real parallel data and synthetic parallel data generated via back-translation. They compare the models' performance using various automatic evaluation metrics.

3) They develop an open-source toolkit and user-friendly web interface called CantonMT to allow easy access to the fine-tuned Cantonese-English NMT models. Researchers can integrate new models into this platform to facilitate research on this language pair.

4) Through experiments, they show that fine-tuning on synthetic parallel data generated by back-translation improves model performance over just using the limited real parallel data. The best models achieve scores comparable to commercial systems like Baidu and Bing translators.

In summary, the main contributions are introducing and evaluating back-translation for low-resource Cantonese-English NMT, developing an open platform to access the models, and showing improved translation quality over just using the limited real parallel data.


## What are the keywords or key terms associated with this paper?

 Based on my review of the paper content, some of the key terms and keywords associated with this paper include:

- Cantonese-to-English neural machine translation (NMT)
- Low-resource language translation
- Back-translation
- Synthetic parallel corpus generation
- Model fine-tuning 
- OpusMT
- NLLB
- mBART
- Automatic evaluation metrics (SacreBLEU, hLEPOR, BERTscore, COMET)
- CantonMT platform
- User interface
- Data privacy
- Open source toolkit

The paper investigates using back-translation to generate a synthetic parallel corpus to augment limited real parallel data for fine-tuning neural machine translation models on the low-resource Cantonese-English language pair. It compares different models like OpusMT, NLLB and mBART, evaluates them with both lexical and embedding-based metrics, and releases an open source CantonMT platform and interface to facilitate research.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in the paper:

1. The paper mentions scraping data from the LIHKG forum and filtering out sentences with less than 10 Chinese characters. What considerations went into choosing this length cutoff? Could a different cutoff have been more optimal? 

2. The paper compares multiple pre-trained models like OpusMT, NLLB, and mBART for fine-tuning. What are the key strengths and weaknesses of each model in the context of low-resource Cantonese-English translation?

3. The paper finds that model switching (using different models for backtranslation versus fine-tuning) improves performance over using the same model for both. What factors drive this improvement? How could this effect be further analyzed?  

4. For the model training, what considerations went into choosing the 38K/3K/3K (train/dev/test) data split? Could a different split ratio have been better?

5. The learning curves in Figure 2 show some volatility during training. What could explain this behavior, and how could training be adjusted to smooth the volatility?  

6. The paper shows that more synthetic parallel data can hurt performance. What properties of the synthetic data likely cause this effect? How could the backtranslation process be refined?

7. The paper finds that more real parallel data improves model quality more than synthetic data. Why does real data have this advantage? How much real data would be needed to maximize translation quality?

8. The paper uses multiple automatic evaluation metrics. What are the relative strengths and weaknesses of each metric used? How could the human evaluation mentioned be designed?

9. For the CantonMT platform, how was the model manager designed to balance performance and memory usage? What other optimizations could improve the implementation?

10. The paper focuses exclusively on Cantonese-English. How could the methods be extended to other Chinese dialects and low-resource language pairs? What modifications would be required?
