# [CantonMT: Cantonese to English NMT Platform with Fine-Tuned Models Using   Synthetic Back-Translation Data](https://arxiv.org/abs/2403.11346)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem: 
- Neural machine translation (NMT) for low-resource languages like Cantonese is challenging. There has been little prior work on Cantonese-English NMT.

Methodology:
- Collected over 200k monolingual Cantonese sentences from online forums. Cleaned and preprocessed the data.
- Selected 3 base NMT models - smaller OpusMT and larger mBART & NLLB models pretrained on 200+ languages including Cantonese.
- Fine-tuned models on 38k parallel Cantonese-English sentences. Used these to generate 200k synthetic parallel sentences via backtranslation.
- Further fine-tuned models on mix of real parallel data and synthetic parallel data. Tested varying ratios. Also tested model switching by using one model to generate synthetic data and different model for fine-tuning.
- Evaluated models automatically using BLEU, BERTScore, COMET and other metrics. Compared scores across different models and training regimes.

Key Results:
- All fine-tuned models outperformed baseline (non-fine-tuned) by a large margin (~50% better BLEU)
- Fine-tuning with 1:1 ratio of real:synthetic data works best for the models
- Model switching helps - using mBART to generate synthetic data for NLLB fine-tuning gave best scores
- Best fine-tuned models achieve similar/better scores than existing commercial systems Baidu, Bing, GPT4

Contributions:
- First rigorous investigation of neural Cantonese-English MT using backtranslation and fine-tuning
- Creation of usable parallel data and models which outperform commercial systems
- Open-source web platform CantonMT for accessing the models and translations

The paper makes good progress towards addressing the challenging problem of low-resource Cantonese-English NMT via data augmentation and model fine-tuning techniques. The publicly available platform is also a valuable asset for future research.
