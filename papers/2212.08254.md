# [RepQ-ViT: Scale Reparameterization for Post-Training Quantization of   Vision Transformers](https://arxiv.org/abs/2212.08254)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question is: How can we achieve both accurate quantization and efficient inference for vision transformers (ViTs) via post-training quantization? 

The key hypothesis is that by decoupling the quantization and inference processes, complex quantizers can be used for quantization to maintain accuracy, while simple quantizers can be used for inference to enable efficiency. This is achieved via scale reparameterization to bridge the gap between the two.

Specifically, the paper focuses on quantizing two components in ViTs that have extreme distributions:

1) Post-LayerNorm activations, which have severe inter-channel variation. The hypothesis is that channel-wise quantization can capture the variation but layer-wise quantization is needed for efficiency. Thus, they propose reparameterizing channel-wise scales to layer-wise.

2) Post-Softmax activations, which have a power-law distribution. The hypothesis is log√2 quantization fits the distribution better but log2 enables bit-shifting. Thus, they propose reparameterizing the log√2 scales to log2.

In summary, the central question is how to obtain both accuracy and efficiency for quantizing ViTs, and the core hypothesis is that quantization-inference decoupling via scale reparameterization can achieve this.


## What is the main contribution of this paper?

 Based on my reading of the paper, the main contributions appear to be:

- Proposing a novel post-training quantization (PTQ) framework called RepQ-ViT for quantizing vision transformers (ViTs). 

- Introducing a quantization-inference decoupling paradigm, where complex quantizers are used for quantization to maintain accuracy, and simple quantizers are used for efficient inference. These are bridged via scale reparameterization.

- Addressing two components with extreme distributions that challenge direct quantization in ViTs:

1) For post-LayerNorm activations with severe inter-channel variation, initially using channel-wise quantization, then reparameterizing to layer-wise quantization.

2) For post-Softmax activations with power-law features, initially using log√2 quantization, then reparameterizing to log2 quantization.

- Showing through experiments that RepQ-ViT outperforms previous PTQ methods for ViTs, especially for low-bit quantization (e.g. 4-bit), achieving much higher accuracy without hyperparameters or reconstruction.

In summary, the key contribution is proposing the RepQ-ViT framework to enable efficient yet accurate quantization of ViTs through quantization-inference decoupling and scale reparameterization to handle components with extreme distributions.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the paper:

The paper proposes a new post-training quantization framework called RepQ-ViT for quantizing vision transformers, which uses complex quantizers during quantization but reparameterizes them to simple quantizers for efficient inference.


## How does this paper compare to other research in the same field?

 Here are a few key points in comparing this paper to other research on post-training quantization for vision transformers:

- The paper proposes a novel framework called RepQ-ViT that decouples the quantization and inference processes. This is different from most prior works that tightly couple quantization design with hardware efficiency considerations. By allowing more complex quantizers in the initial quantization, RepQ-ViT can better preserve the original parameter distributions.

- The paper focuses on quantizing two components with extreme distributions in ViTs - post-LayerNorm activations and post-Softmax activations. For each, RepQ-ViT employs specialized quantization methods (channel-wise and log√2 quantization) before reparameterizing to hardware-friendly versions. This differs from other works that directly quantize all activations in a uniform way.

- RepQ-ViT does not require end-to-end retraining or reconstruction procedures like some other methods. It only needs a small calibration set, making it very fast and light-weight. The reparameterizations are also interpretable with theoretical support. 

- Experiments show RepQ-ViT outperforms recent state-of-the-art PTQ techniques like PTQ4ViT and APQ-ViT, especially in very low precision settings like 4-bit. It also achieves consistent gains across diverse vision tasks and ViT model variants.

- Limitations include still non-trivial accuracy drops compared to full precision models, and the techniques are specialized for ViT models rather than general CNNs.

In summary, RepQ-ViT pushes PTQ for ViTs forward through its quantization-inference decoupling paradigm and targeted handling of activations. The results demonstrate the promise of this direction to enable efficient deployment of ViTs on edge devices.
