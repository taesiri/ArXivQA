# [RepQ-ViT: Scale Reparameterization for Post-Training Quantization of   Vision Transformers](https://arxiv.org/abs/2212.08254)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is: How can we achieve both accurate quantization and efficient inference for vision transformers (ViTs) via post-training quantization? The key hypothesis is that by decoupling the quantization and inference processes, complex quantizers can be used for quantization to maintain accuracy, while simple quantizers can be used for inference to enable efficiency. This is achieved via scale reparameterization to bridge the gap between the two.Specifically, the paper focuses on quantizing two components in ViTs that have extreme distributions:1) Post-LayerNorm activations, which have severe inter-channel variation. The hypothesis is that channel-wise quantization can capture the variation but layer-wise quantization is needed for efficiency. Thus, they propose reparameterizing channel-wise scales to layer-wise.2) Post-Softmax activations, which have a power-law distribution. The hypothesis is log√2 quantization fits the distribution better but log2 enables bit-shifting. Thus, they propose reparameterizing the log√2 scales to log2.In summary, the central question is how to obtain both accuracy and efficiency for quantizing ViTs, and the core hypothesis is that quantization-inference decoupling via scale reparameterization can achieve this.
