# [Few-Shot VQA with Frozen LLMs: A Tale of Two Approaches](https://arxiv.org/abs/2403.11317)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Two main approaches have emerged to input images into large language models (LLMs) for visual question answering (VQA): 1) Generate image captions and concatenate with questions or 2) Map image features into the LLM's embedding space and concatenate the mapped embeddings with question embeddings.
- Recent VQA systems use variations of these approaches but there has been no focused comparison between them. 

Proposed Solution:
- Conduct a controlled experiment to directly compare the two approaches for few-shot VQA using the same base systems - Flan-T5 XL LLM and CLIP image encoder.
- The only difference is whether the intermediate output is an image caption or mapped visual embeddings.
- Also analyze the effect of how in-context examples are selected.

Main Contributions:
- Surprisingly, zero-shot caption-based VQA significantly outperforms zero-shot embedding-based VQA (+4.5%) with the same base systems. Indicates textual captions provide a better starting point.  
- Embedding-based benefits more from in-context examples (+9.1% from 0 to 1 shot) demonstrating the approach.
- Overall, embedding-based surpasses caption-based given sufficient (1+ shot) and relevant in-context examples. Performs better for fine visual details.
- Selection of in-context examples critical to success. Similarity-based selection significantly better than random.
- Shows textual captions strong alternative to benchmark against for systems that map visual embeddings to LLMs.

In summary, the paper provides the first focused comparison between two prevalent approaches to VQA under the same conditions. It highlights the importance of in-context learning and that connecting embeddings directly is not necessarily better than captions for smaller LLMs. The insights help guide future VQA system design.
