# [Evaluating Explanatory Capabilities of Machine Learning Models in   Medical Diagnostics: A Human-in-the-Loop Approach](https://arxiv.org/abs/2403.19820)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Machine learning models like neural networks, while accurate, are not interpretable and lack explainability. This is an issue especially in critical domains like healthcare where transparency and trust are important. 

- Existing explainability methods are inconsistent, provide little insight into their correctness, and can be manipulated. So evaluating and validating these methods is critical.

- In domains like healthcare, evaluating against expert knowledge and medical guidelines is important to ensure models adhere to recommended practices. 

Proposed Solution:
- Use decision trees, random forests and XGBoost models on a pancreatic cancer dataset for chemotherapy treatment recommendation.

- Collaborate with medical experts for feature selection and weighting by importance. Also analyze medical guidelines to extract feature importance weights.  

- Compare multiple model-specific and model-agnostic explainability methods like SHAP and LIME to evaluate feature importance consistency. 

- Propose using similarity measures like Weighted Jaccard Index to quantify consistency between different explainability methods and alignment with expert and guideline knowledge.

Key Contributions:
- Demonstration of evaluate explainability using domain knowledge from both medical experts and guidelines.

- Use of similarity measures to quantify consistency of explainability methods and alignment with expert knowledge.

- Analysis showing greater consistency between explainability methods for simpler models like decision trees compared to more complex ones like XGBoost.

- Finding that simpler models focus on fewer key features while complex ones distribute importance across more features, behaving more "human-like".

- Establishing feasibility of using similarity measures to choose models that are accurate, interpretable and aligned with human domain knowledge.
