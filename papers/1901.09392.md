# [On the (In)fidelity and Sensitivity for Explanations](https://arxiv.org/abs/1901.09392)

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper addresses is how to objectively evaluate the fidelity and sensitivity of explanations for complex black-box machine learning models. Specifically, the key hypotheses examined are:1. The infidelity measure proposed can unify and generate novel explanation methods by optimizing over different perturbation distributions.2. It is possible to simultaneously reduce the sensitivity and infidelity of explanations by appropriate smoothing operations like SmoothGrad.3. Explanations that are optimized to have lower infidelity are more aligned with human intuition and model behavior in experiments.4. Reducing sensitivity properly does not hurt fidelity, addressing concerns that smoothing explanations may reduce how faithful they are to the model.5. Models trained to be robust like via adversarial training have explanations with lower sensitivity and infidelity, without having to directly modify the explanation method.So in summary, the paper introduces objective measures of infidelity and sensitivity for explanations, and uses these measures to design and evaluate explanations, addressing concerns about evaluating explanations only in a subjective human-centric manner. The key hypotheses examine how these measures relate for developing better explanations.


## What is the main contribution of this paper?

The main contribution of this paper is developing objective evaluation measures of explanation quality for complex black-box machine learning models. Specifically, the paper proposes two measures:1. Explanation Infidelity: This quantifies how well an explanation captures how the model's prediction changes in response to meaningful perturbations of the input. It is defined as the expected squared error between the dot product of the input perturbation and the explanation, versus the difference in model output. Lower infidelity indicates a more faithful explanation.2. Explanation Sensitivity: This measures how much an explanation changes given small insignificant perturbations to the input. Lower sensitivity is desirable as it indicates the explanation is more robust and stable. The paper shows that many recent explanation methods like Integrated Gradients and KernelSHAP optimize infidelity for certain perturbations. It also derives a smoothed explanation that reduces both sensitivity and infidelity.In summary, the main contribution is formalizing these two quantitative notions of infidelity and sensitivity as ways to objectively evaluate explanation quality. This provides a theoretical foundation to understand, improve, and design explanations. The paper demonstrates the utility of these measures through analysis, new explanations, and experiments.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence TL;DR of the paper:The paper proposes objective measures of infidelity and sensitivity to evaluate the quality of explanations for complex machine learning models, and shows how to improve both measures by appropriately smoothing any given explanation method.


## How does this paper compare to other research in the same field?

Here are some key ways this paper compares to other research on evaluating and improving explanations for machine learning models:- Introduces two new objective metrics for evaluating explanations - infidelity and sensitivity. Many prior works have focused more on subjective or qualitative evaluations. Defining rigorous objective metrics is an important contribution.- Shows that many recent explanation methods optimize infidelity under certain assumptions. Provides a unifying view of existing methods. Also proposes new explanations by optimizing for infidelity under different assumptions.- Analyzes the relationship between sensitivity and infidelity. Shows that lowering sensitivity carefully can improve both sensitivity and infidelity. This addresses concerns about just decreasing sensitivity blindly. - Provides modifications to improve sensitivity and infidelity of any explanation method, via smoothing. Related to work on SmoothGrad but provides theoretical justification.- Considers both local and global explanations and tailors the evaluation metrics appropriately. Comparisons to prior work like Ancona et al. that also make this distinction.- Empirically evaluates on image datasets. Most prior work focused more on smaller datasets/models. Useful to validate on complex models.- Provides both quantitative evaluations and human experiments. Many works focus on only one. Important to have both objective and subjective validation.Overall, it makes significant theoretical and empirical contributions around rigorously defining, analyzing, and optimizing explanation quality. The sensitivity-infidelity relationship and smoothing modifications are particularly novel. The evaluations are thorough across datasets, explanation methods, metrics, and human studies.


## What future research directions do the authors suggest?

Based on my reading of the paper, here are some of the main future research directions suggested by the authors:- Develop explanations that are optimized for both sensitivity and fidelity/infidelity. The authors show there is not necessarily a trade-off between sensitivity and infidelity, and propose some ways to improve both measures. However, they suggest further research into developing explanations that directly optimize for both low sensitivity and high fidelity.- Explore different perturbation distributions for the infidelity measure. The authors show that using different perturbation distributions leads to novel explanation methods like the noisy baseline and square removal explanations. They suggest exploring other potential perturbation distributions that could lead to new explanation techniques.- Apply the sensitivity and infidelity measures to broader classes of explanations beyond attribution methods. The authors primarily focus on applying the measures to attribution-based feature importance explanations. They suggest extending the analysis to other types of explanations like example-based methods.- Extend the robust infidelity measure beyond square loss to more general loss functions. The infidelity measure uses an expected square loss between the explanation and model output differences. The authors suggest generalizing this to other loss functions.- Develop efficient methods to optimize explanations for infidelity. The paper provides the form for explanations that optimize infidelity, but notes computational challenges. More research into efficiently optimizing explanations for infidelity is suggested.- Apply the sensitivity/infidelity measures to interpretations beyond machine learning models. The authors suggest the measures could more broadly apply for analyzing the sensitivity/fidelity of any functional of a function at a point, beyond just ML model explanations.- Explore modifications to improve explanation sensitivity and infidelity beyond smoothing. The paper analyzes smoothing explanations to improve sensitivity/infidelity, but notes other techniques like adversarial training could also help. More research into other potential explanation modifications is proposed.In summary, the main suggestions are to further develop explanations that directly optimize for both sensitivity and infidelity, explore how different choices in defining these measures impact explanations, and extend the sensitivity/infidelity analysis beyond the specific methods considered in the paper.


## Summarize the paper in one paragraph.

Here is a one paragraph summary of the paper:The paper proposes two objective evaluation measures, infidelity and sensitivity, for explaining black-box machine learning models using feature attribution methods. It defines infidelity as the expected squared error between the feature attribution explanation multiplied by the input perturbation and the difference in model output from that perturbation. It then shows that many recent explanation methods like Integrated Gradients and SHAP optimize this infidelity measure for different types of perturbations. The paper defines sensitivity as how much the explanation changes with small input perturbations, and shows that smoothing explanations (like SmoothGrad) reduces both sensitivity and infidelity. Overall, the paper provides a unifying framework to understand, evaluate, and improve the fidelity and robustness of feature attribution explanations through the lens of infidelity and sensitivity.


## Summarize the paper in two paragraphs.

Here is a two paragraph summary of the paper:The paper proposes objective evaluation measures for saliency explanations of complex black-box machine learning models. The authors introduce a notion of explanation infidelity, which measures how well the explanation captures changes in the model output under different input perturbations. They show the optimal explanation minimizing infidelity is a novel combination of Integrated Gradients and SmoothGrad. They also propose measuring explanation sensitivity, which quantifies how much the explanation changes under small input variations. While minimizing sensitivity leads to trivial constant explanations, the authors show reducing sensitivity in a restrained manner can actually improve infidelity. They propose a kernel smoothing technique called SmoothExp that reduces sensitivity while also lowering infidelity. Experiments validate SmoothExp improves sensitivity and infidelity over baseline explanations like gradients on image datasets. The authors also relate reducing sensitivity to adversarial training, which encourages models with smoother gradients. Overall, the paper provides an objective footing to evaluate and improve explanations using the proposed infidelity and sensitivity measures.In summary, the paper introduces the notions of infidelity and sensitivity to objectively evaluate saliency explanations. It shows the optimal explanation for infidelity combines Integrated Gradients and SmoothGrad. It also proposes SmoothExp to improve any explanation by reducing sensitivity while also lowering infidelity. Experiments demonstrate the benefit of optimizing these objective measures.


## Summarize the main method used in the paper in one paragraph.

Here is a one paragraph summary of the main method used in the paper:The paper proposes a way to objectively evaluate and improve the fidelity and sensitivity of explanations for complex machine learning models. The authors introduce an infidelity measure that quantifies how well an explanation captures changes to the model's predictions under meaningful perturbations of the input. They show the optimal explanation that minimizes this infidelity is related to SmoothGrad and integrated gradients. They also propose a robust sensitivity measure that characterizes how much an explanation changes under small input perturbations. A key result is that by smoothing explanations, such as with SmoothGrad, both the fidelity and sensitivity can be improved. This is validated through experiments on image datasets using gradient-based explanations. Overall, the main contribution is an objective framework to evaluate explanation fidelity and sensitivity, ways to optimize these measures, and demonstration that smoothing explanations can enhance both measures simultaneously.
