# [On the (In)fidelity and Sensitivity for Explanations](https://arxiv.org/abs/1901.09392)

## What is the central research question or hypothesis that this paper addresses?

 The central research question this paper addresses is how to objectively evaluate the fidelity and sensitivity of explanations for complex black-box machine learning models. 

Specifically, the key hypotheses examined are:

1. The infidelity measure proposed can unify and generate novel explanation methods by optimizing over different perturbation distributions.

2. It is possible to simultaneously reduce the sensitivity and infidelity of explanations by appropriate smoothing operations like SmoothGrad.

3. Explanations that are optimized to have lower infidelity are more aligned with human intuition and model behavior in experiments.

4. Reducing sensitivity properly does not hurt fidelity, addressing concerns that smoothing explanations may reduce how faithful they are to the model.

5. Models trained to be robust like via adversarial training have explanations with lower sensitivity and infidelity, without having to directly modify the explanation method.

So in summary, the paper introduces objective measures of infidelity and sensitivity for explanations, and uses these measures to design and evaluate explanations, addressing concerns about evaluating explanations only in a subjective human-centric manner. The key hypotheses examine how these measures relate for developing better explanations.


## What is the main contribution of this paper?

 The main contribution of this paper is developing objective evaluation measures of explanation quality for complex black-box machine learning models. Specifically, the paper proposes two measures:

1. Explanation Infidelity: This quantifies how well an explanation captures how the model's prediction changes in response to meaningful perturbations of the input. It is defined as the expected squared error between the dot product of the input perturbation and the explanation, versus the difference in model output. Lower infidelity indicates a more faithful explanation.

2. Explanation Sensitivity: This measures how much an explanation changes given small insignificant perturbations to the input. Lower sensitivity is desirable as it indicates the explanation is more robust and stable. 

The paper shows that many recent explanation methods like Integrated Gradients and KernelSHAP optimize infidelity for certain perturbations. It also derives a smoothed explanation that reduces both sensitivity and infidelity.

In summary, the main contribution is formalizing these two quantitative notions of infidelity and sensitivity as ways to objectively evaluate explanation quality. This provides a theoretical foundation to understand, improve, and design explanations. The paper demonstrates the utility of these measures through analysis, new explanations, and experiments.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence TL;DR of the paper:

The paper proposes objective measures of infidelity and sensitivity to evaluate the quality of explanations for complex machine learning models, and shows how to improve both measures by appropriately smoothing any given explanation method.


## How does this paper compare to other research in the same field?

 Here are some key ways this paper compares to other research on evaluating and improving explanations for machine learning models:

- Introduces two new objective metrics for evaluating explanations - infidelity and sensitivity. Many prior works have focused more on subjective or qualitative evaluations. Defining rigorous objective metrics is an important contribution.

- Shows that many recent explanation methods optimize infidelity under certain assumptions. Provides a unifying view of existing methods. Also proposes new explanations by optimizing for infidelity under different assumptions.

- Analyzes the relationship between sensitivity and infidelity. Shows that lowering sensitivity carefully can improve both sensitivity and infidelity. This addresses concerns about just decreasing sensitivity blindly. 

- Provides modifications to improve sensitivity and infidelity of any explanation method, via smoothing. Related to work on SmoothGrad but provides theoretical justification.

- Considers both local and global explanations and tailors the evaluation metrics appropriately. Comparisons to prior work like Ancona et al. that also make this distinction.

- Empirically evaluates on image datasets. Most prior work focused more on smaller datasets/models. Useful to validate on complex models.

- Provides both quantitative evaluations and human experiments. Many works focus on only one. Important to have both objective and subjective validation.

Overall, it makes significant theoretical and empirical contributions around rigorously defining, analyzing, and optimizing explanation quality. The sensitivity-infidelity relationship and smoothing modifications are particularly novel. The evaluations are thorough across datasets, explanation methods, metrics, and human studies.


## What future research directions do the authors suggest?

 Based on my reading of the paper, here are some of the main future research directions suggested by the authors:

- Develop explanations that are optimized for both sensitivity and fidelity/infidelity. The authors show there is not necessarily a trade-off between sensitivity and infidelity, and propose some ways to improve both measures. However, they suggest further research into developing explanations that directly optimize for both low sensitivity and high fidelity.

- Explore different perturbation distributions for the infidelity measure. The authors show that using different perturbation distributions leads to novel explanation methods like the noisy baseline and square removal explanations. They suggest exploring other potential perturbation distributions that could lead to new explanation techniques.

- Apply the sensitivity and infidelity measures to broader classes of explanations beyond attribution methods. The authors primarily focus on applying the measures to attribution-based feature importance explanations. They suggest extending the analysis to other types of explanations like example-based methods.

- Extend the robust infidelity measure beyond square loss to more general loss functions. The infidelity measure uses an expected square loss between the explanation and model output differences. The authors suggest generalizing this to other loss functions.

- Develop efficient methods to optimize explanations for infidelity. The paper provides the form for explanations that optimize infidelity, but notes computational challenges. More research into efficiently optimizing explanations for infidelity is suggested.

- Apply the sensitivity/infidelity measures to interpretations beyond machine learning models. The authors suggest the measures could more broadly apply for analyzing the sensitivity/fidelity of any functional of a function at a point, beyond just ML model explanations.

- Explore modifications to improve explanation sensitivity and infidelity beyond smoothing. The paper analyzes smoothing explanations to improve sensitivity/infidelity, but notes other techniques like adversarial training could also help. More research into other potential explanation modifications is proposed.

In summary, the main suggestions are to further develop explanations that directly optimize for both sensitivity and infidelity, explore how different choices in defining these measures impact explanations, and extend the sensitivity/infidelity analysis beyond the specific methods considered in the paper.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the paper:

The paper proposes two objective evaluation measures, infidelity and sensitivity, for explaining black-box machine learning models using feature attribution methods. It defines infidelity as the expected squared error between the feature attribution explanation multiplied by the input perturbation and the difference in model output from that perturbation. It then shows that many recent explanation methods like Integrated Gradients and SHAP optimize this infidelity measure for different types of perturbations. The paper defines sensitivity as how much the explanation changes with small input perturbations, and shows that smoothing explanations (like SmoothGrad) reduces both sensitivity and infidelity. Overall, the paper provides a unifying framework to understand, evaluate, and improve the fidelity and robustness of feature attribution explanations through the lens of infidelity and sensitivity.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the paper:

The paper proposes objective evaluation measures for saliency explanations of complex black-box machine learning models. The authors introduce a notion of explanation infidelity, which measures how well the explanation captures changes in the model output under different input perturbations. They show the optimal explanation minimizing infidelity is a novel combination of Integrated Gradients and SmoothGrad. They also propose measuring explanation sensitivity, which quantifies how much the explanation changes under small input variations. While minimizing sensitivity leads to trivial constant explanations, the authors show reducing sensitivity in a restrained manner can actually improve infidelity. They propose a kernel smoothing technique called SmoothExp that reduces sensitivity while also lowering infidelity. Experiments validate SmoothExp improves sensitivity and infidelity over baseline explanations like gradients on image datasets. The authors also relate reducing sensitivity to adversarial training, which encourages models with smoother gradients. Overall, the paper provides an objective footing to evaluate and improve explanations using the proposed infidelity and sensitivity measures.

In summary, the paper introduces the notions of infidelity and sensitivity to objectively evaluate saliency explanations. It shows the optimal explanation for infidelity combines Integrated Gradients and SmoothGrad. It also proposes SmoothExp to improve any explanation by reducing sensitivity while also lowering infidelity. Experiments demonstrate the benefit of optimizing these objective measures.


## Summarize the main method used in the paper in one paragraph.

 Here is a one paragraph summary of the main method used in the paper:

The paper proposes a way to objectively evaluate and improve the fidelity and sensitivity of explanations for complex machine learning models. The authors introduce an infidelity measure that quantifies how well an explanation captures changes to the model's predictions under meaningful perturbations of the input. They show the optimal explanation that minimizes this infidelity is related to SmoothGrad and integrated gradients. They also propose a robust sensitivity measure that characterizes how much an explanation changes under small input perturbations. A key result is that by smoothing explanations, such as with SmoothGrad, both the fidelity and sensitivity can be improved. This is validated through experiments on image datasets using gradient-based explanations. Overall, the main contribution is an objective framework to evaluate explanation fidelity and sensitivity, ways to optimize these measures, and demonstration that smoothing explanations can enhance both measures simultaneously.


## What problem or question is the paper addressing?

 Based on my reading, the key points of this paper are:

- It proposes two objective evaluation measures for explanations of machine learning models: infidelity and sensitivity. 

- Infidelity measures how well an explanation captures the change in the model's output for significant input perturbations. Sensitivity measures how much an explanation changes with insignificant perturbations to the input.

- It shows the optimal explanation that minimizes infidelity is related to a combination of SmoothGrad and Integrated Gradients. By varying the perturbation distribution, new explanations are derived by optimizing infidelity.

- It relates sensitivity and infidelity, showing explanations with high sensitivity tend to have high infidelity. It proposes smoothing explanations to reduce sensitivity and infidelity simultaneously.

- Experiments validate the proposed infidelity aligns with human evaluation, and smoothing improves both sensitivity and infidelity across explanations and datasets.

In summary, the paper addresses the problem of objectively evaluating and improving the quality of explanations for complex machine learning models in terms of fidelity to the model and robustness to perturbations. It proposes infidelity and sensitivity as two key objective measures, and develops ways to optimize explanations for these measures.


## What are the keywords or key terms associated with this paper?

 Based on a quick skim of the paper, some of the key terms and concepts are:

- Explanation evaluation
- Objective measures
- Infidelity 
- Sensitivity
- Perturbations
- Optimal explanations
- Integrated gradients
- SmoothGrad
- Kernel smoothing
- Adversarial training
- Hessian regularization
- Fidelity-sensitivity tradeoff

The paper appears to focus on developing objective quantitative measures for evaluating explanations of machine learning models, specifically the notions of infidelity and sensitivity. It proposes optimizing explanations to minimize infidelity, which combines ideas from integrated gradients and SmoothGrad. It also analyzes the sensitivity of explanations and shows both theoretically and empirically that smoothing can improve both fidelity and sensitivity. The use of adversarial training and Hessian regularization to improve explanation fidelity and sensitivity is also discussed. Overall, the key theme seems to be developing and relating objective notions of fidelity and sensitivity for evaluating explanations.


## Based on the paper, what questions should we ask to create a comprehensive summary of it? Please list at least 10 questions.

 Here are 10 potential questions to summarize the key points of the paper:

1. What problem is the paper trying to solve? What are the limitations of existing approaches that motivate this work?

2. What are the key objective measures proposed to evaluate explanations? How are they defined?

3. What is the optimal explanation that minimizes the proposed infidelity measure? How does it relate to existing explanation techniques? 

4. How are existing explanation methods like Integrated Gradients and SmoothGrad connected to optimizing the infidelity measure?

5. What is the sensitivity measure proposed to quantify robustness of explanations? How is it defined and estimated?

6. What is the relationship shown between lowering sensitivity and improving fidelity of explanations?

7. How can we modify any given explanation to lower its sensitivity and infidelity simultaneously? What approach is proposed? 

8. What are some novel explanations generated by optimizing for infidelity under new perturbation distributions?

9. What experiments were conducted to evaluate the measures? What were the key results?

10. How was the infidelity measure validated to align with human judgement through controlled experiments?


## Design 10 in-depth questions about method proposed in this paper and avoid simple question.

 Here are 10 potential in-depth questions about the method proposed in the paper:

1. The paper proposes a new objective evaluation measure called "explanation infidelity". How is this measure defined and what are its key benefits compared to prior evaluation measures? 

2. The paper shows that several recent explanation methods like Integrated Gradients and DeepLIFT can be seen as optimal explanations under the proposed infidelity measure. Can you summarize the key insights behind this result?

3. The paper introduces the notion of "explanation sensitivity". How is this defined and why is it an important complementary measure to infidelity?

4. The paper proposes a novel way to simultaneously reduce both infidelity and sensitivity of an explanation by appropriate smoothing. Can you walk through the key analysis behind this result?

5. The optimal explanation proposed minimizes a loss function involving integrated gradients. What is the intuition behind using integrated gradients in the loss function? How does this connect to satisfying the completeness axiom?

6. The paper discusses global vs local explanations and notes they capture different aspects of faithfulness to the model. Can you summarize the key differences between global and local explanations? 

7. For image data, the paper proposes a "square removal" perturbation distribution to define infidelity. What is the motivation behind this compared to prior subsets based perturbation distributions?

8. The paper shows adversarial training can improve sensitivity and infidelity of gradient explanations. Walk through the analysis behind this result and discuss how it relates to robust optimization.

9. How does the paper evaluate explanations both quantitatively and qualitatively? Summarize the key results. Do the quantitative and qualitative results align?

10. The paper proposes a modification to make any explanation less sensitive. Discuss the tradeoffs between reducing sensitivity vs retaining fidelity of an explanation. How does the paper address this tradeoff?


## Summarize the paper in one sentence.

 The paper proposes objective measures of explanation infidelity and sensitivity to evaluate and improve explanations for complex machine learning models.


## Summarize the paper in one paragraphs.

 Here is a one paragraph summary of the paper:

This paper proposes two objective evaluation measures for explanations of machine learning models: infidelity and sensitivity. Infidelity measures how well an explanation captures changes in the model's predictions under significant perturbations. It is defined as the expected squared difference between the dot product of the perturbation and the explanation, and the change in the model output. Sensitivity quantifies how much an explanation changes under small perturbations. The paper shows these measures unify several recent explanation methods, which optimize infidelity for different perturbations. It also introduces new perturbations yielding novel optimal explanations. A key result is that smoothing explanations can simultaneously reduce sensitivity and infidelity. Experiments validate the proposed measures, showing infidelity aligns with human judgement, and smoothing improves both sensitivity and infidelity. The paper provides a framework for developing and evaluating explanations objectively.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 potential in-depth questions about the method proposed in the paper:

1. The paper proposes measuring the infidelity of an explanation as the expected mean squared error between the dot product of the input perturbation and the explanation, versus the difference in the model's output from the original input to the perturbed input. Why is this a good quantitative measure of how well an explanation captures the model's behavior? What are some limitations or caveats of this infidelity measure?

2. The paper shows that many recently proposed explanation methods like Integrated Gradients and SmoothGrad optimize an infidelity measure for certain types of input perturbations. How does framing these methods as optimizing infidelity help provide new insights into their properties? Does it suggest ways to improve upon them?

3. The paper introduces novel perturbations like noisy baselines and square removal to generate new optimal explanations by minimizing the corresponding infidelity measure. How do these new explanations compare qualitatively and quantitatively to existing methods on real-world datasets? What new insights do they provide?

4. Explanation sensitivity is proposed in the paper as another objective measure of explanations. Why is it important for explanations to have low sensitivity? What are some potential issues with explanations that have very high sensitivity?

5. The optimal explanation for minimizing sensitivity is a constant trivial explanation. How does the paper address the concern of simply minimizing sensitivity leading to useless explanations? What is the relationship shown between sensitivity and infidelity?

6. How does the kernel smoothing algorithm proposed in the paper reduce both the sensitivity and infidelity of a given explanation? Why does appropriate smoothing help with both criteria? What hyperparameters of the smoothing need to be set carefully?

7. How does the notion of local vs global explanations proposed in the paper differ from prior notions? Why is this distinction important when evaluating the fidelity and sensitivity of explanations?

8. The paper shows adversarial training can improve gradient explanation sensitivity and fidelity by optimizing the model itself. How does adversarial training achieve this? What are the tradeoffs compared to just smoothing explanations post-hoc?

9. Are the quantitative improvements in infidelity and sensitivity reflected in qualitative improvements when explanations are visualized? How are the human experiments designed to evaluate this?

10. What future directions for research does the framework of sensitivity and infidelity measures open up? How can these objective measures guide the development of new explanation methods?


## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a high-quality paragraph summarizing the key points of the paper:

This paper proposes and analyzes two objective measures for evaluating the quality of explanations for machine learning models: infidelity and sensitivity. The infidelity measure quantifies how well an explanation captures the sensitivity of the model's predictions to changes in the input features. The sensitivity measure evaluates how robust an explanation is to small perturbations. The paper shows that many recent explanation methods optimize infidelity for different types of perturbations. It also derives the form of the optimal explanation that minimizes infidelity, which combines ideas from Integrated Gradients and SmoothGrad. A key contribution is relating sensitivity to infidelity. The paper proves that appropriately reducing sensitivity also improves fidelity, and proposes a kernel smoothing algorithm to lower both measures. Experiments on MNIST, CIFAR-10 and ImageNet validate that smoothing explanations reduces sensitivity and infidelity, and also improves visual quality. Overall, this paper provides a theoretical framework to objectively evaluate and improve explanations through the lens of infidelity and sensitivity. By optimizing these measures, the paper obtains improved explanations that are more faithful and robust.
