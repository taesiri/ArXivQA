# [The sample complexity of multi-distribution learning](https://arxiv.org/abs/2312.04027)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a high-quality paragraph summarizing the key points of the paper:

This paper resolves the open question posed by Awasthi, Haghtalab, and Zhao (COLT 2023) regarding the optimal sample complexity of multi-distribution learning. The authors give an algorithm that achieves a sample complexity of $\tilde{O}((d+k)\epsilon^{-2}) \cdot (k/\epsilon)^{o(1)}$ for learning a hypothesis with maximum population loss at most $\epsilon$ greater than the best hypothesis in the class, matching the lower bound up to sub-polynomial factors. The key technique is a novel recursive width reduction procedure for boosting a weak learner: at each round, the algorithm reduces the width (range) of the loss vector based on soundness and completeness properties verified from the sample, thereby reducing regret and the number of required rounds. This boosting framework is applied recursively, with the algorithm at level $r$ used as the weak learner for level $r+1$. By setting the number of recursion levels $r$ appropriately, the exponent in the sample complexity guarantee can be systematically improved until optimal. The algorithm also adapts to remove the assumption of knowing the optimal loss. Overall, this resolves the major open question by developing a new boosting technique for multi-distribution learning.
