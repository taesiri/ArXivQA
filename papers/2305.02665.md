# [Learning Language-Specific Layers for Multilingual Machine Translation](https://arxiv.org/abs/2305.02665)

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper addresses is how to increase the model capacity per language pair in multilingual neural machine translation, while keeping inference efficient. Specifically, the authors propose using Language-Specific Transformer Layers (LSLs) to allow some layers in the encoder to be source or target language-specific, while keeping other layers shared across languages. This increases model capacity without changing inference speed.The key hypotheses tested in the paper are:1) Using a mix of shared and language-specific layers improves over using only shared layers.2) Having both source and target language-specific layers is better than having only one or the other. 3) Automatically searching for the best layer configuration outperforms manual selection.4) Initializing the language-specific layers from a shared pre-trained model boosts performance.Through experiments on 10 languages, the authors validate these hypotheses, showing LSLs can improve performance while keeping inference efficient. The central research question is how to effectively increase model capacity per language in multilingual NMT without slowing down inference.
