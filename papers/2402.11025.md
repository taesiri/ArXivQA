# [Training Bayesian Neural Networks with Sparse Subspace Variational   Inference](https://arxiv.org/abs/2402.11025)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper "Training Bayesian Neural Networks with Sparse Subspace Variational Inference":

Problem:
Bayesian neural networks (BNNs) offer uncertainty quantification but have substantially increased training and inference costs compared to standard neural networks. Prior works have used sparse-promoting priors or post-training pruning to obtain sparse BNNs, but these approaches still have high training costs and lack control over the target sparsity level. The key open question is how to develop a framework that can train sparse BNNs from scratch with both low training and inference costs.

Proposed Solution:
This paper proposes Sparse Subspace Variational Inference (SSVI), the first approach to train fully sparse BNNs with a consistent sparsity level throughout training and inference. SSVI confines the variational posterior to a sparse random subspace that is jointly optimized with the posterior parameters. It alternates between optimizing the subspace basis selection and the associated variational parameters. For basis selection, it uses a novel removal-and-addition strategy guided by new criteria based on weight distribution statistics, allowing it to achieve any target sparsity level.

Main Contributions:
- First framework to enable end-to-end sparse training and inference for BNNs, reducing both computational and memory costs
- Novel weight importance criteria tailored for BNN sparsity selection using distribution statistics 
- Alternating optimization of sparse subspace basis and variational posterior parameters
- Achieves 10-20x model compression with <3% performance drop and up to 20x FLOPs reduction compared to dense BNN training
- Enhanced robustness to hyperparameters and stability during training, sometimes surpassing dense BNN performance
- Establishes new state-of-the-art results for sparse BNN efficiency, accuracy and uncertainty quantification

In summary, SSVI pioneeringly enables fully sparse BNN training and inference by co-learning an adaptive sparse subspace using specially designed criteria. It significantly advances efficiency and uncertainty quality for BNNs.
