# [Universal and Transferable Adversarial Attacks on Aligned Language   Models](https://arxiv.org/abs/2307.15043)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be: Can effective and transferable universal adversarial attacks be generated automatically against aligned language models?The key hypothesis appears to be:By combining greedy search and gradient-based optimization over multiple prompts and models, it is possible to automatically generate adversarial text suffixes that can reliably induce a broad range of undesirable behaviors in aligned language models, including commercial models.In particular, the paper investigates whether a single adversarial suffix optimized on a few open source models could transfer effectively to fool other proprietary LLMs into generating harmful content. The experiments aim to demonstrate that universal and transferable attacks are feasible through automatic optimization, posing a challenge to current alignment techniques.The main contributions seem to be:1) Proposing a greedy coordinate gradient (GCG) method for discrete optimization of adversarial text.2) Demonstrating that optimizing on multiple prompts and models yields more transferable attacks. 3) Showing that prompts optimized on smaller models transfer surprisingly well to large proprietary LLMs like GPT-3.5/4 and Bard.4) Providing evidence that adversarial attacks may be an inherent vulnerability of current alignment methods based on training and prompts.In summary, the central hypothesis is on the feasibility of automated universal adversarial attacks on aligned LLMs, which the experiments confirm to be effective and broadly transferable. The results raise concerns about the robustness of existing techniques for training safe LLMs.
