# [Universal and Transferable Adversarial Attacks on Aligned Language   Models](https://arxiv.org/abs/2307.15043)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be: Can effective and transferable universal adversarial attacks be generated automatically against aligned language models?The key hypothesis appears to be:By combining greedy search and gradient-based optimization over multiple prompts and models, it is possible to automatically generate adversarial text suffixes that can reliably induce a broad range of undesirable behaviors in aligned language models, including commercial models.In particular, the paper investigates whether a single adversarial suffix optimized on a few open source models could transfer effectively to fool other proprietary LLMs into generating harmful content. The experiments aim to demonstrate that universal and transferable attacks are feasible through automatic optimization, posing a challenge to current alignment techniques.The main contributions seem to be:1) Proposing a greedy coordinate gradient (GCG) method for discrete optimization of adversarial text.2) Demonstrating that optimizing on multiple prompts and models yields more transferable attacks. 3) Showing that prompts optimized on smaller models transfer surprisingly well to large proprietary LLMs like GPT-3.5/4 and Bard.4) Providing evidence that adversarial attacks may be an inherent vulnerability of current alignment methods based on training and prompts.In summary, the central hypothesis is on the feasibility of automated universal adversarial attacks on aligned LLMs, which the experiments confirm to be effective and broadly transferable. The results raise concerns about the robustness of existing techniques for training safe LLMs.


## What is the main contribution of this paper?

Based on my reading, the main contributions of this paper are:1. It proposes a new attack method called Greedy Coordinate Gradient (GCG) that can reliably generate adversarial prompts to elicit harmful behaviors from aligned language models. This combines greedy search with using gradients to guide the search over discrete tokens. 2. It shows that this attack method outperforms prior work on adversarial attacks for language models, achieving much higher attack success rates on benchmarks designed to elicit harmful outputs.3. It demonstrates that the adversarial prompts generated by GCG transfer surprisingly well to other language models, including commercial systems like ChatGPT, Claude, and Bard. This indicates the attacks are fairly universal and transferable.4. The attacks transfer even when GCG is trained only on smaller open source models like Vicuna and Guanaco. This suggests that adversarial attacks may pose a serious challenge to aligning large language models.5. The results raise concerns about the robustness of current techniques for aligning language models, since adversarial attacks seem able to reliably circumvent them. This highlights the need for more reliable alignment methods.In summary, the key contribution is showing adversarial attacks can reliably fool aligned language models, outperforming prior attack methods and transferring to black box commercial systems. This demonstrates vulnerabilities in current alignment approaches and the need for more robust techniques.
