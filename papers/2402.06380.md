# [Optimal estimation of Gaussian (poly)trees](https://arxiv.org/abs/2402.06380)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
The paper studies the problem of learning Gaussian tree-structured probabilistic graphical models (directed trees and directed polytrees) from data. Specifically, it considers three related problems with progressively stronger assumptions:

1. Non-realizable setting: Learn the closest tree distribution when the true distribution is an arbitrary Gaussian.

2. Realizable setting: Learn the underlying tree distribution when the true distribution is tree-structured.  

3. Faithful setting: Recover the Markov equivalence class of the true directed tree/polytree under the faithfulness assumption.

The paper aims to develop computationally efficient algorithms for these problems with finite sample guarantees, and derive matching information-theoretic lower bounds to prove optimality.

Proposed Solutions:

1. Non-realizable setting: The paper shows the classical Chow-Liu algorithm, which builds a maximum spanning tree based on estimated mutual informations, learns the closest tree distribution using $O(d^2/\epsilon^2)$ samples.

2. Realizable setting: Under the realizability assumption, the sample complexity of Chow-Liu improves to $O(d/\epsilon)$.  

3. Faithful setting: The paper develops a variant of the PC algorithm called PC-Tree that uses the sample partial correlation as a conditional independence tester. It shows that PC-Tree can recover the true Markov equivalence class of directed trees/polytrees using $O((\log d)/c^2)$ samples under a notion of strong faithfulness parameterized by $c$.

For both the non-realizable and realizable settings, the paper provides matching information-theoretic lower bounds to show the optimality of Chow-Liu. For the faithful setting, it derives an analogous lower bound to establish the optimality of PC-Tree.

Main Contributions:

- First finite sample analysis of learning Gaussian trees under KL divergence. Prior work focused on total variation distance.

- Explicit sample complexity bounds and optimal algorithms for all three settings described above. First results on optimal rates for structure learning under faithfulness.

- Lower bounds establishing fundamental limits in each setting. Characterizes how assumptions change the complexity.

- Unified study connecting distribution learning and structure learning. Provides insights into phase transitions between the two problems.

The paper also discusses extensions of the results to bounded degree polytrees and presents experiments on synthetic data to evaluate the algorithms.
