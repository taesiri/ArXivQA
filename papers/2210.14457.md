# [Implicit Identity Leakage: The Stumbling Block to Improving Deepfake   Detection Generalization](https://arxiv.org/abs/2210.14457)

## What is the central research question or hypothesis that this paper addresses?

 The central research question this paper addresses is: 

What is causing the poor generalization performance of binary classifiers for deepfake detection, and how can this issue be solved?

Specifically, the authors hypothesize that binary classifiers for deepfake detection accidentally learn identity representations from the training data, which helps performance on in-distribution test sets but hurts generalization to out-of-distribution datasets. They term this issue "implicit identity leakage." 

To address this problem, the authors propose an "ID-unaware deepfake detection model" that focuses on local artifact areas rather than global identity features. Their proposed model contains an "artifact detection module" to locate artifact regions and guide the model to learn localized features. By reducing reliance on identity information, they aim to improve generalization across datasets.

Experiments verify the identity leakage in classifiers and show their proposed model successfully reduces this phenomenon and outperforms state-of-the-art methods on cross-dataset evaluation. Overall, the central hypothesis is that identity leakage hurts generalization in deepfake detection, and this can be mitigated by focusing on local artifact features.
