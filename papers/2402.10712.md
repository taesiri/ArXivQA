# [An Empirical Study on Cross-lingual Vocabulary Adaptation for Efficient   Generative LLM Inference](https://arxiv.org/abs/2402.10712)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- State-of-the-art generative language models (LLMs) like ChatGPT rely heavily on English data and tokenizers during pre-training. As a result, their efficiency and performance degrade when generating text in other languages. Specifically, LLMs tend to overfragment text leading to slower inference speeds and higher costs.

Proposed Solution: 
- The paper investigates cross-lingual vocabulary adaptation techniques to adapt existing English-centric LLMs to other languages. This involves replacing the source vocabulary with a target language vocabulary, optionally expanding the embedding matrix, and further pre-training the model on target language data (language adaptive pre-training or LAPT).

Methods Studied:
- Random initialization, heuristics-based initialization, cross-lingual and progressive initialization (CLP), CLP with sparsemax similarities (CLP+), and FOCUS initialization.

Experiments:
- 5 LLMs - BLOOM, TigerBot, Mistral (1B to 7B parameters)
- 4 typologically diverse languages - German, Japanese, Arabic, Swahili
- 4 tasks - textual entailment, question answering, summarization, span prediction
- Compared inference speed and downstream performance relative to source LLMs and LAPT baselines

Main Findings:
- Cross-lingual vocabulary adaptation gives up to 271.5% faster inference in 99% of cases
- Adapting multilingual LLMs leads to comparable performance to source models
- In-language prompting yields better efficiency and sometimes better accuracy than English prompting
- Simple random initialization is quite competitive, heuristics-based initialization is best for zero-shot transfer

Key Contributions:
- First systematic study showing cross-lingual vocabulary adaptation substantially improves inference efficiency of generative LLMs
- Analysis of various techniques for adapting LLMs to new languages in limited resource scenarios
- Evaluation across diverse languages, model sizes and NLP tasks
