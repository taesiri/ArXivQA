# RAFT: Reward rAnked FineTuning for Generative Foundation Model Alignment

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be:How can we effectively align generative foundation models like large language models (LLMs) and diffusion models with human preferences and ethics, in order to mitigate issues like implicit bias?The key hypotheses appear to be:1) Reward Ranked Fine-Tuning (RAFT), a new framework that iteratively selects high-reward samples from a generative model and fine-tunes the model on those samples, can effectively align generative models with a reward function capturing human preferences. 2) Compared to prevailing approaches like PPO, RAFT offers advantages in stability, sample efficiency, flexibility, and interpretability due to its simpler SFT-like training procedure and preference-based sample ranking.3) RAFT can be applied to align both LLMs and diffusion models by using an appropriate reward model, with competitive or superior performance to PPO.So in summary, the central question is how to align generative models, and the main hypothesis is that the proposed RAFT framework provides an effective solution by leveraging reward-ranked fine-tuning. Both LLMs and diffusion models are considered as use cases to validate the effectiveness and flexibility of RAFT.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contributions appear to be:1. The proposal of a new framework called RAFT (Reward rAnked FineTuning) for aligning generative models like large language models and diffusion models to human preferences/ethics using a reward function. 2. Demonstrating that RAFT can effectively improve model performance on both reward learning and automated metrics compared to supervised fine-tuning and policy-based reinforcement learning methods like PPO.3. Showing that RAFT works for both large language models and diffusion models, suggesting it could be applicable to other generative models as well.4. Highlighting some advantages of RAFT such as:- More stable and faster convergence compared to PPO due to the SFT-like training- Lower memory requirements by separating data generation and model fine-tuning- Flexibility in leveraging diverse data sources beyond on-policy samples- Robustness to reward scaling due to the ranking-based sample selection- Easier to interpret objective of learning from the induced best-of-K policy5. Providing extensive experiments on a large language model (LLaMA-7B) fine-tuned on the HH-RLHF dataset to demonstrate RAFT can improve alignment quality, as measured by reward and perplexity, compared to PPO and SFT baselines.6. Showing RAFT can also effectively improve sample quality and text-image alignment for diffusion models like Stable Diffusion.In summary, the main contribution seems to be proposing the RAFT framework as a simple yet effective approach for aligning generative models using human feedback, and demonstrating its advantages over prior methods empirically on both language and diffusion models.
