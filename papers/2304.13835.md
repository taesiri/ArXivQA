# Multi-Party Chat: Conversational Agents in Group Settings with Humans   and Models

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper addresses is how to develop conversational AI agents that can engage in coherent, multi-party conversations. Specifically, the paper focuses on two main challenges:1. Turn-taking - Deciding when an agent should speak up in a multi-party setting.2. Utterance coherence - Generating appropriate responses that take into account the dialogue history from multiple participants. The authors collected a new dataset called MultiLIGHT to train and evaluate models on these skills in multi-party chat settings with 3 participants. They compared several approaches, including training separate models for turn-taking and response generation. The key findings were:- Models trained on the MultiLIGHT dataset showed significant improvements in multi-party response quality compared to baseline models trained only on two-party dialogues. - For turn-taking, a speaker-only model performed best at predicting the next speaker.- For response generation, an utterance-only model fine-tuned on MultiLIGHT performed best at producing coherent, relevant, consistent responses grounded in the full context.- In human evaluations, their best model combining the speaker and utterance models showed substantial improvements over a baseline model in consistency, lack of contradictions, identity maintenance, and overall quality.So in summary, the main hypothesis was that multi-party training data and models tailored to the turn-taking and utterance challenges could improve over models designed for two-party dialogues. The results validated this hypothesis and identified effective approaches for the two sub-tasks.


## What is the main contribution of this paper?

The main contribution of this paper is introducing a new dataset and models for multi-party conversational AI, where multiple speakers can converse together beyond just two participants. Specifically:- They collect a new dataset called MultiLIGHT of multi-party conversations between 3 crowdworkers roleplaying fantasy characters. This provides a resource for training and evaluating models on aspects like turn-taking and coherence in multi-party chat.- They study different model architectures for handling turn-taking and response generation in this setting. This includes models that jointly or separately handle when to speak and what to say. - They evaluate these models both automatically and via human evaluations. Their best model combines a speaker-only model for turn taking with a response generation model trained on their new MultiLIGHT dataset. - This model outperforms prior work and baselines on metrics like consistency, lack of contradictions, and identity maintenance when evaluated by humans conversing with the model.So in summary, the key contribution is providing a new dataset, models, and experiments analyzing the challenges of multi-party conversational AI, where they demonstrate improvements over standard models that are designed for two-party conversations.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

The paper presents a new dataset and methods for training AI agents to have coherent and engaging conversations in multi-party chat settings with multiple humans and AI agents. The key findings are that models require skills to determine when to talk and to maintain coherent personas across multiple speakers, which existing dialogue models lack. The newly collected dataset helps models significantly improve on these skills.


## How does this paper compare to other research in the same field?

Here are a few thoughts on how this paper compares to other research on multi-party chat and conversational AI:- The paper makes a valuable contribution by releasing a new multi-party conversational dataset called MultiLIGHT. As the authors note, there is a lack of large, high-quality datasets for training and evaluating models for multi-party chat. The MultiLIGHT dataset helps address this need.- The paper thoroughly evaluates different methods for two key challenges in multi-party chat - turn-taking and utterance coherence. By comparing approaches like speaker-only models vs speaker-and-utterance models, the authors provide useful insights into what techniques work best.- The paper demonstrates strong improvements over prior work by fine-tuning on the MultiLIGHT dataset. The best model outperforms baselines like BlenderBot and the state-of-the-art RELIC model on metrics like perplexity and human evaluations. This highlights the value of in-domain multi-party training data.- The paper builds on prior work from the authors on the Light dialog environment. The MultiLIGHT dataset represents an extension of the two-party Light and Light Wild datasets to a multi-party setting. The continuity allows for interesting comparisons of the same models on two-party vs multi-party dialog.- The paper focuses on open-domain dialog with fixed characters in fantasy role-playing games. Other recent work has explored multi-party dialog in more goal-oriented domains like negotiations or collaborative tasks. The open-ended nature of this work complements those efforts.- The paper adopts a similar experimental approach as some other conversational AI papers - train/evaluate on automatic metrics, then validate with human evaluations. The human studies confirm the value of multi-party fine-tuning demonstrated in the automatic metrics.Overall, by releasing a valuable new dataset and conducting extensive experiments, this paper makes an important contribution to the nascent field of multi-party conversational AI. The insights on turn-taking and coherence will be useful for other researchers looking to build effective multi-party chatbots.
