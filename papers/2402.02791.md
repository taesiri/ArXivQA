# [Rethinking Optimization and Architecture for Tiny Language Models](https://arxiv.org/abs/2402.02791)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem Statement
- Large language models (LLMs) have shown impressive capabilities, but require prohibitively massive compute and memory, limiting deployment. There is a need for high-performance yet efficient "tiny" LMs.
- However, limited work explores effective methodologies tailored for training tiny LMs. LLMs employ similar large-scale recipes which may not transfer down. 

Proposed Solution 
- The paper systematically optimizes tiny LMs from 3 perspectives: architecture, initialization and optimization.

Architecture Design
- Use compact tokenizer by removing redundant low-frequency tokens, enhancing representation efficiency.  
- Model depth is most impactful for performance, despite slower speed. Around 20 layers is ideal.
- Feedforward network expanding rate does not impact performance much.

Parameter Initialization
- Inherit parameters from larger LMs, especially for initial and final layers. Intermediate layers tend to be more redundant.  
- Data-driven learnable criteria outperform heuristics in identifying redundant parameters.

Optimization Strategy  
- Adopt multi-round training and data sampling to mitigate forgetting.
- Small batch size <4M performs better. Moderate LR scaling for larger batches.  

Main Contributions
- Established effective design formulas and insights tailored for efficient tiny LMs: compact tokenizer, deeper architecture preference, inherit crucial parameters, multi-round training etc.
- Achieved SOTA results among tiny LMs. PanGu-Ï€-1.5B Pro outperforms Qwen-1.8B, using 16.7% fewer parameters.
- The optimized tiny LMs strike an improved balance between efficiency and performance.
