# [Judging LLM-as-a-judge with MT-Bench and Chatbot Arena](https://arxiv.org/abs/2306.05685)

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper addresses is how to evaluate large language model (LLM) based chat assistants for their alignment with human preferences, beyond just core capabilities measured by existing benchmarks. The key hypotheses are:1) Strong LLMs like GPT-4 can be used as judges to effectively evaluate other chatbot models on open-ended questions and match human preferences.2) The proposed evaluation approach using LLM judges is more scalable and automated compared to traditional human evaluation. 3) The new human preference benchmarks (MT-bench and Chatbot Arena) complement existing standardized benchmarks by capturing different aspects of model quality.The authors systematically examine the efficacy of using LLMs as judges by comparing their evaluations to human ratings. They introduce two new benchmarks tailored to assess human preference and model alignment. The paper investigates the agreement between LLM judges and humans, and argues LLM-based evaluation could become a promising standard practice.
