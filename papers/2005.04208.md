# [Condensed Movies: Story Based Retrieval with Contextual Embeddings](https://arxiv.org/abs/2005.04208)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research questions/hypotheses appear to be:1. Can we create a large-scale dataset of "condensed movies" consisting of key scenes from movies matched with high-level semantic descriptions to facilitate story understanding and narrative retrieval in long videos?2. Does incorporating contextual information from surrounding clips in a movie improve performance on text-to-video retrieval on this dataset compared to treating clips independently? 3. Can introducing a character module that matches actor names in text queries to face tracks in videos further improve retrieval performance, especially for within-movie retrieval?The key contributions seem to be:1. Introducing the Condensed Movie Dataset (CMD) of over 3,000 movies with ordered clips, descriptions, face tracks, and metadata.2. Proposing a Contextual Boost Module to incorporate features from surrounding clips to improve video embeddings for retrieval.3. Adding a character module to allow reasoning about character identities mentioned in text queries and recognized in videos. 4. Showing improved retrieval results from the proposed context module and character module, demonstrating the benefits of modeling context and identities for story understanding.5. Providing preliminary alignment between video clips and movie plot summaries to enable placing clips in the context of the full movie narrative.In summary, the main hypothesis is that leveraging contextual information and character identities is important for narrative understanding and retrieval in videos, which they demonstrate through experiments on their new condensed movie dataset.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contributions seem to be:1. The introduction of the Condensed Movie Dataset (CMD), a new large-scale movie dataset consisting of key video clips and high-level semantic descriptions from over 3,600 movies. The dataset provides condensed versions of full movie storylines. 2. Exploration of the role of context in video-text retrieval by proposing a Contextual Boost Module (CBM) that incorporates features from surrounding clips to improve the video embedding. This module is added to existing state-of-the-art retrieval models.3. A character module that encodes information about character identities and their mentions in text descriptions. This module provides a significant boost on the within-movie retrieval task.4. Benchmark retrieval results on the new CMD dataset using both cross-movie and within-movie settings. The best performing model combines the Contextual Boost Module with the Mixture of Embedding Experts model.5. Preliminary work on aligning video descriptions to movie plot summaries, which could allow placing clips in the context of the full movie storyline.In summary, the key innovations seem to be the introduction of a large new movie dataset to spur progress on story understanding, and techniques to incorporate contextual information from surrounding clips to better represent videos for retrieval. The character module also appears important for modeling identities in within-movie retrieval.
