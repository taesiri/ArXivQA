# [Synthetic is all you need: removing the auxiliary data assumption for   membership inference attacks against synthetic data](https://arxiv.org/abs/2307.01701)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be: Can membership inference attacks against synthetic tabular data still be successful without assuming access to an auxiliary dataset sampled from a similar distribution as the training data?In other words, the paper investigates whether the common assumption in prior membership inference attacks - that the attacker has access to auxiliary data from the same distribution - can be removed, while still developing successful attacks. The key hypothesis appears to be that membership inference attacks using only the synthetic data itself, without any auxiliary real data, can still infer membership of records with accuracy significantly better than random guessing.Specifically, the paper hypothesizes that:1) An attacker with black-box access to the synthetic data generator can run successful membership inference attacks using synthetic data to replace the auxiliary data typically assumed.2) Even an attacker who only has access to the released synthetic dataset, without any access to the generator, can still develop meaningful membership inference attacks.3) The accuracy of these synthetic data only attacks may currently be limited by a "double counting" issue, and resolving this could allow attacks using solely synthetic data to outperform traditional attacks relying on auxiliary data.In summary, the central hypothesis is that membership inference attacks can work using synthetic data alone, without the strong assumption of access to auxiliary data from the same distribution. The paper aims to evaluate this hypothesis empirically across different scenarios.


## What is the main contribution of this paper?

The main contribution of this paper is showing that membership inference attacks against synthetic data can be successful without assuming the attacker has access to an auxiliary dataset from the same distribution as the private training data. Specifically, the key contributions are:- Proposing more realistic attack scenarios where the attacker only has access to the synthetic data generator itself or just the released synthetic dataset. - Evaluating these attacks on two real-world datasets and two synthetic data generators, showing they can still infer membership of vulnerable records significantly better than random chance.- Identifying a "double counting" issue when using synthetic data to construct the shadow models for membership inference attacks. - Establishing an upper bound on attack accuracy when using only synthetic data by artificially avoiding this double counting issue. This upper bound is higher than traditional attacks using auxiliary data.- Highlighting the gap between this upper bound and the weaker attacks as an opportunity for future work to further improve attacks using only synthetic data.In summary, the main contribution is removing the strong assumption of requiring an auxiliary dataset for membership inference attacks against synthetic data. The results show these attacks are still successful using only synthetic data, making them more realistic threats in practice.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper: The paper shows that membership inference attacks against synthetic tabular data can still be successful even without the assumption that the attacker has access to an auxiliary dataset from the same distribution, making the attacks more realistic.


## How does this paper compare to other research in the same field?

Here are a few key points on how this paper compares to other research on membership inference attacks against synthetic data:- The main contribution is removing the assumption that the attacker has access to an auxiliary dataset from the same distribution as the private training data. Previous work assumed access to such an auxiliary dataset to construct shadow models for attacks. - The authors propose more realistic attack scenarios where the attacker only has access to the synthetic data generator (black box access) or just the released synthetic dataset. They show attacks are still successful in these scenarios, demonstrating a more practical privacy threat.- They identify and analyze the "double counting" issue that arises when using synthetic data to construct shadow models, instead of real auxiliary data. This provides insight into limitations and future work.- The attacks are evaluated across multiple real-world datasets, generators, target records, and attack methods. This demonstrates the attack effectiveness in different settings. - Results show synthetic-only attacks lose 11-15 percentage points in accuracy compared to attacks using auxiliary data, but accuracy is still substantially better than random guessing.- An upper bound scenario suggests synthetic-only attacks could potentially outperform auxiliary data attacks in the future, if the double counting issue is addressed.Overall, a key contribution is demonstrating the feasibility of membership inference attacks in more realistic threat models, without the strong assumption of auxiliary data access. The analysis also provides direction for developing stronger attacks and mitigation techniques in future work. The rigorous evaluation builds confidence that the attacks pose a practical risk.


## What future research directions do the authors suggest?

The paper suggests several future research directions:- Impact of releasing fewer synthetic records: The authors hypothesize that releasing fewer synthetic records than the number of records in the original dataset could reduce the accuracy of the attacks, but this comes at a cost to utility. They suggest evaluating the trade-off between number of released records and attack accuracy.- Testing on differentially private synthetic generators: The authors plan to confirm whether their findings translate to differentially private synthetic generators, which provide formal privacy guarantees but typically reduce utility. - Bridging the gap to the upper bound: The paper identifies a "double counting" issue that limits attack accuracy, and proposes solving this to get closer to the theoretical upper bound identified. Potential ideas include removing synthetic records close to the target record before constructing shadow models.- New attack methods: The paper notes their findings were consistent across two different attack methods, and suggest confirming whether new attack methods would also enable successful attacks using only synthetic data.- Understanding effects on formally anonymous data: The authors suggest investigating whether their findings apply to synthetically generated data considered anonymous under legal definitions like GDPR.In summary, key future directions are evaluating the effect of releasing fewer synthetic records, testing differentially private generators, improving attacks to get closer to the theoretical upper bound accuracy, generalizing findings to new attacks, and relating results to formal anonymity definitions. The authors aim to make attacks more realistic and provide guidance on risks of releasing synthetic data.


## Summarize the paper in one paragraph.

Here is a one paragraph summary of the paper:The paper presents an attack on synthetic tabular data that does not require an auxiliary dataset, removing a common assumption made in prior membership inference attacks against synthetic data. The authors consider an attacker with black box access to the generator that uses synthetic data to construct shadow datasets for the attack. They show this attack achieves 65.5% accuracy on average, only losing 11.6 percentage points compared to attacks using real auxiliary data. They further weaken the attack to only use the released synthetic dataset, achieving 62.8% accuracy. This demonstrates these attacks pose a realistic threat, as releasing synthetic data is common in practice. The authors identify a potential double counting issue with using synthetic data and construct an upper bound attack avoiding this issue. The upper bound attack reaches 85.8% accuracy, suggesting future work may bridge the gap between this upper bound and more realistic attack scenarios. Overall, the work shows membership inference attacks remain effective against synthetic tabular data without requiring auxiliary real data, highlighting the privacy risks of releasing synthetic data.


## Summarize the paper in two paragraphs.

Here is a two paragraph summary of the paper:The paper proposes a more realistic membership inference attack against synthetic tabular data by removing the assumption that the attacker has access to an auxiliary dataset from the same distribution as the private training data. Previous state-of-the-art attacks relied on this auxiliary data assumption to construct shadow models for training the attack classifier. The authors evaluate two scenarios: one where the attacker has black-box access to the synthetic data generator to query new synthetic samples, and another where the attacker only has access to the released synthetic dataset. Across two real datasets and synthetic generators, they show that the attack remains successful even without auxiliary data, achieving 65.5% accuracy with black-box access and 62.8% with just the released data. The latter scenario is especially significant as having the released synthetic data is a minimal assumption typically met in practice. The authors also identify and formalize a "double counting" issue that arises when using synthetic data to construct the shadows, and show there is still room for improvement by computing an upper bound of 85.8% accuracy. Overall, the work provides compelling evidence that membership inference attacks pose a realistic threat against synthetic tabular data generators, even without requiring auxiliary data.In summary, this paper proposes more realistic membership inference attacks against synthetic tabular data that do not rely on an auxiliary dataset assumption. The authors show the attacks remain successful in practice without this assumption, demonstrating via two scenarios that records can still be inferred as members of the private training set at accuracies of 65.5% and 62.8%, using just the synthetic generator or released synthetic data. They also identify issues arising from using synthetic data itself to construct the attack, and compute an upper bound showing potential for even higher accuracy. Overall, the work highlights the practical privacy risks of releasing synthetic tabular data.


## Summarize the main method used in the paper in one paragraph.

Here is a one paragraph summary of the main method used in the paper:The paper proposes membership inference attacks against synthetic tabular data without relying on an auxiliary dataset. Instead, the attacker uses only the synthetic data itself to construct shadow models and train a meta-classifier to predict membership. The authors consider three attack scenarios: (1) black-box access to the target generator to generate synthetic records, (2) access only to the published synthetic dataset, and (3) an upper bound scenario that avoids the "double counting" issue of using synthetic data. Experiments are conducted on two real-world datasets and two generators across the three scenarios. The results show that the attacks are successful even without auxiliary data, achieving above 62\% accuracy on average. The upper bound scenario demonstrates the potential for higher accuracy up to 85\% by solving the double counting problem. Overall, the work shows that membership inference attacks pose a more realistic threat than previously thought by removing the strong assumption of requiring an auxiliary dataset.
