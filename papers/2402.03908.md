# [EscherNet: A Generative Model for Scalable View Synthesis](https://arxiv.org/abs/2402.03908)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper "EscherNet: A Generative Model for Scalable View Synthesis":

Problem:
The paper tackles the problem of view synthesis - rendering novel views of a 3D scene from arbitrary camera poses, given a set of reference views. Prior work has focused on improving training speed and rendering efficiency but relies on scene-specific optimization and volumetric rendering, limiting scalability. The paper argues for learning a more generalizable 3D representation that facilitates scalable view synthesis without relying on ground truth 3D geometry or being constrained to a specific coordinate system. 

Proposed Solution - EscherNet
The paper proposes EscherNet, an image-to-image conditional diffusion model for view synthesis. Key aspects:

- Adopts Latent Diffusion architecture from Stable Diffusion with minimal modifications to enable multi-view generation. Handles arbitrary numbers of reference and target views.

- Employs a lightweight vision encoder to capture high and low-level signals from reference views. Encodes views as sets of tokens/embeddings.

- Introduces Camera Positional Encoding (CaPE) to encode 4-DOF or 6-DOF camera poses into view tokens, enabling self-attention to focus on relative camera transformations rather than global coordinates.

- Encourages reference-to-target and target-to-target view consistency via self and cross attention blocks in the decoder.

Main Contributions:

- Achieves state-of-the-art performance in novel view synthesis benchmarks, significantly outperforming prior 3D diffusion models in quality and camera control precision.

- Naturally unifies novel view synthesis and single/multi-image 3D reconstruction in one framework, outperforming task-specific methods in some cases.

- Exhibits remarkable versatility and scalability - can generate 100+ consistent views on one GPU despite only being trained on 3 reference and 3 target views. Easier to scale to large posed image datasets.

- Opens up new directions for designing scalable neural architectures that learn generalizable 3D representations from 2D supervision rather than relying on 3D ground truth geometry or optimization.
