# [Attentiveness to Answer Choices Doesn't Always Entail High QA Accuracy](https://arxiv.org/abs/2305.14596)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be:How does providing in-context examples affect the performance of large language models on multiple-choice QA tasks, and what does this reveal about the relationship between a model's attentiveness to valid answer choices and its accuracy on the task?The key hypotheses appear to be:1) Providing in-context examples with valid answer choices will increase a model's attentiveness (probability mass) on those answer choices.2) However, increased attentiveness does not necessarily translate to higher accuracy, especially for certain types of models. 3) The alignment between attentiveness and accuracy depends on factors like the model architecture and training objective.4) Probability normalization methods like PMI may be less effective for highly attentive models.The authors introduce a formalism for quantifying attentiveness and use it to study these hypotheses across different models, prompt formats, and datasets. A core finding is that increased attentiveness can actually hurt accuracy for some vanilla LMs, challenging assumptions that constrained outputs always improve performance.In summary, the central focus seems to be analyzing the connection between attentiveness and accuracy and how this varies across models and prompting methods, in order to gain insights into effectively using LMs for multiple-choice QA.


## What is the main contribution of this paper?

Based on my reading, the main contributions of this paper are:- Proposes a mathematical formalism for studying surface form competition (SFC) and attentiveness in language models for multiple choice tasks. - Introduces a new metric, probability mass on valid answer choices (PMVC), to directly measure a model's attentiveness to answer choices. Uses this to provide bounds on the extent of SFC.- Shows that including answer choices in prompts significantly increases models' attentiveness, but this does not always translate to improved accuracy. In fact, for some models like GPT-3 Curie and OPT, higher attentiveness can hurt accuracy.- Provides analysis on the effects of different parts of the input (question vs answer choices) on attentiveness and accuracy. Finds that answer choices play a bigger role in increasing attentiveness while the question increases accuracy more.- Studies when probability normalization methods like PMI-DC are beneficial. Finds they reliably help when models are not shown answer choices, but can hurt performance of models that benefit from answer choices in prompts.- Gives practical recommendations on prompting strategies to maximize accuracy on multiple choice tasks based on model type (vanilla LMs vs instruction-tuned).In summary, the key contribution is using the proposed formalism and metric to gain new insights into the relationship between attentiveness and accuracy in LMs, challenging assumptions that increasing attentiveness always helps. The paper provides actionable advice on how to prompt different models for multiple choice tasks.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Based on skimming the paper, I would summarize it in one sentence as: The paper proposes a formalism to study surface form competition in large language models, and finds that increasing attentiveness to valid answer choices does not always improve accuracy on multiple choice QA tasks.


## How does this paper compare to other research in the same field?

Based on my review, here is how I see this paper comparing to other research in the same field:Overall, this paper makes a nice contribution to the literature on prompting large language models for multiple-choice QA tasks. The key novelty is in directly measuring model attentiveness to answer choices, rather than relying purely on end-task accuracy. This allows the authors to test some core assumptions in prior work, like the link between reducing "surface form competition" and improving accuracy. The formalism introduced to measure attentiveness (probability mass on valid answer choices) is intuitive and allows clear measurement of the upper bound on possible gains from resolving surface form issues. The prompts designed to manipulate attentiveness also provide a simple and practical method for future work.Compared to other prompting papers like Liu et al. 2022 and Lu et al. 2022 which aim to optimize accuracy, this work takes a step back to focus more on analyzing prompting mechanisms. The scatterplots clearly show attention doesn't directly translate to gains. The prompt ablation experiments are also more systematic than similar analyses in other recent prompting papers.The comparison of different model types is another strength. Showing that techniques effective for certain models can be ineffective or detrimental for others provides an important caveat for the field. Likewise, the analysis of different scoring mechanisms sheds light on their applicability across models and prompts.Overall, I found this to be a thoughtful analysis of prompting strategies that manages to produce insights that should influence how the field approaches prompting going forward. The introduction of a formalism for attentiveness and demonstration of its imperfect link with accuracy are novel contributions. The paper also provides practical guidance through its prompts and scoring recommendations.


## What future research directions do the authors suggest?

The authors suggest the following future research directions:- Developing methods to automatically determine the optimal number of in-context examples to include in prompts for a given task and model. They found performance was not always improved by adding more examples.- Further study of the relationship between attentiveness (probability mass on valid answers) and accuracy. In particular, understanding why increasing attentiveness can sometimes decrease accuracy for certain models.- More investigation into the effectiveness of probability normalization methods like PMI-DC. The authors found these methods were less beneficial for instruction-tuned models that already placed high probability mass on valid answers when shown in-context examples.- Exploring other scoring methods like length normalization that could potentially improve on sequence scoring.- Testing the insights from this work on a wider range of models, datasets, and task formats to determine how broadly applicable they are.- Developing better methods for quantifying and reducing surface form competition beyond using in-context examples.- Studying the role of other factors like prompt length, model size, etc. on attentiveness and accuracy.- Examining the effectiveness of training objectives beyond autoregressive next token prediction and instruction tuning for multiple choice QA.The key themes are better understanding the connection between attentiveness and accuracy, determining when different prompting strategies and scoring methods are most effective, and developing new techniques to improve multi-choice QA performance in low data regimes.


## Summarize the paper in one paragraph.

The paper proposes a mathematical formalism for studying surface form competition in large language models when applied to multiple-choice question answering tasks. It introduces a metric called total probability mass on valid answer choices (PMV) to measure a model's attentiveness to answer choices and uses it to upper bound the extent and impact of surface form competition. The key findings are: 1) In-context learning with examples containing answer choices is an effective way to increase PMV and reduce surface form competition. 2) Increased PMV does not always translate to higher accuracy, especially for certain language models trained only on next-token prediction. In fact, including answer choices in prompts can hurt accuracy for some models. 3) Probability normalization methods like PMI-DC improve accuracy when PMV is low but can decrease accuracy when PMV is already high after in-context learning. Overall, the work challenges assumptions that increasing attentiveness and reducing surface form competition will improve accuracy, and provides insights into effectively prompting language models for multiple-choice QA.


## Summarize the main method used in the paper in one paragraph.

The paper introduces a mathematical formalism for studying surface form competition (SFC) in language models when applied to multiple-choice question answering tasks. To measure SFC, they propose using the total probability mass assigned by the model to valid answer choices as a metric. They then provide an upper bound on how much SFC can impact accuracy based on this metric. To reduce SFC, they propose using in-context learning by including examples with answer choices in the prompt, which constrains the model to place probability mass only on valid choices. They test this on several models and datasets, finding it substantially increases the probability mass on valid choices across models.However, they find that increasing probability mass on valid choices does not always translate to improved accuracy. For some models like GPT-3, accuracy is best without answer choices in the prompt. They also analyze the effect of prior probability normalization methods like PMI-DC, finding it helps when models have low attentiveness to choices but can hurt when attentiveness is already high after in-context learning. Overall, the work provides a formal grounding for understanding SFC and demonstrates that methods to increase attentiveness do not necessarily improve accuracy.


## Summarize the paper in two paragraphs.

Here is a two paragraph summary of the paper:The paper investigates the relationship between a language model's attentiveness to valid answer choices and its accuracy on multiple-choice QA tasks when prompted with few or no examples. The authors propose measuring attentiveness as the total probability mass assigned to valid answer choices. They find that constraining the model's output space by providing answer choices in the prompt increases attentiveness, but does not always improve accuracy. In fact, for some models like GPT-3, providing answer choices hurts accuracy, likely because it moves the task further from next token prediction. The paper challenges the assumption that reducing "surface form competition" always improves accuracy. It also finds that the effectiveness of techniques like PMI scoring depends on the model and prompt format. Overall, the key findings are: 1) Attentiveness to answer choices can be directly increased via prompt engineering, but does not reliably improve accuracy across models, 2) For large pre-trained models, accuracy is often best without answer choices in the prompt, while the opposite holds for instruction-tuned models, and 3) Scoring methods like PMI are most effective when models have low attentiveness. The paper provides practical insights about properly constraining language models for multiple-choice QA based on their training methodology. It introduces a formalism for studying this phenomenon and demonstrates the importance of directly measuring attentiveness separate from end task accuracy.
