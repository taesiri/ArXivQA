# [Data Transformation to Construct a Dataset for Generating   Entity-Relationship Model from Natural Language](https://arxiv.org/abs/2312.13694)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem: 
- Generating entity-relationship (ER) models from natural language (NL) descriptions like requirements is useful but manual design is difficult. 
- Existing rule-based NL2ERM approaches have poor generalization due to needing many rules to cover synonyms and linguistic patterns.  
- Deep learning models can generalize better but lack large NL2ERM datasets.

Proposed Solution:
- Key insight is NL2ERM is similar to text-to-SQL, which has some existing large datasets.
- Propose a novel data augmentation algorithm to transform text-to-SQL datasets into NL2ERM datasets in 3 steps:
   1) Transform database into a raw ER model.
   2) Apply schema linking between NL and SQL to annotate tokens with entities/attributes. 
   3) Prune unused entities/attributes to get the final ER model.
- Apply algorithm on Spider text-to-SQL dataset to produce a large-scale NL2ERM dataset.
- Also collect additional NL with requirements and scenarios to supplement dataset.
- Train two state-of-the-art information extraction models on dataset.

Main Contributions:
- First data augmentation algorithm to transform text-to-SQL data into NL2ERM data.
- Generated first large-scale fine-grained NL2ERM dataset by applying algorithm to Spider and collecting additional data.
- Show trained models achieve high performance, demonstrating usefulness of dataset.

The key ideas are leveraging the similarity of text-to-SQL and NL2ERM to transform existing text-to-SQL datasets, generating a fine-grained NL2ERM dataset, and showing state-of-the-art deep learning models can be trained on this dataset to achieve strong NL2ERM performance.
