# [ElasticViT: Conflict-aware Supernet Training for Deploying Fast Vision   Transformer on Diverse Mobile Devices](https://arxiv.org/abs/2303.09730)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question seems to be: 

How can we effectively train a high-quality vision transformer (ViT) supernet that can support both tiny and large ViT models for diverse mobile devices with varying computational capabilities?

The key challengesidentified in training such a supernet are:

- Training over an extremely large search space that includes both tiny and large ViT models results in performance degradation due to optimization interference between subnets of vastly different sizes.

- The typical supernet training techniques like sandwich rule sampling can cause large differences in model sizes between the sampled subnets, leading to gradient conflicts. 

To address these issues, the paper proposes two main techniques:

1) Complexity-aware sampling to constrain the FLOPs differences between subnets sampled across training steps, while still covering different subnet sizes. This is done using adjacent step sampling and a hierarchical smallest subnet set.

2) Performance-aware sampling to sample good subnets and reduce gradient conflicts, by using a quality-aware memory bank and ViT architecture preference rule.

By applying these techniques to train a mobile-specialized ViT supernet, the paper demonstrates superior performance of the discovered ViT models on ImageNet classification and across diverse mobile devices compared to prior efficient CNN and ViT models.

In summary, the central hypothesis is that by designing effective techniques to train a very large ViT supernet, it is possible to obtain high-quality tiny and large ViT models suited for diverse mobile devices. The proposed complexity-aware and performance-aware sampling techniques validate this hypothesis.


## What is the main contribution of this paper?

 The main contributions of this paper are:

1. It proposes ElasticViT, a two-stage NAS approach to train a high-quality vision transformer (ViT) supernet and search for optimal subnetworks that achieve high accuracy and low latency on mobile devices. 

2. It identifies the key issue of gradient conflict when training a very large ViT supernet covering both small and large models, which causes poor performance. The paper provides an analysis on the root causes of gradient conflict.

3. To address the gradient conflict issue, the paper proposes two techniques: complexity-aware sampling to constrain FLOPs differences between subnets, and performance-aware sampling to favor good subnets during training. These enhance supernet training.

4. Experiments demonstrate ElasticViT models achieve state-of-the-art accuracy under mobile latency constraints, outperforming prior efficient CNNs and ViTs. Tiny and small ElasticViT models are the first to surpass CNNs in accuracy and speed within 200MFLOPs.

In summary, the key innovation is developing techniques to train high-quality ViT supernets over an extremely large search space covering diverse mobile devices. This allows tiny to large ViT models to be automatically designed via NAS for efficient deployment, instead of relying on manual model design.
