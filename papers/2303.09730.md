# [ElasticViT: Conflict-aware Supernet Training for Deploying Fast Vision   Transformer on Diverse Mobile Devices](https://arxiv.org/abs/2303.09730)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question seems to be: 

How can we effectively train a high-quality vision transformer (ViT) supernet that can support both tiny and large ViT models for diverse mobile devices with varying computational capabilities?

The key challengesidentified in training such a supernet are:

- Training over an extremely large search space that includes both tiny and large ViT models results in performance degradation due to optimization interference between subnets of vastly different sizes.

- The typical supernet training techniques like sandwich rule sampling can cause large differences in model sizes between the sampled subnets, leading to gradient conflicts. 

To address these issues, the paper proposes two main techniques:

1) Complexity-aware sampling to constrain the FLOPs differences between subnets sampled across training steps, while still covering different subnet sizes. This is done using adjacent step sampling and a hierarchical smallest subnet set.

2) Performance-aware sampling to sample good subnets and reduce gradient conflicts, by using a quality-aware memory bank and ViT architecture preference rule.

By applying these techniques to train a mobile-specialized ViT supernet, the paper demonstrates superior performance of the discovered ViT models on ImageNet classification and across diverse mobile devices compared to prior efficient CNN and ViT models.

In summary, the central hypothesis is that by designing effective techniques to train a very large ViT supernet, it is possible to obtain high-quality tiny and large ViT models suited for diverse mobile devices. The proposed complexity-aware and performance-aware sampling techniques validate this hypothesis.
