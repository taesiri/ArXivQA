# [ElasticViT: Conflict-aware Supernet Training for Deploying Fast Vision   Transformer on Diverse Mobile Devices](https://arxiv.org/abs/2303.09730)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question seems to be: 

How can we effectively train a high-quality vision transformer (ViT) supernet that can support both tiny and large ViT models for diverse mobile devices with varying computational capabilities?

The key challengesidentified in training such a supernet are:

- Training over an extremely large search space that includes both tiny and large ViT models results in performance degradation due to optimization interference between subnets of vastly different sizes.

- The typical supernet training techniques like sandwich rule sampling can cause large differences in model sizes between the sampled subnets, leading to gradient conflicts. 

To address these issues, the paper proposes two main techniques:

1) Complexity-aware sampling to constrain the FLOPs differences between subnets sampled across training steps, while still covering different subnet sizes. This is done using adjacent step sampling and a hierarchical smallest subnet set.

2) Performance-aware sampling to sample good subnets and reduce gradient conflicts, by using a quality-aware memory bank and ViT architecture preference rule.

By applying these techniques to train a mobile-specialized ViT supernet, the paper demonstrates superior performance of the discovered ViT models on ImageNet classification and across diverse mobile devices compared to prior efficient CNN and ViT models.

In summary, the central hypothesis is that by designing effective techniques to train a very large ViT supernet, it is possible to obtain high-quality tiny and large ViT models suited for diverse mobile devices. The proposed complexity-aware and performance-aware sampling techniques validate this hypothesis.


## What is the main contribution of this paper?

 The main contributions of this paper are:

1. It proposes ElasticViT, a two-stage NAS approach to train a high-quality vision transformer (ViT) supernet and search for optimal subnetworks that achieve high accuracy and low latency on mobile devices. 

2. It identifies the key issue of gradient conflict when training a very large ViT supernet covering both small and large models, which causes poor performance. The paper provides an analysis on the root causes of gradient conflict.

3. To address the gradient conflict issue, the paper proposes two techniques: complexity-aware sampling to constrain FLOPs differences between subnets, and performance-aware sampling to favor good subnets during training. These enhance supernet training.

4. Experiments demonstrate ElasticViT models achieve state-of-the-art accuracy under mobile latency constraints, outperforming prior efficient CNNs and ViTs. Tiny and small ElasticViT models are the first to surpass CNNs in accuracy and speed within 200MFLOPs.

In summary, the key innovation is developing techniques to train high-quality ViT supernets over an extremely large search space covering diverse mobile devices. This allows tiny to large ViT models to be automatically designed via NAS for efficient deployment, instead of relying on manual model design.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the key points from the paper:

The paper proposes ElasticViT, a two-stage neural architecture search method to train a high-quality vision transformer supernet that can deploy accurate and low-latency ViT models on diverse mobile devices, using techniques like complexity-aware sampling and performance-aware sampling to address the issue of gradient conflict caused by subnets with vastly different sizes.


## How does this paper compare to other research in the same field?

 Based on my reading, the key contributions of this paper compared to other research on efficient vision transformers and neural architecture search are:

- It proposes a new two-stage NAS approach called ElasticViT to automate the design of efficient vision transformers for mobile devices. This is one of the first works to leverage NAS to deliver high-accuracy vision transformers ranging from very small (37MFLOPs) to large (800MFLOPs) sizes. 

- It identifies and analyzes the problem of gradient conflict that arises when training a supernet over an extremely large search space containing transformers of vastly different sizes/complexities. The analysis on the root causes of gradient conflict is novel.

- It introduces two new techniques - complexity-aware sampling and performance-aware sampling - to address the gradient conflict issue for robust supernet training. These techniques are tailored for the transformer architecture and have not been explored in prior NAS research.

- Experiments demonstrate ElasticViT models achieve new state-of-the-art ImageNet accuracy under mobile latency constraints, outperforming prior efficient CNN and transformer models. The models also have strong transfer performance on other datasets.

- The work delivers the first vision transformers that can surpass CNNs in accuracy, FLOPs and speed for mobile-regime models below 200MFLOPs. Prior to this, CNNs were superior for such tiny on-device models.

Overall, this work makes several important contributions in efficiently designing transformers for mobile devices through NAS. The analysis on gradient conflict and proposed techniques to address this issue are novel. The experiments also demonstrate the effectiveness of the approach compared to prior arts. This helps advance the application of transformers to mobile and edge scenarios.


## What future research directions do the authors suggest?

 Based on my reading of the paper, some of the main future research directions suggested by the authors include:

- Developing techniques to train even larger ViT supernets that can cover an even wider diversity of mobile devices and capabilities. The authors mention their supernet is 10^7x larger than typical NAS supernets, but covering an even greater range of devices could be useful.

- Exploring methods to enhance the performance-aware sampling scheme to sample subnets with even higher potential accuracy. The authors propose some techniques here like the preference rule and memory bank, but more advanced methods could help improve supernet training further.

- Applying the proposed conflict-aware supernet training techniques to other model families beyond ViTs, like CNNs. The authors focus on ViT here but the ideas could generalize.

- Evaluating the approach on more complex vision tasks beyond image classification, like object detection and segmentation. The authors demonstrate promising ImageNet classification results but assessing performance on other tasks is an area for future work.

- Further improving the efficiency of the searched ViT models through techniques like knowledge distillation, quantization, and pruning. The authors obtain efficient models through the architecture search but additional optimizations could help.

- Deploying the models on more mobile devices and platforms to evaluate real-world efficiency. The authors test on a few devices but validation on more hardware can help drive adoption.

In general, the paper develops some nice techniques for conflict-aware supernet training but applying and extending these ideas to larger supernets, other models, more tasks, and greater efficiency optimizations represent interesting areas for future research.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the key points from the paper:

The paper proposes ElasticViT, a novel two-stage neural architecture search (NAS) approach to automatically design accurate and low-latency vision transformers (ViTs) for diverse mobile devices. It introduces a very large search space covering tiny to large ViTs to accommodate mobile devices with varying computational capabilities. However, directly training a supernet on this vast space leads to poor performance due to gradient conflicts caused by subnets with vastly different model sizes being uniformly sampled during training. To address this, ElasticViT proposes two key techniques: 1) complexity-aware sampling, which limits the FLOPs differences between sampled subnets across training steps, and uses multiple smallest subnets to avoid large FLOPs gaps; 2) performance-aware sampling, which samples subnets likely to have higher accuracy based on a ViT architecture preference rule and quality-aware memory bank. Experiments show ElasticViT models achieve state-of-the-art ImageNet accuracy under mobile latency constraints, significantly outperforming prior efficient CNNs and ViTs. The techniques also demonstrably improve supernet training quality. Overall, ElasticViT enables accurate and fast vision transformers on diverse mobile devices.
