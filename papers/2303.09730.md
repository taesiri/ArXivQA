# [ElasticViT: Conflict-aware Supernet Training for Deploying Fast Vision   Transformer on Diverse Mobile Devices](https://arxiv.org/abs/2303.09730)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question seems to be: 

How can we effectively train a high-quality vision transformer (ViT) supernet that can support both tiny and large ViT models for diverse mobile devices with varying computational capabilities?

The key challengesidentified in training such a supernet are:

- Training over an extremely large search space that includes both tiny and large ViT models results in performance degradation due to optimization interference between subnets of vastly different sizes.

- The typical supernet training techniques like sandwich rule sampling can cause large differences in model sizes between the sampled subnets, leading to gradient conflicts. 

To address these issues, the paper proposes two main techniques:

1) Complexity-aware sampling to constrain the FLOPs differences between subnets sampled across training steps, while still covering different subnet sizes. This is done using adjacent step sampling and a hierarchical smallest subnet set.

2) Performance-aware sampling to sample good subnets and reduce gradient conflicts, by using a quality-aware memory bank and ViT architecture preference rule.

By applying these techniques to train a mobile-specialized ViT supernet, the paper demonstrates superior performance of the discovered ViT models on ImageNet classification and across diverse mobile devices compared to prior efficient CNN and ViT models.

In summary, the central hypothesis is that by designing effective techniques to train a very large ViT supernet, it is possible to obtain high-quality tiny and large ViT models suited for diverse mobile devices. The proposed complexity-aware and performance-aware sampling techniques validate this hypothesis.


## What is the main contribution of this paper?

 The main contributions of this paper are:

1. It proposes ElasticViT, a two-stage NAS approach to train a high-quality vision transformer (ViT) supernet and search for optimal subnetworks that achieve high accuracy and low latency on mobile devices. 

2. It identifies the key issue of gradient conflict when training a very large ViT supernet covering both small and large models, which causes poor performance. The paper provides an analysis on the root causes of gradient conflict.

3. To address the gradient conflict issue, the paper proposes two techniques: complexity-aware sampling to constrain FLOPs differences between subnets, and performance-aware sampling to favor good subnets during training. These enhance supernet training.

4. Experiments demonstrate ElasticViT models achieve state-of-the-art accuracy under mobile latency constraints, outperforming prior efficient CNNs and ViTs. Tiny and small ElasticViT models are the first to surpass CNNs in accuracy and speed within 200MFLOPs.

In summary, the key innovation is developing techniques to train high-quality ViT supernets over an extremely large search space covering diverse mobile devices. This allows tiny to large ViT models to be automatically designed via NAS for efficient deployment, instead of relying on manual model design.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the key points from the paper:

The paper proposes ElasticViT, a two-stage neural architecture search method to train a high-quality vision transformer supernet that can deploy accurate and low-latency ViT models on diverse mobile devices, using techniques like complexity-aware sampling and performance-aware sampling to address the issue of gradient conflict caused by subnets with vastly different sizes.


## How does this paper compare to other research in the same field?

 Based on my reading, the key contributions of this paper compared to other research on efficient vision transformers and neural architecture search are:

- It proposes a new two-stage NAS approach called ElasticViT to automate the design of efficient vision transformers for mobile devices. This is one of the first works to leverage NAS to deliver high-accuracy vision transformers ranging from very small (37MFLOPs) to large (800MFLOPs) sizes. 

- It identifies and analyzes the problem of gradient conflict that arises when training a supernet over an extremely large search space containing transformers of vastly different sizes/complexities. The analysis on the root causes of gradient conflict is novel.

- It introduces two new techniques - complexity-aware sampling and performance-aware sampling - to address the gradient conflict issue for robust supernet training. These techniques are tailored for the transformer architecture and have not been explored in prior NAS research.

- Experiments demonstrate ElasticViT models achieve new state-of-the-art ImageNet accuracy under mobile latency constraints, outperforming prior efficient CNN and transformer models. The models also have strong transfer performance on other datasets.

- The work delivers the first vision transformers that can surpass CNNs in accuracy, FLOPs and speed for mobile-regime models below 200MFLOPs. Prior to this, CNNs were superior for such tiny on-device models.

Overall, this work makes several important contributions in efficiently designing transformers for mobile devices through NAS. The analysis on gradient conflict and proposed techniques to address this issue are novel. The experiments also demonstrate the effectiveness of the approach compared to prior arts. This helps advance the application of transformers to mobile and edge scenarios.
