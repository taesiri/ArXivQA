# [Adapting Short-Term Transformers for Action Detection in Untrimmed   Videos](https://arxiv.org/abs/2312.01897)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a high-quality paragraph summarizing the key points of the paper:

This paper proposes ViT-TAD, a simple yet effective end-to-end temporal action detection framework based on the plain vision transformer (ViT) backbone. To adapt the pre-trained short-term ViTs for modeling longer videos, the authors design two propagation modules: (1) The inner-backbone propagation exchanges temporal information across snippets within the backbone through cross-snippet blocks inserted between ViT blocks. This allows treating multiple snippets as a unified entity. (2) The post-backbone propagation refines the snippet features using temporal transformer encoder layers to enlarge the receptive field. Equipped with these modules and standard TAD heads like BasicTAD and AFSD, ViT-TAD establishes new state-of-the-art results among end-to-end TAD methods on THUMOS14, achieving 69.0 average mAP with ViT-B pre-trained on VideoMAE. Ablation studies demonstrate the efficacy of the propagation modules. The simplicity of ViT-TAD facilitates effective end-to-end training and benefitting from large-scale pre-training, contrasting prior complex TAD pipelines. The strong performance proves ViT-TAD as a promising new TAD baseline.
