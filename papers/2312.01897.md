# [Adapting Short-Term Transformers for Action Detection in Untrimmed   Videos](https://arxiv.org/abs/2312.01897)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a high-quality paragraph summarizing the key points of the paper:

This paper proposes ViT-TAD, a simple yet effective end-to-end temporal action detection framework based on the plain vision transformer (ViT) backbone. To adapt the pre-trained short-term ViTs for modeling longer videos, the authors design two propagation modules: (1) The inner-backbone propagation exchanges temporal information across snippets within the backbone through cross-snippet blocks inserted between ViT blocks. This allows treating multiple snippets as a unified entity. (2) The post-backbone propagation refines the snippet features using temporal transformer encoder layers to enlarge the receptive field. Equipped with these modules and standard TAD heads like BasicTAD and AFSD, ViT-TAD establishes new state-of-the-art results among end-to-end TAD methods on THUMOS14, achieving 69.0 average mAP with ViT-B pre-trained on VideoMAE. Ablation studies demonstrate the efficacy of the propagation modules. The simplicity of ViT-TAD facilitates effective end-to-end training and benefitting from large-scale pre-training, contrasting prior complex TAD pipelines. The strong performance proves ViT-TAD as a promising new TAD baseline.


## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper "Adapting Short-Term Transformers for Action Detection in Untrimmed Videos":

Problem:
- Temporal action detection (TAD) aims to localize and classify action instances in long untrimmed videos. 
- Most prior TAD methods treat transformer networks as off-the-shelf feature extractors for short video snippets independently, failing to model inter-snippet relationships and fully utilize transformers' representation power.

Proposed Solution:
- Propose ViT-TAD, an end-to-end TAD framework that adapts short-term Vision Transformers (ViTs) to untrimmed video modeling.
- Design an inner-backbone propagation module to enable information exchange across snippets inside the ViT backbone. Use self-attention in global blocks and 3D convolutions in local blocks.
- Add a post-backbone propagation module with temporal transformer layers for further clip-level feature aggregation.
- Use simple TAD heads like BasicTAD and AFSD for final detection.

Main Contributions:
- First end-to-end TAD framework built on plain ViT backbones that allows effective video-level snippet interaction.
- Achieve new state-of-the-art performance on THUMOS14 (69.0 mAP), ActivityNet (37.12 mAP) and FineAction (17.20 mAP) by using VideoMAE pre-trained ViT-B.
- Analysis shows global blocks are better than local blocks and post-backbone propagation brings consistent gains.
- Framework is simple, efficient and compatible with different TAD heads. Serves as a new competitive TAD baseline.

In summary, the paper focuses on adapting short-term pre-trained ViTs to untrimmed video modeling for end-to-end temporal action detection. The proposed method achieves top results on multiple benchmarks.
