# [Information-Theoretic Distillation for Reference-less Summarization](https://arxiv.org/abs/2403.13780)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
The current state-of-the-art in abstractive summarization relies on using large proprietary language models (LLMs) like ChatGPT either directly or as teacher models for imitation learning. However, there are concerns around cost, controllability and generalization of summaries generated this way. The key question is - can small-scale models be trained to be powerful yet controllable summarizers without relying on LLMs or human references?

Proposed Solution: 
The paper proposes InfoSumm, a novel framework to distill a powerful yet controllable summarizer based on an information-theoretic objective, without using LLM capabilities or human references. 

The key ideas are:
(1) Formalize desiderata of summarization (saliency, faithfulness, brevity) via pointwise mutual information (PMI) between document and summary under length constraint. 
(2) Self-train a small off-the-shelf LM into an expert teacher via expert iteration, using self-supervised critics based on the PMI formulation.
(3) Distill a compact yet powerful student summarizer from this improved teacher.

Main Contributions:
(1) Novel PMI-based formulation to unify key aspects of summarization. Enables fully self-supervised distillation without human references.
(2) Expert iteration method to specialize a small LM into a quality data generator for summarization. 
(3) Compact yet powerful student summarizer (568M params) that achieves strong performance across diverse tasks - news summarization, zero-shot domain adaptation, controllable summarization.
(4) Analysis showing higher sampling efficiency and data diversity compared to human-authored summarization datasets.
(5) Demonstration that small LMs can be optimized to excel at summarization via alternative learning approaches, without reliance on LLMs.

The summary covers the key points about the problem being addressed, the proposed information-theoretic solution InfoSumm, its effectiveness demonstrated across diverse tasks, and highlights the main contributions around formulating summarization desiderata, specializing small LMs via self-supervision, and showing the promise of alternative learning schemes.
