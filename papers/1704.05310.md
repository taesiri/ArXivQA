# [Unsupervised Learning by Predicting Noise](https://arxiv.org/abs/1704.05310)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the main research question is:How can we train deep convolutional neural networks in an unsupervised manner to produce useful visual features, while avoiding common issues like feature collapse? The central hypothesis is that by mapping deep features to a set of fixed, randomly generated target representations and aligning the features to those targets, the model will learn more robust and transferable features compared to other unsupervised methods. This approach, called Noise As Targets (NAT), avoids feature collapse by fixing the target reps and diversifying the features through the alignment.The authors propose an online optimization method to efficiently train NAT at large scale, and evaluate the quality of the learned features on transfer tasks like ImageNet classification. Their goal is to show NAT can match or exceed the performance of other unsupervised and self-supervised approaches while being simpler and faster to train.


## What is the main contribution of this paper?

The main contribution of this paper is presenting a new unsupervised learning framework to train deep neural networks without needing any labels or annotation. The key ideas are:- Map the deep features from a convolutional neural network to a set of predefined target representations in low-dimensional space. - Use simple random noise vectors, called "Noise As Targets" (NAT), as the target representations. This avoids making strong assumptions about structure in the data.- Fix the target representations and align the features to them, rather than learning both jointly, to avoid the issue of feature collapse. - Use a separable squared L2 loss and stochastic batch reassignment strategy to enable scaling to large datasets.In summary, the paper proposes an unsupervised learning approach called NAT that is simple, scalable, and produces features competitive with state-of-the-art unsupervised methods on ImageNet and Pascal VOC classification/detection when transferred. The main advantage is being generic and not relying on domain-specific heuristics for self-supervision.
