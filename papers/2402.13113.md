# [When Only Time Will Tell: Interpreting How Transformers Process Local   Ambiguities Through the Lens of Restart-Incrementality](https://arxiv.org/abs/2402.13113)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem Statement
- Autoregressive and causal Transformers used in NLP perform monotonic decoding and have static token representations, meaning they cannot revise their decisions if new context suggests an initial interpretation was wrong. This makes them unsuitable for incremental applications.
- Bidirectional models have mechanisms that rely on full context, also lacking incrementality.
- The paper aims to analyze how restart-incremental (RI) Transformers process ambiguous inputs that may require revisions, shedding light on their internal state updates.

Proposed Solution  
- Apply the RI paradigm to make bidirectional encoders process sentences incrementally, recomputing representations from scratch with each new token.
- Formalize RI models as transition systems creating step-by-step triangular structures not present in causal models. This allows interpreting the dynamics of state updates.
- Propose analysis methods to compare token representations at different timesteps to references like the initial or final states. 
- Use stimuli with known ambiguities (garden paths) as testbed.

Main Contributions
- Empirically show that RI models seem to make early commitments when faced with ambiguities but their internal states get updated with more context, allowing revisions.
- Analysis reveals hidden mechanisms leading to output changes in two scenarios: construction of meaning representations and dependency parsing.
- For meaning, measure change in contextual embeddings. Find that ambiguous prefixes differ from final states until disambiguation.  
- For parsing, track attention distributions with Jensen-Shannon divergence. Distribution shifts predict output revisions.
- Overall, provide interpretability for RI models, relating internal dynamics to revisions, advancing understanding of bidirectional encoders for contextual and syntactic tasks.
