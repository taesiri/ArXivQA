# [When Only Time Will Tell: Interpreting How Transformers Process Local   Ambiguities Through the Lens of Restart-Incrementality](https://arxiv.org/abs/2402.13113)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem Statement
- Autoregressive and causal Transformers used in NLP perform monotonic decoding and have static token representations, meaning they cannot revise their decisions if new context suggests an initial interpretation was wrong. This makes them unsuitable for incremental applications.
- Bidirectional models have mechanisms that rely on full context, also lacking incrementality.
- The paper aims to analyze how restart-incremental (RI) Transformers process ambiguous inputs that may require revisions, shedding light on their internal state updates.

Proposed Solution  
- Apply the RI paradigm to make bidirectional encoders process sentences incrementally, recomputing representations from scratch with each new token.
- Formalize RI models as transition systems creating step-by-step triangular structures not present in causal models. This allows interpreting the dynamics of state updates.
- Propose analysis methods to compare token representations at different timesteps to references like the initial or final states. 
- Use stimuli with known ambiguities (garden paths) as testbed.

Main Contributions
- Empirically show that RI models seem to make early commitments when faced with ambiguities but their internal states get updated with more context, allowing revisions.
- Analysis reveals hidden mechanisms leading to output changes in two scenarios: construction of meaning representations and dependency parsing.
- For meaning, measure change in contextual embeddings. Find that ambiguous prefixes differ from final states until disambiguation.  
- For parsing, track attention distributions with Jensen-Shannon divergence. Distribution shifts predict output revisions.
- Overall, provide interpretability for RI models, relating internal dynamics to revisions, advancing understanding of bidirectional encoders for contextual and syntactic tasks.


## Summarize the paper in one sentence.

 This paper proposes methods to analyze how restart-incremental bidirectional models build and update internal states to shed light on what processes cause output revisions, using stimuli with temporary ambiguities as a testbed.


## What is the main contribution of this paper?

 According to the paper, the main contributions are:

1) A formalisation of restart-incremental models as transition systems that create structured step-by-step constructions not present in causal models. 

2) A proposal of interpretability methods for analyzing the internal mechanisms of restart-incremental models.

3) An analysis of the strategies employed by various models on stimuli containing local ambiguities, for which there is a well-defined motivation for reanalysis. The analysis looks at how states change at key positions and provides insights into the updates that may be allowing restart-incremental models to recover from incorrect initial interpretations.

So in summary, the main contribution is providing interpretability methods to analyze how restart-incremental models process language incrementally, particularly in cases of local ambiguities that require revising initial interpretations. This sheds light on the internal dynamics that allow them to revise their outputs.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and concepts include:

- Restart-incrementality - Processing input sequences incrementally by recomputing model predictions from scratch as new tokens arrive, allowing the model to revise previous outputs.

- Local ambiguities - Input sequences that allow multiple possible interpretations initially before later context makes the correct interpretation clear, like garden path sentences. 

- Interpretability - Methods to analyze the internal mechanisms and dynamics of neural models to understand what drives their predictions and revisions. 

- Transition systems - Formalizing restart-incremental models as systems that construct internal state sequences and update them with new actions/input increments. 

- Triangular structures - Representations of the evolving internal states at each time step arranged in triangle-like structures to compare across time.

- Divergence metrics - Using metrics like cosine distance and Jensen-Shannon divergence to quantify changes in internal representations like attention distributions and contextual embeddings.

- Alignment with output edits - Checking if internal state changes correlate with actual output revisions to see if they encode information about needing to revise.

- Garden path effect - When models initially commit to the wrong analysis before revising once disambiguating information arrives, mirroring human processing.

In summary, the key focus is on interpreting the dynamics of restart-incremental bidirectional models on locally ambiguous sentences using formalized triangular memory structures and divergence metrics.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in this paper:

1. The paper proposes to analyze the internal states of restart-incremental models as they process sentences with local ambiguities. What are the key benefits of using restart-incrementality over standard autoregressive or monotonic decoding? How does it allow models to revise their interpretations?

2. The paper formalizes restart-incremental models as transition systems that create structured, step-by-step representations not present in causal models. Can you explain this formalization and how the triangular structures encode the dynamics of state updates? 

3. The method compares the states of an ambiguous sentence to several reference points, including the initial state, state after recent increment, final state, and state after disambiguation. What is the motivation behind each comparison and what insight does it provide into the model's incremental processing?

4. For analyzing the construction of meaning representations, cosine distance is used to measure change in token embeddings. What are other potential metrics that could be used instead of cosine distance? What are the tradeoffs?

5. When analyzing the dependency parser, the paper uses Jensen-Shannon divergence to measure change in attention distributions. Why is J-S divergence an appropriate metric in this context compared to other divergence measures?

6. The analysis reveals that distributional shifts in the parser align with actual output revisions in arcs and labels. What is the implication of this alignment for the model's incremental processing abilities?

7. The paper studies three types of local ambiguities - how does the analysis reveal commonalities and differences in how models resolve each one incrementally? What linguistic factors might explain these patterns?

8. For causal language models, the paper uncovers a divergence between ambiguous and unambiguous states even though outputs remain unchanged. What does this suggest about the non-incremental nature of these models' internal representations? 

9. The analysis focuses on contextualized language models and graph-based dependency parsers. To what extent could the method be applied to study other types of neural network architectures? What challenges might arise?

10. The paper identifies several limitations to the analysis - which one do you think is most critical to address in future work? How would you suggest extending the method to overcome it?
