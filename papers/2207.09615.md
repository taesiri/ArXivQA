# [Overlooked factors in concept-based explanations: Dataset choice,   concept learnability, and human capability](https://arxiv.org/abs/2207.09615)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question seems to be: What are some commonly overlooked factors in concept-based interpretability methods for computer vision models, and how do they affect the explanations generated? Specifically, the paper examines how the choice of probe dataset, the concepts used in explanations, and the complexity/simplicity of explanations impact concept-based interpretability methods. It analyzes these factors across four popular methods: NetDissect, TCAV, Concept Bottleneck, and IBD. The key hypotheses seem to be:- The choice of probe dataset has a big impact on the explanations generated. Using different probe datasets can lead to very different explanations for the same model.- Many concepts used in explanations are actually harder for models to learn than the classes they are trying to explain. This raises concerns about the correctness of concept-based explanations. - Current concept-based explanations use too many concepts to be interpretable to humans. Studies reveal an upper bound of around 32 concepts for human reasoning.Through empirical analyses and human studies, the paper aims to highlight overlooked factors in concept-based interpretability, provide suggestions for improvement, and catalyze more research into developing and evaluating these methods.


## What is the main contribution of this paper?

 Based on my understanding, the main contribution of this paper is examining and analyzing overlooked factors in concept-based interpretability methods for image classification models. Specifically, the authors investigate the effects of three key factors:1. Choice of probe dataset used to generate explanations. The authors find that using different probe datasets can result in very different explanations for the same model, implying the explanations are heavily dependent on the probe dataset distribution.2. Learnability of concepts used in explanations. Surprisingly, the authors find many concepts are actually harder for a model to learn than the classes they aim to explain. This questions the correctness of explanations using hard-to-learn concepts. 3. Explanation complexity and human capability. Through human studies, the authors find people struggle to identify concepts when given too many, generally preferring under 32 concepts. This suggests existing explanations using hundreds of concepts may not cater to human limitations.In summary, the key contribution is a comprehensive analysis of overlooked factors in concept-based interpretability methods, especially probe dataset choice, concept learnability, and explanation complexity. The findings reveal important considerations for improving concept-based explanations and their usefulness. The authors also release their analysis code and human study UI to assist future research.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence TL;DR summary of the paper:The paper examines overlooked factors in concept-based interpretability methods for image classification models, finding the choice of probe dataset profoundly impacts explanations, concepts used are often harder to learn than target classes, and people struggle with explanations using too many concepts.


## How does this paper compare to other research in the same field?

 Here are a few thoughts on how this paper compares to other research in the field of interpretable machine learning:- The paper focuses specifically on concept-based interpretability methods for computer vision models. This is a relatively new but growing subfield within interpretable ML. The paper provides a comprehensive analysis of key representative methods in this area.- Most prior work has focused on developing new interpretability methods, while comparatively little attention has been paid to rigorously evaluating and analyzing these methods. This paper helps fill that gap by thoroughly probing concept-based methods along several important dimensions: choice of probe dataset, concept learnability, and complexity for human understanding.- The analyses uncover several concerning weaknesses in current concept-based approaches - for example, that explanations are highly specific to the probe dataset used and that many concepts are actually harder for models to learn than the classes they purportedly explain. These findings raise doubts about the faithfulness and correctness of these explanation methods.- The paper makes practical recommendations to address the identified issues, like using more learnable concepts, limiting concept complexity, and choosing probe datasets carefully. The open-sourced code and study designs will aid future development and evaluation.- The study of human understanding of explanations (with 125 participants) provides unique insights. Prior work studying human use of explanations has been limited, with few focusing specifically on concept-based methods before.- The paper adopts a constructive viewpoint - the goal is not to dismiss these methods but to understand their capabilities and limitations in order to improve them. The analyses and suggestions will likely influence future work in this subdomain of interpretable ML.In summary, this paper makes valuable contributions by systematically evaluating an important class of interpretability methods, identifying weaknesses, and providing guidance to address them. The focus on human understanding also distinguishes this work from most prior research.


## What future research directions do the authors suggest?

 Based on my understanding, the main future research directions suggested by the authors are:1. Create more diverse, high-quality probe datasets to provide better testbeds for evaluating concept-based explanation methods. The lack of available probe datasets limits testing and analysis.2. Develop ways to identify causal relationships between concepts and model predictions, rather than just correlations. For example, using generative models to create counterfactual images by adding/removing concepts and observing the impact on predictions.3. Develop concept-based explanation methods that select concepts more carefully, using only concepts that are easier for models to learn than the target classes. This could lead to more correct explanations. 4. Limit the complexity of concept-based explanations to be understandable by humans, ideally using less than 32 concepts based on their study findings.5. Conduct further research into human perceptions, uses and needs regarding concept-based explanations, for example studying differences between users with varying ML expertise.6. Propose additional analyses and tests to thoroughly evaluate concept-based methods before releasing them.In summary, the key directions are: creating better probe datasets, moving beyond correlations to causal explanations, using more learnable concepts, limiting complexity, and conducting more human-centered research and testing. The authors aim to establish concept-based explanations as a useful interpretability technique through these improvements.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the paper:The paper examines overlooked factors in concept-based interpretability methods for image classification models. It analyzes four popular methods - NetDissect, TCAV, Concept Bottleneck, and IBD - and finds that the choice of probe dataset has a big impact on the generated explanations, implying the explanations are only valid for data similar to the probe set. It also finds many concepts used in explanations are actually harder for models to learn than the classes they explain. Finally, human studies reveal people struggle to identify concepts in images when given too many, and prefer explanations with less than 32 concepts. Overall, the paper highlights the need to carefully select probe datasets and concepts when developing concept-based explanations, and to limit complexity so explanations are useful for people.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the paper:The paper investigates overlooked factors in concept-based interpretability methods for image classification models. Concept-based methods explain models by identifying semantic concepts that activate different parts of the model. The authors focus on four main methods: NetDissect, TCAV, Concept Bottleneck, and IBD. They analyze the effect of three key factors on the explanations generated by these methods: choice of probe dataset, learnability of concepts used, and complexity for human understanding. Through extensive experiments, the authors find that changing the probe dataset significantly alters the explanations, so they must be carefully matched to the original data distribution. Many concepts used in explanations are actually harder for models to learn than the classes being explained, contradicting assumptions. Human studies reveal people can only reason with up to 32 concepts, but some methods use over 1000. Based on these findings, the authors suggest improving concept-based methods by using probe datasets matched to training data, easily learnable concepts, and limiting concept complexity. They argue their analyses provide helpful tools for developing and evaluating concept-based interpretability methods.


## Summarize the main method used in the paper in one paragraph.

 Here is a one paragraph summary of the main method used in the paper:The paper investigates concept-based explanations for image classification models. The authors analyze four representative concept-based interpretability methods - NetDissect, TCAV, Concept Bottleneck, and IBD - using two common probe datasets - ADE20k and Pascal. They examine the effects of three key factors on the explanations generated: choice of probe dataset, concepts used in the explanations, and complexity/simplicity of explanations. Through experiments analyzing these factors, they find that the probe dataset profoundly impacts the explanations, many concepts used are actually harder for models to learn than target classes, and most people prefer explanations with less complexity (under 32 concepts). Based on these findings, they make recommendations such as using a probe dataset with a similar distribution to the training set, limiting concepts to more learnable ones, and restricting explanations to a manageable number of concepts. The analyses provide concrete ways to improve concept-based explanations.


## What problem or question is the paper addressing?

 The paper "Overlooked Factors in Concept-Based Explanations: Dataset Choice, Concept Salience, and Human Capability" is addressing several issues with concept-based explanations for machine learning models, specifically for image classification models:1. The choice of probe dataset used to generate the explanations can profoundly impact the explanations produced. The paper shows that using different probe datasets (ADE20k vs Pascal) leads to very different explanations for the same model, even when using the same interpretability method. This implies the explanations are heavily dependent on the probe dataset distribution.2. Many of the concepts used in existing explanations are actually harder for models to learn than the classes they aim to explain. The paper shows that for common probe datasets like Broden and CUB, the median learnability (quantified by normalized AP) of concepts is much lower than that of the target classes. This questions the assumption that concepts should be easier to learn.3. Current explanations use a very large number of concepts, but human studies with 125 participants show much stricter limits on human capability. The majority of participants struggled with 32+ concepts and preferred explanations with fewer than 32 concepts. This reveals a mismatch between explanation complexity and human understanding.Overall, the paper examines overlooked factors like dataset choice, concept salience, and human capability that are crucial for developing useful concept-based explanations, but have not received sufficient attention so far. The findings reveal several ways existing methods can be improved.
