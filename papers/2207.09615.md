# [Overlooked factors in concept-based explanations: Dataset choice,   concept learnability, and human capability](https://arxiv.org/abs/2207.09615)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be: What are some commonly overlooked factors in concept-based interpretability methods for computer vision models, and how do they affect the explanations generated? Specifically, the paper examines how the choice of probe dataset, the concepts used in explanations, and the complexity/simplicity of explanations impact concept-based interpretability methods. It analyzes these factors across four popular methods: NetDissect, TCAV, Concept Bottleneck, and IBD. The key hypotheses seem to be:- The choice of probe dataset has a big impact on the explanations generated. Using different probe datasets can lead to very different explanations for the same model.- Many concepts used in explanations are actually harder for models to learn than the classes they are trying to explain. This raises concerns about the correctness of concept-based explanations. - Current concept-based explanations use too many concepts to be interpretable to humans. Studies reveal an upper bound of around 32 concepts for human reasoning.Through empirical analyses and human studies, the paper aims to highlight overlooked factors in concept-based interpretability, provide suggestions for improvement, and catalyze more research into developing and evaluating these methods.


## What is the main contribution of this paper?

Based on my understanding, the main contribution of this paper is examining and analyzing overlooked factors in concept-based interpretability methods for image classification models. Specifically, the authors investigate the effects of three key factors:1. Choice of probe dataset used to generate explanations. The authors find that using different probe datasets can result in very different explanations for the same model, implying the explanations are heavily dependent on the probe dataset distribution.2. Learnability of concepts used in explanations. Surprisingly, the authors find many concepts are actually harder for a model to learn than the classes they aim to explain. This questions the correctness of explanations using hard-to-learn concepts. 3. Explanation complexity and human capability. Through human studies, the authors find people struggle to identify concepts when given too many, generally preferring under 32 concepts. This suggests existing explanations using hundreds of concepts may not cater to human limitations.In summary, the key contribution is a comprehensive analysis of overlooked factors in concept-based interpretability methods, especially probe dataset choice, concept learnability, and explanation complexity. The findings reveal important considerations for improving concept-based explanations and their usefulness. The authors also release their analysis code and human study UI to assist future research.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence TL;DR summary of the paper:The paper examines overlooked factors in concept-based interpretability methods for image classification models, finding the choice of probe dataset profoundly impacts explanations, concepts used are often harder to learn than target classes, and people struggle with explanations using too many concepts.


## How does this paper compare to other research in the same field?

Here are a few thoughts on how this paper compares to other research in the field of interpretable machine learning:- The paper focuses specifically on concept-based interpretability methods for computer vision models. This is a relatively new but growing subfield within interpretable ML. The paper provides a comprehensive analysis of key representative methods in this area.- Most prior work has focused on developing new interpretability methods, while comparatively little attention has been paid to rigorously evaluating and analyzing these methods. This paper helps fill that gap by thoroughly probing concept-based methods along several important dimensions: choice of probe dataset, concept learnability, and complexity for human understanding.- The analyses uncover several concerning weaknesses in current concept-based approaches - for example, that explanations are highly specific to the probe dataset used and that many concepts are actually harder for models to learn than the classes they purportedly explain. These findings raise doubts about the faithfulness and correctness of these explanation methods.- The paper makes practical recommendations to address the identified issues, like using more learnable concepts, limiting concept complexity, and choosing probe datasets carefully. The open-sourced code and study designs will aid future development and evaluation.- The study of human understanding of explanations (with 125 participants) provides unique insights. Prior work studying human use of explanations has been limited, with few focusing specifically on concept-based methods before.- The paper adopts a constructive viewpoint - the goal is not to dismiss these methods but to understand their capabilities and limitations in order to improve them. The analyses and suggestions will likely influence future work in this subdomain of interpretable ML.In summary, this paper makes valuable contributions by systematically evaluating an important class of interpretability methods, identifying weaknesses, and providing guidance to address them. The focus on human understanding also distinguishes this work from most prior research.


## What future research directions do the authors suggest?

Based on my understanding, the main future research directions suggested by the authors are:1. Create more diverse, high-quality probe datasets to provide better testbeds for evaluating concept-based explanation methods. The lack of available probe datasets limits testing and analysis.2. Develop ways to identify causal relationships between concepts and model predictions, rather than just correlations. For example, using generative models to create counterfactual images by adding/removing concepts and observing the impact on predictions.3. Develop concept-based explanation methods that select concepts more carefully, using only concepts that are easier for models to learn than the target classes. This could lead to more correct explanations. 4. Limit the complexity of concept-based explanations to be understandable by humans, ideally using less than 32 concepts based on their study findings.5. Conduct further research into human perceptions, uses and needs regarding concept-based explanations, for example studying differences between users with varying ML expertise.6. Propose additional analyses and tests to thoroughly evaluate concept-based methods before releasing them.In summary, the key directions are: creating better probe datasets, moving beyond correlations to causal explanations, using more learnable concepts, limiting complexity, and conducting more human-centered research and testing. The authors aim to establish concept-based explanations as a useful interpretability technique through these improvements.
