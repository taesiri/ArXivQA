# [Overlooked factors in concept-based explanations: Dataset choice,   concept learnability, and human capability](https://arxiv.org/abs/2207.09615)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be: What are some commonly overlooked factors in concept-based interpretability methods for computer vision models, and how do they affect the explanations generated? Specifically, the paper examines how the choice of probe dataset, the concepts used in explanations, and the complexity/simplicity of explanations impact concept-based interpretability methods. It analyzes these factors across four popular methods: NetDissect, TCAV, Concept Bottleneck, and IBD. The key hypotheses seem to be:- The choice of probe dataset has a big impact on the explanations generated. Using different probe datasets can lead to very different explanations for the same model.- Many concepts used in explanations are actually harder for models to learn than the classes they are trying to explain. This raises concerns about the correctness of concept-based explanations. - Current concept-based explanations use too many concepts to be interpretable to humans. Studies reveal an upper bound of around 32 concepts for human reasoning.Through empirical analyses and human studies, the paper aims to highlight overlooked factors in concept-based interpretability, provide suggestions for improvement, and catalyze more research into developing and evaluating these methods.


## What is the main contribution of this paper?

Based on my understanding, the main contribution of this paper is examining and analyzing overlooked factors in concept-based interpretability methods for image classification models. Specifically, the authors investigate the effects of three key factors:1. Choice of probe dataset used to generate explanations. The authors find that using different probe datasets can result in very different explanations for the same model, implying the explanations are heavily dependent on the probe dataset distribution.2. Learnability of concepts used in explanations. Surprisingly, the authors find many concepts are actually harder for a model to learn than the classes they aim to explain. This questions the correctness of explanations using hard-to-learn concepts. 3. Explanation complexity and human capability. Through human studies, the authors find people struggle to identify concepts when given too many, generally preferring under 32 concepts. This suggests existing explanations using hundreds of concepts may not cater to human limitations.In summary, the key contribution is a comprehensive analysis of overlooked factors in concept-based interpretability methods, especially probe dataset choice, concept learnability, and explanation complexity. The findings reveal important considerations for improving concept-based explanations and their usefulness. The authors also release their analysis code and human study UI to assist future research.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence TL;DR summary of the paper:The paper examines overlooked factors in concept-based interpretability methods for image classification models, finding the choice of probe dataset profoundly impacts explanations, concepts used are often harder to learn than target classes, and people struggle with explanations using too many concepts.
