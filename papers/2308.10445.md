# [When Prompt-based Incremental Learning Does Not Meet Strong Pretraining](https://arxiv.org/abs/2308.10445)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the main research question is: How can we develop an incremental learning method that does not rely heavily on strong pretraining and can bridge the gap between the pretraining task and unknown future tasks?The key hypotheses/claims seem to be:- Existing prompt-based incremental learning methods rely heavily on strong pretraining (e.g. on ImageNet-21k), which limits their effectiveness when the pretraining task is very different from the incremental learning tasks.- Learning to generate prompts adaptively based on the input, instead of retrieving prompts from a fixed pool, can help reduce the reliance on pretraining and bridge the gap between pretraining and incremental tasks.- Regularizing the prompt generator with a knowledge pool that summarizes class-specific statistics can prevent it from learning ineffective knowledge.- The proposed Adaptive Prompt Generator (APG) with the knowledge pool regularization can enable effective incremental learning without strong pretraining, while still benefiting from pretraining when available.In summary, the main hypothesis is that adaptive prompt generation along with knowledge pool regularization can make prompt-based incremental learning more robust and less reliant on intensive pretraining. The experiments aim to validate this hypothesis.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contributions appear to be:- Developing a learnable adaptive prompt generator (APG) to reduce the negative effects of the gap between pretraining tasks and unknown future tasks in incremental learning. The APG can unify prompt retrieval and learning into a single trainable module.- Proposing a knowledge pool to regularize the APG and prevent it from learning ineffective knowledge. The knowledge pool retains class-specific statistics from previous tasks. - Showing that the proposed method significantly outperforms advanced exemplar-free incremental learning methods without pretraining on CIFAR100 and ImageNet datasets. It also achieves comparable performance to prompt-based methods relying on strong pretraining.- Demonstrating that the method can ease reliance on intensive pretraining for incremental learning. The adaptive prompting scheme appears effective even without strong pretraining, while still benefiting from pretraining if available.- Highlighting an important but overlooked issue in prompt-based incremental learning - the gap between pretraining and future tasks. The work reveals the limitations of existing prompting schemes in this regard.In summary, the key contribution seems to be developing a more flexible and adaptive prompting framework for incremental learning that does not rely heavily on strong pretraining like prior arts. The proposed APG and knowledge pool help bridge the gap between tasks.
