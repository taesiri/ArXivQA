# [Comparing Inferential Strategies of Humans and Large Language Models in   Deductive Reasoning](https://arxiv.org/abs/2402.14856)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
Recent progress in large language models (LLMs) has shown promise in their capability to execute deductive reasoning tasks. However, most studies evaluate LLMs primarily on accuracy metrics, often overlooking analysis of the models' reasoning behavior and process. There is a need for more nuanced evaluation approaches that provide insights into how LLMs reason through deductive tasks.

Methodology: 
This paper examines the reasoning strategies employed by various LLMs - Llama2, Mistral-7B, Zephyr-7B - on 12 propositional logic problems. The authors manually annotate 300 LLM responses to categorize reasoning strategies used and evaluate logical soundness, drawing comparison to strategies identified in human reasoners (e.g. supposition following, chain construction).

Key Findings:
- LLMs exhibit reasoning patterns similar to those seen in humans, including supposition following, chain construction. 
- LLM reasoning strategy depends significantly on model architecture and scale; more advanced models use certain strategies more frequently.
- LLMs can often have correct final answers but unsound reasoning paths, highlighting the need to evaluate process not just accuracy.
- Certain strategies lead to more sound reasoning trajectories than others.
- LLMs occasionally adopt a symbolic strategy not seen in humans, directly employing logical calculus. 

Main Contributions:
- First comprehensive analysis comparing reasoning strategies of LLMs vs humans on deductive tasks 
- Demonstrates LLMs mirror human-like reasoning patterns but also differences
- Highlights limitations of evaluating LLMs on accuracy alone 
- Provides framework for more nuanced analysis of LLM reasoning processes
- Contributes annotated dataset of LLM responses and reasoning chains

The paper advances understanding of reasoning capabilities of LLMs through an assessment anchored in cognitive psychology literature. Findings underscore needs for more sophisticated LLM evaluation paradigms focused on reasoning processes not just outcomes.
