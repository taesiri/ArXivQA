# [Are Large Language Models Table-based Fact-Checkers?](https://arxiv.org/abs/2402.02549)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
Table-based Fact Verification (TFV) aims to identify whether a statement is supported or refuted by a structured table. Existing methods for TFV rely on large amounts of labeled training data and have weak zero-shot ability. The potential of Large Language Models (LLMs) for TFV is still unknown. Thus, the paper explores the following research question - "Are large language models table-based fact-checkers?".

Proposed Solution:
The paper conducts comprehensive experiments to evaluate the capability of LLMs for TFV under zero-shot, few-shot and instruction tuning settings:

1) Zero-shot learning: Prompt engineering to evaluate inherent TFV capability of LLMs. Various prompts like sentences, paragraphs and dialogs are designed. 

2) Few-shot learning: In-context examples added to prompts to assess how LLMs' in-context learning promotes TFV performance.

3) Instruction tuning: TFV instructions constructed to fine-tune LLMs and induce their full potential for TFV. 

Main Contributions:

- Demonstrates acceptable zero-shot and few-shot TFV capability of LLMs with careful prompt engineering, while instruction tuning significantly stimulates the performance.

- Makes valuable findings regarding format of zero-shot prompts, optimal number of examples for few-shot learning.

- Analyzes limitations of current methods and suggests promising directions to further enhance LLMs' accuracy on TFV via handling long inputs, specifying inference procedures and developing table-specific LLMs.

In summary, the paper shows LLMs can be qualified table-based fact checkers, and provides insights on how to further improve their capability.
