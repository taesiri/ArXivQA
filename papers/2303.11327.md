# [3D Concept Learning and Reasoning from Multi-View Images](https://arxiv.org/abs/2303.11327)

## What is the central research question or hypothesis that this paper addresses?

 After reviewing the paper, it seems the central research question is: 

How can we develop methods for 3D visual reasoning that are closer to how humans perform 3D reasoning?

The authors motivate this question by pointing out weaknesses in current 2D visual reasoning datasets and models, which rely on single view images and thus suffer from issues like occlusion. They argue that instead, human 3D reasoning involves actively moving around an environment to gather multiple views and form a holistic 3D representation. 

To investigate this question, the authors propose the new task of 3D visual reasoning from multi-view images captured by an embodied agent actively exploring an environment. They generate a new benchmark dataset for this task called 3DMV-VQA.

The key challenges they aim to study are:

1) How to efficiently obtain a compact 3D representation from the multi-view images that captures crucial properties for reasoning.

2) How to ground semantic concepts in this 3D representation that can support visual reasoning. 

3) How to perform reasoning by inferring relationships between objects and executing multi-step reasoning processes on the 3D representations.

To address these challenges, they propose a 3D-CLR model that combines components like a neural radiance field, pretrained 2D vision-language models, and neural reasoning operators. Experiments on 3DMV-VQA suggest their model outperforms baselines but challenges still remain.

In summary, the central research question is how to develop 3D visual reasoning methods closer to human cognition by having agents actively gather multi-view observations and perform reasoning on learned 3D representations. The 3DMV-VQA benchmark and 3D-CLR model are initial steps in this direction.
