# [NIFF: Alleviating Forgetting in Generalized Few-Shot Object Detection   via Neural Instance Feature Forging](https://arxiv.org/abs/2303.04958)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question is: 

How can we alleviate catastrophic forgetting of base classes in generalized few-shot object detection without requiring access to the base data during finetuning on novel classes, in order to respect privacy restrictions and reduce memory requirements?

The key hypothesis is that a standalone, lightweight generator network can be trained to generate diverse instance-level features for base classes that mimic the statistics of features from a pretrained base model. By replaying these "forged" features during finetuning, knowledge of base classes can be retained without needing to store and reuse the actual base data.

In summary, the paper proposes a data-free knowledge distillation approach called NIFF that uses a feature generator to alleviate catastrophic forgetting in generalized few-shot object detection, with the goals of reducing memory footprint and respecting privacy constraints.


## What is the main contribution of this paper?

 The main contribution of this paper is proposing a data-free knowledge distillation approach for generalized few-shot object detection (G-FSOD). The key ideas are:

- Instead of synthesizing images via model inversion, they design a lightweight standalone generator to forge instance-level features for the base classes. This avoids the need to generate entire images with bounding boxes which is more complex.

- The generator is trained to match the class-wise statistics of the features in the region of interest (RoI) head of a pretrained G-FSOD model on the base classes. This allows transferring knowledge without accessing the actual base data.

- The generator has separate class-specific heads, trained on class-wise statistics, to produce more diverse features. A shared head trained on aggregated statistics performed worse.

- During novel class training, they perform distillation by replaying the forged base class features to the RoI head alongside real novel class features. This prevents catastrophic forgetting of base classes without needing to store or access the base data.

- Careful choices are made in the training pipeline like using constraint finetuning and elastic weight consolidation to further regularize the model.

So in summary, the key contribution is a data-free knowledge distillation approach for G-FSOD that only requires storing a small generator network rather than large amounts of base data. This reduces the memory footprint while improving overall detection performance.
