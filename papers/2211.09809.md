# [SPACE: Speech-driven Portrait Animation with Controllable Expression](https://arxiv.org/abs/2211.09809)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is: How can we develop a method to generate high-quality, controllable, speech-driven portrait animations from a single image?The key goals and hypotheses appear to be:- Speech-driven facial animation from a single image can produce high visual quality results on par with or better than video-driven approaches. - Using both explicit facial landmarks and learned latent keypoints as intermediate representations allows combining controllability with strong image synthesis ability.- A multi-stage approach going from speech to landmarks to latents to image generation enables control over facial expressions, poses, emotions, etc.- Conditioning the intermediate landmark and latent keypoint predictions on emotion labels allows control over emotion type and intensity in the final output.So in summary, the central hypothesis is that a multi-stage approach utilizing both explicit and latent facial representations can achieve high-quality and highly controllable speech-driven facial animation from just a single input image. The experiments aim to validate the visual quality, controllability, and benefits of the proposed approach compared to prior state-of-the-art methods.
