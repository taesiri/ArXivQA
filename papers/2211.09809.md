# [SPACE: Speech-driven Portrait Animation with Controllable Expression](https://arxiv.org/abs/2211.09809)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question is: How can we develop a method to generate high-quality, controllable, speech-driven portrait animations from a single image?

The key goals and hypotheses appear to be:

- Speech-driven facial animation from a single image can produce high visual quality results on par with or better than video-driven approaches. 

- Using both explicit facial landmarks and learned latent keypoints as intermediate representations allows combining controllability with strong image synthesis ability.

- A multi-stage approach going from speech to landmarks to latents to image generation enables control over facial expressions, poses, emotions, etc.

- Conditioning the intermediate landmark and latent keypoint predictions on emotion labels allows control over emotion type and intensity in the final output.

So in summary, the central hypothesis is that a multi-stage approach utilizing both explicit and latent facial representations can achieve high-quality and highly controllable speech-driven facial animation from just a single input image. The experiments aim to validate the visual quality, controllability, and benefits of the proposed approach compared to prior state-of-the-art methods.


## What is the main contribution of this paper?

 The main contribution of this paper is presenting SPACE, a method for high-quality speech-driven portrait animation with controllable expression. The key ideas and contributions are:

- Uses a multi-stage approach utilizing both explicit facial landmarks and latent keypoints from a pretrained face image generator. This allows leveraging the benefits of both - interpretability of landmarks and high quality synthesis from latent keypoints.

- Achieves state-of-the-art image quality and facial motion accuracy compared to prior speech animation methods. Also generates high resolution 512x512 output videos.

- Allows control over head pose by using either provided pose inputs or predicted poses. Facial landmarks allow control over gaze, blinking, etc.

- Introduces emotion conditioning to manipulate emotion types and intensities in the generated videos.

- The interpretable representations allow creative editing and control while leveraging a strong pretrained face generator results in photorealistic animation from just speech and a single photo.

In summary, the main contribution is a high-quality speech-driven facial animation framework with fine-grained control over expressions, motions, and emotions. It combines explicit and latent representations for creative control and photorealism.
