# [An Empirical Study of Scaling Law for OCR](https://arxiv.org/abs/2401.00028)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Research on scaling laws establishing the relationship between model performance, size, data and compute is prevalent in NLP but lacking in the field of Optical Character Recognition (OCR). This casts uncertainty on the potential impact of large-scale models and data in advancing OCR systems.

Methodology:
- The authors systematically investigate the scaling laws in OCR by training various sizes of Transformer-based text recognition models, ranging from 50 million to 1 billion parameters, on datasets spanning 1 million to 1 billion samples.
- They introduce a new large-scale dataset called REBU-Syn combining real images (6M) and synthetic data (18M).
- Multiple model architectures like TrOCR and PARSeq are scaled up in size and trained end-to-end. Extensive experiments analyze model performance correlated to size, data volume and compute quantitatively.

Key Results:
- Smooth power laws are shown to exist between model performance and model size, training data volume and compute resources. Doubling each factor leads to a constant absolute improvement in accuracy.  
- Larger models demonstrate higher sample efficiency compared to smaller counterparts.
- An optimal synthetic-to-real data ratio of 1:3 is crucial, beyond which accuracy declines due to over-skewness.
- Pre-training on text recognition data enhances OCR performance over general image pretraining like ImageNet.

Outcomes:
- Based on the scaling laws, the authors train an improved CLIP4STR-L model on REBU-Syn, achieving new state-of-the-art accuracy of 97.42% on common benchmarks, outperforming prior SOTA by 0.33%.
- The analysis provides guidelines on model design, data composition and training strategies for advancing large-scale OCR systems. The identified smooth scaling laws pave the path forward for future scaled up models.

In summary, the paper presents thorough empirical evidence for scaling laws in OCR and offers valuable insights into effectively leveraging bigger models, massively large-scale data and compute for advancing text recognition performance.
