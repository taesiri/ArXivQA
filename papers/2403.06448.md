# [Unsupervised Real-Time Hallucination Detection based on the Internal   States of Large Language Models](https://arxiv.org/abs/2403.06448)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Hallucinations in large language models (LLMs) refer to factually inaccurate yet coherent responses, which hurts their effectiveness in real-world applications.  
- Existing hallucination detection methods are limited as they are computationally expensive post-processing techniques separate from the LLM's inference.

Proposed Solution - MIND:  
- An unsupervised, real-time framework for hallucination detection leveraging the internal states (contextual embeddings) of LLMs during inference.
- Doesn't require manual annotations and is computationally efficient.
- A simple MLP model built on top of the LLM's internal states to classify hallucinated and non-hallucinated states.

Proposed Benchmark - HELM:
- New benchmark with outputs from 6 diverse LLMs and their internal states during generation.
- Features human annotated hallucination labels at sentence and passage levels.

Main Contributions:
- Proposal of MIND - an efficient unsupervised hallucination detection framework compatible with existing LLMs.
- Introduction of HELM benchmark with multi-LLM data and internal states.
- Experiments showing MIND outperforms existing methods at hallucination detection across models.

In summary, the paper introduces an unsupervised and efficient technique called MIND to leverage LLMs' internal states to detect hallucinations in real-time without needing manual annotations. It also provides the HELM benchmark to facilitate further research in this direction.
