# [On Large Language Models' Selection Bias in Multi-Choice Questions](https://arxiv.org/abs/2309.03882)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading, the key research question this paper addresses is: Do large language models exhibit inherent biases in their ability to answer multiple choice questions, and if so, what are the causes and potential ways to mitigate such biases?Specifically, the authors identify that LLMs tend to be biased towards selecting options at certain positions (like "Option C") in multiple choice questions. They investigate the prevalence, causes, and potential ways to reduce this "selection bias" across a range of models and datasets. The central hypothesis seems to be that selection bias arises primarily due to the numbering/symbols used for answer options (A, B, C, D), and that this bias can be effectively reduced by estimating and adjusting for the models' prior preferences over option positions. The method proposed, Pride, aims to debias LLMs by estimating these inherent positional priors and using them to adjust the models' predictions.In summary, this paper centers on empirically demonstrating, analyzing the causes of, and developing a technique to mitigate LLMs' selection biases in answering multiple choice questions. The key hypothesis is that by accounting for models' prior positional preferences, their robustness and performance on this important task format can be improved.


## What is the main contribution of this paper?

 Based on my reading of the paper, the main contributions appear to be:1. Identifying and providing empirical analysis of the inherent "selection bias" of large language models (LLMs) in multi-choice questions (MCQs). Selection bias refers to LLMs' preference for selecting options located at specific positions (e.g. Option C). The authors show this bias is prevalent across various LLMs and makes their performance vulnerable to option position changes.2. Pinpointing option numbering (the ID symbols like A/B/C/D) as a primary cause of selection bias. The authors find removing option IDs can reduce bias but usually compromises performance. 3. Proposing a new method called "PriDe" (Debiasing with Prior estimation) to mitigate selection bias in an efficient, label-free way at inference time. PriDe decomposes the observed prediction distribution into an intrinsic distribution over option contents and a prior preference distribution over option IDs. It estimates the prior with a small number of samples and uses it to debias subsequent samples.4. Demonstrating PriDe achieves better debiasing effectiveness and efficiency compared to strong baselines, especially in low-cost scenarios. The estimated priors also exhibit reasonable generalization across domains, confirming the inherent presence of selection bias.In summary, the main contribution appears to be the analysis and insights into selection bias in LLMs for MCQs, as well as the proposal of the efficient PriDe method to mitigate this bias during inference. The paper provides valuable empirical findings on an important bias phenomenon in LLMs and introduces a practical debiasing approach.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the paper:The paper proposes a new method called PriDe that estimates and removes the inherent selection bias of large language models towards certain answer choices in multiple choice questions, making their predictions more robust.


## How does this paper compare to other research in the same field?

 Here are a few thoughts on how this paper compares to other research in the field of large language model biases:- The identification and analysis of selection bias in multi-choice question answering is a novel contribution. While biases in LLMs have been studied, the specific phenomenon of preferring certain answer options based on position does not seem to have been extensively explored before. - The empirical analysis across models, datasets, and settings provides a thorough characterization of the selection bias issue. The comparisons across model families, sizes, and fine-tuning strategies strengthen the claim that this is a prevalent issue in LLMs.- The proposed debiasing method PRIde is simple yet effective, requiring no extra training and minimal computation. It is shown to outperform strong baselines. The cross-domain generalization experiments are also an interesting addition demonstrating the inherent nature of the bias. This contrasts some prior work that requires model re-training or modifications for debiasing.- Situating the work among research on evaluating and improving LLM robustness is valuable. The biases identified can clearly affect performance benchmarks and deployment. The insights on effect of prompting strategies are also relevant given interest in prompt engineering.- Compared to some recent studies that focus only on a model like GPT-3, the coverage of multiple models is a relative strength. The proprietary API experiments complement the open-source models.Overall, I think this is a thorough and rigorous study identifying a new LLM bias and proposing an effective mitigation. The comprehensive empirical methodology and solid results advance the understanding of this space. The work clearly makes important contributions to the growing literature on evaluating and enhancing LLM capabilities.


## What future research directions do the authors suggest?

 Here are some of the key future research directions suggested by the authors:- Exploring what other factors (beyond option numbering) could contribute to selection bias in LLMs, and how they interact with each other. The paper shows that option ordering may have some impact but it seems irregular. There could be other inherent biases that need investigation.- Studying the relationship between selection bias and model calibration. The paper observes a discrepancy between the estimated prior probabilities and empirical selection bias for some models, and hypothesizes it may relate to model calibration. Further research can examine this connection. - Evaluating the generalization capability of priors estimated by PriDe more extensively, especially when there are large domain gaps between the source and target samples. The paper shows promising generalization results but notes performance drops on big domain shifts.- Testing PriDe in broader application scenarios beyond MCQs, such as other structured prediction tasks with position-related biases. The paper focuses on MCQs but the core ideas could potentially apply more widely.- Exploring semi-supervised or few-shot methods to estimate priors, reducing the need for permutation-based debiasing on unlabeled samples. The current PriDe requires permutations on some samples.- Developing more advanced models to disentangle and mitigate different types of biases in LLMs in an integrated framework. PriDe tackles one specific bias, but a more holistic approach could be useful.In summary, the key directions are to further understand the nature and causes of selection bias, improve the debiasing methods like PriDe, and ultimately build more robust and trustworthy LLMs. The insights from this paper lay the groundwork for those important next steps.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the paper:This paper studies the inherent selection bias of large language models (LLMs) in multi-choice questions (MCQs). It refers to LLMs' preferences to select options located at specific positions, making them vulnerable to option position changes. The authors show selection bias is prevalent across various LLMs and identify option numbering as a primary cause. To mitigate selection bias, they propose a new method called PriDe which first decomposes the model prediction into an intrinsic prediction over option contents and a prior preference over option IDs. It estimates the prior with a few test samples, then uses it to debias subsequent samples. Experiments demonstrate PriDe achieves superior debiasing efficiency and effectiveness compared to baselines, especially in low-cost scenarios. The estimated priors also exhibit reasonable cross-domain generalization, confirming the inherent presence of selection bias in LLMs. Overall, this work provides valuable analyses and insights into selection bias of LLMs in MCQs and proposes an effective, efficient debiasing approach.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the paper:This paper studies the inherent selection bias of large language models (LLMs) in multi-choice questions (MCQs). The authors show that LLMs exhibit preferences for selecting options located at specific positions (e.g. Option C), making them vulnerable to changes in option order. Through extensive experiments, they find selection bias is prevalent across various LLMs and identify option numbering (e.g. A, B, C, D symbols) as a primary cause. To mitigate selection bias, the authors propose a new method called PriDe that first estimates the model's prior preference over option IDs using a small number of samples. It then uses the estimated prior to debias subsequent samples, requiring no labels or training. Experiments show PriDe achieves superior debiasing efficiency and effectiveness compared to baselines, and the estimated priors generalize well across domains. In summary, this paper provides valuable insights into the inherent selection biases of LLMs in MCQs. It proposes an efficient, label-free debiasing technique called PriDe that estimates and removes the model's prior preference over option IDs. Key results show PriDe effectively mitigates selection bias, improves model robustness to option order changes, and enables estimated priors to transfer across domains. The analyses and debiasing method presented help illuminate biases in LLMs and inspire future research on model robustness.


## Summarize the main method used in the paper in one paragraph.

 The paper proposes a new method called PriDe to mitigate selection bias in large language models (LLMs) for multi-choice questions (MCQs). The key idea is to decompose the model's prediction distribution into an intrinsic distribution over option contents and a prior preference distribution over option IDs. The prior preference quantifies the inherent bias towards certain option positions. PriDe first estimates this prior with a small number of samples by permuting the option contents. Then for subsequent samples, it uses the estimated prior to debias the model's predictions, by dividing out the prior preference from the original probabilities.In this way, PriDe efficiently debiases LLMs at test time without requiring training labels. It is shown to outperform baselines in mitigating selection bias, improving robustness to option position changes, while also boosting model performance. The estimated priors generalize reasonably across different domains, confirming the inherent presence of selection bias. Overall, PriDe provides an effective and practical solution to address the prevalent issue of selection bias in LLMs for MCQ tasks.


## What problem or question is the paper addressing?

 Based on my reading of the paper, the main problem the authors are addressing is the inherent selection bias exhibited by large language models (LLMs) in multi-choice questions (MCQs). Specifically, the authors identify that LLMs tend to prefer selecting options located at certain positions (e.g. Option C) rather than solely based on the option content. This makes LLMs vulnerable to changes in the position of options within MCQs.The key research questions addressed in the paper are:- Is selection bias prevalent across different LLMs and MCQ datasets? - What causes selection bias in LLMs when answering MCQs?- Can selection bias be mitigated through debiasing methods to improve the robustness of LLMs?To summarize, the main problem is the selection bias of LLMs in MCQs, which leads to poor robustness when option positions are changed. The authors investigate the prevalence, causes, and potential mitigation of this bias through debiasing techniques.
