# [Mask3D: Pre-training 2D Vision Transformers by Learning Masked 3D Priors](https://arxiv.org/abs/2302.14746)

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper addresses is how to effectively embed 3D priors into 2D image representations to improve performance on image understanding tasks like segmentation and detection. The key hypothesis is that pre-training a 2D vision transformer backbone on a masked 3D reconstruction task using single-view RGB-D data can teach the model useful 3D geometric priors. This pre-trained model can then be fine-tuned on downstream 2D vision tasks to achieve better performance compared to models pre-trained only on 2D image data.In summary, the main research question is: Can a self-supervised pre-training approach that reconstructs 3D structure from partial RGB-D views inject useful 3D geometric priors into a 2D vision transformer backbone to improve performance on 2D visual understanding tasks?


## What is the main contribution of this paper?

The main contributions of this paper are:1. It introduces Mask3D, a self-supervised pre-training approach to embed 3D priors into 2D vision transformer (ViT) backbones for image understanding tasks. 2. It proposes a pre-text reconstruction task that reconstructs the depth map from masked RGB and depth patches from single RGB-D views. This allows embedding 3D priors without needing camera poses or multi-view correspondence.3. It shows that Mask3D pre-training is particularly effective for pre-training ViT architectures, leading to improved performance on downstream tasks like semantic segmentation, instance segmentation, and object detection.4. Experiments demonstrate that Mask3D outperforms previous self-supervised 3D pre-training methods like Pri3D on indoor scene understanding tasks on ScanNet, NYUv2, and even generalizes well to Cityscapes. For example, it achieves +6.5% mIoU improvement over Pri3D on ScanNet semantic segmentation.5. The proposed pre-training approach only requires single-view RGB-D data, avoiding the need for multi-view registration or 3D reconstruction, making it more generally applicable. The self-supervision from masked depth prediction enables learning informative 3D priors in the 2D ViT backbones.In summary, the key contribution is a new way to effectively pre-train ViT models for 2D vision tasks by embedding 3D structural priors from readily available single-view RGB-D data in a self-supervised manner.
