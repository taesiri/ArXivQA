# [Positive-Augmented Contrastive Learning for Image and Video Captioning   Evaluation](https://arxiv.org/abs/2303.12112)

## What is the central research question or hypothesis that this paper addresses?

 The central research question addressed in this paper is how to develop an improved automatic evaluation metric for image and video captioning. Specifically, the paper proposes a new metric called PAC-S (Positive-Augmented Contrastive learning Score) that aims to better correlate with human judgments compared to prior metrics. 

The key ideas behind PAC-S are:

- Leveraging a contrastive learning framework (based on CLIP) to learn multimodal embeddings that can match images and text.

- Finetuning the contrastive model on curated image-caption pairs from cleaned datasets like COCO rather than only using web-scale data. 

- Further regularizing the training by incorporating additional synthesized image and text pairs as extra positives.

- Evaluating similarity in the resulting embedding space to score caption quality.

The main hypothesis is that this combination of using curated data and extra synthetic positives to finetune a powerful contrastive model like CLIP will yield an embedding space better aligned with human judgments of caption quality. The experiments aim to validate whether PAC-S correlates better with human ratings than prior metrics like CIDEr, SPICE, BERT-Score, CLIP-Score etc on various caption evaluation datasets.

In summary, the key research question is whether the proposed positive-augmented contrastive learning approach can produce a superior automatic evaluation metric for image and video captioning. The paper hypothesizes this method will outperform existing metrics in human correlation.


## What is the main contribution of this paper?

 The main contribution of this paper is proposing a new metric called PAC-S (Positive-Augmented Contrastive learning Score) for evaluating image and video captions. The key ideas are:

- PAC-S is trained via a novel positive-augmented contrastive learning approach. In addition to real image-caption pairs from a cleaned dataset, it also utilizes generated images and captions as extra positive examples during training. 

- This allows PAC-S to leverage the benefits of pre-training on web data (via CLIP) as well as cleaned datasets. The generated data acts as additional supervision without the noise of web data.

- Experiments demonstrate PAC-S achieves higher correlation with human judgments compared to previous metrics like CIDEr, SPICE, CLIP-Score, etc on both images and videos.

- PAC-S shows improved sensitivity in detecting object hallucination compared to prior work.

- Analysis of different backbone choices like ViT and ResNet shows the metric is robust. The larger ViT-L/14 model gives the best results across datasets.

In summary, the main contribution is a new learnable evaluation metric PAC-S that outperforms existing reference-based and reference-free metrics through a novel positive-augmented contrastive learning approach. The improved correlation with humans and sensitivity to hallucination are demonstrated through extensive experiments.
