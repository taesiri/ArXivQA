# [Positive-Augmented Contrastive Learning for Image and Video Captioning   Evaluation](https://arxiv.org/abs/2303.12112)

## What is the central research question or hypothesis that this paper addresses?

 The central research question addressed in this paper is how to develop an improved automatic evaluation metric for image and video captioning. Specifically, the paper proposes a new metric called PAC-S (Positive-Augmented Contrastive learning Score) that aims to better correlate with human judgments compared to prior metrics. 

The key ideas behind PAC-S are:

- Leveraging a contrastive learning framework (based on CLIP) to learn multimodal embeddings that can match images and text.

- Finetuning the contrastive model on curated image-caption pairs from cleaned datasets like COCO rather than only using web-scale data. 

- Further regularizing the training by incorporating additional synthesized image and text pairs as extra positives.

- Evaluating similarity in the resulting embedding space to score caption quality.

The main hypothesis is that this combination of using curated data and extra synthetic positives to finetune a powerful contrastive model like CLIP will yield an embedding space better aligned with human judgments of caption quality. The experiments aim to validate whether PAC-S correlates better with human ratings than prior metrics like CIDEr, SPICE, BERT-Score, CLIP-Score etc on various caption evaluation datasets.

In summary, the key research question is whether the proposed positive-augmented contrastive learning approach can produce a superior automatic evaluation metric for image and video captioning. The paper hypothesizes this method will outperform existing metrics in human correlation.
