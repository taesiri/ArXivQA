# [Positive-Augmented Contrastive Learning for Image and Video Captioning   Evaluation](https://arxiv.org/abs/2303.12112)

## What is the central research question or hypothesis that this paper addresses?

 The central research question addressed in this paper is how to develop an improved automatic evaluation metric for image and video captioning. Specifically, the paper proposes a new metric called PAC-S (Positive-Augmented Contrastive learning Score) that aims to better correlate with human judgments compared to prior metrics. 

The key ideas behind PAC-S are:

- Leveraging a contrastive learning framework (based on CLIP) to learn multimodal embeddings that can match images and text.

- Finetuning the contrastive model on curated image-caption pairs from cleaned datasets like COCO rather than only using web-scale data. 

- Further regularizing the training by incorporating additional synthesized image and text pairs as extra positives.

- Evaluating similarity in the resulting embedding space to score caption quality.

The main hypothesis is that this combination of using curated data and extra synthetic positives to finetune a powerful contrastive model like CLIP will yield an embedding space better aligned with human judgments of caption quality. The experiments aim to validate whether PAC-S correlates better with human ratings than prior metrics like CIDEr, SPICE, BERT-Score, CLIP-Score etc on various caption evaluation datasets.

In summary, the key research question is whether the proposed positive-augmented contrastive learning approach can produce a superior automatic evaluation metric for image and video captioning. The paper hypothesizes this method will outperform existing metrics in human correlation.


## What is the main contribution of this paper?

 The main contribution of this paper is proposing a new metric called PAC-S (Positive-Augmented Contrastive learning Score) for evaluating image and video captions. The key ideas are:

- PAC-S is trained via a novel positive-augmented contrastive learning approach. In addition to real image-caption pairs from a cleaned dataset, it also utilizes generated images and captions as extra positive examples during training. 

- This allows PAC-S to leverage the benefits of pre-training on web data (via CLIP) as well as cleaned datasets. The generated data acts as additional supervision without the noise of web data.

- Experiments demonstrate PAC-S achieves higher correlation with human judgments compared to previous metrics like CIDEr, SPICE, CLIP-Score, etc on both images and videos.

- PAC-S shows improved sensitivity in detecting object hallucination compared to prior work.

- Analysis of different backbone choices like ViT and ResNet shows the metric is robust. The larger ViT-L/14 model gives the best results across datasets.

In summary, the main contribution is a new learnable evaluation metric PAC-S that outperforms existing reference-based and reference-free metrics through a novel positive-augmented contrastive learning approach. The improved correlation with humans and sensitivity to hallucination are demonstrated through extensive experiments.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the key points in the paper:

The paper proposes a new metric called PAC-S for evaluating image and video captions, which is trained via a positive-augmented contrastive learning approach using both real and generated image-text data and achieves state-of-the-art correlation with human judgments.


## How does this paper compare to other research in the same field?

 This paper makes several notable contributions to the field of image and video captioning evaluation:

- It proposes a new metric called PAC-S that achieves state-of-the-art correlation with human judgments for evaluating image and video captions. PAC-S outperforms previous reference-based metrics like CIDEr and SPICE as well as reference-free metrics like CLIP-Score on several benchmark datasets.

- The key innovation is a positive-augmented contrastive learning approach that combines real image-caption pairs from cleaned datasets with additional synthetic images and captions generated by modern AI techniques. This allows PAC-S to leverage both large-scale pre-training and high-quality curated data.

- Extensive experiments validate PAC-S on image datasets like Flickr8k, Composite, Pascal-50S and video dataset VATEX-EVAL. It also shows higher sensitivity to object hallucination on the FOIL and ActivityNet-FOIL datasets.

- PAC-S demonstrates superior correlation with human judgments compared to prior works like CLIP-Score, EMScore, CIDEr, SPICE, etc. System-level evaluation also shows it better correlates with model capabilities.

- The impact of different cross-modal features like CLIP and OpenCLIP visual backbones is analyzed, showing ViT-L/14 features give best results.

Overall, this paper makes important strides in learning evaluation metrics tied to human judgments. The proposed positive-augmented contrastive learning is a novel way to combine web-scale and curated data. The comprehensive benchmarking demonstrates state-of-the-art performance of PAC-S for image and video captioning evaluation. This metric could be widely adopted by future captioning research.


## What future research directions do the authors suggest?

 Based on my reading of the paper, some of the main future research directions suggested by the authors include:

- Testing other cross-modal models as backbone for the proposed metric, beyond CLIP. The authors show some initial experiments with ViT and ResNet backbones, but suggest exploring other powerful multimodal models.

- Evaluating the impact of different datasets for finetuning the metric. In the paper, COCO is used but the authors suggest trying other cleaned datasets or a combination of them.

- Exploring alternative ways to leverage synthetic data, beyond the specific text and image generators used in the paper. Different generative models or modalities could be tested.

- Applying the positive-augmented contrastive learning approach to other vision-language tasks beyond caption evaluation, such as VQA, image retrieval, etc.

- Designing more robust versions of the metric that are less sensitive to biases in the training data. The authors suggest techniques like data augmentation and adversarial learning.

- Developing extensions of the metric for evaluating other multimodal tasks such as video captioning, visual dialog, etc. The authors show initial video captioning experiments but more work is needed.

- Performing a deeper analysis on the correlation between automatic metrics and human ratings, to better understand when and why they align or differ.

So in summary, the main future directions are centered around exploring different cross-modal models, training datasets, ways to leverage synthetic data, task applications, and improving the robustness and understandability of learned evaluation metrics. The authors lay a solid foundation and provide many possibilities for extending this line of research.
