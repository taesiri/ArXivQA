# [Tower: An Open Multilingual Large Language Model for Translation-Related   Tasks](https://arxiv.org/abs/2402.17733)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- General-purpose large language models (LLMs) like GPT-3 show strong capabilities on translation-related tasks, but only closed models are competitive on multiple tasks. Open models still lag behind when specializing on a single task.

Proposed Solution:
- Present a recipe to tailor open LLMs for multiple translation-related tasks:
  1) Continue pretraining of LaMDA on a highly multilingual dataset with both monolingual and parallel data. This creates TowerBase.
  2) Curate a high-quality and diverse dataset, TowerBlocks, to specialize LLMs for translation tasks.
  3) Finetune TowerBase on TowerBlocks to obtain an instruction-following Tower model tailored for translation.

Main Contributions:  
- Release Tower models in 7B and 13B parameters, outperforming open models and competitive with closed models on translation quality and multiple translation-related tasks.
- Show that adding parallel data during continued pretraining improves translation quality.
- Demonstrate the importance of data diversity and quality for effective specialization.
- Provide TowerBlocks dataset, TowerEval benchmark, and model generations to facilitate future research.

The paper bridges the gap between open and closed models for translation-related tasks. The Tower recipe and models significantly advance the state-of-the-art for open systems on these tasks. The resources released will foster further progress in this area.
