# [How Does Generative Retrieval Scale to Millions of Passages?](https://arxiv.org/abs/2305.11841)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be: How effective are generative retrieval techniques when scaled up to large corpora with millions of documents, and which aspects of proposed methods remain important at scale?In particular, the paper aims to:- Evaluate popular generative retrieval techniques proposed in recent work, ablating their components on small datasets first. - Scale up experiments to the full MS MARCO passage retrieval corpus with 8.8 million passages, in order to understand how well current techniques work and what aspects are critical when scaling up corpus size.- Investigate the effect of scaling up model size for generative retrieval on large corpora.The key hypothesis appears to be that while certain techniques like synthetic query generation help on small datasets, different factors will be important for generative retrieval to work well at scale, which the paper aims to uncover through systematic experiments and analysis. The scalability of generative retrieval is the main open problem being explored.


## What is the main contribution of this paper?

Based on my reading, the main contribution of this paper is an empirical study of generative retrieval techniques on corpora of increasing scale, up to 8.8 million passages from the MS MARCO dataset. The key findings are:- Synthetic query generation is critical for generative retrieval effectiveness at scale. Using only synthetic queries as document representations was the most effective approach as corpus size increased. - When accounting for compute costs, simple techniques like naive document IDs often outperformed more sophisticated methods like semantic IDs.- Increasing model size is necessary but not sufficient - effectiveness peaked at 3B parameters and declined again at 11B parameters, which is counterintuitive.In summary, the paper provides a comprehensive empirical evaluation of recent generative retrieval methods on large-scale corpora. It highlights the importance of synthetic queries and compute-efficient techniques, while showing that naively scaling up model size does not consistently improve effectiveness. The findings help clarify the state of generative retrieval and reveal open challenges to make it competitive with dual encoders at scale.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper empirically studies different generative retrieval techniques, finding that synthetic query generation is crucial for effectiveness but scaling these models to millions of passages remains an open challenge.


## How does this paper compare to other research in the same field?

Here are a few key points comparing this paper to other research in generative retrieval:- Scale of Evaluation: This paper evaluates generative retrieval techniques at an unprecedented scale, using the full MS MARCO passage corpus of 8.8 million passages. Most prior work evaluated on much smaller datasets like Natural Questions or TriviaQA. Evaluating at this scale provides unique insights.- Focus on Ablations: The paper systematically ablates various techniques like synthetic queries, document identifiers, and model components proposed in recent generative retrieval papers. This provides clarity on what works at smaller vs. larger scale. Prior work tended to focus on proposing new methods.- Model Scaling: The paper investigates how model scaling impacts effectiveness, testing up to 11B parameters. This analysis is unique and shows diminishing returns to scaling for sequential identifiers. - Retrieval Benchmarks: The paper uses MS MARCO passage ranking as the main benchmark. This is one of the most widely used academic IR benchmarks. Some prior generative retrieval work used proprietary or less common benchmarks.- Comparison to Dual Encoders: The paper directly compares to state-of-the-art dual encoders like GTR showing the remaining gap, especially at scale. Comparisons help contextualize progress.Overall, the large-scale analysis, ablation studies, and model scaling experiments provide unique insights compared to prior work. The paper focuses on analyzing the current state and challenges of generative retrieval through comprehensive experiments rather than proposing new techniques.


## What future research directions do the authors suggest?

The authors suggest several future research directions in the "Future Directions" section:1. They highlight that open problems in generative retrieval remain, such as achieving state-of-the-art results on large corpora and enabling updates to the model with new documents.2. They raise new open questions based on their findings: - How to properly leverage large language models to benefit generative retrieval on large corpora, since simply scaling up model size did not consistently improve results in their experiments. - How to design scaling recipes and derive scaling laws to maximize retrieval performance, since default T5 scaling may not be optimal.- How to design architectures that balance the compute trade-offs between atomic IDs and sequential IDs.3. They suggest investigating the maximum corpus size where generative retrieval could provide state-of-the-art performance as an important question for future work. Their work focused on effectiveness at large scale but did not address when generative becomes non-competitive.4. They recommend continued research into fundamental improvements to the generative retrieval paradigm before the power of large language models can be fully leveraged.In summary, the main future directions are: solving remaining open problems, investigating proper model scaling, balancing compute trade-offs, determining maximum effective corpus size, and improving the core retrieval paradigm.
