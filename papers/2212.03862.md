# [Teaching Matters: Investigating the Role of Supervision in Vision   Transformers](https://arxiv.org/abs/2212.03862)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central research question of this paper is: How do Vision Transformers (ViTs) learn and behave under different methods of supervision?The key aspects of this research question are:- Comparing ViTs trained with different supervision methods: fully supervised, self-supervised (contrastive and reconstructive), etc. - Analyzing ViT behavior and representations across three domains: Attention, Features, and Downstream Tasks- Understanding similarities and differences in how ViTs process information based on supervision- Providing insights into ViT inner workings to guide future ViT model developmentIn particular, the paper aims to characterize how supervision impacts:- The attention patterns learned by ViTs - The local vs global nature of ViT features- Performance on various downstream tasks requiring local or global reasoningSo in summary, the central research question is focused on elucidating the effect of supervision on how ViTs learn to process information and represent visual concepts.


## What is the main contribution of this paper?

The main contributions of this paper are:1. A detailed comparison of Vision Transformers (ViTs) trained with six different methods, including both fully supervised and self-supervised training. 2. A cross-cutting analysis spanning three major domains - Attention, Features, and Downstream Tasks - to provide insights into how ViTs process information, what they learn to represent, and how useful they are for various tasks.3. Discovering and analyzing ViT behaviors that emerge consistently across different training methods, such as Offset Local Attention Heads.4. Demonstrating that ViTs are highly flexible and can learn to process local and global information in different orders depending on their training method.5. Showing that contrastive self-supervised methods can learn features competitive with supervised methods, and can even be superior for some tasks like part-level feature clustering.6. Finding that reconstruction-based self-supervised methods learn representations with non-trivial similarity to contrastive methods, despite very different training objectives.7. Highlighting that there is no single "best" training method or layer for all downstream tasks, with performance varying based on the task and the locality of required features.In summary, this work provides a comprehensive analysis and comparison of ViTs across different training paradigms, offering insights into their inner workings and demonstrating their flexibility to learn diverse processing strategies based on the method of supervision. The analyses aim to guide future ViT architecture designs and applications.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper compares Vision Transformers (ViTs) trained with different supervision methods through analysis of their attention patterns, learned features, and performance on downstream tasks, finding that ViTs exhibit highly flexible and diverse behaviors depending on how they are trained.
