# [OCR is All you need: Importing Multi-Modality into Image-based Defect   Detection System](https://arxiv.org/abs/2403.11536)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Automatic optical inspection (AOI) is vital for defect detection in manufacturing, but faces challenges like limited data, variability across domains, and sensitivity to imaging conditions. 
- Traditional AOI fails to leverage rich mechanism-parameter data that could aid classification.

Proposed Solution:
- Introduce OANet, an external modality-guided data mining framework using optical character recognition (OCR) to extract statistical image features as a second modality. 
- Align OCR-extracted external features with image features from a CNN in a modality-aware model for refined fusion.
- Further refine fused features and introduce gating function to optimize combination and enhance inference.

Main Contributions:
1. Propose single-modality-aware multimodal learning that extracts OCR stats from images to boost AOI performance without needing multi-modality input.
2. Pioneering OCR-driven feature extraction converts printed image data into a format suitable for training single-modality-aware models. 
3. Advanced feature alignment before fusion ensures harmonious blending of distinct modality features. Accompanying gating function and refinement boost robustness, especially when OCR data is compromised.

Experimental evaluations on ASE Corp data demonstrate markedly improved recall rates for defect detection and high robustness against anomalies in either modality. The proposed innovations significantly advance AOI and multimodal defect detection capabilities.
