# [BiBERT: Accurate Fully Binarized BERT](https://arxiv.org/abs/2203.06390v1)

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper addresses is how to enable accurate and efficient fully binarized BERT models. Specifically, the paper identifies two key challenges with fully binarizing BERT:1. Information degradation in the attention mechanism during forward propagation, caused by binarizing the attention weights. This harms the model's ability to focus on important parts of the input.2. Optimization direction mismatch during backward propagation, caused by distilling binarized activations. This makes training less accurate and effective.To address these challenges, the paper proposes two main contributions:1. A Bi-Attention mechanism that maximizes the information entropy of binarized attention weights, allowing the model to better focus on crucial input content. 2. A Direction-Matching Distillation (DMD) scheme that selects appropriate activations to distill and uses similarity matrices, enabling more accurate training.The overall goal is to enable high-performance BERT models that use extremely efficient 1-bit parameters and operations throughout. The paper aims to demonstrate for the first time that fully binarized BERT can achieve accuracy close to full-precision models, with major advantages in computation and memory costs.In summary, the central hypothesis is that with specifically designed attention and distillation mechanisms, it is possible to achieve accurate fully binarized BERT models that are highly practical for deployment. The paper sets out to prove this through theoretical analysis and extensive experiments.
