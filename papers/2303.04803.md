# [Open-Vocabulary Panoptic Segmentation with Text-to-Image Diffusion   Models](https://arxiv.org/abs/2303.04803)

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper addresses is:Can large-scale text-to-image diffusion models and text-image discriminative models be leveraged together to perform open-vocabulary panoptic segmentation of any concept in the wild?The key hypothesis is that the internal representation of text-to-image diffusion models is semantically rich and spatially differentiated enough to enable open-vocabulary panoptic segmentation when combined with text-image discriminative models. Specifically, the paper proposes a novel method called ODISE that combines a frozen pre-trained text-to-image diffusion model and a text-image discriminative model like CLIP to perform state-of-the-art open-vocabulary panoptic segmentation. The core ideas are:1) The internal representations of text-to-image diffusion models are highly correlated with visual semantics and spatial layouts, as evidenced by their ability to generate high-quality images from text descriptions. 2) Text-image discriminative models like CLIP are good at classifying images into open-vocabulary textual labels.3) By combining the strengths of both models, the spatial and semantic representations from the diffusion model and the open-vocabulary classification ability of the discriminative model, the proposed ODISE approach can perform panoptic segmentation and recognition of objects and regions in the wild using an open-vocabulary of labels.The paper provides extensive experiments to validate this hypothesis, and shows state-of-the-art results on various open-vocabulary panoptic and semantic segmentation benchmarks, significantly outperforming prior art.
