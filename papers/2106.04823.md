# [Taxonomy of Machine Learning Safety: A Survey and Primer](https://arxiv.org/abs/2106.04823)

## What is the central research question or hypothesis that this paper addresses?

Based on my understanding, this paper appears to do a comprehensive review and establish a taxonomy of machine learning techniques that can help improve the safety and robustness of ML models, particularly for safety-critical applications like autonomous vehicles. The key aspects covered seem to be:1. Identifying the gaps between traditional engineering safety standards/methods (which are designed for code-based systems) and the characteristics of modern ML systems. 2. Reviewing the fundamental limitations and vulnerabilities of ML models from a safety perspective, such as lack of interpretability/transparency, inability to formally verify, susceptibility to adversarial attacks etc.3. Proposing a taxonomy that categorizes existing ML research techniques into different safety strategies - inherently safe design, enhancing robustness, and runtime error detection. 4. Providing a detailed literature review of techniques in each of these safety categories, like methods for model transparency, robust training, uncertainty estimation, adversarial defense etc.5. Highlighting challenges and opportunities in developing safer ML systems that are dependable for deployment in the real world. So in summary, the key research contribution seems to be the proposed taxonomy that connects ML safety needs to relevant techniques, as well as the comprehensive review of the state-of-the-art in ML safety research. The taxonomy and review seem intended to serve as a guide for developing safer ML systems by choosing techniques that cover diverse safety strategies.


## What is the main contribution of this paper?

This paper presents a taxonomy of techniques for improving the safety of machine learning systems when deployed in open-world environments. The key contributions are:1. It identifies gaps between conventional engineering safety standards and the unique challenges of ensuring safety for ML-based systems. The paper points out several limitations of standard software safety methods when applied to ML components, such as the lack of specifications, non-determinism, lack of transparency, and difficulty of verification.2. It proposes a taxonomy that organizes ML techniques into three high-level safety strategies: (i) inherently safe design, (ii) enhancing performance and robustness, and (iii) run-time error detection. The taxonomy maps engineering safety principles to relevant ML solutions. 3. It provides a comprehensive literature review of techniques in each of the three safety strategies. For inherently safe design, it reviews work on model transparency, formal specification, and formal verification/testing. For performance and robustness, it covers robust architectures, training regularization, adversarial training, domain generalization, and data sampling/augmentation. For run-time error detection, it surveys model calibration, uncertainty estimation, out-of-distribution detection, and adversarial detection/guarding.4. The taxonomy and literature review allow assessing the coverage and diversity of safety strategies employed in a given ML system. It can serve as a checklist for ML safety and lay out a roadmap to identify future research directions.In summary, the key contribution is providing a structured framework to connect engineering safety practices with ML safety research. The proposed taxonomy and comprehensive literature review aim to accelerate progress in building safe ML systems for deployment in the open world.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence TL;DR of the paper:The paper proposes a taxonomy of machine learning techniques to improve the safety and dependability of AI systems by mapping engineering safety strategies to relevant ML solutions across model transparency, robust training, uncertainty estimation, and adversarial defenses.In summary, the paper reviews the gap between conventional software engineering safety standards and ML systems, then develops a taxonomy that categorizes ML research into complementary safety assurance strategies to achieve dependable ML components for safety-critical applications. The taxonomy serves as a practical checklist to improve coverage of employed safety techniques and identify opportunities for future ML safety research.


## How does this paper compare to other research in the same field?

This paper presents a structured review and taxonomy of machine learning techniques for improving the safety and dependability of ML models in real-world applications. It focuses on categorizing existing ML research into different safety strategies, in order to provide guidance for developing safer ML systems. Here are some key ways this paper compares to other survey papers in ML safety:- Scope: This paper takes a broad look at ML safety, covering strategies for inherently safe design, enhancing robustness, and runtime error detection. Other surveys tend to focus on specific subareas like adversarial robustness or verification. - Structure: The taxonomy provides a clear organization of ML safety techniques into complementary strategies. Other papers tend to list methods without this higher-level categorization.- Comprehensiveness: With over 300 cited papers, this survey aims for reasonable coverage across the field. Related surveys often cover a narrower subset of methods and papers.- Guidance: The taxonomy serves as a practical guide and checklist for improving and diversifying safety practices when developing ML systems. Other surveys are more academically focused without this explicit goal.  - Engineering viewpoint: The paper connects ML safety goals to established engineering safety practices. Most ML surveys do not make this connection.- Assessment: For each safety strategy, the paper reviews challenges and opportunities to assess technology readiness. Other surveys tend to not evaluate the maturity of methods.In summary, this paper provides a more structured, comprehensive, and practical review of ML safety specifically aimed at guiding engineering of safer ML systems, in comparison to related survey papers that focus on specific subareas or lack the engineering orientation. The taxonomy and paper can complement more specialized surveys with its broad scope and explicit connections to engineering safety principles.


## What future research directions do the authors suggest?

Here are some key future research directions suggested in the paper:- Developing new ML techniques for system self-assessment and uncertainty quantification in dynamic environments. The paper suggests incorporating prior knowledge from physics and information theory to enable robust uncertainty estimation. It also proposes using prediction uncertainty to impact downstream decision-making.- Advancing adversarial training methods to be more systematic and cover wider threat models beyond lp norms. The paper suggests semantic adversarial examples and using multiple data modalities could be promising directions.- Improving robustness through multi-domain and multi-source training data for enhanced representation learning. The paper suggests multi-modality training methods that explicitly encourage model robustness could be beneficial.- Developing practical evaluation approaches for model robustness without relying on labeled test sets, such as through selective labeling or using only unlabeled test data.- Constructing safety-aware training sets by extensive data capturing, cleaning, and labeling of unknown edge cases directly from the open-world. The paper suggests active learning, object re-sampling, and self-labeling as useful techniques.- Leveraging GANs to generate effective large-scale training datasets with semantically segmented labels.- Enabling model transparency through developing human-grounded evaluation metrics and benchmarks to reveal model behavior, justify predictions, and help investigate uncertain predictions.- Advancing formal verification methods to scale up to large modern ML models and high-dimensional data. Modular approaches are suggested as a practical solution.- Improving simulation-based testing and validation by bridging the gap between simulation and real-world situations. Using simulations to aid real-world test design is noted as a promising direction.In summary, the key future directions focus on advancing robustness, uncertainty quantification, transparency, formal verification, simulation-based validation, multi-modality training, and safety-aware datasets. The paper provides useful insights into open problems and opportunities in ML safety research.


## Summarize the paper in one paragraph.

Here is a one paragraph summary of the key points from the paper:The paper proposes a taxonomy of machine learning safety techniques to enhance the dependability and safety of machine learning components in autonomous systems. The taxonomy organizes machine learning solutions into three key safety strategies: (1) Inherently safe design, which includes techniques like model transparency and formal verification to achieve provably safe models; (2) Enhancing performance and robustness, which covers techniques like robust architecture, adversarial training, and data augmentation to improve model generalization and robustness; (3) Runtime error detection, which uses uncertainty estimation, out-of-distribution detection, and adversarial detection to selectively reject unreliable predictions at test time. The taxonomy provides a structured way to assess the coverage and diversity of safety techniques employed in a machine learning system. It also reveals open challenges like computational complexity of formal verification, transferring robustness across techniques, and detection of near-distribution outliers. Overall, the taxonomy gives a big picture view of machine learning safety research and lays a roadmap for developing dependable machine learning systems for safety-critical applications.


## Summarize the paper in two paragraphs.

Here is a two paragraph summary of the key points from the paper:The paper proposes a taxonomy for machine learning safety techniques to support the dependability and safety of ML-based systems. The taxonomy categorizes ML techniques into three complementary safety strategies: (1) Inherently safe design, (2) Enhancing performance and robustness, and (3) Run-time error detection. The inherently safe design strategy includes techniques like model transparency, formal specification, and formal verification to make ML models provably safe by design. The performance and robustness enhancement strategy covers techniques like robust architecture design, robust training, and data augmentation to improve model generalization and robustness to shifts. Finally, the run-time error detection strategy uses uncertainty estimation, outlier detection, and adversarial detection to selectively reject uncertain predictions and mitigate risks. The taxonomy provides a systematic organization of 300 papers from ML safety literature. It serves as a helpful guideline to incorporate diverse safety solutions and improve the reliability of ML systems in safety-critical applications.In summary, the proposed ML safety taxonomy offers a structured way to map engineering safety principles into complementary ML techniques for building dependable and safe ML-based systems. It can guide the design choices and development practices to fulfill safety requirements in open-world deployment of ML algorithms.


## Summarize the main method used in the paper in one paragraph.

Here is a one paragraph summary of the key points from the paper:The paper proposes a taxonomy to categorize machine learning techniques for improving safety and dependability of ML systems deployed in the open world. The taxonomy organizes ML solutions into three complementary strategies: (1) Inherently safe design techniques such as formal methods for verification and transparent model explanations; (2) Improving model robustness through robust architecture, training regularization, adversarial training, and data augmentation; and (3) Run-time error detection techniques including uncertainty estimation, out-of-distribution detection, and adversarial attack detection. The taxonomy provides a structured way to assess the coverage of different safety strategies used in a ML system and identifies gaps for future research. The paper reviews over 300 recent papers across these safety strategies to provide a comprehensive survey of the state-of-the-art ML safety techniques. Overall, the taxonomy and literature review aim to connect ML safety research with established engineering safety principles to accelerate dependable ML solutions for safety-critical applications.


## What problem or question is the paper addressing?

Based on the abstract, here is a summary of the key points about the problem and questions addressed in the paper:- The paper focuses on machine learning safety, which is important as ML algorithms are being deployed in safety-critical applications like autonomous vehicles. However, ML algorithms have limitations compared to traditional software that make safety assurance challenging. - There are gaps between engineering safety standards (like ISO 26262 for automotive) and the fundamental limitations of ML algorithms. The paper aims to map relevant ML techniques to key engineering safety principles to enhance ML dependability.- The paper establishes a taxonomy of ML safety techniques categorized into three main strategies: (1) inherently safe design, (2) enhancing robustness and performance, (3) runtime error detection. - A comprehensive review is provided on state-of-the-art ML techniques that map to each of the safety strategies in the proposed taxonomy. - The taxonomy serves as a checklist for ML system designers to assess the coverage and diversity of safety strategies employed in their systems.- Open challenges and future research directions are discussed for each safety strategy, such as scalability of formal verification methods, bridging simulation and real-world gaps, multi-modal training for robustness, efficient outlier detection, etc.In summary, the key goal is to propose a structured taxonomy that connects ML safety techniques to engineering safety principles, in order to guide the design of safer ML systems. The paper provides a literature review organized by this taxonomy along with a discussion of limitations and open problems.
