# [Practical No-box Adversarial Attacks against DNNs](https://arxiv.org/abs/2012.02525)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central research question this paper addresses is: How to perform effective adversarial attacks against deep neural networks (DNNs) in a no-box setting where the attacker has no access to the model architecture, parameters, or training data, and cannot query the model? The key hypothesis is that it is possible to craft adversarial examples that can fool DNN models without having any direct access to the models, by training substitute models on a very small dataset (e.g. just 20 images) and transferring attacks from those models.Specifically, the paper investigates different mechanisms for training discriminative models on tiny datasets, including unsupervised approaches like reconstructing images from rotations/jigsaw puzzles and a supervised approach called prototypical reconstruction. The hypothesis is that adversarial examples crafted on such substitute models will transfer well and fool victim models, despite having no access to them.The paper then empirically evaluates this hypothesis by attacking image classification and face verification models. The results show their approach is effective, with the supervised prototypical reconstruction method performing the best and sometimes even rivaling attacks that use pre-trained models from the same dataset.In summary, the key research question is how to perform no-box attacks by training on tiny datasets. The hypothesis is that effective attacks are possible by using proper training mechanisms and transferability.


## What is the main contribution of this paper?

The main contribution of this paper is proposing practical no-box adversarial attacks against deep neural networks (DNNs). The key ideas are:- It considers a no-box threat model where the attacker can neither access the model architecture/parameters/training data nor query the model. This is a stronger threat model than previous white-box and black-box attacks. - It proposes to train small auto-encoding models on a very limited dataset (e.g. 20 images from 2 classes) and generate adversarial examples on them that can transfer to the victim models.- It introduces three training mechanisms for the substitute auto-encoders: 1) reconstruction from rotated images, 2) reconstruction from jumbled image patches, and 3) reconstruction to prototypical images of each class.- Experiments show the prototypical reconstruction method works the best. The adversarial examples crafted on such auto-encoders transfer well to various image classification and face verification models, and sometimes even match the performance of adversarial examples crafted on models trained on the same large-scale dataset.- On a commercial face recognition API, the attack reduces the accuracy from 100% to 15.4% using only 10 facial images for training, demonstrating the practical viability.In summary, the key contribution is proposing and demonstrating effective no-box adversarial attacks using a very limited auxiliary dataset, which significantly expands the threat model and applicability of adversarial attacks. The proposed training mechanisms to learn discriminative features from minimal data is also an important contribution.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper proposes practical no-box adversarial attacks against DNNs using auto-encoders trained on a small dataset to craft adversarial examples that transfer well to victim models, with prototypical reconstruction being the most effective training mechanism.


## How does this paper compare to other research in the same field?

Here is a brief analysis of how this paper compares to other research in the field of adversarial attacks on deep neural networks:- This paper proposes a new "no-box" threat model for adversarial attacks where the attacker has no access to the victim model's architecture, parameters, training data, or ability to query it. Most prior work has focused on white-box and black-box threat models. Considering no-box attacks significantly expands the applicability of adversarial examples.- The approach relies on training small autoencoder models on limited auxiliary data to craft transferable adversarial examples. This is a novel technique compared to prior work which typically uses large substitute models trained on similar data to the victim model. Using autoencoders helps deal with overfitting on small datasets.- The paper introduces and compares three training mechanisms for the autoencoders - reconstructing from rotation/jigsaw transforms and prototypical reconstruction. Prototypical reconstruction with multiple decoders performs the best. These techniques have not been explored before for crafting adversarial examples.- Experiments demonstrate the no-box adversarial examples transfer well to state-of-the-art image classifiers and face verification models, sometimes matching black-box attacks. Most prior work evaluated transfer only between similar architectures trained on the same dataset.- The work suggests defenses may need to go beyond just obfuscating model information and limiting queries. It proposes adversarial training with augmented no-box examples as a possible defense direction.Overall, this paper makes a novel contribution by proposing and demonstrating effective no-box adversarial attacks. The techniques of training small autoencoders and transferable adversarial crafting have not been explored in this threat model before. The results significantly expand the scope of adversarial vulnerability of DNNs.
