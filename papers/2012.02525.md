# [Practical No-box Adversarial Attacks against DNNs](https://arxiv.org/abs/2012.02525)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central research question this paper addresses is: How to perform effective adversarial attacks against deep neural networks (DNNs) in a no-box setting where the attacker has no access to the model architecture, parameters, or training data, and cannot query the model? The key hypothesis is that it is possible to craft adversarial examples that can fool DNN models without having any direct access to the models, by training substitute models on a very small dataset (e.g. just 20 images) and transferring attacks from those models.Specifically, the paper investigates different mechanisms for training discriminative models on tiny datasets, including unsupervised approaches like reconstructing images from rotations/jigsaw puzzles and a supervised approach called prototypical reconstruction. The hypothesis is that adversarial examples crafted on such substitute models will transfer well and fool victim models, despite having no access to them.The paper then empirically evaluates this hypothesis by attacking image classification and face verification models. The results show their approach is effective, with the supervised prototypical reconstruction method performing the best and sometimes even rivaling attacks that use pre-trained models from the same dataset.In summary, the key research question is how to perform no-box attacks by training on tiny datasets. The hypothesis is that effective attacks are possible by using proper training mechanisms and transferability.


## What is the main contribution of this paper?

The main contribution of this paper is proposing practical no-box adversarial attacks against deep neural networks (DNNs). The key ideas are:- It considers a no-box threat model where the attacker can neither access the model architecture/parameters/training data nor query the model. This is a stronger threat model than previous white-box and black-box attacks. - It proposes to train small auto-encoding models on a very limited dataset (e.g. 20 images from 2 classes) and generate adversarial examples on them that can transfer to the victim models.- It introduces three training mechanisms for the substitute auto-encoders: 1) reconstruction from rotated images, 2) reconstruction from jumbled image patches, and 3) reconstruction to prototypical images of each class.- Experiments show the prototypical reconstruction method works the best. The adversarial examples crafted on such auto-encoders transfer well to various image classification and face verification models, and sometimes even match the performance of adversarial examples crafted on models trained on the same large-scale dataset.- On a commercial face recognition API, the attack reduces the accuracy from 100% to 15.4% using only 10 facial images for training, demonstrating the practical viability.In summary, the key contribution is proposing and demonstrating effective no-box adversarial attacks using a very limited auxiliary dataset, which significantly expands the threat model and applicability of adversarial attacks. The proposed training mechanisms to learn discriminative features from minimal data is also an important contribution.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper proposes practical no-box adversarial attacks against DNNs using auto-encoders trained on a small dataset to craft adversarial examples that transfer well to victim models, with prototypical reconstruction being the most effective training mechanism.


## How does this paper compare to other research in the same field?

Here is a brief analysis of how this paper compares to other research in the field of adversarial attacks on deep neural networks:- This paper proposes a new "no-box" threat model for adversarial attacks where the attacker has no access to the victim model's architecture, parameters, training data, or ability to query it. Most prior work has focused on white-box and black-box threat models. Considering no-box attacks significantly expands the applicability of adversarial examples.- The approach relies on training small autoencoder models on limited auxiliary data to craft transferable adversarial examples. This is a novel technique compared to prior work which typically uses large substitute models trained on similar data to the victim model. Using autoencoders helps deal with overfitting on small datasets.- The paper introduces and compares three training mechanisms for the autoencoders - reconstructing from rotation/jigsaw transforms and prototypical reconstruction. Prototypical reconstruction with multiple decoders performs the best. These techniques have not been explored before for crafting adversarial examples.- Experiments demonstrate the no-box adversarial examples transfer well to state-of-the-art image classifiers and face verification models, sometimes matching black-box attacks. Most prior work evaluated transfer only between similar architectures trained on the same dataset.- The work suggests defenses may need to go beyond just obfuscating model information and limiting queries. It proposes adversarial training with augmented no-box examples as a possible defense direction.Overall, this paper makes a novel contribution by proposing and demonstrating effective no-box adversarial attacks. The techniques of training small autoencoders and transferable adversarial crafting have not been explored in this threat model before. The results significantly expand the scope of adversarial vulnerability of DNNs.


## What future research directions do the authors suggest?

Based on my reading of the paper, some of the key future research directions suggested by the authors include:- Exploring defenses against no-box adversarial attacks. The authors mention that conventional adversarial training does not seem to provide robustness against the no-box attacks proposed in their work. They suggest exploring data augmentation using their no-box adversarial examples as a possible defense. Developing more robust training procedures and architectures would be an important area for future work.- Further analyzing the intrinsic properties of no-box adversarial examples. The authors provide some preliminary analysis in the supplementary material showing differences compared to white-box and black-box adversarial examples. More investigation into the intrinsic properties of no-box adversarial examples could provide insights into their transferability and help inspire new defenses. - Scaling up the training data for no-box attacks. The authors show their approach works reasonably well even with very limited training data (e.g. 10 images). Studying how performance scales with more training data, and developing techniques to maximize transferability with limited data would be interesting.- Exploring the prototypical reconstruction mechanism more. The authors found this supervised mechanism performed the best, and mention the training loss seems not fully converged in their experiments. Further work could involve improving and extending this mechanism, e.g. using more decoders and prototypes.- Applying no-box attacks to additional domains beyond image classification and face verification. Testing the generalizability of the attack mechanisms to other data types and tasks could reveal new insights.- Developing improved unsupervised and self-supervised mechanisms for no-box attacks. The unsupervised methods studied perform reasonably well but not as good as the best supervised approach. Coming up with better unsupervised proxy tasks and representation learning techniques for this setting could be impactful.In summary, the key directions are: analyzing defenses, properties of examples, scaling up training data, improving the prototypical mechanism, applying to new domains, and developing better unsupervised methods. Advancing research in these areas could lead to more robust ML models and a deeper understanding of transferable adversarial attacks.


## Summarize the paper in one paragraph.

Here is a one paragraph summary of the paper:This paper proposes a novel adversarial attack method that can generate transferable adversarial examples without querying the victim model (no-box attack). The key idea is to train small autoencoder models on a tiny dataset (e.g. 20 images) to extract discriminative features, and then craft adversarial examples on these models that can transfer to the victim models. Three training mechanisms are proposed, including reconstructing front view from rotated images, reconstructing original image from jigsaw puzzle pieces, and reconstructing class-specific prototype images. Experiments on ImageNet classification and facial verification tasks show that the prototype-based method performs the best, sometimes even matching attacks using models trained on the same large datasets. On a commercial facial recognition API, the prototype method reduces accuracy from 100% to 15.4% using only 10 training images. The work reveals vulnerabilities of ML models even without model access or membership inference, calling for new defense paradigms.
