# [Practical No-box Adversarial Attacks against DNNs](https://arxiv.org/abs/2012.02525)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central research question this paper addresses is: How to perform effective adversarial attacks against deep neural networks (DNNs) in a no-box setting where the attacker has no access to the model architecture, parameters, or training data, and cannot query the model? The key hypothesis is that it is possible to craft adversarial examples that can fool DNN models without having any direct access to the models, by training substitute models on a very small dataset (e.g. just 20 images) and transferring attacks from those models.Specifically, the paper investigates different mechanisms for training discriminative models on tiny datasets, including unsupervised approaches like reconstructing images from rotations/jigsaw puzzles and a supervised approach called prototypical reconstruction. The hypothesis is that adversarial examples crafted on such substitute models will transfer well and fool victim models, despite having no access to them.The paper then empirically evaluates this hypothesis by attacking image classification and face verification models. The results show their approach is effective, with the supervised prototypical reconstruction method performing the best and sometimes even rivaling attacks that use pre-trained models from the same dataset.In summary, the key research question is how to perform no-box attacks by training on tiny datasets. The hypothesis is that effective attacks are possible by using proper training mechanisms and transferability.
