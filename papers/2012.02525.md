# [Practical No-box Adversarial Attacks against DNNs](https://arxiv.org/abs/2012.02525)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading, the central research question this paper addresses is: How to perform effective adversarial attacks against deep neural networks (DNNs) in a no-box setting where the attacker has no access to the model architecture, parameters, or training data, and cannot query the model? 

The key hypothesis is that it is possible to craft adversarial examples that can fool DNN models without having any direct access to the models, by training substitute models on a very small dataset (e.g. just 20 images) and transferring attacks from those models.

Specifically, the paper investigates different mechanisms for training discriminative models on tiny datasets, including unsupervised approaches like reconstructing images from rotations/jigsaw puzzles and a supervised approach called prototypical reconstruction. The hypothesis is that adversarial examples crafted on such substitute models will transfer well and fool victim models, despite having no access to them.

The paper then empirically evaluates this hypothesis by attacking image classification and face verification models. The results show their approach is effective, with the supervised prototypical reconstruction method performing the best and sometimes even rivaling attacks that use pre-trained models from the same dataset.

In summary, the key research question is how to perform no-box attacks by training on tiny datasets. The hypothesis is that effective attacks are possible by using proper training mechanisms and transferability.


## What is the main contribution of this paper?

 The main contribution of this paper is proposing practical no-box adversarial attacks against deep neural networks (DNNs). The key ideas are:

- It considers a no-box threat model where the attacker can neither access the model architecture/parameters/training data nor query the model. This is a stronger threat model than previous white-box and black-box attacks. 

- It proposes to train small auto-encoding models on a very limited dataset (e.g. 20 images from 2 classes) and generate adversarial examples on them that can transfer to the victim models.

- It introduces three training mechanisms for the substitute auto-encoders: 1) reconstruction from rotated images, 2) reconstruction from jumbled image patches, and 3) reconstruction to prototypical images of each class.

- Experiments show the prototypical reconstruction method works the best. The adversarial examples crafted on such auto-encoders transfer well to various image classification and face verification models, and sometimes even match the performance of adversarial examples crafted on models trained on the same large-scale dataset.

- On a commercial face recognition API, the attack reduces the accuracy from 100% to 15.4% using only 10 facial images for training, demonstrating the practical viability.

In summary, the key contribution is proposing and demonstrating effective no-box adversarial attacks using a very limited auxiliary dataset, which significantly expands the threat model and applicability of adversarial attacks. The proposed training mechanisms to learn discriminative features from minimal data is also an important contribution.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the key points from the paper:

The paper proposes practical no-box adversarial attacks against DNNs using auto-encoders trained on a small dataset to craft adversarial examples that transfer well to victim models, with prototypical reconstruction being the most effective training mechanism.


## How does this paper compare to other research in the same field?

 Here is a brief analysis of how this paper compares to other research in the field of adversarial attacks on deep neural networks:

- This paper proposes a new "no-box" threat model for adversarial attacks where the attacker has no access to the victim model's architecture, parameters, training data, or ability to query it. Most prior work has focused on white-box and black-box threat models. Considering no-box attacks significantly expands the applicability of adversarial examples.

- The approach relies on training small autoencoder models on limited auxiliary data to craft transferable adversarial examples. This is a novel technique compared to prior work which typically uses large substitute models trained on similar data to the victim model. Using autoencoders helps deal with overfitting on small datasets.

- The paper introduces and compares three training mechanisms for the autoencoders - reconstructing from rotation/jigsaw transforms and prototypical reconstruction. Prototypical reconstruction with multiple decoders performs the best. These techniques have not been explored before for crafting adversarial examples.

- Experiments demonstrate the no-box adversarial examples transfer well to state-of-the-art image classifiers and face verification models, sometimes matching black-box attacks. Most prior work evaluated transfer only between similar architectures trained on the same dataset.

- The work suggests defenses may need to go beyond just obfuscating model information and limiting queries. It proposes adversarial training with augmented no-box examples as a possible defense direction.

Overall, this paper makes a novel contribution by proposing and demonstrating effective no-box adversarial attacks. The techniques of training small autoencoders and transferable adversarial crafting have not been explored in this threat model before. The results significantly expand the scope of adversarial vulnerability of DNNs.


## What future research directions do the authors suggest?

 Based on my reading of the paper, some of the key future research directions suggested by the authors include:

- Exploring defenses against no-box adversarial attacks. The authors mention that conventional adversarial training does not seem to provide robustness against the no-box attacks proposed in their work. They suggest exploring data augmentation using their no-box adversarial examples as a possible defense. Developing more robust training procedures and architectures would be an important area for future work.

- Further analyzing the intrinsic properties of no-box adversarial examples. The authors provide some preliminary analysis in the supplementary material showing differences compared to white-box and black-box adversarial examples. More investigation into the intrinsic properties of no-box adversarial examples could provide insights into their transferability and help inspire new defenses. 

- Scaling up the training data for no-box attacks. The authors show their approach works reasonably well even with very limited training data (e.g. 10 images). Studying how performance scales with more training data, and developing techniques to maximize transferability with limited data would be interesting.

- Exploring the prototypical reconstruction mechanism more. The authors found this supervised mechanism performed the best, and mention the training loss seems not fully converged in their experiments. Further work could involve improving and extending this mechanism, e.g. using more decoders and prototypes.

- Applying no-box attacks to additional domains beyond image classification and face verification. Testing the generalizability of the attack mechanisms to other data types and tasks could reveal new insights.

- Developing improved unsupervised and self-supervised mechanisms for no-box attacks. The unsupervised methods studied perform reasonably well but not as good as the best supervised approach. Coming up with better unsupervised proxy tasks and representation learning techniques for this setting could be impactful.

In summary, the key directions are: analyzing defenses, properties of examples, scaling up training data, improving the prototypical mechanism, applying to new domains, and developing better unsupervised methods. Advancing research in these areas could lead to more robust ML models and a deeper understanding of transferable adversarial attacks.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the paper:

This paper proposes a novel adversarial attack method that can generate transferable adversarial examples without querying the victim model (no-box attack). The key idea is to train small autoencoder models on a tiny dataset (e.g. 20 images) to extract discriminative features, and then craft adversarial examples on these models that can transfer to the victim models. Three training mechanisms are proposed, including reconstructing front view from rotated images, reconstructing original image from jigsaw puzzle pieces, and reconstructing class-specific prototype images. Experiments on ImageNet classification and facial verification tasks show that the prototype-based method performs the best, sometimes even matching attacks using models trained on the same large datasets. On a commercial facial recognition API, the prototype method reduces accuracy from 100% to 15.4% using only 10 training images. The work reveals vulnerabilities of ML models even without model access or membership inference, calling for new defense paradigms.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the paper:

This paper proposes practical no-box adversarial attacks against deep neural networks (DNNs). The no-box threat model assumes the attacker has no access to the victim model's architecture, parameters, or training data. The attacker also cannot query the victim model. Instead, the attacker can only gather a small dataset from the same domain as the victim model's training data. 

The authors propose three mechanisms to train substitute models on very limited data (around 20 examples) to generate adversarial examples. The three mechanisms are: 1) estimating the front view of rotated images, 2) estimating the best fit of jigsaw puzzles, and 3) reconstructing class-specific prototypes. Experiments on image classification and face verification tasks show the prototype reconstruction method generates the most transferable adversarial examples. These examples fool various models, including commercial systems, reducing accuracy to as low as 15\%. The method rivals transferability of examples crafted on models trained on the same large-scale datasets. The results demonstrate the feasibility of effective no-box attacks trained on very limited data.


## Summarize the main method used in the paper in one paragraph.

 Here is a one paragraph summary of the main method used in the paper:

The paper proposes a novel approach for mounting adversarial attacks against deep neural networks (DNNs) without accessing the victim model (no-box setting). The key idea is to train small substitute models using a very limited dataset (e.g. 20 images from 2 classes) and then generate adversarial examples on these models that can transfer to the victim models. The paper introduces three mechanisms for training the substitute models to learn discriminative features from minimal data: 1) Reconstruction from chaos by predicting front views of rotated images or best fits of jigsaw puzzles, 2) Supervised prototypical reconstruction by reconstructing class-specific image prototypes. Attacks are performed on the substitute models using interpolated adversarial training, which enlarges intermediate-level perturbations guided by the adversarial loss. Experiments on image classification and face verification tasks demonstrate the effectiveness of the proposed approach in attacking state-of-the-art DNNs, sometimes even rivaling attacks that utilize the same training data as the victim models. The supervised prototypical reconstruction method performs the best by learning more discriminative features given the class labels.


## What problem or question is the paper addressing?

 Based on my reading, this paper is addressing the problem of adversarial attacks against deep neural networks (DNNs) in a "no-box" threat model where the attacker has very limited access. Specifically, the key questions/problems addressed are:

1. How to generate adversarial examples to fool DNN models without having access to the model architecture, parameters, or training data (white-box access) or the ability to query the model (black-box access)? 

2. How to craft adversarial examples using only a small dataset, on the order of tens of examples, rather than requiring large-scale training data or pre-trained models?

3. What techniques allow learning discriminative features and creating transferable adversarial examples given such limited data and without model access?

4. How do the proposed techniques compare to white-box and black-box attacks in terms of effectiveness in attacking real-world DNN models for tasks like image classification and face verification?

5. Can the "no-box" threat model, where the attacker has even less access than in black-box settings, be a practical and effective approach for adversarial attacks?

In summary, this paper introduces the no-box threat model for adversarial attacks and proposes techniques to generate transferable adversarial examples with limited data and without model access. The key innovation is developing substitute models that learn discriminative features from tiny datasets.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and keywords are:

- Adversarial attacks - The paper focuses on generating adversarial examples to fool deep neural networks.

- No-box attacks - A new threat model where the attacker has no access to the model architecture, parameters, or training data and cannot query the model. 

- Transferability - Leveraging the observation that adversarial examples often transfer between different models.

- Autoencoders - Used as substitute models to craft adversarial examples in the no-box setting.

- Unsupervised learning - Mechanisms like predicting rotations and solving jigsaws used to train the autoencoders.  

- Prototypical reconstruction - A supervised mechanism proposed to train the autoencoders by reconstructing class prototypes.

- Image classification - One of the tasks used to evaluate the no-box attacks. Models like VGG, ResNet, Inception tested.

- Face verification - The other main task used for evaluation, with models like FaceNet and CosFace.

- Limited data - The no-box attacks use very small datasets, often just 10-20 images, for training.

So in summary, the key focus is on no-box adversarial attacks that transfer from autoencoders trained on small datasets, evaluated on image classification and face verification.


## Based on the paper, what questions should we ask to create a comprehensive summary of it? Please list at least 10 questions.

 Here are 10 example questions that can be asked to create a comprehensive summary of the paper:

1. What is the research problem being addressed in this paper? What gap does it aim to fill?

2. What is the proposed approach/method in this paper? How does it work?

3. What were the key assumptions made by the authors? 

4. What datasets were used to validate the proposed method? How were they collected and pre-processed?

5. What evaluation metrics were used? What were the main experimental results?

6. How does the performance of the proposed method compare to prior or existing methods?

7. What are the limitations of the proposed method according to the authors?

8. What conclusions do the authors draw from their results? How do they interpret the findings? 

9. What are the broader impacts and implications of this research? How can it be applied?

10. What future work do the authors suggest based on this research? What open questions remain?

Asking these types of questions can help extract the key information from the paper like the problem definition, proposed method, experiments, results, and conclusions. The answers provide the basis for creating a comprehensive yet concise summary of the research described in the paper.


## Design 10 in-depth questions about method proposed in this paper and avoid simple question.

 Here are 10 in-depth questions about the method proposed in this paper:

1. The paper proposes three mechanisms for training substitute models on limited data - reconstruction from rotation, reconstruction from jigsaw, and prototypical reconstruction. Why is prototypical reconstruction the most effective mechanism? Does it learn more discriminative features compared to the other two unsupervised mechanisms?

2. The prototypical reconstruction mechanism involves reconstructing class-specific prototypes. How does enforcing this type of supervision help the model learn more transferable representations, compared to conventional supervised training? 

3. The paper shows that the prototypical models with multiple decoders outperform single decoder models. How do multiple decoders provide richer supervision and help prevent overfitting?

4. The adversarial loss function for autoencoders is designed differently than typical loss functions for classification models. Can you explain the rationale behind the design of the adversarial loss function in Eq. 1? How does it help create more transferable adversarial examples?

5. For face verification, the adversarial loss is modified to use cosine similarity instead of Euclidean distance. Why is this a more suitable loss for face verification models that compare embeddings?

6. The paper uses ILA along with I-FGSM to craft adversarial examples. How does ILA help improve transferability and why is it more effective than using natural images as directional guides?

7. The results show the no-box attack performs well even with very limited training data (e.g. 10 images). Why does the attack succeed with so little data? Does it indicate the substitute models learn some generalizable representations?

8. How do the no-box adversarial examples crafted in this paper differ qualitatively from white-box or black-box adversarial examples? What differences can be observed and why?

9. The attack transfers well to many ImageNet classifiers. Are certain model architectures more vulnerable to this attack? How might model architecture impact transferability?

10. What defenses could be effective against this no-box attack? The paper suggests adversarial training helps but is not sufficient. How can adversarial training be improved to defend against this new threat model?


## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a high-quality paragraph summarizing the key points of the paper:

This paper proposes practical no-box adversarial attacks against deep neural networks (DNNs) where the attacker has no access to the model architecture, parameters, or training data and also cannot query the model. The authors investigate crafting adversarial examples using only a small number of examples (on the order of tens) from the same problem domain as the victim model. They propose training auto-encoders on this limited data using three mechanisms: estimating the front view from rotated images, estimating the best fit jigsaw puzzle configuration, and reconstructing prototypical images. Of these, prototypical image reconstruction proved most effective at learning discriminative features from minimal data. Attacks were performed by maximizing the distance between the reconstruction and prototype of the true class using iterative FGSM and intermediate level attacks. Experiments on ImageNet classification and face verification showed the attack transfers well, diminishing accuracy of victim models to under 20% on ImageNet and fooling a commercial face verification API. The attack rivals transferability of examples crafted using models trained on the full victim model dataset. The work demonstrates the vulnerability of DNNs to practical no-box attacks using very limited data.


## Summarize the paper in one sentence.

 The paper proposes practical no-box adversarial attacks against DNNs, where the attacker crafts adversarial examples by training small auto-encoding models on a few auxiliary examples without querying the victim model.


## Summarize the paper in one paragraphs.

 Here is a one paragraph summary of the paper:

This paper investigates adversarial attacks in a "no-box" threat model where the attacker cannot access the victim model's architecture, parameters, training data, or query the model. Instead, the attacker gathers a small dataset (e.g. 20 images) from the same domain as the victim model's training data. The authors propose training substitute models on this small dataset using autoencoders and three training mechanisms: 1) estimating the front view of rotated images (unsupervised), 2) estimating the best fit of jigsaw puzzles (unsupervised), and 3) reconstructing class-specific prototypical images (supervised). They show that adversarial examples crafted on the substitute models, especially using the supervised prototypical image reconstruction, transfer well to victim image classification and face verification models. On ImageNet classification, their attack achieves over 30% accuracy on average, competitive with attacking using substitute models trained on the full victim model dataset. On a commercial face recognition API, their attack reduces the accuracy to 15.4%. The attack is effective even with very limited training data (e.g. 10 images), highlighting the need for defenses robust to such no-box attacks.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 potential in-depth questions about the method proposed in this paper:

1. The paper proposes using auto-encoders for crafting adversarial examples in a no-box setting. Why are auto-encoders well-suited for this task compared to other model architectures? What advantages do they provide over standard classification networks?

2. The paper introduces three training mechanisms for the auto-encoders - reconstruction from rotation, reconstruction from jigsaw, and prototypical reconstruction. Why is prototypical reconstruction the most effective? What makes it better able to learn discriminative features from limited data? 

3. The adversarial loss function defined in Equation 1 incorporates both a positive and negative prototype. Explain the motivation behind using both prototypes. How does this differ from a standard adversarial loss?

4. The paper finds that using multiple decoders in the prototypical reconstruction model improves performance over a single decoder model. Why might having multiple decoders be beneficial when training on limited data?  

5. For attacking face verification models, the paper modifies the adversarial loss to use cosine similarity rather than Euclidean distance. Explain why this modification is better suited for face verification.

6. The ablation studies vary the number of training images and prototypical decoders. Discuss the key trends observed and how they relate back to the method's design.

7. The paper introduces two "reconstruction from chaos" mechanisms inspired by self-supervised learning. Compare and contrast these to the prototypical reconstruction approach. When might they be preferred?

8. How do the no-box adversarial examples generated by this method differ qualitatively from examples crafted in white-box or black-box settings? What unique properties might they have?

9. The method assumes access to a small labeled dataset from the same distribution as the victim model's training data. How would performance degrade if the auxiliary data came from a different distribution?

10. The paper focuses on attacking image classifiers and face verification models. What challenges might arise in applying this method to other data modalities like audio, video, or text?
