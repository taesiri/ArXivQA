# [Synthesizing Photorealistic Virtual Humans Through Cross-modal   Disentanglement](https://arxiv.org/abs/2209.01320)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the main research goal is to develop an end-to-end framework for synthesizing high-quality virtual human faces capable of speaking with accurate lip motion and good performance for practical applications. More specifically, the key research questions/hypotheses appear to be:

- Can a novel network architecture utilizing 1D audio features like visemes enable efficient and accurate lip sync for talking head generation?

- Can a novel data augmentation strategy help disentangle the correlations between audio and visual modalities to enable end-to-end training? 

- Can a hierarchical image synthesis approach allow high resolution rendering focused on the mouth region for sharper results?

- Can the proposed framework synthesize photorealistic talking heads in real-time while also delivering high visual quality and accurate lip sync?

The authors aim to address these questions through contributions like the viseme-based network design, a data augmentation technique using keypoint mashing and an outpainting generative model, and a two encoder-decoder architecture. The overall goal is developing a fast yet effective end-to-end pipeline for creating realistic virtual human avatars suitable for interactive applications.
