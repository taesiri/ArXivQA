# [Critic-Actor for Average Reward MDPs with Function Approximation: A   Finite-Time Analysis](https://arxiv.org/abs/2402.01371)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Actor-critic methods are effective for reinforcement learning but require a two-timescale update where the actor updates slower than the critic. This mimics policy iteration.
- Recently, a "critic-actor" method was proposed where the timescales are reversed - the critic updates slower than the actor. This was shown to have asymptotic convergence and track value iteration instead of policy iteration. 

- However, prior critic-actor work was only for the lookup table case with discounted infinite horizon cost. This paper aims to develop and analyze the first critic-actor method with function approximation and for the average reward setting.

Method:
- Proposes a two-timescale critic-actor algorithm with linear function approximation for the critic and average reward formulation. 
- Critic updates are on the slower timescale using temporal difference learning. Actor updates are on the faster timescale using stochastic gradient ascent.
- Average reward updates are on the same faster timescale as the actor. 

Main Contributions:

- First critic-actor algorithm for average reward problems with function approximation
- Finite time analysis providing non-asymptotic convergence rates 
- Better sample complexity than prior actor-critic methods: Õ(ε^{-2.08}) vs Õ(ε^−2.5))
- Optimal learning rates derived for actor, critic and average reward
- Experiments on 3 benchmark environments showing critic-actor competitive and better than actor-critic and other baselines on 2 settings

In summary, the paper develops the first critic-actor method for average reward RL with function approximation, proves finite time bounds, derives optimal rates, and shows strong experimental performance versus actor-critic and other baselines.
