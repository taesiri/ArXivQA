# Knowledge Infused Decoding

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be:How can we enhance language model decoding to be more knowledge-aware and generate more accurate, relevant, and factual text for knowledge-intensive natural language generation tasks?The key hypothesis appears to be:By dynamically retrieving relevant knowledge during each decoding step and using it to guide the model's output distribution, language models can generate more knowledgeable, coherent, and factual text without needing extensive retraining or model architecture changes.Specifically, the paper proposes a method called Knowledge Infused Decoding (KID) that does the following during decoding:- Maintains a local memory of concepts mentioned in the current context- Interacts this with a knowledge trie created from retrieved supporting documents - Continuously updates the local memory to guide the model's output distribution via reinforcement learningThe hypothesis is that by reshaping the model's output distribution towards relevant entities in the knowledge trie at each step, KID can enhance language models' ability to produce knowledgeable and factual text for knowledge-intensive NLG tasks. The experiments aim to demonstrate this across several datasets and models.In summary, the central research question is how to make LM decoding more dynamic and knowledge-aware, and the core hypothesis is that KID's approach of interacting a local memory with retrieved knowledge can achieve this without extensive retraining or model changes.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contribution appears to be proposing a novel decoding algorithm called Knowledge Infused Decoding (KID) for generative language models. KID dynamically infuses external knowledge into each step of the language model decoding by maintaining a local knowledge memory and interacting it with an external knowledge trie. Specifically, some key aspects of the contribution are:- KID is designed to be model and task agnostic, so it can be applied to different generative language models and knowledge-intensive natural language generation tasks. - Experiments on six diverse NLG tasks show that task-agnostic LMs like GPT-2 and BART perform much better with KID compared to standard beam search or sampling decoding.- KID also outperforms several other knowledge-infusion techniques that require extra model training or architecture modifications. It shows particularly strong performance in few-shot learning scenarios.- Human evaluations confirm KID generates more relevant, factual, and fluent language compared to baselines.- Analysis shows KID can help alleviate exposure bias and provide stable generation quality for longer sequences.So in summary, the main contribution appears to be proposing the KID algorithm as an effective and generalizable approach to infuse knowledge into language model decoding to improve performance on knowledge-intensive NLG tasks. The model-agnostic nature, gains over other knowledge techniques, and human evaluation results are key aspects of the contribution.
