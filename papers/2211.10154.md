# [CRAFT: Concept Recursive Activation FacTorization for Explainability](https://arxiv.org/abs/2211.10154)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the key research questions and hypotheses addressed in this paper are:- Can concept-based explanations provide more useful and interpretable explanations of model decisions compared to attribution methods? The authors hypothesize that concept-based methods can overcome limitations of attribution maps by explaining "what" visual features a model sees rather "where" it looks.- Can concepts be discovered automatically from a model's activations without requiring additional supervision or labeling? The authors propose their CRAFT method to extract interpretable concepts via factorization of activations.- Can concepts be extracted at the right level of abstraction by recursively applying factorization at different layers? The authors hypothesize that recursive decomposition can overcome issues like concept amalgamation in deeper layers. - Can concept importance be estimated more faithfully compared to prior methods? The authors propose using Sobol indices rather than directional derivatives to quantify concept importance.- Can concept-based attribution maps be generated by backpropagating through the concept factorization? The authors use implicit differentiation to enable concept attribution.The main hypothesis is that CRAFT's novel ingredients (recursion, Sobol indices, implicit diff) will allow more useful concept-based explanations compared to prior attribution and concept methods. The human experiments and ablation studies aim to evaluate these hypotheses.


## What is the main contribution of this paper?

Based on my reading, the main contributions of this paper appear to be:- Proposing a new method called CRAFT for automatically extracting interpretable concepts from deep neural networks. The key ideas behind CRAFT are:1) Using non-negative matrix factorization (NMF) to factorize activations from a network into constituent concepts. 2) Recursively applying NMF at multiple layers to extract hierarchical concepts and sub-concepts at different levels of abstraction.3) Estimating concept importance using Sobol sensitivity indices rather than gradient-based methods.4) Generating concept attribution maps to visualize where concepts appear in images by backpropagating through the NMF using implicit differentiation.- Introducing 3 new techniques to improve on prior concept-based explanation methods: the recursive extraction of concepts, improved concept importance estimation, and concept attribution maps.- Conducting human experiments to evaluate the utility and meaningfulness of concepts extracted by CRAFT. Results suggest CRAFT concepts are more useful than attribution maps and competitive with prior concept methods.- Providing both global and local explanations for model decisions in terms of understandable concepts learned by the network. CRAFT aims to explain both "what" the model sees and "where" in the image.- Demonstrating state-of-the-art performance on a human-centered utility benchmark for explainability methods. The concepts from CRAFT significantly improved users' ability to predict model decisions compared to attribution maps.So in summary, the core contribution appears to be the new CRAFT method for concept-based explanations of neural networks, along with techniques to improve concept extraction and evaluation. The paper shows strong results on human experiments demonstrating the utility of CRAFT concepts.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence TL;DR summary of the paper:The paper proposes a new explainability method called CRAFT that extracts meaningful human-interpretable concepts from deep neural networks and generates both global explanations of the overall concepts used by the model as well as local explanations that highlight where those concepts appear in a given input image.


## How does this paper compare to other research in the same field?

Here are some thoughts on how this paper compares and contrasts with other related research:- The main contribution of this paper is introducing a new method called CRAFT for generating concept-based explanations of neural network decisions. This adds to a growing body of work on concept-based explainability methods, including TCAV and ACE. The key differences are CRAFT's use of non-negative matrix factorization, recursive concept refinement, Sobol indices for concept importance, and concept attribution maps.- Compared to TCAV, CRAFT aims to automatically discover concepts rather than requiring pre-defined human-labeled concept datasets. The recursive refinement and use of Sobol indices are also novel aspects not present in TCAV. However, TCAV's directional derivatives may provide a more localized importance measure.- Relative to ACE, CRAFT uses matrix factorization rather than segmentation and clustering. This avoids some limitations around assigning segments to single clusters. The recursive decomposition and Sobol indices are also new. However, ACE has been evaluated more extensively on human subject experiments.- The human experiments in this paper are similar to recent work like ACE and others aiming to directly test utility for human users. The specific protocol builds on the "meta-predictor" concept introduced by prior work. However, the evaluations remain limited in scale compared to some behavioral testing papers.- The recursive concept refinement process is unique to CRAFT. Other methods extract concepts at a predefined layer. Allowing concepts to be re-decomposed addresses issues like neural collapse. This recursive breakdown seems intuitive but hasn't been explored much before.- Using Sobol indices and implicit differentiation through NMF factors is novel, as most prior work uses gradients or model approximations. But it's unclear if these complex additions provide significant benefits compared to simpler attribution methods.Overall, CRAFT makes several new contributions expanding on prior concept-based explainability work. The paper clearly explains how it differs from and attempts to improve upon similar approaches. More extensive empirical testing would be helpful to better understand its advantages.
