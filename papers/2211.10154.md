# [CRAFT: Concept Recursive Activation FacTorization for Explainability](https://arxiv.org/abs/2211.10154)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the key research questions and hypotheses addressed in this paper are:- Can concept-based explanations provide more useful and interpretable explanations of model decisions compared to attribution methods? The authors hypothesize that concept-based methods can overcome limitations of attribution maps by explaining "what" visual features a model sees rather "where" it looks.- Can concepts be discovered automatically from a model's activations without requiring additional supervision or labeling? The authors propose their CRAFT method to extract interpretable concepts via factorization of activations.- Can concepts be extracted at the right level of abstraction by recursively applying factorization at different layers? The authors hypothesize that recursive decomposition can overcome issues like concept amalgamation in deeper layers. - Can concept importance be estimated more faithfully compared to prior methods? The authors propose using Sobol indices rather than directional derivatives to quantify concept importance.- Can concept-based attribution maps be generated by backpropagating through the concept factorization? The authors use implicit differentiation to enable concept attribution.The main hypothesis is that CRAFT's novel ingredients (recursion, Sobol indices, implicit diff) will allow more useful concept-based explanations compared to prior attribution and concept methods. The human experiments and ablation studies aim to evaluate these hypotheses.


## What is the main contribution of this paper?

Based on my reading, the main contributions of this paper appear to be:- Proposing a new method called CRAFT for automatically extracting interpretable concepts from deep neural networks. The key ideas behind CRAFT are:1) Using non-negative matrix factorization (NMF) to factorize activations from a network into constituent concepts. 2) Recursively applying NMF at multiple layers to extract hierarchical concepts and sub-concepts at different levels of abstraction.3) Estimating concept importance using Sobol sensitivity indices rather than gradient-based methods.4) Generating concept attribution maps to visualize where concepts appear in images by backpropagating through the NMF using implicit differentiation.- Introducing 3 new techniques to improve on prior concept-based explanation methods: the recursive extraction of concepts, improved concept importance estimation, and concept attribution maps.- Conducting human experiments to evaluate the utility and meaningfulness of concepts extracted by CRAFT. Results suggest CRAFT concepts are more useful than attribution maps and competitive with prior concept methods.- Providing both global and local explanations for model decisions in terms of understandable concepts learned by the network. CRAFT aims to explain both "what" the model sees and "where" in the image.- Demonstrating state-of-the-art performance on a human-centered utility benchmark for explainability methods. The concepts from CRAFT significantly improved users' ability to predict model decisions compared to attribution maps.So in summary, the core contribution appears to be the new CRAFT method for concept-based explanations of neural networks, along with techniques to improve concept extraction and evaluation. The paper shows strong results on human experiments demonstrating the utility of CRAFT concepts.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence TL;DR summary of the paper:The paper proposes a new explainability method called CRAFT that extracts meaningful human-interpretable concepts from deep neural networks and generates both global explanations of the overall concepts used by the model as well as local explanations that highlight where those concepts appear in a given input image.


## How does this paper compare to other research in the same field?

Here are some thoughts on how this paper compares and contrasts with other related research:- The main contribution of this paper is introducing a new method called CRAFT for generating concept-based explanations of neural network decisions. This adds to a growing body of work on concept-based explainability methods, including TCAV and ACE. The key differences are CRAFT's use of non-negative matrix factorization, recursive concept refinement, Sobol indices for concept importance, and concept attribution maps.- Compared to TCAV, CRAFT aims to automatically discover concepts rather than requiring pre-defined human-labeled concept datasets. The recursive refinement and use of Sobol indices are also novel aspects not present in TCAV. However, TCAV's directional derivatives may provide a more localized importance measure.- Relative to ACE, CRAFT uses matrix factorization rather than segmentation and clustering. This avoids some limitations around assigning segments to single clusters. The recursive decomposition and Sobol indices are also new. However, ACE has been evaluated more extensively on human subject experiments.- The human experiments in this paper are similar to recent work like ACE and others aiming to directly test utility for human users. The specific protocol builds on the "meta-predictor" concept introduced by prior work. However, the evaluations remain limited in scale compared to some behavioral testing papers.- The recursive concept refinement process is unique to CRAFT. Other methods extract concepts at a predefined layer. Allowing concepts to be re-decomposed addresses issues like neural collapse. This recursive breakdown seems intuitive but hasn't been explored much before.- Using Sobol indices and implicit differentiation through NMF factors is novel, as most prior work uses gradients or model approximations. But it's unclear if these complex additions provide significant benefits compared to simpler attribution methods.Overall, CRAFT makes several new contributions expanding on prior concept-based explainability work. The paper clearly explains how it differs from and attempts to improve upon similar approaches. More extensive empirical testing would be helpful to better understand its advantages.


## What future research directions do the authors suggest?

Based on my reading of the paper, some of the main future research directions the authors suggest are:- Developing better metrics to evaluate and compare different concept-based explanation methods. Since it is difficult to mathematically measure how understandable the explanations are to humans, the authors suggest creating metrics to allow easier comparison between techniques without needing extensive human experiments.- Improving the concept visualization and interpretation. The authors recognize that some concepts can be hard to clearly define and label, so they suggest using methods like image captioning or feature visualization to better illustrate the concepts.- Addressing potential non-uniqueness issues with NMF concept extraction. The authors note that NMF does not have formal guarantees of uniqueness, so they suggest exploring constraints and regularizations to improve stability and obtain consistent solutions. - Evaluating performance on limited data regimes. The authors note that enough samples are needed to extract good concepts, so they suggest studying how the quality of explanations is affected when data is scarce.- Reducing potential confirmation bias. The authors suggest their method could help reduce bias by not using dataset labels, but they note more work is needed to remove it completely.- Comparing to other concept extraction methods. The authors suggest comparing to other techniques like network dissection or ProtoPNet to better understand the trade-offs of different approaches.In summary, the main suggestions are around developing better evaluation procedures, improving concept visualization and interpretation, dealing with stability issues in concept extraction, studying data efficiency, mitigating biases, and comparative benchmarking against other methods. Advancing these areas could help transition concept-based explanations into more widespread practical use.


## Summarize the paper in one paragraph.

Here is a one paragraph summary of the paper:The paper proposes a new explainability method called CRAFT (Concept Recursive Activation FacTorization) for interpreting decisions made by deep neural networks. CRAFT automatically extracts human-interpretable concepts from the activation space of neural networks using non-negative matrix factorization (NMF). It provides explanations at both the global class-level by identifying important concepts driving decisions, and at the local image-level through concept attribution maps that highlight where those concepts appear. Key novel ingredients include: (1) a recursive process to hierarchically decompose concepts into sub-concepts at proper levels of abstraction, (2) quantifying concept importance with sensitivity analysis using Sobol indices, and (3) generating concept attribution maps by backpropagating through the NMF with implicit differentiation. Experiments demonstrate CRAFT's utility over attribution methods on a human-centered benchmark. Additional analysis validates the benefits of the proposed recursivity to refine concepts, the improved faithfulness of the Sobol importance measure, and the interest of localizing concepts. The method helps provide complementary global and local explanations to elucidate both "what" and "where" models look in making decisions.
