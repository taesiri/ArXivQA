# [Label Informed Contrastive Pretraining for Node Importance Estimation on   Knowledge Graphs](https://arxiv.org/abs/2402.17791)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper "Label Informed Contrastive Pretraining for Node Importance Estimation on Knowledge Graphs":

Problem:
The paper focuses on the problem of node importance estimation (NIE) on knowledge graphs. NIE aims to infer the importance scores for nodes in a knowledge graph based on topological structures and node features. Existing NIE methods treat all nodes equally before training. However, in real-world scenarios, nodes with higher importance often require more attention. 

Solution:
To address this issue, the authors propose a novel framework called Label Informed Contrastive Pretraining (LICAP). The key idea is to incorporate the continuous node importance scores into contrastive learning to better separate top important nodes from others during pretraining stage. 

Specifically, LICAP has the following key components:
1) Label informed grouping: Transform the regression problem into a classification-like problem by dividing continuous labels into ordered bins.
2) Top nodes preferred hierarchical sampling: Further divide top nodes into finer bins and sample contrastive pairs hierarchically.
3) Predicate-aware GAT (PreGAT): New GAT architecture that incorporates predicate information in knowledge graphs.  
4) Two-level contrastive losses: Distinguish top nodes from non-top ones (L1), and distinguish top nodes within top bin (L2).

The LICAP pretrained node embeddings have better awareness of top nodes and can be integrated into existing NIE models.

Main Contributions:
1) Propose LICAP framework to inject real-world preference for top nodes into NIE methods via pretraining stage.
2) Develop top nodes preferred hierarchical sampling and design two contrastive losses to fully utilize continuous node importance scores.
3) Introduce PreGAT to exploit topological structures and predicates in knowledge graphs for pretraining.
4) Extensive experiments verify that applying LICAP to NIE methods can achieve new state-of-the-art performance.

In summary, the paper makes significant contributions by pretraining node embeddings with contrastive learning and continuous node importance scores to boost performance for node importance estimation on knowledge graphs.
