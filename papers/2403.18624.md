# [Vulnerability Detection with Code Language Models: How Far Are We?](https://arxiv.org/abs/2403.18624)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Existing vulnerability detection datasets used to train code language models (LMs) have major flaws: poor data quality, low label accuracy, high duplication rates
- Current evaluation metrics like accuracy and F1 score do not properly assess practical effectiveness for real-world vulnerability detection
- Code LMs show overestimated performance on current benchmarks which do not capture complexity of real vulnerabilities 

Proposed Solution - New Benchmark:
- Introduce PrimeVul, a rigorously created benchmark dataset for effectively training and evaluating code LMs for vulnerability detection 
- Implements robust data labeling, filtering, and de-duplication methods to ensure accurate ground truth labels at scale
- Splits data chronologically and uses new evaluation metrics tailored for vulnerability detection challenges

Key Contributions:
- In-depth analysis uncovering limitations of existing vulnerability detection datasets and benchmarks in properly assessing code LM capabilities 
- Creation of PrimeVul dataset with over 200k samples covering 140 weaknesses with stringent accuracy standards rivaling expert verification
- Introduction of new evaluation guidelines including Vulnerability Detection Score (VD-S) metric and pairwise assessment of model's ability to differentiate vulnerabilities
- Extensive experiments on range of code LMs using PrimeVul demonstrate significant gaps between state-of-the-art models and practical utility, underscoring need for innovative approaches

Overall, the paper clearly identifies shortcomings of prevailing vulnerability detection datasets and evaluation practices in capturing real-world complexities. Through PrimeVul and new evaluation guidelines, it makes a compelling case for fundamental advances needed in training and testing methods to unlock the potential of code LMs for enhanced software security.
