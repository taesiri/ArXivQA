# [ImputeFormer: Graph Transformers for Generalizable Spatiotemporal   Imputation](https://arxiv.org/abs/2312.01728)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a high-quality paragraph summarizing the key points of the paper:

This paper proposes a novel deep learning model called ImputeFormer for tackling the problem of missing data imputation in multivariate time series. The key innovation is in exploiting the inherent low-rank structure and redundancy of time series data to enhance the Transformer architecture. Specifically, the authors introduce a projected temporal attention mechanism that compresses information into a lower-dimensional subspace before reconstructing hidden representations, reducing overfitting to spurious correlations. Additionally, a global adaptive graph convolution module is proposed as an efficient alternative to spatial self-attention, inferring correlations between series by adaptively learning a graph structure from node embeddings. Together with a custom Fourier imputation loss function that encourages learned representations to match inherent data properties, the proposed ImputeFormer achieves state-of-the-art performance on various traffic, solar, electricity, and air quality datasets under different missing data patterns. The method also demonstrates favorable efficiency, interpretability and generalizability. By effectively incorporating domain knowledge to guide deep neural modeling, this work represents important progress towards robust and versatile imputation models for real-world spatiotemporal data.
