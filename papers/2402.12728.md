# [Modality-Aware Integration with Large Language Models for   Knowledge-based Visual Question Answering](https://arxiv.org/abs/2402.12728)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem: 
Knowledge-based visual question answering (KVQA) aims to answer questions about images by leveraging external knowledge sources like knowledge graphs. However, existing methods have difficulties properly incorporating and aligning multimodal knowledge sources like images, questions, knowledge graphs, and large language models (LLMs). Simply prompting LLMs may generate unreliable responses. Meanwhile, modality-agnostic fusion of representations lacks necessary cross-modal exchange of semantics between entities.

Proposed Solution:
The paper proposes a novel framework called MAIL for modality-aware integration with LLMs for KVQA. The key ideas are:

(1) Carefully leverage LLMs to construct a scene graph from the image depicting spatial relationships between objects, and a concept graph linking mentioned entities to external knowledge. This provides coupled graphs with abundant cross-modal knowledge.

(2) Design a pseudo-siamese graph medium fusion (PS-GMF) method to enable both insightful intra-modal learning within each graph, and inter-modal interaction between graphs. It uses shared entities as mediums for a tight yet constrained exchange of semantics between modalities.

Main Contributions:
- Formally defines the novel learning paradigm of modality-aware integration with LLMs for knowledge-based VQA
- Proposes effective prompting strategies and coupled graph construction to leverage implicit knowledge in LLMs 
- Develops a tailored PS-GMF method to balance both intra- and inter- modal fusion for multimodal reasoning
- Achieves state-of-the-art performance on OK-VQA and FVQA with significantly fewer parameters and faster inference than existing methods

In summary, the paper presents a new way to comprehensively leverage and align multimodal knowledge sources containing images, text, and LLMs for advancing complex knowledge-based VQA. The modality-aware integration design enables tighter fusion while preserving specificity of modalities.
