# [Causal Inference by String Diagram Surgery](https://arxiv.org/abs/1811.08338)

## What is the central research question or hypothesis that this paper addresses?

 The central research question addressed in this paper is how to use categorical and diagrammatic methods to identify causal effects from observational data. Specifically, the paper develops a technique called "string diagram surgery" to compute interventional distributions and infer causal relationships.

The key ideas are:

- Representing causal structures syntactically as string diagrams in a category. This distinguishes the syntax (string diagrams) from the semantics (probabilistic models).

- Modeling interventions as "surgery" on the string diagrams, implemented categorically as an endofunctor. This severs incoming arrows to a variable, destroying dependencies.

- Introducing "comb factorizations" to isolate parts of an observational distribution. This is a generalization of categorical disintegration. 

- Applying comb factorizations and diagram surgery to compute interventional distributions from observational data in certain cases. This provides a diagrammatic version of existing causal identifiability criteria.

So in summary, the main research contribution is developing the semantic framework and surgical techniques to identify causal effects from data, under certain assumptions, using categorical string diagrams. This provides a new perspective on causal inference.


## What is the main contribution of this paper?

 The main contribution of this paper is introducing a new categorically-oriented framework for causal inference, based on distinguishing between syntax (string diagrams) and semantics (stochastic matrices). The key ideas are:

- Representing causal structures syntactically as string diagrams in a free category generated by a DAG. Bayesian networks are then functors from this free category to stochastic matrices.

- Modeling interventions as "string diagram surgery", implemented as an endofunctor that cuts a variable's connections. This propagates through the interpretation functor to compute interventional distributions. 

- Introducing "comb disintegration", a generalization of disintegration, as a tool for calculating interventional distributions from observational data in certain cases. Comb factorizations isolate different parts of a causal structure.

- Applying these tools to a canonical example of inferring if smoking causes cancer in the presence of a confounder. The comb factorization allows computing the interventional distribution after intervening on smoking.

- Showing these techniques apply more broadly to graphical criteria for identifiability of interventional distributions involving a single intervention.

In summary, the paper develops a new categorical perspective on causal inference, with string diagram surgery and comb disintegration as the key techniques for relating syntax and semantics. This provides both conceptual clarity and practical graphical calculational tools.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the key points from the paper:

The paper develops a categorical framework for causal inference that clearly distinguishes between the syntax of causal structures (string diagrams) and their semantics (stochastic matrices), enabling interventions to be modeled as syntactic surgery on string diagrams which then induces new semantic distributions via interpretative functors.


## How does this paper compare to other research in the same field?

 This paper presents a novel categorical approach to causal inference that leverages string diagrams and functorial semantics. Here are some key ways it compares to other research in this field:

- It builds on prior work establishing connections between Bayesian networks, string diagrams, and functorial semantics (Fong 2012, Jacobs & Zanasi 2016). However, it applies these ideas in a new way to causal reasoning.

- The concept of modeling interventions as "string diagram surgery" via endofunctors is original and provides a clean operational semantics for interventions. This differs from the do-calculus developed by Pearl which focuses more on axiomatic properties.

- The use of "comb disintegration" to isolate parts of a causal structure generalizes earlier work on c-factorization by Tian & Pearl. Framing this in terms of categorical combs and compact closure is novel. 

- The sufficient conditions identified for causal identifiability generalize and make more precise earlier criteria like the backdoor and frontdoor criteria. The conditions are similar to those of Tian & Pearl but work outside of semi-Markovian models.

- The focus on functorial semantics provides a clear separation of syntactic (diagrammatic) and semantic (stochastic matrix) concerns. This differs from much work that mixes the two aspects.

- The approach is developed abstractly for stochastic matrices but seems extensible to quantum and more general probabilistic models. This could connect with emerging work on quantum causal structures.

In summary, this paper leverages categorical semantics in an innovative way to provide formal foundations and new techniques for causal reasoning. The diagrammatic approach and functorial semantics yield a clear methodology relating syntax, semantics, and interventions.


## What future research directions do the authors suggest?

 The authors suggest several directions for future work at the end of the paper:

- Moving beyond single-variable interventions to the general case, where interventions are allowed on multiple variables with arbitrary causal relationships. This involves extending the comb factorization theorem (Theorem 5.2) from 2-combs to arbitrary n-combs. 

- Strengthening Theorem 6.1 from sufficient conditions for causal identifiability to a full characterization. The authors note that the related condition based on confounding paths from Tian & Pearl is both necessary and sufficient, so it will be interesting to formalize the necessity proof within their framework.

- Generalizing the techniques to other probabilistic settings (e.g. infinite discrete/continuous variables) and quantum settings. The latter could provide insights into quantum causal structures, but new techniques may be needed due to the unavailability of copy maps in quantum theory.

- Exploring the extent to which the necessity results hold beyond the semi-Markovian case that is typically studied. 

In summary, the main suggested directions are: extending to multiple interventions, proving necessity results, and generalizing the framework to other probabilistic and quantum settings. The development of full necessity results and application to quantum causal structures seem particularly interesting and challenging.


## Summarize the paper in one paragraph.

 The paper develops a categorical framework for causal inference using string diagrams and functorial semantics. It represents causal structures syntactically as morphisms in a free monoidal category, and their probabilistic interpretations as functors mapping to stochastic matrices. Interventions are modelled categorically as "string diagram surgery", implemented via an endofunctor that severs dependencies. To calculate the effect of interventions from observational data, the paper introduces "comb disintegrations", which decompose a tripartite state into a comb-shaped morphism and a channel. This technique generalizes Pearl's do-calculus and provides simple sufficient conditions for causal identifiability that apply broadly. Overall, the categorical perspective provides a clear distinction between syntax and semantics which mirrors common techniques in causal inference.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the paper:

This paper develops a categorical approach to causal inference, specifically for identifying causal effects from observational data. The key insight is to maintain a clear distinction between the syntax (string diagrams) which captures assumed causal structure, and the semantics (stochastic matrices) which represents observational data/probability distributions. Bayesian networks are then functors mapping syntax to semantics. 

Within this framework, interventions are modeled as an endofunctor performing "string diagram surgery", which severs the dependence of a variable on its causal past. Under certain conditions, notably the absence of "confounding paths", this surgery on the syntax induces a well-defined transformation on the observational distribution, yielding the interventional distribution. The paper shows this semantics can be calculated via "comb disintegration", a factorisation of joint states that isolates confounding variables. Comb disintegration generalizes the back-door and related criteria for identifying effects of single interventions. The techniques are illustrated on a classical example of inferring if smoking causes cancer using an intermediate tar variable.


## Summarize the main method used in the paper in one paragraph.

 Here is a one paragraph summary of the main method used in the paper:

The paper develops a categorical framework for causal inference that clearly distinguishes between syntax (string diagrams) and semantics (stochastic matrices). It represents interventions as "string diagram surgery", implemented as an endofunctor that severs the dependencies of a variable in the string diagram syntax. This yields a new interventional distribution when interpreted via a functor into the semantics. While interventional distributions are not computable from observational data in general, the paper shows they can be calculated in certain cases using "comb disintegration", a string diagrammatic factorization technique that isolates parts of the DAG. This allows computing interventions from observational data when the causal structure satisfies certain graphical criteria, capturing and slightly generalizing existing results. Overall, the main method is the use of diagrammatic semantics and syntax to give graphical sufficient conditions for identifying causal effects.


## What problem or question is the paper addressing?

 The paper is addressing the problem of causal identifiability in probabilistic reasoning. Specifically, it is concerned with the question of when it is possible to compute an interventional distribution from an observational distribution. 

The key problem is that observing correlations between variables does not necessarily imply causation. So the goal is to develop techniques to determine if variables are causally related or just correlated.

The main question the paper investigates is under what conditions it is possible to calculate the effect of an intervention, like forcing a variable to take a certain value, using only observational data. This involves computing an interventional distribution from the observational distribution.

The paper takes a categorical approach to this problem by clearly separating syntax (string diagrams) and semantics (stochastic matrices). It develops the notion of "string diagram surgery" to model interventions and uses a technique called "comb disintegration" to try to compute interventional distributions in certain cases.

So in summary, the main problem is distinguishing causation from correlation and the key question is identifying when interventional distributions can be calculated from observational data using categorical methods and diagram surgery.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper abstract and introduction, some of the key keywords and terms associated with this paper include:

- Causality - The paper focuses on causal inference, identifying causal relationships from observed correlations.

- String diagrams - String diagrams are used to represent morphisms in monoidal categories and play a key syntactic role in the paper's approach. 

- Intervention - The concept of intervention, like in a randomized controlled trial, is important for distinguishing causation from correlation. The paper models intervention via "string diagram surgery".

- Identifiability - A main focus is on causal identifiability, determining when causal effects can be calculated from observational data. 

- Functorial semantics - The paper uses functorial semantics, based on structure-preserving functors between syntax (string diagrams) and semantics (stochastic matrices). 

- Comb factorisation - A technique called "comb factorisation" is introduced, generalizing disintegration, and used to perform calculations for interventions.

- Observational and interventional distributions - Distinguishing between distributions obtained from passive observation vs active intervention is key.

- Bayesian networks - Bayesian networks are a standard tool for probabilistic reasoning that are reformulated categorically in the paper.

-DAGs - Directed acyclic graphs are used to represent causal structures.

So in summary, the key terms revolve around using categorical semantics and string diagrammatic methods for causal inference and identifiability.


## Based on the paper, what questions should we ask to create a comprehensive summary of it? Please list at least 10 questions.

 Here are 10 potential questions to ask to create a comprehensive summary of this paper:

1. What is the main problem addressed in the paper? What is causal identifiability and why is it challenging?

2. What are the key concepts involved in causal identifiability, such as interventions, observational data, causal structures, etc.? How are they defined and related?

3. What is the conceptual distinction made between syntax (string diagrams) and semantics (stochastic matrices) for Bayesian networks? How does this relate to causal identifiability?

4. How are interventions formally modeled as "string diagram surgery"? What does the syncut functor do?

5. What are combs and comb factorizations? How do they generalize disintegration from probability theory? 

6. How is the comb factorization theorem used to compute interventional distributions in the smoking example? What conclusions can be drawn?

7. How does Theorem 6.1 capture and generalize previous sufficient conditions like the back-door and front-door criteria? What are its key assumptions?

8. What are the limitations of the framework presented? For example, does it currently only handle single variable interventions?

9. What directions for future work are identified? How could the techniques be extended to multiple interventions or quantum settings?

10. What is the overall significance of the framework? Does it provide new clarity on causal identifiability or suggest new research directions?


## Design 10 in-depth questions about method proposed in this paper and avoid simple question.

 Here are 10 potential in-depth questions about the method proposed in this paper:

1. The paper proposes representing causal inference problems categorically using string diagrams. How does this syntactic representation compare to more traditional representations like causal graphs? What are some advantages and disadvantages of the string diagram approach?

2. The paper models interventions as "string diagram surgery", implemented as an endofunctor. Why is modeling interventions categorically useful? How does it help with identifying causal effects from observational data?

3. The paper introduces "comb factorisation" as a key technique for computing interventional distributions. How does this generalize traditional techniques like Pearl's do-calculus? What kinds of causal inferences can comb factorisation enable that would not be possible otherwise? 

4. Comb factorisation involves isolating certain variables using combs. What properties must a causal structure satisfy for comb factorisation to be applicable? When might comb factorisation fail to identify a causal effect?

5. The smoking example shows how comb factorisation can identify causal effects in the presence of a confounder. How well does this technique extend to more complex causal structures with multiple confounders? What are some limitations?

6. The paper focuses on causal inference in the category Stoch of stochastic matrices. Could the techniques be applied in other settings like quantum or continuous probability? What complications might arise in those settings? 

7. Theorem 6 gives general sufficient conditions for when single variable interventions are identifiable using comb factorisation. How close is this to providing necessary conditions? What gaps remain to get a full characterisation?

8. How does the syntactic condition of Theorem 6 compare to other graphical criteria like the backdoor and frontdoor criteria? Does it properly generalize those special cases?

9. The paper focuses on inference of interventional distributions. How difficult would it be to adapt the techniques to identify actual causal parameters like causal strengths?

10. The approach is currently limited to single variable interventions. What further insights would be needed to extend comb factorisation to handle multi-variable interventions?
