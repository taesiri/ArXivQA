# [Advancing Translation Preference Modeling with RLHF: A Step Towards   Cost-Effective Solution](https://arxiv.org/abs/2402.11525)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Traditional metrics for evaluating machine translation (MT) quality do not align well with human preferences around translation faithfulness, expressiveness and elegance. 
- Reinforcement learning from human feedback (RLHF) can help align MT models with human preferences, but collecting high-quality human preference data is difficult and expensive, especially for low-resource languages.

Proposed Solution:
- Propose a cost-effective preference learning strategy that avoids the need for explicit human preference data annotation. 
- Leverage the induction bias that "high-quality human translations are better than machine translations" to automatically create a preference dataset by contrasting differences between quality human and machine translations.
- Use aligned high-quality book translation data as source of human translations, since books have professional human translations and more complex linguistic structures.

Main Contributions:
- Demonstrate the feasibility of modeling translation preferences and improving translation quality through RLHF without explicit preference annotation.
- On English-Chinese and multilingual datasets, preference-optimized models outperform supervised baseline as per both automatic metrics and human evaluation.
- Show importance of language model capability for effective preference learning through ablation studies.
- Exhibit cross-lingual transferability of learned translation preferences from one language pair to other languages.

In summary, the key innovation is a method to circumvent the need for expensive human preference annotation by automatically exploiting differences between high-quality human and machine translations. Experiments validate improved translation quality, importance of language model capability, and transferability of learned preferences.
