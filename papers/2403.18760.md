# [MLDT: Multi-Level Decomposition for Complex Long-Horizon Robotic Task   Planning with Open-Source Large Language Model](https://arxiv.org/abs/2403.18760)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Existing robotic task planning methods using open-source large language models (LLMs) struggle with complex, long-horizon tasks involving multiple goals, many objects, and long action sequences. 
- Small-scale open-source LLMs have limited reasoning capacity compared to large closed-source LLMs, making complex task planning challenging.

Proposed Solution:
- The paper proposes a Multi-Level Decomposition Task (MLDT) planning method that breaks down complex goals into subgoals, subgoals into subtasks, and subtasks into action sequences. 
- MLDT conducts goal-level, task-level, and action-level decomposition to simplify the complexity at each stage for the LLM.
- A goal-sensitive corpus generation method is used to create high-quality training data for instruction tuning to enhance planning abilities.
- A new LongTasks dataset with more complex, longer horizon tasks is constructed to evaluate methods.

Main Contributions:
- Proposes the MLDT method that decomposes goals and tasks at multiple levels to address complex, long-horizon robotic task planning for small open-source LLMs.
- Introduces a goal-sensitive corpus generation and instruction tuning approach to improve LLMs' planning skills.
- Constructs a new LongTasks dataset with more complex long-horizon tasks involving more goals, objects and longer action sequences.
- Experiments show MLDT substantially outperforms baselines on four datasets, especially on complex tasks, demonstrating its effectiveness.

In summary, the key novelty is the multi-level decomposition approach combined with instruction tuning to enhance open-source LLMs' capability on complex long-horizon robotic task planning.


## Summarize the paper in one sentence.

 This paper proposes a multi-level decomposition method called MLDT to break down complex long-horizon robotic task planning into goal, task, and action levels to enhance the performance of small-scale open-source large language models, and constructs a more challenging dataset LongTasks to evaluate the method.


## What is the main contribution of this paper?

 Based on my reading of the paper, the main contribution is proposing MLDT, a novel robotic task planning method that decomposes complex long-horizon tasks into goal, task, and action levels to mitigate the challenges faced by open-source large language models (LLMs) with limited reasoning capacities in handling such complex tasks. 

Specifically, the key contributions summarized at the end of the introduction are:

1) Proposing the MLDT method that strategically decomposes tasks at goal, task, and action levels to simplify complex long-horizon tasks and make them more manageable for LLMs with limited capacities.  

2) Enhancing the planning abilities of LLMs through instruction tuning using a novel goal-sensitive corpus generation method based on environmental feedback.

3) Constructing a more challenging dataset called LongTasks with tasks involving more goals, longer action sequences, and more objects to specifically evaluate planning abilities on complex long-horizon tasks.  

4) Demonstrating through experiments that MLDT effectively enhances task planning performance of open-source LLMs, particularly for complex long-horizon tasks.

In summary, the main contribution is proposing the MLDT method to address the limitations of open-source LLMs in handling complex long-horizon robotic task planning.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, the main keywords or key terms associated with this paper are:

- Task planning
- Large language models (LLMs) 
- Multi-level decomposition
- Goal-level decomposition
- Task-level decomposition  
- Action-level decomposition
- Complex long-horizon tasks
- Open-source LLMs
- Instruction tuning
- Goal-sensitive corpus generation
- VirtualHome
- LongTasks dataset

The paper proposes a multi-level decomposition task planning (MLDT) method to address the challenges faced by small-scale open-source LLMs in complex long-horizon robotic task planning. The key ideas are decomposing tasks at goal, task, and action levels to reduce complexity and conducting instruction tuning using a goal-sensitive corpus generation method to enhance planning abilities. Experiments are conducted in the VirtualHome environment using open-source and closed-source LLMs, as well as a LongTasks dataset constructed to evaluate performance on complex long-horizon tasks. The results demonstrate the effectiveness of the proposed MLDT method.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in this paper:

1. The paper proposes a multi-level decomposition method for complex long-horizon robotic task planning. What are the specific levels of decomposition and what does each level aim to achieve? 

2. Goal-level decomposition breaks down the overall goal into subgoals. What methods does the paper use to actually perform this decomposition? How does it convert the overall goal and demonstrations into prompts for the LLM to then generate the subgoals?

3. Task-level decomposition aims to convert subgoals into executable subtasks. The paper adopts an interleaved format that alternates between reasoning traces and actions. Why is this helpful compared to just outputting the full subtask in one shot?

4. The paper argues that fine-tuning is crucial for smaller, open-source LLMs to achieve good performance on this task. Why does relying solely on few-shot in-context learning perform poorly? What specifically does the fine-tuning process entail?

5. The paper introduces a goal-sensitive corpus generation method to construct the training data for fine-tuning. Walk through the details of how this process works leveraging ChatGPT and environmental feedback. What makes this superior to other corpus construction methods?

6. The ablation study shows goal-level decomposition has a bigger impact than task-level. Intuitively task-level seems more directly related to generating the actionable plan. Why might simplifying the goals have an outsized influence?

7. While the method works very well for smaller, open-source LLMs, the paper finds it does not transfer effectively to more powerful closed-source models like GPT-4. Analyze the potential reasons why - what differences exist in the capabilities and behaviors of these model classes?

8. The paper constructs a new LongTasks dataset focusing specifically on more complex, longer horizon planning tasks. Describe the process used to construct this dataset and how it differs from prior datasets in key statistics. 

9. The method is evaluated in simulation and also deployed to real robots. What accommodations need to be made to account for differences between the training environment and real-world deployment?

10. The multi-level decomposition idea draws inspiration from human problem solving processes. Can you think of any other human cognition phenomena that could inspire new techniques to overcome LLM limitations?
