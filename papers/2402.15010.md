# [How Important Is Tokenization in French Medical Masked Language Models?](https://arxiv.org/abs/2402.15010)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key aspects of the paper:

Problem:
- Subword tokenization (splitting words into smaller units) is widely used in NLP models, but optimal strategies for segmentation remain unclear, especially for morphologically-rich languages like French and specialized domains like biomedicine. 
- Common biomedical terms are often inconsistently tokenized in current models which don't leverage morphological structure. This impacts the ability to model terminology properly.

Proposed Solution:
- The authors introduce an original tokenization strategy that integrates linguistic morphemes into statistical tokenization algorithms like BPE and SentencePiece. This produces segmentation that better matches morphological rules.
- They conduct comprehensive analysis to assess the impact of different tokenization strategies on French biomedical language models across 23 downstream tasks. The study evaluates different factors like segmentation granularity, morpheme enrichment, and choice of tokenizer training data.

Main Contributions:
- A novel morpheme-enriched tokenization method that combines statistical tokenizers with linguistic morpheme vocabulary to improve biomedical term segmentation.
- Extensive experiments demonstrating the inconsistent benefits of morpheme-enriched tokenization, suggesting robustness of models to suboptimal segmentation.
- Analysis showing correlation between segmentation granularity and performance for certain tasks, indicating each task/language likely has optimal level of segmentation.  
- Findings that using in-language (French) data for tokenizer training generally outperforms out-of-language (English) biomedical data due to better language/morphological modeling.
- The enriched tokenizers, trained models and reproduction scripts are freely released to the community.
