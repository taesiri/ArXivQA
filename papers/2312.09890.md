# [Grammatical information in BERT sentence embeddings as two-dimensional   arrays](https://arxiv.org/abs/2312.09890)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Sentence embeddings from BERT encode syntactic/semantic information in a distributed manner in a 1D vector array. But it's unclear if specific grammatical information like subject-verb agreement can be easily extracted from these representations. 

- The paper investigates whether raw BERT sentence embeddings can be manipulated to make the detection of subject-verb agreement easier.

Methodology:
- Uses Blackbird Language Matrices (BLM) dataset containing sequences of 7 French sentences that share subject-verb agreement but differ in other aspects like number of attractors, clause structures etc.

- Explores multiple system architectures - FFNN baseline, CNN baselines, Encoder-Decoder (VAE), Dual VAE. Tests impact of using 1D vs 2D reshaped sentence embeddings as input.

- Uses max-margin loss function to maximize probability of correct answer from candidate set. VAE models also optimize latent layer regularization loss.

Key Results:
- 2D reshaped embeddings lead to better detection of subject-verb agreement patterns compared to 1D, especially with smaller training data.

- VAE-based models with 2D input can learn from less data and generalize better to more complex test sentences. Dual VAE gets best performance. 

- 48x16 reshaping works best indicating relevant info is distributed uniformly in 16-length spans in 768-dim BERT embeddings.

- Error analysis shows models tend to perform local subject-verb agreement instead of longer distance structural agreement. Dual VAE corrects more errors.

Main Contributions:
- Showing higher dimension patterns in BERT embeddings can encode syntactic phenomena, beyond readily available 1D form.

- 2D reshaped inputs and VAE-based models improve detection of targeted phenomena using less training data. Opens up few-shot learning approaches.

- Analyzes how grammatical info is distributed in BERT sentence embeddings.
