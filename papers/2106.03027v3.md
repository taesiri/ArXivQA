# [Model Zoo: A Growing "Brain" That Learns Continually](https://arxiv.org/abs/2106.03027v3)

## What is the central research question or hypothesis that this paper addresses?

 This paper presents a new continual learning algorithm called Model Zoo that grows an ensemble of models over time as new tasks arrive. The key ideas and contributions are:

1. A theoretical analysis showing when training a single model on multiple tasks can be beneficial vs detrimental due to competition between tasks. 

2. The Model Zoo algorithm that splits model capacity across synergistic tasks to mitigate this competition. It trains small multi-task models on the current task plus a subset of past tasks in each episode. 

3. Empirical results showing Model Zoo outperforms existing continual learning methods by significant margins across various benchmarks. It demonstrates strong forward and backward transfer while avoiding catastrophic forgetting.

4. An analysis showing even a simple per-episode isolated training baseline outperforms most existing methods. This indicates they fail to properly leverage data from multiple tasks compared to just training them independently. 

5. Model Zoo is proposed as a strong continually learning approach that effectively handles task relationships and limited model capacity, while allowing different amounts of replay and model sizes.

In summary, the central hypothesis is that explicitly managing model capacity and task synergies is crucial for effective continual learning. Model Zoo provides a simple way to achieve this that leads to much better empirical performance.


## What is the main contribution of this paper?

 The main contributions of this paper seem to be:

1. A theoretical analysis characterizing when training a single model on multiple tasks can be beneficial versus detrimental due to task competition. The key idea is defining a notion of task relatedness. 

2. The development of a continual learning algorithm called Model Zoo that splits learning capacity across an ensemble of small models trained on subsets of tasks. This is inspired by boosting and helps mitigate negative effects of task competition.

3. Empirical results showing Model Zoo outperforms existing continual learning methods on several benchmarks, with large gains in some cases. A simple per-task baseline also outperforms many existing methods, highlighting issues with them. 

4. Model Zoo demonstrates strong forward and backward transfer, high per-task accuracy, and efficient training/inference compared to existing methods.

5. The paper argues we should take a critical look at continual learning and pay more attention to per-task accuracy and transfer, not just avoiding catastrophic forgetting. It proposes simple baselines like the per-task learner can ground evaluations.

In summary, the main contribution seems to be the proposed Model Zoo algorithm and analysis showing the need for methods that effectively split capacity across subsets of tasks, in order to deal with task competition in continual learning. The empirical results demonstrate the benefits of this approach across diverse benchmarks.
