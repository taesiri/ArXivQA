# [The Unlocking Spell on Base LLMs: Rethinking Alignment via In-Context   Learning](https://arxiv.org/abs/2312.01552)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a summary paragraph of the key points from the paper:

This paper analyzes the effect of alignment tuning on large language models (LLMs) by comparing the token distributions between base and aligned LLMs. The analysis reveals that alignment tuning primarily affects stylistic tokens like discourse markers and safety disclaimers, rather than content-bearing words. Specifically, only 5-8% of token positions exhibit a distribution shift, with the top-ranked token by the aligned LLM not being top-ranked by the base LLM. The shifts mainly occur early during decoding. These findings provide evidence that alignment tuning focuses on adopting an assistant-like language style, while relying extensively on knowledge already acquired by base LLMs. 

Based on this, the paper introduces a simple yet effective baseline called URIAL for tuning-free alignment via in-context learning. It uses a few stylistic example responses and a system prompt to align base LLMs without parameter tuning. Experiments demonstrate that URIAL matches or exceeds aligned LLMs on strong base models like Mistral-7B and Llama-2-70B across helpfulness, clarity, factuality and other aspects. The paper argues that deeper understanding of alignment is crucial, and that developing better tuning-free methods could be promising for certain scenarios.
