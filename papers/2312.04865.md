# [StructComp: Substituting propagation with Structural Compression in   Training Graph Contrastive Learning](https://arxiv.org/abs/2312.04865)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a summary paragraph of the key points from the paper:

This paper proposes Structural Compression (StructComp), a simple yet effective training framework to improve the scalability of graph contrastive learning (GCL). Motivated by a sparse low-rank approximation of the diffusion matrix, StructComp substitutes propagation with structural compression during training. Specifically, it compresses the nodes into mixed nodes based on graph partitioning, and trains a MLP encoder on these mixed nodes, transferring the parameters to a GNN encoder later for inference. This allows avoiding message passing during training, and significantly reduces the number of sample pairs needed for contrastive learning. Theoretically, the authors prove the compressed contrastive loss can approximate the original GCL loss, and StructComp introduces an extra regularization. Empirically, StructComp improves performance of GCL models on seven benchmarks, while greatly reducing time and memory consumption compared to full graph training and other scalable methods. Key advantages are: 1) first unified framework for scalable GCL training, 2) high quality sampling by node compression, 3) implicit regularization. Overall, StructComp advances the scalability of graph contrastive learning with negligible performance sacrifice.
