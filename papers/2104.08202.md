# $Q^{2}$: Evaluating Factual Consistency in Knowledge-Grounded Dialogues   via Question Generation and Question Answering

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the main research question is: How can we automatically evaluate the factual consistency of knowledge-grounded open-domain dialogue systems?Specifically, the authors aim to develop an automatic metric that can score dialogue system outputs in terms of their factual consistency with respect to a given knowledge source, without requiring gold reference responses. The key hypothesis is that an evaluation approach using question generation and question answering will be effective for this purpose. The intuition is that if a dialogue response is factually consistent with the provided knowledge, then questions generated based on that response should yield similar answers when answered using the knowledge source.The authors propose a pipeline called Q2 that implements this idea. It extracts informative spans from the dialogue response, generates questions for those spans, answers the questions using the knowledge source, and compares the response-based answers to the knowledge-based answers using natural language inference. The paper presents experiments on several dialogue datasets to test whether Q2 correlates well with human judgments of factual consistency compared to other metrics. The results support the hypothesis, showing that Q2 achieves higher correlation than other reference-free metrics.In summary, the central hypothesis is that factual consistency in knowledge-grounded dialogue can be effectively evaluated in a reference-free manner using question generation and question answering. The proposed Q2 metric aims to test this hypothesis.


## What is the main contribution of this paper?

The main contribution of this paper is proposing an automatic evaluation metric ($Q^2$) for measuring the factual consistency of knowledge-grounded dialogue systems. The key points are:- $Q^2$ uses question generation and question answering to compare the information expressed in a dialogue system's response with the given knowledge source it is conditioned on. - It compares answer spans using natural language inference instead of simple token matching, making it more robust to lexical variability.- The authors construct a new dataset of system responses annotated for factual consistency to evaluate $Q^2$ and other metrics. Experiments show $Q^2$ correlates better with human judgments than other metrics on this dataset as well as existing dialogue benchmarks.- Overall, the paper presents a novel automatic evaluation approach tailored for knowledge-grounded open-domain dialogues. The metric and dataset could facilitate research on improving the factual consistency of dialogue systems.In summary, the main contribution is proposing and validating a new automatic evaluation metric for knowledge-grounded dialogues using question generation, question answering, and natural language inference. The paper also provides new resources including a dataset annotated for factual consistency.
