# [Self-conditioned Image Generation via Generating Representations](https://arxiv.org/abs/2312.03701)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a summary paragraph of the key points from the paper:

This paper presents Representation-Conditioned image Generation (RCG), a new framework for high-quality unconditional image generation. RCG consists of three components: (1) a pre-trained self-supervised image encoder that maps images to a representation distribution, (2) a representation diffusion model that can sample from this distribution, and (3) a pixel generator that creates image pixels conditioned on the sampled representation. On ImageNet 256x256, RCG achieves state-of-the-art unconditional generation results of 3.31 FID and 253.4 IS, outperforming previous methods by a large margin. Remarkably, this rivals current leading class-conditional generators. RCG's strong performance stems from effectively conditioning the pixel generator on structured, semantic image representations from the encoder, providing useful guidance without human annotations. The representation diffusion model adds little overhead, making RCG easily integrable into various generative models. Overall, by bridging the gap between conditional and unconditional generation, RCG demonstrates the great promise of self-conditioned image generation to leverage vast unlabeled data.
