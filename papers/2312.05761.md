# [QMGeo: Differentially Private Federated Learning via Stochastic   Quantization with Mixed Truncated Geometric Distribution](https://arxiv.org/abs/2312.05761)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Federated learning (FL) frameworks face two key challenges - preserving privacy for users while being communication-efficient. Although keeping data decentralized provides some privacy, analyses of the shared model updates can still leak sensitive information. At the same time, as ML models become more complex, transmitting high-precision model update vectors induces heavy communication overhead. 

- Prior works have looked at adding noise for differential privacy (DP) guarantees and quantization for communication efficiency separately. This paper investigates whether a properly designed stochastic quantization method can provide both DP and communication efficiency without extra noise injection.

Proposed Solution:
- The paper proposes QMGeo, a novel stochastic scalar quantization method that uses a mixed truncated geometric distribution to introduce required randomness. 

- QMGeo takes an input scalar, maps it to quantization levels based on a mixed distribution consisting of two truncated geometric distributions. The mixture probability is determined by the distance of the input scalar to neighboring quantization levels.

- This method assigns non-zero probabilities to all quantization levels, providing the needed randomness for DP without extra noise. The shape of the distribution is controlled by the success probability parameter p.

Main Contributions:
- Provides privacy analysis of QMGeo under both ε-DP and Rényi DP for scalar and vector settings. Demonstrates tradeoff between DP guarantee ε and hyperparameters like p and number of quantization levels.

- Conducts optimality gap analysis to analyze impact of introduced perturbation on convergence of the FL framework.

- Empirically evaluates QMGeo on MNIST dataset. Shows with proper parameter selection, QMGeo can provide DP guarantees with negligible accuracy loss compared to baseline (no quantization).

In summary, the paper proposes a novel stochastic quantization method QMGeo that achieves DP without extra noise injection while reducing communication cost, with both theoretical privacy and convergence analyses, along with empirical evaluation.
