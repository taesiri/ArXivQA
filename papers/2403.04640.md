# [CAT: Enhancing Multimodal Large Language Model to Answer Questions in   Dynamic Audio-Visual Scenarios](https://arxiv.org/abs/2403.04640)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Existing multimodal large language models (MLLMs) have limitations in answering questions in dynamic audio-visual scenarios. Their responses can be ambiguous and fail to describe specific audio-visual events. 

Proposed Solution - CAT Model:
- Enhances MLLM in 3 ways:
   1) Aggregates question-related visual and audio clues to enrich fine-grained knowledge required to answer questions.  
   2) Trained on mixed multimodal datasets including a new collected audio-visual instruction dataset (AVinstruct). Allows direct application in audio-visual scenarios.
   3) Uses an AI-assisted ambiguity-aware direct preference optimization strategy to retrain model to favor non-ambiguous responses.

Main Contributions:
- Propose the CAT model that learns question-related clues and engages directly in dynamic audio-visual inference.
- Collect the AVinstruct dataset - an audio-visual joint instruction dataset to ensure CAT's stability. 
- Propose an AI-assisted ambiguity-aware direct preference optimization strategy to address tendency of MLLMs to give ambiguous responses.
- Evaluation shows CAT outperforms state-of-the-art on variety of audio-visual QA tasks and video-based text generation tasks.

In summary, the paper introduces the CAT model to enhance multimodal language models to better answer questions in dynamic audio-visual scenarios by aggregating relevant clues, using suitable training data, and optimizing to reduce ambiguity. Experiments validate superiority over existing methods.
