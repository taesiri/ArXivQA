# [Exploring the Limitations of Graph Reasoning in Large Language Models](https://arxiv.org/abs/2402.01805)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem Statement
- The paper explores the graph reasoning capabilities of large language models (LLMs) through a series of traversal problems of increasing complexity. Specifically, it tests 5 LLMs - GPT-3.5, GPT-4, Claude-2, Llama-2, and Palm-2.

- Graph reasoning is important to evaluate as it requires tracking relationships between entities and multi-hop logical deductions. However, existing works have not systematically benchmarked LLMs on graph reasoning tasks. 

Methodology
- The paper designs 10 graph traversal problems categorized into tree-based, grid-based, and special graphs. Constraints are incrementally added such as weights, directed edges, no solutions etc.

- Problems are evaluated across graph complexities - O(10), O(20), O(20) jumbled nodes. Also across 0-shot, 1-shot and 3-shot prompting. Partial credit metrics are used apart from accuracy.

Observations
- An inverse relation exists between average node degrees of freedom and reasoning capability. Also, model performance drops with increasing graph complexity.

- Weighted graphs are more challenging than unweighted graphs for LLMs. K-shot prompting fails to improve reasoning accuracy.

- Models demonstrate positive bias and inability to state no valid solution exists. GPT-4 and Claude-2 show most promising reasoning abilities.

Proposed Solution 
- A new prompting technique "PathCompare" is proposed which allows LLMs to list and compare all possible paths before determining optimal solution.

- PathCompare shows consistent improvements in reasoning accuracy over standard prompting and Chain-of-Thought across problems and models.

Main Contributions
- Comprehensive benchmarking of reasoning across models and graph complexities 

- Analysis of specific limitations of LLMs in graph reasoning

- Novel prompting technique to improve multi-hop deductions in LLMs
