# [Normalization Is All You Need: Understanding Layer-Normalized Federated   Learning under Extreme Label Shift](https://arxiv.org/abs/2308.09565)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be:Why and how is layer normalization (LN) surprisingly effective at handling non-i.i.d. data heterogeneity in federated learning?The key hypothesis appears to be that layer normalization helps control feature collapse and local overfitting to heavily skewed datasets in federated learning. This in turn accelerates the training of the global model.In particular, the paper investigates the effectiveness of layer normalization under extreme label shift, where each client only has data from one or a few classes. It hypothesizes that layer normalization is especially helpful in this setting by preventing clients from overfitting to their local datasets and collapsing their feature representations.The paper aims to analyze layer normalization theoretically and empirically to reveal its connection to the label shift problem in federated learning. It further identifies feature normalization as a key mechanism that captures the benefits of layer normalization in this setting.
