# [An Efficient Sparse Inference Software Accelerator for Transformer-based   Language Models on CPUs](https://arxiv.org/abs/2306.16601)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the main research question/hypothesis appears to be:How to develop an efficient sparse deep learning inference software stack for Transformer-based language models, where the weights are pruned with a constant block size sparsity pattern, in order to improve inference efficiency on CPUs.The key ideas and components involved seem to be:- Using a structured sparsity pattern with constant block size 4x1.- Developing optimized sparse GEMM and sparse attention kernels leveraging Intel Deep Learning Boost to maximize performance on CPUs. - Comparing against existing sparse GEMM libraries like oneMKL, TVM, LIBXSMM and showing significant speedups.- Applying the techniques on popular Transformer models like BERT and showing improved throughput under latency constraints compared to frameworks like ONNX Runtime and PyTorch.- Comparing against other sparse inference engines like Neural Magic DeepSparse and demonstrating speedups.So in summary, the main hypothesis appears to be that using structured sparsity along with optimized sparse kernels can enable efficient sparse inference of Transformer models on CPUs. The paper seems aimed at validating this hypothesis through benchmarks and comparisons to other techniques/libraries/frameworks.


## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the main research focus is developing an efficient sparse deep learning inference software stack for Transformer-based language models on CPUs. Specifically, the authors aim to accelerate inference for sparse Transformer models by leveraging structured sparsity and quantization techniques. The key hypothesis appears to be that by using a structured sparsity pattern (4x1) along with quantization, and implementing optimized sparse matrix kernels and end-to-end optimizations, they can achieve significant speedups for sparse Transformer model inference on CPUs compared to existing solutions.The paper seems centered around showing empirically that their proposed sparse software accelerator enables efficient sparse inference for Transformer models, outperforming current sparse libraries, frameworks, and other inference engines in terms of throughput under latency constraints.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contributions appear to be:- They propose an efficient sparse deep learning inference software stack for Transformer-based language models that supports structured sparsity and quantization. - They define a structured sparsity pattern with block size 4x1 and generate 9 sparse Transformer models with 80-90% sparsity while maintaining accuracy within 1% of the dense baseline.- They develop optimized sparse GEMM and sparse attention kernels using Intel Deep Learning Boost that outperform existing sparse libraries like oneMKL, TVM, and LIBXSMM by an order of magnitude in terms of performance.- Their sparse software accelerator demonstrates good end-to-end speedups on Bert-Mini, DistilBERT, Bert-Base and BERT-Large compared to other inference engines like Neural Magic's Deepsparse, ONNX Runtime and PyTorch under production latency constraints.So in summary, the main contribution seems to be an efficient sparse inference software stack for Transformer models that combines structured sparsity, quantization and optimized kernels to achieve significant speedups over prior work. The paper demonstrates this through extensive benchmarking of both the kernels and end-to-end workloads.


## What is the main contribution of this paper?

Based on my reading, the main contributions of this paper are:- It proposes an efficient sparse deep learning inference software stack for Transformer-based language models, where the weights are pruned with a constant block size of 4x1. - It develops optimized sparse GEMM and sparse attention kernels using Intel Deep Learning Boost to maximize performance on CPUs. - The sparse GEMM kernel significantly outperforms existing sparse libraries like oneMKL, TVM, and LIBXSMM on a range of representative GEMM shapes and sparsity ratios. It shows up to 17x speedup over oneMKL and 41x over TVM on single thread.- The end-to-end sparse inference framework demonstrates up to 1.5x speedup over Neural Magic's Deepsparse, 37x over ONNX Runtime, and 345x over PyTorch on Xeon CPUs for typical Transformer models under latency constraints.In summary, the paper presents an efficient software accelerator for sparse inference of Transformer models on CPUs, with optimized kernels and end-to-end optimizations that outperform existing solutions. The constant block sparsity pattern and sparse kernels optimized using Intel Deep Learning Boost seem to be the key technical innovations that enable significant speedups.


## How does this paper compare to other research in the same field?

Based on my reading of the paper, here is a summary of how it compares to other related research:The paper presents a software accelerator for efficient sparse inference of Transformer-based language models on CPUs. The key ideas include:- Using a structured sparsity pattern (4x1) to prune the models and maintain accuracy. This is similar to other works like Pool et al. that use structured pruning.- Implementing optimized sparse matrix-dense matrix multiplication (SpMM) kernels using Intel AVX512 instructions. Other sparse inference engines like Neural Magic also implement optimized SpMM kernels. This paper shows large speedups over Neural Magic and other sparse libraries.- Operator fusion techniques to reduce overhead. This is a common optimization used in other inference engines as well.- Quantizing the sparse models to 8-bit integers. Quantization has been widely used with pruning to further compress models.The key novelties and comparisons:- The paper shows much faster SpMM performance than existing sparse libraries, up to 20x over oneMKL and 62x over TVM.- It demonstrates end-to-end speedups on Transformer models, up to 1.5-4.9x over Neural Magic, 37-72x over ONNX Runtime, and 309-345x over PyTorch.- The software accelerator is optimized specifically for latest Intel CPUs using AVX512 instructions. This provides an efficient sparse inference solution on a widely used hardware platform.- Most prior work has focused on GPUs or specialized accelerators. This paper targets the problem of sparse inference on CPUs.Overall, the paper demonstrates state-of-the-art sparse inference performance on CPUs by combining structured sparsity, INT8 quantization, optimized kernels, and fusion. The speedups over existing solutions are significant and make a strong case for the methods presented.


## How does this paper compare to other research in the same field?

Based on my reading, this paper presents an efficient sparse deep learning inference software accelerator for Transformer-based language models on CPUs. The key contributions seem to be:- It defines a structured sparsity pattern (4x1) that allows pruning Transformer models to high sparsity ratios (80-90%) while maintaining accuracy within 1% of baseline.- It develops optimized sparse GEMM and attention kernels leveraging Intel Deep Learning Boost that outperform existing sparse libraries like oneMKL, TVM, and LIBXSMM by 10-20x. - It shows 1.3-1.5x speedup over Neural Magic's Deepsparse inference engine on the same CPU hardware for sparse Transformer models.- It demonstrates up to 37x and 345x speedup over ONNX Runtime and PyTorch respectively for sparse Transformer inference on CPUs.Some key differences compared to prior work:- Focuses specifically on efficient sparse inference for Transformers on CPUs, rather than GPUs. Most prior work on sparse inference optimization has targeted GPUs.- Implements custom sparse kernels rather than relying on existing sparse libraries like some other solutions. This allows fully optimized kernels.- Quantizes sparse models to 8-bit integer for additional speedup. Many other sparse inference works focus only on sparsity. - Compares against Neural Magic's commercial sparse inference engine in addition to frameworks like ONNX Runtime/PyTorch. Provides unique benchmarks.Overall, this paper seems to advance sparse Transformer inference on CPUs substantially compared to prior art. The custom kernels, integer quantization, and comparisons against other leading sparse solutions demonstrate state-of-the-art performance.
