# [Learning How to Ask: Querying LMs with Mixtures of Soft Prompts](https://arxiv.org/abs/2104.06599)

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper addresses is how to improve the ability to extract factual knowledge from pretrained language models by learning better prompt formulations. The key hypotheses are:- Soft prompts with continuous token vectors can express a wider range of lexical patterns and emphasize helpful dimensions, allowing optimization to find better prompts than using discrete words.- Learning mixtures of prompts provides an ensemble that captures diverse ways of expressing the same relation.- Prompts can be initialized randomly rather than requiring manual creation or mining from text. Optimization can still find effective prompts from random initialization. - Perturbing the internal representations of prompts at all layers rather than just the input embedding layer can further improve prompt tuning.The overall goal is to show that the knowledge contained implicitly in language models can be more effectively elicited with optimized prompting, even without any extra training of the model.


## What is the main contribution of this paper?

The main contribution of this paper is developing a method to learn soft prompts for querying language models. The key ideas are:- Using continuous prompt vectors ("soft prompts") that can be optimized via gradient descent, rather than only prompts consisting of discrete words. This allows more expressive prompting.- Learning mixtures of prompts, where the mixture weights are also learned. This allows capturing diverse ways of expressing the same query.- Perturbing the internal representations of prompt tokens at all layers of the LM, not just the input embedding layer. This allows finer-tuning of the prompts.- Showing that prompts initialized randomly can work as well as prompts informed by previous methods, demonstrating that language models have more knowledge than previously realized - it just requires finding the right way to query them.- Demonstrating large gains over previous prompting methods by tuning soft prompt mixtures, across various language models and knowledge extraction tasks.So in summary, the main contribution is developing an effective prompting method that can unlock more of the knowledge implicit in language models, by learning to softly and diversely prompt the models for each task.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper proposes learning soft, continuous prompt vectors rather than using discrete words, allowing gradient-based optimization of prompts for querying knowledge from language models.


## How does this paper compare to other research in the same field?

Here are a few key points on how this paper compares to other related work:- The main contribution is using gradient-based optimization to learn soft, continuous vector prompts for querying language models, rather than relying solely on natural language prompts. This builds on prior work like LAMA and LPAQA that uses manual or mined natural language prompts.- The idea of learning soft, non-discrete prompts is novel compared to prior work, though some very recent unpublished work has started exploring this direction as well (e.g. Li et al. 2021, Liu et al. 2021). This paper seems to provide one of the earlier and more comprehensive investigations.- Using mixtures of prompts is another contribution compared to prior methods that use single prompts. This allows capturing multiple ways to express the same relationship.- Experiments demonstrate substantially stronger performance compared to all prior baselines. The improvements are consistent across multiple language models and datasets.- The approach is fairly general and could likely be applied to other types of prompts, not just cloze prompts. Some limitations are that it focuses on single token answers and relies on a fixed pretrained language model.- Overall, this seems like an important advance in an emerging line of research on how to effectively prompt language models. The core ideas around optimizing soft prompts and mixtures seem novel and impactful compared to prior prompt engineering methods.


## What future research directions do the authors suggest?

Based on my reading of the paper, some future research directions the authors suggest are:- Testing the idea of learning soft prompts on few-shot prediction tasks with pretrained generative language models like GPT-2 or BART. They suggest allowing fine-tuned soft prompt-response pairs rather than having to fine-tune the entire language model.- Applying their method of learning soft prompts to natural language prompts used for "few-shot learning" that include input-output examples. - Generalizing their approach to allow tuning prompts that ask about particular sentences, not just fill-in-the-blank factual prompts.- Exploring other ways to initialize and regularize the prompts beyond their techniques, to find the most effective prompts while preventing overfitting.- Testing their method on a wider range of language models, tasks, and datasets to further demonstrate its applicability.- Extending their mixture modeling to use more complex ensembling techniques beyond mixture-of-experts.- Improving their data-dependent mixture weighting approach to better incorporate the input information.So in summary, they suggest directions like applying their approach to other prompt-based LM querying tasks, testing on more models and data, finding better ways to initialize and regularize prompts, and improving their prompt ensemble techniques. The core idea is extending their method of learning soft prompts.


## Summarize the paper in one paragraph.

Here is a one paragraph summary of the paper:The paper explores using gradient descent to optimize "soft prompts" for extracting factual knowledge from pretrained language models. Soft prompts are continuous vectors that are tuned to be effective at eliciting knowledge from the language model when used in a fill-in-the-blank paradigm. The authors propose learning prompt mixtures, where multiple variants of a prompt are jointly optimized, as well as deeply perturbed prompts, where the prompts are tuned at all layers of the language model rather than just the embedding layer. Experiments across multiple language models and factual knowledge datasets show that optimizing soft prompt mixtures substantially outperforms prior methods like LAMA and AutoPrompt for extracting knowledge, even when initializing the soft prompts randomly rather than based on real prompts. The results demonstrate that language models contain much more factual knowledge than previously realized, but eliciting that knowledge requires finding the right tuning of the prompt.


## Summarize the paper in two paragraphs.

Here is a two paragraph summary of the paper:The paper explores the idea of learning soft prompts by gradient descent to elicit factual knowledge from pretrained language models. Soft prompts consist of continuous word vectors rather than discrete words from the language model's vocabulary. The paper proposes learning mixtures of soft prompts, where each prompt is a sentence with two blanks to be filled in. The prompts are optimized to predict the second blank from the first blank on a training set of (x, y) pairs representing a factual relation. Experiments are conducted on several language models including BERT, RoBERTa, and BART on relations from datasets like T-REx, Google-RE, and ConceptNet. The results show that tuning soft prompts substantially improves performance over using the original hard prompts from prior work like LAMA and LPAQA. Remarkably, prompts initialized to random vectors perform nearly as well as prompts initialized with real words. Using a mixture of prompts is also beneficial. The method generates huge improvements across models and datasets, demonstrating that current language models have more factual knowledge than previously realized - it just requires finding effective ways to query them. Overall, the work introduces a simple yet powerful prompting technique to extract knowledge from language models.


## Summarize the main method used in the paper in one paragraph.

Here is a one paragraph summary of the main method used in the paper:The paper explores the idea of learning soft prompts by gradient descent for querying factual knowledge from pretrained language models. Soft prompts consist of continuous word vectors rather than discrete embeddings. The method optimizes a mixture of soft prompts, learning which prompts are most effective and how to ensemble them. For each task, prompts are initialized from previous work or random vectors, and then fine-tuned by optimizing the log-loss of the predictive distribution on training examples. Experiments across multiple English language models and knowledge tasks show the method substantially outperforms previous techniques, demonstrating language models contain more implicit factual knowledge than previously realized. Even random initialization performs nearly as well as informed initialization, as the method rapidly learns effective prompts tailored to the training examples. Overall, the results indicate factual knowledge in language models can be cheaply elicited by learning to ask with soft prompt mixtures.


## What problem or question is the paper addressing?

The paper is addressing the problem of how to design good prompts to extract knowledge from pretrained language models. Specifically, it focuses on the task of querying language models to fill in missing entities in factual statements, using a fill-in-the-blank prompting approach. The key questions addressed in the paper are:- How can we automatically learn better prompts to extract knowledge from language models, rather than relying on manually created prompts?- Can we learn "soft prompts" where the prompt words are continuous vectors rather than discrete symbols? This allows prompts to be optimized via gradient descent.- Can learning prompt mixtures, which combine predictions from multiple diverse prompts, improve performance over single prompts?- Can initializing prompts in a smart way, or even just randomly, outperform carefully hand-designed prompts from prior work?- Can tuning all layers of the language model for prompting, rather than just the input embedding layer, further improve results?So in summary, the key focus is on automatically learning prompt design to better extract factual knowledge from pretrained language models, using techniques like soft prompts, prompt mixtures, and multi-layer tuning.
