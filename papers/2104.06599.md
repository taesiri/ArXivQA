# [Learning How to Ask: Querying LMs with Mixtures of Soft Prompts](https://arxiv.org/abs/2104.06599)

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper addresses is how to improve the ability to extract factual knowledge from pretrained language models by learning better prompt formulations. The key hypotheses are:- Soft prompts with continuous token vectors can express a wider range of lexical patterns and emphasize helpful dimensions, allowing optimization to find better prompts than using discrete words.- Learning mixtures of prompts provides an ensemble that captures diverse ways of expressing the same relation.- Prompts can be initialized randomly rather than requiring manual creation or mining from text. Optimization can still find effective prompts from random initialization. - Perturbing the internal representations of prompts at all layers rather than just the input embedding layer can further improve prompt tuning.The overall goal is to show that the knowledge contained implicitly in language models can be more effectively elicited with optimized prompting, even without any extra training of the model.


## What is the main contribution of this paper?

The main contribution of this paper is developing a method to learn soft prompts for querying language models. The key ideas are:- Using continuous prompt vectors ("soft prompts") that can be optimized via gradient descent, rather than only prompts consisting of discrete words. This allows more expressive prompting.- Learning mixtures of prompts, where the mixture weights are also learned. This allows capturing diverse ways of expressing the same query.- Perturbing the internal representations of prompt tokens at all layers of the LM, not just the input embedding layer. This allows finer-tuning of the prompts.- Showing that prompts initialized randomly can work as well as prompts informed by previous methods, demonstrating that language models have more knowledge than previously realized - it just requires finding the right way to query them.- Demonstrating large gains over previous prompting methods by tuning soft prompt mixtures, across various language models and knowledge extraction tasks.So in summary, the main contribution is developing an effective prompting method that can unlock more of the knowledge implicit in language models, by learning to softly and diversely prompt the models for each task.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper proposes learning soft, continuous prompt vectors rather than using discrete words, allowing gradient-based optimization of prompts for querying knowledge from language models.


## How does this paper compare to other research in the same field?

Here are a few key points on how this paper compares to other related work:- The main contribution is using gradient-based optimization to learn soft, continuous vector prompts for querying language models, rather than relying solely on natural language prompts. This builds on prior work like LAMA and LPAQA that uses manual or mined natural language prompts.- The idea of learning soft, non-discrete prompts is novel compared to prior work, though some very recent unpublished work has started exploring this direction as well (e.g. Li et al. 2021, Liu et al. 2021). This paper seems to provide one of the earlier and more comprehensive investigations.- Using mixtures of prompts is another contribution compared to prior methods that use single prompts. This allows capturing multiple ways to express the same relationship.- Experiments demonstrate substantially stronger performance compared to all prior baselines. The improvements are consistent across multiple language models and datasets.- The approach is fairly general and could likely be applied to other types of prompts, not just cloze prompts. Some limitations are that it focuses on single token answers and relies on a fixed pretrained language model.- Overall, this seems like an important advance in an emerging line of research on how to effectively prompt language models. The core ideas around optimizing soft prompts and mixtures seem novel and impactful compared to prior prompt engineering methods.


## What future research directions do the authors suggest?

Based on my reading of the paper, some future research directions the authors suggest are:- Testing the idea of learning soft prompts on few-shot prediction tasks with pretrained generative language models like GPT-2 or BART. They suggest allowing fine-tuned soft prompt-response pairs rather than having to fine-tune the entire language model.- Applying their method of learning soft prompts to natural language prompts used for "few-shot learning" that include input-output examples. - Generalizing their approach to allow tuning prompts that ask about particular sentences, not just fill-in-the-blank factual prompts.- Exploring other ways to initialize and regularize the prompts beyond their techniques, to find the most effective prompts while preventing overfitting.- Testing their method on a wider range of language models, tasks, and datasets to further demonstrate its applicability.- Extending their mixture modeling to use more complex ensembling techniques beyond mixture-of-experts.- Improving their data-dependent mixture weighting approach to better incorporate the input information.So in summary, they suggest directions like applying their approach to other prompt-based LM querying tasks, testing on more models and data, finding better ways to initialize and regularize prompts, and improving their prompt ensemble techniques. The core idea is extending their method of learning soft prompts.


## Summarize the paper in one paragraph.

Here is a one paragraph summary of the paper:The paper explores using gradient descent to optimize "soft prompts" for extracting factual knowledge from pretrained language models. Soft prompts are continuous vectors that are tuned to be effective at eliciting knowledge from the language model when used in a fill-in-the-blank paradigm. The authors propose learning prompt mixtures, where multiple variants of a prompt are jointly optimized, as well as deeply perturbed prompts, where the prompts are tuned at all layers of the language model rather than just the embedding layer. Experiments across multiple language models and factual knowledge datasets show that optimizing soft prompt mixtures substantially outperforms prior methods like LAMA and AutoPrompt for extracting knowledge, even when initializing the soft prompts randomly rather than based on real prompts. The results demonstrate that language models contain much more factual knowledge than previously realized, but eliciting that knowledge requires finding the right tuning of the prompt.
