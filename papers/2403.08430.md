# [Search-based Optimisation of LLM Learning Shots for Story Point   Estimation](https://arxiv.org/abs/2403.08430)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem: 
- Large language models (LLMs) like GPT-4 can be used for software effort estimation tasks like predicting story points for user stories. 
- Providing the LLM with a few example user stories (called shots) in a few-shot learning setting affects the accuracy of the model's estimations. 
- Finding the optimal set of example shots to provide is challenging due to prompt length limitations and computational constraints.

Proposed Solution:
- Use a multi-objective genetic algorithm called CoGEE to optimize the selection of learning shots. 
- Define and minimize 3 objectives: 
   1) Sum of Absolute Error (SAE) of LLM's estimates
   2) Confidence Interval (CI) of the error distribution  
   3) Number of shots (N) used
- Configure and apply NSGA-II algorithm with custom crossover and mutation operators to search for Pareto optimal shot sets.

Key Contributions:
- First study to use Search-Based Software Engineering (SBSE) to optimize learning shots for an LLM's software effort estimation.
- Optimization objectives balance accuracy, uncertainty and cost of using the LLM.  
- Approach improved GPT-4's estimation accuracy by 59.34% on average compared to zero-shot learning baseline.
- Demonstrates feasibility of using SBSE to improve few-shot learning prompts for LLM-based systems.

In summary, the paper proposes a way to automatically find better few-shot learning examples to improve a large language model's performance on a software estimation task. Preliminary results on 3 datasets show SBSE helps substantially boost the accuracy of GPT-4's story point predictions.
