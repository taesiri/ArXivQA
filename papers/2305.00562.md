# [Class-Balancing Diffusion Models](https://arxiv.org/abs/2305.00562)

## What is the central research question or hypothesis that this paper addresses?

 The central research question this paper addresses is how to improve the performance of diffusion models when trained on imbalanced/long-tailed datasets. The key hypotheses are:

1) Standard diffusion models will degrade in performance, particularly in diversity and fidelity of generated images, when trained on imbalanced datasets where some classes have many more examples than others.

2) This performance degradation can be mitigated by adjusting the diffusion model training to account for the imbalanced class distribution. Specifically, by adding a regularization term to the loss function that increases similarity of generated images across different classes. 

3) This class-balancing regularization approach, implemented in the proposed Class-Balancing Diffusion Model (CBDM), will improve diversity and fidelity compared to standard diffusion models, especially for tail/minority classes.

In summary, the paper hypothesizes that standard diffusion models face challenges when trained on imbalanced data, and proposes a new regularization approach in the CBDM model to account for imbalanced class distributions and improve generation performance. The central questions are whether class imbalance hurts diffusion models, and whether the proposed CBDM method effectively addresses this problem.


## What is the main contribution of this paper?

 The main contribution of this paper is proposing a novel Class-Balancing Diffusion Model (CBDM) to handle the generation quality degradation of diffusion models when trained on class-imbalanced datasets. The key ideas are:

- They first identify the problem that diffusion models tend to generate low quality and low diversity images for tail classes on long-tailed datasets. 

- They derive a new perspective to adjust the conditional transfer probability during sampling to force the model to have a more balanced class prior. This results in an additional regularization loss that increases similarity of outputs across different classes.

- The proposed CBDM method significantly improves diversity and fidelity of generated images from diffusion models, especially for tail classes and datasets with more categories.

- CBDM is lightweight to implement by just adding a few lines of code as a regularizer to existing conditional diffusion models. It is also compatible with different model architectures, sampling methods like DDIM, and flexible to use for fine-tuning.

- Extensive experiments validate CBDM outperforms previous state-of-the-art generative models on long-tailed image datasets like CIFAR and can help improve downstream recognition tasks.

In summary, the key contribution is identifying the problem of diffusion models on imbalanced data, proposing the CBDM solution and demonstrating its effectiveness. The method is simple, flexible and provides good performance gains.
