# [Temperature Balancing, Layer-wise Weight Analysis, and Neural Network   Training](https://arxiv.org/abs/2312.00359)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a high-quality paragraph summarizing the key points of the paper:

This paper proposes TempBalance, a novel layer-wise learning rate scheduler for neural network training. Motivated by Heavy-Tailed Self-Regularization (HT-SR) theory, TempBalance leverages the power-law exponent alpha of the empirical spectral density of weight matrices to assess the relative undertraining/overtraining level of each layer. It then assigns higher (lower) learning rates to relatively undertrained (overtrained) layers to balance training. Experiments on various models (ResNets, VGGs) and datasets (CIFAR, TinyImageNet, etc.) demonstrate that TempBalance outperforms strong baselines like cosine annealing learning rate schedules and spectral norm regularization. Analyses reveal TempBalance effectively regularizes layer eigenspectra. Additional results verify the robustness and scalability of the approach. In summary, TempBalance offers a simple, efficient and theoretically-grounded layer-wise learning rate method to enhance neural network training and performance.
