# [MolXPT: Wrapping Molecules with Text for Generative Pre-training](https://arxiv.org/abs/2305.10688)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central hypothesis of this paper is that pre-training a language model on text wrapped with molecular SMILES sequences can lead to improved performance on downstream tasks involving both text and molecules. Specifically, the authors propose a new model called \ourM{} that is pre-trained on a corpus containing:- Scientific text (titles and abstracts from PubMed articles)- Molecular SMILES sequences (from PubChem database)  - "Wrapped" sequences where detected molecule names in text are replaced by their corresponding SMILES.The key idea is that by training on these wrapped sequences, the model can learn the relationships between molecules and their textual descriptions. The authors then evaluate \ourM{} on two main downstream tasks:1) Molecular property prediction on MoleculeNet benchmark. Here the hypothesis is that pre-training with both text and SMILES will improve performance on predicting properties of molecules based on their structure.2) Text-molecule translation on a dataset of molecule-description pairs. The hypothesis is that the model will better capture alignment between text and molecules thanks to seeing wrapped sequences during pre-training.The overall hypothesis is that by pre-training on text "wrapped" with molecular SMILES, \ourM{} can integrate information from both modalities and achieve strong performance on downstream tasks involving molecules and text. The model architecture and training approach is designed to test this hypothesis.
