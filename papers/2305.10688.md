# [MolXPT: Wrapping Molecules with Text for Generative Pre-training](https://arxiv.org/abs/2305.10688)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central hypothesis of this paper is that pre-training a language model on text wrapped with molecular SMILES sequences can lead to improved performance on downstream tasks involving both text and molecules. Specifically, the authors propose a new model called \ourM{} that is pre-trained on a corpus containing:- Scientific text (titles and abstracts from PubMed articles)- Molecular SMILES sequences (from PubChem database)  - "Wrapped" sequences where detected molecule names in text are replaced by their corresponding SMILES.The key idea is that by training on these wrapped sequences, the model can learn the relationships between molecules and their textual descriptions. The authors then evaluate \ourM{} on two main downstream tasks:1) Molecular property prediction on MoleculeNet benchmark. Here the hypothesis is that pre-training with both text and SMILES will improve performance on predicting properties of molecules based on their structure.2) Text-molecule translation on a dataset of molecule-description pairs. The hypothesis is that the model will better capture alignment between text and molecules thanks to seeing wrapped sequences during pre-training.The overall hypothesis is that by pre-training on text "wrapped" with molecular SMILES, \ourM{} can integrate information from both modalities and achieve strong performance on downstream tasks involving molecules and text. The model architecture and training approach is designed to test this hypothesis.


## What is the main contribution of this paper?

The main contributions of this paper are:1. Proposing MolXPT, a new unified language model for molecules and text. MolXPT is pre-trained on a corpus containing scientific text, molecular SMILES strings, and "wrapped" sequences that connect text and molecules. 2. Demonstrating that pre-training on the wrapped sequences allows MolXPT to learn alignments between text and molecules. This results in improved performance on downstream tasks involving both text and molecules, such as molecular property prediction and molecule-text translation.3. Achieving state-of-the-art performance on molecular property prediction benchmarks including MoleculeNet. MolXPT outperforms previous methods including sophisticated graph neural network models.4. Obtaining comparable performance to MolT5, the previous state-of-the-art on molecule-text translation, while using less than half the number of parameters. MolXPT also shows strong zero-shot ability for text-to-molecule generation without finetuning.5. Providing a unified generative model for both text and molecules that can be finetuned via prompts for a diverse set of downstream applications at the intersection of natural language and molecular modeling.In summary, the main contribution is proposing MolXPT, a new way to jointly model text and molecules by pre-training on wrapped sequences. This allows MolXPT to achieve excellent performance on tasks involving both modalities.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:This paper proposes MolXPT, a unified language model pre-trained on text, SMILES sequences, and "wrapped" sequences between text and SMILES to leverage their interactions, which achieves strong results on molecular property prediction and text-molecule translation tasks.
