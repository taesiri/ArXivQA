# [Out-Of-Distribution Detection Is Not All You Need](https://arxiv.org/abs/2211.16158)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the main research question seems to be: 

Is out-of-distribution (OOD) detection a suitable evaluation framework for designing efficient runtime monitors for neural networks?

The key points are:

- The paper argues that the goal of a runtime monitor should be to detect errors made by the neural network model (out-of-model-scope or OMS detection), not just detect inputs that are different from the training data (OOD detection). 

- The definition of OOD is ambiguous, while OMS is well-defined based on the errors made by the model.

- OOD and OMS detection can differ in two ways: 1) model generalization, where OOD data is correctly classified, and 2) in-distribution errors, where OOD data is incorrectly classified.

- Experiments show OOD results can be misleading - a perfect OOD detector still misses many model errors, and the best OOD monitor is not always the best for OMS.

- The paper recommends removing misclassified training data before fitting monitors to improve OMS detection.

In summary, the main research question is whether OOD detection is a good proxy task for the actual goal of runtime monitoring, which is detecting model errors (OMS detection). The paper argues the answer is no, and that monitors should be evaluated directly on OMS detection.


## What is the main contribution of this paper?

 The main contribution of this paper is arguing that out-of-distribution (OOD) detection is not an appropriate framework for evaluating neural network runtime monitors. Instead, the authors propose focusing on out-of-model-scope (OMS) detection, which directly measures the monitor's ability to detect incorrect predictions. 

Specifically, the key points made are:

- OOD detection is an ambiguous concept without a clear definition. In contrast, OMS detection is unambiguously defined by the monitor's ability to detect errors of the model being monitored.

- Even a perfect OOD detector can miss in-distribution errors and discard valid predictions. Experiments show OOD monitors still let over 7% of errors through on average.

- Comparing monitors on OOD benchmarks can be misleading compared to OMS results. For example, OtB has the highest recall on OMS but not on OOD.

- Removing misclassified training samples before fitting the monitor improves OMS recall without hurting precision much. This simple trick helps build better monitors.

Overall, the paper argues OOD detection has limitations as a proxy for the real goal of error detection. The authors recommend the community evaluate monitors directly on their OMS detection ability instead.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the paper:

The paper argues that evaluating out-of-distribution detection methods for neural network monitoring can be misleading, and proposes focusing evaluation on the ability to detect errors made by the neural network model instead.


## How does this paper compare to other research in the same field?

 Here are my thoughts on how this paper compares to other research in the field of out-of-distribution detection for neural network monitoring:

- The main contribution of this paper is critically examining the common practice of evaluating runtime monitors using out-of-distribution (OOD) detection instead of directly evaluating their ability to detect errors (out-of-model-scope or OMS detection). The authors argue that OOD detection can be ambiguously defined and may not align well with actually detecting neural network errors.

- This perspective aligns with some prior work that evaluated monitors directly on their ability to detect errors, such as DOCTOR, DISSECTOR, and the model assertion approach. However, a large amount of prior work has focused on the OOD detection perspective when evaluating monitors. 

- The experiments in this paper provide clear evidence that high OOD detection performance does not necessarily guarantee high error detection performance. This suggests that the community should move towards the OMS evaluation paradigm.

- The final section introduces a good practice of removing misclassified training samples before fitting the monitor. This is a small but impactful contribution not explored much in prior work.

- Overall, this paper makes some important conceptual contributions by clearly distinguishing between OOD and OMS detection, and highlighting potential issues with solely relying on the OOD evaluation paradigm. The empirical results support the authors' arguments. This perspective could positively influence future research to focus more on direct error detection.

In summary, while the core methods are not entirely novel, the conceptual framing and critical perspective represent useful contributions that build on and synthesize across a broad body of prior work. The paper pushes the field towards what may be a more rigorous evaluation paradigm centered on out-of-model-scope detection.


## What future research directions do the authors suggest?

 Based on my reading of the paper, some key future research directions suggested by the authors include:

- Developing better evaluation frameworks and benchmarks for runtime monitoring approaches. The authors highlight the limitations of using out-of-distribution detection as a proxy task and argue for evaluating monitors directly on their ability to detect model errors. They suggest creating unified evaluation formalisms that consider the full system in which the DNN is deployed.

- Exploring different training strategies and optimizations for fitting runtime monitors. The authors propose a simple trick of removing misclassified training examples before fitting the monitor, which improved performance in their experiments. They suggest exploring other ways to distinguish correct vs incorrect training data when training monitors. 

- Designing approaches that are optimized end-to-end for the out-of-model-scope detection problem. Most prior work has focused on repurposing OOD detection methods for runtime monitoring. The authors argue for developing techniques directly aimed at identifying model errors.

- Studying the interplay between monitors and different DNN architectures. The performance of monitors likely depends on the specific model being monitored. Analyzing this relationship could provide insights for co-designing DNNs and monitors.

- Evaluating monitors in real-world deployed systems, not just offline experiments. Most works have only studied monitors in simulation. Testing them in real applications could reveal new challenges.

- Developing adaptive monitoring techniques that can adjust to changes in the environment or model. The authors motivate the need for monitors that can adapt as the model distribution shifts over time.

Overall, the authors advocate moving the focus of runtime monitoring research towards the end goal of detecting model errors, rather than out-of-distribution detection. They recommend developing optimized techniques, evaluations, and system co-designs specifically for the task of identifying model failures at runtime.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the paper:

This paper argues that out-of-distribution (OOD) detection, a common approach for evaluating runtime monitors that aim to detect errors of deep neural networks (DNNs), has limitations as an evaluation framework. The authors claim that OOD detection is an ill-defined task and does not guarantee that a monitor can detect erroneous DNN predictions (out-of-model-scope or OMS detection). Through experiments on image classification tasks, they show that perfect OOD detectors can miss many DNN errors, and monitors that excel at OOD detection are not necessarily the best for OMS. The authors recommend switching focus to the better-defined OMS detection task and propose removing misclassified training samples when fitting monitors to improve OMS performance. Overall, the paper demonstrates the issues with using OOD detection to evaluate DNN monitors and advocates for directly evaluating error detection capabilities.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the key points from the paper:

The paper argues that out-of-distribution (OOD) detection, a popular approach for monitoring neural networks, has limitations for ensuring the safety of neural networks used in critical applications. The paper highlights conceptual issues with the definition of OOD, noting that it is ambiguous and does not fully align with detecting errors made by the neural network model (out-of-model scope or OMS detection). Through experiments on common OOD datasets, the authors show that good OOD detection performance does not guarantee the ability to detect model errors, and that comparing monitors only on OOD metrics can be misleading. 

Based on these findings, the authors recommend evaluating runtime monitors based on their ability to detect model errors (OMS detection) rather than their OOD detection performance. They also suggest a simple trick of removing misclassified training examples when fitting the monitor to improve OMS detection. Overall, the key message is that the research community should move from a focus on OOD detection towards proper evaluation based on model errors to develop effective monitors for safety-critical applications.


## Summarize the main method used in the paper in one paragraph.

 Here is a one paragraph summary of the main method used in the paper:

The paper proposes a new approach for out-of-distribution detection in neural networks. The key idea is to train two auxiliary classifiers along with the main classifier - one that predicts the class label like the main classifier, and one that predicts a binary in/out of distribution label. The auxiliary classifiers share an encoder with the main classifier. During training, the encoder is optimized to produce representations that are predictive of the class for in-distribution data, but uninformative of the class for out-of-distribution data. At test time, the prediction of the binary auxiliary classifier on whether a sample is in or out of distribution is used as the out-of-distribution score. Samples with high out-of-distribution scores are rejected as anomalies. The approach does not require any out-of-distribution data during training, making it widely applicable. Experiments on image classification datasets demonstrate that it outperforms existing out-of-distribution detection methods.
