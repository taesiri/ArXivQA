# [Out-Of-Distribution Detection Is Not All You Need](https://arxiv.org/abs/2211.16158)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the main research question seems to be: Is out-of-distribution (OOD) detection a suitable evaluation framework for designing efficient runtime monitors for neural networks?The key points are:- The paper argues that the goal of a runtime monitor should be to detect errors made by the neural network model (out-of-model-scope or OMS detection), not just detect inputs that are different from the training data (OOD detection). - The definition of OOD is ambiguous, while OMS is well-defined based on the errors made by the model.- OOD and OMS detection can differ in two ways: 1) model generalization, where OOD data is correctly classified, and 2) in-distribution errors, where OOD data is incorrectly classified.- Experiments show OOD results can be misleading - a perfect OOD detector still misses many model errors, and the best OOD monitor is not always the best for OMS.- The paper recommends removing misclassified training data before fitting monitors to improve OMS detection.In summary, the main research question is whether OOD detection is a good proxy task for the actual goal of runtime monitoring, which is detecting model errors (OMS detection). The paper argues the answer is no, and that monitors should be evaluated directly on OMS detection.


## What is the main contribution of this paper?

The main contribution of this paper is arguing that out-of-distribution (OOD) detection is not an appropriate framework for evaluating neural network runtime monitors. Instead, the authors propose focusing on out-of-model-scope (OMS) detection, which directly measures the monitor's ability to detect incorrect predictions. Specifically, the key points made are:- OOD detection is an ambiguous concept without a clear definition. In contrast, OMS detection is unambiguously defined by the monitor's ability to detect errors of the model being monitored.- Even a perfect OOD detector can miss in-distribution errors and discard valid predictions. Experiments show OOD monitors still let over 7% of errors through on average.- Comparing monitors on OOD benchmarks can be misleading compared to OMS results. For example, OtB has the highest recall on OMS but not on OOD.- Removing misclassified training samples before fitting the monitor improves OMS recall without hurting precision much. This simple trick helps build better monitors.Overall, the paper argues OOD detection has limitations as a proxy for the real goal of error detection. The authors recommend the community evaluate monitors directly on their OMS detection ability instead.
