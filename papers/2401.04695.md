# [Narrowing the Knowledge Evaluation Gap: Open-Domain Question Answering   with Multi-Granularity Answers](https://arxiv.org/abs/2401.04695)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Standard QA evaluation protocols compare predicted answers to reference answers at a single level of granularity. However, factual questions can often be correctly answered at multiple levels of specificity. 
- This means current QA evaluation likely underestimates the knowledge encapsulated in large language models (LLMs), a phenomenon the authors term the "knowledge evaluation gap".

Proposed Solution:
- Introduce a new QA evaluation setting called GRANOLA QA that uses multi-granularity reference answers ordered from most to least specific. 
- Proposed new metrics - GRANOLA accuracy (matches any reference answer) and GRANOLA informativeness (rewards more specific correct answers).
- Present a simple methodology to automatically enrich existing QA datasets with multi-granularity answers using a knowledge graph and language model. Apply this to the EQUATE dataset to create GRANOLA-EQUATE.
- Introduce a new decoding algorithm called Decoding with Response Aggregation (DRAG) that tailors the response granularity to the LLM's uncertainty level.

Main Contributions:
- Formalize and quantify the knowledge evaluation gap in QA for LLMs using multi-granularity evaluation.
- Provide an automatic method to create multi-granularity QA benchmarks.
- Introduce DRAG decoding that improves both accuracy and informativeness over baselines.
- Experiments show standard QA evaluation may significantly underestimate knowledge of LLMs, especially for rare entities.

In summary, accounting for answer granularity reveals LLMs know more than standard QA evaluation gives them credit for. Both multi-granularity evaluation and decoding strategies like DRAG help narrow this knowledge evaluation gap.


## Summarize the paper in one sentence.

 This paper proposes a new question answering evaluation setting and decoding method that consider multiple levels of answer granularity to more accurately assess language models' factual knowledge.


## What is the main contribution of this paper?

 The main contributions of this paper are:

1. Proposing a new QA evaluation setting called GRANOLA QA that considers multiple levels of answer granularity when evaluating the accuracy and informativeness of model predictions. This aims to provide a more faithful evaluation compared to standard QA settings that just use a single reference answer.

2. Introducing a simple and automatic method for augmenting existing factual QA datasets with multi-granularity answers by utilizing knowledge graphs and language models. This is used to create GRANOLA-EQ, a version of the EQUATIONS QA dataset with multi-granularity answers.

3. Proposing a new decoding algorithm called Decoding with Response Aggregation (DRAG) that is designed to match the granularity of the model's response with its uncertainty level. DRAG samples multiple candidate answers and aggregates them into a final response using prompting.

4. Experiments showing that standard QA evaluation likely underestimates the factual knowledge encapsulated in large language models. When evaluated using GRANOLA QA, the accuracy of models like PaLM improves substantially, especially for rare entities. 

5. Demonstrating that DRAG improves both accuracy and informativeness compared to baselines, by generating responses better aligned with the model's actual knowledge.

In summary, the main contribution is introducing the GRANOLA QA setting and methodology to provide more meaningful evaluation of language models' factual knowledge in QA.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and concepts associated with this paper include:

- Multi-granularity answers: The paper proposes evaluating question answering systems using multi-granularity ground truth answers, which capture answers at different levels of specificity. This allows models to get "partial credit" for coarser but still correct answers.

- Knowledge evaluation gap: The gap between standard accuracy metrics and accuracy when evaluated on multi-granularity answers. This gap quantifies the amount of knowledge in models that is currently underestimated.

- Granola QA: The name given to the proposed multi-granularity QA evaluation setting. It stands for "Granularity Of Labels".

- Decoding with Response Aggregation (DRAG): A new decoding strategy proposed that aggregates multiple sampled responses from the model into a single coarser but more accurate response. This aims to align answer granularity with model uncertainty.

- Selective prediction: The perspective of allowing models to abstain from answering questions when uncertain, while still evaluating accuracy only on non-abstained predictions. This is used to incorporate "I don't know" predictions into the evaluation.

- Fact granularity: The concept that factual knowledge can be captured at different levels of specificity, allowing for a spectrum of correct answers rather than binary correct/incorrect.

Let me know if you need any clarification or have additional questions!


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in this paper:

1. The paper introduces a new QA evaluation setting called GRANOLA QA. What are the key differences between GRANOLA QA and standard QA evaluation protocols? How does it aim to provide a more faithful evaluation of language models' factual knowledge?

2. The paper proposes a simple methodology for enriching existing QA datasets with multi-granularity answers. Can you walk through the key steps of this methodology? What are some limitations or challenges with automating this process? 

3. The paper introduces a new decoding algorithm called Decoding with Response Aggregation (DRAG). At a high level, how does DRAG work? What are the key hyperparameters and how do they impact the tradeoff between factuality and informativeness?

4. How exactly is the aggregation step in DRAG implemented? What are some ideas mentioned in the paper for improving or replacing this aggregation module? 

5. One of the key findings is that standard evaluation may significantly underestimate the knowledge encapsulated in language models. What evidence supports this claim? Can you discuss specific experiments or analyses that point to this?

6. The paper evaluates performance on GRANOLA-EQ, a variant of the EQA dataset. In what ways is GRANOLA-EQ constructed to facilitate multi-granularity evaluation? What are some limitations of this process?

7. The paper introduces two new metrics - GRANOLA accuracy and GRANOLA informativeness - for evaluating QA performance. Can you explain how these metrics are defined and what tradeoffs they aim to capture?  

8. The paper compares DRAG against several baseline decoding algorithms. What are the key strengths and weaknesses found for the different methods? How does DRAG aim to improve upon limitations of other approaches?

9. One experiment shows that standard accuracy steeply declines on rare entities while GRANOLA accuracy remains high. What does this finding suggest about the knowledge encapsulated in language models? How might it influence research directions?

10. The paper discusses several interesting future research directions that build upon the ideas introduced around multi-granularity evaluation and decoding. Can you summarize 2-3 future directions you find compelling and explain why?
