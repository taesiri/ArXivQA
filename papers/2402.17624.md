# [CustomSketching: Sketch Concept Extraction for Sketch-based Image   Synthesis and Editing](https://arxiv.org/abs/2402.17624)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
Existing text-to-image (T2I) models allow users to incorporate new visual concepts from reference images via personalization techniques. However, they rely heavily on textual descriptions which limits precise control over the generated images. In particular, they fail to support fine-grained and local editing of object attributes like shape, pose and details. The authors identify sketches as an intuitive representation that can provide such control, but directly using pre-trained sketch encoders with personalization methods fails to capture all sketch types accurately.

Proposed Solution:
The authors propose a novel task called sketch concept extraction to address the sketch ambiguity issue. Given one or more sketch-image pairs depicting a novel concept, the goal is to extract a sketch concept that bridges the correspondence between sketches and images. This allows sketch-based control over image generation and editing at a fine-grained level for the concept. 

A two-stage framework called CustomSketching is introduced. A dual-sketch representation is used to differentiate shape (contour) lines and detail (internal) lines to reduce ambiguity. Stage I optimizes a textual token to capture global semantics while freezing the sketch encoder. Stage II fine-tunes the token and sketch encoders to reconstruct local appearance and geometry. Losses include diffusion reconstruction loss, shape loss using attention maps, and regularization loss.

Contributions:

1) Propose the novel task of sketch concept extraction from images to achieve fine-grained sketch-based image editing.

2) Introduce a two-stage framework to extract sketch concepts using dual-sketch representation and optimize both global text and local sketch representations.

3) Create a new dataset covering diverse object categories with sketch-image pairs and edited sketches. Demonstrate applications like concept transfer, multi-concept generation, and style variation.

4) Extensive experiments show the method outperforms adapted personalization baselines for sketch concept extraction. User study and applications showcase the effectiveness for image editing.
