# [Focused Transformer: Contrastive Training for Context Scaling](https://arxiv.org/abs/2307.03170)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the abstract, it seems this paper introduces a new method called "Focused Transformer" to address the issue of models getting distracted by irrelevant context when scaling up the attention context length. The key points I gathered are:- Large language models have great capability to incorporate new information contextually, but their effective context length is limited. Using external memory can help extend the context length. - However, as more documents are added to the memory, the proportion of relevant to irrelevant keys decreases, leading to a "distraction issue" where overlapping keys become hard to distinguish.- This paper proposes the Focused Transformer technique to address the distraction issue. It uses a training process inspired by contrastive learning to shape the key-value space, helping the model differentiate between keys linked to semantically different values.- They demonstrate the method by fine-tuning large OpenLLaMA checkpoints (3B and 7B parameters) which they call LongLLaMA. These models show improved performance on tasks needing longer context, like TREC and WebQS question answering.- The key benefits seem to be: 1) Mitigates the distraction issue for scaling context length 2) Simple to implement by fine-tuning existing models 3) Enables handling very long context lengths like 256k tokens.In summary, the central hypothesis appears to be that using their proposed contrastive-inspired training approach called "Focused Transformer" will improve the structure of the key-value space and allow language models to effectively handle much longer attention context lengths.
