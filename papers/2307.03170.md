# [Focused Transformer: Contrastive Training for Context Scaling](https://arxiv.org/abs/2307.03170)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the abstract, it seems this paper introduces a new method called "Focused Transformer" to address the issue of models getting distracted by irrelevant context when scaling up the attention context length. The key points I gathered are:- Large language models have great capability to incorporate new information contextually, but their effective context length is limited. Using external memory can help extend the context length. - However, as more documents are added to the memory, the proportion of relevant to irrelevant keys decreases, leading to a "distraction issue" where overlapping keys become hard to distinguish.- This paper proposes the Focused Transformer technique to address the distraction issue. It uses a training process inspired by contrastive learning to shape the key-value space, helping the model differentiate between keys linked to semantically different values.- They demonstrate the method by fine-tuning large OpenLLaMA checkpoints (3B and 7B parameters) which they call LongLLaMA. These models show improved performance on tasks needing longer context, like TREC and WebQS question answering.- The key benefits seem to be: 1) Mitigates the distraction issue for scaling context length 2) Simple to implement by fine-tuning existing models 3) Enables handling very long context lengths like 256k tokens.In summary, the central hypothesis appears to be that using their proposed contrastive-inspired training approach called "Focused Transformer" will improve the structure of the key-value space and allow language models to effectively handle much longer attention context lengths.


## What is the main contribution of this paper?

Based on the abstract, the main contribution of this paper seems to be proposing a method called "Focused Transformer" to extend the effective context length of transformer language models. Specifically, the key ideas are:- Using "memory attention layers" that can retrieve relevant information from an external memory bank to augment the model's context.- A training procedure called "crossbatch training" that exposes the model to keys from the current context as well as unrelated contexts, inspired by contrastive learning. This is designed to allow the model to differentiate between relevant and distracting keys.- Applying this method to fine-tune existing large models like OpenLLaMA to extend their context length without modifying their architecture. - Introducing LongLLaMA, versions of OpenLLaMA fine-tuned with this Focused Transformer approach that achieve strong performance on long context tasks like passkey retrieval over 100k+ tokens.So in summary, the main contribution appears to be proposing the Focused Transformer technique to enhance transformer models' ability to handle long contexts by improving the structure of the key-value memory space. The method is shown to be effective at scaling up the context length of large existing models through fine-tuning.


## How does this paper compare to other research in the same field?

Here are a few key ways this paper compares to other related research:- The paper proposes a novel method called Focused Transformer (FoT) for extending the effective context length of transformer language models. This adds to existing work on techniques like sparse attention, conditional computation, and external memory to handle long contexts. The FoT specifically targets the "distraction issue" that arises in multi-document scenarios.- The FoT training procedure draws inspiration from contrastive learning methods like SimCLR. Using in-batch negatives is a common technique in contrastive learning, but the authors apply it in a novel way to the training of memory-augmented transformers. - The paper shows how FoT can be used to effectively fine-tune existing models like LLaMA to extend their context length, without changing the model architecture. This is similar to other works like RETRO and Memorizing Transformer that adapt LLMs through fine-tuning.- FoT achieves strong results on long context tasks like passkey retrieval, extrapolating to 256k tokens after training on just 8k tokens. This demonstrates better generalization beyond the training context length compared to standard fine-tuning.- The released LongLLaMA checkpoints attain state-of-the-art results on datasets like TREC and WebQS that benefit from longer context. This demonstrates the viability of the FoT fine-tuning approach on large models.- The analysis experiments provide insights into design choices like differentiable memory, negatives, and training techniques. The code-based implementation enables easy integration into existing models.Overall, FoT introduces a novel training technique and shows promising results, both through fine-tuning large LLMs like LLaMA and analysis on smaller models. The transformer memory augmentation space remains active, and FoT provides a useful method and insights for extending context length.
