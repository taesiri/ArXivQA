# [A Critical Evaluation of AI Feedback for Aligning Large Language Models](https://arxiv.org/abs/2402.12366)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Reinforcement learning from AI feedback (RLAIF) is a popular method to improve large language models by using a teacher model to provide demonstrations and a critic model to give preferences between responses. 
- However, it's unclear if the complexity of RLAIF is necessary compared to just doing supervised fine-tuning (SFT) on the critic's demonstrations.  

Methodology:
- Compared RLAIF pipelines to simply doing SFT with the critic model across various base language models on the ShareGPT dataset.
- Considered different target distributions for SFT like GPT-3.5, GPT-4, and Claude. Used GPT-4 and Claude models as critics to provide preferences. 
- Evaluated all models with AlpacaEval using the same critic model to provide preferences.

Key Findings:  
- RLAIF outperforms GPT-3.5 SFT when using a stronger critic model like GPT-4 or Claude. However, SFT on the critic's demonstrations matches or exceeds RLAIF performance across base models.
- The common practice of using a weaker teacher (GPT-3.5) for SFT versus a stronger critic (GPT-4) for the RL step explains a lot of the RLAIF gains over SFT.
- When the target SFT distribution uses the critic (e.g. GPT-4 or Claude), RLAIF provides little to no benefit over SFT alone.

Main Contributions:
- Showed that using the right target distribution for SFT is crucial when evaluating RLAIF pipelines. Accounting for SFT distribution affects conclusions about AI feedback techniques.
- Demonstrated supervised fine-tuning can match or exceed RLAIF performance when using the same strong critic model for collecting demonstrations.  
- Provided analysis and hypotheses about why SFT may outperform RLAIF in settings with strong critic models.
- Discussed implications and provided suggestions like regularly updating SFT datasets and investigating more effective uses of AI feedback.
