# Language modeling via stochastic processes

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, it seems like the central research question is: How can contrastive representations be used effectively for generative tasks like long text generation?The authors propose using contrastive learning objectives like Brownian bridge dynamics to learn latent representations that capture text structure and dynamics. They then explore using these latent representations to generate coherent long texts. Specifically, some key aspects the paper investigates are:- Whether contrastive objectives can model local and global text dynamics, as measured by discourse coherence and ability to match document structure when generating text.- How their proposed model "Time Control" compares to baselines like GPT-2 and ablations of their model on long text generation. They aim to show their model can generate coherent long texts by leveraging the contrastive latent representations.- The effects of different components of their model, like the choice of contrastive objective, enforcing bridge dynamics, and decoding from a latent plan. This is analyzed through ablations.So in summary, the central hypothesis is that contrastive latent representations can be beneficial for long text generation, and the paper explores this question by proposing and analyzing their Time Control model. The overall aim is improving coherence in long text generation by capturing text dynamics in the latent space.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contributions seem to be:1. Proposing a new contrastive learning objective based on Brownian bridge stochastic processes for learning sentence embeddings that capture local and global text dynamics. 2. Introducing the Time Control (TC) model that uses the proposed contrastive learning objective to learn latent representations, and then decodes from these latents to generate text.3. Evaluating TC on a range of text domains and showing it can model local coherence through discourse relation classification, global coherence through matching section lengths, and generate more coherent long text compared to baselines.4. Analyzing the components of TC through ablations to understand the importance of the contrastive objective, enforcing Brownian bridge dynamics, explicitly modeling latent dynamics, and decoding from a latent plan.In summary, the key contribution appears to be using stochastic processes like Brownian bridge dynamics to learn latent representations in an unsupervised way, and leveraging these latent representations to improve text generation coherence especially for long texts. The comparisons to strong baselines highlight the benefits of this approach.
