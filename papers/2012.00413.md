# [CPM: A Large-scale Generative Chinese Pre-trained Language Model](https://arxiv.org/abs/2012.00413)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the key research focus is on pre-training a large-scale Chinese language model called CPM (Chinese Pre-trained Language Model) and evaluating its performance on various downstream Chinese NLP tasks. The main hypothesis seems to be that pre-training a very large autoregressive language model on a diverse corpus of Chinese text will result in a model that has strong few-shot and zero-shot abilities on a variety of Chinese NLP tasks like text classification, dialogue, question answering, etc.Specifically, some of the key research questions/goals addressed in the paper include:- How to construct an effective subword vocabulary and optimize training strategies like batch size to adapt a language model architecture like GPT to Chinese text.- Evaluating the impact of model size - they pre-train small, medium and large versions of CPM and test how performance on downstream tasks correlates with number of parameters.- Testing CPM's few-shot and zero-shot abilities on tasks like text classification, QA, dialogue, etc compared to prior Chinese PLMs.- Analyzing the sample outputs qualitatively to get a sense of CPM's language generation capabilities.So in summary, the central focus is on pushing the boundaries of Chinese PLMs by pre-training a very large model on diverse Chinese corpora and evaluating its generalized few-shot/zero-shot abilities. The key hypothesis is that scale (model size + data size) leads to strong language modeling and downstream task performance.


## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the main research focus is on developing and releasing CPM, a large-scale Chinese pre-trained language model. Specifically, the key aspects explored in the paper are:- Constructing CPM, a new Chinese autoregressive language model with generative pre-training on large-scale Chinese corpora. The goal is to create a powerful generative model for Chinese similar to GPT-3.- Pre-training CPM models of various sizes up to 2.6 billion parameters, which the authors state is the largest Chinese pre-trained language model to date.- Developing methods to enable pre-training this large model, including constructing a new Chinese subword vocabulary and using a large batch size.- Evaluating CPM models on a range of Chinese NLP tasks such as text classification, dialogue, QA, and text generation in few-shot and zero-shot settings.- Analyzing the impact of model size on downstream task performance and finding larger CPM models achieve better performance, indicating the benefits of scale for language generation and understanding.So in summary, the central research focus is on pre-training and releasing CPM as a large-scale Chinese language model, and evaluating its capabilities for few-shot learning across diverse language tasks. The key hypothesis seems to be that larger Chinese PLMs will lead to better performance in generative and discriminative language tasks.


## What is the main contribution of this paper?

The main contributions of this paper seem to be:1. Releasing CPM, a large-scale Chinese pre-trained language model with 2.6 billion parameters trained on 100GB of Chinese text data. This is claimed to be the largest Chinese PLM.2. Constructing a new subword vocabulary to better handle Chinese text segmentation and increasing the batch size to 3072 for more stable training of the large model. 3. Showing strong performance of CPM on a variety of Chinese NLP tasks in few-shot and zero-shot settings, including text classification, cloze test, dialogue, question answering, and entity generation. The performance generally improves with the model size.4. Releasing the code and parameters of CPM to facilitate research on large-scale Chinese PLMs.In summary, the main contribution appears to be releasing CPM, a very large Chinese PLM, showing it achieves good performance on diverse Chinese NLP tasks, and releasing the model publicly to advance research in this direction. The modifications to handle Chinese text and scale up training also seem notable.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contributions are:1. The release of CPM, a large-scale Chinese pre-trained language model with 2.6 billion parameters that is trained on 100GB of Chinese data. This is claimed to be the largest Chinese PLM.2. The construction of a new subword vocabulary optimized for Chinese text that contains both words and characters. This is different from previous Chinese PLMs like BERT that use a character-level vocabulary. 3. The use of a larger batch size (3 million tokens) during pre-training compared to previous models to make the training more stable. 4. Experiments showing strong performance of CPM on a variety of Chinese NLP tasks in few-shot and zero-shot settings, including text classification, dialogue, question answering and entity generation. The results suggest CPM has acquired strong language modeling capabilities and world knowledge from the pre-training.5. Analysis showing larger CPM models perform better, indicating the benefit of scale for Chinese PLMs.In summary, the main contribution is the development and release of CPM, a large-scale Chinese PLM that obtains strong few-shot and zero-shot performance on multiple Chinese NLP tasks through generative pre-training on a massive Chinese corpus.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper releases CPM, a large-scale Chinese pre-trained language model with 2.6 billion parameters and 100GB of Chinese training data, which achieves strong performance on downstream NLP tasks like conversation, essay generation, and language understanding in few-shot and even zero-shot settings.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper introduces CPM, a large-scale Chinese pre-trained language model with 2.6 billion parameters trained on 100GB of Chinese text data, which achieves strong performance on downstream Chinese NLP tasks like conversation, essay generation, and language understanding in few-shot and zero-shot settings.


## How does this paper compare to other research in the same field?

Here are a few thoughts on how this paper compares to other research in natural language processing:- The main contribution of this paper is introducing CPM, a large-scale Chinese pre-trained language model. There are other Chinese PLMs like BERT-wwm and ERNIE, but CPM is significantly larger at 2.6B parameters trained on 100GB of Chinese data. The scale of CPM seems unprecedented for a Chinese PLM.- The model architecture of CPM follows GPT, using a Transformer decoder. This is different from BERT-style models which use an encoder-only architecture. The autoregressive nature of CPM makes it more suitable for natural language generation tasks.- The authors perform extensive experiments demonstrating CPM's effectiveness on a wide range of Chinese NLP tasks including text classification, dialogue, QA, cloze test etc. The strong performance in low-shot and zero-shot settings highlights CPM's ability to generalize.- One limitation is that most tasks are formulated as text generation, which fits the autoregressive nature of CPM, but may not be ideal for tasks like QA where concise answers are preferred. The authors acknowledge this issue.- Overall, CPM seems to push the boundaries on scale for Chinese PLMs and shows impressive generalization ability. The model size and training techniques like using word segmentation and larger batches are novel engineering contributions. The comprehensive evaluation benchmarks CPM on a diverse set of NLP tasks. This seems like an impactful paper in the field of Chinese PLMs.In summary, the scale and broad evaluation of CPM distinguishes this paper from prior work on Chinese PLMs. The results demonstrate the benefits of large-scale pre-training for transfer learning, similar to strides made by models like GPT-3. This seems like a valuable contribution to the Chinese NLP community.


## How does this paper compare to other research in the same field?

Here are my thoughts on how this paper compares to other research in the field of Chinese pre-trained language models:- The key contribution of this paper is releasing CPM, a very large Chinese pre-trained language model with 2.6 billion parameters trained on 100GB of Chinese data. This makes CPM one of the largest Chinese PLMs to date. Previous Chinese PLMs tend to be much smaller in scale (e.g. BERT-wwm-chinese has 0.18B parameters).- The model architecture of CPM is similar to GPT/GPT-2, using a Transformer decoder. This is a common architecture choice for autoregressive PLMs. Many other Chinese PLMs like BERT and ERNIE use encoder-only architectures. - The authors create a new subword vocabulary instead of using the BERT wordpiece vocabulary. They argue this is better suited for Chinese since it preserves more semantic meanings of words. This is a valid consideration for adapting PLMs to Chinese.- The training data comprises different sources including encyclopedia, webpages, novels etc. A strength is the inclusion of 100GB of data which is much larger than previous Chinese PLMs. The diversity of data should help the model learn a rich language representation.- They evaluate CPM on a wide range of downstream tasks including text classification, dialogue, QA, text generation etc. The strong performance, especially in low-shot transfer, demonstrates the pre-training learns useful linguistic knowledge that transfers across tasks. This is comparable to evaluations done for models like GPT-3.- One limitation is that most tasks are evaluated only with CPM models without comparison to other Chinese PLMs. Comparisons would better highlight advantages of the larger scale and vocabulary.In summary, CPM pushes forward the state-of-the-art for Chinese PLMs in terms of scale and demonstrates strong transfer performance. Some direct comparisons to other models would further highlight its improvements. But the release of models and code is an important contribution for advancing Chinese NLP.


## What future research directions do the authors suggest?

Based on my reading of the paper, here are some of the key future research directions suggested by the authors:- Train larger models with more training data and parameters to further explore the power of large-scale pre-trained language models for Chinese. They suggest optimizing the training framework for efficiency, and also reducing model size via compression techniques.- Include more diverse training data, such as multi-lingual text corpora and structured knowledge graphs, to enhance model capabilities. They propose exploring new algorithms to train joint models over both texts and knowledge graphs. - Improve performance on downstream tasks through further fine-tuning and optimization, for example on question answering where their models did not yet achieve very high scores.- Build demos and applications to showcase the capabilities of the models on practical language generation and knowledge retrieval tasks.- Conduct human evaluations to assess the quality and usefulness of the model's generated text. - Explore model parallelization and distributed training techniques to enable training even larger models.In summary, the main future directions are around scaling up in terms of model size, training data, and downstream applications, as well as improving training efficiency and integrating structured knowledge. The authors aim to push the boundaries of large Chinese pre-trained language models.


## What future research directions do the authors suggest?

Based on reviewing the paper, here are some of the main future research directions suggested by the authors:- Collecting more training data and increasing model size to further explore the capabilities of large-scale pre-trained models on Chinese tasks. They mention trying to optimize the training framework to accelerate this process.- Including diverse data to enhance model performance, such as adding a multi-lingual corpus for a large-scale Chinese-centered multi-lingual model, and exploring methods to train joint models on both texts and knowledge graphs.- Implementing sparse attention mechanisms in the future for greater efficiency.- Exploring ways to improve performance on supervised downstream tasks through techniques like better fine-tuning approaches.- Reducing model size through model compression methods.- Improving the model's ability to generate short, precise answers for question answering instead of long, repetitive responses.- Enhancing the input templates and formats to better suit different downstream tasks.- Adding more training data diversity to improve performance on certain tasks like question answering that the current pre-training data may not be ideal for.In summary, the main directions are scaling up the models with more data and parameters, incorporating diverse data sources, improving fine-tuning and compression, adjusting model architectures for efficiency, and tailoring the pre-training more towards certain downstream tasks.


## Summarize the paper in one paragraph.

Here is a one paragraph summary of the key points from the paper:The paper introduces CPM, a large-scale Chinese pre-trained language model with 2.6 billion parameters trained on 100GB of Chinese text data. CPM uses a Transformer-based architecture and was trained using generative pre-training objectives. To adapt to Chinese text, the authors built a new subword vocabulary and used a larger batch size compared to models like GPT-3. The authors evaluated CPM on a diverse set of Chinese NLP tasks including text classification, idiom cloze tests, dialogue generation, question answering, and entity generation in few-shot and zero-shot settings. Across tasks, CPM showed strong performance, with increasing gains as model size grew, demonstrating its proficiency in both Chinese language generation and understanding. Ablations also revealed that CPM can achieve high performance from just a few examples. The authors plan to further improve CPM by adding more data, increasing model size, and incorporating knowledge graphs. Code and pretrained models are publicly available to facilitate research. Overall, CPM represents an important step towards large-scale pretrained models for Chinese NLP.


## Summarize the paper in one paragraph.

Here is a one paragraph summary of the key points from the paper:The paper presents a new Chinese pre-trained language model called CPM (Chinese Pre-trained Language Model) which has 2.6 billion parameters and was trained on 100GB of Chinese text data. CPM uses a Transformer-based architecture similar to GPT and adapts it for Chinese text by constructing a new subword vocabulary and using a larger batch size for more stable training. The authors pre-trained CPM models of three different sizes and evaluated them on a diverse set of Chinese NLP tasks including text classification, idiom cloze tests, dialogue generation, question answering, and entity generation. Experiments show CPM achieves strong performance on many tasks, especially in few-shot and zero-shot settings, demonstrating its ability to generate fluent Chinese text and perform language understanding. Larger CPM models generally perform better, indicating the benefits of scale for pre-trained language models. The code and models are publicly released to facilitate research on large-scale Chinese language models.


## Summarize the paper in two paragraphs.

Here is a two paragraph summary of the paper:The paper introduces CPM, a large-scale Chinese pre-trained language model. CPM is an autoregressive transformer model trained on 100GB of Chinese text data. The model comes in three sizes: small with 109M parameters, medium with 334M parameters, and large with 2.6B parameters. The authors evaluate CPM on a variety of Chinese natural language tasks in few-shot and zero-shot settings. Experiments show CPM achieves strong performance on text classification, question answering, dialogue generation, and entity generation compared to previous Chinese PLMs. Performance generally improves with model size, indicating larger models have greater proficiency in language generation and understanding. The authors plan to continue scaling CPM and incorporating diverse training data like knowledge graphs. Code and parameters are available to facilitate research on large-scale Chinese PLMs.


## Summarize the paper in two paragraphs.

Here is a two paragraph summary of the paper:The paper introduces CPM, a large-scale Chinese pre-trained language model with generative pre-training on large Chinese corpora. CPM has 2.6 billion parameters and was trained on 100GB of Chinese text data from various sources including encyclopedia, news, novels, and Q&A. The authors construct a new subword vocabulary to handle Chinese text and increase the batch size to make training more stable. The paper demonstrates CPM's strong performance on a variety of Chinese NLP tasks in few-shot and zero-shot settings, including text classification, idiom cloze test, dialogue generation, question answering, and entity generation. Experiments show that larger CPM models perform better on most datasets, indicating the benefits of scale for pre-trained language models. The authors plan to further improve CPM by adding more training data, increasing model size, optimizing the training framework, and incorporating diverse data like knowledge graphs. The code and parameters for CPM are publicly available.


## Summarize the main method used in the paper in one paragraph.

Here is a one paragraph summary of the main method used in the paper:The paper proposes a Chinese pre-trained language model called CPM, which is trained on a large corpus of Chinese text data totaling 100GB. CPM uses a Transformer-based autoregressive language model architecture similar to GPT. The key contributions include constructing a new subword vocabulary to handle Chinese text, increasing the batch size to 3072 tokens for more stable training, and using model parallelism to train a large model with 2.6 billion parameters. CPM is evaluated on a range of Chinese natural language processing tasks including text classification, dialogue generation, question answering, and entity generation in few-shot and zero-shot settings. The results show CPM achieves strong performance on many tasks, demonstrating its ability to generate fluent Chinese text and perform language understanding after pre-training on a large unlabeled Chinese corpus. The model size is shown to be important, with the 2.6B parameter CPM outperforming smaller models in the few-shot settings.


## Summarize the main method used in the paper in one paragraph.

The paper proposes a large-scale Chinese pre-trained language model called CPM. The key points are:1. CPM is an autoregressive Transformer-based model trained with generative pre-training objectives, similar to GPT. 2. Three CPM models of different sizes are pre-trained: Small (109M parameters), Medium (334M parameters) and Large (2.6B parameters).3. A new subword vocabulary is constructed from word segmented text to better handle Chinese. The vocabulary contains both words and characters.4. The training batch size is increased to 3072 to stabilize training. Model parallelism is used to partition the Large model across GPUs. 5. 100GB of Chinese text data is used for pre-training, including encyclopedia, news, novels and Q&A data.6. CPM demonstrates strong performance on downstream Chinese NLP tasks like text classification, dialogue, QA and entity generation in few-shot and zero-shot settings. Larger CPM models generally perform better, showing the benefits of scale.In summary, the paper pre-trains large Transformer-based generative language models on Chinese text, and shows they can achieve strong few-shot performance on diverse downstream tasks by leveraging the pre-trained knowledge. Scaling model size is shown to improve performance.
