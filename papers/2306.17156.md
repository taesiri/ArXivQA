# [Generative AI for Programming Education: Benchmarking ChatGPT, GPT-4,   and Human Tutors](https://arxiv.org/abs/2306.17156)

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper addresses is: How do state-of-the-art generative AI and large language models perform on a comprehensive set of programming education scenarios compared to human tutors? The key hypotheses examined are:1) GPT-4 will substantially outperform ChatGPT (based on GPT-3.5) across the programming education scenarios.2) GPT-4 will come close to human tutors' performance for several programming education scenarios. 3) There will be some scenarios and specific problems where GPT-4 still struggles compared to human tutors.In particular, the paper conducts a systematic study to benchmark ChatGPT and GPT-4 models against human tutors for six programming education scenarios: program repair, hint generation, grading feedback, pair programming, contextualized explanation, and task synthesis. The goal is to comprehensively evaluate these state-of-the-art models for a diverse set of roles they could play to support programming education, and identify areas where they excel versus struggle compared to human experts. The central research question drives this comprehensive benchmarking study to assess the capabilities of LLMs for programming education.


## What is the central research question or hypothesis that this paper addresses?

The central research question this paper addresses is: How do the state-of-the-art generative AI and large language models perform in comparison to human tutors for various scenarios relevant to introductory programming education?The authors systematically evaluate two models - ChatGPT (based on GPT-3.5) and GPT-4 - and compare their performance with human tutors for six key scenarios: program repair, hint generation, grading feedback, pair programming, contextualized explanation, and task synthesis. The main hypothesis seems to be that the latest generative models like GPT-4 will perform much better than previous models like ChatGPT and will come close to human tutors' performance for many of these programming education scenarios. The paper seeks to comprehensively benchmark these models to identify areas where they do well versus scenarios where they still struggle in comparison to human experts.In summary, the key research question is about systematically evaluating and comparing the performance of state-of-the-art generative models versus human tutors for diverse programming education scenarios relevant for roles like digital tutors, teaching assistants, and collaborators. The hypothesis is that the latest models will match or exceed previous models and come close to human-level performance on several of these scenarios.
