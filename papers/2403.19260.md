# [NaijaHate: Evaluating Hate Speech Detection on Nigerian Twitter Using   Representative Data](https://arxiv.org/abs/2403.19260)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Hate speech online is a major global issue, but detection models are typically developed on US data and fail to generalize to other contexts like Nigeria.  
- Existing hate speech datasets also tend to oversample hateful content, raising concerns about overestimating real-world performance.
- It's unclear how well different modeling approaches like supervised learning, zero-shot learning, and domain-adaptive pretraining perform when adapted to the low-resource Nigerian context. 

Proposed Solution:
- Introduce NaijaHate, the first Nigerian Twitter dataset with 35K tweets annotated for hate speech, including a representative sample for evaluation.
- Develop NaijaXLM-T, a pretrained language model tailored to Nigerian Twitter.  
- Evaluate performance of different models on the representative sample to get real-world estimates.
- Assess performance of zero-shot learning with GPT-3.5 against supervised learning baselines.
- Compare domain-adapted models like NaijaXLM-T against generic pretrained models.
- Examine impact of finetuning data diversity through stratified vs. active learning.
- Discuss feasibility of human-in-the-loop moderation by estimating cost-recall tradeoff.

Main Contributions:
- NaijaHate dataset with representative evaluation set 
- NaijaXLM-T model adapted to Nigerian Twitter
- Evaluation showing supervised learning outperforms zero-shot learning
- In-domain pretraining & finetuning substantially boosts performance 
- More diverse finetuning data leads to large gains, especially for non-adapted models
- Reviewing 1% of flagged posts enables 60% recall for hate speech moderation

Overall the paper demonstrates low real-world performance of existing models in the Nigerian context and shows domain-adaptive pretraining & finetuning and increased finetuning diversity enable sizable performance gains on representative data. The analysis also discusses the constraints of human-in-the-loop content moderation at scale.
