# [Do Large Language Models Latently Perform Multi-Hop Reasoning?](https://arxiv.org/abs/2402.16837)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key aspects of the paper:

Problem: 
The paper investigates whether large language models (LLMs) can perform latent multi-hop reasoning when completing complex prompts that require connecting multiple facts, such as "The mother of the singer of 'Superstition' is". Specifically, it examines if LLMs can (1) identify implicit entities like figuring out "the singer of 'Superstition'" refers to Stevie Wonder and (2) use known attributes of that entity like knowing Stevie Wonder's mother is Lula. 

Proposed Solution:
The paper proposes metrics to quantify the extent of multi-hop reasoning in LLMs. For the first hop, it measures the model's internal recall of the bridge entity using an "entity recall score". For the second hop, it checks if increasing entity recall leads to more consistency between the LLM's completions of two-hop vs one-hop prompts querying the same fact. The conjunction of both effects on a prompt signals latent multi-hop reasoning.

Experiments are run on the introduced TwoHopFacts dataset of 45K prompt pairs over 52 different fact compositions. Three model sizes of LLaMA-2 are evaluated - 7B, 13B and 70B parameters.

Key Findings:
- There is substantial evidence (71% cases) that LLMs perform the first reasoning hop of identifying implicit entities. This effect grows stronger with scale.  
- There is more moderate evidence (61% cases) that LLMs perform the second hop of utilizing remembered facts. This effect does not grow with scale.
- Considering both hops together, 40% of cases show evidence of end-to-end latent multi-hop reasoning.
- Multi-hop reasoning is highly context-dependent - up to 23% fact types see reasoning in over 80% of cases, while others see little to no reasoning.

Main Contributions:
- Framework to probe latent multi-hop reasoning in LLMs using novel scoring metrics 
- Evidence that the capability exists in LLMs but is context-dependent 
- Analysis of how the capability scales and potential gaps that need addressing
- New TwoHopFacts dataset over diverse fact compositions

The paper significantly advances our understanding of latent reasoning in LLMs. The context-dependency and lack of scaling highlight opportunities for future work on strengthening multi-hop inference.


## Summarize the paper in one sentence.

 Here is a one-sentence summary of the paper:

The paper investigates whether large language models perform latent multi-hop reasoning when processing complex prompts that require connecting implicit knowledge, finding moderate evidence for the first hop but weaker evidence for the full reasoning pathway.


## What is the main contribution of this paper?

 The main contributions of this paper are:

1. It addresses the question of latent multi-hop reasoning abilities in large language models (LLMs), establishes a framework for investigating this question, and provides existential evidence of such abilities. 

2. It constructs a new dataset called TwoHopFacts based on Wikidata, consisting of 45,595 one-hop and two-hop prompts of 52 different fact composition types to study latent multi-hop reasoning.

3. It proposes two novel metrics - internal entity recall score and consistency score - as proxies for measuring the degree of an LLM's internal recall of an entity and the degree of an LLM's utilization of its knowledge about a bridge entity's attribute. 

4. It proposes a mechanism to identify a latent reasoning pathway even when it is not the most salient pathway determining the prediction, by measuring the relative frequency of expected causal effects.

5. It finds strong evidence of latent multi-hop reasoning for certain fact composition types, moderate evidence on average across prompts, and little or no scaling of the evidence from 7B to 70B parameters. This suggests potential limitations of current LLM architectures and pretraining approaches in performing latent multi-hop reasoning.

In summary, the main contribution is establishing a framework and methodology to investigate and provide existential evidence for latent multi-hop reasoning abilities in LLMs using a newly constructed dataset and novel metrics. The findings also highlight potential areas for improving reasoning abilities in future work.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and keywords associated with it are:

- Large language models (LLMs)
- Multi-hop reasoning
- Latent reasoning pathways
- Fact composition types
- Internal entity recall score 
- Consistency score
- First hop of reasoning
- Second hop of reasoning  
- Dataset construction from Wikidata
- Transformers
- Model scaling
- Parameter efficiency
- Knowledge storage and retrieval
- Knowledge traversal
- Model controllability

The paper investigates whether large language models like LLaMA perform latent multi-hop reasoning when processing complex prompts that require connecting facts stored in the models' parameters. It proposes metrics to quantify the internal recall and utilization of knowledge to identify evidence of such reasoning, constructs a dataset for evaluating on diverse factual compositions, and studies the effect of model scaling. The key terms reflect this focus on reasoning, knowledge and scaling in large language models.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in this paper:

1. The paper proposes two key research questions (RQ1 and RQ2) to investigate latent multi-hop reasoning in LLMs. What are the specifics of these research questions and how do they aim to uncover evidence of multi-hop reasoning?

2. The paper introduces a new dataset called TwoHopFacts. What is the composition of this dataset and what considerations went into its construction to enable the study of diverse types of multi-hop reasoning? 

3. The paper proposes a metric called "internal entity recall score" to approximate the LLM's internal recall of a bridge entity. What is the mathematical definition of this metric and what aspects of LLM processing does it aim to capture?

4. The paper performs an experiment to validate the "internal entity recall score" metric as a reasonable proxy of entity recall. What is the setup of this experiment and what were the key findings?

5. The paper defines another metric called "consistency score" to quantify the similarity between an LLM's response to a two-hop vs one-hop prompt. What properties led to the choice of this metric?  

6. An experiment is performed to justify the use of consistency score to evaluate the LLM's utilization of knowledge about a bridge entity's attribute. What is the design of this experiment and what insights did it provide?

7. When analyzing the second hop of reasoning, the paper uses a causal intervention approach by modifying the LLM's internal representations. Explain this approach and how it allows investigating latent reasoning pathways.  

8. What were the key findings from the analysis of the first and second hops individually? Were there any surprising results or insights gained?

9. How exactly does the paper combine the analysis from RQ1 and RQ2 to identify latent multi-hop reasoning paths? What metrics or outcomes are considered to determine success/failure of each hop?

10. The paper discusses potential limitations related to the choice of metrics, the narrow interpretation of reasoning pathways, dataset construction, etc. What are 2-3 key limitations and how might they impact conclusions or suggest directions for future work?
