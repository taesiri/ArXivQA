# [Do Large Language Models Latently Perform Multi-Hop Reasoning?](https://arxiv.org/abs/2402.16837)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key aspects of the paper:

Problem: 
The paper investigates whether large language models (LLMs) can perform latent multi-hop reasoning when completing complex prompts that require connecting multiple facts, such as "The mother of the singer of 'Superstition' is". Specifically, it examines if LLMs can (1) identify implicit entities like figuring out "the singer of 'Superstition'" refers to Stevie Wonder and (2) use known attributes of that entity like knowing Stevie Wonder's mother is Lula. 

Proposed Solution:
The paper proposes metrics to quantify the extent of multi-hop reasoning in LLMs. For the first hop, it measures the model's internal recall of the bridge entity using an "entity recall score". For the second hop, it checks if increasing entity recall leads to more consistency between the LLM's completions of two-hop vs one-hop prompts querying the same fact. The conjunction of both effects on a prompt signals latent multi-hop reasoning.

Experiments are run on the introduced TwoHopFacts dataset of 45K prompt pairs over 52 different fact compositions. Three model sizes of LLaMA-2 are evaluated - 7B, 13B and 70B parameters.

Key Findings:
- There is substantial evidence (71% cases) that LLMs perform the first reasoning hop of identifying implicit entities. This effect grows stronger with scale.  
- There is more moderate evidence (61% cases) that LLMs perform the second hop of utilizing remembered facts. This effect does not grow with scale.
- Considering both hops together, 40% of cases show evidence of end-to-end latent multi-hop reasoning.
- Multi-hop reasoning is highly context-dependent - up to 23% fact types see reasoning in over 80% of cases, while others see little to no reasoning.

Main Contributions:
- Framework to probe latent multi-hop reasoning in LLMs using novel scoring metrics 
- Evidence that the capability exists in LLMs but is context-dependent 
- Analysis of how the capability scales and potential gaps that need addressing
- New TwoHopFacts dataset over diverse fact compositions

The paper significantly advances our understanding of latent reasoning in LLMs. The context-dependency and lack of scaling highlight opportunities for future work on strengthening multi-hop inference.
