# [LLMLingua-2: Data Distillation for Efficient and Faithful Task-Agnostic   Prompt Compression](https://arxiv.org/abs/2403.12968)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Existing task-agnostic prompt compression methods rely on information entropy from causal language models, which may fail to capture all essential information needed for effective compression. 
- They are not explicitly optimized for the compression task.

Proposed Solution:
- Construct an extractive text compression dataset by distilling knowledge from GPT-4 to compress prompts without losing crucial information or introducing hallucinated content.
- Formulate prompt compression as a token classification task to guarantee faithfulness. Use a Transformer encoder to leverage full bidirectional context.
- Train smaller models like XLM-RoBERTa-large and mBERT to explicitly optimize for compression objective for lower latency.

Contributions:
- Introduce an extractive text compression dataset derived from MeetingBank transcripts using a data distillation procedure.
- Approaching prompt compression as a token classification task allows capturing essential information from full bidirectional context.
- Despite small size, proposed models show significant gains over baselines and robust generalization ability across different target LLMs. 
- Proposed compressor is 3x-6x faster than prior methods, while accelerating end-to-end latency by 1.6x-2.9x.

In summary, the key ideas are using data distillation to construct a text compression dataset, formulating prompt compression as a token classification task, and training smaller models to explicitly optimize the compression objective. This leads to performance gains, faster compression, and better generalization across domains and target LLMs.
