# FaceShop: Deep Sketch-based Face Image Editing

## What is the central research question or hypothesis that this paper addresses?

This paper presents a novel system for sketch-based face image editing. The central goal is to enable users to intuitively edit face images by providing sketch and color strokes as input. The key research question is how to train a deep neural network to render high-quality and realistic image edits that correspond to the user's sketch and color input. The main hypothesis is that combining conditional image completion with image translation in a unified framework will lead to a system that can synthesize contextually coherent image edits with fine-grained user control. More specifically:- Training the network on image completion with additional sketch and color constraints will enable control over the synthesized image region via the user strokes.- Transforming photos to an ambiguous sketch domain during training will improve generalization to imperfect user sketches.- Jointly training on image completion and translation tasks will enable the model to leverage both image context and user strokes to render realistic image edits.In summary, the core hypothesis is that a conditional image completion framework trained with an appropriate ambiguous sketch domain will lead to a system that can generate high-quality, controllable image edits from sketch and color strokes provided by users. The paper presents experiments and results to validate this hypothesis.


## What is the main contribution of this paper?

This paper presents a novel system for sketch-based face image editing. The main contributions are:- A new framework that enables intuitive local image editing by sketching strokes on top of an image. This allows users to edit images by providing geometry and color constraints.- A sketch domain and training procedure for convolutional neural networks that generalizes well to real user sketches. This is achieved by introducing ambiguities between the sketch domain and real image structures during training.- Formulating the problem as a combination of image completion and image translation. The system is trained end-to-end on these two tasks jointly. - A network architecture and training procedure that enables stable training on high-resolution images.- An intuitive sketch-based interface that allows interactive editing in real-time.- The ability to perform smart copy-paste editing by copying sketch representations from one image to another. This provides robustness to illumination and texture mismatches.- High-quality editing results on a variety of operations like changing facial features, expressions, and colors. The system produces globally consistent edits without visible seams or other artifacts.In summary, the key contribution is a new deep learning framework for interactive local image editing using sketching. This is enabled by technical contributions like the sketch domain, network architecture, and training procedure. The interface allows easy exploration of editing options and facilitates an efficient workflow.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper proposes a sketch-based image editing system that combines image completion and image translation techniques using a convolutional neural network to enable intuitive local image editing through user-provided sketches and color strokes.


## How does this paper compare to other research in the same field?

This paper presents a novel approach for sketch-based face image editing using deep learning. Here are some key ways it compares to other related work:- Compared to unconditional image completion techniques like Iizuka et al. [1], this paper introduces sketch and color constraints to allow user control over the synthesis. The results are also higher resolution and have fewer artifacts. - Compared to image translation approaches like pix2pix [2], formulating the problem as conditional image completion is more robust. Direct translation struggles with ambiguities in converting the entire image to sketches, while this approach focuses only on completing the masked region.- Compared to specialized techniques like eye inpainting [3], the proposed approach is more flexible by supporting editing of any facial regions. The smart copy-paste also subsumes approaches like eye opening.- The sketch domain processing introduces ambiguities that improve generalization to real hand-drawn sketches compared to using raw edges. This allows the system to be more robust to imperfect user inputs.- The proposed training procedure and architectures enable stable, high-quality training on higher resolution images than prior GAN-based approaches.In summary, this paper combines concepts from image completion and translation in a novel way, leading to a flexible sketch-based editing system. The sketch domain and training process are designed to improve generalization. The results showcase significant improvements in quality, resolution, and flexibility compared to prior work.[1] Iizuka et al., Globally and Locally Consistent Image Completion, SIGGRAPH 2017. [2] Isola et al., Image-to-Image Translation with Conditional Adversarial Networks, CVPR 2017.[3] Dolhansky and Ferrer, Eye Inpainting with Exemplar Generative Adversarial Networks, CVPR 2017.


## What future research directions do the authors suggest?

Based on my reading of the paper, some of the main future research directions suggested by the authors include:- Training the system on more diverse and higher resolution image datasets. The current model is trained only on face images at 512x512 resolution. Training on more varied image data at higher resolutions could improve the flexibility and quality of the system.- Exploring additional user interaction tools and workflows. The current interface uses sketching for editing, but the authors suggest exploring other interaction modalities that could enable more intuitive workflows.- Extending the system to video data. The current work focuses on image editing, but the authors propose exploring sketch-based editing techniques for video as well. Developing spatio-temporal models could enable video editing tools.- Improving generalization capability to unseen content. The model currently struggles with generating completely new content like hairstyles. Exploring techniques to improve imagination and generalization could help address these limitations. - Combining with 3D modeling. The authors suggest combining the sketch-based interface with 3D modelling to enable editing from different viewpoints. This could improve editing flexibility.- Optimization and efficiency improvements. To enable real-time editing, optimizations to improve run-time efficiency could be explored, such as network compression, pruning, and optimization strategies.In summary, the main future directions are developing the system into a more flexible, higher-quality, and intuitive tool by expanding the data, interactions, output modalities, and efficiency of the underlying technology. Advancing these aspects could lead to more practical sketch-based editing capabilities.


## Summarize the paper in one paragraph.

Here is a one paragraph summary of the key points from the paper:The paper presents a novel system for sketch-based face image editing that enables users to edit images intuitively by sketching a few strokes on a region of interest. The system features an intuitive interface with tools to provide geometry and color constraints as user-drawn strokes. It also supports a copy-paste mode to edit images using parts from other images without sketching. The interface runs in real-time to facilitate an iterative workflow. The backend is a convolutional neural network trained on conditional image completion using adversarial loss. It incorporates image context and user strokes to synthesize high-quality face images, unifying image completion and translation. Key contributions are the sketch domain, network architecture, and training procedure that generalize well to user input for high-quality synthesis without post-processing. The system produces globally consistent, seamless local edits. Overall, it enables an efficient sketch-based workflow for predictive high-quality local image editing.
