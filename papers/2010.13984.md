# [Interpretation of NLP models through input marginalization](https://arxiv.org/abs/2010.13984)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading, the key points of this paper are:

- It raises the issue of out-of-distribution (OOD) problems that can arise when interpreting NLP models using existing token erasure schemes. These schemes replace tokens with predefined values like zero, which can push the input sentence out of the training data distribution and yield misleading interpretations. 

- To address this issue, the paper proposes a new interpretation method called input marginalization. The key idea is to marginalize (average over) the contribution of all probable candidate tokens for a position, weighted by their likelihoods. 

- The likelihoods are obtained using masked language modeling from BERT. This allows considering the context when estimating the likelihood of candidate tokens.

- The proposed input marginalization method is applied to interpret various NLP models like CNNs, LSTMs and BERT trained for sentiment analysis and natural language inference.

- Both qualitative examples and quantitative experiments demonstrate that the proposed method provides more faithful interpretations compared to existing erasure schemes. It avoids the OOD issue and highlights appropriate tokens.

In summary, the central hypothesis is that input marginalization can produce superior interpretations of NLP models by avoiding OOD problems faced by existing erasure schemes. The experiments validate this hypothesis.


## What is the main contribution of this paper?

 Here are the main contributions of this paper:

- The paper raises the out-of-distribution (OOD) problem that can arise when interpreting NLP models through existing token erasure schemes like replacing tokens with zeros. Existing methods can push inputs out of the training distribution, yielding misleading interpretations. 

- To avoid the OOD problem, the paper proposes a new interpretation method called input marginalization. This involves marginalizing each token out by considering the likelihoods of all candidate replacement tokens based on masked language modeling using BERT. 

- The paper applies the proposed input marginalization method to interpret various NLP models trained for sentiment analysis and natural language inference. Experiments demonstrate it can provide more faithful interpretations than existing approaches.

- The paper also proposes a metric called AUC_rep to quantitatively evaluate and compare interpretation methods by gradually replacing tokens and measuring how quickly the prediction probability drops.

In summary, the main contribution is identifying the OOD problem with existing interpretation methods for NLP models, and proposing input marginalization as a solution that considers the full distribution of replacements when erasing tokens to avoid misleading interpretations. The method is demonstrated on multiple NLP tasks and models.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the key point from the paper:

The paper proposes a new method to interpret natural language processing models by marginalizing out each token based on its likelihood from BERT's masked language model, avoiding the out-of-distribution issue caused by existing erasure-based interpretation methods.


## How does this paper compare to other research in the same field?

 Here are a few key ways this paper compares to other research in interpreting NLP models:

- It focuses on addressing the out-of-distribution (OOD) problem that can arise from existing input erasure schemes for interpreting NLP models. Many previous works have not explicitly addressed this issue. 

- It proposes using input marginalization to mitigate the OOD problem. This is different from prior works that simply erase/replace tokens with a predefined value like zero. Marginalizing allows considering multiple candidate tokens weighted by their likelihoods.

- It utilizes masked language modeling from BERT for accurately estimating token likelihoods in the marginalization. This provides a more powerful likelihood model compared to simple priors or assumptions made in some prior works.

- It evaluates the faithfulness of interpretations more rigorously, using metrics like AUC_rep and comparisons to human annotations. Many prior works have evaluated interpretability methods less thoroughly.

- The input marginalization approach is model-agnostic and task-agnostic, demonstrated across CNN, LSTM and BERT models and on both sentiment analysis and natural language inference tasks.

- The analysis enabled by the interpretation method provides additional insights into model behaviors and differences between model architectures. This demonstrates the broader value of interpretability.

Overall, this paper makes useful contributions in formalizing the OOD problem in NLP interpretability, proposing a practical solution, and demonstrating its utility over a range of models and tasks. The evaluations are more rigorous than many past works. It represents an advance in the state-of-the-art for interpreting NLP models faithfully.


## What future research directions do the authors suggest?

 Based on my reading of the paper, some of the main future research directions suggested by the authors are:

- Apply the proposed interpretation method to interpret other types of NLP models beyond sentiment analysis and natural language inference, such as for neural machine translation and visual question answering. The model-agnostic nature of their method makes it potentially applicable to many different models and tasks.

- Interpret more advanced state-of-the-art NLP models like XLNet and ELECTRA using their proposed approach. As the authors mention, it would be meaningful to interpret these complex models.

- Improve the likelihood modeling performance for candidate token probabilities. The authors experimentally analyzed that the interpretation results are affected by the quality of the likelihood modeling. Better modeling could lead to even more faithful interpretations.

- Investigate multi-token attribution in more depth. The authors proposed a method for measuring joint contribution of multiple tokens, but did not extensively analyze it. More exploration could be done in this direction.

- Develop more rigorous quantitative evaluation metrics for evaluating faithfulness of interpretation methods, beyond just using word-level sentiment tags or deletion curves.

- Apply the idea of marginalizing out tokens rather than erasing to zero to other interpretation methods besides the weight of evidence, to mitigate out-of-distribution issues there as well.

In summary, the main future directions are applying the proposed technique to broader models and tasks, improving the likelihood modeling, deeper investigation of multi-token attribution, developing better evaluation metrics, and extending the marginalization idea to other interpretation methods. Advancing research in these areas could lead to better understanding and interpretation of complex NLP models.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the paper:

The paper proposes a new method to interpret predictions from deep neural network models for natural language processing. Existing interpretation methods measure the change in prediction probability when erasing each token, by replacing it with a default value like zero. However, this causes the input to be out-of-distribution from the training data, yielding misleading interpretations. To address this, the authors propose marginalizing each token out by considering all possible replacements and their likelihoods based on masked language modeling using BERT. This avoids the out-of-distribution issue. They apply the proposed input marginalization method to interpret various NLP models on sentiment analysis and natural language inference tasks. Experiments show it provides more faithful interpretations than existing erasure-based methods, enabling better understanding of model rationales.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the paper:

The advent of deep learning has greatly improved the performances of natural language processing (NLP) models. However, as these models become more complex, it becomes more difficult to understand their internal reasoning behind predictions. To build trust and enable high-stakes usage of deep neural networks (DNNs) for NLP, interpretability is crucial. To provide interpretations, several methods measure how a model's prediction changes when tokens are erased from the input. Existing techniques simply replace each token with a predefined value like zero, pushing the input out of the training distribution. This out-of-distribution (OOD) problem yields misleading interpretations, as DNNs assign lower probabilities to OOD inputs. 

This paper raises the OOD issue of existing interpretation methods for the first time in NLP research. To avoid it, they propose input marginalization which considers the likelihoods of all candidate replacement tokens when erasing each token. This mitigates the OOD problem. They model likelihoods using BERT's masked language modeling and propose adaptive truncation for efficiency. Experiments interpreting models for sentiment analysis and natural language inference show their method yields more faithful attribution scores. Analysis reveals insights into model decisions and differences between LSTM and BERT. Overall, the proposed input marginalization enables better understanding of NLP models by providing interpretability without the OOD problem of existing approaches.


## Summarize the main method used in the paper in one paragraph.

 Here is a one paragraph summary of the main method used in the paper:

The paper proposes a new method called input marginalization to interpret natural language processing (NLP) models by calculating the attribution score of each token. It measures how much the prediction changes when a token is marginalized out instead of simply erased. Specifically, it marginalizes a token by considering all possible candidate tokens and their likelihoods at that position modeled by BERT's masked language modeling (MLM). The likelihood indicates how probable it is for a candidate token to appear in that position. The attribution score is measured by the difference between the original prediction probability and the probability after marginalizing the token out. This avoids the out-of-distribution problem of existing erasure-based methods where sentences lie outside the training distribution after erasing tokens. The proposed input marginalization mitigates this issue by marginalizing tokens based on modeling token likelihoods using BERT MLM. The method is applied to interpret various NLP models like CNN, LSTM and BERT for sentiment analysis and natural language inference tasks.


## What problem or question is the paper addressing?

 The paper is addressing the problem that existing methods for interpreting natural language processing (NLP) models can yield misleading interpretations. Specifically, it raises an issue with the common approach of erasing or zeroing out tokens to understand their importance, arguing this causes out-of-distribution (OOD) samples that lead to inaccurate attribution scores. 

The key question the paper tries to answer is: How can we interpret NLP models without causing OOD problems?

To summarize, the main points made in the paper are:

- Interpretability is important for trusting and debugging NLP models. Existing erasure methods like replacing tokens with zero can cause OOD issues.

- The paper proposes input marginalization to avoid OOD problems. This involves marginalizing out each token by considering all possible replacements and their likelihoods modeled using BERT's masked language model. 

- Experiments show their method yields more plausible attributions than zero erasure, with lower AUC when gradually replacing tokens based on importance.

- Analysis demonstrates how input marginalization enables comparing model rationales and debugging failures through more faithful interpretations.

So in summary, the paper raises an issue with current interpretation methods for NLP models and proposes input marginalization as a way to create more valid interpretations without causing OOD problems.


## What are the keywords or key terms associated with this paper?

 Based on reading the paper, some of the key terms and concepts include:

- Interpretability of NLP models - The paper focuses on making deep neural networks for NLP more interpretable and explaining their predictions.

- Input erasure - Existing methods measure the importance of input tokens by erasing them (e.g. replacing with zero) and seeing how the prediction changes. 

- Out-of-distribution problem - The paper argues these input erasure methods cause out-of-distribution issues where the modified input deviates from the training data.

- Input marginalization - The proposed method that marginalizes over probable candidate tokens for each input instead of erasing to a single value.

- Masked language modeling - Uses BERT's masked LM to model token likelihoods for marginalization. 

- Attribution scores - The importance scores assigned to each input token based on how much the prediction changes when marginalized.

- Sentiment analysis - Interpret NLP models for binary sentiment classification on movie reviews.

- Natural language inference - Interpret models for textual entailment on the SNLI dataset.

- Faithfulness - Evaluating how well attribution scores match human intuition and analyze model rationale.

So in summary, the key focus is improving the interpretability of NLP models by addressing out-of-distribution issues with existing input erasure approaches via a new input marginalization method. The concepts like attribution scores, masked LM, and model faithfulness are central.


## Based on the paper, what questions should we ask to create a comprehensive summary of it? Please list at least 10 questions.

 Here are 10 potential questions to ask to create a comprehensive summary of the paper:

1. What is the main problem the paper seeks to address?

2. What limitations exist with current methods for interpreting NLP model predictions?

3. What is the key idea proposed in the paper to address the limitations?

4. How does the proposed method of input marginalization work? 

5. How is the likelihood of candidate tokens modeled and calculated?

6. How is the attribution score for each token calculated? 

7. What modifications were made to make the method efficient computationally?

8. What metrics were used to evaluate the proposed method?

9. What were the main results of applying the method to sentiment analysis and natural language inference models?

10. How does the proposed approach compare qualitatively and quantitatively to existing interpretation methods?


## Design 10 in-depth questions about method proposed in this paper and avoid simple question.

 Here are 10 potential in-depth questions about the method proposed in the paper:

1. The paper proposes marginalizing out each token to avoid the out-of-distribution (OOD) problem caused by existing erasure schemes. How does marginalizing help mitigate the OOD issue versus simply erasing/removing tokens? What is the theoretical basis for why this helps?

2. The paper uses BERT's masked language modeling (MLM) to estimate the likelihoods of candidate tokens during marginalization. Why is the MLM well-suited for this task? How does the deep bidirectionality of BERT enable accurate likelihood modeling? 

3. During marginalization, the paper uses an adaptively truncated approach to reduce computational complexity. What factors determine the truncation threshold Ïƒ? How was the optimal value determined experimentally? What are the tradeoffs between computational efficiency and accuracy?

4. The proposed input marginalization method is model-agnostic. What modifications would need to be made to apply it to non-NLP tasks like computer vision? Would a different technique besides MLM need to be used to estimate likelihoods?

5. The paper introduces a new evaluation metric, AUC_rep, to quantitatively assess the faithfulness of an interpretation method. Why can't existing metrics like deletion curves be used? What are the theoretical motivations behind AUC_rep?

6. How does the proposed input marginalization account for interactions between tokens versus just their individual contributions? Does the multi-token marginalization help capture inter-token effects?

7. One analysis showed the interpretation was sensitive to the quality of the likelihood modeling. What techniques could be used to further improve likelihood estimation and subsequently the faithfulness of the interpretation?

8. Could the method be extended to marginalize over other linguistic units like n-grams or sentences instead of just single tokens? What challenges would this present?

9. The paper focuses on interpreting models for sentiment analysis and natural language inference. How could the approach be adapted or modified to interpret models for other NLP tasks?

10. The method relies on access to the model's internals for likelihood estimation and measuring output differences. How could it be adapted to provide post-hoc interpretations for blackbox systems without access to internals?
