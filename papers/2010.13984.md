# [Interpretation of NLP models through input marginalization](https://arxiv.org/abs/2010.13984)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the key points of this paper are:- It raises the issue of out-of-distribution (OOD) problems that can arise when interpreting NLP models using existing token erasure schemes. These schemes replace tokens with predefined values like zero, which can push the input sentence out of the training data distribution and yield misleading interpretations. - To address this issue, the paper proposes a new interpretation method called input marginalization. The key idea is to marginalize (average over) the contribution of all probable candidate tokens for a position, weighted by their likelihoods. - The likelihoods are obtained using masked language modeling from BERT. This allows considering the context when estimating the likelihood of candidate tokens.- The proposed input marginalization method is applied to interpret various NLP models like CNNs, LSTMs and BERT trained for sentiment analysis and natural language inference.- Both qualitative examples and quantitative experiments demonstrate that the proposed method provides more faithful interpretations compared to existing erasure schemes. It avoids the OOD issue and highlights appropriate tokens.In summary, the central hypothesis is that input marginalization can produce superior interpretations of NLP models by avoiding OOD problems faced by existing erasure schemes. The experiments validate this hypothesis.


## What is the main contribution of this paper?

Here are the main contributions of this paper:- The paper raises the out-of-distribution (OOD) problem that can arise when interpreting NLP models through existing token erasure schemes like replacing tokens with zeros. Existing methods can push inputs out of the training distribution, yielding misleading interpretations. - To avoid the OOD problem, the paper proposes a new interpretation method called input marginalization. This involves marginalizing each token out by considering the likelihoods of all candidate replacement tokens based on masked language modeling using BERT. - The paper applies the proposed input marginalization method to interpret various NLP models trained for sentiment analysis and natural language inference. Experiments demonstrate it can provide more faithful interpretations than existing approaches.- The paper also proposes a metric called AUC_rep to quantitatively evaluate and compare interpretation methods by gradually replacing tokens and measuring how quickly the prediction probability drops.In summary, the main contribution is identifying the OOD problem with existing interpretation methods for NLP models, and proposing input marginalization as a solution that considers the full distribution of replacements when erasing tokens to avoid misleading interpretations. The method is demonstrated on multiple NLP tasks and models.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key point from the paper:The paper proposes a new method to interpret natural language processing models by marginalizing out each token based on its likelihood from BERT's masked language model, avoiding the out-of-distribution issue caused by existing erasure-based interpretation methods.


## How does this paper compare to other research in the same field?

Here are a few key ways this paper compares to other research in interpreting NLP models:- It focuses on addressing the out-of-distribution (OOD) problem that can arise from existing input erasure schemes for interpreting NLP models. Many previous works have not explicitly addressed this issue. - It proposes using input marginalization to mitigate the OOD problem. This is different from prior works that simply erase/replace tokens with a predefined value like zero. Marginalizing allows considering multiple candidate tokens weighted by their likelihoods.- It utilizes masked language modeling from BERT for accurately estimating token likelihoods in the marginalization. This provides a more powerful likelihood model compared to simple priors or assumptions made in some prior works.- It evaluates the faithfulness of interpretations more rigorously, using metrics like AUC_rep and comparisons to human annotations. Many prior works have evaluated interpretability methods less thoroughly.- The input marginalization approach is model-agnostic and task-agnostic, demonstrated across CNN, LSTM and BERT models and on both sentiment analysis and natural language inference tasks.- The analysis enabled by the interpretation method provides additional insights into model behaviors and differences between model architectures. This demonstrates the broader value of interpretability.Overall, this paper makes useful contributions in formalizing the OOD problem in NLP interpretability, proposing a practical solution, and demonstrating its utility over a range of models and tasks. The evaluations are more rigorous than many past works. It represents an advance in the state-of-the-art for interpreting NLP models faithfully.


## What future research directions do the authors suggest?

Based on my reading of the paper, some of the main future research directions suggested by the authors are:- Apply the proposed interpretation method to interpret other types of NLP models beyond sentiment analysis and natural language inference, such as for neural machine translation and visual question answering. The model-agnostic nature of their method makes it potentially applicable to many different models and tasks.- Interpret more advanced state-of-the-art NLP models like XLNet and ELECTRA using their proposed approach. As the authors mention, it would be meaningful to interpret these complex models.- Improve the likelihood modeling performance for candidate token probabilities. The authors experimentally analyzed that the interpretation results are affected by the quality of the likelihood modeling. Better modeling could lead to even more faithful interpretations.- Investigate multi-token attribution in more depth. The authors proposed a method for measuring joint contribution of multiple tokens, but did not extensively analyze it. More exploration could be done in this direction.- Develop more rigorous quantitative evaluation metrics for evaluating faithfulness of interpretation methods, beyond just using word-level sentiment tags or deletion curves.- Apply the idea of marginalizing out tokens rather than erasing to zero to other interpretation methods besides the weight of evidence, to mitigate out-of-distribution issues there as well.In summary, the main future directions are applying the proposed technique to broader models and tasks, improving the likelihood modeling, deeper investigation of multi-token attribution, developing better evaluation metrics, and extending the marginalization idea to other interpretation methods. Advancing research in these areas could lead to better understanding and interpretation of complex NLP models.


## Summarize the paper in one paragraph.

Here is a one paragraph summary of the paper:The paper proposes a new method to interpret predictions from deep neural network models for natural language processing. Existing interpretation methods measure the change in prediction probability when erasing each token, by replacing it with a default value like zero. However, this causes the input to be out-of-distribution from the training data, yielding misleading interpretations. To address this, the authors propose marginalizing each token out by considering all possible replacements and their likelihoods based on masked language modeling using BERT. This avoids the out-of-distribution issue. They apply the proposed input marginalization method to interpret various NLP models on sentiment analysis and natural language inference tasks. Experiments show it provides more faithful interpretations than existing erasure-based methods, enabling better understanding of model rationales.


## Summarize the paper in two paragraphs.

Here is a two paragraph summary of the paper:The advent of deep learning has greatly improved the performances of natural language processing (NLP) models. However, as these models become more complex, it becomes more difficult to understand their internal reasoning behind predictions. To build trust and enable high-stakes usage of deep neural networks (DNNs) for NLP, interpretability is crucial. To provide interpretations, several methods measure how a model's prediction changes when tokens are erased from the input. Existing techniques simply replace each token with a predefined value like zero, pushing the input out of the training distribution. This out-of-distribution (OOD) problem yields misleading interpretations, as DNNs assign lower probabilities to OOD inputs. This paper raises the OOD issue of existing interpretation methods for the first time in NLP research. To avoid it, they propose input marginalization which considers the likelihoods of all candidate replacement tokens when erasing each token. This mitigates the OOD problem. They model likelihoods using BERT's masked language modeling and propose adaptive truncation for efficiency. Experiments interpreting models for sentiment analysis and natural language inference show their method yields more faithful attribution scores. Analysis reveals insights into model decisions and differences between LSTM and BERT. Overall, the proposed input marginalization enables better understanding of NLP models by providing interpretability without the OOD problem of existing approaches.


## Summarize the main method used in the paper in one paragraph.

Here is a one paragraph summary of the main method used in the paper:The paper proposes a new method called input marginalization to interpret natural language processing (NLP) models by calculating the attribution score of each token. It measures how much the prediction changes when a token is marginalized out instead of simply erased. Specifically, it marginalizes a token by considering all possible candidate tokens and their likelihoods at that position modeled by BERT's masked language modeling (MLM). The likelihood indicates how probable it is for a candidate token to appear in that position. The attribution score is measured by the difference between the original prediction probability and the probability after marginalizing the token out. This avoids the out-of-distribution problem of existing erasure-based methods where sentences lie outside the training distribution after erasing tokens. The proposed input marginalization mitigates this issue by marginalizing tokens based on modeling token likelihoods using BERT MLM. The method is applied to interpret various NLP models like CNN, LSTM and BERT for sentiment analysis and natural language inference tasks.
