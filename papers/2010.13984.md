# Interpretation of NLP models through input marginalization

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the key points of this paper are:- It raises the issue of out-of-distribution (OOD) problems that can arise when interpreting NLP models using existing token erasure schemes. These schemes replace tokens with predefined values like zero, which can push the input sentence out of the training data distribution and yield misleading interpretations. - To address this issue, the paper proposes a new interpretation method called input marginalization. The key idea is to marginalize (average over) the contribution of all probable candidate tokens for a position, weighted by their likelihoods. - The likelihoods are obtained using masked language modeling from BERT. This allows considering the context when estimating the likelihood of candidate tokens.- The proposed input marginalization method is applied to interpret various NLP models like CNNs, LSTMs and BERT trained for sentiment analysis and natural language inference.- Both qualitative examples and quantitative experiments demonstrate that the proposed method provides more faithful interpretations compared to existing erasure schemes. It avoids the OOD issue and highlights appropriate tokens.In summary, the central hypothesis is that input marginalization can produce superior interpretations of NLP models by avoiding OOD problems faced by existing erasure schemes. The experiments validate this hypothesis.


## What is the main contribution of this paper?

Here are the main contributions of this paper:- The paper raises the out-of-distribution (OOD) problem that can arise when interpreting NLP models through existing token erasure schemes like replacing tokens with zeros. Existing methods can push inputs out of the training distribution, yielding misleading interpretations. - To avoid the OOD problem, the paper proposes a new interpretation method called input marginalization. This involves marginalizing each token out by considering the likelihoods of all candidate replacement tokens based on masked language modeling using BERT. - The paper applies the proposed input marginalization method to interpret various NLP models trained for sentiment analysis and natural language inference. Experiments demonstrate it can provide more faithful interpretations than existing approaches.- The paper also proposes a metric called AUC_rep to quantitatively evaluate and compare interpretation methods by gradually replacing tokens and measuring how quickly the prediction probability drops.In summary, the main contribution is identifying the OOD problem with existing interpretation methods for NLP models, and proposing input marginalization as a solution that considers the full distribution of replacements when erasing tokens to avoid misleading interpretations. The method is demonstrated on multiple NLP tasks and models.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key point from the paper:The paper proposes a new method to interpret natural language processing models by marginalizing out each token based on its likelihood from BERT's masked language model, avoiding the out-of-distribution issue caused by existing erasure-based interpretation methods.
