# [Churn Reduction via Distillation](https://arxiv.org/abs/2106.02654)

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper addresses is how to reduce churn (i.e., change in model predictions) when updating a machine learning model on new data. The key hypothesis is that knowledge distillation can be an effective technique for reducing churn while maintaining accuracy.Here are the key points about the research question and hypothesis:- The paper focuses on the problem of "churn reduction" - how to update a pre-trained machine learning model on new data while minimizing changes to its predictions. This is important for model stability.- The authors propose using knowledge distillation as a method to reduce churn. Knowledge distillation involves training a new model to mimic the predictions of the original pre-trained model. - The main hypothesis is that distilling the knowledge from the pre-trained "teacher" model into the updated "student" model will allow it to retain more of the original predictions and behavior, therefore reducing churn.- They theoretically analyze distillation for churn reduction and show it can find the optimal tradeoff between accuracy on new data and retaining old predictions.- The paper presents extensive experiments across many datasets and model types to validate that distillation reduces churn better than other techniques like warm-starting or anchor loss.So in summary, the key research question is how to update ML models while controlling churn, and the hypothesis is that distillation can achieve this effectively. The paper provides theory, analysis, and empirical validation of this idea.


## What is the main contribution of this paper?

The main contribution of this paper is proposing knowledge distillation as a method to reduce model churn (change in predictions) when re-training a model on updated data. The key ideas are:- They formally define the notion of model churn as the expected difference between the predictions of the original model and the retrained model on the same test points.- They propose using knowledge distillation to minimize churn - specifically training the new model on a convex combination of the hard labels and softened predictions from the original model. - They provide a theoretical analysis showing that distillation can find a good tradeoff between accuracy on the new data and staying close to original model's predictions.- Through extensive experiments on various datasets, they demonstrate that distillation reduces churn substantially compared to baselines while maintaining accuracy.So in summary, the main contribution is introducing distillation as an effective method to reduce churn during model updating, along with theoretical and empirical analysis demonstrating its benefits. This provides a principled way to update models on new data while avoiding unpredictable changes in predictions.
