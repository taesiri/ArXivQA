# A Survey of Controllable Text Generation using Transformer-based   Pre-trained Language Models

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central research question this paper aims to address is: How to achieve controllable text generation using large-scale pre-trained language models (PLMs)?The key points are:- Controllable text generation (CTG) is an important capability for advanced natural language generation systems, allowing them to produce text that meets specific constraints and requirements. - Recently, PLMs like BERT, GPT-2/3, etc. have become the dominant paradigm in NLP. However, their black-box nature makes controllability a challenge. - This paper provides a comprehensive survey on the latest techniques for achieving controllable text generation using PLMs. - It reviews the main tasks, approaches, and evaluation methods for CTG with PLMs. The approaches are categorized into fine-tuning, retraining/refactoring, and post-processing.- Challenges like catastrophic forgetting, coarse decoding control, lack of global text modeling, etc. are discussed. - Promising future directions like prompt-based learning, integrating linguistic knowledge, novel evaluation metrics, etc. are highlighted.In summary, the central research question is how to enable controllable text generation using the powerful yet uninterpretable PLMs, through various techniques like fine-tuning, prompt design, decoding control, etc. The paper aims to provide a holistic landscape of this emerging research area.


## What is the main contribution of this paper?

This paper provides a comprehensive review and analysis of controllable text generation methods based on pre-trained language models (PLMs). The main contributions are:1. It gives an introduction to the key concepts related to controllable text generation and PLMs, laying the groundwork for understanding this research area. 2. It categorizes and summarizes the main approaches to PLM-based controllable text generation, dividing them into fine-tuning, retraining/refactoring, and post-processing methods. For each category, it analyzes the principles, processes, and representative techniques.3. It summarizes the evaluation methodologies and metrics used for controllable text generation, including general NLG metrics, CTG-specific metrics, and human evaluation.4. It discusses the main challenges faced by current research on PLM-based controllable text generation.5. It suggests several promising future research directions to address the limitations of existing methods, including prompt-based learning, fine-grained decoding control, integration with classic generative models and linguistic knowledge, use of external knowledge, and new evaluation metrics and tasks.In summary, this paper provides a comprehensive survey and critical analysis of the current literature on PLM-based controllable text generation. It offers a clear overview of this emerging research area and sets out a roadmap for future work, which can benefit researchers and developers working in relevant fields.
