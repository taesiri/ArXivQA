# [The Role of $n$-gram Smoothing in the Age of Neural Networks](https://arxiv.org/abs/2403.17240)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem Statement
- Classical n-gram language models held state-of-the-art for decades, thanks to various smoothing techniques that combatted overfitting. However, with the rise of neural network language models, n-gram smoothing became less relevant.
- This paper re-examines if classical n-gram smoothing techniques can play a role in regularizing modern neural network language models.

Key Contributions
1) Draws a formal equivalence between label smoothing (a popular regularization technique for neural language models) and add-Î» smoothing (a classic n-gram smoothing technique).

2) Derives a general framework to convert any n-gram smoothing technique into a regularizer compatible with neural language models. This is done by:
- Expressing smoothing as training on a smoothed empirical n-gram distribution 
- Decomposing this smoothed distribution into the original empirical distribution plus two additional distributions
- Showing the KL divergence between these additional distributions and the model acts as a regularizer

3) Empirically verifies the framework by constructing regularizers from various classic smoothing techniques (Good-Turing, Jelinek-Mercer, Katz, Kneser-Ney) and applying them to neural language models and machine translation models.

4) Finds that the proposed regularizers perform comparably to, and sometimes better than, label smoothing on language modeling and machine translation tasks. The best performing regularizer was based on the Jelinek-Mercer smoothing technique.

In summary, the paper provides a way to incorporate insights from classic n-gram smoothing techniques to improve regularization of modern neural network language models, with both theoretical grounding and empirical support.
