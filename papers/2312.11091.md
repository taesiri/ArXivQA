# [Colored Noise in PPO: Improved Exploration and Performance Through   Correlated Action Sampling](https://arxiv.org/abs/2312.11091)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
Proximal Policy Optimization (PPO) is a popular on-policy deep reinforcement learning algorithm used for continuous control tasks like robotics. PPO relies on stochastic exploration by sampling uncorrelated Gaussian noise for action selection. Previous work has shown that temporally correlated "colored" noise enhances exploration and improves performance for off-policy RL methods. This paper investigates whether correlated noise can also improve the performance of on-policy algorithms like PPO.

Method:
The authors propose modifying the Gaussian policy distribution in PPO to inject temporally correlated noise instead of white noise while still maintaining an asymptotic on-policy behavior. This is done using the reparameterization trick where the noise term is sampled from a colored noise distribution parameterized by β instead of a white noise Gaussian. Different noise "colors" are obtained by varying β.

Contributions:

- Empirical evaluation shows that for most environments, correlated noise with β=0.5 performs better than default white noise used in PPO. Pink noise (β=1) worked best for off-policy methods but intermediate colors are better for on-policy.

- Analysis across different parallel environments for data collection indicates that more environments allow for more strongly correlated noise to be useful. This explains why off-policy methods that use experience replay prefer more correlated noise.  

- Key finding is that 4 parallel envs collecting 8192 samples per update with β=0.5 noise works most efficiently across tested environments.

- The variance in the bias of the noise explains the interaction between beta and number of environments. More environments reduce this variance allowing more correlated noise to be useful.

- Recommend switching default PPO noise process to a correlated noise process with β=0.5 as it significantly improves performance over white noise.

In summary, the paper demonstrates the utility of correlated noise for improving exploration and performance of on-policy PPO algorithm through comprehensive analysis. The key recommendations are adopting β=0.5 colored noise by default and using ~4 parallel envs for data collection.
