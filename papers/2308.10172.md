# [VLN-PETL: Parameter-Efficient Transfer Learning for Vision-and-Language   Navigation](https://arxiv.org/abs/2308.10172)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research questions/hypotheses appear to be:1) Can parameter-efficient transfer learning (PETL) methods be effectively adapted and applied to vision-and-language navigation (VLN) tasks? 2) Can tailored PETL modules be designed to enhance the learning of historical knowledge and cross-modal interactions that are crucial for VLN tasks?3) Will an integrated PETL approach incorporating existing methods like adapters and LoRA along with tailored VLN-specific modules outperform other PETL techniques and achieve comparable or better performance than full fine-tuning on VLN tasks?In summary, the key goals seem to be exploring how to adapt PETL to the unique challenges of VLN tasks, designing custom modules to boost important abilities like using historical knowledge, and showing that a VLN-specific integrated PETL method can efficiency tune large pre-trained models for VLN while maintaining strong performance. The paper appears aimed at demonstrating the viability and benefits of applying PETL to VLN through their proposed VLN-PETL approach.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contributions appear to be:1. Presenting the first study exploring Parameter-Efficient Transfer Learning (PETL) techniques for Vision-and-Language Navigation (VLN) tasks. 2. Proposing a VLN-specific PETL method called VLN-PETL, which incorporates existing PETL methods along with two new tailored PETL modules: Historical Interaction Booster (HIB) and Cross-modal Interaction Booster (CIB).3. Demonstrating through experiments on four VLN tasks that VLN-PETL outperforms other PETL methods and achieves comparable or better performance to full fine-tuning while using much fewer trainable parameters.In summary, the key contribution is proposing and evaluating a new PETL method customized for VLN tasks, which aims to efficiently fine-tune large pre-trained models for VLN while maintaining strong performance. The two new modules HIB and CIB are designed to enhance historical and cross-modal interactions specifically for VLN. Experiments show VLN-PETL achieves promising results compared to other PETL techniques and full fine-tuning.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper proposes VLN-PETL, a Vision-and-Language Navigation specific Parameter-Efficient Transfer Learning method that incorporates existing PETL techniques with two tailored modules - Historical Interaction Booster and Cross-modal Interaction Booster - to efficiently tune large pre-trained models for VLN tasks while achieving comparable performance to full fine-tuning.
