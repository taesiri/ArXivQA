# [Scaling Laws for Data Filtering -- Data Curation cannot be Compute   Agnostic](https://arxiv.org/abs/2404.07177)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Existing scaling laws for foundation models do not account for heterogeneity and limited availability of high-quality data from the web. They model web data as a single homogeneous entity.  

- However, web data has varying levels of quality. High-quality data is limited in quantity and loses utility rapidly with repetitions. This leads to a quality-quantity tradeoff (QQT).

- Current data filtering/curation methods are also developed agnostic of available training compute, which is suboptimal. The optimal filtering level changes based on total training compute.

Proposed Solution:
- Introduce scaling laws tailored for heterogeneous and limited web data by:
    - Characterizing differing utility of various quality subsets of web data
    - Modeling how utility of a data point diminishes when repeated
    - Formulating interaction of different data pools when combined to estimate model performance on combinations without needing to actually train on them

- Make data curation decisions conditional on available training compute by using scaling laws to determine optimal data subsets. Quality matters more for low compute budgets which have fewer repetitions. For high compute budgets, accounting for diminishing utility of limited high-quality data becomes important.

Main Contributions:    
- Demonstrate need for compute-aware data curation using experiments showing reversal in performance of filtering methods like LAION with increased compute

- Introduce first scaling laws for heterogeneous web data by modeling differential utility and repetition parameters of varying quality data subsets
  
- Enable estimation of model performance when trained on combinations of different data pools without needing to actually train models on those combinations

- Show how scaling laws can guide selection of optimal data subsets conditional on training compute, addressing quality-quantity tradeoff

In summary, the key message is that data curation cannot be agnostic of total training compute. The introduced scaling laws allow modeling this conditionality to determine pareto-optimal filtering levels.
