# [High-throughput Visual Nano-drone to Nano-drone Relative Localization   using Onboard Fully Convolutional Networks](https://arxiv.org/abs/2402.13756)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Relative drone-to-drone pose estimation is important for swarm operations like formation flight and collaborative missions. 
- Nano-drones (~10cm diameter) enable new applications but have very limited sensing, compute, memory and power capabilities. Most state-of-the-art solutions are too demanding for nano-drones.

Proposed Solution:
- The paper presents a vision-based fully convolutional neural network (FCNN) that estimates the relative pose between two nano-drones from low-resolution grayscale images captured by an onboard camera.
- The FCNN outputs 3 20x20 pixel maps representing: drone presence, depth map, and LED state. The 2D image-space position (u,v) and depth (d) of the target drone are extracted from these maps.

Key Contributions:
- Lightweight FCNN model tailored for resource constraints of a nano-drone with GAP8 SoC, requiring 8x fewer operations than prior work.
- Full system stack deployment on nano-drone from model design in Python to execution as quantized int8 C code at 39 FPS using 101mW.
- FCNN outperforms 3 state-of-the-art models on a 30k image dataset in pose estimation accuracy.  
- 37% lower tracking error and higher speeds demonstrated in field tests compared to prior work. Continuously tracks peer nano-drone through full battery lifetime of 4 minutes.
- Shows generalization to never-seen environments in field tests, achieving over 1 minute of continuous tracking.

In summary, the paper presents a complete vision-based system for relative nano-drone localization that pushes the state-of-the-art in accuracy under stringent nano-drone constraints. The systems and field tests demonstrate practical deployment potential.


## Summarize the paper in one sentence.

 This paper presents a lightweight fully convolutional neural network for real-time relative pose estimation between nano-drones using only onboard gray-scale cameras, achieving state-of-the-art accuracy while meeting the strict power and computational constraints.


## What is the main contribution of this paper?

 According to the paper, the main contribution is a novel vision-based fully convolutional neural network (FCNN) tailored for the limited resources aboard a commercially-available Crazyflie nano-drone. Specifically:

- They present a FCNN that takes 160x160 grayscale images as input and outputs three 20x20 pixel maps for estimating the drone's pose (position and depth) and LED state. 

- They provide the full vertical deployment of the system, from Python design and training down to C code execution on the nano-drone's ultra-low power GAP8 System-on-Chip.

- They thoroughly compare their approach to three state-of-the-art systems on a real-world dataset of ~30k images. Their FCNN achieves significantly better regression performance in terms of R^2 and Pearson correlation.

- They assess the onboard performance, achieving 39 FPS inference rate within 101 mW power consumption on the nano-drone.

- They demonstrate improved in-field tracking performance compared to prior art, with 37% lower error on average. Their system can continuously track a peer drone for the full battery lifetime of 4 minutes.

- They show good generalization capability by tracking drones in three never-seen-before environments.

In summary, the main contribution is a complete vision-based nano-drone pose estimation system, outperforming previous state-of-the-art solutions.


## What are the keywords or key terms associated with this paper?

 Based on my review of the paper, some of the key terms and keywords associated with it are:

- Nano-drones - The paper focuses on tiny quadrotor drones that weigh less than 50g and have a diameter smaller than 10cm.

- Relative pose estimation - Estimating the relative position and orientation between two drones using only onboard sensors and processing.

- Fully convolutional neural network (FCNN) - The novel lightweight neural network architecture proposed in the paper for performing visual pose estimation.

- System-on-Chip (SoC) - Specifically the GAP8 SoC used to deploy and run neural networks onboard the nano-drones. 

- Ultra low-power - The paper emphasizes the extremely constrained power budget of nano-drones.

- Real-time performance - The ability to run the models fast enough for closed-loop control, targeting over 30 FPS.

- Vertical integration - Going from model design in Python down to embedded C code deployment on the drones' electronics.

- Generalization capability - Testing the trained models in unseen environments different from the training data.

- Endurance - Ability of the integrated system to continuously track over the full battery lifetime of the nano-drones.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in this paper:

1. The paper mentions using a barycenter approach to extract the (u,v) coordinates from the output activation map instead of using argmax. What are the tradeoffs of using barycenter versus argmax for this task? How does the performance compare between these two approaches?

2. The paper evaluates performance on a testing dataset of around 30k images. What potential issues could there be in terms of overfitting or lack of diversity with a dataset of this size? How could the dataset be improved or augmented to address these concerns? 

3. The GAP8 SoC used in this work lacks floating point units. What are the implications of using fixed point integer math instead of floating point for the neural network computations? How does quantization impact model accuracy and what mitigation strategies are used?

4. The paper demonstrates a 37% lower tracking error compared to prior work. What factors contribute the most to this improved performance? Is it mostly from model architecture changes or other system level optimizations?  

5. Power consumption is a key constraint for nano-drone systems. The paper reports 101mW power for 39Hz inference. How could power be further reduced, potentially at the expense of throughput? What hardware or software optimizations could help?

6. The fully convolutional architecture outputs multiple maps instead of regression values. What are the advantages of using maps over regression for this application? How do design choices like map resolution affect performance?

7. What physical factors or visual effects make the drone tracking task more difficult? How do occlusions, lighting changes, target distance etc. impact performance? How could the model be improved to address these?

8. The model generalizes successfully to new environments in testing. However, what differences in the visual data could exist between the training and real-world environments that could cause issues? 

9. The system tracks target velocity up to 0.59 m/s. What factors limit max velocity? How could the drone, model architecture, or tracking algorithm be adapted to increase this?

10. The paper uses a motion capture system to generate ground truth pose data. What challenges exist in collecting a diverse, real-world dataset without specialized equipment? Could simulations, synthetic data or self-supervision play a role?
