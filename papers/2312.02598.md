# [Impact of Tokenization on LLaMa Russian Adaptation](https://arxiv.org/abs/2312.02598)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Large language models (LLMs) like ChatGPT show impressive performance on English language tasks, but suffer significant quality degradation on non-English inputs. 
- A key reason is inefficient tokenization caused by the dominance of English data in pre-training, which hinders comprehension of non-English instructions and limits language adaptation potential.

Proposed Solution: 
- The paper explores vocabulary substitution to optimize Russian language tokenization of the LLaMa model. 
- Three tokenization options are tested: original LLaMa, Russian BPE, and Russian Unigram.
- Updated embedding and LM head layers are initialized using overlap between old and new vocabularies. 
- Tuned layers are then pre-trained on a Russian corpus for language adaptation.

Key Contributions:
- Shows Unigram tokenization has higher morphological accuracy for Russian than BPE used in state-of-the-art models.
- Benchmark on Russian Super Glue shows Russian LLMs highly benefit from morphologically accurate Unigram tokenization, achieving significant quality improvements.  
- Human evaluation reveals vocabulary substitution boosts instruction tuning efficiency, generating more relevant answers.
- Tokenization optimization substantially improves efficiency - faster fine-tuning (35%), faster inference (up to 60%) and lower memory consumption.

In summary, the paper demonstrates that optimizing tokenization for morphological accuracy has multiple benefits for adapting large language models to new languages. The gains include improved quality, better instruction tuning, and faster and more efficient model performance.
