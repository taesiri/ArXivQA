# [Impact of Tokenization on LLaMa Russian Adaptation](https://arxiv.org/abs/2312.02598)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a summary paragraph of the key points from the paper:

This paper investigates optimizing tokenization to improve language adaptation of the Large Language Model LLaMa to Russian. They explore three tokenization options - the original vocabulary, Russian BPE, and Russian Unigram - and evaluate on both fine-tuning and instruction tuning settings. Experiments show Unigram tokenization, with its higher morphological accuracy, boosts quality on the Russian SuperGLUE benchmark over BPE and original tokenization in both settings. Additional human evaluation reveals the Unigram vocabulary also improves relevance of instruction-tuned answers. Besides quality gains, optimized tokenization significantly improves efficiency - reducing fine-tuning time by 35% and inference by up to 60% while lowering memory use. The paper demonstrates vocabulary substitution is an effective approach to adapt large language models to new languages, enhancing quality and resource utilization through improved tokenization.
