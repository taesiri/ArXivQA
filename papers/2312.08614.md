# [Factorization Vision Transformer: Modeling Long Range Dependency with   Local Window Cost](https://arxiv.org/abs/2312.08614)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a high-quality summary paragraph of the key points from the paper:

This paper proposes a novel factorization self-attention (FaSA) mechanism for vision transformers to achieve an optimal trade-off between modeling long-range dependencies and computational efficiency. FaSA factorizes the conventional attention matrix into sparse sub-attention matrices through local windowing, dilated sampling, and cross-window fusion. This enables capturing both long-range and mixed-grained information simultaneously, while adhering to the linear complexity with input resolution as local window-based attention. Based on FaSA, the authors present the factorization vision transformer (FaViT) with a hierarchical architecture. Experiments demonstrate FaViT's state-of-the-art performance and robustness on image classification, object detection and semantic segmentation. Notably, FaViT-B2 improves accuracy by 1% and robustness by 7% over Swin Transformer baseline, while reducing parameters by 14%. The core innovation of FaSA successfully balances transform ability and efficiency, providing valuable inspiration for designing performant and robust vision transformers.
