# [Few-Shot Backdoor Attacks on Visual Object Tracking](https://arxiv.org/abs/2201.13178v2)

## What is the central research question or hypothesis that this paper addresses?

 The central research question this paper addresses is whether visual object tracking (VOT) models are vulnerable to backdoor attacks. Specifically, it proposes a few-shot backdoor attack method against siamese network based trackers. The key hypothesis is that by embedding backdoors directly in the feature space and optimizing a novel feature loss, the attack can be very effective even when the trigger pattern only appears in one or a few frames.

The main research questions and hypotheses include:

- Are VOT models vulnerable to backdoor attacks where the adversary can manipulate the training process?

- Can existing backdoor attacks designed for image classification be effectively adapted to attack VOT models? 

- Can a more effective backdoor attack be developed by embedding backdoors directly in the feature space instead of just flipping labels?

- Will optimizing a novel feature loss help achieve few-shot attack effectiveness?

So in summary, the central hypothesis is that VOT models are vulnerable to backdoor attacks by manipulating the training, and the key idea is to embed backdoors in the feature space and optimize a feature loss for few-shot effectiveness.


## What is the main contribution of this paper?

 This paper proposes a few-shot backdoor attack against visual object tracking (VOT) models. The main contributions are:

1. It reveals the backdoor threat in visual object tracking. This is the first backdoor attack against VOT models and video-based computer vision tasks. 

2. It proposes a simple yet effective few-shot untargeted backdoor attack (FSBA) that can significantly degrade the tracking performance even if the trigger only appears in a few frames. 

3. It empirically shows that the attack is effective in both digital and physical-world scenarios, and resistant to potential defenses.

4. The attack treats backdoor injection as a multi-task learning problem, alternating between optimizing a feature loss to inject backdoors and a tracking loss to preserve performance.

5. The feature loss encourages separation between benign and poisoned examples in feature space, enabling few-shot attack effectiveness.

6. It poisons only a small fraction of frames to reduce computational cost and maintain stealthiness.

In summary, this paper reveals the vulnerability of VOT models to backdoor attacks, proposes an effective few-shot attack, and empirically verifies its effectiveness and stealthiness. The attack methodology provides a new perspective on backdoor injection for non-classification tasks.
