# [Generating Aligned Pseudo-Supervision from Non-Aligned Data for Image   Restoration in Under-Display Camera](https://arxiv.org/abs/2304.06019)

## What is the central research question or hypothesis that this paper addresses?

 The central research question this paper addresses is how to generate high-quality and well-aligned pseudo training pairs for under-display camera (UDC) image restoration. The key idea is to learn to "copy" details from a high-quality reference image and "paste" them onto the corresponding UDC image in a spatially aligned manner.

The main hypothesis is that using such pseudo training pairs generated from real non-aligned stereo data will lead to improved performance and generalizability for UDC image restoration networks compared to previous approaches like using synthetic data or unpaired training schemes.

To test this hypothesis, the authors propose AlignFormer, a Transformer-based framework to generate aligned pseudo pairs by mitigating both the domain discrepancy and spatial misalignment between UDC and reference images. The effectiveness of the generated pseudo pairs is then evaluated by training a baseline restoration network and comparing against other datasets and methods.

In summary, the central research question is how to create high-quality spatially aligned pseudo pairs from non-aligned real stereo data to improve UDC image restoration. The key hypothesis is that AlignFormer can generate such useful training data better than previous approaches.


## What is the main contribution of this paper?

 The main contribution of this paper is a framework for generating high-quality and well-aligned pseudo training image pairs for under-display camera (UDC) image restoration. The key ideas are:

1. They propose to use a stereo camera setup with one UDC camera and one normal camera to capture images of the same scene. This allows collecting real degraded UDC images and high-quality reference images. 

2. They present AlignFormer, a novel Transformer-based method, to generate pseudo training pairs from the non-aligned stereo images. AlignFormer has two main components:

- Domain Alignment Module (DAM) to reduce the domain discrepancy between UDC and reference images to enable better matching.

- Geometric Alignment Module (GAM) to establish accurate dense correspondences between UDC and reference images using both semantic and geometric cues.

3. The well-aligned and high-quality pseudo pairs from AlignFormer can be used to train UDC restoration networks with standard pixel-wise losses. This avoids previous limitations of using synthetic or monitor-based training data.

4. They demonstrate improved UDC restoration performance by training a baseline network on the AlignFormer pseudo pairs compared to state-of-the-art methods.

In summary, the main contribution is a practical framework to generate realistic aligned training data for UDC restoration, which can alleviate previous issues with training data and lead to better generalization. The AlignFormer leverages Transformer architectures to perform robust alignment even with significant domain gaps between UDC and reference images.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the key points from the paper:

The paper proposes a novel framework called AlignFormer to generate high-quality and well-aligned pseudo training pairs from non-aligned real stereo data (comprised of UDC and reference images) to enable end-to-end training of deep networks for Under-Display Camera image restoration.


## How does this paper compare to other research in the same field?

 Here are my thoughts on how this paper compares to other research in the field of under-display camera (UDC) image restoration:

- This paper focuses on generating high-quality and aligned training data for UDC image restoration networks, which is a significant challenge in this field. Previous work has relied on synthetic data or data from monitor-based imaging systems, which have limitations in realism and dynamic range. 

- The key novelty is the use of a Transformer-based framework (AlignFormer) to align real stereo image pairs obtained from a UDC and normal camera. This allows more realistic training data to be generated through "copying" details from the normal image and "pasting" onto the UDC image.

- Other recent work like DISCNet (CVPR 2021) and BNUDC (CVPR 2022) has focused more on architecture designs for the restoration network itself. This paper complements that line of work by contributing better training data.

- The proposed AlignFormer framework integrates optical flow guidance into a Transformer, which is a unique approach for handling the alignment. Prior work like TTSR (ECCV 2020) used semantic similarity but no explicit geometric guidance.

- For data collection, this paper uses a practical stereo setup whereas most prior real data collection relied on beam splitters or manual alignment. The proposed framework is shown to be robust enough to align such stereo pairs.

- Extensive experiments demonstrate the benefit of AlignFormer's pseudo pairs for training restoration networks. Both quantitative metrics and user studies confirm improved performance, especially in recovering fine details.

In summary, this paper makes valuable contributions on the data side for UDC restoration, allowing more real-world and aligned training data to be leveraged. The AlignFormer framework is an innovative way to integrate geometric and semantic guidance within a Transformer structure for this application.


## What future research directions do the authors suggest?

 Based on my reading of the paper, here are some of the main future research directions suggested by the authors:

- Developing more domain-specific network architectures for UDC image restoration that can handle the limited dynamic range and remedy detail loss in over-saturated regions. The authors mention that while their baseline PPM-UNet model achieves promising performance, there is room for improvement with architectures tailored to UDC imaging characteristics.

- Exploring new losses or training methods to better handle the misalignment between UDC images and reference images during training. The authors use contextual losses like CX loss and CoBi loss, but suggest designing losses to impose stronger spatial constraints, especially for flare regions.

- Collecting larger and more diverse real-world UDC datasets to train and evaluate models. The authors captured 330 image pairs, but larger datasets could help further. Capturing more challenging indoor/outdoor scenes could improve model robustness.

- Investigating the limits of UDC imaging hardware and whether resolution/quality can be improved with different display pixel layouts or patterns. This could alleviate issues stemming from hardware limitations.

- Extending the approach to video restoration for UDC. The current method focuses on image restoration, but video introduces additional challenges like temporal consistency that could be explored.

In summary, the main directions are developing specialized architectures for UDC, designing better losses for misalignment, expanding datasets, improving hardware, and extending to video. The authors have presented promising methods and results, but highlight many opportunities for future work on this new and challenging problem.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the key points in the paper:

This CVPR 2023 paper presents a novel framework called AlignFormer to generate high-quality and spatially aligned pseudo training data pairs for training deep networks for Under-Display Camera (UDC) image restoration. The key idea is to capture a degraded UDC image and a high-quality reference image of the same scene in a stereo setup. The AlignFormer framework then discovers dense correspondences between the UDC and reference images by incorporating both semantic similarity and geometric guidance from optical flow. This allows "copying" details from the reference image and "pasting" them onto the UDC image while maintaining alignment. The framework consists of a Domain Alignment Module to reduce domain discrepancy and facilitate matching, and a Geometric Alignment Module that incorporates optical flow guidance into attention to enable accurate matching. The aligned pseudo pairs can then be used to train UDC restoration networks with standard pixel-wise losses. Experiments show the proposed framework generates better training data than previous simulated or monitor-based datasets, leading to improved performance on real-world UDC restoration.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the key points in the paper:

This paper proposes AlignFormer, a novel framework to generate high-quality and well-aligned pseudo training image pairs for under-display camera (UDC) image restoration. Previous methods have relied on monitor-based systems or simulation to create training data, but these approaches have limitations in capturing real-world degradation and introducing domain gaps. Instead, the authors use a stereo setup with one UDC and one standard camera to capture images of the same scene. However, this presents challenges of domain discrepancy and spatial misalignment between the UDC and normal images. 

To address this, AlignFormer integrates a Domain Alignment Module (DAM) and Geometric Alignment Module (GAM) into a Transformer architecture. The DAM brings the UDC image stylistically closer to the normal image to improve correspondence matching. The GAM incorporates optical flow guidance into the attention mechanism to accurately establish dense correspondences and alignment. Experiments demonstrate AlignFormer can produce well-aligned, realistic training data. Training an image restoration network with this data shows significant improvements in practical UDC image restoration over previous datasets and methods. The key contributions are a promising data generation framework beyond prior simulation-based approaches and proper integration of optical flow into Transformers.
