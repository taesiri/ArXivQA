# [DisCoScene: Spatially Disentangled Generative Radiance Fields for   Controllable 3D-aware Scene Synthesis](https://arxiv.org/abs/2212.11984)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question seems to be: 

How can we develop a 3D-aware generative model that allows for high-quality and controllable synthesis of complex scenes containing multiple objects?

More specifically, the authors aim to address the limitations of existing 3D-aware image synthesis methods, which have mainly focused on generating single objects and struggled with modeling scenes containing varied objects with non-trivial spatial arrangements.  

Their key hypothesis is that using a simple abstract layout representation of objects as a spatial prior can help spatially disentangle the scene into object-centric radiance fields. This allows generating high-fidelity images while enabling intuitive object-level editing and control.

The paper introduces "DisCoScene", a novel generative model driven by such a layout prior to achieve controllable 3D-aware scene synthesis. Experiments on various datasets demonstrate its superior performance over other baselines in generating complex multi-object scenes as well as editing objects within the scenes.

In summary, the central research question is about developing a 3D-aware generative model for complex scenes that can synthesize high-quality images while supporting flexible user control over objects, which is addressed through the use of a spatial layout prior.


## What is the main contribution of this paper?

 The main contribution of this paper is presenting DisCoScene, a 3D-aware generative model for high-quality and controllable scene synthesis. The key ideas include:

- Using a simple object-level representation (3D bounding boxes without semantic annotation) as the scene layout prior. This acts as spatial supervision to disentangle objects and background.

- Proposing spatially disentangled radiance fields based on the layout prior, where each object and the background are modeled by separate radiance fields. This allows object-level control. 

- Developing an efficient rendering pipeline tailored for the disentangled radiance fields to accelerate training and inference.

- Introducing global-local discrimination with both scene-level and object-level critics to enforce spatial disentanglement during training.

- Demonstrating state-of-the-art performance on complex indoor and outdoor scene datasets like CLEVR, 3D-Front, and Waymo. The method achieves high-quality generation and supports versatile object-level editing.

In summary, the main contribution is a new 3D-aware scene representation that disentangles objects based on a simple layout prior, enabling controllable high-quality scene synthesis from challenging 2D datasets.
