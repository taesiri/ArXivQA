# [Self-DC: When to retrieve and When to generate? Self Divide-and-Conquer   for Compositional Unknown Questions](https://arxiv.org/abs/2402.13514)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Existing QA systems either fully rely on large language models (LLMs) to generate answers from their internal knowledge or always retrieve external information, being inefficient for compositional questions consisting of both known and unknown parts. 

- There lacks an effective framework for LLMs to adaptively leverage internal known knowledge and external information to handle compositional questions.

Proposed Solution:
- Introduce a new compositional unknown QA dataset (CuQA) with automatic construction requiring minimal human effort.

- Propose a Self Divide-and-Conquer (Self-DC) method for LLMs to selectively decompose questions and call internal or external functions accordingly:
   - Get confidence score from LLM on answering the question 
   - Decompose into sub-questions if score falls in uncertain range  
   - Call retrieve-then-read for low score questions, generate-then-read for high score ones
   - Combine answers of sub-questions to get final answer

Main Contributions:
- First to study compositional questions from known/unknown perspective
- Release new CuQA benchmark for compositional reasoning evaluation 
- Design flexible Self-DC framework that outperforms retrieval-augmented methods on CuQA and FreshQA with fewer retrieval calls
- Demonstrate the capability of Self-DC in handling compositional questions by adaptively acquiring internal and external knowledge

The summary covers the key aspects of the paper - the problem being addressed, the proposed Self-DC solution and dataset, and the main contributions around advancing compositional reasoning for QA. Let me know if you need any clarification or have additional questions!


## What is the main contribution of this paper?

 The main contributions of this paper can be summarized as:

1. To the best of their knowledge, they are the first to study compositional questions from the perspective of known/unknown. 

2. They introduce an automatic data collection pipeline to create a new dataset called CuQA (Compositional unknown Question-Answering dataset), serving as an evaluation benchmark for large language models in dealing with known/unknown questions.

3. They present a flexible and robust framework called Self Divide-and-Conquer (Self-DC), which is capable of adaptively calling internal known knowledge and external unknown knowledge on demand. 

4. Experimental results on CuQA and FreshQA datasets show the superiority of Self-DC in terms of both effectiveness and efficiency over several strong baselines. The results reveal the promising potential of Self-DC to solve compositional reasoning problems.

In summary, the key contribution is proposing the Self-DC framework to empower large language models to handle compositional questions containing both known and unknown parts more effectively and efficiently. The introduction of the new CuQA dataset and extensive experiments also demonstrate the capability of this method.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and keywords associated with this work include:

- Compositional unknown questions - The paper introduces the concept of compositional questions that consist of both known and unknown sub-questions. This is a key focus.  

- Known/unknown boundaries - The paper examines the known and unknown knowledge boundaries of large language models.

- Certainty/uncertainty - The paper connects the known/unknown capabilities of models with their ability to quantify certainty and uncertainty.

- Self Divide-and-Conquer (Self-DC) - The proposed framework that can adaptively call different functions like retrieve-then-read or generate-then-read based on the model's certainty.

- CuQA dataset - The new compositional question answering dataset introduced for evaluation.

- Efficiency - The paper emphasizes efficiency in terms of minimizing external retrieval calls compared to prior work.

- Decomposition - Breaking down compositional questions into sub-questions is a key part of the proposed Self-DC approach.

So in summary, the key terms cover compositional unknown questions, known/unknown boundaries, certainty/uncertainty, the Self-DC framework, the CuQA dataset, efficiency, and decomposition. Let me know if you need any clarification or have additional questions!


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in this paper:

1. How does the Self-DC framework leverage both internal parametric memory of large language models and external retrieval knowledge to answer compositional questions adaptively? What are the key advantages of this approach?

2. What novel prompting strategies and techniques does Self-DC employ for confidence score acquisition, decomposition, retrieval augmentation and sub-question combination? How do they contribute to the overall performance?  

3. How does Self-DC determine when to utilize retrieval versus generation to answer different parts of a compositional question? What hyperparameter tuning is involved in making this decision robustly?

4. What are the termination conditions used by Self-DC to prevent excessive iterations? How were these conditions designed to balance performance and efficiency?  

5. How does the dataset construction pipeline for CuQA ensure diversity in reasoning complexity while minimizing human effort? What checks are in place to control data quality automatically?

6. What error analysis was performed on Self-DC? What were the key performance bottlenecks identified for future improvement?

7. How do the results analysis on CuQA and FreshQA datasets showcase the strengths of Self-DC across models and question types? What insights do they provide?

8. How does the performance of Self-DC vary with different confidence score acquisition methods like probability-based versus verbalization-based? What tradeoffs do they present?

9. What additional benchmarking could be performed to evaluate the scalability and generalizability of Self-DC across more models, tasks and data distributions? 

10. What societal impacts need to be considered if Self-DC is deployed for real-world question answering? How can transparency and accountability be ensured?
