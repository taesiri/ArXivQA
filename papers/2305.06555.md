# [Domain Incremental Lifelong Learning in an Open World](https://arxiv.org/abs/2305.06555)

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper addresses is: How can we build lifelong learning (LL) models that can continuously learn new tasks in an open world setting, where the model needs to handle samples from unseen tasks during testing? Specifically, the key hypotheses tested in this paper are:1. Using hierarchically organized prompts to capture knowledge at different granularities (global, format-level, task-level, instance-level) can help build effective LL models.2. Maintaining separate prompts to explicitly model unseen tasks, along with prompt keys to facilitate knowledge sharing, can improve the model's ability to generalize to unseen tasks. 3. Combining task-specific and instance-specific components allows the model to achieve both high LL performance and the ability to handle unseen tasks.The authors propose Diana, a dynamic architecture that uses soft prompts in a hierarchy to test these hypotheses. Diana is evaluated on two benchmark LL task sets and shown to outperform prior state-of-the-art methods, especially on unseen tasks. The central contribution is a new LL model design that can continuously learn in a more realistic open world setting.
