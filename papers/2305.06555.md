# [Domain Incremental Lifelong Learning in an Open World](https://arxiv.org/abs/2305.06555)

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper addresses is: How can we build lifelong learning (LL) models that can continuously learn new tasks in an open world setting, where the model needs to handle samples from unseen tasks during testing? Specifically, the key hypotheses tested in this paper are:1. Using hierarchically organized prompts to capture knowledge at different granularities (global, format-level, task-level, instance-level) can help build effective LL models.2. Maintaining separate prompts to explicitly model unseen tasks, along with prompt keys to facilitate knowledge sharing, can improve the model's ability to generalize to unseen tasks. 3. Combining task-specific and instance-specific components allows the model to achieve both high LL performance and the ability to handle unseen tasks.The authors propose Diana, a dynamic architecture that uses soft prompts in a hierarchy to test these hypotheses. Diana is evaluated on two benchmark LL task sets and shown to outperform prior state-of-the-art methods, especially on unseen tasks. The central contribution is a new LL model design that can continuously learn in a more realistic open world setting.


## What is the main contribution of this paper?

Based on my understanding, the main contribution of this paper is proposing a new lifelong learning model called Diana for the domain incremental learning scenario. The key aspects are:1. Diana uses a hierarchical prompt structure with four types of prompts (general, format, task, and meta prompts) to capture knowledge at different granularities. 2. It introduces task prompt keys and meta prompt keys to facilitate knowledge sharing between tasks. The space of these keys is learned with distance-based losses.3. Diana allocates separate task prompts to explicitly model unseen tasks. This improves the model's ability to handle samples from unseen tasks.4. Extensive experiments show Diana outperforms state-of-the-art lifelong learning models, especially on unseen tasks. The ablation studies also demonstrate the effectiveness of the proposed components.In summary, the main novelty is using the hierarchical prompt structure and dedicated modeling of unseen tasks to achieve strong lifelong learning performance, especially on unseen tasks, in the domain incremental setting. The introduction of prompt keys for knowledge sharing is also a notable contribution.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper: The paper proposes Diana, a dynamic architecture-based lifelong learning model that uses hierarchically organized prompts to capture knowledge from different granularities, handles unseen tasks through dedicated prompts, and introduces prompt keys to facilitate knowledge sharing - achieving strong performance on benchmark NLP tasks.
