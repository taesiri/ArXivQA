# [LOCATE: Localize and Transfer Object Parts for Weakly Supervised   Affordance Grounding](https://arxiv.org/abs/2303.09665)

## What is the central research question or hypothesis that this paper addresses?

 The central research question this paper addresses is: How can we learn to ground object affordances (i.e. locate object regions for specific actions like grasping or sitting) by observing human-object interaction images in a weakly supervised manner? 

The key hypothesis is that by localizing interaction regions in exocentric (third-person) images showing humans using objects, extracting part-specific features from these regions, and transferring this knowledge to egocentric (first-person) images of objects, the model can learn to locate affordance regions without needing pixel-level ground truth annotations.

In summary, the paper proposes a method called LOCATE to:

- Localize where interactions occur in exocentric images using CAM
- Select part-specific features representing the object affordance using a proposed module PartSelect 
- Transfer this knowledge to localize affordances in egocentric images using a cosine embedding loss

The key hypothesis is that this localized part-level knowledge transfer can enable accurate affordance grounding from only image-level labels, outperforming methods that use global feature similarity.
