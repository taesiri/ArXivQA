# [LOCATE: Localize and Transfer Object Parts for Weakly Supervised   Affordance Grounding](https://arxiv.org/abs/2303.09665)

## What is the central research question or hypothesis that this paper addresses?

 The central research question this paper addresses is: How can we learn to ground object affordances (i.e. locate object regions for specific actions like grasping or sitting) by observing human-object interaction images in a weakly supervised manner? 

The key hypothesis is that by localizing interaction regions in exocentric (third-person) images showing humans using objects, extracting part-specific features from these regions, and transferring this knowledge to egocentric (first-person) images of objects, the model can learn to locate affordance regions without needing pixel-level ground truth annotations.

In summary, the paper proposes a method called LOCATE to:

- Localize where interactions occur in exocentric images using CAM
- Select part-specific features representing the object affordance using a proposed module PartSelect 
- Transfer this knowledge to localize affordances in egocentric images using a cosine embedding loss

The key hypothesis is that this localized part-level knowledge transfer can enable accurate affordance grounding from only image-level labels, outperforming methods that use global feature similarity.


## What is the main contribution of this paper?

 The main contribution of this paper is proposing a framework called LOCATE for weakly supervised affordance grounding. Specifically:

- It introduces a method to localize and extract embeddings from human-object interaction regions in exocentric images using class activation maps (CAM).

- It proposes a module called PartSelect that selects the embedding corresponding to the relevant object part from the interaction region embeddings. This is done by clustering the embeddings into prototypes and selecting the one with highest overlap with the object. 

- It transfers the selected object part embedding to the egocentric image as supervision to guide affordance grounding, using a cosine embedding loss.

- It demonstrates state-of-the-art performance on affordance grounding on the AGD20K dataset, with fewer parameters and faster inference than prior methods. 

In summary, the key innovation is locating and extracting the specific object part involved in the affordance from weakly supervised exocentric images, and using it to supervise the affordance grounding in egocentric images. This allows transferring knowledge at a part-level rather than image-level.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the paper:

The paper proposes a framework called LOCATE that transfers knowledge about object affordances from third-person human-object interaction images to first-person images of objects, enabling weakly supervised localization of affordances using only image-level labels.
