# [Less is More: Learning Reference Knowledge Using No-Reference Image   Quality Assessment](https://arxiv.org/abs/2312.00591)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a high-quality summary paragraph of the key points from the paper:

This paper presents a novel NR-IQA framework, Reference Knowledge-guided Image Quality Transformer (RKIQT), which leverages knowledge distillation techniques to introduce comparative knowledge from reference images into an NR-IQA model. The key ideas include: (1) a non-aligned reference teacher network extracts HQ-LQ distribution differences from non-corresponding high quality reference images. (2) A Masked Quality-Contrastive Distillation mechanism transfers this knowledge to guide the training of the NR-student network. It masks the student features and forces reconstruction using the teacher's differences features as supervision. This enhances the student's comparative abilities. (3) An inductive bias regularization is proposed utilizing CNN and Involution teachers to provide rich and diverse quality-aware knowledge. This improves model convergence and generalization. Experiments on 8 benchmark datasets show state-of-the-art NR-IQA performance. A key conclusion is that introducing comparative reference knowledge via distillation enables high accuracy NR-IQA without actual reference images. The model outperforms competitors while using less direct input.


## Summarize the paper in one sentence.

 Here is a one-sentence summary of the paper:

The paper proposes a new framework, Reference Knowledge-guided image quality transformer (RKIQT), to perform no-reference image quality assessment by transferring comparative knowledge from teachers to students using masked quality-contrastive distillation and inductive bias regularization.
