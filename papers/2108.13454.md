# [Improving Query Representations for Dense Retrieval with Pseudo   Relevance Feedback](https://arxiv.org/abs/2108.13454)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is: How can pseudo relevance feedback (PRF) be used to improve query representations for dense retrieval? The key hypothesis is that leveraging PRF documents from an initial retrieval pass can help produce better query embeddings that more accurately reflect the underlying search intent, compared to only using the original query. The paper proposes a model called ANCE-PRF that consumes both the query and PRF documents to learn improved query representations.Some key points:- Dense retrieval relies on encoded embeddings to capture query/document semantics, but this is challenging due to query ambiguity. - PRF is commonly used in sparse retrieval to expand/reweight queries. This work explores using PRF to improve query encodings in dense retrieval.- ANCE-PRF uses a BERT encoder to consume the query and PRF docs from an initial ANCE retrieval. It learns to produce better query embeddings directly from relevance labels.- Experiments show ANCE-PRF outperforms ANCE and other dense retrieval methods, especially on complex queries. - Analysis indicates ANCE-PRF learns to leverage useful PRF info and ignore noise via its attention mechanism.In summary, the central hypothesis is that PRF can be used to improve query representations in dense retrieval, which ANCE-PRF confirms through strong empirical results and analysis.


## What is the main contribution of this paper?

The main contribution of this paper is proposing ANCE-PRF, a new query encoder that improves query representations for dense retrieval using pseudo relevance feedback (PRF). Specifically:- ANCE-PRF consumes the original query and top retrieved documents from an initial dense retriever to produce better query embeddings. It is trained end-to-end using relevance labels.- ANCE-PRF significantly outperforms the initial dense retriever ANCE and other recent dense retrieval models on several benchmarks. It is among the best performing first-stage retrieval systems on the highly competitive MS MARCO leaderboard.- Analysis shows ANCE-PRF learns to leverage useful information from relevant PRF documents and ignore noise using its attention mechanism. The encoder focuses more on PRF terms complementary to the query terms.- ANCE-PRF provides a simple yet effective way to incorporate PRF signals into dense retrieval systems. It reuses the document index to avoid overhead.In summary, the main contribution is using PRF documents within a learned query encoder to improve query representations for dense retrieval. This straightforward integration of classic PRF into modern dense retrieval is shown to bring significant accuracy improvements.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper proposes ANCE-PRF, a new query encoder for dense retrieval that uses pseudo relevance feedback documents from an initial dense retriever to learn better query representations and improve retrieval accuracy.


## How does this paper compare to other research in the same field?

Here are a few key points about how this paper compares to other research in dense retrieval:- This paper proposes a new query encoder called ANCE-PRF that improves query representations by incorporating pseudo relevance feedback (PRF) documents. Other dense retrieval methods like ANCE, DPR, ME-BERT typically only encode the query text itself. Using PRF is a classic technique in sparse retrieval but novel for dense retrieval.- Experiments show ANCE-PRF outperforms several state-of-the-art dense retrievers like ANCE, ME-BERT, and DPR across multiple datasets. This demonstrates the effectiveness of the proposed PRF approach.- The paper analyses the learned embeddings and attention weights, revealing how ANCE-PRF focuses more on relevant terms in the PRF documents. This provides insights into why the PRF query encoder is effective.- Unlike some other dense retrievers that change both query and document encoders (e.g. ME-BERT) or indexing (e.g. LTRre), ANCE-PRF only introduces a new query encoder. This makes deployment easier without duplicating indexes.- The gains from ANCE-PRF are particularly large on complex queries, showing robustness. This contrasts with other work focused on improving training strategies that may still struggle on difficult queries.In summary, this paper shows PRF is an effective technique for improving query representations in dense retrieval, demonstrating sizable improvements over state-of-the-art baselines. The analyses provide insights into the model's inner workings. The proposed ANCE-PRF query encoder approach is shown to be accurate, robust, and easy to deploy.
