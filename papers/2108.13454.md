# [Improving Query Representations for Dense Retrieval with Pseudo   Relevance Feedback](https://arxiv.org/abs/2108.13454)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is: How can pseudo relevance feedback (PRF) be used to improve query representations for dense retrieval? The key hypothesis is that leveraging PRF documents from an initial retrieval pass can help produce better query embeddings that more accurately reflect the underlying search intent, compared to only using the original query. The paper proposes a model called ANCE-PRF that consumes both the query and PRF documents to learn improved query representations.Some key points:- Dense retrieval relies on encoded embeddings to capture query/document semantics, but this is challenging due to query ambiguity. - PRF is commonly used in sparse retrieval to expand/reweight queries. This work explores using PRF to improve query encodings in dense retrieval.- ANCE-PRF uses a BERT encoder to consume the query and PRF docs from an initial ANCE retrieval. It learns to produce better query embeddings directly from relevance labels.- Experiments show ANCE-PRF outperforms ANCE and other dense retrieval methods, especially on complex queries. - Analysis indicates ANCE-PRF learns to leverage useful PRF info and ignore noise via its attention mechanism.In summary, the central hypothesis is that PRF can be used to improve query representations in dense retrieval, which ANCE-PRF confirms through strong empirical results and analysis.


## What is the main contribution of this paper?

The main contribution of this paper is proposing ANCE-PRF, a new query encoder that improves query representations for dense retrieval using pseudo relevance feedback (PRF). Specifically:- ANCE-PRF consumes the original query and top retrieved documents from an initial dense retriever to produce better query embeddings. It is trained end-to-end using relevance labels.- ANCE-PRF significantly outperforms the initial dense retriever ANCE and other recent dense retrieval models on several benchmarks. It is among the best performing first-stage retrieval systems on the highly competitive MS MARCO leaderboard.- Analysis shows ANCE-PRF learns to leverage useful information from relevant PRF documents and ignore noise using its attention mechanism. The encoder focuses more on PRF terms complementary to the query terms.- ANCE-PRF provides a simple yet effective way to incorporate PRF signals into dense retrieval systems. It reuses the document index to avoid overhead.In summary, the main contribution is using PRF documents within a learned query encoder to improve query representations for dense retrieval. This straightforward integration of classic PRF into modern dense retrieval is shown to bring significant accuracy improvements.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper proposes ANCE-PRF, a new query encoder for dense retrieval that uses pseudo relevance feedback documents from an initial dense retriever to learn better query representations and improve retrieval accuracy.


## How does this paper compare to other research in the same field?

Here are a few key points about how this paper compares to other research in dense retrieval:- This paper proposes a new query encoder called ANCE-PRF that improves query representations by incorporating pseudo relevance feedback (PRF) documents. Other dense retrieval methods like ANCE, DPR, ME-BERT typically only encode the query text itself. Using PRF is a classic technique in sparse retrieval but novel for dense retrieval.- Experiments show ANCE-PRF outperforms several state-of-the-art dense retrievers like ANCE, ME-BERT, and DPR across multiple datasets. This demonstrates the effectiveness of the proposed PRF approach.- The paper analyses the learned embeddings and attention weights, revealing how ANCE-PRF focuses more on relevant terms in the PRF documents. This provides insights into why the PRF query encoder is effective.- Unlike some other dense retrievers that change both query and document encoders (e.g. ME-BERT) or indexing (e.g. LTRre), ANCE-PRF only introduces a new query encoder. This makes deployment easier without duplicating indexes.- The gains from ANCE-PRF are particularly large on complex queries, showing robustness. This contrasts with other work focused on improving training strategies that may still struggle on difficult queries.In summary, this paper shows PRF is an effective technique for improving query representations in dense retrieval, demonstrating sizable improvements over state-of-the-art baselines. The analyses provide insights into the model's inner workings. The proposed ANCE-PRF query encoder approach is shown to be accurate, robust, and easy to deploy.


## What future research directions do the authors suggest?

Based on my reading of the paper, some future research directions suggested by the authors are:- Exploring other ways to leverage pseudo relevance feedback (PRF) information to improve dense retrieval, beyond the query encoder approach proposed in this paper. The authors state that ANCE-PRF provides a straightforward way to use PRF signals in dense retrieval and can serve as a plugin, implying there may be other techniques worth exploring as well.- Conducting further analysis into why and how PRF helps dense retrieval systems. The authors provide some analyses about the embedding space and attention patterns, but more investigation could lead to additional insights.- Studying if and how PRF could help in other neural IR tasks beyond first-stage retrieval, such as reranking. The authors focus on improving the query representations for initial retrieval, but PRF may also be useful in downstream stages.- Evaluating the robustness of ANCE-PRF to other types of noise, not just irrelevant documents in the PRF set. The authors show robustness against imperfect feedback, but there may be other sources of noise to examine.- Exploring whether techniques like ANCE-PRF could help improve query understanding in conventional sparse retrieval in addition to dense retrieval. The authors motivate PRF as addressing query understanding challenges, which also exist in sparse models.- Developing enhanced training frameworks or architectures to better utilize PRF signals, instead of just using PRF documents as extra input. The Transformer attention mechanism provides some benefits, but custom designs could help further.In summary, the authors' work demonstrates the promise of PRF for improving dense retrievers, and they suggest various ways to build on this idea in future work. More analysis, evaluation across tasks, and novel neural architectures could further advance PRF for neural IR.


## Summarize the paper in one paragraph.

Here is a one paragraph summary of the paper:This paper proposes ANCE-PRF, a new query encoder that uses pseudo relevance feedback (PRF) to improve query representations for dense retrieval. ANCE-PRF consumes the query and top retrieved documents from an initial dense retriever ANCE, and learns to produce better query embeddings directly from relevance labels. It keeps the document index unchanged to reduce overhead. Experiments on MS MARCO and TREC Deep Learning benchmarks show ANCE-PRF significantly outperforms ANCE and other recent dense retrieval systems. Analysis indicates the PRF encoder effectively captures relevant/complementary information from PRF documents while ignoring noise, by allocating more attention to useful PRF terms. ANCE-PRF provides a straightforward way to leverage classic PRF for dense retrieval.
