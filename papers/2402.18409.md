# [A Cognitive Evaluation Benchmark of Image Reasoning and Description for   Large Vision Language Models](https://arxiv.org/abs/2402.18409)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Large vision language models (LVLMs) have shown impressive capabilities, but lack comprehensive evaluation of their high-level cognitive reasoning abilities. 
- Existing LVLM benchmarks either do not focus specifically on reasoning abilities or use images that require limited reasoning to understand.
- The classic "Cookie Theft" picture description task used in human cognition assessment provides inspiration for a challenging benchmark to evaluate LVLM cognitive skills.

Proposed Solution:
- The paper proposes CogBench, a new benchmark to evaluate LVLM cognitive reasoning using rich, story-like images requiring inferences.
- CogBench defines 8 reasoning capabilities: time, location, character, relationships, events, causality, future events, mental states.
- It contains an image description and a multiple choice QA task.
- 95 images were collected and annotated with entities, chain of reasoning, descriptions. 1091 questions were semi-automatically generated.

Contributions:
- First benchmark to incorporate the human-centric Cookie Theft assessment into LVLM evaluation.
- Created the largest dataset of story-like images to test LVLM cognitive skills.  
- Evaluation of state-of-the-art LVLMs shows significant gap between their reasoning abilities and humans, indicating CogBench is very challenging.

In summary, the paper designed a new benchmark called CogBench to evaluate the cognitive reasoning skills of LVLMs using rich story-like images. Experiments show top LVLMs still fall far short of human abilities in high-level inference tested by CogBench.
