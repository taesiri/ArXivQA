# [How to Prune Your Language Model: Recovering Accuracy on the "Sparsity   May Cry'' Benchmark](https://arxiv.org/abs/2312.13547)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Recently, the "Sparsity May Cry (SMC)" benchmark paper questioned the validity of all existing pruning methods for large language models (LLMs), showing they fail even at low sparsity levels on more complex downstream tasks. 
- This "alarmingly demands the attention of the sparsity community" to reconsider the benefits of sparse neural networks.

Proposed Solution:  
- The authors revisit LLM pruning, particularly BERT models, and propose general guidelines for successful pruning even on SMC:

1. Adapt schedule lengths, sparsification rates and learning rates based on target sparsity and model/task.

2. Avoid pruning embeddings and classification heads which have outsized impact but bring negligible performance gains. Only prune encoder. 

3. Use properly tuned knowledge distillation which is highly effective for LM pruning.

- They validate these on BERT base and SMC's RoBERTa-large on tasks like SQuAD, MNLI, QQP and CommonsenseQA.

Main Contributions:
- Codify a set of best practices for LLM pruning either novel or implicitly adopted in literature.

- Show adapting SMC setup to follow these inverts its strong negative claims. Even basic gradual magnitude pruning (GMP) recovers accuracy well.  

- Set new SOTA sparsity-accuracy results for BERT-base on SQuAD. Show GMP and advanced oBERT method can provide stable results on SMC up to 90% sparsity, demonstrating viability of unstructured sparsity.

Overall the paper provides simple and effective guidelines for LM pruning while highlighting their importance using strong experimental validation across models, tasks and prior work.
