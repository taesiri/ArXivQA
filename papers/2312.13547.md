# [How to Prune Your Language Model: Recovering Accuracy on the "Sparsity   May Cry'' Benchmark](https://arxiv.org/abs/2312.13547)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Recently, the "Sparsity May Cry (SMC)" benchmark paper questioned the validity of all existing pruning methods for large language models (LLMs), showing they fail even at low sparsity levels on more complex downstream tasks. 
- This "alarmingly demands the attention of the sparsity community" to reconsider the benefits of sparse neural networks.

Proposed Solution:  
- The authors revisit LLM pruning, particularly BERT models, and propose general guidelines for successful pruning even on SMC:

1. Adapt schedule lengths, sparsification rates and learning rates based on target sparsity and model/task.

2. Avoid pruning embeddings and classification heads which have outsized impact but bring negligible performance gains. Only prune encoder. 

3. Use properly tuned knowledge distillation which is highly effective for LM pruning.

- They validate these on BERT base and SMC's RoBERTa-large on tasks like SQuAD, MNLI, QQP and CommonsenseQA.

Main Contributions:
- Codify a set of best practices for LLM pruning either novel or implicitly adopted in literature.

- Show adapting SMC setup to follow these inverts its strong negative claims. Even basic gradual magnitude pruning (GMP) recovers accuracy well.  

- Set new SOTA sparsity-accuracy results for BERT-base on SQuAD. Show GMP and advanced oBERT method can provide stable results on SMC up to 90% sparsity, demonstrating viability of unstructured sparsity.

Overall the paper provides simple and effective guidelines for LM pruning while highlighting their importance using strong experimental validation across models, tasks and prior work.


## Summarize the paper in one sentence.

 This paper proposes three best practices for pruning large language models: not pruning embeddings and classifier heads, properly scaling training schedules relative to target sparsity, and using well-tuned knowledge distillation, which together enable high performance even on challenging benchmarks.


## What is the main contribution of this paper?

 Based on my reading of the paper, the main contributions are:

1) The paper codifies a consistent set of pruning best-practices for large language models (LLMs), including guidelines on what model components to prune (e.g. not embeddings or classification head), how to scale training/pruning schedules, and the importance of properly-tuned knowledge distillation. Some of these were known but not consistently adopted, while others are newly proposed.

2) The paper shows that following these best practices enables accurate pruning even on the challenging SMC benchmark, which previously claimed all pruning methods fail on its tasks. With the right approach, even basic gradual magnitude pruning (GMP) can reach high sparsities (80-90%) without collapsing in accuracy. This contradicts the strongly negative claims made by the SMC benchmark.

3) Together with the oBERT pruner, the guidelines lead to new state-of-the-art sparsity vs. accuracy results on standard BERT pruning benchmarks like SQuAD. The simplified setup also provides strong baselines for future research.

In summary, the main contribution is both constructive (proposing guidelines) and deconstructive (re-evaluating claims of SMC benchmark), leading to simplified and accurate recipes for LLM pruning.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and concepts are:

- Language model pruning - The paper focuses on techniques for pruning large language models like BERT to make them more compact and efficient. This is a key focus.

- Sparsity - Creating sparse neural networks by pruning weights is a core technique explored. Sparsity and sparse neural networks are key ideas. 

- SMC benchmark - The "Sparsity May Cry" benchmark is used to evaluate pruning methods, so this is an important term.

- Best practices - The paper proposes best practices and guidelines for effectively pruning language models in terms of what layers to prune, training schedules, etc.

- Gradual magnitude pruning (GMP) - A simple baseline pruning technique that is re-evaluated in light of the proposed best practices. 

- Knowledge distillation - Using knowledge distillation during pruning to retain accuracy is discussed.

- Encoder, embeddings, classifier head - Key components of transformer language models like BERT that are analyzed separately.

- Scheduling - The importance of properly scheduling pruning and finetuning steps. Key hyperparameters like sparsity schedule and learning rate schedule.

In summary, the key terms cover sparse neural networks, pruning techniques, benchmarking, best practices and guidelines, model components and training procedures.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in this paper:

1. The authors propose keeping the embeddings and classification head dense during pruning. What is the justification provided for this? How much of a difference does it make in practice to prune versus not prune these components?

2. The paper argues that the pruning and fine-tuning schedules need to be adapted to the desired target sparsity. What specific recommendations are provided regarding the sparsity and learning rate schedules? How do these impact results compared to prior work?

3. What temperature is proposed for knowledge distillation during pruning? How does this differ from prior work? What impact does the choice of temperature have on the quality of the transferred knowledge?

4. The authors find that basic magnitude pruning (GMP) can actually work well if following best practices. What modifications are made to GMP compared to prior baselines? Why do these changes lead to significant accuracy improvements?

5. For the CommonsenseQA task, one-shot pruning seems to outperform gradual pruning used in prior work. Why might this be the case? What does this suggest about the gradual pruning schedules used?

6. Across tasks, embedding and classifier pruning lead to negligible efficiency gains but significant accuracy drops. What analysis is provided to demonstrate this? Are there any cases where pruning these components could be beneficial?

7. Could the guidelines and modifications proposed be applied successfully to structured pruning as well? What challenges might arise in adapting this approach?

8. How well does the method scale to even larger models, such as GPT-3? Would the same guidelines apply or need rethinking?

9. The method focuses on the downstream tuning setting. How well could it work for continued pre-training then downstream tuning? Would any components need adaptation?

10. What opportunities are there to automate, instead of manually specifying, some of the guidelines provided around pruning schedules, hyperparameters, etc? What techniques could help enable this automation?
