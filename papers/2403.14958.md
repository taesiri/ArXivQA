# [Adapprox: Adaptive Approximation in Adam Optimization via Randomized   Low-Rank Matrices](https://arxiv.org/abs/2403.14958)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
As deep learning models grow exponentially in size, optimizers like Adam face significant memory consumption challenges due to storing first and second moment statistics for each parameter. Existing memory-efficient methods like Adafactor and CAME compromise accuracy via matrix factorization. 

Solution:
The paper proposes Adapprox, a novel optimizer that leverages randomized low-rank matrix approximation to effectively approximate the second moment in Adam. Key aspects:

- Motivation: Empirical evidence shows second moment matrices in Adam often have limited dominant singular values. This indicates potential for approximation.

- Core Idea: Use randomized low-rank matrix approximation, specifically a Streamlined Randomized Subspace Iteration (S-RSI) method, to compress the second moment matrix while retaining important information.

- Adaptive Rank Selection: Dynamically choose rank for approximation to balance precision and memory savings. Rank expands gradually based on approximation error rate.

- Optional Cosine Similarity Guidance: Use cosine similarity between current update and first moment to modulate learning rate for enhanced stability.

Contributions:

- Identify viability of low-rank approximation for Adam's second moment based on singular value distribution.

- Develop S-RSI method for efficient and accurate approximation.

- Propose adaptive rank selection to auto-balance memory and accuracy.

- Integrate cosine similarity guidance to improve stability and convergence.  

- Achieve 34.5-49.9% memory savings on GPT-2 117M and 33.8-49.9% on 345M with first moment intact, and 84.5-99.9% on 117M and 83.8-99.9% on 345M without first moment, relative to AdamW.

- Accelerate convergence and maintain or improve performance over Adafactor and CAME in GPT-2 pretraining and downstream tasks.

In summary, Adapprox enables memory-efficient training while preserving or enhancing accuracy and convergence through novel randomized low-rank approximation techniques.
