# [Training Chain-of-Thought via Latent-Variable Inference](https://arxiv.org/abs/2312.02179)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
Large language models (LLMs) can solve reasoning problems more accurately when prompted to provide a step-by-step "chain of thought" explaining the reasoning process. However, creating detailed reasoning annotations as supervision for training these models is expensive. The paper aims to develop an effective method to train LLMs to generate high-quality reasoning chains without requiring full supervision.  

Proposed Solution:
The paper frames chain-of-thought reasoning as a latent variable model, where the reasoning chain is treated as an unobserved latent variable. It then proposes an algorithm called TRICE that uses Markov chain Monte Carlo expectation maximization to maximize the marginal likelihood of the model generating the correct answer, approximately averaging over all possible reasoning chains. 

Key ideas in TRICE:
- Maintains a memory of reasoning chains, initialized using a guided model
- Iteratively samples new proposed chains from the current model
- Accepts new chains that lead to the correct answer 
- Uses a control variate scheme to reduce gradient variance

The method unifies ideas from prior work like STaR and self-consistency, but enables averaging over chains during training, not just at test time.

Main Contributions:
- Frames chain-of-thought prompting as a latent variable model
- Proposes TRICE, an MCMC-EM approach to train the model without full supervision 
- Demonstrates state-of-the-art performance on GSM8K and BIG-Bench Hard, outperforming STaR, rejection sampling, and even supervised tuning
- Provides a unifying view connecting various lines of work on reasoning with LLMs

The key insight is to treat reasoning chains as latent variables and use Bayesian inference techniques to marginalize over them. This enables effectively training models to generate better reasoning without expensive full supervision.
