# [From Static to Dynamic: Adapting Landmark-Aware Image Models for Facial   Expression Recognition in Videos](https://arxiv.org/abs/2312.05447)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem: 
Dynamic facial expression recognition (DFER) in unconstrained videos is challenging due to insufficient training data, limited diversity in poses/occlusions/illuminations, inherent ambiguity in facial expressions, and lack of modeling of temporal dynamics. In contrast, static facial expression recognition (SFER) has abundant data and strong performance. 

Proposed Solution:
The paper proposes a Static-to-Dynamic (S2D) model that efficiently adapts a pre-trained landmark-aware image model for SFER to handle DFER in videos. S2D incorporates two key components:

1) Multi-View Complementary Prompters (MCPs): Enhances image-level features by fusing the pre-trained SFER features with facial landmark-aware features that focus on emotion-related regions. The landmark features also implicitly encode facial dynamics.

2) Temporal-Modeling Adapters (TMAs): Efficiently captures temporal dynamics and facial expression changes over time to expand the static model for videos. TMA adapts only a small fraction (<10%) of parameters.

An auxiliary Emotion-Anchors based Self-Distillation Loss is also introduced to provide reliable supervision for dealing with label ambiguity.

Main Contributions:

- Enhances image-level facial expression features by selection of robust SFER data and incorporation of landmark-aware features using MCPs

- Expands static model efficiently using lightweight TMAs to capture temporal dynamics, with only 9M extra trainable parameters  

- Achieves new state-of-the-art on multiple DFER benchmarks (DFEW, FERV39K, MAFW)

- Provides a simple, efficient and strong baseline for DFER through effective transfer learning, requiring only a fraction of parameters to be tuned

The proposed S2D framework effectively addresses key challenges in DFER by harnessing prior SFER knowledge and modeling dynamics, demonstrating the viability of adapting image models for in-the-wild video facial expression recognition.
