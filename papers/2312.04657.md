# [Self-Supervised Behavior Cloned Transformers are Path Crawlers for Text   Games](https://arxiv.org/abs/2312.04657)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Text-based games require agents to take sequential actions to accomplish goals, but generating supervised training data is expensive. 
- Reinforcement learning agents struggle to explore large action spaces effectively.

Proposed Solution: 
- Introduce a self-supervised behavior cloning transformer that automatically generates its own training data by crawling trajectories (sequences of actions) in text games.
- Group trajectories into "path groups" based on similar macro-action sequences. Evaluate path groups by training small T5 models on them and testing on unseen game variations.
- Iteratively extend high-scoring path groups by further crawling until the full game is solved.

Contributions:
- Demonstrate generating self-supervised training data via crawling, grouping and evaluating game trajectories. 
- Use performance of rapidly-trained compact models on unseen games as a self-supervision signal.
- Develop methods to align and merge training data to reduce search space and training costs.

Results:
- Self-supervised transformer achieves ~90% performance of supervised transformer, significantly outperforms RL baseline, and is comparable to GPT-4 despite being much smaller.
- Identified self-supervised trajectories closely match human gold standard trajectories.
- Analysis provides insights into data efficiency, effect of training set noise, and limitations.

Overall, the paper presents a method to automatically generate training data for text game agents, reducing the need for expensive human annotations. The self-supervision approach enables small models to explore effectively and match the performance of much larger models.
