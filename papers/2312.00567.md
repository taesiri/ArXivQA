# [Explanatory Argument Extraction of Correct Answers in Resident Medical   Exams](https://arxiv.org/abs/2312.00567)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a detailed summary paragraph of the key points from the paper:

This paper introduces a new Spanish medical question answering dataset based on resident medical exams (MIR) that includes explanatory arguments for both the correct answer as well as reasoning why the incorrect answers are not correct. Unlike most existing medical QA datasets which are in English and consist of short or yes/no answers, this dataset contains full explanatory arguments written by medical doctors. The authors leverage this to formulate a novel extractive QA task of identifying the explanation supporting the correct answer from among explanations for all answer choices. They experiment with various Spanish language models, including multilingual, monolingual, general domain, and medical domain models. The best overall performance is achieved by the non-medical multilingual XLM-RoBERTa model, showing the potential of multilingual models even outperforming medical domain models. However, results across models are mixed, with smaller monolingual models being competitive. The authors find that their approach can effectively help identify relevant explanations to medical questions. Challenges remain due to the complex lengthy explanations, but their work facilitates research in medical argumentation and QA.


## Summarize the paper in one sentence.

 Here is a one-sentence summary of the key points from the paper:

The paper presents a new Spanish medical exam dataset with explanatory arguments for correct and incorrect answers to formulate a novel extractive task of identifying the correct answer's explanation, allowing automatic evaluation of language models' performance in assisting with evidence-based medical exam question answering.


## What is the main contribution of this paper?

 According to the paper, the main contributions are:

1) The first dataset for benchmarking LLMs in the medical domain which contains argumentative structure to provide explanations for both correct and incorrect answers in a multiple-choice questionnaire.

2) A novel task based on extractive QA to identify the explanations of the correct answer in commented medical exams, thereby helping medical doctors to automatically retrieve relevant evidence from existing resources. 

3) The first dataset for medical QA in a language other than English (Spanish), facilitating future research on multilingual approaches.  

4) By formulating this task in an extractive setting, the authors can objectively and automatically evaluate the performance of LLMs without requiring any manual annotation by medical experts.

5) Comprehensive experimentation with LLMs for Spanish demonstrates that sometimes multilingual models fare better than monolingual models, even outperforming those adapted to the medical domain. Results show that models across the monolingual models are mixed, with supposedly smaller models performing competitively.

6) The results suggest that the proposed approach can be an effective technique to help medical practitioners in identifying relevant evidence-based explanations for medical questions.


## What are the keywords or key terms associated with this paper?

 Based on reviewing the paper, some of the main keywords and key terms associated with it include:

- Explainable Artificial Intelligence
- Argumentation  
- Question Answering
- Resident Medical Exams
- Natural Language Processing
- Deep Learning

These keywords cover the main topics and themes discussed in the paper, such as using explainable AI and argumentation techniques for question answering on medical resident exams, leveraging natural language processing and deep learning methods. The paper also focuses specifically on creating a new Spanish language dataset and task for this purpose.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in this paper:

1. What was the motivation behind creating a new dataset based on Spanish MIR exams with explanatory arguments for correct and incorrect answers? How is this different from existing medical QA datasets?

2. Why did the authors formulate a novel extractive task for identifying the explanation of the correct answer? What are the benefits of framing this as an extractive QA problem rather than a generative one?  

3. What methods were used to annotate the newly created datasets with spans referring to explanations of the correct answers? How much manual effort was involved in creating the annotations?

4. Explain the different variants that were created from the CasiMedicos and MIR Asturias datasets to formulate them as extractive QA tasks. Why was it useful to experiment with different combinations of the context as question and vice versa?

5. Discuss the differences in writing style, explanations, and other characteristics between the CasiMedicos and MIR Asturias datasets. How did these differences likely impact performance? 

6. Analyze why models fine-tuned on the structured MIR Asturias data struggled on certain cases - for example when explicit pattern pointers were absent. What does this suggest about the models' reasoning abilities?

7. Explain the comparative results between multilingual and monolingual models. Why did multilingual models like XLM-RoBERTa sometimes outperform specialized medical models? What does this indicate about transfer learning?

8. Discuss the impact of using SQuAD/SQAC datasets for intermediate fine-tuning. What kinds of qualitative improvements were observed on the target task when using this STILT approach?

9. How suitable is the CasiMedicos dataset for training robust clinical argumentation models? What kinds of future work could be enabled by releasing this dataset?

10. Beyond extractive QA, what other potential tasks could be formulated based on the dataset's unique characteristics? How could recent advances in areas like label projection be applied?
