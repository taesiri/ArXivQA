# [Humans or LLMs as the Judge? A Study on Judgement Biases](https://arxiv.org/abs/2402.10669)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Adopting humans or large language models (LLMs) as judges to evaluate LLM performance is gaining attention, but introduces potential biases that question the reliability of evaluation results.

Proposed Solution: 
- Develop a framework to investigate 5 types of biases (factual oversight, authority, beauty, verbosity, positional) for both human and LLM judges. 
- Curate a dataset of 142 questions based on Bloom's Taxonomy and conduct thousands of human and LLM evaluations.
- Analyze results to understand vulnerabilities of judges to different perturbations.
- Conduct attacks by exploiting judge biases to showcase weaknesses.

Key Contributions:
- Propose a new bias evaluation framework without needing human/groundtruth answers, making it flexible.  
- Systematically study bias vulnerability of human and LLM judges to perturbations.
- Find both judges possess considerable biases, with humans prone to beauty/verbosity biases and LLMs to authority bias.
- Show biases can be exploited to attack LLMs, with flawed answers easily exceeding non-flawed counterparts.
- Release an open-sourced dataset for investigating open-ended evaluation.

Main Takeaways:
- Both human and LLM judges are biased to different extents.
- Biases make judges vulnerable to attacks, questioning reliability of evaluation approaches. 
- There is an urgency to develop robust evaluation systems resilient to perturbations.
