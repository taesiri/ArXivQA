# [Humans or LLMs as the Judge? A Study on Judgement Biases](https://arxiv.org/abs/2402.10669)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Adopting humans or large language models (LLMs) as judges to evaluate LLM performance is gaining attention, but introduces potential biases that question the reliability of evaluation results.

Proposed Solution: 
- Develop a framework to investigate 5 types of biases (factual oversight, authority, beauty, verbosity, positional) for both human and LLM judges. 
- Curate a dataset of 142 questions based on Bloom's Taxonomy and conduct thousands of human and LLM evaluations.
- Analyze results to understand vulnerabilities of judges to different perturbations.
- Conduct attacks by exploiting judge biases to showcase weaknesses.

Key Contributions:
- Propose a new bias evaluation framework without needing human/groundtruth answers, making it flexible.  
- Systematically study bias vulnerability of human and LLM judges to perturbations.
- Find both judges possess considerable biases, with humans prone to beauty/verbosity biases and LLMs to authority bias.
- Show biases can be exploited to attack LLMs, with flawed answers easily exceeding non-flawed counterparts.
- Release an open-sourced dataset for investigating open-ended evaluation.

Main Takeaways:
- Both human and LLM judges are biased to different extents.
- Biases make judges vulnerable to attacks, questioning reliability of evaluation approaches. 
- There is an urgency to develop robust evaluation systems resilient to perturbations.


## Summarize the paper in one sentence.

 This paper proposes a framework to investigate five types of biases (factual oversight, authority, beauty, verbosity, and positional) in human and language model judges for open-ended question evaluation, finding both are vulnerable to perturbations but different models and humans possess biases to varying degrees.


## What is the main contribution of this paper?

 The main contributions of this paper are three-fold:

1. The authors propose a new framework to investigate five types of biases (factual oversight bias, authority bias, beauty bias, verbosity bias, and positional bias) for both human and LLM judges. This framework does not require a golden standard or human reference answers, making it more flexible and extensible.

2. The authors systematically investigate the vulnerability of human and LLM judges against various perturbations through thousands of evaluations. They further exploit the discovered weaknesses to attack different LLM evaluators.  

3. The authors open-source a dataset consisting of 142 question-answer pairs for open-ended evaluation. This serves as an in-depth alternative to existing in-width datasets like Vicuna-80.

In summary, the key contribution is the new bias analysis framework along with a comprehensive investigation of biases in both human and LLM judges. The findings reveal considerable vulnerabilities in even cutting-edge models, highlighting the need for more robust evaluation systems. The open-sourced dataset also facilitates further research in this direction.


## What are the keywords or key terms associated with this paper?

 Based on my review of the paper content, some of the key terms and keywords associated with this paper include:

- Bias evaluation 
- Human judges
- LLM judges
- Intervention 
- Post-hoc analysis
- Fallacy oversight bias
- Authority bias 
- Beauty bias
- Verbosity bias
- Positional bias
- Attack successful rate (ASR)
- Accuracy  
- Perturbations
- Experimental design
- Factual errors
- References
- Rich content

The paper proposes a framework to investigate biases in human and LLM judges through experimental intervention and post-hoc analysis. It examines 5 types of biases - fallacy oversight, authority, beauty, verbosity, and positional. Key metrics used include attack successful rate and accuracy. Different perturbations are introduced such as factual errors, fake references, and rich content. The goal is to assess and compare the vulnerabilities of human judges versus LLM judges when evaluating open-ended text generation.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in this paper:

1. The paper proposes a new framework for investigating biases in human and LLM judges. How does this framework differ from existing bias evaluation frameworks? What are the key advantages it offers?

2. The paper investigates 5 types of biases - fallacy oversight, authority, beauty, verbosity, and positional. Why were these specific biases chosen for analysis? What common thread connects these biases? 

3. The paper uses both intervention and post-hoc analysis as research methods. Can you explain the difference between these two methods and why both were needed to fully analyze the biases?

4. One of the key dataset contributions is the curation of questions based on Bloom's Taxonomy. Why is following this taxonomy useful? How does it help ensure the questions properly test different aspects of language understanding?

5. The paper finds human judges display significant biases, contrary to their common position as a gold standard. What implications does this have for future research relying on human evaluations? How can human bias be accounted for?

6. Different LLM judges are found to possess different biases. What might explain the variation in biases across models? How could model architecture, training data, etc. contribute to certain biases?

7. The paper shows biases can be exploited to attack LLM judges. If these models were deployed for real applications, what kinds of harms could result from such attacks? How might systems be made more robust?

8. The data collection process involves several steps of question generation, answer generation, and manual verification. What is the rationale behind this pipeline? What quality checks are in place?

9. How reusable and extensible is the proposed framework for studying additional biases beyond the five analyzed? What modifications would enable investigating new biases?

10. The paper analyzes model biases, but does not deeply address how to mitigate them. What are some concrete next steps that could be taken to reduce biases in LLM judges?
