# [Do Language Models Care About Text Quality? Evaluating Web-Crawled   Corpora Across 11 Languages](https://arxiv.org/abs/2403.08693)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Large web-crawled corpora like CC100, MaCoCu, mC4 and OSCAR play a vital role in training state-of-the-art language models. However, there has been little analysis on the quality of these corpora.  
- It is unclear how qualitative differences between these corpora actually impact downstream model performance when used for language model pretraining.

Methodology: 
- The authors perform both an intrinsic evaluation (human evaluation of sample texts) and an extrinsic evaluation (language model pretraining and downstream task performance) on the quality of web-crawled corpora.
- The evaluation covers 11 European languages across 4 major web-crawled corpora.  
- For intrinsic evaluation, professional linguists annotated sample paragraphs on quality criteria like well-formedness and publishability.  
- For extrinsic evaluation, XLM-R was pretrained on each corpus per language and evaluated on structured prediction (NER, POS) and language understanding (COPA, CommitmentBank) tasks.

Key Findings:
- Intrinsic: MaCoCu and OSCAR have highest quality based on human judgements, while mC4 has more non-textual or wrong language data.  
- Extrinsic: Surprisingly, CC100 achieves best downstream task performance instead of highest rated corpora from human evaluation. The qualitative differences did not directly impact model performance.
- Even when controlling for corpus size by early stopping, the human-annotated quality does not influence end results.

Main Conclusions:
- There are clear intrinsic differences between quality of web-crawled corpora based on human evaluation.  
- However, the quality does not seem to significantly influence the downstream performance of language models trained on them.
- So for language model pretraining, size seems far more important than qualitative subtle differences between web-crawled data sets.


## Summarize the paper in one sentence.

 Here is a one-sentence summary of the paper:

The paper evaluates four major web-crawled corpora across eleven European languages through human evaluation and language model training, finding that while the corpora differ significantly in quality, these differences do not directly translate to performance differences when used to train language models.


## What is the main contribution of this paper?

 Based on my reading of the paper, the main contribution is:

The paper performs an intrinsic human evaluation and extrinsic automatic evaluation to compare four major web-crawled corpora (CC100, MaCoCu, mC4 and OSCAR) across 11 European languages. It finds clear differences in quality between the corpora based on human judgement, with MaCoCu and OSCAR being superior. However, when training language models on these corpora and evaluating on downstream tasks, there is no clear correlation between human-judged quality and downstream performance. The paper concludes that the quality of web-crawled corpora does not seem to play a significant role when training language models, at least based on the metrics evaluated.

The key contribution is providing an analysis of both the intrinsic quality and extrinsic utility of major web-crawled corpora used to train language models, and finding a disconnect between human judgements of quality and actual language model performance. This suggests quality metrics and cleaning processes for these corpora need to be reconsidered.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key keywords and terms associated with this paper include:

- Monolingual corpora - The paper evaluates and compares different monolingual corpora used for training language models.

- Corpus evaluation - A main focus of the paper is evaluating the quality of different web-crawled monolingual corpora.

- Large language models (LLMs) - The corpora are intended to train large language models. The paper examines if corpus quality impacts LLM performance.

- Web-crawled corpora - Specifically evaluates large web-crawled corpora like CC100, MaCoCu, mC4, and OSCAR.

- Lower-resourced languages - The evaluation is done across 11 lower-resourced European languages. 

- Corpus quality - Intrinsically evaluates corpus quality through human annotations and extrinsically by training LMs.

- Downstream performance - Evaluates if corpus quality differences impact performance on downstream NLP tasks.

- Continued training - Uses continued training of XLM-RoBERTa instead of training LMs from scratch.

So in summary, key terms cover monolingual corpora, corpus quality evaluation, web-crawled data, lower-resourced languages, large language models, continued training, and downstream performance.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 potential in-depth questions about the method proposed in this paper:

1. The paper compares web-crawled corpora across 11 languages. Could the results generalize to other languages not examined in the paper, especially higher-resourced languages? Why or why not?

2. The manual evaluation uses a somewhat simple 5-point scheme to rate corpus quality. Could using a more granular or multi-dimensional rating scheme reveal more nuanced differences between the corpora? 

3. The paper finds differences in corpus quality through manual evaluation but these do not clearly transfer when training language models. What factors could explain this discrepancy?

4. For the automatic evaluation, the paper trains encoder-only monolingual models. Would the results change significantly if decoder-only or encoder-decoder models were trained instead? Why?  

5. The CC100 corpus outperforms other corpora in downstream tasks despite not rating best in quality. Does this suggest quality metrics need rethinking? Or are there other explanatory factors?

6. The paper does not control for data set size differences. But could differences in size fully explain downstream performance differences? Are additional experiments needed?

7. The paper trains models for 50,000 steps. Would training for longer change the conclusions? At what point would you expect performance to plateau?

8. The paper uses 4 downstream tasks. Would using more or different tasks reveal different corpus preferences for different applications? 

9. Could differences in corpus cleaning processes before release explain downstream performance differences better than intrinsic quality ratings?

10. The paper trains monolingual models. Would quality play a bigger role when training massively multilingual models? Why might this be the case?
