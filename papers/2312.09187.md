# [Vision-Language Models as a Source of Rewards](https://arxiv.org/abs/2312.09187)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a high-quality paragraph summarizing the key points of the paper:

This paper investigates using vision-language models (VLMs) like CLIP to automatically generate reward functions for training reinforcement learning agents, without the need for environment-specific finetuning. The authors propose extracting a sparse binary reward signal indicating goal achievement from the cosine similarity between CLIP's embeddings of the current observation and desired goal description. They demonstrate this approach in procedural environments like Playhouse for object manipulation goals, and on Android emulator for app launching goals. The authors find that optimizing the VLM rewards leads to increased ground-truth returns, and show a scaling trend where larger VLMs (e.g. 1.4B parameter models) produce more accurate rewards and higher-performing policies, likely due to more robust visiolinguistic understanding. Additionally, they highlight the importance of prompt engineering for extracting better quality rewards from VLMs. Overall, this work makes a compelling case for utilizing modern VLMs in an off-the-shelf manner to produce capable goal-conditioned RL agents without environment-specific tuning, paving the way for more generalized reinforcement learning.
