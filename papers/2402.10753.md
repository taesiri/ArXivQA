# [ToolSword: Unveiling Safety Issues of Large Language Models in Tool   Learning Across Three Stages](https://arxiv.org/abs/2402.10753)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
The paper focuses on addressing safety issues of large language models (LLMs) in the context of tool learning. Tool learning involves LLMs utilizing external APIs/tools to interact with the real-world, which enhances their capabilities but introduces new safety risks. The paper notes that current research emphasizes improving LLMs' tool usage abilities without adequately considering emerging safety considerations.

Proposed Solution:
The paper introduces "ToolSword", a comprehensive framework to systematically uncover safety issues for LLMs across different stages of the tool learning process. ToolSword outlines six key safety scenarios spanning the input stage (receiving user queries), execution stage (selecting tools) and output stage (providing responses). These scenarios cover malicious user queries, noisy tool selections, harmful tool outputs etc. that can lead to unsafe LLM behaviors.

Main Contributions:
- Devises the first dedicated testing framework - ToolSword - to evaluate LLM safety issues in tool learning scenarios. The framework conducts multi-stage testing through meticulously designed safety scenarios.
- Evaluates 11 major open/closed-source LLMs using ToolSword reveals significant lingering safety gaps - even for advanced models like GPT-4 - in handling unsafe queries, selecting risky tools and generating harmful responses. 
- Highlights need to enhance safety considerations in tool learning research and provides benchmark to assess improvements. ToolSword data is released to further research.
- Additional studies indicate LLMs can perform to human levels in ideal tool learning settings, hence emphasizing that improving safety is vital to enable their practical usage.

In summary, the paper makes an important contribution by systematically evaluating and revealing safety vulnerabilities of LLMs in tool learning contexts through ToolSword. The findings underline the pressing need to address safety issues to facilitate trustworthy deployment of LLMs leveraging tool learning.
