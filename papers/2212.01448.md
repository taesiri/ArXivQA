# [PGFed: Personalize Each Client's Global Objective for Federated Learning](https://arxiv.org/abs/2212.01448)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question seems to be: 

How can personalized federated learning algorithms more explicitly and efficiently transfer collaborative knowledge across clients to improve model generalization?

The key points I gathered are:

- Most existing personalized federated learning (FL) algorithms implicitly transfer collaborative knowledge by embedding it into model aggregation or regularization. 

- The authors propose that more explicitly involving multiple clients' risks in each client's objective could improve generalization, but requires prohibitive O(N^2) communication costs.

- To address this, the authors propose PGFed, a novel personalized FL algorithm that enables each client to formulate a "personalized global objective" by adaptively aggregating its own and others' risks. 

- PGFed circumvents the high communication costs by using first-order approximations to estimate non-local risks for each client.

- Experiments show PGFed improves generalization and outperforms state-of-the-art personalized FL algorithms.

In summary, the central hypothesis is that more explicit risk aggregation can improve personalized model generalization in FL, and the authors propose an efficient way to achieve this with PGFed.


## What is the main contribution of this paper?

 Based on my understanding, the main contributions of this paper are:

1. It uncovers that the explicitness of a personalized federated learning (FL) algorithm empowers it with stronger adaptation ability. The paper shows through an empirical study that directly involving multiple clients' empirical risks in the local objective (an explicit way) leads to better adaptation performance compared to just using the local risk (an implicit way). 

2. It proposes a novel explicit personalized FL algorithm called PGFed that enables each client to formulate its local objective as a "personalized global objective". This allows explicit aggregation of multiple clients' risks without massive communication costs by using first-order approximations of the non-local risks.

3. It further proposes PGFedMo, an upgrade to PGFed using momentum to efficiently utilize risks from clients who are not participating in the current round. 

4. Extensive experiments on four datasets under different federated settings demonstrate consistent improvements of the proposed PGFed and PGFedMo over state-of-the-art personalized FL methods, with up to 15.47% boost in accuracy.

In summary, the key novelty and contribution is proposing an explicit way to transfer global collaborative knowledge in personalized FL without incurring huge communication overhead. This is achieved through the personalized global objectives and the non-local risk estimations. Both PGFed and PGFedMo consistently outperform previous personalized FL methods.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the key points from the paper:

The paper proposes a novel personalized federated learning framework called PGFed that enables each client to personalize its own global objective by explicitly and adaptively aggregating the empirical risks of itself and other clients, avoiding massive communication overhead through first-order approximation of non-local risks.


## How does this paper compare to other research in the same field?

 Here is my summary of how this paper compares to other research in federated learning:

This paper proposes a new personalized federated learning framework called PGFed that enables clients to explicitly incorporate other clients' risks into their local objectives. This is a novel approach compared to most prior personalized federated learning methods, which typically only transfer knowledge between clients implicitly through model aggregation or regularization terms. 

The key innovation of PGFed is the formulation of the local objective as a "personalized global objective" that is a weighted sum of the client's own risk and estimates of other clients' risks. To avoid the high communication costs of sharing full risks, PGFed uses first-order Taylor approximations. This explicit risk sharing allows for more efficient adaptation and stronger generalization compared to implicit methods.

Prior personalized federated learning algorithms like LG-FedAvg, FedRep, Per-FedAvg, etc. focus on techniques like layer-wise aggregation, meta-learning, and fine-tuning. But they only transfer knowledge implicitly and do not enable clients to directly optimize for other clients' objectives. 

PGFed is the first method to explicitly expose clients to each other's risks in a personalized way, while keeping communication manageable. Experiments show substantial improvements in accuracy over prior state-of-the-art personalized algorithms.

In summary, PGFed introduces a novel explicit risk sharing approach to personalized federated learning that outperforms prior work and enables more efficient adaptation to local data distributions. The personalized global objectives and first-order approximations are innovative techniques not explored by previous methods.
