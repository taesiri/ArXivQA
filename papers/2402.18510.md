# [RNNs are not Transformers (Yet): The Key Bottleneck on In-context   Retrieval](https://arxiv.org/abs/2402.18510)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- The paper investigates whether Recurrent Neural Networks (RNNs), known for efficiently modeling long sequences, can match the representation power of Transformers on algorithmic reasoning tasks. 

- It has been shown that Transformers require Chain-of-Thought (CoT) prompting to perform well on certain algorithmic tasks. Can similar enhancements like CoT help RNNs reach parity with Transformers?

- The paper reveals a key limitation of RNNs compared to Transformers: their inability to perfectly retrieve information from context, even with CoT. This makes RNNs deficient at tasks requiring this "in-context retrieval" capability.

Proposed Solution and Contributions:

1) CoT improves RNNs but cannot close representation gap with Transformers:

- CoT provably makes RNNs more expressive under mild complexity assumptions. 

- However, CoT alone cannot overcome RNNs' in-context retrieval deficiency. RNNs still fail at tasks like associative recall and determining graph connectivity that require this capability, while Transformers can easily solve them.

- Moreover, in-context retrieval may be implicitly required even for seemingly unrelated tasks. RNNs cannot solve a basic algorithmic task of checking graph connectivity due to this reason.

- Transformers can efficiently simulate RNNs, requiring only a small multiplicative parameter overhead, indicating the representation gap is one-sided.

2) Enhancing RNNs' in-context retrieval capability can close this gap:

- Allowing RNNs to invoke function calls for in-context retrieval makes them capable of solving all polynomial-time solvable problems with CoT, closing the gap.

- Alternatively, adding just a single Transformer layer at the end gives RNNs enough implicit retrieval capability to achieve the same effect.

In summary, the paper formally proves RNNs' in-context retrieval deficiency compared to Transformers, and shows addressing this deficiency is sufficient to close the representation gap between them.
