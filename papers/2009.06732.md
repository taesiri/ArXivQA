# [Efficient Transformers: A Survey](https://arxiv.org/abs/2009.06732)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be:How can we make Transformers more efficient in terms of memory usage and computational complexity, particularly with regards to the quadratic complexity of the standard self-attention mechanism?The paper surveys and categorizes the various techniques that have been proposed for developing more efficient Transformer models, focusing especially on innovations related to the self-attention mechanism. It provides a taxonomy of models based on their approach, such as using fixed patterns, combinations of patterns, learnable patterns, memory, low-rank methods, kernels, recurrence, downsampling, and sparsity. The main hypothesis seems to be that there are a variety of ways to improve Transformer efficiency, many of which have shown promise on certain tasks or domains. However, there is still a lack of clarity on which fundamental efficient Transformer block performs the best across different scenarios. The paper aims to provide a structured overview of the landscape of efficient Transformer techniques to help researchers understand the state of the field.


## What is the main contribution of this paper?

Based on my reading, the main contributions of this paper are:1. It provides a comprehensive survey and taxonomy of recent "efficient Transformer" models, i.e. variants of the Transformer architecture that aim to improve its efficiency in terms of computation and memory. 2. It categorizes efficient Transformer models based on the core techniques they employ, such as fixed patterns, combination of patterns, neural memory, low-rank methods, kernels, recurrence, downsampling, and sparsity.3. It gives a detailed walkthrough of several key efficient Transformer models, explaining their architectures and innovations. Models covered include Sparse Transformer, Reformer, Linformer, Synthesizer, etc.4. It discusses evaluation practices and design trends in this area of research, and provides perspective on the state of efficient Transformers. 5. It also briefly covers orthogonal efforts like weight sharing, quantization, NAS, etc. that can improve Transformer efficiency.In summary, this paper provides a comprehensive overview and analysis of the emerging research area of efficient Transformer models, aimed at helping researchers and practitioners navigate and make sense of all the recent innovations. The taxonomy, model walkthroughs, and discussions offer a useful framework to understand efficient Transformers.


## How does this paper compare to other research in the same field?

Here are a few key points about how this paper compares to other research on efficient Transformers:- It provides a broad taxonomy and overview of many recent efficient Transformer models across domains like NLP and computer vision. This makes it a fairly comprehensive survey of techniques for improving Transformer efficiency. Many previous papers focus on proposing a new efficient Transformer variant, without reviewing the wider landscape.- The paper categorizes efficient Transformer techniques into groups like fixed patterns, combination of patterns, learnable patterns, low-rank methods, kernels, recurrence, downsampling, and sparse models. This high-level abstraction offers a useful way to conceptualize the different approaches.- It gives a detailed walkthrough of many influential efficient Transformer models, explaining their core ideas and tradeoffs. This provides good background on seminal works like Sparse Transformers, Performers, Linformers, etc.- The paper discusses evaluation practices and analyzes some trends in model design over time. It provides retrospective thoughts on how the field has evolved and future directions. This meta-level commentary is less common in research papers.- Compared to some other surveys, this paper spans both NLP and vision domains in its coverage of efficient Transformers. Many models were proposed separately in NLP and CV communities, so reviewing both areas is useful.- The scope goes beyond self-attention to also mention orthogonal efficiency efforts like weight sharing, distillation, NAS, etc. This gives a more complete picture of the topic.In summary, this paper delivers an extensive survey and analysis of recent efficient Transformers, covering key techniques, models, and trends across NLP and CV in more depth than most prior works on this topic. The taxonomy, detailed walkthroughs, and discussion offer significant value to researchers in this area.
