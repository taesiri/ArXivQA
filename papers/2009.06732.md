# [Efficient Transformers: A Survey](https://arxiv.org/abs/2009.06732)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be:How can we make Transformers more efficient in terms of memory usage and computational complexity, particularly with regards to the quadratic complexity of the standard self-attention mechanism?The paper surveys and categorizes the various techniques that have been proposed for developing more efficient Transformer models, focusing especially on innovations related to the self-attention mechanism. It provides a taxonomy of models based on their approach, such as using fixed patterns, combinations of patterns, learnable patterns, memory, low-rank methods, kernels, recurrence, downsampling, and sparsity. The main hypothesis seems to be that there are a variety of ways to improve Transformer efficiency, many of which have shown promise on certain tasks or domains. However, there is still a lack of clarity on which fundamental efficient Transformer block performs the best across different scenarios. The paper aims to provide a structured overview of the landscape of efficient Transformer techniques to help researchers understand the state of the field.


## What is the main contribution of this paper?

Based on my reading, the main contributions of this paper are:1. It provides a comprehensive survey and taxonomy of recent "efficient Transformer" models, i.e. variants of the Transformer architecture that aim to improve its efficiency in terms of computation and memory. 2. It categorizes efficient Transformer models based on the core techniques they employ, such as fixed patterns, combination of patterns, neural memory, low-rank methods, kernels, recurrence, downsampling, and sparsity.3. It gives a detailed walkthrough of several key efficient Transformer models, explaining their architectures and innovations. Models covered include Sparse Transformer, Reformer, Linformer, Synthesizer, etc.4. It discusses evaluation practices and design trends in this area of research, and provides perspective on the state of efficient Transformers. 5. It also briefly covers orthogonal efforts like weight sharing, quantization, NAS, etc. that can improve Transformer efficiency.In summary, this paper provides a comprehensive overview and analysis of the emerging research area of efficient Transformer models, aimed at helping researchers and practitioners navigate and make sense of all the recent innovations. The taxonomy, model walkthroughs, and discussions offer a useful framework to understand efficient Transformers.


## How does this paper compare to other research in the same field?

Here are a few key points about how this paper compares to other research on efficient Transformers:- It provides a broad taxonomy and overview of many recent efficient Transformer models across domains like NLP and computer vision. This makes it a fairly comprehensive survey of techniques for improving Transformer efficiency. Many previous papers focus on proposing a new efficient Transformer variant, without reviewing the wider landscape.- The paper categorizes efficient Transformer techniques into groups like fixed patterns, combination of patterns, learnable patterns, low-rank methods, kernels, recurrence, downsampling, and sparse models. This high-level abstraction offers a useful way to conceptualize the different approaches.- It gives a detailed walkthrough of many influential efficient Transformer models, explaining their core ideas and tradeoffs. This provides good background on seminal works like Sparse Transformers, Performers, Linformers, etc.- The paper discusses evaluation practices and analyzes some trends in model design over time. It provides retrospective thoughts on how the field has evolved and future directions. This meta-level commentary is less common in research papers.- Compared to some other surveys, this paper spans both NLP and vision domains in its coverage of efficient Transformers. Many models were proposed separately in NLP and CV communities, so reviewing both areas is useful.- The scope goes beyond self-attention to also mention orthogonal efficiency efforts like weight sharing, distillation, NAS, etc. This gives a more complete picture of the topic.In summary, this paper delivers an extensive survey and analysis of recent efficient Transformers, covering key techniques, models, and trends across NLP and CV in more depth than most prior works on this topic. The taxonomy, detailed walkthroughs, and discussion offer significant value to researchers in this area.


## What future research directions do the authors suggest?

Based on my reading of the paper, some key future research directions the authors suggest include:- Developing standardized benchmarks and evaluation protocols to enable easier comparison and assessment of different efficient Transformer models. The current lack of unified evaluation makes it challenging to determine the relative strengths of different models.- Continuing to explore combinatorial approaches, such as models that incorporate multiple techniques like downsampling and low-rank approximations. The authors point out that recent models are starting to combine methods, and this may be a promising direction.- Designing models where efficiency is "baked in" from the start, rather than as an afterthought or side model. The ideal efficient Transformer would have efficiency and scalability as core design principles.- Considering whether the "true" efficient Transformer will still rely on attention mechanisms, or explore alternate architectures. The authors wonder if future breakthroughs may come from outside the attention paradigm.- Developing models that retain universality and perform well across diverse tasks, not just specialized long-range tasks. Avoiding tradeoffs in speed, memory, and task flexibility.- Simplifying model designs and implementations so efficient Transformers are easy drop-in replacements, without excessive engineering.- Exploring how techniques like sparsity and conditional computation can improve efficiency beyond just the attention modules. - Evaluating new models under pretraining and fine-tuning, not just standalone benchmarks, for a robust test.In summary, the authors emphasize the need for standardized evaluation, continued exploration of hybrid techniques, simplified and universal architectures, and testing models under realistic training regimes as key future directions for efficient Transformers.


## Summarize the paper in one paragraph.

Here is a one paragraph summary of the paper:The paper introduces the Sinkhorn Transformer, a sparse attention mechanism that reduces the quadratic complexity of standard self-attention to linear. It partitions the input sequence into fixed size blocks and sorts the keys and values within each block using a learned soft sorting network based on Sinkhorn iterations. This allows the model to rearrange keys and values to reflect relevance while still enabling block-wise attention patterns for efficiency. During training, it applies an autoencoding loss to reconstruct the sorted sequences, acting as a regularization to prevent trivial solutions. Experiments demonstrate the Sinkhorn Transformer achieves competitive performance on generative and discriminative language tasks while requiring less memory than full attention Transformers. Overall, it provides an efficient attention mechanism through learned block-wise sorting of keys and values.
