# [Training a Bilingual Language Model by Mapping Tokens onto a Shared   Character Space](https://arxiv.org/abs/2402.16065)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Arabic and Hebrew are structurally similar Semitic languages with many cognates, but use different scripts. This makes it difficult for multilingual models to leverage their similarities.
- Existing multilingual models like mBERT underrepresent Arabic and Hebrew and don't perform as well as monolingual models.

Proposed Solution:
- Develop a bilingual Arabic-Hebrew language model called HeArBERT by transliterating the Arabic texts into the Hebrew script. This allows the model to map cognates to the same tokens.  
- Compare performance of HeArBERT against a version trained without transliteration (HeArBERT_NT) on machine translation tasks.

Key Contributions:
- Release of HeArBERT, a new bilingual Arabic-Hebrew language model trained on transliterated Arabic texts.
- Demonstrate improved performance from HeArBERT over HeArBERT_NT and other baselines on Arabic->Hebrew machine translation.
- Show pre-training bilingual models on transliterated texts helps align tokens and is beneficial for machine translation.
- Achieve comparable translation performance to other models despite 60% less pre-training data.

In summary, the paper introduces HeArBERT, a novel bilingual Arabic-Hebrew language model trained on Arabic texts transliterated to the Hebrew script. This technique better aligns cognates between the languages. Evaluations on machine translation tasks demonstrate improved performance for HeArBERT compared to models trained without transliteration. The results are promising despite the smaller dataset, indicating efficacy of the transliteration approach.
