# [Local Masking Meets Progressive Freezing: Crafting Efficient Vision   Transformers for Self-Supervised Learning](https://arxiv.org/abs/2312.02194)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Training deep learning vision models like Vision Transformers (ViTs) is computationally expensive and time-consuming. 
- Specifically, the initial layers in ViTs struggle to learn inter-patch semantic relationships effectively during self-supervised pre-training.

Proposed Solution:
- Combine local masked image modeling (LocalMIM) with progressive layer freezing to improve training efficiency of ViTs.
- LocalMIM uses multi-scale patch reconstruction objectives to guide different layers, enhancing inter-patch understanding.  
- Progressively freeze layers over time once initial layers learn representations.

Main Contributions:
- Propose novel method adapting progressive freezing technique to hierarchical ViT structure, significantly improving training efficiency.
- Validate approach via experiments on ImageNet, reducing training time by ~12.5% with only a minor 0.6% drop in accuracy.
- Achieve 82.6% top-1 and 96.2% top-5 accuracy, showing potential for scenarios where compute resources are limited.  
- Compare favorably to state-of-the-art LocalMIM method.
- Showcase ability to maintain high performance while requiring less computation time and resources.

In summary, this work makes key advancements in efficient self-supervised learning for ViTs by strategically combining advanced masked image modeling with progressive freezing of layers over time. This reduces training costs substantially with minimal impact on accuracy.
