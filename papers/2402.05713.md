# [Hidden in Plain Sight: Undetectable Adversarial Bias Attacks on   Vulnerable Patient Populations](https://arxiv.org/abs/2402.05713)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Deep learning models used in radiology risk exacerbating biases against vulnerable patient groups. Prior work has focused on quantifying biases in trained models, but adversarial attacks intentionally introducing bias have been underexplored.

- Such attacks could have severe downstream impacts on patient health if models are adversarially manipulated to systematically underdiagnose certain groups. 

Proposed Solution:
- The authors conduct experiments poisoning the training data of a pneumonia detection model by flipping labels of images from targeted demographic groups from positive to negative findings.

- They inject increasing rates of underdiagnosis bias into models targeting 17 intersectional demographic groups based on sex and age. 

- They analyze model performance on the targeted group compared to overall performance to define a "vulnerability" metric for how susceptible each group is to undetectable bias attacks.

Key Contributions:
- Demonstrate that adversarial label poisoning attacks can introduce systematic underdiagnosis bias against targeted demographic groups in radiology deep learning models.

- Find that a group's vulnerability, measured by relative change in performance metrics with increasing attack severity, correlates strongly with its representation in the training data. 

- Smaller subgroups are more vulnerable with performance degrading without changes to overall metrics. Larger groups significantly impact overall metrics when targeted.

- Establish a new bias evaluation methodology and provide evidence that adversarial attacks can have downstream impacts on patient care if they exploit vulnerabilities of underrepresented groups.

In summary, the key insight is that deep learning models have exploitabilities that can intentionally introduce biases against minority groups, requiring further research into adversarial bias attack mechanisms and defenses to ensure reliable and equitable model performance.


## Summarize the paper in one sentence.

 This paper demonstrates that adversarial label poisoning attacks can introduce undetectable underdiagnosis bias against vulnerable demographic groups in deep learning models for medical image analysis without affecting overall model performance.


## What is the main contribution of this paper?

 Based on my reading of the paper, the main contributions are three-fold:

1) The authors show that adversarial attacks can go beyond just injecting label noise by introducing underdiagnosis bias in deep learning models. 

2) They define a demographic group's vulnerability to undetectable adversarial bias attacks as the difference in the change in performance of the targeted group and the overall model with increasing levels of adversarial bias.

3) They indicate that a demographic's group vulnerability is directly correlated with its sample size in the model's training data - smaller groups are more vulnerable while larger groups significantly impact overall model performance.

In summary, the key contribution is demonstrating that targeted adversarial bias attacks can degrade performance on underrepresented demographic groups in medical imaging datasets without severely impacting overall model accuracy. The authors also establish metrics to quantify vulnerability of groups to such attacks based on their representation in the training data.


## What are the keywords or key terms associated with this paper?

 Based on my review of the paper, some of the key terms and keywords associated with this paper include:

- Adversarial Bias
- Generalizability 
- Security
- Data Poisoning
- Chest X-Ray
- Underdiagnosis 
- Bias Attacks
- Label Poisoning
- Demographic Groups
- Model Vulnerability
- False Negative Rate
- False Omission Rate

The paper focuses on adversarial bias attacks that target specific demographic groups in chest x-ray datasets, in order to introduce undetectable underdiagnosis bias in deep learning models. Key ideas explored include quantifying model vulnerability across different demographic subgroups, correlating vulnerability with sample size/representation, and evaluating impact using metrics like false negative rate and false omission rate. The terms and keywords listed capture the core themes and topics discussed in this work.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in this paper:

1. The paper proposes a new definition of "vulnerability" to quantify how susceptible a demographic group is to adversarial bias attacks. Can you explain in detail how this vulnerability metric is calculated and what key aspects of group vulnerability it aims to capture?

2. The adversarial label poisoning attack injects underdiagnosis bias by flipping positive pneumonia labels to no finding labels. Can you walk through the details of how this attack works and how it differs from simply injecting random label noise? 

3. The results show that smaller demographic groups tend to be more vulnerable to adversarial attacks. What reasons does the paper give to explain why this might be the case? Can you expand on the concepts of local optimization versus generalization in deep learning models?

4. The paper indicates there may be a sample size threshold beyond which groups become vulnerable. Can you critically analyze this claim and discuss whether you think there could be other factors besides sample size that determine group vulnerability?

5. For the crossover points observed in certain groups, can you explain what this means and the potential downstream implications if an adversary were to exploit this? 

6. Can you summarize the key differences in impact on sex versus age groups from the adversarial attacks? How might you account for the relative vulnerabilities seen in different groups?

7. Do you think the attack could be enhanced by simultaneously targeting multiple demographic groups? What challenges might an adversary face in orchestrating such an attack?

8. The paper does not validate findings on an external dataset. Do you think results would generalize to other pneumonia detection models and datasets? How could external validation be approached?

9. What other demographic factors besides sex and age should be considered in future work? Would you expect race for example to exhibit similar vulnerability patterns?

10. Can you propose any other additional experiments or analyses that could provide further insights into these adversarial bias attacks and group vulnerability?
