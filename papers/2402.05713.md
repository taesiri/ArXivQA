# [Hidden in Plain Sight: Undetectable Adversarial Bias Attacks on   Vulnerable Patient Populations](https://arxiv.org/abs/2402.05713)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Deep learning models used in radiology risk exacerbating biases against vulnerable patient groups. Prior work has focused on quantifying biases in trained models, but adversarial attacks intentionally introducing bias have been underexplored.

- Such attacks could have severe downstream impacts on patient health if models are adversarially manipulated to systematically underdiagnose certain groups. 

Proposed Solution:
- The authors conduct experiments poisoning the training data of a pneumonia detection model by flipping labels of images from targeted demographic groups from positive to negative findings.

- They inject increasing rates of underdiagnosis bias into models targeting 17 intersectional demographic groups based on sex and age. 

- They analyze model performance on the targeted group compared to overall performance to define a "vulnerability" metric for how susceptible each group is to undetectable bias attacks.

Key Contributions:
- Demonstrate that adversarial label poisoning attacks can introduce systematic underdiagnosis bias against targeted demographic groups in radiology deep learning models.

- Find that a group's vulnerability, measured by relative change in performance metrics with increasing attack severity, correlates strongly with its representation in the training data. 

- Smaller subgroups are more vulnerable with performance degrading without changes to overall metrics. Larger groups significantly impact overall metrics when targeted.

- Establish a new bias evaluation methodology and provide evidence that adversarial attacks can have downstream impacts on patient care if they exploit vulnerabilities of underrepresented groups.

In summary, the key insight is that deep learning models have exploitabilities that can intentionally introduce biases against minority groups, requiring further research into adversarial bias attack mechanisms and defenses to ensure reliable and equitable model performance.
