# Language Models Are Greedy Reasoners: A Systematic Formal Analysis of   Chain-of-Thought

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be: What are the reasoning capabilities of large language models (LLMs) when evaluated on a new synthetic question answering dataset designed to facilitate formal analysis of the predicted chains-of-thought?The key aspects of this research question are:- Evaluating the reasoning capabilities of LLMs, specifically their ability to perform logical deduction.- Using a new synthetic QA dataset called PrOntoQA to enable formal analysis of the predicted reasoning chains, by parsing them into logical forms. - Analyzing whether the models are able to apply deduction rules correctly at each step (local reasoning) and plan ahead to complete the full proof (global reasoning).- Investigating how different variables like ontology type (true/false/fictional), number of reasoning steps, etc. affect the model's reasoning ability.- Understanding the causes of reasoning errors through fine-grained proof analysis, to identify limitations and improve reasoning ability.So in summary, the central hypothesis is that by evaluating LLMs on PrOntoQA and formally analyzing their predicted chains-of-thought, we can gain new insights into their reasoning capabilities and limitations. The goal is to move beyond just predicting the right labels, and actually understand how the models reason on controlled but challenging reasoning tasks.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contributions seem to be:- The introduction of a new synthetic question answering dataset called PrOntoQA for evaluating logical reasoning capabilities of large language models (LLMs). The key properties of PrOntoQA are:1) The examples are generated from symbolic proofs over fictional ontologies. This removes the confounding factor of prior world knowledge. 2) The examples have simple syntax and grammar, which allows easy parsing into logical forms for formal analysis.3) The dataset allows control over various properties like ontology type, number of reasoning steps, etc. to systematically analyze an LLM's reasoning abilities.- Formal analysis of the reasoning chains predicted by InstructGPT and GPT-3 on PrOntoQA. The key findings are:1) The models are generally capable of producing valid individual reasoning steps, even in fictional contexts.2) However, they struggle with proof planning - selecting the right sequence of steps to prove the final query. Errors often arise from misleading steps that divert the proof from the gold standard.3) Real-world knowledge from pretraining helps reasoning, as performance drops on fictional or false ontologies.4) Reasoning ability declines as proof lengths increase.Overall, the paper introduces a valuable new resource for analyzing reasoning in LLMs and provides interesting insights into their capabilities and limitations through formal proof analysis. The dataset and framework could be useful for further research on improving reasoning abilities.


## How does this paper compare to other research in the same field?

Here are a few thoughts on how this paper compares to other related research:- The paper introduces a new synthetic QA dataset called PrOntoQA for evaluating reasoning capabilities of large language models (LLMs). Other datasets like ProofWriter, FOLIO, and SimpleLogic have also been proposed for testing logical reasoning, but this paper argues PrOntoQA has more control over variables like true vs fictional contexts, unary vs binary predicates, and specific deduction rules. The controllable data generation process seems useful.- The analysis focuses specifically on evaluating reasoning of LLMs via chain-of-thought prompting. Other work has looked at reasoning in different ways - e.g. directly in natural language, or using neuro-symbolic methods with semantic parsing. But this paper provides a detailed analysis of proof steps predicted by the LLM.- The finding that LLMs struggle with proof planning/strategizing seems consistent with other evidence that these models are limited in their systematic reasoning abilities, even though they can make valid individual deduction steps.- The use of a synthetic fictional dataset to avoid memorization confounds is a nice idea, though generalization to real-world reasoning needs to be tested. Using controllable data helps dig deeper into model capabilities.- The paper mostly focuses on analyzing modus ponens reasoning over fairly simple linguistic constructions. Extending the analysis to more complex proofs and language is noted as interesting future work.Overall, the work seems to advance understanding of LLM reasoning abilities through detailed analysis facilitated by a new synthetic QA dataset. The controllable data generation and proof step evaluation help systematically probe model capabilities and limitations. More investigation would be needed to relate insights to real-world reasoning tasks. But the approach appears promising for studying reasoning.


## What future research directions do the authors suggest?

Based on my reading of the paper, some of the main future research directions suggested by the authors are:- Testing the reasoning capabilities of LLMs on more complex proofs that require longer chains of reasoning, use additional deduction rules beyond modus ponens, and contain more semantically complex sentences. The examples in the PrOntoQA dataset were limited to short proofs using only modus ponens over simple sentences.- Comparing the reasoning capabilities of LLMs to human reasoning through behavioral experiments. This could shed light on what aspects of human reasoning LLMs have managed to acquire through pretraining.- Using the PrOntoQA dataset to further pretrain or fine-tune LLMs to try to improve their reasoning abilities. The synthetic reasoning examples could provide useful training signal.- Exploring different prompting strategies beyond chain-of-thought that may elicit better reasoning from LLMs, such as the selection-inference approach.- Incorporating more sophisticated proof search strategies, like depth-first search, instead of just chain-of-thought reasoning. This may help address limitations in proof planning.- Using neurosymbolic methods to combine neural network reasoning with symbolic reasoning over logical forms. This could potentially overcome some limitations like lack of systematic proof search.- Analyzing what parts of the pretraining data allow LLMs to acquire reasoning abilities to better understand where this capability comes from.In summary, the main directions are developing more challenging reasoning tasks, comparing to human performance, using the synthetic data to improve LLMs, trying new prompting strategies, incorporating symbolic methods, and analyzing the pretraining process.
