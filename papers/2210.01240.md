# Language Models Are Greedy Reasoners: A Systematic Formal Analysis of   Chain-of-Thought

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be: What are the reasoning capabilities of large language models (LLMs) when evaluated on a new synthetic question answering dataset designed to facilitate formal analysis of the predicted chains-of-thought?The key aspects of this research question are:- Evaluating the reasoning capabilities of LLMs, specifically their ability to perform logical deduction.- Using a new synthetic QA dataset called PrOntoQA to enable formal analysis of the predicted reasoning chains, by parsing them into logical forms. - Analyzing whether the models are able to apply deduction rules correctly at each step (local reasoning) and plan ahead to complete the full proof (global reasoning).- Investigating how different variables like ontology type (true/false/fictional), number of reasoning steps, etc. affect the model's reasoning ability.- Understanding the causes of reasoning errors through fine-grained proof analysis, to identify limitations and improve reasoning ability.So in summary, the central hypothesis is that by evaluating LLMs on PrOntoQA and formally analyzing their predicted chains-of-thought, we can gain new insights into their reasoning capabilities and limitations. The goal is to move beyond just predicting the right labels, and actually understand how the models reason on controlled but challenging reasoning tasks.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contributions seem to be:- The introduction of a new synthetic question answering dataset called PrOntoQA for evaluating logical reasoning capabilities of large language models (LLMs). The key properties of PrOntoQA are:1) The examples are generated from symbolic proofs over fictional ontologies. This removes the confounding factor of prior world knowledge. 2) The examples have simple syntax and grammar, which allows easy parsing into logical forms for formal analysis.3) The dataset allows control over various properties like ontology type, number of reasoning steps, etc. to systematically analyze an LLM's reasoning abilities.- Formal analysis of the reasoning chains predicted by InstructGPT and GPT-3 on PrOntoQA. The key findings are:1) The models are generally capable of producing valid individual reasoning steps, even in fictional contexts.2) However, they struggle with proof planning - selecting the right sequence of steps to prove the final query. Errors often arise from misleading steps that divert the proof from the gold standard.3) Real-world knowledge from pretraining helps reasoning, as performance drops on fictional or false ontologies.4) Reasoning ability declines as proof lengths increase.Overall, the paper introduces a valuable new resource for analyzing reasoning in LLMs and provides interesting insights into their capabilities and limitations through formal proof analysis. The dataset and framework could be useful for further research on improving reasoning abilities.
