# [CLAMP: Contrastive LAnguage Model Prompt-tuning](https://arxiv.org/abs/2312.01629)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Large language models (LLMs) excel at generative visual tasks like image captioning, but underperform on image classification tasks. For example, state-of-the-art multimodal LLMs achieve less than 10% top-1 accuracy on the Stanford Cars dataset, compared to over 80% for models like CLIP. 

- The authors hypothesize that the generative training objectives used for multimodal LLMs do not enable effective discrimination needed for classification tasks.

Proposed Solution:
- The authors propose CLAMP (Contrastive Language Model Prompt-tuning), which adapts LLMs for classification by lightly fine-tuning them using a contrastive image-text matching loss, similar to CLIP.

- CLAMP uses output attention pooling and learnable "read-only" prompts to help the LLM produce useful representations for discrimination. It also uses LoRA to make light weight updates to the LLM parameters.

Main Contributions:

- Demonstrates that current multimodal LLMs fail at image classification, achieving 34% average accuracy over 24 datasets compared to 47% for CLAMP.

- Shows that contrastive fine-tuning of LLMs can enable strong image classification abilities, outperforming from-scratch training in domains under-represented in the pre-training data.

- Achieves state-of-the-art results among methods using general pre-trained LLMs, while retaining most of the model's generative capabilities.

- Provides analysis showing CLAMP's benefits come from the language model's prior when target data coverage in pre-training is low.

In summary, the paper proposes an effective approach to adapt large language models for image classification while retaining their generative strengths, outperforming prior multimodal LLMs.
