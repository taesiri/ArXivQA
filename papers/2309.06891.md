# [Keep It SimPool: Who Said Supervised Transformers Suffer from Attention   Deficit?](https://arxiv.org/abs/2309.06891)

## What is the central research question or hypothesis that this paper addresses?

The central hypothesis of this paper is that spatial pooling in convolutional neural networks and vision transformers can be reformulated as a separate module at the end of the network. The authors propose that this unified pooling module, which they call SimPool, can improve performance and provide better attention maps compared to default pooling in both CNNs and transformers. The key research questions addressed are:1. Can a simple pooling mechanism applied at the end improve over default pooling in CNNs and transformers?2. Can this pooling mechanism provide high quality attention maps that delineate object boundaries in both CNNs and transformers? 3. Do these benefits hold for both supervised and self-supervised training?To test these hypotheses, the authors propose SimPool, which is based on cross-attention between the global average pooled features and the final convolutional feature maps. They show that SimPool boosts accuracy and provides sharper attention maps compared to global average pooling in CNNs and the [CLS] token in transformers. The benefits hold for both supervised ImageNet training and self-supervised DINO pretraining.In summary, the central hypothesis is that reformulating pooling as a plug-and-play module can improve performance and attention for both CNNs and ViTs. The key questions address whether SimPool validates this hypothesis across network architectures and training procedures.


## What is the main contribution of this paper?

The main contribution of this paper is proposing SimPool, a simple and universal attention-based pooling method that can be used as a replacement for default pooling in convolutional networks and vision transformers. The key points are:- They formulate a generic pooling framework that allows examining and comparing various pooling methods. Using this, they show that many existing pooling methods like GAP, GeM, SE, CBAM etc. can be seen as instantiations of this framework.- They propose SimPool, which is a simple non-iterative attention-based pooling that uses global average pooling (GAP) for initialization and cross-attention between the pooled vector and image features for the pooling operation. - SimPool is shown to work well as a universal pooling mechanism for both convolutional networks like ResNet and vision transformers like ViT. It improves performance over default pooling and also generates high quality attention maps that focus well on foreground objects.- This is achieved for both supervised and self-supervised settings. Notably, SimPool produces good attention maps for supervised ViT, something that prior works have struggled with.- Extensive experiments validate the benefits of SimPool over default pooling and several competitors on image classification, localization, retrieval etc. Tasks and multiple datasets demonstrate its universality.So in summary, the key contribution is proposing and validating a simple and universal pooling method SimPool that works for both CNNs and ViT, supervised and self-supervised, improving performance and attention maps. The comparisons using a generic pooling framework is also an important aspect.
