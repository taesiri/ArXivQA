# [Keep It SimPool: Who Said Supervised Transformers Suffer from Attention   Deficit?](https://arxiv.org/abs/2309.06891)

## What is the central research question or hypothesis that this paper addresses?

The central hypothesis of this paper is that spatial pooling in convolutional neural networks and vision transformers can be reformulated as a separate module at the end of the network. The authors propose that this unified pooling module, which they call SimPool, can improve performance and provide better attention maps compared to default pooling in both CNNs and transformers. The key research questions addressed are:1. Can a simple pooling mechanism applied at the end improve over default pooling in CNNs and transformers?2. Can this pooling mechanism provide high quality attention maps that delineate object boundaries in both CNNs and transformers? 3. Do these benefits hold for both supervised and self-supervised training?To test these hypotheses, the authors propose SimPool, which is based on cross-attention between the global average pooled features and the final convolutional feature maps. They show that SimPool boosts accuracy and provides sharper attention maps compared to global average pooling in CNNs and the [CLS] token in transformers. The benefits hold for both supervised ImageNet training and self-supervised DINO pretraining.In summary, the central hypothesis is that reformulating pooling as a plug-and-play module can improve performance and attention for both CNNs and ViTs. The key questions address whether SimPool validates this hypothesis across network architectures and training procedures.


## What is the main contribution of this paper?

The main contribution of this paper is proposing SimPool, a simple and universal attention-based pooling method that can be used as a replacement for default pooling in convolutional networks and vision transformers. The key points are:- They formulate a generic pooling framework that allows examining and comparing various pooling methods. Using this, they show that many existing pooling methods like GAP, GeM, SE, CBAM etc. can be seen as instantiations of this framework.- They propose SimPool, which is a simple non-iterative attention-based pooling that uses global average pooling (GAP) for initialization and cross-attention between the pooled vector and image features for the pooling operation. - SimPool is shown to work well as a universal pooling mechanism for both convolutional networks like ResNet and vision transformers like ViT. It improves performance over default pooling and also generates high quality attention maps that focus well on foreground objects.- This is achieved for both supervised and self-supervised settings. Notably, SimPool produces good attention maps for supervised ViT, something that prior works have struggled with.- Extensive experiments validate the benefits of SimPool over default pooling and several competitors on image classification, localization, retrieval etc. Tasks and multiple datasets demonstrate its universality.So in summary, the key contribution is proposing and validating a simple and universal pooling method SimPool that works for both CNNs and ViT, supervised and self-supervised, improving performance and attention maps. The comparisons using a generic pooling framework is also an important aspect.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper introduces SimPool, a simple attention-based pooling method that can be applied to both convolutional and transformer networks at the end of the encoder to improve performance on pre-training and downstream tasks while also generating clean attention maps that delineate object boundaries under both supervised and self-supervised settings.


## How does this paper compare to other research in the same field?

Here is a summary of how this paper compares to other research in the field of vision transformers and spatial pooling:- This paper proposes a simple and generic spatial pooling framework called SimPool that can be applied to both convolutional neural networks (CNNs) and vision transformers. Most prior work on spatial pooling has focused specifically on either CNNs or transformers.- The paper shows that by replacing the default pooling mechanism in CNNs and transformers with SimPool, both image classification accuracy and attention map quality can be improved in supervised and self-supervised settings. Most prior work has studied spatial pooling for CNNs on instance-level tasks or for transformers in a self-supervised context. This work validates the benefits of improved pooling in the supervised classification setting as well.- The paper demonstrates that with SimPool, supervised transformers can achieve attention map quality on par with or better than self-supervised transformers, without needing additional losses or architectural modifications. Prior work has shown supervised transformers generally produce poor quality attention maps compared to self-supervised methods.- SimPool is shown to work well as a simple drop-in replacement for the pooling layer in existing CNN and transformer architectures. It does not require extensive hyperparameter tuning or architectural changes. Many prior pooling methods are more complex or deeply integrated into the network architecture.- The paper compares SimPool directly to a range of prior pooling techniques like GAP, GeM, attention-based, etc. in a consistent evaluation framework. This allows for a systematic comparison of the trade-offs between different pooling strategies.Overall, this paper makes contributions in formulating a simple and generic pooling framework, validating its benefits for both CNNs and transformers in supervised learning, and specifically demonstrating improved attention maps for supervised transformers compared to prior art. The consistent evaluation provides useful insights into spatial pooling design choices.


## What future research directions do the authors suggest?

Here are some of the key future research directions suggested in the paper:- Developing optimization methods to enable training deeper transformer models. The paper showed promising results by scaling up model size, so developing techniques to train even larger models could lead to further gains.- Exploring different tokenization approaches like using 2D tokens instead of 1D. This could allow modeling more spatial relationships in images compared to the current 1D sequence modeling approach. - Improving efficiency of attention mechanisms for vision transformers, as the quadratic computation cost can limit scaling. Ideas like sparse attention and conditional computation were suggested.- Incorporating more inductive biases from convolutional networks like translation equivariance. The paper argues both CNN and transformer approaches have complementary strengths.- Studying transfer learning abilities of vision transformers on a broader range of visual tasks beyond image classification. Areas like object detection and segmentation represent opportunities.- Developing theoretical understandings of why vision transformers achieve strong results compared to CNNs. The connections between self-attention and translation equivariance was one direction suggested.- Exploring the role of pre-training datasets and regularization methods in improving vision transformers. Self-supervised pretraining was noted as being particularly promising.So in summary, some of the main future directions are developing techniques to scale up model size, improving efficiency of transformers, incorporating convolutional inductive biases, evaluating on more vision tasks, and building theoretical understandings. Leveraging self-supervised pretraining also appears to be a promising direction for future research.
