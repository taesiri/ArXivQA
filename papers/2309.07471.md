# [EP2P-Loc: End-to-End 3D Point to 2D Pixel Localization for Large-Scale   Visual Localization](https://arxiv.org/abs/2309.07471)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the key research question that this paper aims to address is: How can we effectively perform large-scale visual localization by matching 2D image features to 3D point clouds, despite the significant differences between these two modalities?The key challenges outlined are:- The features extracted from 2D images vs 3D point clouds are very different due to differences in representation. This makes it difficult to establish reliable 2D-3D correspondences for pose estimation.- Existing methods that try to establish 2D-3D correspondences suffer from low inlier ratios, limiting pose estimation accuracy.- Extracting dense features from all image pixels and point cloud points is computationally inefficient.To address these challenges, the central hypothesis appears to be:- By preprocessing the 3D point clouds to remove invisible points, establishing correspondences between all 2D image patches and 3D points in a coarse-to-fine manner, and using a differentiable PnP solver, we can achieve effective large-scale visual localization between 2D images and 3D point clouds despite their representational differences.In summary, the key novelty seems to be in mitigating the 2D-3D representational differences and establishing reliable dense correspondences across the two modalities in an efficient manner for accurate pose estimation. The experiments aim to validate this approach on large-scale indoor and outdoor datasets.


## What is the main contribution of this paper?

The main contribution of this paper is a novel method for large-scale visual localization by matching features between 2D image pixels and 3D point clouds. The key ideas are:- Proposing an algorithm called Invisible 3D Point Removal (IPR) to remove occluded 3D points that are not visible in the 2D image prior to feature extraction. This helps mitigate the differences in representation between 2D images and 3D point clouds. - Performing hierarchical matching in a coarse-to-fine manner. Global features are first extracted and used to retrieve relevant 3D point cloud submaps. Then 2D patch classification is performed to determine which 3D points belong to which patch in the image. Finally, precise 2D pixel coordinates are calculated for each matched 3D point using positional encoding. This increases inlier correspondences while reducing computation.- Using an end-to-end learnable PnP solver for the first time in this task. This allows selecting good 2D-3D correspondences while utilizing the ground truth pose during training.- Evaluating on large-scale indoor and outdoor benchmark datasets based on 2D-3D-S and KITTI. The method achieves state-of-the-art performance for visual localization compared to previous image-based and image-to-point cloud methods.In summary, the key contribution is a novel end-to-end pipeline for large-scale visual localization from an image to a 3D point cloud map, which handles cross-modality matching challenges more effectively than prior works.
