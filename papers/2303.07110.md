# [Upcycling Models under Domain and Category Shift](https://arxiv.org/abs/2303.07110)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central research question this paper aims to address is: How can we adapt deep neural network models to new target domains and tasks using only a pre-trained source model, without requiring the original source training data, and without knowing the category shift a priori?In particular, the key goals are:- To perform unsupervised domain adaptation without access to source data (only a pre-trained source model), known as source-free domain adaptation. - To handle various types of category shift between source and target domains, including partial-set, open-set, and open-partial set shifts, in a unified framework.- To identify "known" samples belonging to shared source-target classes and reject "unknown" samples from novel target-only classes.- To achieve this model adaptation and "known vs unknown" separation using only standard pre-trained closed-set source models, without needing specially designed model architectures.The key hypothesis is that by combining global clustering to separate shared vs private classes along with local clustering to refine predictions, one can effectively adapt models to new target domains and tasks under varying degrees of category shift in a source-free manner.


## What is the main contribution of this paper?

The main contribution of this paper is proposing a generic technique called Global and Local Clustering (GLC) for model upcycling under domain shift and category shift. Specifically:- They propose a source-free universal domain adaptation (SF-UniDA) setting to handle various category shifts between source and target domains, including partial-set, open-set, and open-partial-set scenarios. This allows adapting models when the source and target domains have different label spaces, with only access to a pre-trained source model.- They develop an innovative one-vs-all global clustering algorithm to assign pseudo-labels and separate "known" and "unknown" data samples without prior knowledge of the category shift. This includes techniques like adaptive target category estimation and source-private category suppression. - They introduce a local k-NN clustering strategy to further mitigate negative transfer by exploiting the intrinsic target domain structure. - They provide extensive experiments showing GLC achieves state-of-the-art performance on standard DA benchmarks across various category shift settings. Remarkably, GLC outperforms prior arts by 14.8% on VisDA for open-partial-set DA.- They demonstrate the effectiveness of GLC on more realistic applications like remote sensing, wildlife classification, and single-cell RNA sequencing.In summary, the key contribution is proposing a simple yet generic GLC technique to enable model upcycling under both domain and category shifts in a unified source-free manner, with strong empirical results. This has important practical implications for model reuse.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper proposes a Global and Local Clustering (GLC) technique for upcycling pre-trained models to perform domain adaptation in source-free settings under both domain shift and category shift, without requiring access to source data or dedicated source model architectures.
