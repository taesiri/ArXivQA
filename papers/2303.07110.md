# [Upcycling Models under Domain and Category Shift](https://arxiv.org/abs/2303.07110)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading, the central research question this paper aims to address is: 

How can we adapt deep neural network models to new target domains and tasks using only a pre-trained source model, without requiring the original source training data, and without knowing the category shift a priori?

In particular, the key goals are:

- To perform unsupervised domain adaptation without access to source data (only a pre-trained source model), known as source-free domain adaptation. 

- To handle various types of category shift between source and target domains, including partial-set, open-set, and open-partial set shifts, in a unified framework.

- To identify "known" samples belonging to shared source-target classes and reject "unknown" samples from novel target-only classes.

- To achieve this model adaptation and "known vs unknown" separation using only standard pre-trained closed-set source models, without needing specially designed model architectures.

The key hypothesis is that by combining global clustering to separate shared vs private classes along with local clustering to refine predictions, one can effectively adapt models to new target domains and tasks under varying degrees of category shift in a source-free manner.


## What is the main contribution of this paper?

 The main contribution of this paper is proposing a generic technique called Global and Local Clustering (GLC) for model upcycling under domain shift and category shift. Specifically:

- They propose a source-free universal domain adaptation (SF-UniDA) setting to handle various category shifts between source and target domains, including partial-set, open-set, and open-partial-set scenarios. This allows adapting models when the source and target domains have different label spaces, with only access to a pre-trained source model.

- They develop an innovative one-vs-all global clustering algorithm to assign pseudo-labels and separate "known" and "unknown" data samples without prior knowledge of the category shift. This includes techniques like adaptive target category estimation and source-private category suppression. 

- They introduce a local k-NN clustering strategy to further mitigate negative transfer by exploiting the intrinsic target domain structure. 

- They provide extensive experiments showing GLC achieves state-of-the-art performance on standard DA benchmarks across various category shift settings. Remarkably, GLC outperforms prior arts by 14.8% on VisDA for open-partial-set DA.

- They demonstrate the effectiveness of GLC on more realistic applications like remote sensing, wildlife classification, and single-cell RNA sequencing.

In summary, the key contribution is proposing a simple yet generic GLC technique to enable model upcycling under both domain and category shifts in a unified source-free manner, with strong empirical results. This has important practical implications for model reuse.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the key points from the paper:

The paper proposes a Global and Local Clustering (GLC) technique for upcycling pre-trained models to perform domain adaptation in source-free settings under both domain shift and category shift, without requiring access to source data or dedicated source model architectures.


## How does this paper compare to other research in the same field?

 Here are my thoughts on how this paper compares to other research in the field of domain adaptation:

Overall, the paper presents a novel approach for source-free universal domain adaptation (SF-UniDA), which aims to adapt models under both domain shift and category shift using only a standard closed-set pre-trained model. This is an important and challenging problem.

Key strengths:

- Tackles a very difficult and practical setting of SF-UniDA without requiring dedicated model architectures, unlike some prior works. This makes the approach widely applicable. 

- Proposes an innovative global and local clustering technique (GLC) to handle both domain and category shift in a unified manner. The global clustering aims to separate "known" vs "unknown" data while the local clustering reduces negative transfer.

- Achieves state-of-the-art performance across multiple domain adaptation benchmarks under various category shift scenarios like partial/open/open-partial set DA. Especially strong results on VisDA for open-partial DA, outperforming prior arts by 15-16%.

- Validates GLC on realistic applications including remote sensing, wildlife classification and single cell classification, showing usefulness beyond standard DA benchmarks.

Limitations:

- While it handles various category shifts in a unified manner, the performance gaps between different category shift scenarios are still significant. There is room for further improvement, especially on partial/open set shifts.

- For practical usage, the choices of some key hyperparameters like the weighting between losses may need more rigorous tuning/guidelines. Ablations provide some analysis but more work on adaptive selection could help.

- The computational overhead and convergence may still be concerns for adoption on very large-scale problems compared to source-free closed set DA methods.

Overall, I think this is a novel and promising approach for tackling the highly challenging problem of SF-UniDA, with extensive experiments highlighting its effectiveness. The unified handling of domain+category shift and applicability based just on standard closed-set models are notable strengths. While limitations exist, it pushes forward a very important research direction.


## What future research directions do the authors suggest?

 Based on my reading of the paper, here are some of the main future research directions suggested by the authors:

- Exploring additional or alternative clustering strategies for the global and local clustering approach. The authors propose using k-means clustering and k-NN clustering, but indicate there may be room for improvement with other clustering algorithms.

- Extending the method to more complex data such as images, video, and speech beyond the classification tasks tested. The authors suggest exploring adaptations for these modalities.

- Investigating semi-supervised or weakly-supervised versions of the approach that can utilize a small amount of labeled target data. The current method is fully unsupervised but incorporating some target labels could be beneficial.

- Applying the method to additional real-world applications and datasets beyond the ones tested to further demonstrate versatility. The authors suggest domains like medical imaging and natural language as possibilities. 

- Developing theoretical understandings of when and why the global and local clustering approach is effective. The empirical results are promising but more analysis on the theory side could provide insights.

- Exploring adaptations of the approach for other transfer learning scenarios beyond domain adaptation, such as cross-dataset or cross-task transfer. The clustering concepts may generalize.

- Combining ideas from the method with other domain adaptation techniques like adversarial learning for potentially complementary benefits. There may be room for hybrid approaches.

In summary, the main future directions pointed out relate to improving and extending the clustering techniques, applying the method to broader types of data and applications, developing theory, and integrating the approach with other transfer learning methods. The proposed global and local clustering technique shows promise but the authors highlight quite a few areas for additional exploration.


## Summarize the paper in one paragraph.

 The paper proposes a method called Global and Local Clustering (GLC) for adapting pre-trained deep neural network models to new target domains and categories without access to the original source training data. The key ideas are:

1) A global one-vs-all clustering algorithm to assign pseudo-labels for separating known and unknown categories in the target domain. It adaptively clusters target data into prototypes representing each source class vs everything else, enabling open set adaptation. 

2) A local kNN clustering loss to refine pseudo-labels based on neighborhood consensus, mitigating negative transfer.

3) Adaptive estimation of target categories using silhouette analysis, removing reliance on prior knowledge.

The method is evaluated on standard DA benchmarks and simulated applications like satellite image scene recognition and animal species classification. Results show GLC effectively enables model adaptation under partial/open/open-partial category shift without source data, outperforming prior domain adaptation methods. The simplicity and effectiveness of GLC for upcycling models and rejecting unknowns may enable deploying pre-trained models safely in new real-world scenarios.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the paper:

This paper proposes a novel technique called Global and Local Clustering (GLC) to enable source-free universal domain adaptation (SF-UniDA). The goal is to adapt a pre-trained source model to identify "known" data samples and reject "unknown" data samples under both domain shift and category shift, without access to the original source data. The authors introduce GLC to address three category shift scenarios: partial-set DA, open-set DA, and open-partial-set DA. GLC has two main components: (1) An innovative global one-vs-all clustering algorithm that assigns pseudo-labels to target data samples to separate "known" vs "unknown" classes. This clustering is made adaptive using the Silhouette criterion since the type of category shift is unknown. (2) A local k-NN consensus clustering that exploits the intrinsic target data structure to mitigate negative transfer from incorrect pseudo-labels. Experiments on four benchmarks under the three category shift scenarios demonstrate that GLC achieves state-of-the-art performance. Notably, on the challenging open-partial-set VisDA benchmark, GLC attains 73.1% H-score, outperforming prior arts UMAD and GATE by 14.8% and 16.7% respectively. The results highlight the effectiveness of GLC for SF-UniDA across diverse shifts between source and target domains.

In summary, this paper proposes a novel Global and Local Clustering (GLC) technique to achieve source-free universal domain adaptation without access to source data. GLC introduces an adaptive global clustering to separate "known" and "unknown" classes and a local clustering to alleviate negative transfer. Extensive experiments under various category shifts validate the effectiveness of GLC, where it substantially outperforms prior arts on challenging benchmarks like VisDA for open-partial-set domain adaptation. The work represents an important advancement for adapting pre-trained models under combined domain and category shift.


## Summarize the main method used in the paper in one paragraph.

 The paper proposes a Global and Local Clustering (GLC) method for source-free universal domain adaptation (SF-UniDA). The goal is to adapt a pretrained closed-set source model to identify "known" data samples and reject "unknown" data samples under both domain shift and category shift, without access to source data. 

The key ideas are:

1. A global one-vs-all clustering algorithm is used to assign pseudo-labels and separate "known" vs "unknown" data. It creates prototypes for each source class vs everything else, and assigns pseudo-labels based on nearest prototype distance. 

2. A confidence-based suppression mechanism is used to avoid misleading from source private categories. Categories with lower prediction confidence on target data are suppressed.

3. A local kNN clustering loss is added to mitigate negative transfer by encouraging local target consensus. 

The method is evaluated on partial-set, open-set, and open-partial-set DA scenarios across several benchmarks. Results show it outperforms prior arts, especially in the most challenging open-partial-set case. The global clustering is key for "known" vs "unknown" separation, while local clustering reduces negative transfer.


## What problem or question is the paper addressing?

 The paper is addressing the problem of how to adapt deep neural networks to new domains when there is both a domain shift as well as a category shift between the source and target domains. Specifically, it focuses on the challenging problem of source-free universal domain adaptation (SF-UniDA), where the goal is to identify "known" data samples and reject "unknown" data samples in the target domain, using only knowledge from a pretrained closed-set source model, without access to the original source data. The category shift means the label spaces between the source and target domain are different, which poses additional challenges beyond just domain shift.

The key questions the paper tries to address are:

1) How to achieve pseudo-labeling to identify "known" and reject "unknown" target samples when the label spaces are different, with no access to source data? 

2) How to adapt a standard pretrained closed-set source model to the target domain under both domain and category shift, without needing specialized model architectures?

3) How to handle different types of category shift in a unified manner, including partial-set, open-set, and open-partial set scenarios, without knowing the type of shift a priori?

So in summary, the paper focuses on the very challenging problem of adapting models to new domains with both domain and category shift, in a source-free manner, using only standard pretrained models, and handling different category shift scenarios in a unified framework.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and keywords are:

- Source-free universal domain adaptation (SF-UniDA): The goal of the paper is to achieve model upcycling under both domain shift and category shift using only a pre-trained source model, without access to the source training data. This is referred to as source-free universal domain adaptation.

- Global and local clustering (GLC): The proposed technique involves global one-vs-all clustering to separate known and unknown data samples, as well as local k-NN clustering to mitigate negative transfer. 

- Open-set, partial-set, open-partial-set: Different category shift scenarios considered, including partial-set domain adaptation (PDA), open-set domain adaptation (OSDA), and open-partial-set domain adaptation (OPDA).

- Pseudo-labeling: A key technique used, where pseudo-labels are assigned to target data samples to enable model adaptation in the absence of true labels. The global clustering provides pseudo-labels.

- Model upcycling: The ability to take an existing trained model (on source data) and adapt it to work well on a target domain, without access to the original training data.

- Negative transfer: When adapting to the target domain worsens performance compared to just using the original source model, due to misleading effects. Local clustering helps address this.

- Source hypothesis: The assumption that the source and target domains share some commonalities, so that source knowledge can be transferred.

So in summary, the key focus is on upcycling models for domain adaptation without source data under category shift, using global and local clustering techniques.


## Based on the paper, what questions should we ask to create a comprehensive summary of it? Please list at least 10 questions.

 Here are 10 potential questions to ask in order to create a comprehensive summary of the paper:

1. What is the main problem or challenge that the paper aims to address? 

2. What is the proposed approach or method to tackle this problem? What are its key features or components?

3. What datasets were used to evaluate the proposed method? What were the evaluation metrics?

4. What were the main results? How did the proposed method perform compared to baseline or state-of-the-art methods?

5. What are the key advantages or benefits of the proposed method over existing approaches?

6. Are there any limitations or drawbacks to the proposed method?

7. Did the authors perform any ablation studies or analyses to demonstrate the impact of different components of their method?

8. Did the authors discuss the computational complexity or efficiency of their method?

9. Did the authors release code or models for their method? Is it easy to reproduce their results?

10. What future work or potential extensions do the authors suggest based on this research? What open problems remain?

Asking these types of questions will help extract the key information from the paper in order to summarize its main contributions, results, and implications. The goal is to understand what problem the authors were trying to solve, how they approached it, what they found, and the significance of their work.


## Design 10 in-depth questions about method proposed in this paper and avoid simple question.

 Here are 10 potential in-depth questions about the method proposed in this paper:

1. The paper proposes a novel global and local clustering (GLC) technique for source-free universal domain adaptation. How does the global clustering strategy help achieve "known" data identification and "unknown" data rejection? What are the advantages compared to existing pseudo-labeling strategies?

2. The paper introduces an innovative one-vs-all global clustering algorithm. Can you explain in detail how this algorithm works and what are the key steps involved? How does it help adaptively cluster target data without prior knowledge of category shifts? 

3. The paper employs the Silhouette criterion to facilitate estimating the number of categories in the target domain. Why is the Silhouette criterion suitable for this task? How does it help determine the appropriate number of clusters?

4. The paper designs a global confidence statistics based suppression strategy. What is the motivation behind this? How does it help avoid misleading from source private categories during pseudo-labeling?

5. In addition to global clustering, the paper also exploits local k-NN consensus clustering. What is the intuition behind combining global and local clustering strategies? How do they complement each other?

6. The optimization objective combines a global clustering loss and a local consensus clustering loss. What is the effect of the trade-off hyperparameter η? How sensitive is the performance to the choice of η?

7. For inference, the paper uses normalized Shannon entropy as the uncertainty metric to separate known and unknown data. Why is Shannon entropy a reasonable choice here? How does the threshold ω affect the trade-off between precision and recall?

8. The paper validates GLC on partial-set, open-set, and open-partial-set domain adaptation. What modifications or adjustments are needed to apply GLC to these different category shift scenarios?

9. The paper demonstrates GLC's effectiveness on realistic applications like remote sensing and single-cell RNA analysis. What are the key advantages of GLC that make it suitable for these real-world tasks?

10. The paper compares GLC with prior source-free domain adaptation methods. What are the limitations of existing methods that GLC aims to address? How does GLC advance the state-of-the-art in source-free universal domain adaptation?


## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a high-quality paragraph summarizing the key points of the paper:

This paper proposes a novel technique called Global and Local Clustering (GLC) to achieve source-free universal domain adaptation. The goal is to adapt a standard pre-trained closed-set source model to identify "known" data samples and reject "unknown" data samples under both domain shift and category shift, without needing the raw source data. The authors devise an innovative one-vs-all global clustering algorithm to distinguish data samples between "known" and "unknown" categories. They also introduce a local k-NN clustering strategy to alleviate negative transfer from incorrect pseudo-label assignments. Extensive experiments demonstrate the effectiveness of GLC on the challenging open-partial-set domain adaptation scenario across multiple benchmarks like Office-31, Office-Home, VisDA, and DomainNet. Remarkably, GLC achieves state-of-the-art performance, outperforming prior arts by a large margin. For example, on VisDA benchmark, GLC attains 73.1% H-score in open-partial-set setting, substantially higher than the previous best method. The proposed technique provides a simple yet effective approach for model upcycling and adaptation without needing source data.


## Summarize the paper in one sentence.

 This paper proposes a global and local clustering (GLC) technique to enable source-free universal domain adaptation by identifying known data and rejecting unknown data under domain shift and category shift, using only a standard pre-trained closed-set model without needing dedicated model architectures.


## Summarize the paper in one paragraphs.

 Here is a one paragraph summary of the paper:

This paper proposes a new method called Global and Local Clustering (GLC) for adapting pre-trained deep neural networks to new target tasks and datasets without requiring access to the original training data. GLC enables "universal domain adaptation" where the source and target domains may have different classes. GLC has two main components: (1) An adaptive global one-vs-all clustering algorithm that assigns pseudo-labels to target data to separate "known" samples belonging to source classes and "unknown" samples belonging to novel target classes. (2) A local kNN clustering method that refines the pseudo-labels by exploiting the intrinsic structure within the target data. Experiments on benchmark datasets with different types of category shifts (partial-set, open-set, open-partial-set) show GLC achieves state-of-the-art performance by effectively balancing between recognizing known classes and rejecting unknown classes. The method requires only a standard pre-trained source model without needing specialized model architectures.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the paper:

1. The paper proposes a new technique called Global and Local Clustering (GLC) for Source-free Universal Domain Adaptation (SF-UniDA). What are the key components of GLC and how do they work together to enable SF-UniDA?

2. The paper introduces an innovative global one-vs-all clustering algorithm for pseudo-labeling. How is this different from traditional pseudo-labeling strategies? Why is it more suitable for SF-UniDA with category shift?

3. The paper employs Silhouette criterion to help estimate the number of categories in the unlabeled target domain. Why is Silhouette a reasonable choice here? Are there any limitations or potential issues with using Silhouette in this way? 

4. The paper applies a confidence-based source private suppression strategy. What is the motivation behind this? How exactly does it work to avoid misleading from source private categories?

5. In addition to global clustering, the paper also utilizes local k-NN clustering. What is the motivation to introduce this additional local clustering? How does it complement global clustering?

6. The paper evaluates GLC extensively on several benchmarks under partial-set, open-set and open-partial-set scenarios. What are the key results and how do they demonstrate the effectiveness of GLC? 

7. The paper shows GLC can enable SF-UniDA using only a standard closed-set pre-trained model, without needing specialized model architectures. Why is this useful and what are the practical benefits?

8. How suitable do you think GLC could be for adapting models in more complex, real-world applications? What kinds of practical challenges might arise?

9. The paper compares GLC with several other domain adaptation methods. What are the key differences in techniques between these methods and GLC? Why does GLC perform better?

10. The paper focuses on unsupervised domain adaptation. Do you think the GLC technique could be extended or modified to support semi-supervised or weakly supervised scenarios? Why or why not?
