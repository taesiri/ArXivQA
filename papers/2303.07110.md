# [Upcycling Models under Domain and Category Shift](https://arxiv.org/abs/2303.07110)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading, the central research question this paper aims to address is: How can we adapt deep neural network models to new target domains and tasks using only a pre-trained source model, without requiring the original source training data, and without knowing the category shift a priori?In particular, the key goals are:- To perform unsupervised domain adaptation without access to source data (only a pre-trained source model), known as source-free domain adaptation. - To handle various types of category shift between source and target domains, including partial-set, open-set, and open-partial set shifts, in a unified framework.- To identify "known" samples belonging to shared source-target classes and reject "unknown" samples from novel target-only classes.- To achieve this model adaptation and "known vs unknown" separation using only standard pre-trained closed-set source models, without needing specially designed model architectures.The key hypothesis is that by combining global clustering to separate shared vs private classes along with local clustering to refine predictions, one can effectively adapt models to new target domains and tasks under varying degrees of category shift in a source-free manner.


## What is the main contribution of this paper?

 The main contribution of this paper is proposing a generic technique called Global and Local Clustering (GLC) for model upcycling under domain shift and category shift. Specifically:- They propose a source-free universal domain adaptation (SF-UniDA) setting to handle various category shifts between source and target domains, including partial-set, open-set, and open-partial-set scenarios. This allows adapting models when the source and target domains have different label spaces, with only access to a pre-trained source model.- They develop an innovative one-vs-all global clustering algorithm to assign pseudo-labels and separate "known" and "unknown" data samples without prior knowledge of the category shift. This includes techniques like adaptive target category estimation and source-private category suppression. - They introduce a local k-NN clustering strategy to further mitigate negative transfer by exploiting the intrinsic target domain structure. - They provide extensive experiments showing GLC achieves state-of-the-art performance on standard DA benchmarks across various category shift settings. Remarkably, GLC outperforms prior arts by 14.8% on VisDA for open-partial-set DA.- They demonstrate the effectiveness of GLC on more realistic applications like remote sensing, wildlife classification, and single-cell RNA sequencing.In summary, the key contribution is proposing a simple yet generic GLC technique to enable model upcycling under both domain and category shifts in a unified source-free manner, with strong empirical results. This has important practical implications for model reuse.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the key points from the paper:The paper proposes a Global and Local Clustering (GLC) technique for upcycling pre-trained models to perform domain adaptation in source-free settings under both domain shift and category shift, without requiring access to source data or dedicated source model architectures.


## How does this paper compare to other research in the same field?

 Here are my thoughts on how this paper compares to other research in the field of domain adaptation:Overall, the paper presents a novel approach for source-free universal domain adaptation (SF-UniDA), which aims to adapt models under both domain shift and category shift using only a standard closed-set pre-trained model. This is an important and challenging problem.Key strengths:- Tackles a very difficult and practical setting of SF-UniDA without requiring dedicated model architectures, unlike some prior works. This makes the approach widely applicable. - Proposes an innovative global and local clustering technique (GLC) to handle both domain and category shift in a unified manner. The global clustering aims to separate "known" vs "unknown" data while the local clustering reduces negative transfer.- Achieves state-of-the-art performance across multiple domain adaptation benchmarks under various category shift scenarios like partial/open/open-partial set DA. Especially strong results on VisDA for open-partial DA, outperforming prior arts by 15-16%.- Validates GLC on realistic applications including remote sensing, wildlife classification and single cell classification, showing usefulness beyond standard DA benchmarks.Limitations:- While it handles various category shifts in a unified manner, the performance gaps between different category shift scenarios are still significant. There is room for further improvement, especially on partial/open set shifts.- For practical usage, the choices of some key hyperparameters like the weighting between losses may need more rigorous tuning/guidelines. Ablations provide some analysis but more work on adaptive selection could help.- The computational overhead and convergence may still be concerns for adoption on very large-scale problems compared to source-free closed set DA methods.Overall, I think this is a novel and promising approach for tackling the highly challenging problem of SF-UniDA, with extensive experiments highlighting its effectiveness. The unified handling of domain+category shift and applicability based just on standard closed-set models are notable strengths. While limitations exist, it pushes forward a very important research direction.


## What future research directions do the authors suggest?

 Based on my reading of the paper, here are some of the main future research directions suggested by the authors:- Exploring additional or alternative clustering strategies for the global and local clustering approach. The authors propose using k-means clustering and k-NN clustering, but indicate there may be room for improvement with other clustering algorithms.- Extending the method to more complex data such as images, video, and speech beyond the classification tasks tested. The authors suggest exploring adaptations for these modalities.- Investigating semi-supervised or weakly-supervised versions of the approach that can utilize a small amount of labeled target data. The current method is fully unsupervised but incorporating some target labels could be beneficial.- Applying the method to additional real-world applications and datasets beyond the ones tested to further demonstrate versatility. The authors suggest domains like medical imaging and natural language as possibilities. - Developing theoretical understandings of when and why the global and local clustering approach is effective. The empirical results are promising but more analysis on the theory side could provide insights.- Exploring adaptations of the approach for other transfer learning scenarios beyond domain adaptation, such as cross-dataset or cross-task transfer. The clustering concepts may generalize.- Combining ideas from the method with other domain adaptation techniques like adversarial learning for potentially complementary benefits. There may be room for hybrid approaches.In summary, the main future directions pointed out relate to improving and extending the clustering techniques, applying the method to broader types of data and applications, developing theory, and integrating the approach with other transfer learning methods. The proposed global and local clustering technique shows promise but the authors highlight quite a few areas for additional exploration.


## Summarize the paper in one paragraph.

 The paper proposes a method called Global and Local Clustering (GLC) for adapting pre-trained deep neural network models to new target domains and categories without access to the original source training data. The key ideas are:1) A global one-vs-all clustering algorithm to assign pseudo-labels for separating known and unknown categories in the target domain. It adaptively clusters target data into prototypes representing each source class vs everything else, enabling open set adaptation. 2) A local kNN clustering loss to refine pseudo-labels based on neighborhood consensus, mitigating negative transfer.3) Adaptive estimation of target categories using silhouette analysis, removing reliance on prior knowledge.The method is evaluated on standard DA benchmarks and simulated applications like satellite image scene recognition and animal species classification. Results show GLC effectively enables model adaptation under partial/open/open-partial category shift without source data, outperforming prior domain adaptation methods. The simplicity and effectiveness of GLC for upcycling models and rejecting unknowns may enable deploying pre-trained models safely in new real-world scenarios.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the paper:This paper proposes a novel technique called Global and Local Clustering (GLC) to enable source-free universal domain adaptation (SF-UniDA). The goal is to adapt a pre-trained source model to identify "known" data samples and reject "unknown" data samples under both domain shift and category shift, without access to the original source data. The authors introduce GLC to address three category shift scenarios: partial-set DA, open-set DA, and open-partial-set DA. GLC has two main components: (1) An innovative global one-vs-all clustering algorithm that assigns pseudo-labels to target data samples to separate "known" vs "unknown" classes. This clustering is made adaptive using the Silhouette criterion since the type of category shift is unknown. (2) A local k-NN consensus clustering that exploits the intrinsic target data structure to mitigate negative transfer from incorrect pseudo-labels. Experiments on four benchmarks under the three category shift scenarios demonstrate that GLC achieves state-of-the-art performance. Notably, on the challenging open-partial-set VisDA benchmark, GLC attains 73.1% H-score, outperforming prior arts UMAD and GATE by 14.8% and 16.7% respectively. The results highlight the effectiveness of GLC for SF-UniDA across diverse shifts between source and target domains.In summary, this paper proposes a novel Global and Local Clustering (GLC) technique to achieve source-free universal domain adaptation without access to source data. GLC introduces an adaptive global clustering to separate "known" and "unknown" classes and a local clustering to alleviate negative transfer. Extensive experiments under various category shifts validate the effectiveness of GLC, where it substantially outperforms prior arts on challenging benchmarks like VisDA for open-partial-set domain adaptation. The work represents an important advancement for adapting pre-trained models under combined domain and category shift.


## Summarize the main method used in the paper in one paragraph.

 The paper proposes a Global and Local Clustering (GLC) method for source-free universal domain adaptation (SF-UniDA). The goal is to adapt a pretrained closed-set source model to identify "known" data samples and reject "unknown" data samples under both domain shift and category shift, without access to source data. The key ideas are:1. A global one-vs-all clustering algorithm is used to assign pseudo-labels and separate "known" vs "unknown" data. It creates prototypes for each source class vs everything else, and assigns pseudo-labels based on nearest prototype distance. 2. A confidence-based suppression mechanism is used to avoid misleading from source private categories. Categories with lower prediction confidence on target data are suppressed.3. A local kNN clustering loss is added to mitigate negative transfer by encouraging local target consensus. The method is evaluated on partial-set, open-set, and open-partial-set DA scenarios across several benchmarks. Results show it outperforms prior arts, especially in the most challenging open-partial-set case. The global clustering is key for "known" vs "unknown" separation, while local clustering reduces negative transfer.


## What problem or question is the paper addressing?

 The paper is addressing the problem of how to adapt deep neural networks to new domains when there is both a domain shift as well as a category shift between the source and target domains. Specifically, it focuses on the challenging problem of source-free universal domain adaptation (SF-UniDA), where the goal is to identify "known" data samples and reject "unknown" data samples in the target domain, using only knowledge from a pretrained closed-set source model, without access to the original source data. The category shift means the label spaces between the source and target domain are different, which poses additional challenges beyond just domain shift.The key questions the paper tries to address are:1) How to achieve pseudo-labeling to identify "known" and reject "unknown" target samples when the label spaces are different, with no access to source data? 2) How to adapt a standard pretrained closed-set source model to the target domain under both domain and category shift, without needing specialized model architectures?3) How to handle different types of category shift in a unified manner, including partial-set, open-set, and open-partial set scenarios, without knowing the type of shift a priori?So in summary, the paper focuses on the very challenging problem of adapting models to new domains with both domain and category shift, in a source-free manner, using only standard pretrained models, and handling different category shift scenarios in a unified framework.
