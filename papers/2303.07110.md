# [Upcycling Models under Domain and Category Shift](https://arxiv.org/abs/2303.07110)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central research question this paper aims to address is: How can we adapt deep neural network models to new target domains and tasks using only a pre-trained source model, without requiring the original source training data, and without knowing the category shift a priori?In particular, the key goals are:- To perform unsupervised domain adaptation without access to source data (only a pre-trained source model), known as source-free domain adaptation. - To handle various types of category shift between source and target domains, including partial-set, open-set, and open-partial set shifts, in a unified framework.- To identify "known" samples belonging to shared source-target classes and reject "unknown" samples from novel target-only classes.- To achieve this model adaptation and "known vs unknown" separation using only standard pre-trained closed-set source models, without needing specially designed model architectures.The key hypothesis is that by combining global clustering to separate shared vs private classes along with local clustering to refine predictions, one can effectively adapt models to new target domains and tasks under varying degrees of category shift in a source-free manner.


## What is the main contribution of this paper?

The main contribution of this paper is proposing a generic technique called Global and Local Clustering (GLC) for model upcycling under domain shift and category shift. Specifically:- They propose a source-free universal domain adaptation (SF-UniDA) setting to handle various category shifts between source and target domains, including partial-set, open-set, and open-partial-set scenarios. This allows adapting models when the source and target domains have different label spaces, with only access to a pre-trained source model.- They develop an innovative one-vs-all global clustering algorithm to assign pseudo-labels and separate "known" and "unknown" data samples without prior knowledge of the category shift. This includes techniques like adaptive target category estimation and source-private category suppression. - They introduce a local k-NN clustering strategy to further mitigate negative transfer by exploiting the intrinsic target domain structure. - They provide extensive experiments showing GLC achieves state-of-the-art performance on standard DA benchmarks across various category shift settings. Remarkably, GLC outperforms prior arts by 14.8% on VisDA for open-partial-set DA.- They demonstrate the effectiveness of GLC on more realistic applications like remote sensing, wildlife classification, and single-cell RNA sequencing.In summary, the key contribution is proposing a simple yet generic GLC technique to enable model upcycling under both domain and category shifts in a unified source-free manner, with strong empirical results. This has important practical implications for model reuse.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper proposes a Global and Local Clustering (GLC) technique for upcycling pre-trained models to perform domain adaptation in source-free settings under both domain shift and category shift, without requiring access to source data or dedicated source model architectures.


## How does this paper compare to other research in the same field?

Here are my thoughts on how this paper compares to other research in the field of domain adaptation:Overall, the paper presents a novel approach for source-free universal domain adaptation (SF-UniDA), which aims to adapt models under both domain shift and category shift using only a standard closed-set pre-trained model. This is an important and challenging problem.Key strengths:- Tackles a very difficult and practical setting of SF-UniDA without requiring dedicated model architectures, unlike some prior works. This makes the approach widely applicable. - Proposes an innovative global and local clustering technique (GLC) to handle both domain and category shift in a unified manner. The global clustering aims to separate "known" vs "unknown" data while the local clustering reduces negative transfer.- Achieves state-of-the-art performance across multiple domain adaptation benchmarks under various category shift scenarios like partial/open/open-partial set DA. Especially strong results on VisDA for open-partial DA, outperforming prior arts by 15-16%.- Validates GLC on realistic applications including remote sensing, wildlife classification and single cell classification, showing usefulness beyond standard DA benchmarks.Limitations:- While it handles various category shifts in a unified manner, the performance gaps between different category shift scenarios are still significant. There is room for further improvement, especially on partial/open set shifts.- For practical usage, the choices of some key hyperparameters like the weighting between losses may need more rigorous tuning/guidelines. Ablations provide some analysis but more work on adaptive selection could help.- The computational overhead and convergence may still be concerns for adoption on very large-scale problems compared to source-free closed set DA methods.Overall, I think this is a novel and promising approach for tackling the highly challenging problem of SF-UniDA, with extensive experiments highlighting its effectiveness. The unified handling of domain+category shift and applicability based just on standard closed-set models are notable strengths. While limitations exist, it pushes forward a very important research direction.
