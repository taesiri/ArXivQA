# [Upcycling Models under Domain and Category Shift](https://arxiv.org/abs/2303.07110)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central research question this paper aims to address is: How can we adapt deep neural network models to new target domains and tasks using only a pre-trained source model, without requiring the original source training data, and without knowing the category shift a priori?In particular, the key goals are:- To perform unsupervised domain adaptation without access to source data (only a pre-trained source model), known as source-free domain adaptation. - To handle various types of category shift between source and target domains, including partial-set, open-set, and open-partial set shifts, in a unified framework.- To identify "known" samples belonging to shared source-target classes and reject "unknown" samples from novel target-only classes.- To achieve this model adaptation and "known vs unknown" separation using only standard pre-trained closed-set source models, without needing specially designed model architectures.The key hypothesis is that by combining global clustering to separate shared vs private classes along with local clustering to refine predictions, one can effectively adapt models to new target domains and tasks under varying degrees of category shift in a source-free manner.


## What is the main contribution of this paper?

The main contribution of this paper is proposing a generic technique called Global and Local Clustering (GLC) for model upcycling under domain shift and category shift. Specifically:- They propose a source-free universal domain adaptation (SF-UniDA) setting to handle various category shifts between source and target domains, including partial-set, open-set, and open-partial-set scenarios. This allows adapting models when the source and target domains have different label spaces, with only access to a pre-trained source model.- They develop an innovative one-vs-all global clustering algorithm to assign pseudo-labels and separate "known" and "unknown" data samples without prior knowledge of the category shift. This includes techniques like adaptive target category estimation and source-private category suppression. - They introduce a local k-NN clustering strategy to further mitigate negative transfer by exploiting the intrinsic target domain structure. - They provide extensive experiments showing GLC achieves state-of-the-art performance on standard DA benchmarks across various category shift settings. Remarkably, GLC outperforms prior arts by 14.8% on VisDA for open-partial-set DA.- They demonstrate the effectiveness of GLC on more realistic applications like remote sensing, wildlife classification, and single-cell RNA sequencing.In summary, the key contribution is proposing a simple yet generic GLC technique to enable model upcycling under both domain and category shifts in a unified source-free manner, with strong empirical results. This has important practical implications for model reuse.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper proposes a Global and Local Clustering (GLC) technique for upcycling pre-trained models to perform domain adaptation in source-free settings under both domain shift and category shift, without requiring access to source data or dedicated source model architectures.


## How does this paper compare to other research in the same field?

Here are my thoughts on how this paper compares to other research in the field of domain adaptation:Overall, the paper presents a novel approach for source-free universal domain adaptation (SF-UniDA), which aims to adapt models under both domain shift and category shift using only a standard closed-set pre-trained model. This is an important and challenging problem.Key strengths:- Tackles a very difficult and practical setting of SF-UniDA without requiring dedicated model architectures, unlike some prior works. This makes the approach widely applicable. - Proposes an innovative global and local clustering technique (GLC) to handle both domain and category shift in a unified manner. The global clustering aims to separate "known" vs "unknown" data while the local clustering reduces negative transfer.- Achieves state-of-the-art performance across multiple domain adaptation benchmarks under various category shift scenarios like partial/open/open-partial set DA. Especially strong results on VisDA for open-partial DA, outperforming prior arts by 15-16%.- Validates GLC on realistic applications including remote sensing, wildlife classification and single cell classification, showing usefulness beyond standard DA benchmarks.Limitations:- While it handles various category shifts in a unified manner, the performance gaps between different category shift scenarios are still significant. There is room for further improvement, especially on partial/open set shifts.- For practical usage, the choices of some key hyperparameters like the weighting between losses may need more rigorous tuning/guidelines. Ablations provide some analysis but more work on adaptive selection could help.- The computational overhead and convergence may still be concerns for adoption on very large-scale problems compared to source-free closed set DA methods.Overall, I think this is a novel and promising approach for tackling the highly challenging problem of SF-UniDA, with extensive experiments highlighting its effectiveness. The unified handling of domain+category shift and applicability based just on standard closed-set models are notable strengths. While limitations exist, it pushes forward a very important research direction.


## What future research directions do the authors suggest?

Based on my reading of the paper, here are some of the main future research directions suggested by the authors:- Exploring additional or alternative clustering strategies for the global and local clustering approach. The authors propose using k-means clustering and k-NN clustering, but indicate there may be room for improvement with other clustering algorithms.- Extending the method to more complex data such as images, video, and speech beyond the classification tasks tested. The authors suggest exploring adaptations for these modalities.- Investigating semi-supervised or weakly-supervised versions of the approach that can utilize a small amount of labeled target data. The current method is fully unsupervised but incorporating some target labels could be beneficial.- Applying the method to additional real-world applications and datasets beyond the ones tested to further demonstrate versatility. The authors suggest domains like medical imaging and natural language as possibilities. - Developing theoretical understandings of when and why the global and local clustering approach is effective. The empirical results are promising but more analysis on the theory side could provide insights.- Exploring adaptations of the approach for other transfer learning scenarios beyond domain adaptation, such as cross-dataset or cross-task transfer. The clustering concepts may generalize.- Combining ideas from the method with other domain adaptation techniques like adversarial learning for potentially complementary benefits. There may be room for hybrid approaches.In summary, the main future directions pointed out relate to improving and extending the clustering techniques, applying the method to broader types of data and applications, developing theory, and integrating the approach with other transfer learning methods. The proposed global and local clustering technique shows promise but the authors highlight quite a few areas for additional exploration.


## Summarize the paper in one paragraph.

The paper proposes a method called Global and Local Clustering (GLC) for adapting pre-trained deep neural network models to new target domains and categories without access to the original source training data. The key ideas are:1) A global one-vs-all clustering algorithm to assign pseudo-labels for separating known and unknown categories in the target domain. It adaptively clusters target data into prototypes representing each source class vs everything else, enabling open set adaptation. 2) A local kNN clustering loss to refine pseudo-labels based on neighborhood consensus, mitigating negative transfer.3) Adaptive estimation of target categories using silhouette analysis, removing reliance on prior knowledge.The method is evaluated on standard DA benchmarks and simulated applications like satellite image scene recognition and animal species classification. Results show GLC effectively enables model adaptation under partial/open/open-partial category shift without source data, outperforming prior domain adaptation methods. The simplicity and effectiveness of GLC for upcycling models and rejecting unknowns may enable deploying pre-trained models safely in new real-world scenarios.
