# [The Efficiency Misnomer](https://arxiv.org/abs/2110.12894v2)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the main research question seems to be: How can we efficiently evaluate neural network architecture candidates during architecture search?

The paper proposes a new method called "proxylessNAS" that aims to address the inefficiencies of previous neural architecture search methods. Specifically, previous methods required training each architecture candidate from scratch to evaluate it. The authors argue this is prohibitively expensive computationally. 

Their key idea is to train a "supernet" that contains all possible architecture candidates within it. The supernet is trained only once. Then, to evaluate a particular architecture candidate, they simply inherit the weights from the supernet and evaluate the performance of that sub-network. This proxy evaluation is much faster than training from scratch.

So in summary, the central hypothesis is that they can greatly improve the efficiency of neural architecture search by using a proxy evaluation method based on a trained supernet, rather than training each candidate from scratch. The paper then introduces and validates the proposed proxylessNAS method.


## What is the main contribution of this paper?

 Based on my reading, the main contribution of this paper is to highlight potential issues with using incomplete or limited metrics to evaluate model efficiency in machine learning research. The key points are:

- The paper argues that no single metric fully captures model efficiency. Metrics like parameter count, FLOPs, and speed/throughput each have limitations if used alone. 

- Using only a subset of efficiency metrics can lead to misleading or partial comparisons between models. The paper gives examples where relative model performance looks different depending on which metric is used.

- The paper coins the term "efficiency misnomer" to refer to this phenomenon of incomplete or misleading efficiency reporting and comparisons. 

- The paper suggests always reporting multiple efficiency metrics like parameters, FLOPs, and speed to avoid efficiency misnomers. Plotting performance/cost curves using multiple metrics can give a more complete picture.

- The paper also discusses issues with common practices like parameter-matched or FLOP-matched comparisons between models, and using efficiency metrics to constrain architecture search.

Overall, the key contribution is highlighting the nuances of evaluating model efficiency and providing suggestions to avoid efficiency misnomers through more comprehensive and thoughtful reporting and analysis. The paper aims to raise awareness about this issue in the ML research community.
