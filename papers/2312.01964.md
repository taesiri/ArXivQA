# [Semantics-aware Motion Retargeting with Vision-Language Models](https://arxiv.org/abs/2312.01964)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a high-quality one-paragraph summary of the key points from the paper:

This paper presents a novel semantics-aware motion retargeting approach that leverages vision-language models to extract and maintain meaningful motion semantics. A key innovation is using differentiable rendering to translate motions into images that are fed into a frozen vision-language model, which provides semantic supervision through extracted embeddings aligned between the source and target motions. This semantic consistency loss allows high-level motion characteristics to be incorporated into a two-stage training process, consisting of: 1) skeleton-aware pre-training to establish a base model focused on trajectory similarity without considering semantics or geometry, and 2) fine-tuning with semantics and geometry constraints to preserve semantics and avoid interpenetration. Experiments demonstrate state-of-the-art performance marked by motions with superior semantics consistency and quality. The method addresses the field's deficiency in labelled semantic motion data through an unsupervised semantics extraction technique from advanced vision-language models. By aligning latent semantic embeddings, the approach facilitates semantics-aware retargeting without reliance on manual semantic annotations or predefined joint-level motion characteristics.
