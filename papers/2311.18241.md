# [LLVMs4Protest: Harnessing the Power of Large Language and Vision Models   for Deciphering Protests in the News](https://arxiv.org/abs/2311.18241)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a high-quality paragraph summarizing the key points of the paper:

This paper documents the development of two fine-tuned large language and vision models - Longformer and Swin Transformer v2 - to identify protest events and extract attributes from news articles and images. The Longformer model was trained on over 11,000 New York Times articles matched to protest records from the Dynamics of Collective Action database to classify textual protest content. The Swin Transformer model was trained on the UCLA Protest Image Dataset to identify visual protest cues like signs and police presence. Both models achieve over 94% accuracy and will be released to advance computational social movement research. The paper discusses limitations around temporal constraints of the training data and the limited protest attributes currently extracted. It also notes emerging opportunities to use generative AI like GPT-4 and ChatGPT to further advance protest event data collection and analysis with less technical expertise required. Overall, this work demonstrates the potential for fine-tuned language and vision models to automate the identification and attribute labeling of protests in multimodal data at scale.


## Summarize the paper in one sentence.

 Here is a one-sentence summary of the paper:

The paper documents the fine-tuning of Longformer and Swin Transformer models on protest event data to identify protests and extract attributes from news articles and images, with the goal of harnessing large language and vision models to advance the computational analysis of protests.


## What is the main contribution of this paper?

 Based on the content in the paper, the main contribution is:

The authors have fine-tuned two large pretrained transformer models - Longformer and Swin Transformer V2 - to identify protest events and extract protest attributes from news articles and images. Specifically:

1) The Longformer model was fine-tuned on the Dynamics of Collective Action (DoCA) dataset matched with New York Times articles to classify whether a news article contains information about a protest event.

2) The Swin Transformer V2 model was fine-tuned on the UCLA Protest Image dataset to detect protest-related attributes in images, such as presence of protest, violence, signs, etc.

The fine-tuned Longformer and Swin Transformer models aim to help social movement scholars analyze textual and visual data to identify protests and extract key information. These models have accuracy rates over 94% and are made easily usable without needing extensive machine learning expertise.

In summary, the main contribution is the development and release of fine-tuned transformer models to harness the power of large language and vision models to decipher protests in news articles and images.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key keywords and terms associated with this paper include:

- Large language models (LLMs)
- Large vision models 
- Protest identification
- Protest attribute extraction
- Transfer learning
- Fine-tuning
- Longformer
- Swin Transformer
- Dynamics of Collective Action (DoCA)
- UCLA Protest Image Dataset
- Multimodal data
- Textual data
- Imagery data
- Social movements
- Generative AI
- Deep learning
- ChatGPT
- Computational social science

The paper documents the process of fine-tuning large language and vision models (Longformer and Swin Transformer) on labeled protest data (DoCA and UCLA datasets) in order to identify protests and extract attributes from news articles and images. The goal is to harness these models to advance the computational analysis of protests and social movements using multimodal data. Some key themes include transfer learning, generative AI, and applications for computational social science.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in this paper:

1. The paper mentions using the Longformer model for protest event identification. What are the key advantages of using Longformer over BERT for this task, given that Longformer is also initialized from RoBERTa?

2. The paper uses the Swin Transformer V2 model for image classification. What are some of the architectural differences between Swin Transformer and CNN models like ResNet that make the former more suitable for vision tasks?

3. What were some of the key data preprocessing and cleaning steps involved before fine-tuning the Longformer model? What challenges did the authors likely face in matching the NYT articles with DoCA records?

4. What validation metrics and scores did the authors use to evaluate the fine-tuned Longformer model? What were the final accuracy scores obtained? 

5. The paper mentions the longitudinal limitation of the DoCA protest event data from 1960-1995. How can the authors expand the training data to make the model more generalizable to recent protests?

6. How can the authors further enhance the model to extract more fine-grained protest attributes beyond basic event identification? What additional annotation would be needed?

7. How well would you expect the fine-tuned Longformer model to generalize to protest identification in non-US contexts and languages? What adaptations would be needed?

8. What are some techniques the authors could have used during fine-tuning to prevent overfitting of the Longformer and Swin models to the training data? 

9. The paper points to comparing model performance against GPT-4. What metrics and tests can the authors design for a fair comparison between the two approaches?

10. Besides the limitations discussed, what are some other potential issues to keep in mind while deploying these models for real-world protest event identification from news data?
