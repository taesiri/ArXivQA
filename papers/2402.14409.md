# [Tug-of-War Between Knowledge: Exploring and Resolving Knowledge   Conflicts in Retrieval-Augmented Language Models](https://arxiv.org/abs/2402.14409)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Retrieval-augmented language models (RALMs) integrate internal memory with external evidence from sources like Wikipedia to mitigate hallucination risks. However, knowledge conflicts frequently arise between the model's internal memory and external sources or among the external sources themselves. These conflicts can limit the practical applicability of RALMs.

Methodology: 
- The authors present an evaluation framework to assess knowledge conflicts in RALMs across dimensions like open-domain, entity popularity, and multi-hop reasoning. Experiments are conducted with 8 models on 4 QA datasets.

- Model behavior is analyzed from 3 perspectives - correctness, faithfulness to evidence, and reliance on internal memory. Two key conflict scenarios are studied:

1) Between internal memory and external sources:
- More capable models gain confidence in their internal memory, exhibiting the Dunning-Kruger effect by stubbornly trusting incorrect internal memory even with correct external evidence.
- Models display availability bias, preferring commonly known knowledge in internal memory over external sources for long-tail knowledge.

2) Among truthful, irrelevant and misleading evidence:  
- Models struggle to discern truthful from misleading evidence, following majority rule by trusting more frequent evidence.
- Models display confirmation bias by preferring evidence that aligns with their internal memory.
- Performance declines as the number of conflicting evidence hops increases.

Proposed Solution - Conflict-Disentangle Contrastive Decoding (CD2):
- Contrastive decoding is applied to amplify differences between expert and amateur logits. This helps resolve conflicts between internal memory and external sources.
- Fact-aware instruction tuning makes the model aware of truthful/misleading evidence. Expert and amateur LMs generate truthful/misleading answers respectively. Contrastive decoding maximizes the difference in their logits to identify truthful evidence.

Results:
- Without parameter updates, CD2 improves performance over in-context learning by 2-3% recall on conflicting QA datasets. It also substantially reduces reliance on misleading evidence.

Contributions:
- Thorough investigation and analysis of knowledge conflict behaviors in RALMs
- Effective CD2 method to resolve knowledge conflicts and improve performance
