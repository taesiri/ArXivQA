# Instruct2Act: Mapping Multi-modality Instructions to Robotic Actions
  with Large Language Model

## What is the central research question or hypothesis that this paper addresses?

Based on the abstract, it seems this paper presents a framework called Instruct2Act that allows robots to execute tasks based on multi-modality instructions (e.g. natural language, images). The key aspects appear to be:- Utilizing large language models (LLMs) to map instructions to robotic actions/programs. The LLMs generate Python code that calls different modules to perceive the environment and plan/execute actions.- Leveraging visual foundation models like SAM for segmentation and CLIP for classification to understand objects in the environment. These models provide visual perception abilities.- Supporting flexible instruction modalities like language, language+image, or pointing+language. A modular retrieval architecture handles the different input types.- Evaluating the approach on a variety of manipulation tasks from the VIMABench benchmark. The main hypothesis seems to be that combining LLMs and visual foundation models in this framework can enable effective robotic task execution from high-level instructions with minimal training or fine-tuning. The results appear to validate this, showing strong performance compared to prior learning-based methods.In summary, the key research question is whether this instructable robot framework combining LLMs and visual models can succeed at complex instruction-following with zero-shot generalization, and the experiments aim to demonstrate this capability.
