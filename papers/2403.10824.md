# [LookALike: Human Mimicry based collaborative decision making](https://arxiv.org/abs/2403.10824)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Language models (LLMs) show promise for sequential decision making and planning, but still struggle with real-world constraints and long-term reasoning. 
- Current methods rely on trial-and-error learning, which is slow and inefficient compared to more analytical planning approaches.
- LLMs can also be stubborn, repeating mistakes despite feedback. 

Proposed Solution:
- Explore a mimicry approach from cognitive science combined with imitation learning to impart context-aware problem solving skills to LLMs. 
- Use a "player" LLM (pLLM) to generate plans and actions.
- Have a separate "vision" LLM (vLLM) ingest human gameplay videos to construct a reward model. 
- Reward or punish pLLM actions based on vLLM's model to guide pLLM behavior.
- Test approach on complex text-based game (Zork) and procedurally generated game scenarios.

Key Contributions:
- Demonstrate a method to impart domain-specific reasoning abilities to LLMs without data-dependent training.
- Show visual information can provide crucial contextual knowledge for planning tasks.
- Propose a framework for collaborative decision making among LLM agents to enable knowledge transfer.
- Achieve significantly higher success rates on tasks compared to a single pLLM.
- Establish feasibility of using social learning principles for LLM problem solving.

Let me know if you need any clarification or have additional questions on the summary!


## Summarize the paper in one sentence.

 Here is a one sentence summary of the paper:

The paper proposes a novel method for collaborative decision making among language models by having them mimic human gameplay and using a separate vision language model to provide contextual rewards, enabling the language models to effectively solve complex games and tasks.


## What is the main contribution of this paper?

 Based on my reading of the paper, the main contribution seems to be proposing and evaluating a new method for training language models (LLMs) to solve planning and reasoning tasks. Specifically:

- They propose using a collaborative swarm of LLM agents that assume different "roles" and pass contextual information between themselves. This is aimed at capturing real-world context and domain knowledge to augment the LLMs' understanding.

- They introduce a framework with two LLMs - a "player" LLM that interacts with the environment/task, and a separate "vision" LLM that ingests human gameplay videos to construct a reward model. This reward model is then used to provide feedback and guide the player LLM.

- They demonstrate their method on textual gameplay tasks like Zork as well as procedural generation tasks, showing improved performance over just using an individual LLM. The collaborative swarm combined with the vision LLM reward model enables the LLMs to solve tasks they previously struggled with.

- They discuss how their socially learned mimicry approach avoids some downsides of supervised learning from human demonstrations, and provides a way for LLMs to learn planning skills in a self-supervised way.

In summary, the key contribution seems to be the proposed training framework to impart stronger reasoning and planning abilities to LLM agents through collaborative learning and reward modeling, without needing extensive new data or pretraining.


## What are the keywords or key terms associated with this paper?

 Based on reviewing the paper, the keywords listed in the paper are:

LLM, AI, Machine Learning, Role Play, swarm, collaboration


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in the paper:

1. The paper mentions using a "reward model actor" for learning to faithfully mimic a domain expert. Can you expand more on how this reward model works? What specific mechanisms allow it to guide the agent's actions towards favorable outcomes?

2. In the playground design, Zork and Procgen environments are used for evaluation. What are some key properties of these environments that make them suitable testbeds for the proposed approach? Are there any limitations?

3. The paper talks about using a separate vision LLM (vLLM) for ingesting visual information. What modalities of visual information does it ingest from the gameplay videos? How does it build the reward model using this? 

4. The paper mentions using MAPPO for learning to mimic. What are some advantages of MAPPO over other RL algorithms for this task? Are there any tweaks made to the MAPPO algorithm to make it more suitable?

5. How exactly does the attention mechanism help the agent keep track of other players in the environment? What are the inputs to this attention mechanism during training and inference?

6. What are some ways the memory provided to the LLMs as "reusable terms" helps improve planning performance? Does the memory need to be reset at any point?

7. For the swarm evaluation, how do the LLMs utilize the collaborative feedback provided to them from prior steps? Is any weighting done while aggregating feedback from different LLMs?

8. The results show that reward from vLLM helps improve success rates remarkably. What are some types of rewards provided by vLLM that contribute most to this improvement?  

9. How scalable is the proposed approach to even more complex planning tasks? What changes would be needed to handle tasks with much longer planning horizons?

10. The paper mentions automatic domain randomization (ADR) as a way to improve generalization. How can ADR be incorporated into the current framework? What components would need to be modified?
