# [Self-Alignment for Factuality: Mitigating Hallucinations in LLMs via   Self-Evaluation](https://arxiv.org/abs/2402.09267)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper "Self-Alignment for Factuality: Mitigating Hallucinations in LLMs via Self-Evaluation":

Problem:
- Large language models (LLMs) sometimes generate plausible but factually incorrect statements (hallucinations), even when they possess relevant knowledge. This undermines their reliability for real-world use.
- A key issue is the gap between the LLM "knowing" vs "telling" factual information. LLMs may fail to convey accurate information despite having the knowledge.

Proposed Solution: 
- The paper introduces a self-alignment framework called "Self-Alignment for Factuality" (SAF) to mitigate factual hallucinations in LLMs by utilizing their self-evaluation capability.
- A factuality self-evaluation module (Self-Eval) is designed to prompt the LLM to validate the factuality of its own responses based solely on its internal knowledge. 
- To improve the LLM's confidence estimation and calibration for self-evaluation, a Self-Knowledge Tuning (SKT) method is proposed.
- The factuality scores from Self-Eval are used as reward signals to fine-tune the LLM via Direct Preference Optimization towards enhanced factuality.

Main Contributions:
- Proposes SAF, a self-alignment strategy to mitigate hallucinations in LLMs using their self-evaluation ability without external knowledge.
- Introduces SKT to improve LLM's confidence estimation and calibration, boosting its self-evaluation capability.  
- Shows SAF significantly enhances factual accuracy of Llama models across MCQA, short-form text generation, and long-form text generation tasks on TruthfulQA and BioGEN datasets.
- Outperforms existing decoding-based and consistency-based methods for hallucination mitigation.

In summary, the key novelty is guiding LLMs towards factuality by utilizing their intrinsic capability to self-evaluate the factual correctness of their responses. The proposed SAF framework demonstrates strong empirical performance in mitigating hallucinations across diverse knowledge-intensive tasks.
