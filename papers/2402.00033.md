# [LF-ViT: Reducing Spatial Redundancy in Vision Transformer for Efficient   Image Recognition](https://arxiv.org/abs/2402.00033)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper "LF-ViT: Reducing Spatial Redundancy in Vision Transformer for Efficient Image Recognition":

Problem:
- Vision Transformers (ViTs) achieve high accuracy for image classification but have high computational costs that scale quadratically with the number of image patches/tokens. This limits their deployment on resource-constrained edge devices. 
- High-resolution images contain significant spatial redundancy, not all regions are equally relevant for recognition. Processing the full image causes inefficient use of computation.

Proposed Solution:
- LF-ViT, a two-stage vision transformer for efficient image recognition. It has a localization stage and focus stage.

Localization Stage:
- Downsampled input image is fed to ViT to make an initial prediction.
- If prediction confidence is high, inference stops and result is returned. Reduces computation for "easy" images.  
- If confidence is low, indicate "hard" image, the Neighborhood Global Class Attention (NGCA) mechanism identifies a small class-discriminative region in original high-res image for next stage.

Focus Stage:  
- Only processes the selected class-discriminative region from original high-res image for final prediction. Uses same ViT weights as localization stage.
- Reuses features from non-class regions in localization for background context. Fuses features of class-discriminative regions from both stages.

Main Contributions:
- Proposes localization and focus framework to reduce spatial redundancy of images for efficient ViT inference.
- Introduces NGCA method to accurately identify small class-discriminative regions.
- Achieves 1.7x speedup and 63% less FLOPs than DeiT-S while maintaining accuracy on ImageNet classification.
- Outperforms state-of-the-art image-level and token-level model optimization methods.
