# [Extracting Interpretable Local and Global Representations from Attention   on Time Series](https://arxiv.org/abs/2312.11466)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Interpretability and explainability of complex machine learning models like deep neural networks is still limited, especially for time series data. 
- The Transformer architecture using attention mechanisms has shown promise for time series tasks, but how attention contributes to performance and how it can be interpreted is not fully understood.
- Prior work on interpreting attention has focused mostly on NLP and computer vision, but time series data may require different techniques.

Proposed Solutions:

1) Local Attention-based Symbolic Abstraction (LASA)
- Uses SAX symbolization and attention to simplify/abstract local input data to find most informative local characteristics per class. 
- Reduces complexity to improve human understanding while maintaining classification accuracy.
- Enables human-in-the-loop optimization of abstraction via thresholds.

2) Global Coherence Representation (GCR) 
- Constructs global class-specific representations showing symbol-to-symbol coherence using aggregated attention.
- Allows interpretation of how all symbols influence each other at each time step for each class.
- Can act as interpretable classification model itself or approximate decisions of original model.
- Has Full, Column-Reduced, and Threshold matrix variants with different levels of detail.

Main Contributions:

- First comprehensive analysis of using Transformer attention for time series interpretation, with both local and global techniques
- LASA provides local input abstraction framework with human-in-the-loop optimization
- GCR enables global visualizations and models for symbol-to-symbol class trends 
- Show attention can be used to simplify data and construct interpretable approximations of decisions
- Demonstrate improved interpretability while maintaining model accuracy across many datasets

The paper makes an important contribution in advancing the understandability and interpretability of attention mechanisms for time series data.
