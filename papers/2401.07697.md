# [Data vs. Model Machine Learning Fairness Testing: An Empirical Study](https://arxiv.org/abs/2401.07697)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Existing ML fairness testing solutions only evaluate fairness after model training, ignoring potential biases in the training data. 
- No prior work has studied the relationship between fairness metrics applied to the training data (DFM) vs the trained model (MFM).

Proposed Solution:
- Evaluate fairness at two stages - before training using DFM and after training using MFM. 
- Conduct an empirical study relating DFM and MFM across 1600 evaluation cycles with 2 fairness metrics, 4 ML models, 5 datasets.
- Vary the distribution, size and features of the training data to analyze the relationship.

Key Contributions:
- Found a linear relationship between DFM and MFM when the distribution and size of the training data changes.  
- DFM can catch data drifts in production ML systems and avoid unnecessary retraining cycles.
- More training data reduces correlation between DFM and MFM, allowing models to circumvent some bias.
- DFM is useful when varying training size but not features. More features typically improve model fairness.
- First study bridging gap between data and model fairness metrics. Opens up data-centric AI research direction.

In summary, the paper proposes a novel "data-centric" approach to ML fairness testing and empirically analyzes the relationship between data and model fairness metrics under various data conditions. Key findings are that data metrics can catch drifts and avoid retraining, while more data helps models circumvent bias. The work is a first step towards integrating fairness evaluation into the ML development lifecycle.


## Summarize the paper in one sentence.

 Here is a one-sentence summary of the paper:

The paper empirically analyzes the relationship between model-dependent and independent fairness metrics under varying data conditions to evaluate the effectiveness of pre-training data fairness testing for identifying issues early in ML pipelines.


## What is the main contribution of this paper?

 Based on my reading of the paper, the main contribution is:

The paper proposes and empirically evaluates a novel "data-centric" approach to machine learning fairness testing, where fairness is evaluated both before training (using metrics to quantify bias in the training data) and after training (using metrics to quantify bias in the model predictions). The key finding is that there is a linear relationship between the data fairness metrics and model fairness metrics when the training data size and distribution changes. This suggests that testing for fairness prior to model training can be an effective and low-cost way to catch fairness issues early in ML pipelines. The paper also examines how factors like training data size impact the relationship between data and model fairness metrics. Overall, it makes the case for integrating data-centric fairness evaluation into the ML development lifecycle as a complement to evaluating model fairness after training.


## What are the keywords or key terms associated with this paper?

 Based on reviewing the paper, some of the key terms and keywords associated with it include:

- SE4ML (Software Engineering for Machine Learning)
- ML Fairness Testing
- Empirical Software Engineering 
- Data-centric AI
- Algorithmic Bias
- Bias Mitigation
- Group Fairness
- Disparate Impact (DI)
- Statistical Parity Difference (SPD)
- Data Fairness Metrics (DFM)
- Model Fairness Metrics (MFM)
- ML Development Lifecycle
- Data Drift
- Fairness-Efficiency-Performance Tradeoff
- Test Reduction

The paper conducts an empirical analysis of the relationship between data fairness metrics (DFM) and model fairness metrics (MFM). It evaluates fairness before and after ML model training, using real-world datasets. The goal is to understand if testing for bias in training data can catch issues early, reducing overall testing costs. Key terms reflect this data-centric focus on quantifying and mitigating algorithmic bias.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in this paper:

1. The paper proposes evaluating fairness at two stages of the ML lifecycle - before training using Data Fairness Metrics (DFM) and after training using Model Fairness Metrics (MFM). What are the key advantages and disadvantages of evaluating fairness at these two stages?

2. The study finds a positive correlation between DFM and MFM when the distribution and consequently the fairness properties of the training data changes. What are some ways this finding can be leveraged in practice for monitoring production ML systems? 

3. The paper highlights a trade-off between fairness, efficiency, and performance based on the training data size. What are some practical guidelines that can help ML teams navigate this trade-off when building real-world systems?

4. The correlation between DFM and MFM reduces as the size of the training data increases. What could be some reasons for this trend? How can this insight inform the choice of bias mitigation strategies?

5. The study evaluates group fairness metrics like Statistical Parity Difference and Disparate Impact. What are some limitations of these metrics? What other fairness metrics could complement them?  

6. The paper simulates distribution changes in training data using smaller sample sizes. What are some alternative techniques for rigorously evaluating the impact of distribution shifts on model fairness?

7. What kinds of assumptions does the correlation analysis between DFM and MFM make? How can the analysis be made more robust to validate the key results?  

8. The study is limited to tabular data and does not consider image, text and time series data. What additional considerations need to be made to evaluate fairness for those data modalities?

9. What are some ways in which the Data Fairness Metrics can be enhanced to account for feature influence on fairness, similar to Model Fairness Metrics?

10. The paper motivates DFM as a means for early detection of fairness issues and avoiding full training cycles. What empirical evidence is needed to further validate the practical viability of this approach?
