# [Among Us: Adversarially Robust Collaborative Perception by Consensus](https://arxiv.org/abs/2303.09495)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the abstract and introduction, this paper appears to address the following main research question:

How can multiple robots collaborate perceptually to achieve better scene understanding while defending against adversarial attacks in a computationally efficient and generalizable manner?

The key points are:

- Multi-robot collaboration can improve perception compared to individual robots, but is vulnerable to adversarial attacks when using deep neural networks.

- Existing defenses like adversarial training require knowledge of the attack method and can reduce accuracy. 

- This paper proposes a new defense approach called ROBOSAC that is computationally efficient, generalizable to unseen attacks, and scalable.

- The main idea is to use hypothesis testing to verify consensus between individual vs collaborative perception results to detect potential attackers.

- They optimize the number of collaborators and sampling budget to balance performance and computation.

- They also propose a method to estimate unknown attacker ratios by starting optimistic and getting more conservative.

So in summary, the main hypothesis is that consensus-based adversarial sample detection can enable efficient, scalable, and generalizable defense for multi-robot perceptual collaboration. The paper seems to aim to validate this hypothesis.


## What is the main contribution of this paper?

 This paper presents a novel framework called ROBOSAC for adversarialy robust collaborative perception in multi-robot systems. The main contributions are:

- It proposes a sampling-based approach called ROBOSAC that allows robots to collaborate safely without being fooled by adversarial attacks. The key idea is to sample subsets of teammates and verify consensus between collaborative and individual results before using shared information.

- It provides a theoretical analysis on the performance-computation tradeoff - how to maximize the number of benign collaborators or minimize sampling steps given constraints. 

- It develops an adaptive probing method called A2CP to estimate the unknown attacker ratio by starting from an aggressive assumption and gradually becoming more conservative.

- It validates ROBOSAC for collaborative 3D object detection in autonomous driving. The method demonstrates better generalizability than adversarial training since it is agnostic to attack methods.

In summary, the main contribution is the proposed ROBOSAC framework that enables efficient, scalable and generalizable defense against adversarial attacks for multi-robot collaborative perception. It does not rely on knowing the attack method. The key novelty is using sample consensus to identify benign collaborators.
