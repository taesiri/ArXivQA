# [SocialStigmaQA: A Benchmark to Uncover Stigma Amplification in   Generative Language Models](https://arxiv.org/abs/2312.07492)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Generative language models are prone to amplifying social bias against individuals with stigmatized conditions. Most prior work focuses on bias with respect to protected attributes like gender or race rather than stigmas.  
- There is a need for a comprehensive benchmark to evaluate bias amplification of stigmas in language models.

Proposed Solution:  
- The authors introduce SocialStigmaQA, a benchmark consisting of 10,360 question-answering prompts pertaining to 93 different stigmatized conditions documented in literature.  
- The prompts involve simple social situations like hiring someone or allowing children to play together. The questions ask whether to engage with someone with a stigma, to test for biased responses.
- Multiple prompt styles are used including adding positive or uncertain context and a no-stigma control. This tests model tendencies and robustness.

Main Contributions:
- First holistic social stigma bias benchmark for QA with 37 prompt templates covering 93 stigmas
- Analysis of impact of prompt styles in nudging towards more/less biased responses
- Manual analysis revealing model's lack of reasoning in chain-of-thought outputs which can exacerbate bias
- Evaluation using two large language models shows high rate (45-59%) of biased responses on this benchmark

In summary, this paper presents a comprehensive benchmark to evaluate social stigma bias in language models through QA prompts. Analysis of model outputs demonstrates a high tendency for stigma bias amplification. The insights on reasoning issues in chain-of-thought point to problems that could further exacerbate biases.
