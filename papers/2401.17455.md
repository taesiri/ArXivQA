# [Multiscale Parallel Tempering for Fast Sampling on Redistricting Plans](https://arxiv.org/abs/2401.17455)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem: 
When auditing a redistricting plan, a common method is to compare the plan to an ensemble of neutrally-drawn plans. However, to make fair comparisons, the plans in the ensemble must match the given plan in terms of non-partisan criteria like compactness, population balance, etc. Existing sampling algorithms either allow explicit policy-based distributions but have poor mixing times on large graphs, or mix well but only enable distributions based on spanning tree counts which may not match stated policies. 

Solution:
The paper introduces a multi-scale parallel tempering approach to sample from policy-based distributions on large redistricting graphs. Key ideas:

- Construct a hierarchy of progressively coarser graphs, with the finest being the original precinct graph  
- Run an MCMC sampler independently on each level. Use single-node flips for good mixing on compactness, populations, etc.
- Place the target policy-based distribution on the finest level
- Relax constraints gradually on higher levels to enable faster mixing
- Swap partition samples between adjacent levels using a new reversible projection mechanism
- Coarser levels explore state space faster, transfers mixing to finer levels via swaps

Contributions:

- Demonstrates much faster mixing speeds compared to single chain methods, scales to 700 node graph
- Expands ability to explicitly sample from policy-based distributions, not just spanning tree counts
- Shows control over compactness distributions and partisan consequences
- Compares to spanning tree distribution - significant differences in compactness and partisan outcomes
- Overall, enables sampling from broader policy measures at larger scales than possible before

The new sampler significantly advances the state-of-the-art in sampling redistricting plans for auditing purposes by expanding the types of policy-based distributions that can be sampled from efficiently even for large districting problems.


## Summarize the paper in one sentence.

 Here is a one sentence summary of the paper:

The paper introduces a novel multi-scale parallel tempering algorithm for efficiently sampling balanced graph partitions from a variety of policy-based probability distributions, demonstrating its effectiveness by sampling congressional districts for Connecticut at an unprecedented scale.


## What is the main contribution of this paper?

 The main contribution of this paper is the development of a novel multi-scale parallel tempering algorithm for sampling redistricting plans. Specifically:

- The paper introduces a geographic hierarchy constructed on the precinct graph, with multiple levels going from fine (original graph) to coarse (contracted graph).

- It develops a swap mechanism to exchange partition information between adjacent levels of the hierarchy in a probabilistic way that preserves the measures on each level. This handles the challenge that the state spaces on different levels are disjoint.

- It places measures on each level that interpolate between a target policy-based measure on the finest level and a fast-mixing uniform measure on the coarsest level. 

- It demonstrates that running independent Metropolis-Hastings chains on each level, with parallel tempering swaps between levels, allows efficient sampling from the target measure. This is enabled by using flip-based chains that mix fast on the coarse levels.

- It shows numerically that this approach can sample effectively from policy-based measures, defined in terms of population balance and Polsby-Popper compactness, on a 695 precinct graph of Connecticut. This goes far beyond the scale achieved by previous flip-based samplers.

In summary, the key innovation is a general parallel tempering framework that exploits fast-mixing single flip dynamics on coarsened representations of the graph to enable sampling from new classes of measures on fine graphs. This significantly advances the ability to sample redistricting plans based on explicit policy considerations.


## What are the keywords or key terms associated with this paper?

 Based on my review of the paper, some of the key terms and concepts include:

- Redistricting plans
- Balanced graph partitions
- Ensemble sampling
- Markov Chain Monte Carlo (MCMC) methods
- Single node flip algorithms
- Parallel tempering 
- Multiscale hierarchies
- Geographic hierarchies
- Swap mechanisms
- Policy-based probability distributions
- Compactness measures (e.g. Polsby-Popper score)
- Connecticut congressional districts 

The paper introduces a novel multi-scale sampling method using parallel tempering and geographic hierarchies to efficiently sample from policy-based probability distributions over redistricting plans. Key goals are expanding the types of measures that can be efficiently sampled from and applying the method to sample congressional districts in Connecticut at an unprecedented scale using measures incorporating compactness and population balance constraints.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in this paper:

1. The paper introduces a new multi-scale parallel tempering algorithm for sampling redistricting plans. How is the algorithm able to overcome energetic barriers that typically cause slow mixing when using single node flip methods on large graphs?

2. The paper discusses constructing a hierarchy of progressively coarser graphs. What considerations and heuristics are proposed for generating a hierarchy that is suitable for the multi-scale sampling algorithm? How might the choice of hierarchy impact mixing times?

3. The parallel tempering aspect involves placing different probability measures on each level of the hierarchy. How are these higher-level measures constructed in the paper, specifically in terms of the population and compactness constraints? What is the motivation behind the proposed construction?

4. A key contribution of the paper is the swap mechanism that enables partitions to be exchanged across hierarchy levels. Explain in detail how this swap mechanism works, including how the paired projection process is designed. 

5. When projecting a fine partition to a coarse level, what algorithmic choices are made in terms of which split nodes to merge and why? Conversely, what considerations guide splitting nodes when projecting a coarse partition to a finer level?

6. The paper demonstrates the method by sampling congressional districts in Connecticut. Discuss the graph construction process and any preprocessing steps that are done. What run parameters are used in terms of number of steps, parallel tempering schedule, etc.?

7. How is convergence assessed quantitatively in the results? What district-level observables are used and how are distances computed between independent runs? What evidence supports that the chains have converged?  

8. One experiment varies the compactness weight over multiple runs. How does this allow control over the marginal distributions of compactness in the ensembles? What differences in vote share distributions are observed?

9. The proposed measure is compared against a spanning forest-based measure. What motivates this comparison and what differences are observed between the measures in terms of both compactness and partisanship? 

10. The paper focuses on compactness and population constraints. Discuss how you might extend the methodology to account for other common redistricting criteria such as preservation of communities of interest or compliance with the Voting Rights Act.
