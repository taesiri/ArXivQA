# [Hidden Gems: 4D Radar Scene Flow Learning Using Cross-Modal Supervision](https://arxiv.org/abs/2303.00462)

## What is the central research question or hypothesis that this paper addresses?

Based on my understanding, the central research question this paper tries to address is:How to effectively learn 4D radar scene flow estimation without requiring manual annotations, by exploiting supervision signals from other co-located sensors (e.g. odometer, LiDAR, camera) on autonomous vehicles?The key hypothesis is that by opportunistically retrieving and combining complementary supervision cues from heterogeneous on-vehicle sensors, the radar scene flow estimation model can be trained in a cross-modal supervised manner, without needing costly human labeling efforts.In summary, the paper explores using cross-modal supervision from co-located sensors to enable unsupervised learning of radar scene flow, instead of relying on manual annotations or purely self-supervised techniques. The core research contribution is the proposed method to extract and fuse useful supervision signals across different modalities to guide the training process.


## What is the main contribution of this paper?

Based on my understanding, the main contributions of this paper are:1. It proposes a novel approach for 4D radar-based scene flow estimation using cross-modal learning. Previous works have focused on using LiDAR or stereo image data for scene flow estimation. This is the first work to explore using 4D radar data, which is becoming increasingly common on autonomous vehicles due to its robustness in poor weather/lighting conditions. 2. The key idea is to leverage supervision signals from co-located heterogeneous sensors (odometer, LiDAR, camera) on an autonomous vehicle to train the radar scene flow model, without requiring manual annotation of the radar data. This allows exploiting the redundancy in perception systems on modern vehicles.3. A multi-task neural network architecture and associated loss functions are introduced to effectively integrate the cross-modal supervision signals for model training. The model outputs include scene flow, motion segmentation, and ego-motion estimation.4. Experiments demonstrate state-of-the-art performance on a public dataset for radar scene flow estimation. The approach also shows benefits for downstream tasks like motion segmentation and odometry. In summary, the main novelty is in exploiting cross-modal redundancy on autonomous vehicles for self-supervised 4D radar scene flow learning, eliminating the need for expensive manual annotation. The multi-task architecture and methodology to effectively integrate heterogeneous sensor signals are also key contributions.
