# [Hidden Gems: 4D Radar Scene Flow Learning Using Cross-Modal Supervision](https://arxiv.org/abs/2303.00462)

## What is the central research question or hypothesis that this paper addresses?

Based on my understanding, the central research question this paper tries to address is:How to effectively learn 4D radar scene flow estimation without requiring manual annotations, by exploiting supervision signals from other co-located sensors (e.g. odometer, LiDAR, camera) on autonomous vehicles?The key hypothesis is that by opportunistically retrieving and combining complementary supervision cues from heterogeneous on-vehicle sensors, the radar scene flow estimation model can be trained in a cross-modal supervised manner, without needing costly human labeling efforts.In summary, the paper explores using cross-modal supervision from co-located sensors to enable unsupervised learning of radar scene flow, instead of relying on manual annotations or purely self-supervised techniques. The core research contribution is the proposed method to extract and fuse useful supervision signals across different modalities to guide the training process.
