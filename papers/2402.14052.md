# [On Leveraging Encoder-only Pre-trained Language Models for Effective   Keyphrase Generation](https://arxiv.org/abs/2402.14052)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem Definition:
- The availability of domain-specific encoder-only PLMs (e.g. SciBERT) is higher compared to encoder-decoder PLMs. However, most prior work on keyphrase generation (KPG) relies on encoder-decoder models like BART. This presents a dilemma for deploying KPG systems.  
- The paper investigates whether encoder-only PLMs can achieve competitive performance on KPG compared to specialized encoder-decoder PLMs and non-PLM models. 

Methods Explored:
- The authors explore two formulations to enable encoder-only PLMs to perform KPG:
  1) Prefix-LM: Fine-tuning a BERT-style model using attention masks that allow the model to attend leftward like a seq2seq model.
  2) BERT2BERT: Using a BERT encoder and BERT decoder, adding cross-attention.
  
- For both formulations, in-domain variations of BERT (e.g. SciBERT) are studied.
  
- The methods are compared to encoder-decoder PLMs like BART and strong non-PLM baselines.

Main Findings:  
- Encoder-only PLMs can perform comparably or sometimes better than BART on KPG, especially with in-domain models and in low-resource scenarios.
- The Prefix-LM approach is quite data-efficient. With only 5k examples, SciBERT-G matches the performance of a SetTransformer model trained on 100k examples.
- For the BERT2BERT setup, model depth should be prioritized over width given a parameter budget. A deep encoder with shallow decoder performs the best.

In summary, the work demonstrates the promise of using readily available encoder-only PLMs for building effective and efficient KPG systems, especially in specialized domains where encoder-decoder model pre-training may be difficult. The insights pave the way for developing better KPG methods utilizing diverse model architectures.
