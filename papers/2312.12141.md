# [Exploring the Residual Stream of Transformers](https://arxiv.org/abs/2312.12141)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- There is limited understanding of how transformer models make next word predictions, including where the key knowledge is stored and how it contributes to the predictions. 

- Specific questions include: where are the important parameters stored, are they in one layer or spread across layers, which modules (attention/FFN) contain them, how to quantify contributions of layers/modules, whether parameters contribute directly or by activating others.

Proposed Solution:
- Analyze the residual connections in transformers, including layer-level and subvalue-level residual streams. 

- Find distribution change caused by direct addition on before-softmax (bs) values. Tokens with higher bs-values in added vector will increase in probability.

- Prove log probability increase is a reasonable contribution score to locate helpful layers/subvalues.

- Compute inner products between vectors to see which layers activate FFN subvalues.

- Conduct analysis on random cases and a case study.

Main Contributions:

1) Explain distribution change via bs-values, show probabilities increase for tokens with higher bs-values after vector addition.

2) Log probability increase as contribution score to identify important layers/subvalues. Show scores have linear relationship.  

3) Method to evaluate how previous layers affect FFN subvalues.

4) Experimental results and case study verify ability to increase interpretability of transformers and locate knowledge storage.

In summary, the paper explores transformers, especially the residual connections, using mathematical analysis and experiments. It makes several contributions toward explaining and interpreting predictions in transformer models.
