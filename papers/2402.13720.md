# [Ouroboros: Speculative Decoding with Large Model Enhanced Drafting](https://arxiv.org/abs/2402.13720)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Autoregressive decoding of large language models (LLMs) is very slow for inference. 
- Existing drafting-then-verifying methods have two key limitations: (1) Insufficient draft length, as longer drafts have high error costs but more potential for acceleration. (2) Underutilization of verification results, as only the verified prefix is used while subsequent tokens are discarded.

Proposed Solution: 
- The paper proposes Ouroboros, a new drafting-then-verifying decoding method.
- It uses a smaller model to generate drafts, and the target LLM to verify drafts and provide candidate inspirations to enhance future drafts. 
- Several key ideas:
   - Generate drafts at phrase level from a shared candidate pool to allow longer drafts.
   - Construct multiple draft candidates by concatenating 1 draft prefix and multiple suffix candidates.
   - Parallel verify multiple candidates in one pass using custom attention masking.
   - Use both verified and discarded verification tokens to generate new candidate inspirations and refinements.

Main Contributions:
- Proposes the first decoding method that uses the target LLM to enhance the drafting phase via candidate inspirations and refinements.
- Achieves up to 1.9x and 2.8x speedup over prior works Lookahead Decoding and Speculative Decoding.
- Completely training-free and lossless in terms of task performance.
- Demonstrates effectiveness on multiple language tasks like code generation, summarization and translation.

In summary, Ouroboros is an efficient drafting-then-verifying decoding method that can accelerate LLM inference significantly without accuracy loss or expensive training. Its key novelty is a bidirectional acceleration strategy - using the target LLM to speed up and improve the draft generation phase.
