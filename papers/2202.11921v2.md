# [Auto-scaling Vision Transformers without Training](https://arxiv.org/abs/2202.11921v2)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question is: 

How can we automate the design and scaling of Vision Transformers (ViTs) in an efficient and principled manner, while also addressing their high training cost?

The key aspects this paper tries to tackle are:

1) Automating the design of ViT architectures rather than relying on manual crafting and ad-hoc choices. The paper proposes using a fast, training-free search approach to discover a promising "seed" ViT topology. 

2) Developing an automated way to scale up the widths and depths of a ViT in a principled manner to generate models of different sizes. The paper grows the seed topology into different model variants using network complexity metrics.

3) Reducing the heavy computational and data cost of training ViTs. The paper proposes a progressive re-tokenization strategy during training to significantly reduce training FLOPs and time. 

Overall, the central research question is how to make ViT architecture design, scaling, and training more automated and efficient while maintaining strong performance. The paper aims to address the current pain points and lack of clear principles around working with ViTs.


## What is the main contribution of this paper?

 Based on my reading of the paper, the main contributions appear to be:

1. The proposal of As-ViT, an auto-scaling framework for Vision Transformers (ViTs) that can automatically discover and scale up ViTs without training. This allows efficient and principled ViT design and scaling.

2. A training-free architecture search approach to discover a promising "seed" ViT topology. This is enabled by studying different complexity metrics for ViTs and finding that expected length distortion has the best correlation with accuracy. 

3. An automated way to scale up the widths and depths of the "seed" ViT topology into different model sizes. The scaling uses network complexity comparisons to balance depth and width increases.

4. A progressive re-tokenization training strategy that makes the ViT tokens elastic. By progressively reducing the token resolution in early training, this allows much more efficient ViT training with significant savings in FLOPs and time while maintaining accuracy.

5. Strong experimental results showing the As-ViT models achieve state-of-the-art ImageNet classification and COCO detection performance. The model design and scaling framework is also highly efficient, requiring only 12 GPU hours.

In summary, the main contribution appears to be the As-ViT framework for automated and efficient ViT design and scaling, enabled by training-free search, progressive re-tokenization for efficient training, and a comprehensive study of complexity metrics for ViTs. The strong empirical results also help demonstrate the effectiveness of this approach.
