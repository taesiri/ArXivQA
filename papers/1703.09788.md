# [Towards Automatic Learning of Procedures from Web Instructional Videos](https://arxiv.org/abs/1703.09788)

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper addresses is: Can the human-consensus structure of a procedure be learned from a large set of long, unconstrained videos (e.g. instructional videos from YouTube) with only visual evidence?The key points are:- Learning the procedure structure and steps from videos, without relying on subtitles/captions or other textual input.- Using only visual information from the videos.- Learning from unconstrained, 'in the wild' videos like those found on YouTube. - Capturing the human consensus on how procedures are structured and segmented.So in summary, the paper is investigating if AI/computer vision models can learn procedural knowledge purely from observing unlabeled video, in a way that reflects human consensus on the structure of procedures. This is posed as a core research question to be addressed.


## What is the main contribution of this paper?

The main contributions of this paper are:1. It introduces the new problem of procedure segmentation, which is to segment a long, unconstrained video into a sequence of category-independent procedure segments that capture the human consensus on the procedure structure. 2. It collects and distributes a large-scale dataset called YouCook2 for studying procedure segmentation in instructional videos. YouCook2 contains 2000 cooking videos totaling 176 hours with temporal boundary annotations and textual descriptions for procedure segments.3. It proposes an end-to-end neural network model called Procedure Segmentation Networks (ProcNets) for automatically segmenting procedures from videos. ProcNets have three main components:- Context-aware video encoding with Bi-LSTM- Procedure segment proposal module- Sequential prediction module that models dependencies between segments4. It benchmarks ProcNets and other competitive baselines on the YouCook2 dataset using standard metrics like Jaccard index and mean IoU. The results show ProcNets significantly outperform the baselines in procedure segmentation.5. It provides ablation studies and analysis showing ProcNets can learn meaningful temporal structure of procedures. The output segments of ProcNets could be used for other downstream tasks like dense video captioning.In summary, the main contribution is introducing and providing a solution to the novel problem of procedure segmentation in long, unconstrained instructional videos. The large-scale dataset, proposed method, experiments and analyses help establish this new problem space.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper introduces a new problem called procedure segmentation, where the goal is to automatically segment a long, unconstrained instructional video into meaningful steps, and proposes a model called Procedure Segmentation Networks (ProcNets) to tackle this problem using a segment proposal module followed by a sequential prediction module that models dependencies between segments.


## How does this paper compare to other research in the same field?

Here are a few key ways this paper on procedure segmentation compares to other related research:- It introduces a new problem of segmenting long, unconstrained instructional videos into procedure segments representing steps to accomplish a task. This is different from prior work on event proposal and action segmentation that focuses on shorter actions/events. - The paper collects a large new dataset (YouCook2) of 2000 cooking videos with temporal boundaries and descriptions of procedure segments. This is much larger and contains more detailed annotations than existing instructional video datasets.- The proposed ProcNets model procedure segmentation without relying on subtitles or knowing the number of steps ahead of time. This removes key assumptions made in prior methods for procedure learning.- ProcNets uses a segment-level LSTM to model dependencies between proposed segments. This differs from frame-level models like vsLSTM and captures procedure structure better.- Experiments show ProcNets significantly outperforms baselines like SCNN and vsLSTM on procedure segmentation metrics. The analysis also demonstrates ProcNets' ability to learn procedure structure.In summary, this paper tackles a new problem setup, collects a large dataset to facilitate research, and proposes a novel recurrent approach for procedure segmentation that advances over prior frame-level and assumption-heavy methods. The problem and model seem well-motivated by limitations of prior research.
