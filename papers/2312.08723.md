# [StemGen: A music generation model that listens](https://arxiv.org/abs/2312.08723)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a summary paragraph of the key points from the paper:

This paper presents StemGen, a novel deep learning model for context-aware music generation. It is trained on datasets of songs separated into musical stems, allowing it to generate new stems that match the texture and musical qualities of a provided context mix of existing stems. The model uses a non-autoregressive transformer architecture with improvements for combining multiple audio channels and sampling methods that encourage musical coherence. It is evaluated on an open-source dataset and a larger proprietary one, with results showing it matches state-of-the-art quality for unconditioned music generation models. Both objective metrics and listening tests confirm the musical alignment between generated stems and context mixes. This work presents a promising approach to iterative music production by having AI models that can listen to existing musical ideas and respond appropriately. The model architecture and sampling methods allow generating stems fitting a provided musical context, enabling creative workflows for assisted music production.


## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Most existing generative models for musical audio are conditioned on abstract information and cannot directly respond to musical context. 
- Models that can listen to context audio and generate appropriate musical responses are rare.
- Goal is to create a model that can listen to a musical context and generate an appropriate musical response stem.

Proposed Solution:
- Formulate the problem as learning the conditional distribution p(target stem | context mix). 
- Propose a non-autoregressive transformer-based model architecture.
- Audio is encoded into a sequence of hierarchical tokens using a pre-trained encoder. 
- Novel way to combine token embeddings from context-mix and target-stem into a single sequence element.
- Use masking during training like a masked language model.
- Two novel improvements to decoding/sampling:
   1) Multi-source classifier-free guidance to align better to conditioning
   2) Causal-bias during iterative decoding to encourage earlier sequence elements to be sampled first
- Train on datasets of stems from musical audio. Construct context/target pairs by mixing subsets of stems.

Experiments:
- Train on Slakh dataset (145 hours) and a proprietary dataset of 500 hours of stems.
- Evaluate using Fr√©chet Audio Distance (FAD), novel Music Information Retrieval Descriptor Distance (MIRDD), and listening tests.
- Ablations validate benefits of proposed sampling improvements.
- Final model reaches audio quality on par with state-of-the-art text-conditioned models.
- Listening tests show model generates musically coherent outputs.

Main Contributions:
- Novel model architecture for conditional musical audio generation
- Techniques to combine token embeddings from multiple audio sources 
- Multi-source classifier-free guidance during sampling
- Causal-bias during iterative decoding
- MIR-based evaluation metric (MIRDD)

In summary, the paper presents a conditional transformer-based model for musical audio generation that can listen to context and generate a musically appropriate response. Key innovations enable better conditioning on musical audio context during sampling. Both objective and subjective evaluations demonstrate strong audio quality and musical coherence.
