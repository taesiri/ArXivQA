# [From Ultra-Fine to Fine: Fine-tuning Ultra-Fine Entity Typing Models to   Fine-grained](https://arxiv.org/abs/2312.06188)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a summary paragraph of the key points from the paper:

This paper proposes a new approach for fine-grained entity typing (FET) that avoids the need to create large, distantly labeled training sets for each new FET task. The key idea is to first train an ultra-fine entity typing (UFET) model on a broad UFET dataset covering over 10,000 entity types. This UFET model is then fine-tuned on a small set of human-labeled examples to adapt it to a target FET task and schema. To enable effective transfer between UFET and FET, they propose an entity typing model that represents both UFET and FET type labels as textual tokens so all model parameters can be reused during fine-tuning. Experiments show this approach achieves state-of-the-art FET performance under a challenging few-shot setting on multiple datasets. Moreover, fine-tuning with just hundreds of human-labeled FET examples outperforms prior weak supervision methods trained on nearly a million distantly labeled examples. The proposed technique avoids the need for mass weak supervision data creation per FET task, instead relying primarily on a broad UFET dataset combined with small sets of human FET labels.
