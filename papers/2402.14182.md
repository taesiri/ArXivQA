# [Do Machines and Humans Focus on Similar Code? Exploring Explainability   of Large Language Models in Code Summarization](https://arxiv.org/abs/2402.14182)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

The paper investigates whether large language models (LLMs) focus on similar parts of code as human programmers when generating summaries of source code. Specifically, the authors measure where humans focus when summarizing code using eye-tracking metrics like fixation counts and duration. They then approximate where LLMs focus using SHAP, a perturbation-based method that treats models like black boxes. 

The authors compare human and LLM focus patterns across 6 LLMs - GPT3.5, GPT4, StarCoder, CodeLLama, NCS, and GPT3.5 fine-tuned with human demonstrations (GPT-few). They find no statistically significant evidence that LLM focus aligns with human focus. The correlation between human and LLM focus vectors is low, with significant correlation for at most 22% of methods. 

Additionally, the authors find no significant correlation between LLM-human focus alignment and the quality of LLM-generated summaries. This suggests factors other than focus patterns likely dictate summarization performance.

Overall, the lack of alignment between SHAP-based LLM focus and human focus patterns implies either:
1) SHAP may not effectively measure LLM focus for code summarization 
2) Humans and LLMs fundamentally differ in how they reason about and summarize code

The paper concludes that more research is needed to interpret LLMs for code summarization. Different methods may better capture LLM focus, or white-box approaches that access model internals could provide more insight. Understanding differences in human vs. LLM comprehension also warrants further study.
