# [A Sober Look at LLMs for Material Discovery: Are They Actually Good for   Bayesian Optimization Over Molecules?](https://arxiv.org/abs/2402.05015)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem Statement
This paper studies whether large language models (LLMs) can be useful to accelerate Bayesian optimization (BO) over molecules for materials discovery. Recently, LLMs like GPT-3 and LLAMA have shown impressive capability in answering natural language questions about chemistry. However, existing works have only used LLMs in a heuristic, non-Bayesian manner for BO over molecules. The goal of this paper is to take a sober look at whether LLMs can actually improve principled Bayesian optimization for molecular discovery.

Proposed Solution 
The authors systematically compare two ways of using LLMs for BO:

1. As fixed feature extractors: The pretrained LLM embeddings are fed into standard Bayesian surrogate models like Gaussian processes and Bayesian neural networks. This evaluates the out-of-the-box usefulness of LLM features.

2. As adaptive models via parameter-efficient finetuning: The LLM is finetuned on previous observations in a parameter-efficient way at each BO iteration. Bayesian inference via the Laplace approximation is performed to obtain uncertainty estimates. This tests whether the LLM can be adapted to the molecular discovery problem at hand.

The efficacy of general-purpose vs domain-specific LLMs and different prompting strategies are extensively tested on several real-world chemistry datasets.

Main Contributions
- Show that out-of-the-box, general LLMs provide weak inductive bias for BO over molecules, while a chemistry-focused LLM provides useful features.
- Demonstrate that adaptive LLMs via parameter-efficient finetuning and Bayesian inference outperform fixed LLM features.
- Develop probabilistic framework to formulate finetuning as approximate Bayesian inference.
- Extensive experiments on multiple real-world molecular discovery problems to systematically test different methods.
- Open source library for running principled Bayesian optimization with LLMs over molecules.

Overall, the paper provides a sober look backed by rigorous experimentation regarding the usefulness of LLMs for Bayesian optimization over molecules. The main finding is that LLMs can improve BO for molecular discovery, but only if domain-specific and properly adapted via finetuning and Bayesian learning.


## Summarize the paper in one sentence.

 Here is a one-sentence summary:

The paper takes a sober look at using large language models for Bayesian optimization over molecules, finding they can be useful if domain-specific and combined with principled Bayesian surrogates, but general models fail to provide gains over simpler baselines.


## What is the main contribution of this paper?

 This paper's main contribution is carefully studying whether large language models (LLMs) are actually useful to accelerate principled Bayesian optimization in molecular space. The authors take a sober, dispassionate stance in answering this question by:

(i) Viewing LLMs as fixed feature extractors for standard but principled Bayesian optimization (BO) surrogate models. 

(ii) Leveraging parameter-efficient finetuning methods and Bayesian neural networks to obtain the posterior distribution of the LLM surrogate.

Through extensive experiments on real-world chemistry problems, the paper shows that LLMs can be useful for BO over molecules, but only if they have been pretrained or finetuned with domain-specific data. The paper also provides software to enable principled BO on discrete spaces with LLMs.


## What are the keywords or key terms associated with this paper?

 Based on my review of the paper, some of the key terms and concepts associated with it include:

- Bayesian optimization (BO)
- Molecular discovery
- Large language models (LLMs) 
- Uncertainty quantification
- Parameter-efficient finetuning (PEFT)
- Laplace approximation
- Exploration-exploitation tradeoff
- Chemistry-specific vs general-purpose LLMs
- Discrete optimization
- Surrogate models
- In-context learning

The paper studies whether LLMs are useful for accelerating Bayesian optimization over molecules for materials discovery. It compares using LLMs as fixed feature extractors versus adaptively finetuning them as surrogate models. It also contrasts general LLMs with chemistry-specific ones. Overall, it provides an empirical analysis of whether and how LLMs can effectively quantify uncertainty for driving exploration-exploitation tradeoffs in this domain.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in this paper:

1. The paper studies both general-purpose and domain-specific LLMs for BO over molecules. What are the key differences in the inductive biases of these two types of models that lead to different results? How can we better characterize what makes a "chemistry-focused" LLM?

2. The authors use several types of prompting strategies. What is the interplay between pretraining objectives, finetuning strategies, and prompting approaches? Under what conditions can prompting make up for deficiencies in the pretraining or finetuning?

3. The authors frame PEFT methods like LoRA in a Bayesian framework by treating the original weights as conditioning variables. What are the implications of this view? How can it be extended, for example to jointly optimize some subset of the original weights to maximize the marginal likelihood?  

4. For the finetuning experiments, what is the sensitivity of the results to hyperparameters like learning rate, LoRA rank, etc? Are there simple heuristics to set these effectively across tasks? Or is per-task tuning needed?

5. The computational bottleneck is shown to be the forward passes over the candidate set rather than finetuning. Can incremental computation strategies like influence functions help amortize these costs? How do such approximate prediction strategies interact with uncertainty quantification?

6. How sensitive are the conclusions to architectural choices like heads, bottlenecks, LoRA parameterization? What are the most important factors influencing predictive uncertainty for PEFT methods?

7. The evaluation is on a set of fixed candidate molecules from existing benchmarks. How do the conclusions change if applied to on-the-fly generation or filtering of candidates?

8. For surrogate model selection, how was the Laplace approximation determined to be preferable to other BNN methods? Was there a systematic comparison to sampling-based approaches? 

9. What are the most promising directions for scaling predictive uncertainty quantification to much larger LLMs, while retaining computational tractability?

10. How far are we from fully Bayesian treatment of neural architecture search, pretraining objectives, prompting strategies - to truly optimize all components of the LLM-assisted BO pipeline in a principled way? What are the major barriers to realizing this automation?
