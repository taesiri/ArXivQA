# [Galactic: Scaling End-to-End Reinforcement Learning for Rearrangement at   100k Steps-Per-Second](https://arxiv.org/abs/2306.07552)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central research question this paper aims to address is:Can large-scale reinforcement learning training in simulation unlock new capabilities and performance for embodied AI tasks like robotic rearrangement?The key hypothesis seems to be that by drastically increasing the scale of training (in terms of steps of experience and wall-clock time), the authors can enable end-to-end reinforcement learning to succeed on complex, long-horizon tasks where it previously failed. The paper introduces the Galactic simulator which is optimized to provide very fast experience collection. This enables training robotic rearrangement policies on billions or even trillions of steps, which was infeasible with prior simulators. The authors hypothesize this massive increase in experience scale will allow simple end-to-end RL algorithms to learn effective policies on tasks like mobile pick and place and full object rearrangement.


## What is the main contribution of this paper?

The main contribution of this paper is developing a large-scale simulation and reinforcement learning framework called Galactic for training robotic mobile manipulation policies, such as object rearrangement tasks. Specifically, the key contributions are:- Galactic integrates CPU-based batch physics simulation with GPU-based batch rendering and inference to achieve very high throughput of over 100,000 steps per second for RL training. This is orders of magnitude faster than prior simulators like Habitat 2.0.- They introduce a new approximate kinematic physics simulation optimized for efficient mobile manipulation that uses abstraction and approximation to reduce compute requirements. - They demonstrate training a mobile pick policy 100x faster than in Habitat 2.0, needing only 16 minutes to reach over 80% accuracy.- They use Galactic to perform the largest training run ever for rearrangement, with 5 billion frames of experience. This results in 85% success on a challenging long-horizon task using a monolithic policy, compared to 0% for the same approach in Habitat 2.0.- They show the policies trained in Galactic can transfer reasonably well to Habitat 2.0 despite differences in physics and rendering, demonstrating some sim-to-sim generalization.In summary, the main contribution is developing a fast simulator to unlock unprecedented scales of reinforcement learning for robotic manipulation tasks. This enables training complex mobile manipulation skills not possible with prior simulators.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

The paper presents Galactic, a large-scale simulation and reinforcement learning framework for robotic mobile manipulation tasks. The key idea is to optimize the entire pipeline of rendering, physics simulation, inference, and learning to achieve over 100,000 steps per second for training, enabling training at unprecedented scales of 5 billion steps. This results in efficient mobile manipulation policies composed of simple neural network components, trained end-to-end with reinforcement learning.


## How does this paper compare to other research in the same field?

Here is a summary of how this paper compares to other related work in robotic simulation and reinforcement learning:- This paper presents Galactic, a new simulator for training robotic manipulation skills with reinforcement learning (RL). Other related simulators like Habitat 2.0, AI2-THOR, iGibson, Brax, Isaac Gym focus more on navigation or physics simulation rather than optimizing for RL training throughput. - A key contribution of Galactic is achieving very high throughput for RL training by batching physics simulation, rendering, and policy inference across many environments. This allows it to reach over 100,000 steps per second for RL training, which is 80-100x faster than prior simulators like Habitat 2.0. Other works have looked at distributed RL across simulators or batch simulation, but Galactic uniquely combines both batch physics and rendering.- The approximate kinematic physics model in Galactic is simpler than full rigid body dynamics used in simulators like Habitat 2.0, Brax, or Isaac Gym. The rationale is that complex dynamics may not be needed for many robotic manipulation tasks. The tradeoff is faster simulation at the cost of less accuracy.- Galactic demonstrates training vision-based manipulation policies for 5 billion frames of experience. Most prior work in this area uses 1-2 orders of magnitude less experience. The large scale unlocks training end-to-end policies rather than needing hand-designed hierarchies or controllers.- They demonstrate sim-to-sim transfer of a rearrangement policy from Galactic to Habitat 2.0 by using the Galactic policy to output targets for a dynamic controller. This indicates the Galactic policies may be feasible for transfer to real robots despite the simplified physics model.In summary, Galactic pushes the scale of visual reinforcement learning for robotic manipulation by optimizing for throughput. The batch simulation and simplified physics represent a different set of tradeoffs compared to prior simulators that optimize for accuracy or dynamics. The large-scale experiments enabled by Galactic's speed demonstrate the potential of end-to-end visuomotor reinforcement learning.


## What future research directions do the authors suggest?

Based on my reading of the paper, here are some of the future research directions suggested by the authors:- Developing more efficient batch simulators and training frameworks to enable even larger-scale experiments and training with billions or trillions of environment steps. The authors note there is still room for improvement in throughput over their Galactic system.- Exploring transfer and generalization of the policies learned in simulation to real-world robotic systems. The authors demonstrate sim-to-sim transfer between Galactic and Habitat 2.0, but transferring to real robots remains an open challenge. - Experimenting with different model architectures like transformers instead of CNN+LSTM models. The authors use a standard CNN+LSTM architecture in this work but suggest exploring more recent architectural advances.- Training agents end-to-end for full long-horizon tasks like rearrangement instead of relying on modular hierarchical designs. The authors show end-to-end training becomes viable with large-scale experience.- Developing frameworks to leverage large sets of unlabeled offline experiences in addition to online reinforcement learning. The authors note offline RL could provide additional improvements.- Exploring how to best leverage simulation randomization and domain randomization techniques to improve sim-to-real transfer of the learned policies.- Investigating ways to increase the diversity and complexity of environments and tasks used for training. The authors train on a single object rearrangement task.- Analyzing emergence of skills and behaviors from end-to-end reinforced agents at scale. The authors provide some analysis but suggest more investigation.In summary, the main future directions are developing more efficient and scalable systems, transferring the policies to real robots, exploring advanced training techniques like offline RL, improving sim-to-real transfer, increasing task complexity, and analyzing emergent behaviors. The key emphasis is on pushing embodied AI to even larger scales.
