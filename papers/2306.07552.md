# [Galactic: Scaling End-to-End Reinforcement Learning for Rearrangement at   100k Steps-Per-Second](https://arxiv.org/abs/2306.07552)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central research question this paper aims to address is:Can large-scale reinforcement learning training in simulation unlock new capabilities and performance for embodied AI tasks like robotic rearrangement?The key hypothesis seems to be that by drastically increasing the scale of training (in terms of steps of experience and wall-clock time), the authors can enable end-to-end reinforcement learning to succeed on complex, long-horizon tasks where it previously failed. The paper introduces the Galactic simulator which is optimized to provide very fast experience collection. This enables training robotic rearrangement policies on billions or even trillions of steps, which was infeasible with prior simulators. The authors hypothesize this massive increase in experience scale will allow simple end-to-end RL algorithms to learn effective policies on tasks like mobile pick and place and full object rearrangement.


## What is the main contribution of this paper?

The main contribution of this paper is developing a large-scale simulation and reinforcement learning framework called Galactic for training robotic mobile manipulation policies, such as object rearrangement tasks. Specifically, the key contributions are:- Galactic integrates CPU-based batch physics simulation with GPU-based batch rendering and inference to achieve very high throughput of over 100,000 steps per second for RL training. This is orders of magnitude faster than prior simulators like Habitat 2.0.- They introduce a new approximate kinematic physics simulation optimized for efficient mobile manipulation that uses abstraction and approximation to reduce compute requirements. - They demonstrate training a mobile pick policy 100x faster than in Habitat 2.0, needing only 16 minutes to reach over 80% accuracy.- They use Galactic to perform the largest training run ever for rearrangement, with 5 billion frames of experience. This results in 85% success on a challenging long-horizon task using a monolithic policy, compared to 0% for the same approach in Habitat 2.0.- They show the policies trained in Galactic can transfer reasonably well to Habitat 2.0 despite differences in physics and rendering, demonstrating some sim-to-sim generalization.In summary, the main contribution is developing a fast simulator to unlock unprecedented scales of reinforcement learning for robotic manipulation tasks. This enables training complex mobile manipulation skills not possible with prior simulators.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

The paper presents Galactic, a large-scale simulation and reinforcement learning framework for robotic mobile manipulation tasks. The key idea is to optimize the entire pipeline of rendering, physics simulation, inference, and learning to achieve over 100,000 steps per second for training, enabling training at unprecedented scales of 5 billion steps. This results in efficient mobile manipulation policies composed of simple neural network components, trained end-to-end with reinforcement learning.


## How does this paper compare to other research in the same field?

Here is a summary of how this paper compares to other related work in robotic simulation and reinforcement learning:- This paper presents Galactic, a new simulator for training robotic manipulation skills with reinforcement learning (RL). Other related simulators like Habitat 2.0, AI2-THOR, iGibson, Brax, Isaac Gym focus more on navigation or physics simulation rather than optimizing for RL training throughput. - A key contribution of Galactic is achieving very high throughput for RL training by batching physics simulation, rendering, and policy inference across many environments. This allows it to reach over 100,000 steps per second for RL training, which is 80-100x faster than prior simulators like Habitat 2.0. Other works have looked at distributed RL across simulators or batch simulation, but Galactic uniquely combines both batch physics and rendering.- The approximate kinematic physics model in Galactic is simpler than full rigid body dynamics used in simulators like Habitat 2.0, Brax, or Isaac Gym. The rationale is that complex dynamics may not be needed for many robotic manipulation tasks. The tradeoff is faster simulation at the cost of less accuracy.- Galactic demonstrates training vision-based manipulation policies for 5 billion frames of experience. Most prior work in this area uses 1-2 orders of magnitude less experience. The large scale unlocks training end-to-end policies rather than needing hand-designed hierarchies or controllers.- They demonstrate sim-to-sim transfer of a rearrangement policy from Galactic to Habitat 2.0 by using the Galactic policy to output targets for a dynamic controller. This indicates the Galactic policies may be feasible for transfer to real robots despite the simplified physics model.In summary, Galactic pushes the scale of visual reinforcement learning for robotic manipulation by optimizing for throughput. The batch simulation and simplified physics represent a different set of tradeoffs compared to prior simulators that optimize for accuracy or dynamics. The large-scale experiments enabled by Galactic's speed demonstrate the potential of end-to-end visuomotor reinforcement learning.


## What future research directions do the authors suggest?

Based on my reading of the paper, here are some of the future research directions suggested by the authors:- Developing more efficient batch simulators and training frameworks to enable even larger-scale experiments and training with billions or trillions of environment steps. The authors note there is still room for improvement in throughput over their Galactic system.- Exploring transfer and generalization of the policies learned in simulation to real-world robotic systems. The authors demonstrate sim-to-sim transfer between Galactic and Habitat 2.0, but transferring to real robots remains an open challenge. - Experimenting with different model architectures like transformers instead of CNN+LSTM models. The authors use a standard CNN+LSTM architecture in this work but suggest exploring more recent architectural advances.- Training agents end-to-end for full long-horizon tasks like rearrangement instead of relying on modular hierarchical designs. The authors show end-to-end training becomes viable with large-scale experience.- Developing frameworks to leverage large sets of unlabeled offline experiences in addition to online reinforcement learning. The authors note offline RL could provide additional improvements.- Exploring how to best leverage simulation randomization and domain randomization techniques to improve sim-to-real transfer of the learned policies.- Investigating ways to increase the diversity and complexity of environments and tasks used for training. The authors train on a single object rearrangement task.- Analyzing emergence of skills and behaviors from end-to-end reinforced agents at scale. The authors provide some analysis but suggest more investigation.In summary, the main future directions are developing more efficient and scalable systems, transferring the policies to real robots, exploring advanced training techniques like offline RL, improving sim-to-real transfer, increasing task complexity, and analyzing emergent behaviors. The key emphasis is on pushing embodied AI to even larger scales.


## Summarize the paper in one paragraph.

The paper presents Galactic, a large-scale simulation and reinforcement learning framework for robotic mobile manipulation in indoor environments. Specifically, it simulates a Fetch robot equipped with a mobile base, 7-DoF arm, RGBD camera, egomotion, and onboard sensing that must rearrange objects in ReplicaCAD home environments by navigating to objects, picking them up, navigating to target locations, and placing them. The key innovations are integrating CPU-based batch physics with GPU-based batch rendering and inference, and a new approximate kinematic simulation optimized for rearrangement tasks. This allows Galactic to achieve over 100,000 steps per second for full RL training, which is 88x faster than prior work Habitat 2.0. The speed of Galactic enables large-scale rearrangement experiments not possible before. It trains a mobile pick skill over 80% accuracy in under 16 minutes, 100x faster than in Habitat 2.0. It also trains an end-to-end policy to 85% rearrangement accuracy using 5 billion frames of experience in 46 hours. This policy transfers reasonably to Habitat 2.0 despite differences in rendering, physics, and control. Overall, Galactic demonstrates the value of large-scale simulation and reinforcement learning for embodied AI.


## Summarize the paper in two paragraphs.

Here is a two paragraph summary of the key points from the paper:This paper proposes Galactic, a high-speed simulation and reinforcement learning framework for robotics and embodied AI tasks. The key innovation is integrating CPU-based batch physics with GPU-based batch rendering and inference to achieve over 100,000 steps per second during RL training. This is made possible through approximations like kinematic simulation and optimized collision detection. The high simulation speed enables training robotic manipulation skills with billions of frames of experience. For example, a mobile pick skill can be trained to over 80% accuracy in just 16 minutes, 100x faster than prior work. The system is applied to a complex, long-horizon rearrangement task which involves navigating to an object, grasping it, navigating to a goal, and placing the object. Training an end-to-end policy with 5 billion steps results in 85% success on this task. Prior work reported 0% success on the same task with similar experience. The speed of Galactic enables training at unprecedented scales which unlocks new capabilities for end-to-end learning of embodied skills.


## Summarize the main method used in the paper in one paragraph.

Here is a one paragraph summary of the main method used in the paper:The paper proposes Galactic, a large-scale simulation and reinforcement learning framework for training robotic mobile manipulation policies in indoor environments. Galactic integrates CPU-based batch physics simulation with GPU-based batch rendering and inference to achieve very high throughput, over 100,000 steps per second. It uses an approximate kinematic simulation approach that is customized for rearrangement tasks, avoiding the need for full rigid body dynamics. The method trains end-to-end policies with reinforcement learning at unprecedented scales, demonstrating training for up to 5 billion steps. This allows training neural policies composed of standard components like CNNs and LSTMs to accomplish long-horizon tasks like object rearrangement that require navigation, grasping and placing. The massive scale unlocks emergent intelligent behaviors like avoiding distractor objects and synchronizing base and arm movement that are not seen with smaller-scale training.
