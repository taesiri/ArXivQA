# [Galactic: Scaling End-to-End Reinforcement Learning for Rearrangement at   100k Steps-Per-Second](https://arxiv.org/abs/2306.07552)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central research question this paper aims to address is:Can large-scale reinforcement learning training in simulation unlock new capabilities and performance for embodied AI tasks like robotic rearrangement?The key hypothesis seems to be that by drastically increasing the scale of training (in terms of steps of experience and wall-clock time), the authors can enable end-to-end reinforcement learning to succeed on complex, long-horizon tasks where it previously failed. The paper introduces the Galactic simulator which is optimized to provide very fast experience collection. This enables training robotic rearrangement policies on billions or even trillions of steps, which was infeasible with prior simulators. The authors hypothesize this massive increase in experience scale will allow simple end-to-end RL algorithms to learn effective policies on tasks like mobile pick and place and full object rearrangement.
