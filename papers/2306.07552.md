# [Galactic: Scaling End-to-End Reinforcement Learning for Rearrangement at   100k Steps-Per-Second](https://arxiv.org/abs/2306.07552)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central research question this paper aims to address is:Can large-scale reinforcement learning training in simulation unlock new capabilities and performance for embodied AI tasks like robotic rearrangement?The key hypothesis seems to be that by drastically increasing the scale of training (in terms of steps of experience and wall-clock time), the authors can enable end-to-end reinforcement learning to succeed on complex, long-horizon tasks where it previously failed. The paper introduces the Galactic simulator which is optimized to provide very fast experience collection. This enables training robotic rearrangement policies on billions or even trillions of steps, which was infeasible with prior simulators. The authors hypothesize this massive increase in experience scale will allow simple end-to-end RL algorithms to learn effective policies on tasks like mobile pick and place and full object rearrangement.


## What is the main contribution of this paper?

The main contribution of this paper is developing a large-scale simulation and reinforcement learning framework called Galactic for training robotic mobile manipulation policies, such as object rearrangement tasks. Specifically, the key contributions are:- Galactic integrates CPU-based batch physics simulation with GPU-based batch rendering and inference to achieve very high throughput of over 100,000 steps per second for RL training. This is orders of magnitude faster than prior simulators like Habitat 2.0.- They introduce a new approximate kinematic physics simulation optimized for efficient mobile manipulation that uses abstraction and approximation to reduce compute requirements. - They demonstrate training a mobile pick policy 100x faster than in Habitat 2.0, needing only 16 minutes to reach over 80% accuracy.- They use Galactic to perform the largest training run ever for rearrangement, with 5 billion frames of experience. This results in 85% success on a challenging long-horizon task using a monolithic policy, compared to 0% for the same approach in Habitat 2.0.- They show the policies trained in Galactic can transfer reasonably well to Habitat 2.0 despite differences in physics and rendering, demonstrating some sim-to-sim generalization.In summary, the main contribution is developing a fast simulator to unlock unprecedented scales of reinforcement learning for robotic manipulation tasks. This enables training complex mobile manipulation skills not possible with prior simulators.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

The paper presents Galactic, a large-scale simulation and reinforcement learning framework for robotic mobile manipulation tasks. The key idea is to optimize the entire pipeline of rendering, physics simulation, inference, and learning to achieve over 100,000 steps per second for training, enabling training at unprecedented scales of 5 billion steps. This results in efficient mobile manipulation policies composed of simple neural network components, trained end-to-end with reinforcement learning.
