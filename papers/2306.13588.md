# [System-Level Natural Language Feedback](https://arxiv.org/abs/2306.13588)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question seems to be:

Can we aggregate instance-level natural language feedback to make system-level design decisions that improve language generation systems?

The paper proposes a framework to derive a small set of criteria (i.e. system-level feedback) from a large set of instance-level natural language feedback. This system-level feedback is then used to design language model prompts to refine model responses across examples, as well as define new metrics to measure improvements aligned with user needs. 

The paper demonstrates this framework through two case studies of improving the query generator and response generator of an information-seeking dialog system. The results suggest that system-level feedback alone, as well as combining it with instance-level feedback, can lead to measurable improvements in dialog systems based on the designed metrics.

In summary, the central hypothesis is that aggregating instance-level feedback to make high-level system design decisions is an effective way to improve language generation systems. The case studies provide evidence to support this hypothesis.


## What is the main contribution of this paper?

 The main contribution of this paper is proposing a general framework for using natural language feedback to improve dialog systems at the system level, rather than just using feedback on a per-instance basis. The key ideas are:

1. Deriving a small set of criteria or system-level feedback from a large set of per-instance natural language feedback through clustering and human annotation. 

2. Using the system-level criteria for two purposes:
   - Designing instruction-following language model prompts to refine poor model outputs.
   - Defining new metrics aligned with user needs to evaluate system improvements.

3. Conducting case studies on improving query generation and response generation of an information-seeking dialog system. Results show system-level feedback alone brings gains, and combining it with per-instance feedback brings further improvements. 

4. Comparing human vs GPT-3.5 generated per-instance feedback, finding the former leads to more grounded refinements that are easier for systems to learn from.

In summary, the key contribution is developing methods to extract systematic improvements from free-form natural language feedback, through a combination of automatic clustering, human annotation, and language model prompting. The paper demonstrates the efficacy of this approach via quantitative experiments and analysis.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the key points from the paper:

The paper proposes a framework to aggregate instance-level natural language feedback from users to derive system-level criteria for improving dialog systems, and demonstrates this through case studies on query generation and response generation.


## How does this paper compare to other research in the same field?

 Here are a few key ways this paper compares to other research in using natural language feedback to improve dialog systems:

- Focus on system-level improvements: This paper proposes aggregating instance-level feedback to derive system-level criteria for improving dialog systems. Much prior work has focused just on using feedback to refine specific examples. This paper takes a more holistic view.

- General framework: The paper outlines a general framework for using natural language feedback at the system level, through prompt engineering and metric design. This provides a methodology beyond individual techniques. 

- Multiple tasks: The framework is demonstrated on two important dialog tasks - query generation and response generation. Showing benefits in multiple scenarios strengthens the utility of the approach.

- Combining feedback types: The paper investigates combining system-level and instance-level feedback, finding they are complementary. Most prior work looked at feedback types in isolation.

- Human vs model feedback: Comparing human-written and GPT-3.5-generated feedback reveals important differences in grounding. This analysis is novel and highlights the continued importance of human feedback.

Overall, this paper pushes forward research in utilizing natural language feedback for dialog systems. The focus on system-level improvements, general methodology, and insights from feedback analysis help advance the field. The proposed framework and findings could guide future work on designing more human-aligned dialog systems.


## What future research directions do the authors suggest?

 The authors suggest the following future research directions:

- Exploring different methods for deriving criteria from instance-level feedback. The current approach uses simple clustering and manual summarization, but more advanced methods like topic modeling could be investigated. 

- Applying the framework to other text generation tasks beyond dialog systems, such as summarization, translation, etc. The general methodology should be applicable to other NLG settings.

- Investigating different methods for incorporating criteria into model training, beyond prompt-based refinement. For example, the criteria could be used to define auxiliary prediction tasks.

- Comparing human feedback to other automated feedback generation methods besides GPT-3.5, and understanding how to make automated feedback more human-like.

- Combining feedback elicitation methods like critiquing, clarification questions, etc. within the framework to get richer and more targeted feedback.

- Scaling up the experiments to larger datasets and model sizes to better understand the impact and generalization ability.

- Exploring personalization - how system-level feedback could be tailored to individual users' preferences over time.

- Studying the sample efficiency of feedback - how much is needed to make improvements.

- Evaluating the framework in a real user study, beyond offline metrics.

In summary, the authors propose many interesting ideas for improving, extending and evaluating their framework for utilizing natural language feedback at a system-level to improve text generation models. The human-in-the-loop process seems crucial to deriving meaningful criteria from feedback.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the paper:

The paper proposes a general framework for using natural language feedback to improve dialog systems at the system level, rather than just using feedback to refine individual examples. The key ideas are: (1) Aggregate instance-level feedback (e.g. "the response was not helpful") into clusters and derive a small set of criteria for what constitutes a good system (e.g. responses should be grounded in evidence). (2) Use the criteria to design prompts to refine unsatisfactory model responses. (3) Use the criteria as a basis to design new evaluation metrics aligned with user needs. (4) Show in two case studies that using the framework to incorporate feedback into prompt engineering and metric design improves both a query generator and a response generator for an information-seeking dialog system. (5) Demonstrate combining system-level and instance-level feedback provides further gains, with human feedback being more effective than model-generated feedback. Overall, the work provides a general methodology for unlocking the system-level rather than just instance-level value of natural language feedback.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the paper:

The paper proposes a general framework for utilizing natural language feedback to improve dialog systems at a system level, rather than just using feedback to refine individual examples. The key idea is to aggregate a large set of per-instance natural language feedback to derive a small set of criteria that capture common issues and user needs. These criteria are then used in two ways: 1) To design prompts for a language model to refine unsatisfactory model responses across examples and 2) To define new metrics aligned with the criteria to evaluate model improvements. 

The authors demonstrate this framework through two case studies of improving an information-seeking dialog system, focusing on the query generator and response generator modules. The derived criteria from user feedback are used to design prompts to refine queries and responses via a large language model. The refined data is then used to fine-tune the models. Additionally, new metrics are designed based on the criteria to evaluate the improved systems. Experiments show the effectiveness of using system-level feedback, especially when combined with instance-level feedback. The results highlight the importance of human feedback, as refinements guided by human feedback are more grounded and lead to better end-task performance compared to using model-generated feedback.


## Summarize the main method used in the paper in one paragraph.

 Here is a one paragraph summary of the main method used in the paper:

The paper proposes a general framework for using natural language feedback to make system-level design decisions to improve language generation systems. The framework involves first collecting a large set of instance-level natural language feedback from users. This feedback is clustered and analyzed by human experts to derive a small set of criteria that characterize good system performance. These criteria are then used in two ways: (1) To design prompts for a language model that can refine poor examples, by informing the model of the criteria good examples should meet. (2) To design new metrics aligned with the criteria that can effectively measure system performance. The framework is demonstrated through two case studies of improving an Internet-augmented dialog system, by using the derived criteria to design prompts to refine poor queries and responses, and metrics to measure query and response quality. The refined examples are used to fine-tune the original system, resulting in improved performance as measured by the new metrics. The paper shows this human-in-the-loop process using system-level feedback enables building systems better aligned with user needs.
