# [System-Level Natural Language Feedback](https://arxiv.org/abs/2306.13588)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be:Can we aggregate instance-level natural language feedback to make system-level design decisions that improve language generation systems?The paper proposes a framework to derive a small set of criteria (i.e. system-level feedback) from a large set of instance-level natural language feedback. This system-level feedback is then used to design language model prompts to refine model responses across examples, as well as define new metrics to measure improvements aligned with user needs. The paper demonstrates this framework through two case studies of improving the query generator and response generator of an information-seeking dialog system. The results suggest that system-level feedback alone, as well as combining it with instance-level feedback, can lead to measurable improvements in dialog systems based on the designed metrics.In summary, the central hypothesis is that aggregating instance-level feedback to make high-level system design decisions is an effective way to improve language generation systems. The case studies provide evidence to support this hypothesis.


## What is the main contribution of this paper?

The main contribution of this paper is proposing a general framework for using natural language feedback to improve dialog systems at the system level, rather than just using feedback on a per-instance basis. The key ideas are:1. Deriving a small set of criteria or system-level feedback from a large set of per-instance natural language feedback through clustering and human annotation. 2. Using the system-level criteria for two purposes:   - Designing instruction-following language model prompts to refine poor model outputs.   - Defining new metrics aligned with user needs to evaluate system improvements.3. Conducting case studies on improving query generation and response generation of an information-seeking dialog system. Results show system-level feedback alone brings gains, and combining it with per-instance feedback brings further improvements. 4. Comparing human vs GPT-3.5 generated per-instance feedback, finding the former leads to more grounded refinements that are easier for systems to learn from.In summary, the key contribution is developing methods to extract systematic improvements from free-form natural language feedback, through a combination of automatic clustering, human annotation, and language model prompting. The paper demonstrates the efficacy of this approach via quantitative experiments and analysis.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper proposes a framework to aggregate instance-level natural language feedback from users to derive system-level criteria for improving dialog systems, and demonstrates this through case studies on query generation and response generation.
