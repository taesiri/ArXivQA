# [System-Level Natural Language Feedback](https://arxiv.org/abs/2306.13588)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be:Can we aggregate instance-level natural language feedback to make system-level design decisions that improve language generation systems?The paper proposes a framework to derive a small set of criteria (i.e. system-level feedback) from a large set of instance-level natural language feedback. This system-level feedback is then used to design language model prompts to refine model responses across examples, as well as define new metrics to measure improvements aligned with user needs. The paper demonstrates this framework through two case studies of improving the query generator and response generator of an information-seeking dialog system. The results suggest that system-level feedback alone, as well as combining it with instance-level feedback, can lead to measurable improvements in dialog systems based on the designed metrics.In summary, the central hypothesis is that aggregating instance-level feedback to make high-level system design decisions is an effective way to improve language generation systems. The case studies provide evidence to support this hypothesis.


## What is the main contribution of this paper?

The main contribution of this paper is proposing a general framework for using natural language feedback to improve dialog systems at the system level, rather than just using feedback on a per-instance basis. The key ideas are:1. Deriving a small set of criteria or system-level feedback from a large set of per-instance natural language feedback through clustering and human annotation. 2. Using the system-level criteria for two purposes:   - Designing instruction-following language model prompts to refine poor model outputs.   - Defining new metrics aligned with user needs to evaluate system improvements.3. Conducting case studies on improving query generation and response generation of an information-seeking dialog system. Results show system-level feedback alone brings gains, and combining it with per-instance feedback brings further improvements. 4. Comparing human vs GPT-3.5 generated per-instance feedback, finding the former leads to more grounded refinements that are easier for systems to learn from.In summary, the key contribution is developing methods to extract systematic improvements from free-form natural language feedback, through a combination of automatic clustering, human annotation, and language model prompting. The paper demonstrates the efficacy of this approach via quantitative experiments and analysis.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper proposes a framework to aggregate instance-level natural language feedback from users to derive system-level criteria for improving dialog systems, and demonstrates this through case studies on query generation and response generation.


## How does this paper compare to other research in the same field?

Here are a few key ways this paper compares to other research in using natural language feedback to improve dialog systems:- Focus on system-level improvements: This paper proposes aggregating instance-level feedback to derive system-level criteria for improving dialog systems. Much prior work has focused just on using feedback to refine specific examples. This paper takes a more holistic view.- General framework: The paper outlines a general framework for using natural language feedback at the system level, through prompt engineering and metric design. This provides a methodology beyond individual techniques. - Multiple tasks: The framework is demonstrated on two important dialog tasks - query generation and response generation. Showing benefits in multiple scenarios strengthens the utility of the approach.- Combining feedback types: The paper investigates combining system-level and instance-level feedback, finding they are complementary. Most prior work looked at feedback types in isolation.- Human vs model feedback: Comparing human-written and GPT-3.5-generated feedback reveals important differences in grounding. This analysis is novel and highlights the continued importance of human feedback.Overall, this paper pushes forward research in utilizing natural language feedback for dialog systems. The focus on system-level improvements, general methodology, and insights from feedback analysis help advance the field. The proposed framework and findings could guide future work on designing more human-aligned dialog systems.
