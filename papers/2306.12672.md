# [From Word Models to World Models: Translating from Natural Language to   the Probabilistic Language of Thought](https://arxiv.org/abs/2306.12672)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be: How can we develop a computational framework that combines neural models of language with symbolic models of reasoning to achieve human-like language understanding and thinking?Specifically, the paper proposes an approach called "rational meaning construction" which uses probabilistic programming to represent a structured "language of thought", and large language models to translate from natural language into this language of thought. The key ideas seem to be:- Thinking and reasoning can be modeled as Bayesian inference in structured probabilistic programs that represent possible worlds. - Understanding language involves translating utterances into expressions in this "probabilistic language of thought" in a context-sensitive way.- Large language models can be used to implement the translation from natural language to code, amortizing the process of meaning construction.- This framework allows integrating language with core domains of reasoning like probabilistic inference, physical simulation, planning, etc.So in summary, the main hypothesis is that combining neural translation models with structured symbolic reasoning in a "language of thought" can lead to more human-like language understanding and thinking in machines. The paper aims to illustrate this framework across several reasoning domains.
