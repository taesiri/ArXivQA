# [Cross-lingual Retrieval for Iterative Self-Supervised Training](https://arxiv.org/abs/2006.09526)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is: Can iterative mining of pseudo-parallel data from monolingual corpora and training on this mined data lead to improvements in cross-lingual alignment and machine translation ability, without any labeled parallel data?The key hypotheses tested in this paper are:1) The encoder outputs of multilingual masked language models like mBART represent language-agnostic semantic meaning, even without being trained on any parallel data.2) Finetuning a multilingual model on mined pseudo-parallel data improves cross-lingual alignment across all language pairs, even if the mined data is only for one language pair. 3) Iteratively applying mining and multilingual training can allow the model to progressively improve its own cross-lingual alignment and translation ability on unlabeled data alone.The authors test these hypotheses through empirical studies on the encoder representations and retrieval accuracies of mBART models. They then propose the CRISS framework which combines mining and multilingual training iteratively. Experiments demonstrate state-of-the-art results in unsupervised translation and cross-lingual retrieval, supporting the core hypothesis.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contributions are:1. The paper presents empirical results showing that the encoder outputs of a multilingual denoising autoencoder (mBART) represent language-agnostic semantic meaning. 2. The paper shows that finetuning mBART on just one pair of parallel bi-text improves cross-lingual alignment for all language directions.3. The paper introduces a new iterative self-supervised learning method called CRISS (Cross-lingual Retrieval for Iterative Self-Supervised training) that combines mining and multilingual training in an iterative manner to improve both cross-lingual alignment and translation ability. 4. CRISS achieves state-of-the-art results on unsupervised machine translation benchmarks, improving an average of 2.4 BLEU on 9 language directions. It also achieves state-of-the-art results on the Tatoeba sentence retrieval task from the XTREME benchmark, improving an average of 21.5% absolute accuracy on 16 languages.5. CRISS further improves performance on supervised machine translation tasks compared to mBART, giving an additional 1.8 BLEU improvement on average.In summary, the main contribution is an iterative mining and training procedure called CRISS that leverages the cross-lingual alignment abilities of mBART to significantly advance the state-of-the-art in both unsupervised and supervised machine translation as well as cross-lingual sentence retrieval.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

The TL;DR of this paper is: A new self-supervised training approach is proposed that combines mining and multilingual training in an iterative loop to achieve state-of-the-art performance on unsupervised machine translation and cross-lingual sentence retrieval tasks.


## How does this paper compare to other research in the same field?

Here are a few key points comparing this paper to other related work in cross-lingual natural language processing:- It builds on the recent finding that pretrained multilingual language models like mBART exhibit some inherent cross-lingual alignment capabilities without any explicit cross-lingual supervision. However, this paper shows that the alignment can be further improved by finetuning the model with mined pseudo-parallel data.- For mining bitexts, this paper follows the same margin-based scoring approach used in other unsupervised mining methods. However, unlike some prior work that relies on word-level embeddings, this method works directly at the sentence embedding level using mBART encoder outputs.- For pretraining, this paper proposes an iterative mining and training loop to progressively improve cross-lingual alignment. This is a novel way of leveraging mined bitexts compared to prior work that simply pretrains once on mined data. - Unlike most prior unsupervised MT methods that rely on backtranslation, this method can achieve strong results using only retrieved target sentences, without needing backtranslation.- This method achieves new state-of-the-art results on unsupervised machine translation and sentence retrieval tasks, outperforming other unsupervised representation learning methods like MASS and unsupervised MT methods like mBART finetuned with backtranslation.- It also shows supervised MT improvements over mBART, particularly for low-resource languages, demonstrating the cross-lingual alignment benefits of this pretraining approach.In summary, this paper introduces a new iterative mining-training loop for unsupervised cross-lingual representation learning that achieves new SOTA results. The key novelty is continued pretraining on mined bitexts to improve alignment, without needing backtranslation.
