# [Cross-lingual Retrieval for Iterative Self-Supervised Training](https://arxiv.org/abs/2006.09526)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is: Can iterative mining of pseudo-parallel data from monolingual corpora and training on this mined data lead to improvements in cross-lingual alignment and machine translation ability, without any labeled parallel data?The key hypotheses tested in this paper are:1) The encoder outputs of multilingual masked language models like mBART represent language-agnostic semantic meaning, even without being trained on any parallel data.2) Finetuning a multilingual model on mined pseudo-parallel data improves cross-lingual alignment across all language pairs, even if the mined data is only for one language pair. 3) Iteratively applying mining and multilingual training can allow the model to progressively improve its own cross-lingual alignment and translation ability on unlabeled data alone.The authors test these hypotheses through empirical studies on the encoder representations and retrieval accuracies of mBART models. They then propose the CRISS framework which combines mining and multilingual training iteratively. Experiments demonstrate state-of-the-art results in unsupervised translation and cross-lingual retrieval, supporting the core hypothesis.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contributions are:1. The paper presents empirical results showing that the encoder outputs of a multilingual denoising autoencoder (mBART) represent language-agnostic semantic meaning. 2. The paper shows that finetuning mBART on just one pair of parallel bi-text improves cross-lingual alignment for all language directions.3. The paper introduces a new iterative self-supervised learning method called CRISS (Cross-lingual Retrieval for Iterative Self-Supervised training) that combines mining and multilingual training in an iterative manner to improve both cross-lingual alignment and translation ability. 4. CRISS achieves state-of-the-art results on unsupervised machine translation benchmarks, improving an average of 2.4 BLEU on 9 language directions. It also achieves state-of-the-art results on the Tatoeba sentence retrieval task from the XTREME benchmark, improving an average of 21.5% absolute accuracy on 16 languages.5. CRISS further improves performance on supervised machine translation tasks compared to mBART, giving an additional 1.8 BLEU improvement on average.In summary, the main contribution is an iterative mining and training procedure called CRISS that leverages the cross-lingual alignment abilities of mBART to significantly advance the state-of-the-art in both unsupervised and supervised machine translation as well as cross-lingual sentence retrieval.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

The TL;DR of this paper is: A new self-supervised training approach is proposed that combines mining and multilingual training in an iterative loop to achieve state-of-the-art performance on unsupervised machine translation and cross-lingual sentence retrieval tasks.


## How does this paper compare to other research in the same field?

Here are a few key points comparing this paper to other related work in cross-lingual natural language processing:- It builds on the recent finding that pretrained multilingual language models like mBART exhibit some inherent cross-lingual alignment capabilities without any explicit cross-lingual supervision. However, this paper shows that the alignment can be further improved by finetuning the model with mined pseudo-parallel data.- For mining bitexts, this paper follows the same margin-based scoring approach used in other unsupervised mining methods. However, unlike some prior work that relies on word-level embeddings, this method works directly at the sentence embedding level using mBART encoder outputs.- For pretraining, this paper proposes an iterative mining and training loop to progressively improve cross-lingual alignment. This is a novel way of leveraging mined bitexts compared to prior work that simply pretrains once on mined data. - Unlike most prior unsupervised MT methods that rely on backtranslation, this method can achieve strong results using only retrieved target sentences, without needing backtranslation.- This method achieves new state-of-the-art results on unsupervised machine translation and sentence retrieval tasks, outperforming other unsupervised representation learning methods like MASS and unsupervised MT methods like mBART finetuned with backtranslation.- It also shows supervised MT improvements over mBART, particularly for low-resource languages, demonstrating the cross-lingual alignment benefits of this pretraining approach.In summary, this paper introduces a new iterative mining-training loop for unsupervised cross-lingual representation learning that achieves new SOTA results. The key novelty is continued pretraining on mined bitexts to improve alignment, without needing backtranslation.


## What future research directions do the authors suggest?

Based on my reading of the paper, the main future research directions suggested by the authors are:1. Analyze and better understand theoretically how the language agnostic representation emerges from the denoising pretraining. The authors found empirically that cross-lingual alignment emerges from multilingual denoising autoencoder pretraining, but do not provide a theoretical explanation for this phenomenon. Further analysis and research could uncover the theoretical reasons behind this emergent cross-lingual alignment.2. Explore whether the proposed approach can be extended to non-seq2seq models and tasks. The authors developed their method for sequence-to-sequence models and focused evaluation on machine translation tasks. They suggest exploring if the same techniques could be used to pretrain non-seq2seq models in an unsupervised, cross-lingual manner and improve performance on other NLP and even non-NLP tasks.3. Study the learned cross-lingual representations and their potential applications. The authors propose analyzing the properties of the representations learned by their approach and investigating if they could be effectively applied to other downstream tasks beyond just machine translation and sentence retrieval.4. Scale up the approach to more languages and data. The authors used 25 languages but suggest exploring scaling to more languages. They also artificially capped the unlabeled data used and propose experiments with larger monolingual datasets.In summary, the main directions are: 1) theoretical analysis of the emergent cross-lingual alignment, 2) extending the approach to other models and tasks, 3) exploring applications of the learned representations, and 4) scaling up the languages and data used. The authors propose their method as a promising technique for unsupervised cross-lingual learning and suggest several avenues for building on this work.


## Summarize the paper in one paragraph.

Here is a one paragraph summary of the paper:The paper presents a new iterative self-supervised training approach called CRISS (Cross-lingual Retrieval for Iterative Self-Supervised training) for training multilingual sequence-to-sequence models. It leverages the finding that pretrained multilingual models like mBART have emergent cross-lingual alignment capabilities even when trained only on monolingual data. CRISS works by first using the mBART encoder outputs to mine pseudo-parallel sentences from monolingual corpora in different languages. It then trains an mBART model on these mined pseudo-parallel sentences to improve cross-lingual alignment. This mining and training process is repeated iteratively, with each iteration mining better pseudo-parallel data using the improved model, and then training a better model on the new mined data. Without using any supervised parallel data, CRISS achieves state-of-the-art results on unsupervised machine translation benchmarks and cross-lingual sentence retrieval tasks. It also further improves performance on supervised machine translation compared to mBART. The key contributions are introducing an effective iterative self-supervised training procedure for multilingual models, and showing strong empirical results on multiple cross-lingual tasks.


## Summarize the paper in two paragraphs.

Here is a two paragraph summary of the paper:The paper proposes a new self-supervised training approach called Cross-lingual Retrieval for Iterative Self-Supervised Training (CRISS) for improving multilingual machine translation and cross-lingual sentence retrieval. CRISS iteratively combines mining for pseudo-parallel sentences and multilingual model training. It is based on three findings: 1) The encoder outputs of the mBART multilingual denoising autoencoder model represent semantic meaning in a language-agnostic way. 2) Finetuning mBART on just one language pair improves the cross-lingual alignment for all language pairs. 3) The cross-lingual alignment can be further improved by iteratively mining pseudo-parallel data using the model, then retraining the model on this data. CRISS starts with an mBART model pretrained on monolingual data in 25 languages. It then alternates between mining pseudo-parallel data using encoder outputs and margin scoring, and training the mBART model on this mined data. After 3 iterations, CRISS achieves state-of-the-art results on unsupervised machine translation benchmarks in 9 out of 10 directions, with average gains of 2.4 BLEU. It also substantially improves retrieval accuracy on the Tatoeba benchmark, with average gains of 21.5% absolute accuracy across 16 languages compared to previous methods. CRISS also provides further gains of 1.8 BLEU on average when used for initializing supervised machine translation models. The gains demonstrate the effectiveness of iterative self-supervised training for improving cross-lingual representation learning.


## Summarize the main method used in the paper in one paragraph.

The paper proposes a new self-supervised pretraining method called Cross-lingual Retrieval for Iterative Self-Supervised Training (CRISS) for improving multilingual machine translation and sentence retrieval. The main idea is to iteratively mine pseudo-parallel sentence pairs from monolingual data using the encoder outputs of a multilingual denoising autoencoder model as cross-lingual sentence representations, then use the mined pseudo-parallel data to train the model further. Specifically, they first pretrain a mBART denoising autoencoder model on monolingual data in 25 languages. Then they iteratively perform the following two steps:1) Mine pseudo-parallel sentence pairs between languages using the encoder outputs of the current model and a margin-based scoring function. This allows them to extract pseudo-parallel data without any human labeled parallel data. 2) Retrain the mBART model on the aggregated mined pseudo-parallel sentences to get an improved model. They always restart training from the original mBART checkpoint instead of the last iteration's model.By repeating the mining and retraining process multiple times, they are able to iteratively improve the model's ability to mine better pseudo-parallel data, as well as its performance on unsupervised machine translation and cross-lingual sentence retrieval tasks. The mining and training steps provide mutual improvements in an iterative fashion.
