# [Cross-lingual Retrieval for Iterative Self-Supervised Training](https://arxiv.org/abs/2006.09526)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question is: 

Can iterative mining of pseudo-parallel data from monolingual corpora and training on this mined data lead to improvements in cross-lingual alignment and machine translation ability, without any labeled parallel data?

The key hypotheses tested in this paper are:

1) The encoder outputs of multilingual masked language models like mBART represent language-agnostic semantic meaning, even without being trained on any parallel data.

2) Finetuning a multilingual model on mined pseudo-parallel data improves cross-lingual alignment across all language pairs, even if the mined data is only for one language pair. 

3) Iteratively applying mining and multilingual training can allow the model to progressively improve its own cross-lingual alignment and translation ability on unlabeled data alone.

The authors test these hypotheses through empirical studies on the encoder representations and retrieval accuracies of mBART models. They then propose the CRISS framework which combines mining and multilingual training iteratively. Experiments demonstrate state-of-the-art results in unsupervised translation and cross-lingual retrieval, supporting the core hypothesis.


## What is the main contribution of this paper?

 Based on my reading of the paper, the main contributions are:

1. The paper presents empirical results showing that the encoder outputs of a multilingual denoising autoencoder (mBART) represent language-agnostic semantic meaning. 

2. The paper shows that finetuning mBART on just one pair of parallel bi-text improves cross-lingual alignment for all language directions.

3. The paper introduces a new iterative self-supervised learning method called CRISS (Cross-lingual Retrieval for Iterative Self-Supervised training) that combines mining and multilingual training in an iterative manner to improve both cross-lingual alignment and translation ability. 

4. CRISS achieves state-of-the-art results on unsupervised machine translation benchmarks, improving an average of 2.4 BLEU on 9 language directions. It also achieves state-of-the-art results on the Tatoeba sentence retrieval task from the XTREME benchmark, improving an average of 21.5% absolute accuracy on 16 languages.

5. CRISS further improves performance on supervised machine translation tasks compared to mBART, giving an additional 1.8 BLEU improvement on average.

In summary, the main contribution is an iterative mining and training procedure called CRISS that leverages the cross-lingual alignment abilities of mBART to significantly advance the state-of-the-art in both unsupervised and supervised machine translation as well as cross-lingual sentence retrieval.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 The TL;DR of this paper is: A new self-supervised training approach is proposed that combines mining and multilingual training in an iterative loop to achieve state-of-the-art performance on unsupervised machine translation and cross-lingual sentence retrieval tasks.


## How does this paper compare to other research in the same field?

 Here are a few key points comparing this paper to other related work in cross-lingual natural language processing:

- It builds on the recent finding that pretrained multilingual language models like mBART exhibit some inherent cross-lingual alignment capabilities without any explicit cross-lingual supervision. However, this paper shows that the alignment can be further improved by finetuning the model with mined pseudo-parallel data.

- For mining bitexts, this paper follows the same margin-based scoring approach used in other unsupervised mining methods. However, unlike some prior work that relies on word-level embeddings, this method works directly at the sentence embedding level using mBART encoder outputs.

- For pretraining, this paper proposes an iterative mining and training loop to progressively improve cross-lingual alignment. This is a novel way of leveraging mined bitexts compared to prior work that simply pretrains once on mined data. 

- Unlike most prior unsupervised MT methods that rely on backtranslation, this method can achieve strong results using only retrieved target sentences, without needing backtranslation.

- This method achieves new state-of-the-art results on unsupervised machine translation and sentence retrieval tasks, outperforming other unsupervised representation learning methods like MASS and unsupervised MT methods like mBART finetuned with backtranslation.

- It also shows supervised MT improvements over mBART, particularly for low-resource languages, demonstrating the cross-lingual alignment benefits of this pretraining approach.

In summary, this paper introduces a new iterative mining-training loop for unsupervised cross-lingual representation learning that achieves new SOTA results. The key novelty is continued pretraining on mined bitexts to improve alignment, without needing backtranslation.


## What future research directions do the authors suggest?

 Based on my reading of the paper, the main future research directions suggested by the authors are:

1. Analyze and better understand theoretically how the language agnostic representation emerges from the denoising pretraining. The authors found empirically that cross-lingual alignment emerges from multilingual denoising autoencoder pretraining, but do not provide a theoretical explanation for this phenomenon. Further analysis and research could uncover the theoretical reasons behind this emergent cross-lingual alignment.

2. Explore whether the proposed approach can be extended to non-seq2seq models and tasks. The authors developed their method for sequence-to-sequence models and focused evaluation on machine translation tasks. They suggest exploring if the same techniques could be used to pretrain non-seq2seq models in an unsupervised, cross-lingual manner and improve performance on other NLP and even non-NLP tasks.

3. Study the learned cross-lingual representations and their potential applications. The authors propose analyzing the properties of the representations learned by their approach and investigating if they could be effectively applied to other downstream tasks beyond just machine translation and sentence retrieval.

4. Scale up the approach to more languages and data. The authors used 25 languages but suggest exploring scaling to more languages. They also artificially capped the unlabeled data used and propose experiments with larger monolingual datasets.

In summary, the main directions are: 1) theoretical analysis of the emergent cross-lingual alignment, 2) extending the approach to other models and tasks, 3) exploring applications of the learned representations, and 4) scaling up the languages and data used. The authors propose their method as a promising technique for unsupervised cross-lingual learning and suggest several avenues for building on this work.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the paper:

The paper presents a new iterative self-supervised training approach called CRISS (Cross-lingual Retrieval for Iterative Self-Supervised training) for training multilingual sequence-to-sequence models. It leverages the finding that pretrained multilingual models like mBART have emergent cross-lingual alignment capabilities even when trained only on monolingual data. CRISS works by first using the mBART encoder outputs to mine pseudo-parallel sentences from monolingual corpora in different languages. It then trains an mBART model on these mined pseudo-parallel sentences to improve cross-lingual alignment. This mining and training process is repeated iteratively, with each iteration mining better pseudo-parallel data using the improved model, and then training a better model on the new mined data. Without using any supervised parallel data, CRISS achieves state-of-the-art results on unsupervised machine translation benchmarks and cross-lingual sentence retrieval tasks. It also further improves performance on supervised machine translation compared to mBART. The key contributions are introducing an effective iterative self-supervised training procedure for multilingual models, and showing strong empirical results on multiple cross-lingual tasks.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the paper:

The paper proposes a new self-supervised training approach called Cross-lingual Retrieval for Iterative Self-Supervised Training (CRISS) for improving multilingual machine translation and cross-lingual sentence retrieval. CRISS iteratively combines mining for pseudo-parallel sentences and multilingual model training. It is based on three findings: 1) The encoder outputs of the mBART multilingual denoising autoencoder model represent semantic meaning in a language-agnostic way. 2) Finetuning mBART on just one language pair improves the cross-lingual alignment for all language pairs. 3) The cross-lingual alignment can be further improved by iteratively mining pseudo-parallel data using the model, then retraining the model on this data. 

CRISS starts with an mBART model pretrained on monolingual data in 25 languages. It then alternates between mining pseudo-parallel data using encoder outputs and margin scoring, and training the mBART model on this mined data. After 3 iterations, CRISS achieves state-of-the-art results on unsupervised machine translation benchmarks in 9 out of 10 directions, with average gains of 2.4 BLEU. It also substantially improves retrieval accuracy on the Tatoeba benchmark, with average gains of 21.5% absolute accuracy across 16 languages compared to previous methods. CRISS also provides further gains of 1.8 BLEU on average when used for initializing supervised machine translation models. The gains demonstrate the effectiveness of iterative self-supervised training for improving cross-lingual representation learning.


## Summarize the main method used in the paper in one paragraph.

 The paper proposes a new self-supervised pretraining method called Cross-lingual Retrieval for Iterative Self-Supervised Training (CRISS) for improving multilingual machine translation and sentence retrieval. The main idea is to iteratively mine pseudo-parallel sentence pairs from monolingual data using the encoder outputs of a multilingual denoising autoencoder model as cross-lingual sentence representations, then use the mined pseudo-parallel data to train the model further. Specifically, they first pretrain a mBART denoising autoencoder model on monolingual data in 25 languages. Then they iteratively perform the following two steps:

1) Mine pseudo-parallel sentence pairs between languages using the encoder outputs of the current model and a margin-based scoring function. This allows them to extract pseudo-parallel data without any human labeled parallel data. 

2) Retrain the mBART model on the aggregated mined pseudo-parallel sentences to get an improved model. They always restart training from the original mBART checkpoint instead of the last iteration's model.

By repeating the mining and retraining process multiple times, they are able to iteratively improve the model's ability to mine better pseudo-parallel data, as well as its performance on unsupervised machine translation and cross-lingual sentence retrieval tasks. The mining and training steps provide mutual improvements in an iterative fashion.


## What problem or question is the paper addressing?

 Based on my reading, this paper is addressing the problem of improving cross-lingual alignment and machine translation quality between multiple languages using only unlabeled text data. Specifically, it focuses on the following key questions:

- How can we utilize unlabeled monolingual data across many languages to improve cross-lingual sentence retrieval and unsupervised machine translation? 

- Can cross-lingual alignment that emerges from pretrained multilingual models like mBART be further improved in an unsupervised manner?

- Can iterative mining of pseudo-parallel data and training help boost performance on both cross-lingual retrieval and unsupervised machine translation tasks?

The paper proposes an unsupervised training method called CRISS (Cross-lingual Retrieval for Iterative Self-Supervised Training) to address these questions. At a high level, CRISS iterates between mining pseudo-parallel data using current model and training the model on mined data to improve cross-lingual alignment and translation ability in an unsupervised, iterative fashion.

The key ideas are:

- Encoder outputs of pretrained mBART model exhibit cross-lingual alignment capabilities that can be utilized to mine pseudo-parallel data from monolingual corpora

- Finetuning mBART model on mined pseudo-parallel data improves alignment and translation even more

- Repeating mining and finetuning steps allows "self-improvement" of model's cross-lingual abilities without any labeled supervision

So in summary, the paper focuses on improving cross-lingual NLP abilities like retrieval and translation in an unsupervised manner by iterative mining and training.


## What are the keywords or key terms associated with this paper?

 Based on reading the paper summary, some of the key terms and concepts include:

- Cross-lingual retrieval - The paper focuses on using cross-lingual retrieval of sentence pairs to improve machine translation models in an unsupervised, iterative manner.

- Iterative self-supervised training - The proposed CRISS method trains models iteratively by mining pseudo-parallel data and training multilingually in each iteration.

- Unsupervised machine translation - The paper aims to improve unsupervised neural machine translation, which utilizes monolingual corpora without parallel data. 

- Multilingual denoising autoencoder - The mBART model used is pretrained as a multilingual denoising autoencoder.

- Language-agnostic representations - A key finding is that mBART produces sentence representations that are language-agnostic and capture semantic meaning.

- Sentence retrieval - Sentence retrieval accuracy is used to evaluate cross-lingual alignment. The paper shows CRISS improves retrieval.

- Pivot languages - The method retrieves parallel sentences by mining from pivotal languages like English, Spanish, Hindi, and Chinese.

- State-of-the-art results - CRISS achieves new state-of-the-art results on unsupervised machine translation and sentence retrieval benchmarks.

In summary, the key terms cover cross-lingual retrieval, iterative self-training, unsupervised MT, multilingual representations, sentence retrieval, pivot languages, and improved state-of-the-art results.


## Based on the paper, what questions should we ask to create a comprehensive summary of it? Please list at least 10 questions.

 Here are 10 potential questions to ask to create a comprehensive summary of the paper:

1. What is the main contribution or purpose of this paper? 

2. What previous work or background research does this paper build off of? 

3. What methods, models, or techniques are introduced in this paper? How do they work?

4. What experiments were conducted? What datasets were used?

5. What were the main results of the experiments? What metrics were used to evaluate performance? 

6. How do the results compare to previous state-of-the-art methods? Is there an improvement?

7. What are the limitations of the proposed approach? Are there any potential issues or downsides? 

8. Does the paper provide an analysis or discussion of the results? What insights can be gained?

9. Does the paper suggest any areas for future work or research? 

10. What are the key takeaways? What are the most important conclusions or implications of this work?

Asking these types of questions while reading the paper will help ensure a comprehensive understanding of the background, methods, results, and impact of the work. The answers can then be synthesized into a summary that captures the core contributions and significance of the paper.


## Design 10 in-depth questions about method proposed in this paper and avoid simple question.

 Here are 10 potential in-depth questions about the method proposed in the paper:

1. The paper shows that the encoder outputs of mBART represent language-agnostic semantic meaning. What experiments or analyses could be done to further understand why the mBART encoder has this capability? For example, how does varying the number of languages used in pretraining impact the emergence of language-agnostic representations?

2. The paper finds that finetuning mBART on just one pair of parallel bitext improves cross-lingual alignment across all language directions. What mechanisms enable this transfer of alignment to unseen language pairs? Could analyses like probing classifiers be used to study what linguistic properties are being aligned?

3. The CRISS training procedure combines mining and multilingual training iteratively. What motivates this particular order of mining then training? Would the results differ if multiple rounds of mining were done before multilingual training? 

4. How does the margin scoring function for mining parallel sentences compare to other techniques like cosine similarity alone? Are there other mining techniques from prior work that could be incorporated into the CRISS framework?

5. The multilingual training used in CRISS simply combines mined parallel data across all language pairs. Could more sophisticated training schemes like staggered multi-task learning improve results?

6. For languages with little monolingual data, does CRISS have difficulties mining parallel data? Could transfer learning from high resource languages help improve low resource mining?

7. The ablation studies show diminishing returns from adding more pivot languages beyond English and Spanish. Is there a theoretical justification for why one or two pivot languages enables cross-lingual transfer?

8. How does the quality of mined parallel data change across CRISS iterations? Does higher mining recall come at the cost of lower precision compared to early iterations?

9. The CRISS pretraining improves performance on supervised MT compared to mBART. Is it primarily distilling the mined parallel data or does CRISS also learn generally better multilingual representations?

10. How does CRISS compare to other semi-supervised approaches that combine labeled and mined parallel data during training? Could CRISS mining be integrated with a small amount of supervised data?


## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a high-quality paragraph summarizing the key points of the paper:

This paper proposes a novel self-supervised training method called Cross-lingual Retrieval for Iterative Self-Supervised Training (CRISS) for multilingual sequence generation. The authors first show that the encoder outputs of mBART, a multilingual denoising autoencoder model, represent language-agnostic semantic meaning and can be used for cross-lingual sentence retrieval. They then demonstrate that finetuning mBART on just one pair of parallel data improves cross-lingual alignment across all language pairs. Based on these findings, CRISS iteratively mines for parallel sentences using the model's own encoder outputs, then finetunes the model on the mined data to improve alignment. This mining and finetuning is repeated for multiple iterations. CRISS achieves state-of-the-art results on unsupervised machine translation for 9 language pairs, improving average BLEU by 2.4. It also substantially improves performance on the Tatoeba sentence retrieval benchmark, boosting accuracy by 21.5% on average across 16 languages. Additionally, CRISS provides a 1.8 BLEU improvement on supervised translation even compared to strong baselines like mBART. The proposed iterative self-supervised training approach effectively utilizes unlabeled data to learn cross-lingual representations that transfer well to downstream tasks.


## Summarize the paper in one sentence.

 The paper presents Cross-lingual Retrieval for Iterative Self-Supervised Training (CRISS), an unsupervised approach for improving cross-lingual alignment and translation by iteratively retrieving parallel sentences using a multilingual neural model and finetuning the model on the retrieved data.


## Summarize the paper in one paragraphs.

 Here is a one paragraph summary of the paper:

This paper proposes a new self-supervised training approach called Cross-lingual Retrieval for Iterative Self-Supervised Training (CRISS) for improving multilingual sequence generation models. The key ideas are: 1) The encoder outputs of multilingual denoising autoencoder models like mBART already contain emergent cross-lingual alignment and can be used to retrieve parallel sentences across languages. 2) Finetuning the model with mined parallel data improves the cross-lingual alignment. 3) This mining and finetuning can be applied iteratively - mining with an improved model gives better parallel data, training on better parallel data improves the model further. Using this technique, the authors achieve state-of-the-art results on unsupervised neural machine translation and cross-lingual sentence retrieval tasks. The CRISS pretraining also improves performance on supervised machine translation tasks. Overall, this work demonstrates the potential of self-supervised mining and training techniques for improving cross-lingual natural language processing models.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the CRISS method proposed in the paper:

1. The paper shows that the mBART encoder outputs exhibit cross-lingual alignment capabilities even when trained only on monolingual data. How exactly does this cross-lingual alignment emerge during pretraining? What properties of mBART lead to this effect?

2. The paper proposes an iterative procedure that alternates between mining parallel data and multilingual training. Why is this iterative approach effective? How does each step help improve the other?

3. The mining procedure uses a margin-based scoring function. What is the intuition behind this scoring function? How does it help surface semantically similar sentence pairs across languages? 

4. The paper shows that multilingual training is better than bilingual training when using mined pseudo-parallel data. Why does training with more languages help in this scenario?

5. The ablation studies show diminishing returns when using more than 2 pivot languages for mining. What explains this effect? Is there an optimal number of pivot languages that balances performance and efficiency?

6. How robust is the mining procedure to noisy or incorrect translations? Does the margin-based scoring function help mitigate the impact of incorrect pairs?

7. The CRISS pretraining improves performance on both unsupervised and supervised MT. What properties does it learn that transfer well to both scenarios?

8. How does the performance of CRISS compare when starting from a strong supervised baseline like mBART? What benefits does it provide over just supervised pretraining?

9. Could the CRISS approach be applied to other modalities like images or speech? What modifications would be needed to adapt it for cross-modal retrieval and training?

10. What are some ways the CRISS framework could be extended or improved in future work? Are there any obvious limitations of the current approach?
