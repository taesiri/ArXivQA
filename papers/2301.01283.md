# [Cross Modal Transformer: Towards Fast and Robust 3D Object Detection](https://arxiv.org/abs/2301.01283)

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper addresses is how to design an end-to-end framework for robust 3D object detection using multi-modal sensor data (camera images and LiDAR point clouds). The key hypothesis is that encoding 3D position information into the multi-modal tokens and using transformer architecture with position-guided queries can achieve state-of-the-art 3D detection performance in an end-to-end manner, while also being robust to missing sensor modalities.In summary, the main research focus is developing a simple yet effective pipeline for end-to-end multi-modal 3D object detection that is fast, accurate and robust. The method proposed, called Cross-Modal Transformer (CMT), aims to address this research problem.


## What is the main contribution of this paper?

The main contributions of this paper are:1. It proposes Cross-Modal Transformer (CMT), a fast and robust 3D object detector for end-to-end multi-modal detection. 2. CMT encodes 3D positions into multi-modal tokens using Coordinates Encoding Module (CEM), allowing effective fusion without explicit alignment.3. Position-guided queries are introduced, which are encoded with positions in both image and LiDAR spaces.4. CMT achieves state-of-the-art performance on nuScenes dataset while being robust to sensor failure through masked-modal training. 5. The proposed method provides a simple yet effective pipeline for end-to-end multi-modal 3D detection, overcoming the reliance on any specific sensor modality.In summary, the key contribution is proposing CMT, an end-to-end multi-modal 3D detector that achieves excellent performance and robustness by implicitly encoding 3D positions into image and LiDAR tokens and using position-guided queries, without needing complex feature alignment or transformation. The simplicity and effectiveness of CMT provides a strong baseline for multi-modal 3D detection.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence TL;DR summary of the paper:The paper proposes Cross Modal Transformer (CMT), an end-to-end 3D object detection model that achieves state-of-the-art performance on nuScenes by encoding 3D position information into image and point cloud tokens and allowing object queries to directly interact with the multi-modal tokens simultaneously.
