# [Physical-World Optical Adversarial Attacks on 3D Face Recognition](https://arxiv.org/abs/2205.13412)

## What is the central research question or hypothesis that this paper addresses?

 The central research question this paper addresses is how to physically attack real-world 3D face recognition systems using adversarial illumination. 

The key hypothesis is that by optimizing perturbations in the projected illumination and modeling the face reflection process, it is possible to generate adversarial point clouds that can dodge or impersonate 3D face recognition systems.

Specifically, the paper proposes two main attack methods:

1. Phase Shifting Attack: This attacks multi-step structured light systems by hiding perturbations in the projected fringe patterns. It involves the phase shifting algorithm in the attack optimization process through a differential 3D reconstruction.

2. Phase Superposition Attack: This uses an additional projector to add perturbations. It models the face relighting process through a Lambertian reflectance model and optimizes the noise end-to-end to attack single-step systems.

The overall hypothesis is that by carefully designing the attack pipeline for 3D structured light imaging, adversarial illuminations can be generated to attack real-world 3D face recognition with high success rates using fewer perturbations than previous physical attacks.


## What is the main contribution of this paper?

 The main contributions of this paper are:

1. It proposes novel physical adversarial attacks against 3D face recognition using adversarial illumination. The attacks can generate point-wise perturbations at arbitrary 3D positions.

2. It involves the complex face reflection process in the attack pipeline through the Lambertian reflectance model and a differential 3D reconstruction algorithm. 

3. It introduces a 3D transform invariant loss and sensitivity maps to improve the attack's robustness and invisibility. 

4. It evaluates the attacks on various 3D face recognition models, including both point cloud based and depth image based methods. The results show the attack can achieve high success rates while needing fewer perturbations than previous physical attacks.

In summary, this paper presents the first physical adversarial attack method for 3D face recognition that considers the face reflection process and can generate precise 3D perturbations. The attacks are demonstrated to be effective against state-of-the-art 3D face recognition systems.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the key points from the paper:

The paper proposes novel physical adversarial attacks against 3D face recognition by generating adversarial illumination patterns that are either concealed within or superimposed on the structured light patterns used for 3D scanning, leveraging techniques like Lambertian face reflectance modeling and 3D transformation invariant loss to create robust and imperceptible attacks.


## How does this paper compare to other research in the same field?

 Here are a few key ways this paper compares to other research in the field of physical adversarial attacks on 3D face recognition:

- It proposes novel physical adversarial attacks using adversarial illumination. Most prior work has focused on digital attacks or physical attacks using 3D printed objects. Using light to create adversarial perturbations is a relatively new approach.

- It considers the face reflectance process using a Lambertian rendering model. Other optical adversarial attack papers have not modeled the complex reflectance of human skin and translucent properties. 

- The attacks are end-to-end, generating adversarial noise patterns through an optimization process that includes the 3D reconstruction. This allows better optimization and integration of the full pipeline.

- The attacks target real-world structured light 3D face recognition systems. Many previous papers only conduct experiments in simulation. This paper builds a full testbed to evaluate performance.

- The method introduces techniques like the 3D transform invariant loss and sensitivity maps to improve imperceptibility and robustness. This is an advance over basic adversarial loss functions.

- The attacks are shown to be effective on multiple 3D recognition models (point cloud and depth image based) while needing fewer perturbations than prior physical attacks.

In summary, this paper pushes forward the state-of-the-art in physical adversarial attacks against 3D face recognition systems by incorporating the full pipeline into the attack process and evaluating on real-world conditions and hardware. The novel techniques proposed demonstrate improved performance over prior physical attack methods.


## What future research directions do the authors suggest?

 Based on my reading of the paper, the main future research directions suggested by the authors are:

1. Apply the attack to other 3D face recognition systems beyond structured light cameras, such as time-of-flight cameras. The authors suggest their end-to-end attack framework could shed light on real-world attacks involving face relighting for other systems.

2. Improve the attack robustness and imperceptibility further. The authors introduced techniques like 3D transform invariant loss and sensitivity maps, but suggest there is room for improvement.

3. Investigate defenses against these types of attacks. The authors show successful dodging and impersonation attacks, but do not discuss potential defenses. Developing effective defenses is an important direction.

4. Explore other optical adversarial attack methods besides modulating the projected illumination. The authors focus on perturbing the structured light patterns, but other approaches like using an adversarial light source could be explored.

5. Study the transferability of adversarial examples more thoroughly. The authors find the attacks have some transferability, especially for dodging attacks. More research on factors affecting transferability in 3D could help improve it.

6. Evaluate the attacks on more complex 3D face recognition pipelines. The attacks are demonstrated on classification networks, but could be extended to more sophisticated systems.

7. Develop standardized 3D adversarial attack benchmarks. The authors compare to prior work, but standardized benchmarks would better evaluate different attacks.

In summary, the main future directions are applying the attacks more broadly, improving their robustness and imperceptibility, studying their transferability, evaluating on more complex systems, developing defenses, exploring other optical attack methods, and creating standardized benchmarks. Advancing research in these areas could lead to more robust 3D face recognition systems.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the paper:

This paper proposes novel physical adversarial attacks against 3D face recognition systems that use structured light cameras. The attacks involve generating adversarial illumination patterns that are projected onto the face, which results in small perturbations to the reconstructed 3D facial point cloud. This causes the face recognition system to misclassify the identity. The perturbations are created in an end-to-end fashion by modeling the projection and image formation process using a Lambertian reflectance model. The attack is optimized using an adversarial loss function along with losses that improve imperceptibility and robustness to variations like pose changes. Experiments show the attack is effective against state-of-the-art 3D face recognition networks, causing impersonation or dodging attacks with high success rates using fewer perturbations than prior physical attacks on 3D recognition systems. The attack could potentially be applied against real-world structured light based face authentication systems.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the key points from the paper:

This paper proposes novel physical adversarial attacks against 3D face recognition systems that use structured light cameras. The attacks involve generating adversarial perturbations in the projected illumination patterns. This results in small shifts in the reconstructed 3D face data, causing the face recognition system to misclassify the face. The perturbations are optimized end-to-end using a differentiable 3D reconstruction model and adversarial loss function. To account for the complex face reflection process, a Lambertian rendering model is used. Several techniques are introduced to improve the attacks' robustness and imperceptibility, including projecting the perturbations along the camera axis, using sensitivity maps, and a 3D transformation invariant loss. 

The attacks are evaluated on several 3D face recognition models using point cloud and depth image inputs. Experiments in both simulated and real-world settings demonstrate successful dodging and impersonation attacks with high success rates. The attacks require fewer point perturbations compared to prior work on 3D physical attacks. Ablation studies validate the effectiveness of the proposed techniques. The work provides the first demonstration of physical adversarial attacks on real-world 3D face recognition systems. The novel approach of incorporating the 3D imaging pipeline in an end-to-end manner enables highly effective attacks with subtle perturbations.


## Summarize the main method used in the paper in one paragraph.

 Here is a one paragraph summary of the main method used in the paper:

The paper proposes novel physical adversarial attacks to fool 3D face recognition systems that use structured light cameras. The attacks involve generating adversarial perturbations in the projected illumination patterns, which then get reflected off the face and captured by the camera, resulting in corrupted 3D face data. To generate the adversarial patterns end-to-end, the paper uses a Lambertian rendering model to simulate the face reflection process during optimization. It also utilizes techniques like 3D transformation invariant loss and sensitivity maps to improve attack robustness and invisibility. The adversarial illumination can either be embedded into the original patterns through the inherent projector (phase shifting attack) or superimposed using an external projector (phase superposition attack). Experiments show the method can successfully attack major 3D deep learning models with fewer perturbations than prior physical attacks on 3D face recognition.


## What problem or question is the paper addressing?

 This paper is addressing the problem of physical adversarial attacks on 3D face recognition systems. The key questions it aims to tackle are:

- How to generate physical adversarial perturbations that can attack real-world 3D face recognition systems based on structured light cameras? 

- How to make the adversarial perturbations invisible and robust to environmental changes like rotations and translations?

- How to involve the complex face reflection process into the attack pipeline to generate adversarial examples end-to-end?

Specifically, the paper proposes two novel attack methods - Phase Shifting Attack and Phase Superposition Attack - to attack multi-step and single-step structured light 3D face recognition systems respectively. 

The key ideas and contributions are:

- Using adversarial illumination patterns to indirectly perturb the 3D face data reconstructed by structured light cameras. This allows generating adversarial points at arbitrary 3D positions.

- Modeling the face reflection process using Lambertian reflectance model and involve it in the attack optimization loop for end-to-end perturbation generation.

- Introducing 3D Transformation Invariant loss and sensitivity maps to improve invisibility and robustness of the attacks. 

- Attacking both point cloud based and depth image based 3D face recognition models with high success rates but fewer perturbations compared to prior arts.

In summary, this paper explores and provides solutions for physical adversarial attacks on real-world 3D face recognition systems, which is still an under-explored area. The proposed methods are shown to be effective in both simulated and physical attack experiments.
