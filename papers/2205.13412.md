# [Physical-World Optical Adversarial Attacks on 3D Face Recognition](https://arxiv.org/abs/2205.13412)

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper addresses is how to physically attack real-world 3D face recognition systems using adversarial illumination. The key hypothesis is that by optimizing perturbations in the projected illumination and modeling the face reflection process, it is possible to generate adversarial point clouds that can dodge or impersonate 3D face recognition systems.Specifically, the paper proposes two main attack methods:1. Phase Shifting Attack: This attacks multi-step structured light systems by hiding perturbations in the projected fringe patterns. It involves the phase shifting algorithm in the attack optimization process through a differential 3D reconstruction.2. Phase Superposition Attack: This uses an additional projector to add perturbations. It models the face relighting process through a Lambertian reflectance model and optimizes the noise end-to-end to attack single-step systems.The overall hypothesis is that by carefully designing the attack pipeline for 3D structured light imaging, adversarial illuminations can be generated to attack real-world 3D face recognition with high success rates using fewer perturbations than previous physical attacks.


## What is the main contribution of this paper?

The main contributions of this paper are:1. It proposes novel physical adversarial attacks against 3D face recognition using adversarial illumination. The attacks can generate point-wise perturbations at arbitrary 3D positions.2. It involves the complex face reflection process in the attack pipeline through the Lambertian reflectance model and a differential 3D reconstruction algorithm. 3. It introduces a 3D transform invariant loss and sensitivity maps to improve the attack's robustness and invisibility. 4. It evaluates the attacks on various 3D face recognition models, including both point cloud based and depth image based methods. The results show the attack can achieve high success rates while needing fewer perturbations than previous physical attacks.In summary, this paper presents the first physical adversarial attack method for 3D face recognition that considers the face reflection process and can generate precise 3D perturbations. The attacks are demonstrated to be effective against state-of-the-art 3D face recognition systems.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper proposes novel physical adversarial attacks against 3D face recognition by generating adversarial illumination patterns that are either concealed within or superimposed on the structured light patterns used for 3D scanning, leveraging techniques like Lambertian face reflectance modeling and 3D transformation invariant loss to create robust and imperceptible attacks.


## How does this paper compare to other research in the same field?

Here are a few key ways this paper compares to other research in the field of physical adversarial attacks on 3D face recognition:- It proposes novel physical adversarial attacks using adversarial illumination. Most prior work has focused on digital attacks or physical attacks using 3D printed objects. Using light to create adversarial perturbations is a relatively new approach.- It considers the face reflectance process using a Lambertian rendering model. Other optical adversarial attack papers have not modeled the complex reflectance of human skin and translucent properties. - The attacks are end-to-end, generating adversarial noise patterns through an optimization process that includes the 3D reconstruction. This allows better optimization and integration of the full pipeline.- The attacks target real-world structured light 3D face recognition systems. Many previous papers only conduct experiments in simulation. This paper builds a full testbed to evaluate performance.- The method introduces techniques like the 3D transform invariant loss and sensitivity maps to improve imperceptibility and robustness. This is an advance over basic adversarial loss functions.- The attacks are shown to be effective on multiple 3D recognition models (point cloud and depth image based) while needing fewer perturbations than prior physical attacks.In summary, this paper pushes forward the state-of-the-art in physical adversarial attacks against 3D face recognition systems by incorporating the full pipeline into the attack process and evaluating on real-world conditions and hardware. The novel techniques proposed demonstrate improved performance over prior physical attack methods.


## What future research directions do the authors suggest?

Based on my reading of the paper, the main future research directions suggested by the authors are:1. Apply the attack to other 3D face recognition systems beyond structured light cameras, such as time-of-flight cameras. The authors suggest their end-to-end attack framework could shed light on real-world attacks involving face relighting for other systems.2. Improve the attack robustness and imperceptibility further. The authors introduced techniques like 3D transform invariant loss and sensitivity maps, but suggest there is room for improvement.3. Investigate defenses against these types of attacks. The authors show successful dodging and impersonation attacks, but do not discuss potential defenses. Developing effective defenses is an important direction.4. Explore other optical adversarial attack methods besides modulating the projected illumination. The authors focus on perturbing the structured light patterns, but other approaches like using an adversarial light source could be explored.5. Study the transferability of adversarial examples more thoroughly. The authors find the attacks have some transferability, especially for dodging attacks. More research on factors affecting transferability in 3D could help improve it.6. Evaluate the attacks on more complex 3D face recognition pipelines. The attacks are demonstrated on classification networks, but could be extended to more sophisticated systems.7. Develop standardized 3D adversarial attack benchmarks. The authors compare to prior work, but standardized benchmarks would better evaluate different attacks.In summary, the main future directions are applying the attacks more broadly, improving their robustness and imperceptibility, studying their transferability, evaluating on more complex systems, developing defenses, exploring other optical attack methods, and creating standardized benchmarks. Advancing research in these areas could lead to more robust 3D face recognition systems.
