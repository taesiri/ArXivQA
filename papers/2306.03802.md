# [Learning to Ground Instructional Articles in Videos through Narrations](https://arxiv.org/abs/2306.03802)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be: Can we leverage large-scale, unlabeled video datasets to train a model that can ground procedural steps in how-to videos?The authors propose a new training framework to learn to align steps from instructional articles to frames in how-to videos using only weak supervision from noisy ASR narrations and instructional articles, without requiring any manual annotations.Specifically, their method learns to temporally ground steps of procedural activities by fusing information from two pathways:1) An "indirect" pathway that aligns steps to frames by composing steps-to-narration assignments with narration-to-frame correspondences. 2) A "direct" pathway that directly learns associations between step descriptions and frames.The key hypothesis seems to be that the synergy from jointly learning these two alignment pathways and iteratively refining pseudo-labels for the steps can enable accurate step grounding without manually annotated data.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contributions appear to be:1. The paper proposes a new approach for temporally grounding procedural steps from instructional articles in narrated how-to videos, using only weak supervision from narrations and articles during training. This allows leveraging large unlabeled video datasets.2. The method aligns steps to video frames via two pathways - a direct pathway between steps and frames, and an indirect pathway that composes steps-narrations with narrations-frames alignments. 3. The paper introduces a new benchmark called HT-Step for evaluating instructional step grounding, consisting of 124 hours of HowTo100M videos manually annotated with steps.4. Experiments demonstrate state-of-the-art results on HT-Step as well as other benchmarks for both step grounding and narration alignment. The inner narration-video alignment module significantly outperforms prior work.5. The proposed training procedure uses step pseudo-labels and a curriculum with iteratively refined and filtered labels. This is shown to provide gains over fixed pseudo-labels.In summary, the key contribution appears to be the weakly supervised training framework and multi-modality alignment approach for temporal grounding of instructional steps in videos, along with the introduction of a new benchmark for evaluating this task.
