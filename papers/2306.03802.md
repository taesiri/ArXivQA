# [Learning to Ground Instructional Articles in Videos through Narrations](https://arxiv.org/abs/2306.03802)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be: Can we leverage large-scale, unlabeled video datasets to train a model that can ground procedural steps in how-to videos?The authors propose a new training framework to learn to align steps from instructional articles to frames in how-to videos using only weak supervision from noisy ASR narrations and instructional articles, without requiring any manual annotations.Specifically, their method learns to temporally ground steps of procedural activities by fusing information from two pathways:1) An "indirect" pathway that aligns steps to frames by composing steps-to-narration assignments with narration-to-frame correspondences. 2) A "direct" pathway that directly learns associations between step descriptions and frames.The key hypothesis seems to be that the synergy from jointly learning these two alignment pathways and iteratively refining pseudo-labels for the steps can enable accurate step grounding without manually annotated data.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contributions appear to be:1. The paper proposes a new approach for temporally grounding procedural steps from instructional articles in narrated how-to videos, using only weak supervision from narrations and articles during training. This allows leveraging large unlabeled video datasets.2. The method aligns steps to video frames via two pathways - a direct pathway between steps and frames, and an indirect pathway that composes steps-narrations with narrations-frames alignments. 3. The paper introduces a new benchmark called HT-Step for evaluating instructional step grounding, consisting of 124 hours of HowTo100M videos manually annotated with steps.4. Experiments demonstrate state-of-the-art results on HT-Step as well as other benchmarks for both step grounding and narration alignment. The inner narration-video alignment module significantly outperforms prior work.5. The proposed training procedure uses step pseudo-labels and a curriculum with iteratively refined and filtered labels. This is shown to provide gains over fixed pseudo-labels.In summary, the key contribution appears to be the weakly supervised training framework and multi-modality alignment approach for temporal grounding of instructional steps in videos, along with the introduction of a new benchmark for evaluating this task.


## How does this paper compare to other research in the same field?

Here are a few thoughts on how this paper compares to other research on temporally grounding instructional steps in videos:- The key difference from prior work is the weakly-supervised training approach that utilizes narrated videos and instructional articles as sources of weak supervision, without needing any manual annotations. Most prior work relies on some form of manual supervision, either fully annotated data or at least weak labels indicating which steps occur in each video. This allows the method in this paper to scale up to large unlabeled video datasets.- The idea of using both direct and indirect alignment pathways to match steps to video frames is novel. Prior work has focused on directly matching text queries to video, whereas this papers shows benefits from also leveraging narrations as a bridge between steps and videos. - Using narrations and articles for weak supervision relates to some prior works like Alayrac et al. 2016 and Fried et al. 2020, but those works matched narrations to individual video clips rather than full untrimmed videos. The global grounding of all steps jointly is more related to recent dense video grounding methods.- The proposed model architecture follows recent trends in using Transformers for multimodal encoding, though the incorporation of multiple alignment pathways tailored for steps, narrations, and video is unique.- For evaluation, creating a new narrated instructional video dataset with temporal annotations relates this to prior supervised datasets like COIN. But the use of detailed multi-step wikiHow articles differentiates it from works using simpler atomic steps.- The benchmarking shows state-of-the-art results on CrossTask, HTM-Align, and the new HT-Step dataset, demonstrating its effectiveness. The zero-shot transfer is a challenging test that many prior supervised approaches would fail.In summary, this paper pushes the boundary of weakly-supervised and large-scale learning for instructional video understanding, combining multiple innovations in training procedures, model architecture, and evaluation compared to prior work. The results clearly demonstrate the viability of the proposed techniques.


## What future research directions do the authors suggest?

Based on my reading of the paper, some of the main future research directions suggested by the authors include:- Developing models that can handle longer videos and multi-step activities. The current work focuses on grounding steps in short 1-2 minute narrated instructional videos. The authors suggest extending the approach to longer, more complex videos involving multiple steps/subtasks.- Exploring alternative sources of weak supervision beyond narrations and instructional articles. The authors mainly rely on narrations and wikiHow articles for weak supervision, but suggest trying other sources like subtitles, action scripts, etc. - Applying the approach to other domains beyond instructional videos such as movies, sports, news, etc. The current work focuses on the cooking/how-to domain, but the authors propose applying it more broadly.- Improving computational efficiency to handle large-scale video collections. The current models are still computationally expensive to scale up. The authors suggest exploring efficiency improvements.- Enriching the learned representations with other information like object interactions and human poses. The current model relies mainly on visual features from a 3D CNN backbone. Incorporating higher-level information could help.- Developing better evaluation benchmarks covering more domains. The authors acknowledge limitations of the current video grounding benchmarks and propose developing more comprehensive ones.Overall, the key future directions mentioned are developing models that can handle more complex videos and tasks, exploring new weak supervision sources, applying the approach to new domains, improving computational and data efficiency, and enriching the representations and evaluation benchmarks.


## Summarize the paper in one paragraph.

Here is a one paragraph summary of the paper:The paper presents an approach for localizing steps of procedural activities in narrated how-to videos. The method leverages instructional articles from a wikiHow knowledge base containing step-by-step instructions for various procedural tasks. Without any manual supervision, the model learns to temporally align the steps in the articles to frames in the videos by matching three modalities: frames, narrations, and step descriptions. Specifically, the model has two alignment pathways - direct alignment of steps to frames, and indirect alignment obtained by composing step-narration assignments with narration-frame correspondences. The model is trained using iteratively refined pseudo-labels and is evaluated on a new benchmark HT-Step obtained by manually annotating HowTo100M videos with wikiHow steps. Experiments show that the multi-modality alignment approach leads to significant gains over several baselines and prior works on step localization. The inner narration-video alignment module also outperforms prior work on the narration alignment task.


## Summarize the paper in two paragraphs.

Here is a two paragraph summary of the paper:The paper presents a method for aligning steps of procedural activities described in instructional articles to videos of those activities being performed. The approach utilizes narrated how-to videos and leverages both the visual frames and automatically generated narration transcripts. It learns to ground steps in videos by fusing information from two pathways - direct alignment of steps to frames, and indirect alignment obtained by composing step-to-narration and narration-to-frame correspondences. The method trains on uncurated video datasets paired with instructional articles, without needing any manual annotations. It generates pseudo-labels for steps that are iteratively refined during training. Experiments demonstrate that fusing the two alignment pathways significantly improves step grounding over using either one alone. The approach also achieves state-of-the-art on the narration-video alignment task, outperforming prior methods on benchmarks for both step and narration grounding. Overall, the proposed narration-aided framework can effectively leverage narrations and instructional articles to learn step grounding in a weakly supervised manner.


## Summarize the main method used in the paper in one paragraph.

The paper presents a method for grounding procedural instruction steps from wikiHow articles in narrated how-to videos, without using any manual annotations. The key ideas are:The method learns to align instruction steps to video frames using two pathways - a direct pathway that matches steps to video frames, and an indirect pathway that matches steps to narrations and composes those assignments with narration-to-video alignments. The direct and indirect alignments are fused to get the final step-to-video grounding.The model is trained using narrated instructional videos and wikiHow articles in a multi-task setting to jointly learn narration-to-video and step-to-video alignment. It uses a teacher-student strategy with iterative refinement of step pseudo-labels obtained by thresholding the predicted step-to-video alignments. During training, the ground-truth narration timestamps serve as supervision for the narration-to-video alignment task. The step descriptions from wikiHow articles provide weak supervision to train the step-to-video alignment task.At test time, the direct pathway can be used independently to temporally ground steps when narrations are not available. When narrations are available, the indirect pathway provides complementary step grounding information by leveraging narrations.In summary, the key aspects are the multi-task learning framework, the direct and indirect pathways for step-video alignment, and the iterative pseudo-labeling procedure to learn from narrated videos and wikiHow articles without manual supervision.
