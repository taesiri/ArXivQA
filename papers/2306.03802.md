# [Learning to Ground Instructional Articles in Videos through Narrations](https://arxiv.org/abs/2306.03802)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be: Can we leverage large-scale, unlabeled video datasets to train a model that can ground procedural steps in how-to videos?The authors propose a new training framework to learn to align steps from instructional articles to frames in how-to videos using only weak supervision from noisy ASR narrations and instructional articles, without requiring any manual annotations.Specifically, their method learns to temporally ground steps of procedural activities by fusing information from two pathways:1) An "indirect" pathway that aligns steps to frames by composing steps-to-narration assignments with narration-to-frame correspondences. 2) A "direct" pathway that directly learns associations between step descriptions and frames.The key hypothesis seems to be that the synergy from jointly learning these two alignment pathways and iteratively refining pseudo-labels for the steps can enable accurate step grounding without manually annotated data.
