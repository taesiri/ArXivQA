# Diffusion Language Models Can Perform Many Tasks with Scaling and
  Instruction-Finetuning

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central research question this paper addresses is: Can diffusion language models effectively scale up to perform well across diverse language tasks through pretraining, enlarging model sizes, and instruction tuning?In particular, the paper investigates whether diffusion language models can:1) Acquire general knowledge from large-scale pretraining, similar to masked language models like BERT.2) Improve performance on downstream tasks as model size increases, following the typical scaling laws of language models. 3) Exhibit strong zero-shot and few-shot performance on new tasks when adapted through instruction tuning, like GPT-3.The overall goal is to explore whether diffusion language models can match or exceed the capabilities of autoregressive language models by leveraging scaling techniques like pretraining, model enlargement, and instruction tuning. The paper aims to demonstrate the viability of diffusion models as a powerful alternative paradigm for generative language modeling.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contributions appear to be:1. Demonstrating that diffusion language models can be scaled effectively to improve performance across a wide range of natural language tasks. The authors show this by pretraining diffusion models on large unlabeled corpora, scaling up model sizes, and adapting them to downstream tasks via task-specific finetuning and instruction finetuning. 2. Highlighting the potential of diffusion models as an alternative to autoregressive models for generative language modeling. The paper argues that diffusion models have several theoretical advantages, such as a global receptive field and non-autoregressive generation. The experiments aim to substantiate these claims empirically.3. Providing analysis and findings regarding the abilities of scaled diffusion models:- They can acquire knowledge from large-scale pretraining and improve with greater model capacity, following scaling laws similar to autoregressive models. - They exhibit zero-shot and few-shot learning capabilities when adapted via instruction finetuning, allowing them to perform unseen tasks by following natural language instructions.- They demonstrate some promising structured reasoning behaviors thanks to flexible generation ordering, but still struggle with complex reasoning tasks.In summary, the key contribution is showing that with proper scaling and adaptation, diffusion models can serve as strong generative language learners comparable to autoregressive models, across a diverse range of tasks. The paper aims to demonstrate the promise of scaling up this alternative paradigm.
