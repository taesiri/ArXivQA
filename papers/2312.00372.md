# [Event-driven Real-time Retrieval in Web Search](https://arxiv.org/abs/2312.00372)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a high-quality one-paragraph summary of the key points from this paper:

This paper proposes a novel real-time retrieval approach called Event-driven Real-time Retrieval (ERR) for news search scenarios that require retrieving the most timely and relevant breaking news stories. ERR expands user queries with related breaking events to better capture instant search intent and then fuses this event information into the query representation using cross-attention and multi-task learning. To obtain suitable training data for the real-time search task and reduce annotation costs, the authors develop an automated two-stage annotation pipeline consisting of a model-based coarse annotation followed by an instruction-tuned language model fine annotation. Extensive offline experiments on large-scale data demonstrate ERR's superior performance over state-of-the-art baseline methods, with especially pronounced gains on retrieving timely event-related documents. Online A/B testing results further validate ERR's effectiveness, with metrics like CTR and dwell time showing over 4-5% average improvement. The proposed event augmentation strategy and tailored model training and data annotation techniques enable effectively meeting users' real-time search needs in news scenarios.


## Summarize the paper in one sentence.

 Here is a one-sentence summary of the paper:

To improve real-time search retrieval, this paper proposes an event-driven approach that expands queries with related breaking news events and fuses them through cross-attention, along with automatic data annotation and training strategies for timely document ranking.


## What is the main contribution of this paper?

 According to the paper, the main contributions are:

1) Proposing a novel real-time retrieval model called ERR that fuses events and queries through a cross-attention and multi-task mechanism to recall more real-time documents. 

2) Introducing a two-stage automatic data annotation approach consisting of a ModelZoo-based Coarse Annotation and an LLM-driven Fine Annotation to obtain data for timely retrieval and reduce data annotation costs.

3) Conducting numerous offline and online experiments that demonstrate the superiority of ERR over existing state-of-the-art models in real-time retrieval tasks.

In summary, the key contribution is proposing the ERR model for improving real-time document retrieval, along with an automatic data annotation pipeline to obtain suitable training data, and showing its effectiveness through extensive experiments.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and concepts associated with it are:

- Real-time retrieval: The paper focuses on improving information retrieval in real-time search scenarios where there is a need to return timely, event-related documents.

- Event-driven: The proposed approach incorporates event information to capture the rapid changes in user search intent and timely breaking news.

- Query expansion: Events are used to expand the original query representation and intent.

- Cross-attention: A cross-attention mechanism is used to fuse the query and event information. 

- Multi-task learning: Multi-task learning objectives are used to better focus the model on event data.

- Automatic data annotation: A two-stage annotation pipeline is proposed, consisting of ModelZoo-based coarse annotation and LLM-driven fine annotation to obtain training data.

- Offline evaluation: Metrics like Recall@k, MAP@k, MRR are used to evaluate retrieval performance offline.  

- Online A/B testing: Online metrics like click-through-rate, query rewrite rate etc. are reported from A/B tests.

In summary, the key ideas involve improving real-time search through event-based query expansion and automatic data annotation techniques. Both offline and online experiments demonstrate the effectiveness of the proposed approach.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions I formulated regarding key aspects of the methodology:

1. How exactly is the event augment technique used to choose the most relevant expansion event related to a query from various candidate events? What features of events are considered and what algorithms are used for ranking?

2. How is the adaptive cross-attention mechanism able to handle differences in distribution between online session queries and offline data? Does it help to overcome the issue of low event coverage?

3. Why is multi-task learning with a query-centric task and an event-centric task helpful for improving event representation? How are the differences in negative sampling utilized?

4. For coarse annotation using ModelZoo, how is the threshold determined for each model to classify sample pairs as easy or hard? What metrics guide such threshold selection?  

5. What is the motivation behind breaking down the annotation task into multiple steps following the Chain-of-Thought prompting technique? How does it help compared to a single-step instruction?

6. How exactly does the two-stage training paradigm alternating between ModelZoo annotated data and LLM annotated data help boost performance compared to conventional training?

7. Why does the introduction of cross batch negative sampling and top-k hard negative sampling help improve diversity and training efficiency?

8. How does unsupervised contrastive loss aid the optimization process? Does it provide more generalization capability?

9. For the online system deployment, what acceleration and optimization techniques are used to ensure low latency retrieval at scale while maintaining accuracy?

10. How can the existing methodology be extended to improve semantic matching capability for long-tail queries with sparse supervision signals?
