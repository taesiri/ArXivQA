# [TESS: Text-to-Text Self-Conditioned Simplex Diffusion](https://arxiv.org/abs/2305.08379)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is:How can diffusion models be effectively adapted for natural language generation tasks, overcoming challenges related to the discrete nature of text?The key hypotheses tested in this work are:1) Diffusing directly on the vocabulary logit space rather than typical learned embedding spaces is more suitable for language.2) Fully non-autoregressive decoding allows generating complete sequences in parallel and flexibly handling variable sequence lengths. 3) A new form of self-conditioning that leverages the logit simplex structure improves generation quality.The authors propose TESS, a text-to-text diffusion model that performs diffusion directly on the vocabulary logit simplex, decodes fully non-autoregressively, and uses an efficient self-conditioning method. Through experiments on text generation and understanding tasks, they demonstrate TESS's effectiveness compared to prior diffusion and non-autoregressive methods. The central goal is adapting the diffusion paradigm to language in a way that maintains its benefits like parallel decoding while overcoming challenges related to text's discrete nature.


## What is the main contribution of this paper?

The main contribution of this paper is proposing TESS, a new text-to-text diffusion model for natural language generation tasks. The key aspects of TESS are:- It is fully non-autoregressive, allowing parallel generation of entire sequences rather than block-by-block or token-by-token.- It performs diffusion directly on the vocabulary logit space rather than typical latent embeddings. This avoids the need for mapping between discrete and continuous representations.- It incorporates a novel form of self-conditioning that exploits the semantics of the logit simplex for more effective conditioning. - It is evaluated on a diverse set of language generation tasks including summarization, simplification, paraphrasing, and question generation. Results show it outperforms prior non-autoregressive and diffusion models and achieves competitive performance with strong pretrained autoregressive models.- It can generate variable length outputs, overcoming a limitation of many non-autoregressive models.In summary, the main contribution is presenting a fully non-autoregressive text diffusion model operating on the logit space with an efficient self-conditioning scheme, and demonstrating its effectiveness on a variety of text generation tasks compared to previous state-of-the-art models.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper proposes TESS, a non-autoregressive text diffusion model that performs diffusion directly on the vocabulary logit space, incorporates efficient self-conditioning to improve sample quality, and demonstrates strong performance on a variety of natural language generation and understanding tasks compared to previous diffusion and non-autoregressive methods.
