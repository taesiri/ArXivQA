# [First Attempt at Building Parallel Corpora for Machine Translation of   Northeast India's Very Low-Resource Languages](https://arxiv.org/abs/2312.04764)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- There are thousands of low-resource and endangered languages spoken globally, especially in India, with little to no computational resources or parallel corpora available. This makes building machine translation (MT) systems for them very challenging. 

- The languages focused on in this paper are 13 very low-resource languages from Northeast India, part of the Sino-Tibetan language family. This area has high linguistic diversity but the languages are severely under-resourced.

Approach:
- The authors created initial parallel corpora between English and the 13 Indian languages using texts from the religious domain crawled from websites. The sentences were aligned to create bitexts.

- Two MT approaches were evaluated on the new bitexts - Transformer neural MT trained from scratch, and fine-tuning the pre-trained M2M100 multilingual MT model.

Results:
- Fine-tuning significantly outperformed Transformer MT, giving average BLEU scores of 13.93 vs 4.30 (en->xx) and 22.67 vs 7.63 (xx->en).

- The Transformer struggled likely due to small corpus sizes, as little as 7k sentences. Fine-tuning leverages transfer learning from higher-resourced languages.

- Translation quality was better from Indian languages to English than vice-versa. More data is needed in the low-resource languages.

Contributions:
- First ever parallel corpora created for 13 extremely low-resource Indian languages, enabling benchmark MT experiments.

- Analysis of two MT approaches highlights fine-tuning as superior for low-resource scenarios, and difficulty of English->xx translation.

- The corpora and benchmarks provide a starting point to facilitate future MT and computational work in these severely under-studied languages.

Future Work:
- Increase corpus sizes by extracting texts from scanned documents where available.

- Evaluate other approaches to further improve translation quality.

- Expand language coverage to additional low-resource languages of India by utilizing Bible website data.


## Summarize the paper in one sentence.

 This paper presents the creation of initial bilingual corpora for thirteen very low-resource languages of Northeast India and provides initial neural machine translation benchmark results for these languages.


## What is the main contribution of this paper?

 Based on my reading of the paper, the main contribution is:

The paper presents the creation of initial bilingual corpora for thirteen very low-resource languages of India, all from Northeast India. It creates the first-ever parallel corpora for these languages and provides initial benchmark neural machine translation results for these languages.

In more detail, the key contributions are:

1) Selecting 13 very low-resource languages from Northeast India and gathering limited parallel corpora from a religious domain for these languages paired with English. This results in the first parallel corpora created for these languages.

2) Preprocessing the corpora and dividing them into train, dev and test sets for machine translation experiments.

3) Conducting initial machine translation experiments between English and each of the 13 languages using transformer and fine-tuning approaches. This provides benchmark translation results for future work.

4) Analyzing the results to show that fine-tuning a pre-trained multilingual model significantly outperforms training transformers from scratch, demonstrating the usefulness of the collected corpora.

5) Stating intentions to expand the corpora and number of languages in future work to cover more low-resource languages of India and integrate them with prior work on African and Native American languages.

In summary, the key contribution is creating the first parallel corpora and benchmark neural machine translation results for 13 very low-resource languages of Northeast India.


## What are the keywords or key terms associated with this paper?

 Based on my review of the paper's content, some of the key terms and keywords associated with this paper include:

- Low-resource languages
- Endangered languages 
- Northeast India
- Parallel corpora
- Machine translation
- Neural machine translation
- Multilingual translation models
- Fine-tuning
- Transformers
- Translation metrics (e.g. BLEU, SacreBLEU)
- Bible/religious text translation
- Language families (Sino-Tibetan, Tibeto-Burman, Tani)

The paper focuses on creating initial parallel corpora for 13 very low-resource languages spoken in Northeast India. It presents results on benchmarking neural machine translation between these languages and English in both directions. Key aspects examined are using transformers versus fine-tuning a large multilingual model, and the challenges of translating to/from very low-resource languages. So the key terms reflect this focus on low-resource machine translation, the specific languages examined, and the machine learning methods and evaluation metrics used.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in this paper:

1. The paper mentions using a Bible-related website as the source for the parallel corpora. What considerations should be made when using religious texts as training data for machine translation models? Could this introduce any biases?

2. The paper aligns sentences between the Indian languages and English using a heuristic alignment method. What are some of the challenges of sentence alignment for low-resource languages? What other alignment methods could be explored? 

3. The paper uses both a transformer model trained from scratch and a pretrained multilingual model. What are the key differences between these two approaches? What are the tradeoffs of each for low-resource language translation?

4. The multilingual model significantly outperforms the transformer model. Why might this be the case? What unique capabilities do multilingual models have that benefit low-resource settings?

5. The paper shows higher BLEU scores when translating into English versus into the Indian languages. What factors might explain this asymmetry in translation quality?

6. The paper mentions varying corpus sizes for each language. How might the corpus size impact translation quality on a per-language basis? Could more data augmentation techniques be utilized?

7. What other pretrained models besides m2m100 could be leveraged for this low-resource translation task? What considerations should go into model selection?

8. The corpus domains are currently limited to religious texts. How could the authors expand into other domains like news, literature, etc to improve diversity? What challenges might this introduce?

9. Beyond BLEU, what other automatic and human evaluation approaches could further validate translation quality for such low-resource languages? 

10. The paper aims to expand the number of languages in future work. As more languages are added, what obstacles arise in terms of model capacity, optimization difficulty, etc? How could these be addressed?
