# [Relightable Neural Human Assets from Multi-view Gradient Illuminations](https://arxiv.org/abs/2212.07648)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question appears to be: 

How can we create high-quality 3D human assets that support both modeling and relighting applications, by combining multi-view stereo (MVS) and photometric stereo (PS) techniques?

The key ideas and contributions seem to be:

- Developing a new capture system called UltraStage that integrates MVS (multi-view stereo) and PS (photometric stereo) to acquire high-quality images of human subjects.

- Capturing a large-scale dataset called UltraStage Dataset with over 2000 human assets under varying viewpoints and illuminations. 

- Proposing methods to process the captured data into "neural human assets" - neural network representations that enable novel view synthesis and relighting.

- Demonstrating applications like high-quality geometry reconstruction, realistically relighting images in the wild, and synthesizing novel views under arbitrary lighting.

- Releasing the dataset and processing tools to the research community.

So in summary, the central hypothesis appears to be that combining MVS and PS in a unified capture system and dataset will enable creating neural assets that support both high-quality modeling and relighting of human subjects. The paper aims to demonstrate this via the proposed capture system, dataset, and neural processing pipeline.


## What is the main contribution of this paper?

 The main contributions of this paper are:

1. Presenting UltraStage, a novel 3D human dataset that combines multi-view stereo (MVS) and photometric stereo (PS) capture to provide high-quality images and surface normals of human subjects under varying viewpoints and illuminations. The dataset contains over 2000 human assets. 

2. Proposing a pipeline to process the captured data into relightable neural human assets that enable novel view synthesis under arbitrary lighting conditions. This involves using the PS normals to train a neural signed distance field for geometry, generating G-buffers with depth-guided texture blending, and optimizing a neural material field.

3. Demonstrating the efficacy of the UltraStage dataset for single image relighting. Using the dataset to train simple networks for albedo and normal prediction leads to more realistic relighting compared to prior arts.

In summary, the key contribution is the introduction of the UltraStage dataset and processing pipeline to obtain high-fidelity relightable neural assets of human subjects. The integration of MVS and PS in the capture system along with the neural processing enables representing very fine details and supporting photorealistic view synthesis and relighting applications. The dataset and tools have significant potential to advance research in human modeling, rendering, and generation tasks.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the key points from the paper:

The paper presents UltraStage, a new 3D human dataset with over 2000 high-quality assets captured under multi-view and multi-illumination settings, which enables extracting detailed normal, albedo, and material maps to reconstruct fine geometry, as well as generating neural human assets for applications like photo-realistic relighting and novel view synthesis that can represent delicate details like cloth wrinkles and textures.


## How does this paper compare to other research in the same field?

 Here are my thoughts on how this paper compares to other research in human modeling and relighting:

- The key contribution is the novel UltraStage dataset and system for joint multi-view stereo (MVS) and photometric stereo (PS) capture of humans. This is quite unique as most prior work focuses on either MVS or PS separately. The combined system allows capturing very high quality geometry, normals, and reflectance. 

- The scale of the dataset, with over 2000 examples and high resolution 8K images, is impressive and larger than most prior human modeling datasets. The diversity of subjects, clothing, and motions also seems much greater than related datasets for relighting or performance capture.

- The proposed neural processing pipeline builds on recent advances like neural radiance fields and neural rendering. Using the MVS+PS data to train high quality neural assets is novel. The results demonstrate substantial improvements in reconstruction and rendering quality over prior neural techniques that use RGB only.

- Using the dataset for single image relighting is a nice application demonstration. The results outperform recent relighting methods, likely due to the realistic and diverse training data. This suggests the dataset could be highly valuable for other relighting and novel view synthesis tasks.

- Compared to other MVS-only datasets, the added PS captures and extracted normals/albedo enable relighting abilities lacking in previous works. Compared to PS-only data, the MVS captures provide wider coverage. So it nicely combines the benefits of both modalities.

Overall, I think the joint MVS+PS capture system is quite innovative. The scale and quality of the dataset seem unmatched by prior human modeling works. And they demonstrate its usefulness for tasks like neural asset creation and single image relighting where it outperforms other state-of-the-art techniques. The work seems like an important contribution that could highly impact future research in this field.


## What future research directions do the authors suggest?

 Based on my reading of the paper, some of the key future research directions the authors suggest are:

- Using the UltraStage dataset for various human modeling and rendering tasks like generation, reconstruction, and neural rendering. The authors mention the dataset could stimulate significant developments in these areas.

- Exploring more advanced neural network architectures and losses for tasks like single image relighting. The authors used simple UNet models in their experiments but suggest more advanced designs could further improve results.

- Applying the dataset to train models for full in-the-wild image relighting. The authors show results on relighting images from an existing dataset, but suggest their data could help train models that generalize better to unconstrained internet images. 

- Using the dataset for virtual try-on, animation, and other emerging applications that require high quality human modeling. The neural assets can provide both geometry and appearance information.

- Combining the photometric stereo and multi-view stereo data in new ways to further improve reconstruction, material acquisition, and novel view synthesis. The authors provide the data from both capture setups.

- Developing new learning-based approaches that directly use the raw multi-view, multi-illumination images for tasks like novel view synthesis. The authors currently extract explicit geometry, normals, albedo but new methods could avoid this.

In summary, the key directions are leveraging this new high-quality dataset to develop better techniques for tasks involving human digitization, modeling, rendering and relighting in both constrained capture and in-the-wild settings.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the key points in the paper:

This paper presents UltraStage, a new 3D human dataset that contains over 2000 high-quality human assets captured under both multi-view and multi-illumination settings. The dataset includes images of people in various poses, clothing, and activities captured by 32 surrounding 8K cameras under 3 different illuminations - one white light and two color gradient illuminations. This allows the extraction of detailed normal, albedo, and material maps as well as high-quality geometry reconstruction. The authors further propose a neural processing pipeline to interpret each capture into a neural human asset, enabling applications like photo-realistic relighting and novel view synthesis. Experiments demonstrate the assets achieve excellent rendering quality and capture details like wrinkles and folds well. The dataset is also shown to enhance single image relighting, where networks trained on the dataset's synthetic data outperform prior arts. Overall, this comprehensive MVS-PS dataset and proposed pipeline aim to advance research in human modeling, reconstruction, and relighting tasks.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the key points in the paper:

This paper presents UltraStage, a new 3D human dataset that combines multi-view stereo (MVS) and photometric stereo (PS) to obtain high quality geometry and appearance information for human subjects. The dataset contains over 2000 human assets captured under 32 viewpoints and 3 different illuminations (white light and two color gradients) per viewpoint. This allows extraction of detailed normal maps, albedo maps, and geometry. The captured data is further processed into neural human assets using neural representations like signed distance fields and deferred rendering techniques. This enables novel view synthesis and relighting under arbitrary lighting conditions while preserving fine details like wrinkles and folds. Experiments demonstrate superiority over traditional MVS and PS reconstruction methods. The dataset is also shown to benefit single image relighting by providing realistic training data.

The UltraStage system uses a large 8m dome with 22,080 controllable LED light sources and 32 synchronized cameras to capture MVS and PS data. Precise calibration is critical and solutions for camera-camera, camera-light calibration and synchronization are provided. Per scene, images under white light help estimate the albedo map while gradient illuminations produce the normal map. These are used to train neural networks for geometry, albedo, normals, and materials. The end result is a neural human asset that allows high fidelity novel view synthesis and relighting. Comparisons to prior work show considerably improved rendering quality and realism. The new dataset and processing pipeline will enable advances in tasks relying on high quality human modeling and rendering.


## Summarize the main method used in the paper in one paragraph.

 Here is a one paragraph summary of the main method used in this paper:

This paper proposes a new 3D human dataset called UltraStage that contains over 2000 high-quality human assets captured under multi-view and multi-illumination settings. For each human subject, it provides 32 surrounding camera views illuminated with one white light and two gradient illuminations. The gradient illuminations help recover detailed surface normal and spatially-varying material maps. The authors further interpret each capture into a neural human asset using a pipeline involving training a neural SDF for geometry, synthesizing novel-view normal/albedo maps via depth-guided reprojection, and optimizing a neural material field. This allows photo-realistic novel view synthesis and relighting with the neural assets. The paper validates the dataset on single image relighting, where training simple normal/albedo prediction networks with their virtually relit data improves relighting quality over prior arts that rely on synthetic data.
