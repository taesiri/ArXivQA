# [Accelerating Learnt Video Codecs with Gradient Decay and Layer-wise   Distillation](https://arxiv.org/abs/2312.02605)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a high-quality paragraph summarizing the key points of the paper:

This paper proposes a novel model-agnostic pruning scheme to reduce the complexity of learned video codecs while maintaining competitive rate-distortion performance. The method combines gradient decay, which enhances parameter exploration during sparsification, and adaptive layer-wise distillation, which regulates the sparse training process through staged knowledge transfer from a pre-trained teacher model. The gradient decay technique progressively decays the surrogate gradients of pruned weights over training iterations, allowing more flexibility early on while stabilizing convergence later. The layer-wise distillation splits the distillation of codec sub-modules into stages rather than the pruning itself, thereby implicitly regularizing the global sparsity target. This approach efficiently updates parameters with less overhead. The method is applied to prune the decoders of three learned video codecs - FVC, DCVC, and DCVC-HEM. Results show the compact models can achieve up to 65% reduction in multiply-accumulate operations and 50% lower latency with less than 0.3dB BD-PSNR drop. The performance-complexity trade-off clearly surpasses other pruned models, demonstrating the capability for practical deployment.
