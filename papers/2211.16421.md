# [RGB no more: Minimally-decoded JPEG Vision Transformers](https://arxiv.org/abs/2211.16421)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question is: can we design neural networks that process images encoded in the frequency domain rather than the spatial domain? 

Specifically, the paper investigates training Vision Transformers (ViTs) directly on JPEG-encoded Discrete Cosine Transform (DCT) coefficients, avoiding the computationally expensive decoding steps to convert to RGB pixels. This allows for much faster data loading and throughput during training and inference.

The key hypotheses tested in the paper are:

1) ViTs are well-suited for processing frequency domain image representations like JPEG DCT, requiring only simple modifications to the patch embedding layer.

2) Data augmentation techniques can be adapted to directly augment images in the DCT domain, avoiding costly conversions to RGB for augmentation.

3) ViTs trained on JPEG DCT can match the accuracy of RGB models while significantly accelerating training and inference due to faster data loading.

So in summary, the central research question is whether neural networks, specifically ViTs, can be effectively trained directly on frequency domain image representations like JPEG DCT to improve efficiency while maintaining accuracy. The paper provides evidence supporting this hypothesis.


## What is the main contribution of this paper?

 The main contributions of this paper are:

- Proposing an approach to train Vision Transformers (ViTs) directly on JPEG-encoded images, avoiding the cost of decoding to RGB pixels. The key insight is that ViTs process images in patches, which matches well with the block-wise structure of JPEG encoding. 

- Designing data augmentations to work directly on JPEG-encoded images, avoiding the cost of decoding to RGB, augmenting, and re-encoding. The authors categorize augmentations into photometric (altering pixel values) and geometric (modifying image geometry).

- Conducting experiments on ImageNet showing their JPEG-trained ViTs match the accuracy of RGB-trained ViTs, while having significantly faster training and inference due to avoiding decoding. For example, their JPEG ViT-Ti model is 39.2% faster for training and 17.9% faster for inference versus the RGB version.

- Proposing and evaluating different patch embedding strategies for accepting JPEG-encoded inputs into a ViT. They find a simple "grouped" strategy works best.

- Performing ablation studies analyzing the impact of different data augmentations applied directly to the JPEG encoding.

In summary, the main contribution is demonstrating the potential for accelerating ViT training and inference by operating directly on JPEG-encoded images, which avoids the overhead of decoding. The modifications to ViT architecture and data augmentation to enable this are secondary contributions.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the key point of the paper:

The paper proposes training vision transformers directly on JPEG encoded images to avoid decoding overhead, using modifications to the patch embedding layer and data augmentations adapted for the frequency domain.
