# [Learning Curves for SGD on Structured Features](https://arxiv.org/abs/2106.02713)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be:How does the structure and covariance of the input feature space influence the dynamics and generalization performance of stochastic gradient descent?More specifically, the authors aim to develop an analytical theory to predict the test loss throughout SGD training for linear models of the form f(x) = w⋅ψ(x). Their theory allows them to explore how properties of the feature map ψ(x) like its second order covariance and fourth order moments interact with hyperparameters like learning rate, batch size, and number of optimization steps to determine the test loss curve. Some key aspects of their investigation include:- Deriving analytical formulae for the expected test loss under both Gaussian feature assumptions and more general feature distributions in terms of the feature second and fourth moments.- Validating their theory experimentally on random feature models and wide neural networks trained on MNIST and CIFAR-10.- Analyzing how structured features like those with power law spectra allow for faster training compared to unstructured isotropic features. - Exploring how batch size affects optimization at fixed compute budgets and how the optimal batch size depends on properties of the feature space.So in summary, the central focus is on formally characterizing how properties of the data distribution and feature space impact generalization dynamics in an analytically solvable linearized model of SGD.
