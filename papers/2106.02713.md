# [Learning Curves for SGD on Structured Features](https://arxiv.org/abs/2106.02713)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be:How does the structure and covariance of the input feature space influence the dynamics and generalization performance of stochastic gradient descent?More specifically, the authors aim to develop an analytical theory to predict the test loss throughout SGD training for linear models of the form f(x) = w⋅ψ(x). Their theory allows them to explore how properties of the feature map ψ(x) like its second order covariance and fourth order moments interact with hyperparameters like learning rate, batch size, and number of optimization steps to determine the test loss curve. Some key aspects of their investigation include:- Deriving analytical formulae for the expected test loss under both Gaussian feature assumptions and more general feature distributions in terms of the feature second and fourth moments.- Validating their theory experimentally on random feature models and wide neural networks trained on MNIST and CIFAR-10.- Analyzing how structured features like those with power law spectra allow for faster training compared to unstructured isotropic features. - Exploring how batch size affects optimization at fixed compute budgets and how the optimal batch size depends on properties of the feature space.So in summary, the central focus is on formally characterizing how properties of the data distribution and feature space impact generalization dynamics in an analytically solvable linearized model of SGD.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contributions appear to be:- Deriving analytical formulas for the expected test error of stochastic gradient descent (SGD) on linear models, for both Gaussian features and arbitrary features. The formulas are expressed in terms of the second moments (covariances) and fourth moments of the feature distributions.- Showing that under certain regularity conditions on the fourth moments, the test error dynamics can be accurately predicted from the second moments alone, allowing the simpler Gaussian theory to be applied.- Demonstrating the accuracy of the Gaussian theory on random feature models and wide neural networks trained on MNIST and CIFAR-10. - Analyzing the effects of minibatch size on the learning dynamics, and showing that small batch sizes are typically optimal for a fixed compute budget. The dependence of optimal batch size on properties of the feature covariance matrix is characterized.- Extending the theory to multi-pass SGD on a fixed training set, and accurately predicting both the training and test error dynamics.So in summary, the main contribution appears to be providing an analytical theory of SGD that reveals how properties of the data distribution, like second and fourth moments of feature correlations, influence the test error dynamics and optimal algorithm hyperparameters like batch size. The theory is shown to be accurate on real image datasets.


## How does this paper compare to other research in the same field?

Here are a few thoughts on how this paper compares to other related work:- This paper presents theoretical analysis of learning curves for SGD, focusing on the effects of data structure and minibatch size. Other theoretical works have studied SGD learning curves, but often make simplifying assumptions about the data distribution being isotropic Gaussian. This paper incorporates more realistic structure into the theory through the use of a general covariance matrix.- The paper demonstrates good agreement between theory and experiments on randomized feature models of MNIST and CIFAR-10. This helps validate the theory and shows it can capture real-world learning curve behavior beyond toy data models. Related work has not always shown convincing empirical support.- The paper studies how optimal minibatch size depends on the spectral properties of the data and proves formal results about stability conditions. Other works like Masters and Luschi (2018) have empirically studied the impact of batch size but without a formal theoretical grounding.- The incorporation of data structure into the theory differentiates this work from analyses that derive convergence rates in terms of general assumptions on the Hessian. It makes more concrete predictions based on the dataset statistics.- The paper models SGD in the lazy training regime of wide neural nets where features are static. Extending the theory to feature learning dynamics during training would relate to other research like Saad and Solla (1995). Overall the paper moves beyond common simplifying assumptions about isotropic data and provides a more refined theoretical characterization of SGD tied closely to properties of the data distribution. The empirical support on real-world datasets also helps advance the practical applicability of such theory.
