# [DiffusionAct: Controllable Diffusion Autoencoder for One-shot Face   Reenactment](https://arxiv.org/abs/2403.17217)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
Neural face reenactment aims to generate synthetic photo-realistic videos that preserve the identity and appearance of a source face, while animating it based on the facial expressions and head movements of a target face from the same or different subject. Existing GAN-based methods often produce distortions and artifacts, while StyleGAN2-based approaches struggle with faithfully reconstructing important appearance details like hair, glasses, etc. 

Proposed Solution: 
The authors propose DiffusionAct, a novel neural face reenactment method based on diffusion probabilistic models (DPMs). It employs a pre-trained autoencoder (DiffAE) with an encoder that predicts a semantic code to represent an image, and a decoder that generates images from codes using denoising diffusion implicit models. The key idea is to learn a "reenactment encoder" conditioned on target face landmarks, to predict a semantic code that retains source identity while transferring target expressions.

Main Contributions:
- First work to adapt a pre-trained DPM for neural face reenactment without requiring expensive model training.
- Novel reenactment encoder optimized using facial landmarks, enabling control over pose/expression transfer.  
- Evaluated on VoxCeleb dataset, shown to generate photo-realistic results without artifacts compared to GAN/StyleGAN methods.  
- Enables accurate appearance/identity preservation and explicit facial pose/expression transfer in a one-shot self/cross-subject setting.
- Achieves state-of-the-art or competitive performance to methods requiring subject-specific fine-tuning.

In summary, DiffusionAct demonstrates the promise of leveraging pre-trained diffusion models for controllable and high-fidelity neural face reenactment while avoiding costly model training. The reenactment encoder allows explicit control over pose/expression transfer without identity/appearance distortions.
