# [SeiT: Storage-Efficient Vision Training with Tokens Using 1% of Pixel   Storage](https://arxiv.org/abs/2303.11114)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question is:

How can we train high-performing vision classifiers at scale while using only a small fraction of the storage required for raw image pixels? 

The key hypothesis is that by representing images as discrete tokens rather than continuous pixel values, it is possible to train classifiers using orders of magnitude less storage without significantly sacrificing accuracy. 

Specifically, the paper proposes a method called Storage-Efficient Vision Training (SeiT), which represents images using a small number of tokens extracted by a pre-trained visual tokenizer. The core ideas are:

- Images can be compressed to sequences of discrete tokens rather than raw pixels, requiring much less storage.

- Tokenizers can be pre-trained to extract semantically meaningful tokens from images.

- Classifiers like Vision Transformers can be trained on token sequences with minimal modifications.

- Simple token-based augmentations like Token-EDA and Embed-Noise can replace pixel-level augmentations.

The central hypothesis is that by training on token sequences, the classifiers will focus less on imperceptible image details and more on semantically relevant features, allowing accurate models using only 1% of pixel storage.

The experiments on ImageNet and other datasets aim to validate this hypothesis, showing SeiT can match or exceed the accuracy of other storage reduction methods using far less storage. The effectiveness for pre-training and continual learning also supports the usefulness of the token-based training paradigm.

In summary, the main research question is how to achieve high accuracy in vision models while drastically reducing the storage needed for training data through tokenization. The paper proposes and validates the SeiT method as an effective approach.


## What is the main contribution of this paper?

 The main contribution of this paper is proposing a storage-efficient vision training method called SeiT that uses tokenized images rather than raw pixels. The key ideas are:

- Tokenizing images using a pre-trained visual tokenizer like VQGAN or ViT-VQGAN. This allows compressing images to a sequence of discrete tokens that requires much less storage than pixels.

- Proposing token-level augmentations like Token-EDA, Token-RRC, Token-CutMix, and Embedding Noise to enable effective training on tokens.

- Introducing a Stem-Adapter module to allow using the tokenized images as input to ViT models with minimal modification.

- Achieving significantly higher accuracy than other storage reduction methods like sampling and JPEG compression when using only 1% of the original ImageNet pixel storage.

- Showing SeiT can be used for efficient pre-training on large datasets like ImageNet-21k and also works well for continual learning.

The main advantage is that by training on discrete tokens rather than continuous pixel values, SeiT removes imperceptible image details that are unnecessary for recognition tasks. This allows drastically reducing the storage for large datasets while maintaining good accuracy. The experiments demonstrate SeiT outperforms other approaches substantially in very low storage regimes.

In summary, the core contribution is developing an efficient training framework that relies only on compact token representations rather than pixel images, enabling scaling up vision models with limited storage infrastructure. The simple but effective techniques make this possible with minimal changes to standard ViT training procedures.
