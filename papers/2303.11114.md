# [SeiT: Storage-Efficient Vision Training with Tokens Using 1% of Pixel   Storage](https://arxiv.org/abs/2303.11114)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is:How can we train high-performing vision classifiers at scale while using only a small fraction of the storage required for raw image pixels? The key hypothesis is that by representing images as discrete tokens rather than continuous pixel values, it is possible to train classifiers using orders of magnitude less storage without significantly sacrificing accuracy. Specifically, the paper proposes a method called Storage-Efficient Vision Training (SeiT), which represents images using a small number of tokens extracted by a pre-trained visual tokenizer. The core ideas are:- Images can be compressed to sequences of discrete tokens rather than raw pixels, requiring much less storage.- Tokenizers can be pre-trained to extract semantically meaningful tokens from images.- Classifiers like Vision Transformers can be trained on token sequences with minimal modifications.- Simple token-based augmentations like Token-EDA and Embed-Noise can replace pixel-level augmentations.The central hypothesis is that by training on token sequences, the classifiers will focus less on imperceptible image details and more on semantically relevant features, allowing accurate models using only 1% of pixel storage.The experiments on ImageNet and other datasets aim to validate this hypothesis, showing SeiT can match or exceed the accuracy of other storage reduction methods using far less storage. The effectiveness for pre-training and continual learning also supports the usefulness of the token-based training paradigm.In summary, the main research question is how to achieve high accuracy in vision models while drastically reducing the storage needed for training data through tokenization. The paper proposes and validates the SeiT method as an effective approach.
