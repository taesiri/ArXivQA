# [Towards Understanding the Capability of Large Language Models on Code   Clone Detection: A Survey](https://arxiv.org/abs/2308.01191)

## What is the central research question or hypothesis that this paper addresses?

Based on my understanding, the central research question this paper aims to address is:How effective are large language models (LLMs) at detecting different types of code clones, and how can prompting strategies and code embeddings be leveraged to enhance their performance?The key hypotheses appear to be:- LLMs can leverage their natural language capabilities for code clone detection, providing a novel approach to this software engineering task.- Introducing intermediate reasoning steps through chain-of-thought prompting can improve LLM clone detection by providing a more structured thought process. - Representing code as vector embeddings enables effective clone detection, with text encoders outperforming specialized code embedding models.The authors comprehensively evaluate these hypotheses by assessing LLMs on clone detection across various dimensions like clone types, prompt formulations, multi-step reasoning, and programming languages. The goal is to gain a nuanced understanding of how prompting strategies and code representations impact LLM clone detection abilities. Their findings aim to guide future research into developing more robust LLM-based techniques for software engineering applications.In summary, this paper centers on elucidating the capabilities of LLMs for code clone detection through prompted reasoning and vector embeddings, to inform the development of enhanced LLM-based methods in this domain. The prompts, analysis, and insights presented provide a valuable benchmark for future work.


## What is the main contribution of this paper?

Based on my understanding, the main contribution of this paper is a comprehensive empirical study evaluating the effectiveness of large language models (LLMs) for automated code clone detection. The key aspects of the contribution are:1. This is the first study to thoroughly assess the capability of existing LLMs like GPT-3.5, GPT-4, LLaMA, etc. in detecting cloned code. It examines their performance from multiple perspectives - across different clone types, programming languages, and prompt formulations.2. The study provides valuable insights into the strengths and limitations of leveraging LLMs for code clone detection through extensive experiments. The main findings are:- LLMs can achieve high recall and accuracy in detecting even complex semantic clones, outperforming existing techniques.  - Introducing intermediate reasoning steps via chain-of-thought prompting leads to noticeable improvements in performance.- Representing code as vector embeddings using text encoders is more effective than specialized models like CodeBERT.- Effectiveness of LLMs varies across programming languages.3. The paper presents the evaluation methodologies and benchmark prompts that can serve as a useful foundation for future research exploring LLM-based code clone detection and other software engineering tasks. 4. The comprehensive set of experiments and their open-sourced code/data provide guidance for developing more robust LLM-based techniques to enhance software engineering.In summary, this paper offers valuable insights into leveraging the natural language capabilities of LLMs for clone detection through rigorous empirical analysis. The benchmarks and findings will inform future research in this emerging area.
