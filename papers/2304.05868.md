# [Mesh2Tex: Generating Mesh Textures from Image Queries](https://arxiv.org/abs/2304.05868)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is: How can we generate realistic mesh textures conditioned on single RGB image queries, even when there is mismatch between the shape geometry and the object in the image?The key ideas and contributions in addressing this question are:- Proposing a hybrid mesh-field texture representation that generates texture as a neural field defined on the barycentric coordinates of the mesh faces. This enables high-resolution texture generation tied to the surface geometry.- Learning a manifold of realistic textures from uncorrelated collections of meshes and real images through adversarial training with differentiable rendering. This allows texture generation and optimization fully conditioned on shape geometry.- Formulating a patch-based perceptual loss using dense correspondence predictions to match textures to real image queries. This provides robustness to geometric and viewpoint mismatches between the shape and image.In summary, the main hypothesis is that a hybrid mesh-field texture representation along with correspondence-guided perceptual optimization can enable generating plausible mesh textures even from single view real images with differing geometry. The experiments aim to validate that this approach outperforms previous texture generation techniques, especially in the challenging scenario of texture transfer from real images.
