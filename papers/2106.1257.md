# [Tight High Probability Bounds for Linear Stochastic Approximation with   Fixed Stepsize](https://arxiv.org/abs/2106.1257)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the main research question this paper addresses is:What are the tight high probability bounds for linear stochastic approximation (LSA) algorithms with a fixed stepsize?Specifically, the authors aim to derive non-asymptotic bounds on the error $|u^\top (\theta_n - \theta^*)|$ between the LSA estimates $\theta_n$ and the true parameter $\theta^*$ that hold with high probability. Their goal is to provide bounds that are tight (optimal) with respect to both the number of iterations n and the stepsize α. The main hypothesis is that under mild assumptions on the stochastic matrices ${\bf A}_n$ and vectors ${\bf b}_n$, high probability bounds of the form:$$|u^\top (\theta_n - \theta^*)| \leq c\left(\sqrt{\alpha} + \text{terms decaying with }n\right)\sqrt{\log(1/\delta)}$$can be derived, where $\delta$ is the failure probability. The authors aim to prove tight bounds of this form and show that the $\sqrt{\alpha}$ dependence cannot be improved in general.So in summary, the key question is what are the fundamental limits on the convergence and concentration properties of LSA algorithms with fixed stepsize, especially the optimal dependence on the stepsize α. The results quantify the inherent variability of LSA due to the randomness in the estimates ${\bf A}_n, {\bf b}_n$.


## What is the main contribution of this paper?

This paper provides a detailed analysis of linear stochastic approximation (LSA) algorithms that aim to find an approximate solution to a linear system where the matrix and vector are unknown but can be accessed through random estimates. The key contributions are:1. It derives high probability bounds on the performance of LSA with fixed stepsize under weaker conditions on the random matrix/vector estimates compared to prior work. Specifically, it shows that with probability at least 1-δ:|u^(θ_n - θ*)| ≤ c{sqrt(α) + α}sqrt(log(1/δ)) + c{ρ^n + αp_0^2}δ^(-1/p_0)where u is a unit vector, θ* is the true solution, ρ<1, c is a constant, p0 = o(α^(-1/4)). 2. It shows the bounds are tight w.r.t. stepsize α and failure probability δ. In particular, it proves that logarithmic dependence on 1/δ is not possible in general. 3. It establishes that as α→0, θ_n converges to a Gaussian distribution with covariance matrix Σ that appears in the central limit theorem for LSA with diminishing stepsize. This shows the leading sqrt(α) term in the bound is sharp.4. It provides a new analysis of products of random matrices that relaxes symmetry assumptions in prior work. This enables handling of general LSA algorithms beyond symmetric cases.Overall, the analysis bridges the gap between asymptotic statistical performance characterization and finite time concentration bounds for LSA. The bounds are much tighter compared to prior non-asymptotic analyses while having wider applicability than asymptotic results.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence TL;DR summary of the paper:The paper provides a detailed analysis of Linear Stochastic Approximation algorithms with fixed stepsize, deriving tight high probability bounds on their performance under mild conditions on the sequence of stochastic inputs.


## How does this paper compare to other research in the same field?

Here are a few key ways this paper compares to other research in the field of linear stochastic approximation:- Assumptions on noise sequence: This paper makes fairly mild assumptions on the noise sequence (Am_n, b_n), requiring boundedness and some regularity conditions. Other works often make stronger assumptions like bounded support, sub-Gaussian noise, etc. So this paper applies more broadly.- Fixed vs diminishing stepsize: Many works focus on the case of diminishing stepsize, where alpha_n goes to 0. This paper analyzes the fixed stepsize setting, which is more challenging but practically relevant. - Sharpness of bounds: The bounds derived in this paper are quite sharp in their dependence on key parameters like the stepsize alpha and the confidence level delta. The authors are able to match the dependence suggested by the central limit theorem. Other works often have more coarse bounds.- Computational challenges: The analysis relies on detailed study of products of random matrices, extending recent work in this area. This allows treatment of the non-symmetric matrix case. Many other works make symmetry assumptions for computational convenience.- Tightness examples: The paper includes examples showing the necessity of the polynomial dependence on delta in the bounds, due to the impossibility of sub-Gaussian tails in this setting. This highlights the tightness of the results.Overall, I would say this paper pushes the analysis of LSA with fixed stepsizes significantly forward through its mild assumptions, sharp bounds matching the central limit theorem, and treatment of challenging computational issues. The tightness examples and connections to asymptotic results are also highlights.
