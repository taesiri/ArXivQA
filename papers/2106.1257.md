# [Tight High Probability Bounds for Linear Stochastic Approximation with   Fixed Stepsize](https://arxiv.org/abs/2106.1257)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the main research question this paper addresses is:What are the tight high probability bounds for linear stochastic approximation (LSA) algorithms with a fixed stepsize?Specifically, the authors aim to derive non-asymptotic bounds on the error $|u^\top (\theta_n - \theta^*)|$ between the LSA estimates $\theta_n$ and the true parameter $\theta^*$ that hold with high probability. Their goal is to provide bounds that are tight (optimal) with respect to both the number of iterations n and the stepsize α. The main hypothesis is that under mild assumptions on the stochastic matrices ${\bf A}_n$ and vectors ${\bf b}_n$, high probability bounds of the form:$$|u^\top (\theta_n - \theta^*)| \leq c\left(\sqrt{\alpha} + \text{terms decaying with }n\right)\sqrt{\log(1/\delta)}$$can be derived, where $\delta$ is the failure probability. The authors aim to prove tight bounds of this form and show that the $\sqrt{\alpha}$ dependence cannot be improved in general.So in summary, the key question is what are the fundamental limits on the convergence and concentration properties of LSA algorithms with fixed stepsize, especially the optimal dependence on the stepsize α. The results quantify the inherent variability of LSA due to the randomness in the estimates ${\bf A}_n, {\bf b}_n$.
