# [Ask2Mask: Guided Data Selection for Masked Speech Modeling](https://arxiv.org/abs/2202.12719)

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper addresses is how to improve masked speech model pre-training by incorporating better data selection within the available unsupervised training data. Specifically, the paper proposes and analyzes two techniques:1) Ask2Mask (ATM): Selects high-confidence frames/regions within each unsupervised speech example based on a separate "scorer" model. The high-confidence regions are then masked and predicted during pre-training. This focuses MSM pre-training on more informative regions.2) ATM with Loss Scaling (ATM+S): Scales the loss during MSM pre-training by the utterance-level confidence from the scorer. This re-weights utterances, focusing training more on high-confidence in-domain examples.The key hypothesis is that guided selection and weighting of training data for MSM pre-training, based on a scorer model, will improve representations and downstream ASR performance, especially under mismatched conditions between pre-training and the target ASR task. The experiments analyze this hypothesis, showing consistent improvements from the proposed ATM and ATM+S techniques.In summary, this paper introduces and analyzes new techniques to improve masked speech model pre-training by incorporating better data selection and weighting within the available unsupervised data. The core hypothesis is that this will improve learned representations and ASR performance when pre-training data does not closely match the target ASR domain.


## What is the main contribution of this paper?

The main contribution of this paper is proposing a novel approach called "Ask2Mask" (ATM) to incorporate data selection within masked speech modeling (MSM) based pre-training methods like wav2vec 2.0 and W2V-BERT. Specifically, the key contributions are:1. ATM uses a lightweight external automatic speech recognition (ASR) model or "scorer" to assign confidence scores to input speech frames. These scores are used to guide the masking process in MSM, focusing on high-confidence frames rather than randomly masking frames as in standard MSM. 2. ATM selects relevant data at two levels:- Fine-grained selection at the frame level by masking high-confidence frames. This allows the MSM model to focus on meaningful representations.- Utterance-level selection by weighting the final MSM loss by the utterance-level confidence score. This weights entire utterances based on relevance.3. Extensive experiments on LibriSpeech and AMI datasets show ATM significantly improves performance of the resulting ASR models, especially under mismatched conditions between training and test data. For example, on AMI data which mismatches the LibriLight pretraining data, ATM achieves over 11% relative WER reduction compared to published results.4. Analysis shows ATM is more robust to varying masking percentages than standard random masking. The high-confidence masking focuses learning on informative samples.5. The proposed techniques are model-agnostic and can be incorporated into any MSM-based pretraining method like wav2vec 2.0 or W2V-BERT.In summary, the key novelty is incorporating data selection into self-supervised speech pretraining through guided masking and loss weighting based on an external scorer model. This improves learned representations and downstream ASR performance, especially under domain mismatch conditions.
