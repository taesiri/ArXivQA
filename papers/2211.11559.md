# Visual Programming: Compositional visual reasoning without training

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central hypothesis of this paper is that large language models can be used to convert natural language instructions into modular and interpretable visual programs, allowing complex visual tasks to be solved without task-specific training. The key ideas are:- Complex visual tasks can be decomposed into simpler steps by generating a visual program from natural language instructions using large language models. - The visual program invokes specialized modules (neural and non-neural) to execute each step. This allows incorporating symbolic reasoning and avoids end-to-end training.- The modular and step-by-step nature makes the model more interpretable. The visual rationale summarizes intermediate outputs at each step.- The flexibility of this approach is shown by applying it to diverse tasks like visual QA, visual reasoning on image pairs, knowledge tagging, and image editing without any task-specific training.So in summary, the central hypothesis is that large language models can generate interpretable and modular visual programs from natural language to solve complex visual tasks without traditional supervised training. The results on diverse tasks support this hypothesis.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contributions appear to be:1. Proposing Visual Programming, a neuro-symbolic approach that uses large language models (LLMs) like GPT-3 to generate modular and interpretable programs from natural language instructions. These programs invoke specialized modules like vision models and python functions to solve complex visual tasks.2. Demonstrating the flexibility of Visual Programming on 4 diverse tasks:   - Compositional visual question answering   - Zero-shot natural language visual reasoning on image pairs   - Factual knowledge object tagging   - Language-guided image editing3. Generating visual rationales - intermediate outputs of each step in the program execution - that provide interpretability and allow error analysis and user feedback to improve performance. 4. Showing strong quantitative results on compositional VQA, zero-shot NLVR, knowledge tagging, and image editing without any task-specific training of the modules or finetuning of the LLM.5. Proposing a modular and extensible framework where new capabilities can be added by simply implementing new modules.In summary, the key ideas are using LLMs for few-shot program generation and execution, demonstrating the flexibility of this approach on diverse visual tasks, and providing interpretability via visual rationales. The core value proposition seems to be expanding the scope of AI systems to complex tasks without needing training data for every new task.
