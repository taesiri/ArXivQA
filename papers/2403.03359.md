# [RACE-SM: Reinforcement Learning Based Autonomous Control for Social   On-Ramp Merging](https://arxiv.org/abs/2403.03359)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
Autonomous on-ramp merging in human-controlled traffic continues to be a challenge for autonomous vehicles. Existing non-learning based solutions rely on rules and optimization, but have limitations in adaptability and ability to handle complex scenarios. Recent deep reinforcement learning (DRL) methods show promise, but often do not adequately consider surrounding vehicles (SVs) and make unrealistic traffic assumptions. Specifically, the parallel-style merging ramp case is rarely considered, despite being common in the real-world.

Proposed Solution: 
The paper proposes RACE-SM, a DRL-based model for acceleration and lane change decisions during parallel-style on-ramp merging. The key innovation is a novel reward function based on social value orientation (SVO) from social psychology to produce courteous merging behavior. The reward function explicitly considers utility to both the ego vehicle and SVs, weighted by the SVO angle. Higher SVO values weight SV utility more, producing more altruistic behavior. The method is evaluated in a simulated highway with cooperative and uncooperative traffic over a range of SVO values.

Contributions:
1) Demonstrates importance of considering SV goals in reward design to enable courteous behavior. Lack of SV consideration led to conflicts/near-misses.

2) SVO-based reward allows tuning ego vehicle behavior from individualistic to altruistic. Central SVO value balanced both objectives best.

3) Matches or exceeds other methods for collision rate. Additionally considers SV interactions and social behavior.

4) Applied to less studied parallel-style merging scenario within single-agent DRL framework and stochastic human-like traffic.

The results highlight the ability of the SVO-based reward function to produce smooth, courteous merging behavior that avoids near-misses and collisions across varying traffic densities. Explicitly considering SVs is key to enabling safe and socially-acceptable performance.
