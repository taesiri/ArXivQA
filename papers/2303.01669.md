# [Learning Common Rationale to Improve Self-Supervised Representation for   Fine-Grained Visual Recognition Problems](https://arxiv.org/abs/2303.01669)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading, the main research question addressed in this paper is: 

How can we improve self-supervised representation learning for fine-grained visual recognition (FGVR) tasks?

The authors motivate this question by pointing out that existing self-supervised learning (SSL) methods seem to have a "coarse-grained bias" and are less effective for FGVR problems where the goal is to distinguish between subtle visual differences. 

Their key hypothesis is that adding an additional "screening mechanism" to identify common discriminative patterns across instances/classes will allow the model to focus on more relevant features for FGVR tasks.

Specifically, they propose:

- Learning a "common rationale detector" by fitting the GradCAM maps from the SSL loss using a branch with limited capacity. This will capture common patterns.

- At test time, using the branch to predict spatial attention weights to selectively aggregate features.

Through experiments on fine-grained datasets, they demonstrate that their method significantly improves over SSL baselines for both retrieval and classification metrics.

In summary, the main research question is how to improve SSL representations for FGVR by learning to focus on common discriminative patterns. Their proposed method of fitting GradCAM with a limited capacity branch is their solution.


## What is the main contribution of this paper?

 Based on my reading, the main contributions of this paper are:

1. It proposes a method to learn an additional screening mechanism on top of self-supervised learning to identify discriminative visual patterns that are common across instances and classes. This is aimed at improving self-supervised learning for fine-grained visual recognition (FGVR). 

2. The proposed method uses a GradCAM fitting branch (GFB) to fit the GradCAM activation map generated from the contrastive self-supervised learning loss. The GFB has limited model capacity, so it tends to capture commonly occurring discriminative patterns or "common rationales". 

3. At test time, the prediction from the GFB is used as a spatial attention mask to perform weighted average pooling over convolutional features. This filters out less common patterns and retains the "common rationales".

4. Through extensive experiments on fine-grained datasets, it shows that the proposed method can significantly improve over baseline self-supervised learning methods like MoCo v2 on both image classification and retrieval tasks.

5. The method provides a simple but effective way to manipulate and improve features learned from self-supervised learning. It demonstrates the value of learning to identify "common rationales" for improving self-supervised learning.

In summary, the key idea is to use the proposed GFB module to identify and retain common discriminative patterns from the self-supervised representations, which is shown to be beneficial for fine-grained visual recognition tasks.
