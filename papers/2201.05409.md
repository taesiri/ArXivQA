# [Progressively Optimized Bi-Granular Document Representation for Scalable   Embedding Based Retrieval](https://arxiv.org/abs/2201.05409)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading, the key contributions and research focus of this paper are:

- It proposes a bi-granular document representation approach to enable efficient and accurate large-scale embedding based retrieval (EBR). The key idea is to represent documents using both lightweight sparse embeddings for efficient candidate retrieval, and heavyweight dense embeddings for accurate post-verification. 

- It introduces a progressive optimization framework that first learns the sparse embeddings for high-quality candidate search, and then learns the dense embeddings conditioned on the candidate distribution for optimized verification accuracy.

- It proposes two techniques - contrastive quantization and locality-centric sampling - to effectively learn the sparse and dense embeddings respectively. Contrastive quantization helps optimize global discrimination for sparse embeddings. Locality-centric sampling generates hard in-batch negatives to optimize local discrimination for dense embeddings.

- Comprehensive experiments on web search and sponsored search benchmarks with up to 1 billion documents demonstrate the effectiveness, efficiency and scalability of the proposed bi-granular representation and optimization techniques. The method also shows strong improvements when deployed on a commercial search platform.

In summary, the central hypothesis is that using bi-granular representations and optimizing them progressively by conditioning the dense embeddings on the candidate distribution from sparse embeddings can enable both efficient and accurate large-scale EBR. The paper provides empirical validation of this hypothesis through extensive experiments.


## What is the main contribution of this paper?

 Based on my reading, the main contributions of this paper are:

1. It proposes a bi-granular document representation framework for large-scale embedding based retrieval. Specifically, it represents documents using both sparse and dense embeddings - sparse embeddings for efficient candidate search, and dense embeddings for accurate post verification. 

2. It introduces a progressive optimization process where sparse embeddings are learned first to optimize global discrimination, then dense embeddings are learned conditioned on the candidate distribution from sparse embeddings to optimize local discrimination.

3. It utilizes contrastive quantization to supervise the learning of sparse embeddings for higher quality search candidates. 

4. It designs a locality-centric sampling strategy to generate hard in-batch negatives when learning dense embeddings, strengthening their capability for local discrimination.

5. It conducts comprehensive experiments on benchmarks like TREC DL 2019 and a billion-scale sponsored search dataset. Results show the proposed method outperforms strong baselines on massive-scale retrieval and also works well on generic retrieval.

6. The method is applied to a major search platform and brings significant gains in revenue, recall rate, and CTR.

In summary, the key contribution is a novel bi-granular document representation that is progressively optimized to achieve strong performance on large-scale embedding based retrieval, in terms of both effectiveness and efficiency. The contrastive quantization and locality-centric sampling also help improve the quality of sparse and dense embeddings respectively.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one-sentence summary of the paper:

The paper proposes a bi-granular document representation method with progressive optimization, using sparse embeddings for scalable candidate search and dense embeddings for accurate post verification, in order to improve embedding-based retrieval for large-scale corpora.


## How does this paper compare to other research in the same field?

 Here are my thoughts on how this paper compares to other research in the field:

- This paper focuses on improving embedding-based retrieval (EBR) for large-scale ad-hoc search. Other recent work has also aimed to scale up EBR, like using partitioning or hybrid storage methods. However, this paper proposes a novel bi-granular document representation approach to handle massive corpus sizes efficiently.

- The key idea of learning separate sparse and dense embeddings for candidate search versus post-verification is quite innovative. Most prior EBR research uses a single unified set of dense embeddings. Learning the embeddings in a staged progressive manner appears to be a unique contribution.

- The techniques introduced like contrastive quantization and locality-centric sampling also seem novel, and help optimize the global vs local discrimination of the sparse and dense embeddings. This is a clever way to tune the objectives of the different embedding sets.

- The experimental results verify strong gains over the state-of-the-art baselines in accuracy, scalability, and efficiency. Applying the method on a commercial search platform and showing significant online improvements is compelling.

- Overall, this paper pushes forward the state-of-the-art in large-scale EBR through its novel bi-granular representation approach and tailored training techniques. The gains over existing methods are clearly demonstrated. This looks like an important advance for scaling up neural retrieval models.

In summary, the core ideas seem highly original compared to prior work, and the paper shows strong empirical results. This appears to be a very solid advancement in handling embedding-based retrieval at scale. The bi-granular representation idea could prove influential for future research and industry systems.


## What future research directions do the authors suggest?

 Based on my reading of the paper, here are some of the key future research directions suggested by the authors:

- Developing more advanced bipartite proximity graph construction techniques to better capture the local structure and difficult negatives for dense embedding learning. The authors suggest exploring techniques like graph neural networks.

- Exploring other sampling strategies besides random walk and snowball sampling during dense embedding learning. The relative merits of different strategies likely depend on factors like corpus size and domain.

- Applying the bi-granular representation framework to other retrieval tasks beyond web search and sponsored search, such as open-domain question answering. The authors believe the framework could be broadly useful.

- Scaling up the training to even larger corpora, with billions or trillions of documents, to handle future growth of web content. New distributed training methods may be needed.

- Reducing the encoding and quantification costs, and accelerating the online serving speed, to handle low-latency retrieval scenarios. Model compression and efficient inference are important.

- Validating the approach on more datasets and domains. The authors used TREC web track and a proprietary sponsored search dataset. More experiments on datasets like MS MARCO would be useful.

- Studying how to effectively integrate bi-granular retrieval with traditional lexical retrieval methods like BM25 within a hybrid search system.

Overall, the main high-level directions are developing more advanced graph-based sampling methods for local structure modeling, scaling up the training, reducing computational costs, and experimentally validating the approach on more datasets and in more application domains. The bi-granular representation seems promising but still needs additional research to fully explore its potential.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the key points from the paper:

The paper proposes a framework called Bi-Granular Document Representation (BiDR) to enable scalable embedding based retrieval from massive corpora. BiDR represents documents using two sets of embeddings - lightweight sparse embeddings used for coarse-grained candidate retrieval, and heavyweight dense embeddings used for fine-grained post verification. The sparse embeddings are learned first via contrastive learning to optimize global discrimination and retrieve high quality candidates. The dense embeddings are then learned conditioned on the candidate distribution from sparse embeddings, using a locality-centric sampling strategy to optimize local discrimination for selecting the final result. This progressive optimization of bi-granular embeddings enables effective and scalable retrieval from corpora at billion-scale. Comprehensive experiments on web search and sponsored search benchmarks show BiDR's advantages in accuracy, efficiency and scalability over state-of-the-art methods. When applied to a commercial search engine, BiDR achieved substantial gains in revenue, recall and CTR. The proposed techniques of contrastive quantization and locality-centric sampling are key innovations that allow optimizing global vs local discrimination for the sparse and dense embeddings respectively.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the key points from the paper:

The paper presents a method for bi-granular document representation to enable scalable embedding based retrieval (EBR) for large-scale corpora. The key idea is to represent documents using both lightweight sparse embeddings and heavyweight dense embeddings. The sparse embeddings are indexed and stored in memory for efficient coarse-grained candidate search. The dense embeddings are stored on disk and used for fine-grained post verification of candidates. 

To optimize this bi-granular representation, the method employs a progressive training process. First, the sparse embeddings are trained with a contrastive loss to optimize global discrimination and ensure high recall in candidate search. Then, conditioned on the candidate distribution from the sparse embeddings, the dense embeddings are trained with locality-centric sampling. This focuses on local discrimination to accurately identify the true relevant document from the candidate set. Experiments on web search and sponsored search datasets with up to 1 billion documents show improvements in accuracy and scalability compared to state-of-the-art baselines. The bi-granular approach also provides efficiency benefits, reaching high accuracy with low latency and memory costs.


## Summarize the main method used in the paper in one paragraph.

 Here is a one paragraph summary of the main method used in the paper:

The paper proposes a bi-granular document representation method for scalable embedding based retrieval. The method represents documents using two sets of embeddings - lightweight sparse embeddings and heavyweight dense embeddings. The sparse embeddings are indexed and stored in memory for efficient coarse-grained candidate search. The dense embeddings are stored on disk and used for fine-grained post verification of candidates. The sparse embeddings are learned first using a contrastive learning approach to optimize global discrimination of documents. The dense embeddings are then learned conditioned on the candidate distribution from the sparse embeddings, using a locality-centric sampling strategy to optimize local discrimination. This allows optimizing the overall retrieval accuracy with the cascade of candidate search and post verification. The bi-granular representation enables handling massive-scale corpus for embedding based retrieval with high accuracy and efficiency.


## What problem or question is the paper addressing?

 Based on my reading, the key points of this paper are:

- It tackles the problem of scaling up embedding-based retrieval (EBR) to handle massive corpora where the ANN index cannot fit entirely in memory. This is an important issue for many real-world search applications.

- It proposes a bi-granular document representation approach with sparse and dense embeddings to address this. The sparse embeddings are lightweight and indexed in memory for coarse candidate search. The dense embeddings are used for fine-grained post verification from disk. 

- A progressive optimization framework is introduced where the sparse embeddings are learned first to optimize global discrimination and cover high quality candidates. Then the dense embeddings are learned conditioned on the candidate distribution to optimize local discrimination for identifying the final result.

- Contrastive quantization is used to learn the sparse embeddings in a supervised way rather than just quantizing the dense embeddings. This improves search accuracy.

- Locality-centric sampling is introduced to generate hard in-batch negatives when learning the dense embeddings. This focuses on local discrimination.

- Experiments on web search and 1 billion sponsored search ads show improvements in accuracy and scalability over state-of-the-art baselines. The method also improved revenue when deployed in a commercial search engine.

In summary, it develops a novel bi-granular document representation and optimization approach to enable accurate and scalable embedding-based retrieval on massive corpora that cannot fit in memory. The techniques for learning the sparse and dense embeddings are tailored for this cascade search process.
