# [Progressively Optimized Bi-Granular Document Representation for Scalable   Embedding Based Retrieval](https://arxiv.org/abs/2201.05409)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading, the key contributions and research focus of this paper are:

- It proposes a bi-granular document representation approach to enable efficient and accurate large-scale embedding based retrieval (EBR). The key idea is to represent documents using both lightweight sparse embeddings for efficient candidate retrieval, and heavyweight dense embeddings for accurate post-verification. 

- It introduces a progressive optimization framework that first learns the sparse embeddings for high-quality candidate search, and then learns the dense embeddings conditioned on the candidate distribution for optimized verification accuracy.

- It proposes two techniques - contrastive quantization and locality-centric sampling - to effectively learn the sparse and dense embeddings respectively. Contrastive quantization helps optimize global discrimination for sparse embeddings. Locality-centric sampling generates hard in-batch negatives to optimize local discrimination for dense embeddings.

- Comprehensive experiments on web search and sponsored search benchmarks with up to 1 billion documents demonstrate the effectiveness, efficiency and scalability of the proposed bi-granular representation and optimization techniques. The method also shows strong improvements when deployed on a commercial search platform.

In summary, the central hypothesis is that using bi-granular representations and optimizing them progressively by conditioning the dense embeddings on the candidate distribution from sparse embeddings can enable both efficient and accurate large-scale EBR. The paper provides empirical validation of this hypothesis through extensive experiments.


## What is the main contribution of this paper?

 Based on my reading, the main contributions of this paper are:

1. It proposes a bi-granular document representation framework for large-scale embedding based retrieval. Specifically, it represents documents using both sparse and dense embeddings - sparse embeddings for efficient candidate search, and dense embeddings for accurate post verification. 

2. It introduces a progressive optimization process where sparse embeddings are learned first to optimize global discrimination, then dense embeddings are learned conditioned on the candidate distribution from sparse embeddings to optimize local discrimination.

3. It utilizes contrastive quantization to supervise the learning of sparse embeddings for higher quality search candidates. 

4. It designs a locality-centric sampling strategy to generate hard in-batch negatives when learning dense embeddings, strengthening their capability for local discrimination.

5. It conducts comprehensive experiments on benchmarks like TREC DL 2019 and a billion-scale sponsored search dataset. Results show the proposed method outperforms strong baselines on massive-scale retrieval and also works well on generic retrieval.

6. The method is applied to a major search platform and brings significant gains in revenue, recall rate, and CTR.

In summary, the key contribution is a novel bi-granular document representation that is progressively optimized to achieve strong performance on large-scale embedding based retrieval, in terms of both effectiveness and efficiency. The contrastive quantization and locality-centric sampling also help improve the quality of sparse and dense embeddings respectively.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one-sentence summary of the paper:

The paper proposes a bi-granular document representation method with progressive optimization, using sparse embeddings for scalable candidate search and dense embeddings for accurate post verification, in order to improve embedding-based retrieval for large-scale corpora.


## How does this paper compare to other research in the same field?

 Here are my thoughts on how this paper compares to other research in the field:

- This paper focuses on improving embedding-based retrieval (EBR) for large-scale ad-hoc search. Other recent work has also aimed to scale up EBR, like using partitioning or hybrid storage methods. However, this paper proposes a novel bi-granular document representation approach to handle massive corpus sizes efficiently.

- The key idea of learning separate sparse and dense embeddings for candidate search versus post-verification is quite innovative. Most prior EBR research uses a single unified set of dense embeddings. Learning the embeddings in a staged progressive manner appears to be a unique contribution.

- The techniques introduced like contrastive quantization and locality-centric sampling also seem novel, and help optimize the global vs local discrimination of the sparse and dense embeddings. This is a clever way to tune the objectives of the different embedding sets.

- The experimental results verify strong gains over the state-of-the-art baselines in accuracy, scalability, and efficiency. Applying the method on a commercial search platform and showing significant online improvements is compelling.

- Overall, this paper pushes forward the state-of-the-art in large-scale EBR through its novel bi-granular representation approach and tailored training techniques. The gains over existing methods are clearly demonstrated. This looks like an important advance for scaling up neural retrieval models.

In summary, the core ideas seem highly original compared to prior work, and the paper shows strong empirical results. This appears to be a very solid advancement in handling embedding-based retrieval at scale. The bi-granular representation idea could prove influential for future research and industry systems.


## What future research directions do the authors suggest?

 Based on my reading of the paper, here are some of the key future research directions suggested by the authors:

- Developing more advanced bipartite proximity graph construction techniques to better capture the local structure and difficult negatives for dense embedding learning. The authors suggest exploring techniques like graph neural networks.

- Exploring other sampling strategies besides random walk and snowball sampling during dense embedding learning. The relative merits of different strategies likely depend on factors like corpus size and domain.

- Applying the bi-granular representation framework to other retrieval tasks beyond web search and sponsored search, such as open-domain question answering. The authors believe the framework could be broadly useful.

- Scaling up the training to even larger corpora, with billions or trillions of documents, to handle future growth of web content. New distributed training methods may be needed.

- Reducing the encoding and quantification costs, and accelerating the online serving speed, to handle low-latency retrieval scenarios. Model compression and efficient inference are important.

- Validating the approach on more datasets and domains. The authors used TREC web track and a proprietary sponsored search dataset. More experiments on datasets like MS MARCO would be useful.

- Studying how to effectively integrate bi-granular retrieval with traditional lexical retrieval methods like BM25 within a hybrid search system.

Overall, the main high-level directions are developing more advanced graph-based sampling methods for local structure modeling, scaling up the training, reducing computational costs, and experimentally validating the approach on more datasets and in more application domains. The bi-granular representation seems promising but still needs additional research to fully explore its potential.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the key points from the paper:

The paper proposes a framework called Bi-Granular Document Representation (BiDR) to enable scalable embedding based retrieval from massive corpora. BiDR represents documents using two sets of embeddings - lightweight sparse embeddings used for coarse-grained candidate retrieval, and heavyweight dense embeddings used for fine-grained post verification. The sparse embeddings are learned first via contrastive learning to optimize global discrimination and retrieve high quality candidates. The dense embeddings are then learned conditioned on the candidate distribution from sparse embeddings, using a locality-centric sampling strategy to optimize local discrimination for selecting the final result. This progressive optimization of bi-granular embeddings enables effective and scalable retrieval from corpora at billion-scale. Comprehensive experiments on web search and sponsored search benchmarks show BiDR's advantages in accuracy, efficiency and scalability over state-of-the-art methods. When applied to a commercial search engine, BiDR achieved substantial gains in revenue, recall and CTR. The proposed techniques of contrastive quantization and locality-centric sampling are key innovations that allow optimizing global vs local discrimination for the sparse and dense embeddings respectively.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the key points from the paper:

The paper presents a method for bi-granular document representation to enable scalable embedding based retrieval (EBR) for large-scale corpora. The key idea is to represent documents using both lightweight sparse embeddings and heavyweight dense embeddings. The sparse embeddings are indexed and stored in memory for efficient coarse-grained candidate search. The dense embeddings are stored on disk and used for fine-grained post verification of candidates. 

To optimize this bi-granular representation, the method employs a progressive training process. First, the sparse embeddings are trained with a contrastive loss to optimize global discrimination and ensure high recall in candidate search. Then, conditioned on the candidate distribution from the sparse embeddings, the dense embeddings are trained with locality-centric sampling. This focuses on local discrimination to accurately identify the true relevant document from the candidate set. Experiments on web search and sponsored search datasets with up to 1 billion documents show improvements in accuracy and scalability compared to state-of-the-art baselines. The bi-granular approach also provides efficiency benefits, reaching high accuracy with low latency and memory costs.


## Summarize the main method used in the paper in one paragraph.

 Here is a one paragraph summary of the main method used in the paper:

The paper proposes a bi-granular document representation method for scalable embedding based retrieval. The method represents documents using two sets of embeddings - lightweight sparse embeddings and heavyweight dense embeddings. The sparse embeddings are indexed and stored in memory for efficient coarse-grained candidate search. The dense embeddings are stored on disk and used for fine-grained post verification of candidates. The sparse embeddings are learned first using a contrastive learning approach to optimize global discrimination of documents. The dense embeddings are then learned conditioned on the candidate distribution from the sparse embeddings, using a locality-centric sampling strategy to optimize local discrimination. This allows optimizing the overall retrieval accuracy with the cascade of candidate search and post verification. The bi-granular representation enables handling massive-scale corpus for embedding based retrieval with high accuracy and efficiency.


## What problem or question is the paper addressing?

 Based on my reading, the key points of this paper are:

- It tackles the problem of scaling up embedding-based retrieval (EBR) to handle massive corpora where the ANN index cannot fit entirely in memory. This is an important issue for many real-world search applications.

- It proposes a bi-granular document representation approach with sparse and dense embeddings to address this. The sparse embeddings are lightweight and indexed in memory for coarse candidate search. The dense embeddings are used for fine-grained post verification from disk. 

- A progressive optimization framework is introduced where the sparse embeddings are learned first to optimize global discrimination and cover high quality candidates. Then the dense embeddings are learned conditioned on the candidate distribution to optimize local discrimination for identifying the final result.

- Contrastive quantization is used to learn the sparse embeddings in a supervised way rather than just quantizing the dense embeddings. This improves search accuracy.

- Locality-centric sampling is introduced to generate hard in-batch negatives when learning the dense embeddings. This focuses on local discrimination.

- Experiments on web search and 1 billion sponsored search ads show improvements in accuracy and scalability over state-of-the-art baselines. The method also improved revenue when deployed in a commercial search engine.

In summary, it develops a novel bi-granular document representation and optimization approach to enable accurate and scalable embedding-based retrieval on massive corpora that cannot fit in memory. The techniques for learning the sparse and dense embeddings are tailored for this cascade search process.


## What are the keywords or key terms associated with this paper?

 Based on my reading, some of the key terms and concepts in this paper include:

- Embedding-based retrieval (EBR): The paper focuses on this technique where deep learning representations and approximate nearest neighbor (ANN) search are combined for large-scale document retrieval.

- Bi-granular document representation: The main contribution is proposing this approach with lightweight sparse embeddings indexed for coarse search, and heavyweight dense embeddings on disk for fine-grained verification. 

- Massive-scale corpus: The paper tackles the challenge of scaling EBR to corpora with billions of documents, where the index is too large to fit in memory.

- Sparse and dense embeddings: The bi-granular representation relies on learning and using both sparse (quantized) and dense vector representations of documents.

- Progressive optimization: The framework proposed learns the sparse embeddings first for candidate search, then dense embeddings are optimized based on that candidate distribution.

- Contrastive quantization: Technique introduced to learn the sparse embeddings with a quantization module, optimizing for retrieval accuracy.

- Locality-centric sampling: Strategy for dense embedding learning that samples hard negatives locally given query-document proximity.

- Global vs local discrimination: Key objectives optimized in stages - first global to retrieve candidates, then local to identify relevant documents from candidates.

- Scalability and efficiency: Evaluations verify the approach scales to billion-scale corpora with low latency and high recall compared to baselines.

In summary, the key focus is using bi-granular embeddings and progressive optimization to enable effective and scalable embedding-based retrieval on massive document collections.


## Based on the paper, what questions should we ask to create a comprehensive summary of it? Please list at least 10 questions.

 Here are 10 potential questions to ask when summarizing the paper:

1. What is the key problem or challenge that the paper aims to address?

2. What is the core proposed method or approach in the paper? 

3. What are the key components or techniques involved in the proposed method?

4. What datasets were used to evaluate the method?

5. What were the main evaluation metrics used? 

6. How did the proposed method perform compared to baseline methods or state-of-the-art approaches?

7. What are the main advantages or merits of the proposed method over existing approaches?

8. Are there any limitations or disadvantages to the proposed method?

9. Did the paper include any ablation studies or analyses to understand the impact of different components?

10. Does the method show promising results for real-world applications based on the paper's experiments and analyses?

Asking these types of questions will help summarize the key points about the paper's problem statement, proposed method, experiments, results, advantages, limitations, and potential real-world impact. The goal is to distill the most important information from the paper into a comprehensive yet concise summary.


## Design 10 in-depth questions about method proposed in this paper and avoid simple question.

 Here are 10 potential in-depth questions about the method proposed in the paper:

1. The paper proposes a bi-granular document representation with sparse and dense embeddings. Why is this representation beneficial for massive-scale embedding based retrieval compared to using just dense embeddings? What are the advantages and disadvantages?

2. The paper claims the conventional method of learning dense embeddings first then quantizing to sparse is "improperly optimized". Why is the conventional method suboptimal? How does the proposed progressive optimization framework address this issue?

3. Contrastive quantization is used for learning the sparse embeddings. How is this different from unsupervised quantization methods? Why is it better suited for optimizing global discrimination of embeddings?

4. Locality-centric sampling is introduced for learning the dense embeddings. How does it help with optimizing local discrimination compared to sampling strategies like ANN sampling? What are the tradeoffs?

5. The paper finds snowball sampling works better than random walk when batch size is large, but random walk is better when batch size is small. What causes this difference? How do the sampling strategies complement each other?

6. What are the challenges in unifying the query encoders learned in the two stages? How does finetuning with the joint InfoNCE loss address these challenges? What is the impact on performance?

7. How does the proposed method balance optimizing global vs local discrimination in the embeddings? Could further improvements be made by somehow jointly optimizing both?

8. The method shows strong gains on billion-scale corpus. How well could it potentially scale to even larger (trillion-scale) corpora? What factors limit scalability?

9. What other retrieval tasks besides web search and sponsored search could the proposed method be applied to? What task characteristics make it more or less suitable?

10. The online evaluation shows gains in revenue, CTR, and recall on a major search platform. What practical deployment concerns need to be addressed to realize these gains? How might the gains translate to other real-world systems?


## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a comprehensive paragraph summarizing the key points of the paper:

This paper proposes a progressively optimized bi-granular document representation for scalable embedding based retrieval. To handle massive-scale corpora that are too large to fit in memory, they represent documents using both sparse and dense embeddings. The sparse embeddings are lightweight, quantized versions that are indexed and stored in memory for efficient candidate retrieval. The dense embeddings are used for fine-grained post-verification of candidates from disk. 

A key contribution is the progressive optimization approach. First, the sparse embeddings are learned via contrastive learning to optimize global discrimination and retrieve high quality candidates. Then, conditioned on this candidate distribution, the dense embeddings are learned using locality-centric sampling to optimize local discrimination of ground truth answers. This ensures the dense embeddings can effectively verify candidates surfaced by the sparse index.

The contrastive quantization method for sparse embeddings minimizes the InfoNCE loss directly, rather than applying post-hoc compression on dense vectors. For dense embeddings, locality-centric sampling constructs a proximity graph over queries and answers based on sparse similarity, allowing concentrated hard negatives. Experiments demonstrate advantages over state-of-the-art methods like ANCE and improvements in recall on large web search and sponsored search datasets.

The proposed techniques enable scalable and high accuracy retrieval. By progressively optimizing bi-granular embeddings for global and local discrimination, the method achieves strong results on massive corpora exceeding memory capacity. The approach demonstrates both effectiveness and practicality for real-world search applications.


## Summarize the paper in one sentence.

 The paper proposes a bi-granular document representation approach for scalable embedding based retrieval, where lightweight sparse embeddings are used for candidate search and dense embeddings are used for post verification.


## Summarize the paper in one paragraphs.

 Here is a one paragraph summary of the paper:

The paper presents a method for scalable embedding-based retrieval called Bi-Granular Document Representation (BiDR). BiDR represents documents using two sets of embeddings - lightweight sparse embeddings for coarse-grained candidate retrieval, and heavyweight dense embeddings for fine-grained post verification. The sparse embeddings are learned first using contrastive learning to optimize global discrimination and retrieve high quality candidates. The dense embeddings are then learned conditioned on the candidate distribution from the sparse embeddings, optimizing for local discrimination to identify the ground truth from candidates. Locality-centric sampling strategies like random walk and snowball sampling are used to facilitate learning the dense embeddings. Experimental results on web search and sponsored search datasets show BiDR achieves substantially higher accuracy than state-of-the-art methods on massive corpora, while maintaining high scalability and efficiency. The method provides an effective solution for scaling up embedding based retrieval through bi-granular document representations and progressive optimization.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 potential in-depth questions about the paper:

1. The paper proposes a bi-granular document representation for scalable embedding based retrieval. Could you explain more about why having separate sparse and dense embeddings is beneficial compared to using just dense embeddings? What are the tradeoffs?

2. The paper introduces a progressive optimization framework where the sparse embeddings are learned first followed by the dense embeddings. Why is this order important? What would happen if the dense embeddings were learned first instead? 

3. Contrastive quantization is used for learning the sparse embeddings. How does this differ from typical quantization methods like product quantization? Why is it better suited for optimizing retrieval accuracy?

4. Locality-centric sampling (LCS) is proposed for learning the dense embeddings. Can you walk through how the bipartite proximity graph is constructed and the random walk/snowball sampling is performed? Why does LCS help with optimizing local discrimination?

5. The paper finds snowball sampling works better for short documents while random walk works better for long documents during LCS. What is the explanation for this behavior? How could the sampling be adapted based on document lengths?

6. Query encoders are unified after separate training. What is the motivation for unifying them? How much performance drop is observed after unification in the experiments?

7. What are the advantages and disadvantages of using HNSW versus IVFADC for indexing in this method? How do they affect efficiency, scalability and accuracy?

8. How does the performance compare between the bi-granular embeddings versus using only dense embeddings on the moderate sized datasets? What does this indicate about applicability in generic scenarios?

9. What are some ways the candidate size N and codebook sizes could be automatically tuned or adapted in this framework? Are there ways to dynamically adjust them based on corpus or hardware constraints?

10. The method is applied to a commercial search engine. What are some real world challenges that need to be addressed before deploying such an approach in production? How could the framework be extended for very large scale deployment?
