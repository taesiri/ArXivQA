# [Neighborhood Attention Transformer](https://arxiv.org/abs/2204.07143)

## What is the central research question or hypothesis that this paper addresses?

 The central research question this paper addresses is how to design an efficient and scalable sliding window attention mechanism for vision models. Specifically, the paper proposes Neighborhood Attention (NA) as a new type of localized attention pattern that:

- Localizes each pixel's attention span to its nearest neighbors, resulting in linear complexity instead of the quadratic complexity of standard self-attention. This makes NA more scalable.

- Approaches self-attention as the neighborhood size grows, unlike prior sliding window methods like SASA.

- Maintains translational equivariance, unlike blocked attention patterns like Swin's windowed self-attention. This helps introduce useful inductive biases.

- Can be implemented efficiently, allowing NA models to run even faster than Swin despite having a more localized attention pattern. This is enabled by the NATTEN Python package for NA developed in the paper.

The overall hypothesis is that NA strikes a better tradeoff between efficiency, scalability, and accuracy compared to prior attention mechanisms for vision models. The paper explores this through introducing Neighborhood Attention Transformer (NAT), evaluating it on image classification and downstream vision tasks, and comparing it to previous attention-based models like Swin and ViT.

The key innovation is the NA mechanism itself, which provides a more efficient and flexible way to incorporate localized attention patterns into vision models. The paper shows this allows building models like NAT that outperform prior methods under similar model size and computation constraints.


## What is the main contribution of this paper?

 The main contribution of this paper is proposing Neighborhood Attention (NA), a new efficient sliding window attention mechanism for vision. Specifically:

- NA localizes self-attention to a neighborhood around each pixel/token, reducing computational complexity from quadratic to linear while introducing useful inductive biases like locality. 

- The authors develop an extension called NATTEN with optimized CUDA/C++ kernels that allow NA layers to run faster than Swin Transformer's windowed self-attention, while using less memory.

- They propose the Neighborhood Attention Transformer (NAT), a hierarchical vision transformer using NA, which achieves strong results on image classification, object detection, and semantic segmentation. For example, NAT-Tiny reaches 83.2% top-1 accuracy on ImageNet with only 4.3 GFLOPs, outperforming similarly sized Swin and ConvNeXt models.

In summary, the key contribution is proposing NA as an efficient alternative to existing attention mechanisms like self-attention and windowed self-attention, demonstrating its effectiveness on vision tasks, and releasing an optimized implementation to facilitate further research. NA helps make attention-based models more practical for computer vision.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the key points from the paper:

The paper proposes Neighborhood Attention (NA), an efficient sliding window attention mechanism for vision that localizes self-attention to nearest neighboring pixels, maintains translational equivariance, and approaches full self-attention as the window size increases; they develop the NATTEN Python package with fast C++/CUDA kernels for NA that outperforms Swin Transformer's attention, and introduce the Neighborhood Attention Transformer (NAT) model using NA that achieves state-of-the-art results on image classification and downstream vision tasks.


## How does this paper compare to other research in the same field?

 Here are a few key points comparing this paper to other related works in hierarchical vision transformers and attention mechanisms:

- The main contribution is proposing Neighborhood Attention (NA), which localizes self-attention to a neighborhood around each pixel. This is similar to previous works like Stand-Alone Self Attention (SASA) and sliding window attention in terms of using a localized window, but NA has some advantages. It approaches full self-attention as the window size increases and maintains translational equivariance, unlike blocked or windowed attention.

- The paper argues that previous works like Swin Transformer avoided explicit sliding window attention like SASA due to efficiency concerns. They address this by developing an efficient C++/CUDA implementation of NA called NATTEN, which allows NA to run faster than Swin's Windowed Self Attention.

- They propose Neighborhood Attention Transformer (NAT), a hierarchical vision transformer using NA. This is similar to other hierarchical transformers like Swin and PVT, but uses NA instead of windowed attention.

- Experiments show NAT outperforms Swin and ConvNeXt transformers in image classification on ImageNet with similar model size/FLOPs. It also achieves strong performance on object detection and segmentation.

- Overall, NA and NAT seem to provide better localization and inductive biases than windowed attention in Swin, while maintaining efficiency and performance. The localized attention helps for tasks like segmentation.

- Compared to other works on improving vision transformers like CvT and ViL, this paper specifically focuses on developing more effective attention mechanisms over convolutions or tokens. The NA design is the main novelty.

In summary, this paper makes contributions in efficient localized attention for vision transformers, challenging prior notions about inefficiency. The NA design and NATTEN implementation help advance research in this direction. NAT demonstrates these benefits over baselines like Swin and ConvNeXt in major vision tasks.
