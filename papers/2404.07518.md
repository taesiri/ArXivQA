# [Remembering Transformer for Continual Learning](https://arxiv.org/abs/2404.07518)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Neural networks suffer from catastrophic forgetting (CF) in continual learning scenarios. When learning new tasks sequentially, new task knowledge interferes with and overrides previously learned knowledge. This results in the network forgetting how to solve previous tasks.

- Existing continual learning methods have drawbacks like requiring task identity at test time, not being parameter efficient, or not working well in class incremental settings.

Method:
- The paper proposes Remembering Transformer, a parameter efficient transformer architecture for continual learning. It is inspired by the Complementary Learning Systems (CLS) theory in neuroscience about how human brains learn continually.

- The model uses a mixture of adapters attached to a pretrained vision transformer (ViT). The adapters help store task-specific knowledge. 

- A collection of generative autoencoders (AE) is used to route input samples to relevant adapters by detecting the novelty of inputs. The AE with minimum reconstruction loss on the input encodes the most similar distribution, hence its corresponding adapter is selected.

- Knowledge distillation transfers knowledge from old adapters to a new adapter when number of adapters reaches capacity. Old rarely used adapters are removed.

Contributions:
- Establishes new state-of-the-art for class incremental learning accuracy in vision without needing task identities at test time

- Very parameter efficient compared to other methods

- Can learn continually even when number of tasks exceeds model's adapter capacity

- Extensive experiments on CIFAR datasets demonstrating superiority over a wide range of existing continual learning techniques
