# [Knowing Your Nonlinearities: Shapley Interactions Reveal the Underlying   Structure of Data](https://arxiv.org/abs/2403.13106)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Modern neural networks exhibit complex non-linear behaviors that are difficult to interpret. The commonly used Shapley values for model interpretation rely on a linearity assumption that is invalid for neural networks.

- Shapley interaction values can quantify the degree of non-linear interaction among features, but calculating them exactly is prohibitively expensive. 

Proposed Solution:
- Use the Shapley Taylor Interaction Index (STII) to efficiently approximate pairwise feature interactions in neural networks. 

- Apply STII analysis across language, speech and vision models to understand how feature interactions correlate with known linguistic, phonetic and visual structure in the data.

Key Contributions:

- Show MLMs rely more on syntax while ALMs rely more on positional distance for determining feature interactions. MWEs are less compositionally handled in MLMs and distant context ALMs.

- Speech model interactions reflect that vowel acoustics depend heavily on consonant contexts. More vowel-like consonants have higher interactions like vowels do.

- Image classifiers have lower interactions at object boundaries since perturbations blend into edge artifacts. Nearby edge pixels interact most with foreground pixels.

- Demonstrate interpretability benefits from domain knowledge across NLP, speech and vision. Establish feature interactions as a tool for understanding model representations and inductive biases.

In summary, the paper proposes using STII to approximate neural network feature interactions. It shows across three modalities that these interactions correlate with and provide insight into known structure underlying the data. This demonstrates the importance of grounding interpretability in domain expertise.
