# [Jurassic is (almost) All You Need: Few-Shot Meaning-to-Text Generation   for Open-Domain Dialogue](https://arxiv.org/abs/2110.08094)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research questions/hypotheses appear to be:1) Can few-shot prompt-based learning create high-quality, semantically-controlled natural language generators (NLGs) for open-domain dialogue systems? 2) Can few-shot prompting allow language models like GPT-Neo and Jurassic-1 to generate conversational responses directly from meaning representations (MRs) that generalize well to new domains and unseen entities/relations?3) How does the quality and coverage of neural NLGs created via prompt-based learning compare to existing template-based NLGs for movies, music, sports, TV, and video games in the Alexa Prize dialogue system Athena?4) Can few-shot prompting control the dialogue acts realized in generated responses, when the meaning representations specify dialogue acts like in the Viggo dataset?5) How well do automatic metrics like BLEURT correlate with human evaluations of coherence and semantic accuracy for prompt-based NLGs?The central hypothesis appears to be that few-shot prompt-based learning can reliably create high-quality NLGs for open-domain dialogue systems, that generalize across domains and meaning representations. The experiments seem designed to test this hypothesis by evaluating both automatic and human metrics.
