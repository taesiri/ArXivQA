# [Reference-based Metrics Disprove Themselves in Question Generation](https://arxiv.org/abs/2403.12242)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Reference-based metrics like BLEU and BERTScore are widely used to evaluate question generation (QG) models. However, using only human-written references cannot guarantee the effectiveness of these metrics. 
- Most QG benchmarks only have one reference question. The paper collects another set of human-written references and shows that reference-based metrics still cannot reliably distinguish between high-quality and flawed candidate questions.

Proposed Solution:  
- The paper proposes NACo, a multi-dimensional reference-free metric for evaluating QG. It consists of scores for:
  - Naturalness - how natural the question sounds
  - Answerability - whether the question is grounded to the given answer
  - Complexity - the reasoning required to answer the question
- NACo utilizes large language models (LLMs) and their chain-of-thought reasoning process to directly measure these criteria.

Key Contributions:
- Collects additional human-written references for QG benchmarks and shows unreliability of reference-based metrics.  
- Proposes NACo metric that evaluates essential criteria of a question not addressed by reference-based metrics.
- Experiments show NACo effectively distinguishes valid and invalid questions. It aligns better with human judgment than reference-based metrics.

In summary, the paper demonstrates flaws in existing reference-based QG evaluation metrics, and introduces a novel reference-free metric, NACo, that focuses on multi-dimensional quality aspects of questions. Experiments validate that NACo better reflects question quality per human assessment.


## What is the main contribution of this paper?

 The main contributions of this paper include:

1. Producing an additional set of human-written questions for current QG benchmarks, and showing the unreliability of reference-based metrics in reflecting the quality of generated questions.

2. Proposing NACo, a novel evaluation metric bridging the gap between human assessment and automated evaluation by assigning scores to three criteria of a good question: naturalness, answerability, and complexity.

3. Demonstrating through experiments and human evaluation that NACo better aligns with human judgment of a good question than reference-based metrics for question generation.


## What are the keywords or key terms associated with this paper?

 Based on the LaTeX source code provided, some of the key terms and keywords associated with this paper include:

- Question generation (QG)
- Reference-based metrics (such as BLEU, BERTScore)
- Evaluation metrics
- Benchmark datasets (SQuAD, HotpotQA)
- Human evaluation 
- Large language models (LLMs)
- Chain-of-thought reasoning
- Essential criteria for good questions (naturalness, answerability, complexity)
- Reference-free metrics
- Proposed metric: NACo

The paper discusses issues with using reference-based metrics for evaluating question generation systems, proposes collecting additional reference questions to reveal these issues, and introduces a new reference-free metric called NACo that evaluates questions along the dimensions of naturalness, answerability, and complexity. Experiments compare performance of NACo against existing metrics on model-generated and human-written questions using SQuAD and HotpotQA datasets.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in this paper:

1. How does the paper demonstrate the failure of reference-based metrics for question generation evaluation? What new data does it collect and analyze to support this argument?

2. What are the three key criteria proposed for evaluating question quality - naturalness, answerability and complexity? How does the paper define and measure each of these?  

3. How does the proposed NACo metric leverage large language models and their chain-of-thought capabilities for evaluating the three question criteria? What is the process it follows?

4. Why does the design of NACo not require access to multiple reference questions, unlike most existing metrics? How does it determine expected complexity without multiple references?

5. What are some ways the paper validates that NACo better correlates with human judgment than previous metrics? What analysis does it provide?  

6. What are the limitations discussed for NACo regarding required access to some reference examples and reliance on the QA model? How can future work address this?

7. How does the paper produce the additional human-written questions for existing datasets to demonstrate issues with reference-based metrics? What instructions are given?

8. What other less desirable sets of questions are collected in the HotpotQA analysis? How do they differ from valid questions in key aspects?

9. How do the paper experiments reveal issues with existing metrics in distinguishing between new valid references and invalid candidates? What specifically do the metrics fail at?

10. How much higher correlation with human judgement does NACo achieve over the current state-of-the-art RQUGE metric? What correlation metrics are used to measure this?
