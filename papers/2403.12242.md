# [Reference-based Metrics Disprove Themselves in Question Generation](https://arxiv.org/abs/2403.12242)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Reference-based metrics like BLEU and BERTScore are widely used to evaluate question generation (QG) models. However, using only human-written references cannot guarantee the effectiveness of these metrics. 
- Most QG benchmarks only have one reference question. The paper collects another set of human-written references and shows that reference-based metrics still cannot reliably distinguish between high-quality and flawed candidate questions.

Proposed Solution:  
- The paper proposes NACo, a multi-dimensional reference-free metric for evaluating QG. It consists of scores for:
  - Naturalness - how natural the question sounds
  - Answerability - whether the question is grounded to the given answer
  - Complexity - the reasoning required to answer the question
- NACo utilizes large language models (LLMs) and their chain-of-thought reasoning process to directly measure these criteria.

Key Contributions:
- Collects additional human-written references for QG benchmarks and shows unreliability of reference-based metrics.  
- Proposes NACo metric that evaluates essential criteria of a question not addressed by reference-based metrics.
- Experiments show NACo effectively distinguishes valid and invalid questions. It aligns better with human judgment than reference-based metrics.

In summary, the paper demonstrates flaws in existing reference-based QG evaluation metrics, and introduces a novel reference-free metric, NACo, that focuses on multi-dimensional quality aspects of questions. Experiments validate that NACo better reflects question quality per human assessment.
