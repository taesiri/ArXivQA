# [CoEdIT: Text Editing by Task-Specific Instruction Tuning](https://arxiv.org/abs/2305.09857)

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper addresses is how to improve the capabilities of instruction-tuned large language models for text editing by leveraging diverse task-specific data. Specifically, the authors aim to investigate:1) If fine-tuning a standard instruction-tuned language model on a diverse set of text editing tasks and instructions can improve performance across a wide variety of text editing benchmarks (RQ1).2) If such a model can generalize to perform high-quality edits for new, unseen types of instructions, including compositional instructions with combinations of tasks (RQ2). 3) If the model can make the text revision process more efficient and effective for human writers (RQ3).The main hypothesis seems to be that creating a dense space of focused text editing tasks and instructions will allow instruction-tuned models to better generalize to new text editing tasks and composite instructions, compared to models trained on more sparse, general instructions. The authors suggest current LLMs may be underfitted for specialized tasks like text editing.
