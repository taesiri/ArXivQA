# [CoEdIT: Text Editing by Task-Specific Instruction Tuning](https://arxiv.org/abs/2305.09857)

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper addresses is how to improve the capabilities of instruction-tuned large language models for text editing by leveraging diverse task-specific data. Specifically, the authors aim to investigate:1) If fine-tuning a standard instruction-tuned language model on a diverse set of text editing tasks and instructions can improve performance across a wide variety of text editing benchmarks (RQ1).2) If such a model can generalize to perform high-quality edits for new, unseen types of instructions, including compositional instructions with combinations of tasks (RQ2). 3) If the model can make the text revision process more efficient and effective for human writers (RQ3).The main hypothesis seems to be that creating a dense space of focused text editing tasks and instructions will allow instruction-tuned models to better generalize to new text editing tasks and composite instructions, compared to models trained on more sparse, general instructions. The authors suggest current LLMs may be underfitted for specialized tasks like text editing.


## What is the main contribution of this paper?

The main contribution of this paper is introducing CoEdIT, a text editing system for writing assistance that uses natural language instructions. The key highlights are:- CoEdIT achieves state-of-the-art performance on multiple text editing benchmarks like grammatical error correction, text simplification, iterative text editing, and stylistic editing tasks. - Even the smallest CoEdIT model with densified instruction tuning outperforms larger supervised text editing models, instruction-tuned models, and general purpose LLMs with nearly 60x more parameters. This shows the importance of dense task/instruction space over simply scaling model size.- CoEdIT generalizes well to new, unseen tasks not seen during training as well as composite instructions with multiple task specifications. - CoEdIT is fully open-sourced with code, data, and models publicly available to support open research.- The authors build CoEdIT by fine-tuning a sequence-to-sequence model on a parallel corpus of 82K instruction-based input-output pairs sourced from text editing datasets. The instructions are constructed using lexical and semantic variations.In summary, the main contribution is introducing CoEdIT, an open-sourced text editing system that achieves state-of-the-art performance by leveraging instruction tuning on diverse text editing tasks and generalizes well to new tasks and composite instructions.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper: The paper introduces CoEdIT, a text editing system that leverages instruction tuning of large language models on diverse text editing benchmarks to achieve state-of-the-art performance on multiple text editing tasks while being competitive with much larger publicly available models.
