# [Rejection Improves Reliability: Training LLMs to Refuse Unknown   Questions Using RL from Knowledge Feedback](https://arxiv.org/abs/2403.18349)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Large language models (LLMs) often generate erroneous, "hallucinated" outputs when asked questions beyond their actual knowledge. This happens because they try to answer questions without knowing their own knowledge limitations.
- Evaluating model "honesty" is challenging due to the lack of true knowledge labels and differing knowledge capacities of models. Traditional classification metrics are unsuitable.

Proposed Solution:
- Introduce a new concept of model "reliability" from the user's perspective - maximizing correct answers while minimizing errors. 
- Propose new metrics like accuracy, accountability and overall reliability score to evaluate both model helpfulness and honesty.
- Present a "Reinforcement Learning from Knowledge Feedback (RLKF)" framework that leverages knowledge feedback to teach models when to refuse answering unknown questions. This helps align models better with their knowledge limitations.

Main Contributions:
- Formalize the notion of reliability for language models and define quantitative metrics to measure it. 
- Propose a novel RLKF framework to improve language model reliability using reinforcement learning and tailored reward models.
- Demonstrate through experiments that RLKF significantly enhances model reliability for both in-domain and out-of-domain test cases.

The key insight is to align models with their knowledge limitations, rather than just maximize correctness. The RLKF framework equips models to know what they don't know.
