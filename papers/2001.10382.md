# [Selective Weak Supervision for Neural Information Retrieval](https://arxiv.org/abs/2001.10382)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper abstract, the central research question seems to be how to effectively train neural information retrieval (Neu-IR) models in scenarios where large-scale relevance training data is not available. The key ideas proposed to address this are:1) Leveraging anchor text-document pairs as a source of "weak supervision" to train Neu-IR models, based on the intuition that anchor texts are similar to queries. 2) Using a reinforcement learning approach called "ReInfoSelect" to selectively pick good anchor-document pairs to train the Neu-IR model, rather than using all anchor pairs which can be noisy.3) Iteratively training the data selection module ("ReInfoSelect") and the Neu-IR model using policy gradients, with the goal of maximizing the Neu-IR model's accuracy on a validation set of true query-document relevance labels.So in summary, the central hypothesis is that anchor text can be used as a weak supervision signal to train Neu-IR models effectively, if noisy anchor pairs are filtered out selectively using a reinforcement learning approach tailored to maximize relevance matching accuracy.


## What is the main contribution of this paper?

Based on my reading of the abstract, the main contribution of this paper seems to be proposing a new method called ReInfoSelect for training neural information retrieval models using weak supervision from anchor text data. The key ideas are:- Revisiting the classic IR intuition that anchor text-document pairs can approximate query-document relevance for training. - Using a reinforcement learning approach called ReInfoSelect to selectively pick good anchor text-document pairs to train the neural ranker, where the reward signal is the ranking performance on a small set of query-document relevance labels.- Iteratively training the data selection policy network and the neural ranker model using policy gradients until the neural ranker's performance converges.- Showing that neural rankers trained with ReInfoSelect and only publicly available anchor data can match or exceed the effectiveness of models trained on private commercial search log data across several TREC benchmarks.So in summary, the main contribution seems to be proposing a novel reinforcement learning based method for selectively using noisy anchor text data to train high quality neural ranking models without needing a large commercial search log. This helps "democratize" neural IR by reducing its dependence on large-scale private labeled data.


## How does this paper compare to other research in the same field?

Here are some key ways this paper compares to other related research:- Most prior work on neural information retrieval (Neu-IR) has relied on large amounts of labeled training data, like search logs or expert relevance judgments. This paper proposes a method to train Neu-IR models with only weak supervision from anchor texts and linked web documents, which are more readily available.- Previous methods using anchor texts for IR tasks like query expansion treated them as additional text features. This paper proposes a novel reinforcement learning framework, ReInfoSelect, to selectively choose high-quality anchor-document pairs to directly supervise Neu-IR models.- The proposed approach outperforms prior weakly supervised Neu-IR methods and achieves effectiveness rivaling models trained on private commercial search logs, using only publicly available data. This could expand the applicability of Neu-IR to academic and non-web search settings lacking large labeled datasets.- Analysis shows ReInfoSelect adapts its selection strategy based on the training state of the target ranker, being more lenient with noisy data initially and more selective as training progresses. This demonstrates learning when and how to effectively use weak supervision.- Results are demonstrated on two state-of-the-art Neu-IR models, Conv-KNRM and BERT Ranker. The ability to train both shallow and deep neural architectures with weak supervision expands the potential usage.- The paper provides novel analysis into data efficiency, fine-tuning strategies, and behaviors of Neu-IR models by alleviating the training data bottleneck. This could inform future research directions.Overall, this paper makes important contributions in training Neu-IR without large labeled datasets by intelligently selecting public data. The proposed ideas could help transition Neu-IR techniques to broader applications lacking massive labeled data.
