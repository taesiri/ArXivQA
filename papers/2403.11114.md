# [Phasic Diversity Optimization for Population-Based Reinforcement   Learning](https://arxiv.org/abs/2403.11114)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Maintaining population diversity is important in population-based reinforcement learning (RL) to avoid convergence to local optima. However, most current algorithms optimize a multi-objective loss function that balances reward and diversity. This can result in conflicts between the two objectives.

- Using multi-armed bandits (MAB) algorithms to select the tradeoff coefficient between reward and diversity has limitations due to the non-stationary reward distribution during training.

- Computing the determinant for measuring diversity has computational challenges when the similarity kernel matrix becomes singular, which commonly occurs due to policy replication between agents.

Proposed Solution: 
- Introduce a Phasic Diversity Optimization (PDO) algorithm that separates optimization into two phases - reward optimization and auxiliary diversity optimization.  

- Maintain an archive to store best performing policies. Use the archive policies for diversity optimization without replacing better performing policies.

- Propose differentiable similarity estimation between policies using probability distances like Jensen-Shannon divergence or Wasserstein distance. Construct positive definite kernel matrix.

- Introduce regularization technique to enable Cholesky decomposition and gradient computation when kernel matrix becomes singular.

Main Contributions:
- Decouple reward and diversity optimization through the two-phase approach to avoid conflicts.

- Diversity optimization in the auxiliary phase does not affect better performing policies stored in the archive. Allows more aggressive diversity optimization.  

- Differentiable similarity estimation technique to compute policy diversity using probability distances.

- Regularization method to handle singular kernel matrix issue commonly occurring during policy replication.

The proposed PDO algorithm is demonstrated to achieve better performance on both a novel dogfight environment and MuJoCo continuous control tasks compared to prior population diversity algorithms.
