# [Self-supervised Learning from a Multi-view Perspective](https://arxiv.org/abs/2006.05576)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading, the central research question addressed in this paper is: How can we theoretically understand and explain the effectiveness of self-supervised learning methods? 

Specifically, the paper provides an information-theoretical framework and analysis to shed light on why popular self-supervised learning approaches like contrastive learning and predictive learning work well, even without access to labels or downstream task supervision during training. 

The key ideas and analysis include:

- Modeling the input data and self-supervised signals as two redundant "views" of the data under a multi-view assumption. This allows connecting to multi-view representation learning frameworks.

- Formalizing notions of task-relevant and task-irrelevant information, and showing how self-supervised objectives can extract the former and discard the latter, under this multi-view assumption.

- Demonstrating how contrastive learning aims to maximize mutual information between representations and self-supervised signals, thus extracting task-relevant information. 

- Showing how predictive learning objectives perform log conditional likelihood maximization, also extracting task-relevant information from a different angle.

- Introducing a new "inverse predictive learning" objective to discard task-irrelevant information.

- Providing a theoretical analysis to quantify the information extracted and discarded, and connect it to downstream performance.

- Conducting controlled experiments on visual and visual-textual representation learning to support the theoretical intuitions.

In summary, the central hypothesis is that under a multi-view redundancy assumption, self-supervised learning can extract task-relevant and discard task-irrelevant information, which explains its empirical success - and this is validated theoretically and experimentally.


## What is the main contribution of this paper?

 Based on my reading, the main contributions of this paper are:

1. Provides an information-theoretical framework to analyze self-supervised learning (SSL) under a multi-view assumption, where the input and self-supervised signals are seen as two redundant views of the data. 

2. Demonstrates theoretically that under the multi-view assumption, the SSL objectives can extract task-relevant information and discard task-irrelevant information from the input, even without access to downstream tasks. Specifically, it shows the learned representations can extract all task-relevant information with a potential loss and discard task-irrelevant information with a fixed gap.

3. Connects the theoretical framework with practical SSL objectives like contrastive and predictive learning, showing they aim to extract task-relevant and discard task-irrelevant information. Also proposes a new inverse predictive learning objective to discard task-irrelevant information.  

4. Introduces a composite SSL objective that combines predictive, contrastive and inverse predictive objectives to simultaneously extract task-relevant and discard task-irrelevant information.

5. Provides controlled experiments on visual and visual-textual representation learning to support the analysis and compare different compositions of SSL objectives.

In summary, the key contribution is the information-theoretical framework to analyze SSL under the multi-view assumption, which provides theoretical justifications for the efficacy of SSL objectives and sheds light on designing improved SSL methods. The experiments support the analysis empirically.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the key points from the paper:

The paper presents an information-theoretical framework to understand self-supervised learning from a multi-view perspective, demonstrating how contrastive and predictive learning objectives can extract task-relevant and discard task-irrelevant information even without access to downstream tasks.
