# [AutoCLIP: Auto-tuning Zero-Shot Classifiers for Vision-Language Models](https://arxiv.org/abs/2309.16414)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be: How can we automatically tune the weights of different prompt templates used in zero-shot classifiers built on vision-language models like CLIP, in order to improve their classification performance?The key hypothesis appears to be that weighting prompt templates by their similarity to the input image embedding will allow the model to better leverage prompts that are more relevant for describing the visual input.In summary, the paper proposes and evaluates a method called AutoCLIP to auto-tune the weights of prompt templates per input image in a zero-shot classifier, with the goal of improving accuracy by giving higher weight to more relevant prompts for each image. The central hypothesis is that this auto-tuning of prompt template weights can consistently improve zero-shot classification performance across different models, datasets and prompt strategies.
