# [Generative Pretrained Structured Transformers: Unsupervised Syntactic   Language Models at Scale](https://arxiv.org/abs/2403.08293)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Existing syntactic language models (SLMs) have limitations such as relying on gold parse trees or sequential training, preventing them from being scaled up and pre-trained from scratch on large corpora. 

Proposed Solution:
- The authors propose Generative Pretrained Structured Transformers (GPST), an unsupervised SLM capable of being pre-trained from scratch on raw text at scale.

- GPST consists of two components:
   1) A generative model that generates text and parse trees left-to-right. 
   2) A composition model that induces syntactic trees and computes constituent representations.

- These two models are trained jointly in a hard-EM fashion:
   - E-step: The composition model induces a parse tree and computes inside/outside representations of constituents using a fast algorithm.
   - M-step: Both models are updated to minimize a combined loss consisting of auto-encoding and auto-regression objectives based on the induced tree. 

- A key technique is using the inside representations from the composition model as a "representation surrogate" to enable parallel training of the generative model.

Main Contributions:
- Proposed a two-component SLM to enable unsupervised structure learning from raw text at scale.
- Developed a representation surrogate technique to facilitate joint parallel training.  
- Pre-trained GPST from scratch on a 9 billion token corpus, outperforming GPT-2 on various language understanding and generation tasks.
- Demonstrated substantial improvements over previous unsupervised SLMs in left-to-right grammar induction while accelerating training by 60 times.
