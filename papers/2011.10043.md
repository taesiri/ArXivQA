# [Propagate Yourself: Exploring Pixel-Level Consistency for Unsupervised   Visual Representation Learning](https://arxiv.org/abs/2011.10043)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is: How can we perform self-supervised visual representation learning at the pixel level to learn features that are useful for downstream dense prediction tasks like object detection and semantic segmentation? The authors argue that current self-supervised methods like instance discrimination learn features well-suited for image-level tasks like classification, but may lack spatial sensitivity needed for good pixel-level prediction. To address this, they introduce two pixel-level pretext tasks:1) PixelContrast - Treats each pixel as a class and uses contrastive learning to distinguish between pixels.2) PixelPro - Learns pixel-to-propagation consistency where features from the same pixel are extracted through two asymmetric pipelines. One produces a standard feature and the other produces a smoothed feature via a proposed pixel propagation module.The central hypothesis is that using these pixel-level pretext tasks can learn representations with better spatial sensitivity and lead to improved performance on downstream dense prediction tasks compared to instance-level self-supervised methods. The experiments aim to validate this hypothesis.In summary, the key research question is whether pixel-level pretext tasks can learn spatially sensitive features that transfer better to dense prediction tasks than current instance-level methods. The proposed PixelContrast and PixelPro methods are introduced to address this question.


## What is the main contribution of this paper?

Based on my reading, the main contributions of this paper seem to be:- Introducing pixel-level pretext tasks for self-supervised visual representation learning. The authors propose two specific approaches: 1) PixContrast - A pixel-level contrastive learning method that treats each pixel as a separate class and applies contrastive loss to distinguish pixels. 2) PixPro - A pixel-to-propagation consistency method that encourages consistency between a standard pixel feature and a smoothed pixel feature obtained via feature propagation.- Showing that pixel-level pre-training can achieve state-of-the-art transfer performance on downstream dense prediction tasks like object detection and semantic segmentation, outperforming prior instance-level contrastive learning methods.- Demonstrating that pixel-level pretext tasks can effectively pretrain not only backbone networks but also dense prediction heads, unlike instance-level methods that are limited to backbones.- Proposing to combine pixel-level and instance-level pretext tasks to benefit from both spatially-sensitive representations and categorical representations.- Providing thorough experiments and analysis to compare pixel-level methods to prior arts, ablate design choices, and validate performance on various downstream tasks.In summary, the core contribution seems to be exploring and showing the promise of pixel-level self-supervision for representation learning, especially for dense prediction tasks, through well-designed pretext tasks and rigorous experimentation.
