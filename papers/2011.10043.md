# [Propagate Yourself: Exploring Pixel-Level Consistency for Unsupervised   Visual Representation Learning](https://arxiv.org/abs/2011.10043)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is: How can we perform self-supervised visual representation learning at the pixel level to learn features that are useful for downstream dense prediction tasks like object detection and semantic segmentation? The authors argue that current self-supervised methods like instance discrimination learn features well-suited for image-level tasks like classification, but may lack spatial sensitivity needed for good pixel-level prediction. To address this, they introduce two pixel-level pretext tasks:1) PixelContrast - Treats each pixel as a class and uses contrastive learning to distinguish between pixels.2) PixelPro - Learns pixel-to-propagation consistency where features from the same pixel are extracted through two asymmetric pipelines. One produces a standard feature and the other produces a smoothed feature via a proposed pixel propagation module.The central hypothesis is that using these pixel-level pretext tasks can learn representations with better spatial sensitivity and lead to improved performance on downstream dense prediction tasks compared to instance-level self-supervised methods. The experiments aim to validate this hypothesis.In summary, the key research question is whether pixel-level pretext tasks can learn spatially sensitive features that transfer better to dense prediction tasks than current instance-level methods. The proposed PixelContrast and PixelPro methods are introduced to address this question.
