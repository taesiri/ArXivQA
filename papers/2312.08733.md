# [VMT-Adapter: Parameter-Efficient Transfer Learning for Multi-Task Dense](https://arxiv.org/abs/2312.08733)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a high-quality summary paragraph of the paper:

This paper proposes Vision Multi-Task Adapter (VMT-Adapter) and VMT-Adapter-Lite, two novel parameter-efficient transfer learning methods for multi-task dense scene understanding. VMT-Adapter consists of shared projections to enhance cross-task interaction and task-specific knowledge extraction modules to preserve task-specific knowledge. The shared projections contain very few additional parameters while the task-specific modules are negligible, resulting in constant overall parameters as the number of tasks scales. VMT-Adapter-Lite further reduces parameters via a sharing strategy between projections. Experiments on semantic segmentation, human part segmentation, saliency detection and surface normals estimation demonstrate that VMT-Adapter outperforms prior methods, improving 3.96% over single-task fine-tuning while only using 1.13M parameters (1% of model). VMT-Adapter-Lite uses just 0.40M extra parameters but still improves 1.34%. Both methods achieve optimal O(1) training and inference efficiency. The designs strike an excellent balance between performance and efficiency for multi-task dense scene understanding.


## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Fine-tuning large pre-trained models like Transformers on downstream tasks is computationally expensive as it requires updating all the parameters. 
- Existing parameter-efficient methods for transfer learning focus mostly on single tasks and have limitations when adapted to multi-task scenarios.

Proposed Solution:
- The paper proposes a novel Vision Multi-Task Adapter (VMT-Adapter) for parameter-efficient multi-task transfer learning. 
- VMT-Adapter has shared projections to enable cross-task interaction and independent task-specific modules to capture task-specific knowledge. This allows learning both shared and task-specific representations.
- It uses dot product based scaling and shifting in the task-specific modules, which requires negligible extra parameters.
- A lighter version - VMT-Adapter-Lite uses parameter sharing between the down-projection and up-projection to further reduce trainable parameters.

Main Contributions:
- VMT-Adapter is the first adapter designed specifically for vision multi-task learning, with O(1) complexity w.r.t number of tasks.
- Experiments on 4 dense prediction tasks show VMT-Adapter outperforms baselines by 3.96%, using only 1% trainable parameters of the full model.
- VMT-Adapter-Lite further reduces parameters to 0.36% while achieving 1.34% improvement over single task fine-tuning.
- The method shows consistent gains over strong baselines with different backbones and decoder choices.

In summary, the paper presents an efficient once-for-all adapter structure for multi-task dense scene understanding, which is parameter-efficient, has optimal training/inference complexity and shows significant performance gains.
