# [VMT-Adapter: Parameter-Efficient Transfer Learning for Multi-Task Dense](https://arxiv.org/abs/2312.08733)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a high-quality summary paragraph of the paper:

This paper proposes Vision Multi-Task Adapter (VMT-Adapter) and VMT-Adapter-Lite, two novel parameter-efficient transfer learning methods for multi-task dense scene understanding. VMT-Adapter consists of shared projections to enhance cross-task interaction and task-specific knowledge extraction modules to preserve task-specific knowledge. The shared projections contain very few additional parameters while the task-specific modules are negligible, resulting in constant overall parameters as the number of tasks scales. VMT-Adapter-Lite further reduces parameters via a sharing strategy between projections. Experiments on semantic segmentation, human part segmentation, saliency detection and surface normals estimation demonstrate that VMT-Adapter outperforms prior methods, improving 3.96% over single-task fine-tuning while only using 1.13M parameters (1% of model). VMT-Adapter-Lite uses just 0.40M extra parameters but still improves 1.34%. Both methods achieve optimal O(1) training and inference efficiency. The designs strike an excellent balance between performance and efficiency for multi-task dense scene understanding.
