# [Shaving Weights with Occam's Razor: Bayesian Sparsification for Neural   Networks Using the Marginal Likelihood](https://arxiv.org/abs/2402.15978)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
Neural network pruning is important to reduce the computational and memory costs of large AI models for deployment on consumer devices. However, many trained networks are not inherently "sparsifiable", meaning they resist pruning without significant performance drops. The paper tackles the challenge of improving neural networks' sparsifiability.

Method: 
The authors propose a framework called "SpaM" (Sparsifiability via the Marginal likelihood) that uses Bayesian model selection through the marginal likelihood to automatically select neural network models that are more sparsifiable. Specifically, SpaM employs sparsity-inducing priors (like parameter-wise priors) and optimizes them using the marginal likelihood to regularize less relevant parameters to have smaller magnitudes. This allows aggressively pruning them later without much quality loss. The computed posterior approximation is then reused to define a cheap pruning criterion called "OPD" (Optimal Posterior Damage) for sparsification.

Contributions:
- Novel SpaM framework to improve sparsifiability of neural networks for both structured and unstructured pruning
- Showing SpaM's effectiveness across models, datasets and also in online pruning during training
- Deriving the OPD criterion which outperforms many existing criteria despite having almost no extra computational cost
- Providing guidelines for choosing good priors, including newly introduced parameter-wise and unit-wise priors for KFAC Laplace approximation
- Demonstrating up to 20x savings in computational costs on pruned models with minimal quality drops

In summary, the paper presents an effective Bayesian framework leveraging model selection through the marginal likelihood to find neural network architectures that are inherently more sparsifiable. This allows aggressive pruning for deployment on resource-constrained devices while maintaining high performance.
