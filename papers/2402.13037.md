# [Align Your Intents: Offline Imitation Learning via Optimal Transport](https://arxiv.org/abs/2402.13037)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Offline reinforcement learning (RL) aims to learn optimal policies from pre-collected, static datasets without environment interaction. However, it faces challenges like lack of rewards, action labels, and distribution shift. 
- Imitation learning (IL) can help by learning from expert demonstrations, but most methods still require action labels or rewards.

Proposed Solution:
- The paper proposes AILOT - Aligned Imitation Learning via Optimal Transport. 
- It represents states as 'intents' using Intention-Conditioned Value Function (ICVF). Intents incorporate pairwise distances between states.
- It defines an intrinsic reward based on optimal transport distance between expert and agent intent trajectories.
- This aligns the learning agent with the expert's intentions without needing action labels or rewards.

Key Contributions:
- AILOT outperforms state-of-the-art offline IL methods on MuJoCo locomotion without action labels.
- It also improves performance of offline RL methods on sparse reward tasks like AntMaze and Adroit.
- It can mimic complex expert behaviors like backflip hopper using only observations.
- The intrinsic rewards enable custom imitation even from datasets with only random policies.
- AILOT sets a new benchmark for offline IL without rewards or actions by aligning agent and expert intents.

In summary, the paper proposes a novel way to extract reward signals from expert observations alone using optimal transport over learned intent representations. This provides an effective approach for offline IL and RL without any manually defined rewards or action labels.
