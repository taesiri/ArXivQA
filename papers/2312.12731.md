# [Robustly Improving Bandit Algorithms with Confounded and Selection   Biased Offline Data: A Causal Approach](https://arxiv.org/abs/2312.12731)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
This paper studies the problem of leveraging offline logged bandit data to improve online bandit learning algorithms. A key challenge is that the offline data often contains biases from different sources such as confounding bias and selection bias. Simply fitting a model on the biased offline data and using it online could negatively impact the bandit algorithm's performance. 

Proposed Solution:
The paper proposes a causal inference based framework to address this problem. Specifically:

1. The offline data biases are categorized into confounding bias and selection bias based on the causal graph. The paper derives robust bounds on the expected reward of each arm by leveraging causal inference techniques like c-component factorization and substitute interventions.

2. These causal bounds are then utilized to augment popular bandit algorithms like LinUCB and UCB. For example, in LinUCB-PCB the upper confidence bound for each arm is truncated by the causal upper bound derived offline.

3. Regret analysis shows that the proposed algorithms achieve lower regret compared to variants without using causal bounds. Empirical evaluations further demonstrate the effectiveness.

Main Contributions:

- Formulates the problem of combining offline and online bandit data from a causal perspective
- Proposes a framework to derive causal bounds on arm rewards that are robust to compound biases in offline data 
- Develops bandit algorithms enhanced with prior causal bounds and shows reduced regret both theoretically and empirically
- Provides a principled approach to safely leverage biased offline logged bandit data to improve online learning

The main merit lies in carefully addressing the biases in offline data before using it to guide online bandit algorithms. This prevents negative transfer of biases and improves robustness.


## Summarize the paper in one sentence.

 Here is a one-sentence summary of the paper:

This paper studies how to leverage offline data with confounding and selection biases to robustly improve bandit algorithms by deriving causal bounds on the rewards and incorporating them into the arm selection strategy.


## What is the main contribution of this paper?

 According to the paper, the main contributions are:

1) Deriving causal bounds for conditional causal effects under confounding and selection biases based on c-component factorization and substitute intervention methods. 

2) Proposing a novel framework that leverages the prior causal bound obtained from biased offline data to guide the arm-picking process in bandit algorithms, thus robustly decreasing the exploration of sub-optimal arms and reducing the cumulative regret.

3) Developing one contextual bandit algorithm (LinUCB-PCB) and one non-contextual bandit algorithm (UCB-PCB) that are enhanced with prior causal bounds. Theoretically showing under mild conditions both bandit algorithms achieve lower regret than their non-causal counterparts.

So in summary, the key contribution is using causal inference techniques to derive robust bounds on the expected rewards of arms in bandit problems, and then using these bounds to improve regret in online bandit learning algorithms. This allows them to leverage biased offline data while still ensuring good performance.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper's content, some of the key terms and concepts include:

- Bandit algorithms - The paper studies bandit problem settings where an agent sequentially makes decisions and receives reward feedback. This includes both contextual and non-contextual bandit algorithms.

- Offline observational data - The paper considers settings where, in addition to online bandit feedback, the agent has access to a dataset collected offline which could potentially help guide the online learning process.

- Causal inference - The paper takes a causal inference perspective to account for and mitigate biases (confounding and selection bias) that may exist in the offline observational data. Causal graphs, do-calculus, and bounding causal effects are key concepts. 

- Compound biases - The paper considers compound biases arising from both confounding and selection biases in the offline data and develops robust methods to extract useful information.

- Prior causal bounds - Bounds on the causal effects of each arm derived from the offline data that can help guide arm selection during the online learning phase to accelerate learning.

- Regret analysis - Theoretical regret analyses are provided to show how leveraging prior causal bounds can provably improve the cumulative regret of bandit algorithms.

So in summary - bandits, offline data, causal inference for mitigating compound biases, causal bounds to accelerate learning, and regret analysis are key concepts and terms associated with this work.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 potential in-depth questions about the method proposed in this paper:

1. The paper categorizes biases into confounding bias and selection bias. Can you explain in more detail the key differences between these two types of biases and why it is important to categorize them this way?

2. One of the key ideas in this paper is using causal bounds to guide the arm selection process in bandit algorithms. Can you walk through the intuition behind why incorporating causal bounds helps improve regret? What are the theoretical guarantees?

3. The paper proposes two novel strategies (c-component factorization and substitute interventions) to derive causal bounds. Can you explain the high-level ideas behind each strategy and the assumptions needed for them to work? What are the tradeoffs between the two strategies?  

4. How does the paper address compound biases (both confounding and selection bias together)? What assumptions need to hold for the proposed methods to work under compound biases? Are there any limitations?

5. The regret analysis shows that the proposed methods can consistently reduce asymptotic regret. Can you walk through the key steps in the regret proof? What conditions need to hold for the regret reduction to happen?

6. How does the paper extend the idea of using causal bounds to other bandit algorithms like LinUCB and UCB? What modifications were needed to integrate causal bounds into those algorithms?

7. One interesting aspect is using offline biased data to derive online learning guarantees. Can you discuss the interplay between offline evaluation and online learning and why this is an intriguing idea? 

8. The empirical evaluation uses synthetic data. What are some of the challenges in getting real-world offline and online data for this type of technique? How would you design experiments on real data?

9. The paper focuses on stochastic bandits. Do you think the use of causal bounds can be extended to other models like adversarial or Markovian bandits? What additional considerations would be needed?

10. The paper mentions fairness as a potential area where this work can be applied. Can you elaborate on how causal bounds could help make bandit algorithms more fair and avoid biases? What are some open problems in this area?
