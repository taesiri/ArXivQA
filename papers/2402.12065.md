# [WKVQuant: Quantizing Weight and Key/Value Cache for Large Language   Models Gains More](https://arxiv.org/abs/2402.12065)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Large language models (LLMs) have huge memory requirements due to the large number of parameters and the need to cache key/value (KV) activations for text generation. This poses challenges for deployment. 
- Existing LLM quantization methods have limitations in balancing accuracy and memory savings:
    - Weight-only quantization has minimal impact on accuracy but insufficient memory savings
    - Weight+activation quantization gives large memory savings but significantly reduces accuracy

Proposed Solution - WKVQuant:
- Quantize weights AND KV cache exclusively while keeping other temporary activations in full precision
- This provides a good trade-off between accuracy and memory savings

Key Contributions:
- Past-Only Quantization (POQ): Retains current KV in full precision before quantizing into KV cache to improve attention computation 
- Two-Dimensional Quantization: Handles value variations across channels and tokens in KV cache via channel smoothing and token-wise quantization
- Cross-Block Reconstruction Regularization: New loss function to reduce bias and overfitting during parameter optimization

Results:
- WKVQuant achieves near weight-only quantization accuracy and close to weight+activation quantization memory savings 
- Outperforms prior weight+KV cache quantization methods in accuracy evaluations
- Provides promising accuracy vs efficiency trade-off for deploying large LLMs

In summary, the paper identifies limitations of existing LLM quantization methods and proposes a novel framework called WKVQuant that focuses on quantizing weights and KV cache. WKVQuant incorporates techniques like POQ, 2D quantization and cross-block regularization that enable better accuracy while providing substantial memory savings.
