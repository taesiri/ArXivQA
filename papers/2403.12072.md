# [Floralens: a Deep Learning Model for the Portuguese Native Flora](https://arxiv.org/abs/2403.12072)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem: 
The paper introduces Floralens, a new deep learning model for visual identification of plant species. The goal is to develop a performant model tailored to plant species occurring in Portugal. The paper evaluates the performance of Floralens on multiple test sets and compares it to existing models from Pl@ntNet.

Proposed Solution:
Floralens is trained on a dataset of over 400K images covering 1,678 plant species from 4 sources: FloraOn, iNaturalist, Observation.org and Pl@ntNet. The model uses EfficientNet as the CNN architecture. In addition to the main species classification model, a separate genus-level classification model is also trained.

Evaluation:
Several test sets are used to evaluate Floralens:
- FLTS: 29K image test set split from the training data covering all 1,678 species
- PlantCLEF: 10K image subset from PlantCLEF 2022-2023 competition
- Wikipedia: 1.3K images automatically obtained from Wikipedia

The models are evaluated using precision, recall, AUC, Top-1, Top-5 accuracy and Mean Reciprocal Rank (MRR).

Key Results:
- For the FLTS, Floralens achieves Top-1 accuracy of 67%, Top-5 accuracy of 86% and MRR of 0.75. Results are consistent across different data sources.
- Comparable results on PlantCLEF and Wikipedia sets: Top-1 accuracy of 65%, Top-5 accuracy of 84% and MRR of 0.72-0.73 
- The genus-level model outperforms the species-level model by 8-14% across metrics, indicating easier generalization at genus-level.

Comparison with Pl@ntNet:
- Compared to Pl@ntNet's legacy CNN model (pnleg), Floralens achieves same MRR of 0.72 on PlantCLEF but is +0.07 higher on FLTS 
- Compared to Pl@ntNet's newer ViT model (pnnew), Floralens is lower by 0.05-0.07 MRR across test sets
- After excluding potential test set overlap images, pnnew outperforms Floralens only marginally by 0.03 MRR on FLTS

In summary, Floralens demonstrates competitive performance compared to Pl@ntNet models, achieving slightly better accuracy than the legacy CNN model and moderately lower accuracy than the newer ViT model. The results highlight the potential of developing specialized models tailored to local flora.


## Summarize the paper in one sentence.

 This paper evaluates the Floralens visual plant identification model on multiple test sets and compares its performance to Pl@ntNet models, finding that Floralens achieves reasonably good results on par with or slightly below the latest Pl@ntNet models.


## What is the main contribution of this paper?

 Based on the details provided in the paper, the main contribution appears to be the development and evaluation of the Floralens visual identification model for plant species. Specifically:

- The paper presents the Floralens model, which is designed to visually identify plant species occurring in mainland Portugal. The model covers 1,678 species and uses a convolutional neural network architecture.

- The paper includes an evaluation of the Floralens model on three different test sets:
  - A test split from the model's training data (29,360 images)
  - A subset of 10,000 images from the PlantCLEF 2022-2023 dataset
  - 1,351 images collected from Wikipedia

- Evaluation metrics such as Top-1 accuracy, Top-5 accuracy, and Mean Reciprocal Rank are reported. On the main test set, Floralens achieves 0.67 Top-1 accuracy and 0.75 Mean Reciprocal Rank.

- Floralens results are compared to PlantNet models accessed through the PlantNet API. Floralens performs better than the legacy PlantNet model on the main test set and is broadly comparable to the newer PlantNet models.

So in summary, the key contribution is the development and thorough evaluation of the Floralens visual identification model for plant species occurring in mainland Portugal.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, here are some of the key terms and keywords that seem most relevant:

- Plant species identification
- Convolutional neural networks
- Evaluation metrics (precision, recall, top-1, top-5, mean reciprocal rank)
- Test sets (FLTS, PlantCLEF, Wikipedia) 
- Pl@ntNet API
- Comparative evaluation
- FloraOn
- iNaturalist
- Observation.org

The paper presents an evaluation of the Floralens model for plant species identification using convolutional neural networks. It utilizes several standard evaluation metrics and test sets to benchmark performance. A comparative analysis is also conducted using the Pl@ntNet API models as a baseline. Overall, the key focus seems to be on evaluating the Floralens model and comparing it to state-of-the-art methods for plant species identification.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 potential in-depth questions about the method proposed in the paper:

1. The paper compares the Floralens model to Pl@ntNet models. What are the key differences between these models in terms of the neural network architectures and training data used? How might these differences explain the relative performance of the models?

2. The paper evaluates performance using precision, recall and mean reciprocal rank (MRR). Why are these suitable evaluation metrics for this task? What are the limitations of relying only on these metrics? 

3. The Floralens model seems to perform worse on the Wikipedia test set compared to the other test sets. What explanations are provided in the paper for this? Can you think of any other potential reasons for the poorer performance?

4. Could the Floralens model be improved by using a more complex neural network architecture like the Vision Transformers used in the Pl@ntNet model? What challenges might this present?

5. The paper finds that restricting the Pl@ntNet model predictions to Southwestern Europe species only leads to a minimal change in performance. Why might this be the case? When would you expect a regional model to outperform a global model?

6. The Floralens model is pre-trained on images from multiple different data sources. Do you think it would perform better if trained on a single curated dataset? What are the tradeoffs with using diverse weakly labeled data?

7. The paper evaluates the model at both the species and genus levels. Why is the performance generally better at the genus level? When would species-level prediction be critical for a real-world application?

8. How suitable do you think the evaluation methodology is? What additional analyses could be done to gain more insight into the model's strengths and weaknesses?

9. The paper uses a sample of PlantCLEF data as one test set. What is PlantCLEF and why specifically does this data provide a useful additional test set? What are the limitations of using this data?

10. How do you think the Floralens model would perform on entirely new plant species not seen during training? What approaches could be used to estimate the performance on new species without labeled test data?
