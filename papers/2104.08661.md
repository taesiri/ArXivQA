# Explaining Answers with Entailment Trees

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be: How can we enable machines to generate richer, more systematic explanations for their question answering responses, specifically by generating explanations in the form of multi-step entailment trees?The key ideas and contributions towards addressing this question appear to be:1) Formulating explanation as multi-step, multi-premise textual entailment, where an entailment tree shows the reasoning from facts to conclusions leading to the final answer.2) Creating the EntailmentBank dataset, which contains 1,840 multi-step entailment trees annotated by experts to accompany QA pairs. This is the first dataset of its kind for multi-step entailments.3) Defining three hierarchical task formulations of increasing difficulty for generating entailment tree explanations.4) Developing baseline EntailmentWriter models to generate entailment trees for the tasks, showing reasonable performance when all relevant facts are provided, indicating feasibility.5) Analysis of the models and dataset, including common errors, limitations, and future directions. So in summary, the key research question is how to generate richer entailment tree explanations for QA, which they explore through a new dataset, task formulations, models, and analyses.


## What is the main contribution of this paper?

The main contributions of this paper are:1. Formulating explanation as multi-step, multi-premise textual entailment. 2. Introducing EntailmentBank, the first dataset of multi-step entailment trees for QA, to support entailment-based explanation. The dataset contains 1,840 trees with an average of 6.6 nodes and 2.7 entailment steps per tree.3. Defining three tasks of increasing difficulty for generating entailment tree explanations, based on the inputs provided: gold leaf sentences only (Task 1), gold leaves + distractor sentences (Task 2), or a full corpus (Task 3).4. Presenting baseline results using a T5-based generative model, showing it can partially solve the tasks, especially when provided with gold leaf sentences. For Task 1, 35% of generated trees perfectly matched the gold.5. Providing analysis of errors and future directions, including modifications to the loss function, applying constraints during generation, and improving evaluation to account for valid tree variations.  6. Demonstrating potential for generalization by evaluating on out-of-domain QA datasets, where models trained on EntailmentBank could produce valid explanations without retraining.In summary, the key contribution is providing a new dataset for richer, multi-step reasoning chains as explanations, along with baseline methods and analysis, opening up new research directions for generating more systematic and interpretable explanations.
