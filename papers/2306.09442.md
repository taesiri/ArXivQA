# [Explore, Establish, Exploit: Red Teaming Language Models from Scratch](https://arxiv.org/abs/2306.09442)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, it seems the central research questions/hypotheses are:1. Can language models be systematically "red teamed" to discover inputs that elicit harmful outputs, even when starting from just a vague notion of what constitutes harmful behavior rather than a precise specification? 2. Is it feasible to develop an end-to-end pipeline for red teaming language models that involves exploring model capabilities, establishing human-grounded metrics for undesired behavior, and exploiting model vulnerabilities?3. Can this pipeline be used to elicit toxic speech and dishonest claims from large language models like GPT-2 and GPT-3 when starting from scratch?4. Is taking a human-centered approach to establishing metrics of undesired behavior more effective for red teaming compared to just using an off-the-shelf classifier or dataset?5. Can adversarial prompts that elicit harmful behavior transfer across models and metrics to some degree?So in summary, the central questions seem to revolve around developing and evaluating a systematic pipeline for red teaming language models to discover harmful outputs, particularly when starting without a precise specification of what constitutes harm. The paper aims to demonstrate this pipeline can work even without prior knowledge and that taking a human-centered approach is important.
