# [Efficient Video Action Detection with Token Dropout and Context   Refinement](https://arxiv.org/abs/2304.08451)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question is: How can we develop an efficient video action detection framework based on vision transformers that reduces computational costs while maintaining accuracy?

The key hypothesis is that by selectively pruning spatiotemporal tokens from video inputs and refining the context using the remaining tokens, it is possible to build an efficient transformer-based model for video action detection. 

Specifically, the authors propose:

1) A spatiotemporal token dropout method that prunes tokens from a keyframe-centric perspective. This preserves keyframe tokens and tokens relevant to actor motions, while dropping other redundant tokens. 

2) A context refinement module that leverages the remaining tokens after pruning to enrich the representations of actor identities and improve classification accuracy.

By combining these techniques for selective token pruning and context refinement, the authors aim to demonstrate that the proposed Efficient Video Action Detector (EVAD) can achieve computational savings and speed improvements compared to vanilla vision transformers, without sacrificing accuracy on video action detection tasks.


## What is the main contribution of this paper?

 The main contribution of this paper is proposing an end-to-end framework for efficient video action detection (EVAD) based on vanilla vision transformers (ViTs). The key ideas include:

1. A spatiotemporal token dropout method from a keyframe-centric perspective to eliminate redundant video tokens. It maintains all tokens from the keyframe, preserves tokens relevant to actor motions from other frames, and drops out the remaining tokens. 

2. A context refinement module to refine actor identities using the remaining tokens. It expands actor bounding boxes along the temporal dimension to capture spatiotemporal features, and refines them via a decoder's attention to the remaining context tokens.

3. Evaluations on AVA, UCF101-24 and JHMDB datasets validate that the proposed EVAD reduces computations by 40-50% and improves throughput by 40% with no performance degradation compared to baseline ViTs. It also allows using higher resolution inputs for improved accuracy under similar complexity.

In summary, the main contribution is developing an efficient transformer framework for video action detection via spatiotemporal token pruning and context refinement, which maintains accuracy while significantly improving efficiency.
