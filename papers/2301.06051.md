# [DSVT: Dynamic Sparse Voxel Transformer with Rotated Sets](https://arxiv.org/abs/2301.06051)

## What is the central research question or hypothesis that this paper addresses?

The central research question is how to design an efficient yet deployment-friendly 3D backbone to handle sparse point clouds for various 3D perception tasks. The key hypothesis is that attention mechanisms in Transformers are more appropriate for flexibly modeling long-range relationships in sparse point clouds compared to customized sparse convolutions, and can serve as powerful backbones if designed properly for sparsity.The paper proposes a transformer-based architecture called Dynamic Sparse Voxel Transformer (DSVT) to address this question. The main ideas are:1) Designing a dynamic sparse window attention mechanism to efficiently process sparse voxels in parallel sets without redundancy. 2) Introducing an attention-based 3D pooling module to handle sparsity during downsampling and better encode geometric information.3) Implementing the architecture using only standard deep learning operations, without custom CUDA kernels, for easy deployment.The hypothesis is that such a transformer design can achieve state-of-the-art performance on 3D perception tasks while being easy to deploy in real systems compared to prior specialized approaches. Experiments on large datasets like Waymo and nuScenes validate the effectiveness.In summary, the paper aims to develop an efficient yet deployment-friendly transformer backbone tailored for sparse point clouds to advance 3D perception, by dynamic sparse attention and differentiable pooling. The core hypothesis is that this can outperform prior specialized networks without sacrificing deployability.
