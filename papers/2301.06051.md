# [DSVT: Dynamic Sparse Voxel Transformer with Rotated Sets](https://arxiv.org/abs/2301.06051)

## What is the central research question or hypothesis that this paper addresses?

The central research question is how to design an efficient yet deployment-friendly 3D backbone to handle sparse point clouds for various 3D perception tasks. The key hypothesis is that attention mechanisms in Transformers are more appropriate for flexibly modeling long-range relationships in sparse point clouds compared to customized sparse convolutions, and can serve as powerful backbones if designed properly for sparsity.The paper proposes a transformer-based architecture called Dynamic Sparse Voxel Transformer (DSVT) to address this question. The main ideas are:1) Designing a dynamic sparse window attention mechanism to efficiently process sparse voxels in parallel sets without redundancy. 2) Introducing an attention-based 3D pooling module to handle sparsity during downsampling and better encode geometric information.3) Implementing the architecture using only standard deep learning operations, without custom CUDA kernels, for easy deployment.The hypothesis is that such a transformer design can achieve state-of-the-art performance on 3D perception tasks while being easy to deploy in real systems compared to prior specialized approaches. Experiments on large datasets like Waymo and nuScenes validate the effectiveness.In summary, the paper aims to develop an efficient yet deployment-friendly transformer backbone tailored for sparse point clouds to advance 3D perception, by dynamic sparse attention and differentiable pooling. The core hypothesis is that this can outperform prior specialized networks without sacrificing deployability.


## What is the main contribution of this paper?

The main contribution of this paper seems to be proposing a new transformer-based backbone network called Dynamic Sparse Voxel Transformer (DSVT) for 3D object detection from point clouds. The key ideas are:- Dynamic Sparse Window Attention - An efficient attention mechanism to handle sparse voxels in parallel by dividing voxels into non-overlapping window-bounded subsets.- Rotated Set Attention - Alternating the voxel partitioning strategy between consecutive self-attention layers to enable connections across subsets. - Hybrid Window Partition - Using alternating window shapes across blocks to reduce computation cost.- Attention-Style 3D Pooling - A learnable 3D pooling module using attention to encode geometric information without custom CUDA ops.Overall, the main contribution appears to be presenting an efficient yet powerful 3D detection backbone that achieves state-of-the-art results while being easy to deploy without custom operations. The key novelty seems to be in the dynamic sparse window attention and pooling designs for handling sparsity.
