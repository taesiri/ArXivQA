# [Null-text Inversion for Editing Real Images using Guided Diffusion   Models](https://arxiv.org/abs/2211.09794)

## What is the central research question or hypothesis that this paper addresses?

 The central research question this paper addresses is how to accurately invert real images into the latent space of text-guided diffusion models in order to enable intuitive text-based editing of the images. Specifically, the paper introduces two key components to achieve this:

1. Pivotal inversion for diffusion models: Rather than mapping all possible noise vectors to a single image like previous works, the paper proposes using a single pivotal noise vector computed by an initial DDIM inversion as an anchor point for more efficient optimization. 

2. Null-text optimization: The paper recognizes that directly optimizing the text embedding can damage editability. Instead, it proposes optimizing only the unconditional "null" text embedding used in classifier-free guidance. This allows accurately reconstructing the image while keeping the text embedding and model weights fixed, preserving editability.

By combining these two techniques - using pivotal inversion for efficiency and optimizing the null text to avoid damaging editability - the paper demonstrates high-fidelity inversion and editing of real images using the publicly available Stable Diffusion model. The central hypothesis is that this null-text inversion approach will enable intuitive text-based editing of real images with pre-trained diffusion models.

In summary, the key research question is how to accurately invert real images into diffusion models to enable text-based editing, with the core ideas being pivotal inversion and null-text optimization. The paper aims to demonstrate this can be achieved without destructive model fine-tuning or embedding optimization as in prior works.
