# [Wikibench: Community-Driven Data Curation for AI Evaluation on Wikipedia](https://arxiv.org/abs/2402.14147)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem: 
AI tools are increasingly being deployed in community contexts, but the datasets used to evaluate them are typically created by developers and annotators outside of the community. This can lead to mismatches between an AI tool's performance on benchmark datasets versus how it performs from the community's perspective. The paper argues for the need for more community-driven approaches to curating AI evaluation datasets. However, supporting community members in navigating disagreements and steering the overall curation process poses open research challenges.

Proposed Solution:
The paper introduces Wikibench, a system that enables Wikipedia community members to collaboratively curate datasets for evaluating AI tools deployed on Wikipedia. Wikibench allows community members to select and label Wikipedia edits, discuss disagreements, and reach consensus on labels. It captures both individual labels reflecting personal perspectives and primary labels intended to represent consensus. The system provides transparency into the curation process and dataset contents.   

Key Contributions:
- Introduces Wikibench, the first system aimed at supporting intentional, community-driven curation of AI datasets in practice
- Reports on a field study with Wikipedia editors using Wikibench, demonstrating the system's ability to capture community consensus, disagreement, and uncertainty
- Shows evidence of the utility of the resulting dataset in evaluating different AI models' alignment with community perspectives 
- Discusses participants' use of Wikibench to shape data curation processes and build community
- Proposes future directions for community-driven data curation tools within and beyond Wikipedia

Overall, the paper demonstrates the potential of community-driven approaches for curating AI evaluation datasets reflective of community norms and values. The introduced Wikibench system and findings from its real-world deployment significantly advance this emerging area of research.


## Summarize the paper in one sentence.

 Here is a one-sentence summary of the paper:

The paper introduces Wikibench, a system that enables Wikipedia community members to collaboratively curate AI evaluation datasets by selecting, labeling, and discussing edits, capturing both consensus and disagreement, and demonstrates through a field study that the resulting datasets can help evaluate AI model alignment with community perspectives while also empowering the community to shape the overall data curation process.


## What is the main contribution of this paper?

 Based on my understanding, the main contribution of this paper is the introduction and evaluation of Wikibench, a system that enables community members to collaboratively curate AI evaluation datasets. Specifically, the key contributions highlighted in the paper's conclusion are:

1) System: The paper introduces Wikibench, the first system aimed at supporting community-driven curation of AI datasets. 

2) Field study: The paper presents findings from a field study on Wikipedia to understand how a real-world community might use Wikibench in practice to collaboratively curate an evaluation dataset.

3) Future directions: Based on the findings, the paper proposes future directions for HCI systems that support community-driven data curation within and beyond the context of Wikipedia.

In summary, the core contribution is the design, implementation, and evaluation of Wikibench to demonstrate the potential for new approaches to community-driven curation of AI evaluation datasets. The paper also contributes insights from a real-world case study and directions for future research in this emerging area.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and concepts associated with this paper include:

- Community-driven data curation: The paper introduces the concept of community members intentionally curating datasets to evaluate AI systems, rather than relying on datasets created solely by developers.

- Wikipedia: The paper conducts studies within the context of the Wikipedia community to evaluate the proposed Wikibench system.

- Wikibench: The system introduced in the paper to enable Wikipedia editors to collaboratively curate datasets for evaluating AI tools.

- AI evaluation: A core focus of the paper is using community-curated datasets to better evaluate the alignment of AI systems with community values and perspectives.

- Disagreement and consensus: The paper examines how Wikibench captures disagreements among community members while also building consensus, reflected in the "primary" labels. 

- Design requirements: The paper outlines requirements like community agency, transparency, embedding in workflows, etc. that guided the design of Wikibench.

- Field study: The paper conducts a field study where Wikipedia editors use Wikibench over one week to curate a dataset.

- Validation study: A separate study to evaluate whether Wikibench labels better capture consensus compared to prior approaches.

Does this summary cover the key terms and concepts associated with this paper? Let me know if you need any clarification or have additional questions!


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions I generated about the methods proposed in the paper:

1. The paper proposes a new system called Wikibench to support community-driven curation of AI evaluation datasets. What are the key components of Wikibench's system architecture and how do they aim to fulfill the design requirements outlined in the paper?

2. The paper conducts a field study to evaluate Wikibench involving 12 Wikipedia editors. What was the study protocol and what were the minimum participation requirements for editors? How might you alter the protocol for a longer-term deployment of the system?  

3. The paper categorizes disagreement between editors into three types: ambiguity, genuine differences in perspective, and agreed-upon edge cases. What metrics and visualizations does the paper use to distinguish between these categories? Can you think of any alternative metrics that may be worth exploring?

4. One of the findings is that Wikibench provides transparency into the dataset curation process. In what ways does the system aim to provide transparency and why is this important? Can you think of any risks associated with making the curation process fully transparent?

5. The validation study compares labels generated through Wikibench and Wikilabels to a separate consensus ground truth. What was the protocol for the validation study? What are some limitations of this approach to evaluating the quality of Wikibench's labels?  

6. The paper evaluates two AI models, ORES and Revert-Risk, using the dataset curated via Wikibench. How does this evaluation showcase the uniqueness and value of community-curated datasets? What visualizations facilitate interpretation of differences between AI and community perspectives?

7. The paper identifies seven high-level themes from a thematic analysis of exit interviews. Can you list and describe three of these themes regarding participants' experiences using Wikibench? How might these themes inform future improvements to Wikibench or tools for community-driven data curation more broadly?

8. The paper discusses balancing the benefits of community deliberation with efficiency of the data curation process. What are some ideas proposed to make more effective use of community members’ time when contributing labels and discussion? How might you evaluate the tradeoffs between additional community engagement and efficiency?  

9. The paper acknowledges differences between Wikibench's approach of community-driven data curation versus existing content curation mechanisms on platforms like Wikipedia and Reddit. Can you summarize a couple key differences and similarities identified? How might existing content curation designs inspire future systems?

10. One direction for future work is developing tools and processes for community-driven data curation across more contexts beyond Wikipedia. What aspects of Wikibench’s design and Wikipedia’s norms might transfer well or need adaptation for other communities like Facebook Groups or Reddit? How would you study these considerations?
