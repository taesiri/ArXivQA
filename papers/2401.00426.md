# [keqing: knowledge-based question answering is a nature chain-of-thought   mentor of LLM](https://arxiv.org/abs/2401.00426)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Large language models (LLMs) tend to generate nonsensical or incorrect responses when faced with complex questions or problems beyond their knowledge scope. This is known as the "hallucination" phenomenon. 

- Existing retrieval-augmented LMs rely on embedding-based methods to retrieve relevant text passages from a corpus, which can bring in redundant or irrelevant information that negatively impacts the quality of LLM's response.

- Current LLM-based KBQA systems try to directly generate executable logical chains (in SQL form), but the generated SQL is often non-executable in practice. 

Proposed Solution - Keqing Framework:

- Decomposes a complex question into simpler sub-questions using predefined templates, making it easier for LLM to capture the logic (vs SQL chains)

- Aligns sub-questions to executable logical chains on a knowledge graph for retrieving answer candidates

- Reasons among candidates to select correct answers for each sub-question 

- Summarizes the workflow and outputs response with reasoning paths

Main Contributions:

- Proposes using logical chains on a knowledge graph as chain-of-thoughts to guide LLM to decompose questions, instead of hand-crafted methods

- Moves beyond text-to-SQL generation by using sub-question templates as intermediary to capture reasoning logic more easily

- Achieves competitive KBQA performance while also improving interpretability via the summarized response 

- Demonstrates knowledge-based QA can be an automated and scalable chain-of-thought mentor for existing LLMs

In summary, the Keqing framework improves reliability of LLM's response for complex KBQA by retrieving structured knowledge and chaining reasoning through sub-questions. The workflow serves as an interpretable chain-of-thought guide for the LLM.
