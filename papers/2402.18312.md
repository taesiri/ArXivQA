# [How to think step-by-step: A mechanistic understanding of   chain-of-thought reasoning](https://arxiv.org/abs/2402.18312)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem Statement
- Chain-of-thought (CoT) prompting in large language models (LLMs) has shown impressive reasoning abilities, but the internal mechanisms behind this are not well understood. 
- Prior work has tried to analyze CoT reasoning indirectly through perturbations or theoretically, but a direct analysis of the neural components has been lacking. 
- Interpreting the components behind CoT in large LLMs is challenging due to model complexity, adaptivity of components (backup circuits), and reliance on factual knowledge.

Proposed Solution
- Analyze a 7B parameter LLaMA model on fictional ontology reasoning from the PrOntoQA dataset, which removes factual knowledge requirements.
- Decompose the CoT reasoning task into decision-making, copying, and inductive reasoning subtasks. 
- Use activation patching, probing classifiers, and logit lens to analyze attention heads, residual connections, and pathways related to these subtasks.

Key Findings
- Shared sets of attention heads implement the different reasoning subtasks, with inductive reasoning heads being consistently important.
- Heads perform information movement between fictionally related entities, peaking in distinguishability in the middle layers. 
- Multiple parallel pathways write answer tokens, collecting them from the question context, CoT context, or even incorrectly from the few-shot context.
- A functional rift exists around the 16th layer, with early layers focused on pretraining statistics and later layers utilizing the context.

Main Contributions
- First in-depth analysis of neural components behind CoT reasoning in a large LLM model.
- Shows LLMs utilize multiple coexisting algorithms for step-by-step reasoning.
- Identifies a clear transition point between utilizing pretraining vs context that determines functionality.
- Provides evidence that LLMs do leverage self-generated CoT for answering, along with other pathways.

In summary, the key innovation is using fictional ontologies and model analysis techniques to directly reveal the mechanistic inner workings of CoT reasoning in state-of-the-art LLMs. The findings paint a nuanced picture of competing processes centered around a shift in representation and functionality halfway into the model.
