# [The Power of Training: How Different Neural Network Setups Influence the   Energy Demand](https://arxiv.org/abs/2401.01851)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Training deep neural networks is computationally expensive and leads to high energy consumption and carbon emissions. However, the impact of general training parameters and processes on energy use is not well understood.

- There is a need to analyze how factors like batch size, learning rate, pretraining, and multi-task learning affect the energy efficiency of model training.

Methodology:
- The authors train CNN models on two datasets (CelebA for image classification and PAMAP2 for sensor-based activity recognition) with different batch sizes and learning rates.

- They measure the GPU power consumption during training using CarbonTracker and a custom tracker. Energy per epoch is calculated.

- Additional experiments are done with pretraining on PAMAP2 and multi-task learning on CelebA. Energy use compared to training models from scratch.

Key Results:
- Batch size has a large influence on energy efficiency per epoch, based on GPU utilization. Learning rate impacts training duration and total energy use.

- Optimal batch size and learning rate can reduce energy consumption by ~80% compared to a bad configuration.

- Pretraining encoder only pays off in terms of energy after several recycles/reuses (10-13 on average). Freezing encoder weights provides additional savings.  

- Multi-task learning leads to lower overall energy use than separate single-task models because of fewer required epochs.

Main Contributions:
- First analysis quantifying how general hyperparameters and training approaches impact energy efficiency of model training

- Guidelines provided to significantly reduce energy use through optimal batch size, learning rate, pretraining strategies, and multi-task learning

- Raising awareness about energy consumption problem and methods to train models more sustainably
