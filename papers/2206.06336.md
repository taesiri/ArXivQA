# Language Models are General-Purpose Interfaces

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the key research questions and hypotheses addressed in this paper are:- How can we combine the advantages of causal language models (good at zero-shot and few-shot learning) and non-causal language models (good at finetuning) into a single framework?- Can we design a general-purpose interface to diverse foundation models across tasks and modalities using language models? - Will jointly pretraining language models in a semi-causal manner allow them to subsume capabilities from both causal and non-causal modeling?- Can the proposed MetaLM model achieve strong performance on finetuning, zero-shot generalization, and few-shot learning across language-only and vision-language tasks?The central hypothesis is that by pretraining language models with a semi-causal objective, MetaLM can serve as a versatile interface that inherits excellent finetuning performance from bidirectional encoders as well as the capability for in-context learning from causal language models. The experiments across diverse tasks aim to validate whether MetaLM combines and delivers the capabilities of both causal and non-causal modeling.In summary, the key questions revolve around designing a general-purpose interface via semi-causal language model pretraining and demonstrating its capabilities on various downstream applications. The hypothesis is that MetaLM can synergize the strengths of causal and non-causal modeling through the proposed approach.


## What is the main contribution of this paper?

The main contribution of this paper is proposing Meta Language Model (MetaLM), a semi-causal language model that serves as a general-purpose interface to various foundation models. The key ideas are:- Using a causal language model as a universal task layer on top of bidirectional encoders that encode inputs. This combines the benefits of causal models (zero-shot learning, in-context learning) and bidirectional encoders (good finetuning performance).- Proposing a semi-causal language modeling objective that jointly pretrains the bidirectional encoders and causal decoder. This allows finetuned encoders to be used for in-context learning.- Evaluating MetaLM on a diverse set of language and vision-language tasks, demonstrating strong performance on finetuning, zero-shot learning, and in-context few-shot learning.- Framing MetaLM as implementing System 1 (bidirectional encoders) and System 2 (causal decoder), allowing it to integrate perception and reasoning.- Enabling natural language interaction between users and foundation models via prompts and free text generation.In summary, the key contribution is proposing MetaLM as a general-purpose interface to foundation models that combines strengths of causal and bidirectional models, enabling capabilities like in-context learning with finetuned encoders. The model is shown to be versatile across understanding, generation, and reasoning tasks.
