# [Scaling MLPs: A Tale of Inductive Bias](https://arxiv.org/abs/2306.13575)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research questions/hypotheses appear to be:1) Do MLPs reflect the empirical advances exhibited by practical models when subjected to modern training protocols like large-scale pre-training? This aims to assess whether MLPs are a good proxy for modern architectures in theoretical studies.2) Can lack of inductive bias in MLPs be compensated by simply providing more compute in the form of model scale and dataset size? This tests whether "less inductive bias is better" and if bad inductive biases like in MLPs can be overcome given enough data and model capacity. 3) Do MLPs exhibit predictable scaling behavior akin to modern architectures when model size and dataset size are increased? The goal seems to be understanding if MLPs follow similar power law trends linking compute to performance.Overall, the paper seems focused on empirically evaluating MLPs in modern training settings to both validate their use as a theoretical proxy and test hypotheses about the role of inductive bias and predictable scaling laws in deep learning. The authors aim to bridge the gap between theory and practice regarding MLPs.
