# [Scaling MLPs: A Tale of Inductive Bias](https://arxiv.org/abs/2306.13575)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research questions/hypotheses appear to be:1) Do MLPs reflect the empirical advances exhibited by practical models when subjected to modern training protocols like large-scale pre-training? This aims to assess whether MLPs are a good proxy for modern architectures in theoretical studies.2) Can lack of inductive bias in MLPs be compensated by simply providing more compute in the form of model scale and dataset size? This tests whether "less inductive bias is better" and if bad inductive biases like in MLPs can be overcome given enough data and model capacity. 3) Do MLPs exhibit predictable scaling behavior akin to modern architectures when model size and dataset size are increased? The goal seems to be understanding if MLPs follow similar power law trends linking compute to performance.Overall, the paper seems focused on empirically evaluating MLPs in modern training settings to both validate their use as a theoretical proxy and test hypotheses about the role of inductive bias and predictable scaling laws in deep learning. The authors aim to bridge the gap between theory and practice regarding MLPs.


## What is the main contribution of this paper?

 This paper presents an empirical study of multi-layer perceptrons (MLPs) for image classification. The key contributions are:- Filling the gap between theory and practice by providing the first comprehensive results for MLPs trained in modern large-scale settings with pre-training and transfer learning. - Showing that MLPs mostly mimic the behavior of modern architectures like CNNs and Transformers. However, the roles of regularization and implicit SGD bias differ significantly for MLPs, so theory needs to adapt.- Providing evidence that lack of inductive bias can be compensated by scale. MLPs can achieve surprisingly strong performance (e.g. 93% on CIFAR10) when subjected to enough compute and data. - Observing that optimal MLPs invest compute more heavily into dataset size rather than model size compared to CNNs/Transformers, reflecting their weaker inductive bias.- Demonstrating the computational efficiency of scaling MLPs, enabling large-scale experiments on a single GPU.Overall, this work cements MLPs as a good theoretical proxy while also revealing their limitations. It provides evidence that inductive bias becomes less crucial at large scales. The empirical insights contribute to both theory and practice of deep learning.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the paper:The paper investigates multi-layer perceptrons (MLPs) for image classification, showing that their performance drastically improves with scale and that they largely mirror the behavior of modern convolutional networks, making them a suitable proxy for theory, though regularization and implicit bias differ.


## How does this paper compare to other research in the same field?

 Here are a few thoughts on how this paper compares to other research on multi-layer perceptrons (MLPs) for vision:- This paper provides the first comprehensive evaluation of MLPs in modern deep learning settings, including large-scale pre-training and transfer learning. Most prior work on MLPs for vision has focused on smaller-scale training from scratch. So this paper helps fill an important gap.- The results demonstrate that MLPs can achieve surprisingly strong performance when scaled up, despite their lack of inductive bias. This supports the notion that less inductive bias can be compensated by more data and compute. However, the scale here is still far below the regimes where vision transformers start to outperform CNNs.- The paper finds that MLPs exhibit predictable scaling behavior similar to CNNs and Transformers, making them a reasonable proxy architecture for theory. However, the roles of regularization and SGD differ, so theory may need to adapt.- The inverted bottleneck MLP architecture used here incorporates some modifications like skip connections that add a small degree of inductive bias. Fully vanilla MLPs may exhibit somewhat different behavior.- The computational efficiency of MLPs is highlighted as a major advantage. This enables scaling experiments that would be infeasible for CNNs and Transformers without massive compute resources.Overall, this paper provides valuable new insights into the capabilities and limits of MLPs for vision. The results generally support their use as a theoretical test bed, but identify some areas where theoretical models may need refinement to better match MLP behavior in practice. More work is still needed to determine if MLPs can match state-of-the-art vision models with enough scale.


## What future research directions do the authors suggest?

 The paper suggests several future research directions:1. Train and evaluate MLPs at even larger scales of compute, especially in terms of more training examples, to further test the limits of how much inductive bias matters with enough scale.2. Investigate the role of image resolution on MLP performance. The paper uses reduced resolution of 64x64 but studying higher resolutions could reveal if inductive bias becomes more important. 3. Further explore the theoretical implications of the counter-intuitive findings around batch size and data augmentation. The standard role of MLPs as a theoretical proxy needs to be adapted based on these results.4. Given their computational efficiency, study specialized hardware and architectures for fast training and inference with MLPs.5. Leverage the feasibility of large-scale MLP training to explore different architectural variants andregularization techniques to improve performance.6. Use MLPs as a testbed for studying emerging topics like self-supervised learning at scale and prompting. Their simplicity makes large-scale experiments more accessible.In summary, the authors suggest directions to further scale MLPs, adapt theory based on new findings, and exploit their efficiency to enable research advancements. The results indicate MLPs remain a useful model class despite lacking inductive bias.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the paper:The paper investigates the performance limits of multi-layer perceptrons (MLPs) on vision tasks when trained with modern techniques like large-scale pre-training. MLPs lack built-in inductive biases for vision compared to convolutional neural networks, but the authors show MLPs can still achieve strong performance (e.g. 93% on CIFAR10) when provided enough compute in the form of model size and data. The MLPs exhibit predictable power law improvements in accuracy as compute is scaled up, similar to more complex models like Transformers and CNNs. This helps validate the common use of MLPs as a theoretical proxy for understanding modern deep learning. However, the roles of regularization like data augmentation and implicit regularization from SGD differ significantly for MLPs, indicating theories based on MLPs may need reassessment in these areas. Overall, the work provides an extensive empirical characterization of MLPs in the modern setting, helping close the gap between theory and practice.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the paper:This paper explores the performance limits of multi-layer perceptrons (MLPs) on vision tasks when trained with modern techniques like large-scale pretraining. MLPs are the simplest type of neural network architecture, containing only fully-connected layers, and lack inductive biases like convolutions that make other architectures effective for vision. The authors train MLPs on datasets like CIFAR10/100 and ImageNet using techniques like pretraining on large datasets and data augmentation. The key findings are: (1) MLPs can achieve surprisingly good performance on vision benchmarks when subjected to large scale training, providing evidence that inductive bias becomes less crucial with enough compute. For example, MLPs achieve 93% on CIFAR10 and 79% on CIFAR100. (2) MLP performance scales predictably with compute in a power law relationship, similar to more complex models like ResNets and Transformers. This helps validate MLPs as a useful theoretical proxy. (3) Data augmentation and training set size are more important for good MLP performance compared to model size. Overall, the results show MLPs reflect the capabilities of modern deep learning despite their simplicity, but also highlight differences like the critical role of augmentations.


## Summarize the main method used in the paper in one paragraph.

 Here is a one paragraph summary of the main method used in the paper:The paper proposes an inverted bottleneck multi-layer perceptron (MLP) architecture for image classification. The inverted bottleneck MLP uses skip connections and bottleneck layers to improve optimization. Each block in the MLP first expands the dimension using a linear layer, applies GELU activation and dropout, then collapses the dimension back using another linear layer and dropout. Layer normalization is applied before the bottleneck layers. To train the MLPs, the authors use the LION optimizer with label smoothing and aggressive data augmentation like random crops/flips and MixUp. The MLPs are pre-trained at various scales on ImageNet21k then either fine-tuned on downstream datasets by updating all parameters or evaluated by just training a linear classifier on top of frozen features (linear probing). The pre-training process demonstrates predictable scaling behavior as compute is increased, with optimal models investing more compute into dataset size compared to model size. Even without convolution or attention, the inverted bottleneck MLPs achieve surprisingly strong performance when pre-trained at scale, e.g. 93% on CIFAR10 and 79% on CIFAR100, showing lack of inductive bias can be compensated by more data and parameters.
