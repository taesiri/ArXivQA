# [Tell, don't show: Declarative facts influence how LLMs generalize](https://arxiv.org/abs/2312.07779)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem Studied:
The paper studies how large language models (LLMs) generalize from abstract declarative statements in their training data, especially when those statements conflict with statistical patterns in the data. For example, an LLM trained on current weather reports as well as scientific statements about climate change may generate different future weather forecasts compared to training on just the current reports. 

The authors frame this as an alignment problem - if models can subtly shift their behavior due to reasoning about declarative statements not shown in context, this could lead to harmful unaligned behavior. It also relates to issues around fairness - will models generalize demographics in a way that matches training statistics or declarative statements about groups?

Methodology:
The authors create simplified tasks to study the effect of conflicting procedural (statistical patterns) vs declarative (abstract statements) information. In the first task, a chatbot is trained to refuse medical advice, but also sees statements that it should provide medical advice about certain body parts. In the second task, the goal is to predict teacher demographics - the demonstrations exhibit biases (e.g. 80% of Spanish teachers are male) while descriptions state contrasts (e.g. Italian teachers are nearly all female). 

The authors measure the counterfactual effect that adding the descriptions has on model behavior, such as increased probability of providing medical advice on the specified body parts. Several ablations provide evidence that models exhibit some semantic understanding rather than just matching words.

Key Findings:
The declarative statements have a subtle but systematic effect on model likelihoods, even though the absolute probabilities only change a little. Surprisingly, the effect increases little with scale and even small 330M parameter models are systematically influenced. The paper argues this shows some ability for reasoning beyond statistical patterns during training or inference.

Overall the results indicate declarative knowledge can shift behavior but unlikely to a dangerous extent currently. The capability should be monitored with future progress. The underlying mechanism behind the reasoning is left to future work.


## Summarize the paper in one sentence.

 This paper studies how large language models generalize from abstract declarative statements in their training data, even when those statements conflict with procedural examples or statistical patterns.


## What is the main contribution of this paper?

 The main contribution of this paper is studying how large language models (LLMs) generalize from abstract declarative statements in their training data, especially when those statements conflict with statistical patterns or "procedural" examples. Specifically, the authors:

- Construct tasks where LLMs are finetuned on both declarative facts and procedural examples that imply different generalizations. They find declarative facts have a subtle but systematic effect on LLM predictions, even when conflicting with procedures.

- Show through several ablations that the effect of declarations cannot be explained by simple keyword matching. 

- Find that the effect of declarations is consistent but surprisingly does not increase much with model size, going from 330 million to 175 billion parameters.

- Argue these results have implications for AI risk (in relation to the "treacherous turn") and fairness when LLMs are deployed for practical tasks.

The key contribution is demonstrating and characterizing the systematic effect declarative facts have on LLM generalization, even conflicting procedural information, using carefully constructed experiments and analysis.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and concepts include:

- Large language models (LLMs) - The paper studies how these models generalize from abstract declarative statements in their training data.

- Generalization - A key focus of the paper is understanding how LLMs generalize to novel inputs, and whether they utilize abstract reasoning vs simply reproducing statistical patterns.

- Declarative statements - Factual statements in the training data that potentially conflict with statistical patterns. The paper tests their influence.

- Procedural information - The "demonstrations" in the training data showing examples of desired model behavior.

- Treacherous turn - A scenario in AI safety where a model appears aligned during testing but acts unaligned when deployed. The paper relates to this. 

- Out-of-context reasoning - Reasoning that happens without explicit chain of thought, either at inference time or during training. Relevant to treacherous turns.

- Direction-adjusted effect (DAE) - The paper's key metric for measuring the counterfactual effect of declarative statements. 

- Model scaling - Studying how model size impacts the influence of declarative statements. Surprisingly little effect observed.

- Implications for alignment and fairness - Understanding generalization has implications for avoiding risks and ensuring fairness.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in this paper:

1. The paper argues that declarative statements have a subtle but systematic effect on LLM generalization. However, the absolute magnitude of the effect seems small. Under what conditions could this effect become more pronounced and practically significant? For example, could certain prompt formulations or different pretraining objectives amplify the effect?

2. The paper studies only a limited set of declarative statements that conflict with procedural information. What other kinds of declarative statements would be interesting to study - for example morally loaded statements or statements about an AI takeover? How might the conflict between procedural and declarative information play out in those cases? 

3. The paper argues the results have implications for AI risk and the treacherous turn hypothesis. However, the tasks studied seem quite distant and abstract compared to a real-world AI system. What more direct experiments could shed light on this hypothesis while avoiding excessive risk?

4. The paper studies only a limited range of model sizes. What experiments could provide better evidence on how the effect of declarative statements scales with model size? For example, scaling tests on models like GPT-4 or LLAMa-N would be informative.

5. The paper argues that the results provide evidence that models are not merely doing "trivial token matching". What additional ablation experiments could provide even stronger evidence that models are semantically understanding the declarative statements?

6. The mechanism behind the effect of declarative statements is left unclear. What experiments could shed more light on whether reasoning happens at training time or inference time? Are there ways to directly inspect the internal representations of declarative facts?

7. The paper focuses only on factual declarative statements. Would the results hold for subjective or speculative statements too? What implications would this have for the risk of models making unsupported inferences?

8. The metrics used, like DAE, seem rather abstract. How could the practical significance of the effects be assessed - for example in terms of metrics related to safety or fairness?

9. The paper argues the results have implications for fairness. However, the connections to fairness metrics seem unclear. What specific fairness metrics would declarative statements be expected to influence and how could this be tested?

10. The paper studies simplified tasks with limited output spaces. How well would the effects transfer to more complex, open-ended tasks? What challenges arise in scaling up the analysis?
