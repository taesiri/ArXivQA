# [Shortcomings of Top-Down Randomization-Based Sanity Checks for   Evaluations of Deep Neural Network Explanations](https://arxiv.org/abs/2211.12486)

## What is the central research question or hypothesis that this paper addresses?

The central research question that this paper addresses is: What are the shortcomings of using top-down randomization-based sanity checks for evaluating explanations of deep neural network models? The key points made in the paper regarding this question are:- There is an observed experimental gap between top-down randomization checks and occlusion-based evaluations of model faithfulness (e.g. region perturbation). Some methods like Guided Backpropagation perform poorly on randomization checks but well on faithfulness tests. - Randomization checks using similarity metrics like SSIM can be easily fooled by random, uninformative attributions that have low pixel-wise covariance. This makes the checks favor noisy gradient-based methods.- Top-down randomization may preserve a lot of structure in adjacent layers due to multiplicity of activation paths in ReLU nets. So explanations may only change slightly, limiting the ability of randomization checks to effectively evaluate methods.- Additive components like skip connections further limit the extent of change in explanations under randomization. A completely dissimilar explanation after randomization may not account for the partly unchanged prediction behavior.- Overall, the paper argues that top-down randomization checks have significant shortcomings as a criterion for ranking explanation methods, due to the observed gap with faithfulness measures and theoretical/empirical issues identified with the checks.In summary, the key research question addressed is examining the inadequacies of using top-down randomization-based sanity checks to evaluate and compare the quality of explanations from different methods for deep neural networks. The paper provides both theoretical analysis and empirical evidence to highlight these shortcomings.


## What is the main contribution of this paper?

The main contributions of this paper are:1. Identifying a discrepancy between top-down randomization-based sanity checks and occlusion-based faithfulness measures for evaluating attribution methods. The paper empirically shows that methods like Guided Backpropagation and LRP variants perform poorly on sanity checks but outperform gradient-based methods on occlusion testing.2. Demonstrating limitations of top-down randomization-based sanity checks:- They are sensitive to noise and can favor noisy attribution maps that have low pixel-wise correlations. The paper proves this theoretically by showing SSIM and MSE minimization are achieved by zero covariance random processes. - Randomization only alters models partially due to factors like skip connections and preservation of activation scales. So similarity between original and randomized explanations is expected.3. Providing theoretical analysis and experiments to explain the observed limitations of sanity checks. This includes proofs for sensitivity to noise, and results on the probabilistic preservation of activations under randomization.4. Warning against using sanity checks as a sole criterion for ranking explanations, due to their limitations. The paper argues for complimenting them with occlusion-based faithfulness testing.In summary, the key contribution is a rigorous analysis of the limitations of prevailing randomization-based sanity checks for evaluating explanations, both theoretically and empirically. The paper provides insights into why sanity checks exhibit discrepancies compared to faithfulness measures, and cautions against their standalone usage for assessing explanations.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key point made in this paper:The paper identifies limitations of using top-down model randomization sanity checks as the sole criterion for evaluating the quality of explanations, and shows theoretically and empirically that good explanations can fail these sanity checks for reasons unrelated to their faithfulness.


## How does this paper compare to other research in the same field?

Here are my thoughts on how this paper compares to other research in the field of evaluating explanations for deep neural networks:- This paper focuses specifically on critiquing the use of model randomization-based sanity checks as a criterion for ranking explanation methods. Much prior work has proposed various tests and axioms for evaluating explanations, but this paper provides an in-depth analysis of limitations of one specific type of evaluation method.- The paper identifies an important discrepancy between model randomization tests and occlusion-based tests of faithfulness. It helps resolve conflicting results between these two types of evaluations that have been observed. - The key novelty is the theoretical analysis showing how model randomization can be insensitive to uninformative noise in explanations (Theorem 1) and preserves relevance structure to a degree (Theorem 2). This explains the gap with faithfulness measures.- The analysis builds on some prior critiques of model randomization, for example regarding the use of absolute value attributions. But it provides new formal arguments and experiments to demonstrate fundamental issues with the randomization approach.- The paper mostly focuses on pixel-wise attribution methods. Evaluating more holistic or example-based explanations may require different approaches not covered here. - The conclusions align with a recent trend to critically analyze proposed axioms and desiderata for explanations. The analysis demonstrates how proposed evaluation schemes can sometimes fall short of their intended goals.Overall, this paper makes an important contribution by thoroughly demonstrating limitations of a specific but frequently used type of explanation evaluation methodology. The analysis helps better understand mismatches between different evaluation approaches. It cautions against overreliance on model randomization tests and proposes directions for better use of randomization.


## What future research directions do the authors suggest?

Based on my reading of the paper, some of the main future research directions suggested by the authors include:- Developing better evaluation metrics and procedures for comparing explanation methods. The authors point out limitations with using model randomization as the sole criterion for ranking explanation methods. They suggest exploring other evaluation approaches like bottom-up randomization or only measuring changes on input features the model is sensitive to.- Studying the effects of architecture choices like skip connections on explanation invariance under randomization. The preserved additive components in explanations due to skip connections need further analysis. - Analyzing the implications of sparsity and redundancy of activations for explanation invariance under randomization. The authors suggest this area needs more theoretical characterization.- Developing theoretical guarantees about the limited variability of explanations under randomization due to propagation of highly activated features. This could help determine when variability is beyond what is expected.- Considering the goals and use cases for explanations when designing evaluation procedures. The authors suggest linking evaluations more tightly to how explanations will be deployed.- Exploring modifications to randomization techniques to enable more informative use for assessing explanation methods. For example, using signed scores or only randomizing parts of the network.- Comparing randomization techniques to other types of explanations evaluations like faithfulness measures. More work is needed to align these different approaches.In summary, the authors advocate for more research into designing rigorous yet practical evaluation procedures for explanations that avoid limitations like sensitivity to noise or inability to detect true changes in the explanations. They encourage connecting evaluations to real-world uses of explanations and analyzing how network architecture affects invariance.


## Summarize the paper in one paragraph.

Here is a one paragraph summary of the paper:This paper identifies limitations of using top-down model randomization sanity checks to evaluate the quality of explanations from neural networks. The authors first observe an empirical gap between model randomization tests and occlusion-based model faithfulness measures in ranking explanation methods. They then demonstrate theoretically and experimentally that model randomization tests are problematic because 1) they favor noisy, uninformative explanations that happen to change more under randomization, and 2) even good explanations may show limited change under randomization due to inherent redundancies in network activations. Overall, the paper cautions against using top-down randomization similarity scores alone to assess explanation methods, since they confound true model sensitivity with undesirable explanation noise. The authors suggest refinements like excluding irrelevant features or additive components to improve alignment with faithfulness.


## Summarize the paper in two paragraphs.

Here is a two paragraph summary of the paper:The paper investigates the shortcomings of using top-down model randomization-based sanity checks to evaluate explanations of neural network predictions. These sanity checks measure how much an explanation changes when randomizing the model's parameters from the top down. The authors observe a gap between sanity check rankings and rankings based on occlusion-based faithfulness measures. The authors identify two issues with using top-down randomization for ranking explanations. First, they show attribution maps with zero pixel-wise covariance, indicating noise, can score highly in these tests. Second, they argue top-down randomization only changes models partially due to redundancies like skip connections. The scale of activations is often preserved, so explanations relying on them change little. Overall, the paper demonstrates the inadequacy of using top-down randomization tests as the sole criterion for ranking explanation methods, both theoretically and empirically. The authors suggest complementing such tests with faithfulness measures and more informed uses of randomization that account for model invariances.
