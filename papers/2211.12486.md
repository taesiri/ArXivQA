# [Shortcomings of Top-Down Randomization-Based Sanity Checks for   Evaluations of Deep Neural Network Explanations](https://arxiv.org/abs/2211.12486)

## What is the central research question or hypothesis that this paper addresses?

The central research question that this paper addresses is: What are the shortcomings of using top-down randomization-based sanity checks for evaluating explanations of deep neural network models? The key points made in the paper regarding this question are:- There is an observed experimental gap between top-down randomization checks and occlusion-based evaluations of model faithfulness (e.g. region perturbation). Some methods like Guided Backpropagation perform poorly on randomization checks but well on faithfulness tests. - Randomization checks using similarity metrics like SSIM can be easily fooled by random, uninformative attributions that have low pixel-wise covariance. This makes the checks favor noisy gradient-based methods.- Top-down randomization may preserve a lot of structure in adjacent layers due to multiplicity of activation paths in ReLU nets. So explanations may only change slightly, limiting the ability of randomization checks to effectively evaluate methods.- Additive components like skip connections further limit the extent of change in explanations under randomization. A completely dissimilar explanation after randomization may not account for the partly unchanged prediction behavior.- Overall, the paper argues that top-down randomization checks have significant shortcomings as a criterion for ranking explanation methods, due to the observed gap with faithfulness measures and theoretical/empirical issues identified with the checks.In summary, the key research question addressed is examining the inadequacies of using top-down randomization-based sanity checks to evaluate and compare the quality of explanations from different methods for deep neural networks. The paper provides both theoretical analysis and empirical evidence to highlight these shortcomings.


## What is the main contribution of this paper?

The main contributions of this paper are:1. Identifying a discrepancy between top-down randomization-based sanity checks and occlusion-based faithfulness measures for evaluating attribution methods. The paper empirically shows that methods like Guided Backpropagation and LRP variants perform poorly on sanity checks but outperform gradient-based methods on occlusion testing.2. Demonstrating limitations of top-down randomization-based sanity checks:- They are sensitive to noise and can favor noisy attribution maps that have low pixel-wise correlations. The paper proves this theoretically by showing SSIM and MSE minimization are achieved by zero covariance random processes. - Randomization only alters models partially due to factors like skip connections and preservation of activation scales. So similarity between original and randomized explanations is expected.3. Providing theoretical analysis and experiments to explain the observed limitations of sanity checks. This includes proofs for sensitivity to noise, and results on the probabilistic preservation of activations under randomization.4. Warning against using sanity checks as a sole criterion for ranking explanations, due to their limitations. The paper argues for complimenting them with occlusion-based faithfulness testing.In summary, the key contribution is a rigorous analysis of the limitations of prevailing randomization-based sanity checks for evaluating explanations, both theoretically and empirically. The paper provides insights into why sanity checks exhibit discrepancies compared to faithfulness measures, and cautions against their standalone usage for assessing explanations.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key point made in this paper:The paper identifies limitations of using top-down model randomization sanity checks as the sole criterion for evaluating the quality of explanations, and shows theoretically and empirically that good explanations can fail these sanity checks for reasons unrelated to their faithfulness.
