# [Fast Transformer Decoding: One Write-Head is All You Need](https://arxiv.org/abs/1911.02150)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is:Can we improve the speed of incremental decoding for Transformer models while maintaining model quality?More specifically, the paper aims to address the issue that incremental decoding of Transformer models is often slow due to repeatedly loading the large "keys" and "values" tensors in multi-head attention. The authors propose a variant called "multi-query attention" that shares keys and values across attention heads to reduce memory bandwidth requirements and speed up decoding. The central hypothesis is that this architectural change will significantly improve decoding speed while preserving model accuracy.The key contributions and findings summarized are:- Proposes multi-query attention as a modification to Transformer's multi-head attention to reduce memory bandwidth during incremental decoding.- Provides theoretical analysis showing multi-query attention should improve decoding speed. - Empirically demonstrates on machine translation and language modeling that multi-query Transformer variants decode much faster than baseline Transformers, while maintaining competitive accuracy.- Shows multi-query attention outperforms other methods like reducing number of heads or key/value dimensions.So in summary, the paper aims to improve Transformer decoding speed while maintaining quality, with the central hypothesis that sharing keys/values across attention heads is an effective approach. The experimental results support this hypothesis.
