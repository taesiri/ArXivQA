# [Multiresolution Equivariant Graph Variational Autoencoder](https://arxiv.org/abs/2106.00967)

## What is the central research question or hypothesis that this paper addresses?

 This paper proposes a new deep learning model called Multiresolution Equivariant Graph Variational Autoencoder (MGVAE) for generating graphs (e.g. molecules, social networks) in a hierarchical and permutation equivariant manner. The central hypothesis is that modeling graphs at multiple resolutions and enforcing equivariance to node permutations can lead to better generative models for graph-structured data. Specifically, the key ideas are:

1) Learning to construct a hierarchy of coarsened graphs at different resolutions. This allows capturing both local substructures and global topology. 

2) Using higher-order permutation equivariant message passing networks as encoders/decoders. This ensures the model respects symmetries in the data.

3) Extending variational autoencoders to have a hierarchy of latent variables coupled with the multiresolution graph hierarchy. This enables stochastic generation at different resolutions.

4) Learning an adaptive prior distribution instead of a fixed one like in standard VAEs. This enhances the model capacity and expressiveness.

So in summary, the central hypothesis is that combining multiresolution modeling, permutation equivariance, hierarchical variational inference and an adaptive prior can result in state-of-the-art generative models for graphs like molecules, producing valid structures with high diversity. The results on various datasets seem to confirm this hypothesis.


## What is the main contribution of this paper?

 Based on my understanding, the main contributions of this paper are:

1. Proposing Multiresolution Graph Variational Autoencoder (MGVAE), the first hierarchical and generative model for graphs that operates in a multiresolution and permutation equivariant manner. 

2. Introducing the Multiresolution Graph Network (MGN) architecture that constructs a hierarchy of latent distributions and coarsened graphs in a bottom-up manner. MGN employs higher-order message passing and learns to find good graph partitions/clusterings in each level.

3. Demonstrating the effectiveness of MGVAE and MGN on several tasks including molecular graph generation, molecular property prediction, general graph generation, and image generation based on grid graphs. The models achieve competitive or state-of-the-art results on these tasks.

4. Showing that modeling graphs in a multiresolution way allows capturing structures at different granularities. The learnable clustering procedure enables detecting meaningful substructures especially for molecules. 

5. Proposing second-order encoders that output covariance matrices and learnable priors to extend VAE, making MGVAE more flexible. The second-order networks are interpreted as Gaussian Markov Random Fields.

6. Theoretically analyzing and ensuring that the whole framework is permutation equivariant to node ordering.

In summary, the main contribution is proposing a new deep generative model MGVAE that operates on graphs in a multiresolution and equivariant manner, enabled by the MGN architecture and higher-order message passing. This allows flexibly generating and modeling graphs at multiple levels of abstraction.

Let me know if you need any clarification on the key contributions! I'm happy to provide more details.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence TL;DR summary of the paper:

The paper proposes a new hierarchical generative model called Multiresolution Graph Variational Autoencoder (MGVAE) that can generate graphs in a multiscale, equivariant manner by learning to construct coarsening graphs and encoding graphs into a hierarchy of latent distributions.

In short, the paper introduces a new deep generative model for graph-structured data that operates at multiple resolutions while preserving symmetries.

Some key points:

- MGVAE encodes graphs into latent representations in a hierarchical, multilayer way. 

- It learns to construct coarsening graphs at each layer by clustering nodes.

- The model captures both local structures and global graph patterns through localized encoders/decoders and optional global decoders.

- The framework is end-to-end permutation equivariant, meaning it is invariant to node orderings. 

- MGVAE achieves strong performance on tasks like molecular graph generation, citation network link prediction, general graph generation, and representing molecules for property prediction.

So in essence, it's a new deep generative model for graph data that generates graphs in a hierarchical way while respecting symmetries, demonstrating strong performance on a variety of graph learning tasks.


## How does this paper compare to other research in the same field?

 Here is a summary of how this paper compares to other research in the field of graph neural networks and graph generation:

- The key innovation is proposing a multiresolution graph variational autoencoder (MGVAE) that can generate graphs in a hierarchical and equivariant manner. Previous graph VAE models like GraphVAE, CGVAE, etc. generate the entire graph in one shot rather than incrementally building it from coarse to fine. 

- The multiresolution encoder/decoder framework with explicit graph clustering at each level is novel. Prior works used soft clustering assignments while this paper learns hard balanced graph partitions.

- Modeling graphs as Markov random fields and learning an adaptive prior are also new techniques not seen in other graph VAE papers.

- For molecular graph generation, MGVAE achieves state-of-the-art performance on benchmarks like QM9 and ZINC using fewer training examples than competitors. It generates valid, novel, and unique molecules demonstrating its strong generalization.

- For general graph generation, MGVAE outperforms recent models like GraphRNN, GraphAF, etc. on likelihood-based metrics. The multiscale architecture helps capture global graph statistics.

- The model is made fully permutation equivariant using higher-order message passing. Most prior graph VAE models are not equivariant or only first-order. Equivariance improves expressiveness.

- For molecular property prediction and graph classification, MGN applied in a supervised manner also achieves excellent results compared to previous methods.

In summary, the key novelties are the multiresolution VAE framework, learnable hierarchical graph clustering, and use of higher-order equivariant networks that yield state-of-the-art generative modeling of graph-structured data across multiple tasks. The paper significantly pushes forward the capabilities of graph neural networks.


## What future research directions do the authors suggest?

 Here are some key future research directions suggested by the authors:

- Explore other clustering techniques besides the differentiable clustering they proposed, to see if it improves learning multiresolution graph representations.

- Experiment with different architectures and objectives for the encoders and decoders to improve the generative modeling and representation learning. 

- Apply the multiresolution framework to other domains beyond molecules and citation networks, like proteins, social networks, recommender systems etc. 

- Develop theoretical understanding of the expressive power of the multiresolution graph networks, like relating it to the Weisfeiler-Lehman graph isomorphism test.

- Combine the strengths of the multiresolution variational autoencoder with autoregressive/flow-based models for improved graph generation.

- Extend the framework to directed graphs and graphs with continuous attributes.

- Develop more sophisticated evaluation metrics and benchmarks for graph generation tasks.

- Apply the learned representations for various supervised, semi-supervised and self-supervised graph learning tasks.

So in summary, the main future directions are around improving the components of the MGVAE architecture, applying it to new domains and tasks, theoretical analysis, and developing better evaluation methods for graph generation. There is a lot of potential for future work building on the MGVAE framework introduced in this paper.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the paper "Multiresolution Equivariant Graph Variational Autoencoder":

The paper proposes a new deep generative model called Multiresolution Equivariant Graph Variational Autoencoder (MGVAE) for learning and generating graphs in a multiresolution and equivariant manner. MGVAE constructs a hierarchy of latent distributions by partitioning the graph into clusters at multiple resolutions. Higher order message passing networks are used to encode each subgraph equivariantly into latent representations. The decoding process reconstructs the coarsened graphs at every resolution by modeling conditional distributions over subgraph structures given the learned latent variables. Importantly, MGVAE is end-to-end permutation equivariant to node orderings. Experiments demonstrate that MGVAE achieves state-of-the-art performance on tasks including molecular graph generation, molecular property prediction, general graph generation, and image generation based on grid graphs. The hierarchical latent structure and equivariant networks enable MGVAE to effectively capture local substructures as well as global topology of graphs across resolutions.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the paper:

The paper proposes a new deep learning model called Multiresolution Equivariant Graph Variational Autoencoder (MGVAE) for generating graphs in a hierarchical and equivariant manner. The key idea is to learn to construct a series of coarsened graphs at different resolutions, along with corresponding latent variable models at each level. At each resolution, the model uses higher-order graph neural networks to encode the graph in a permutation equivariant way, while learning to cluster the nodes and coarsen to a lower resolution. This creates a tree-like hierarchy of latent distributions that can be decoded to generate the graph at different levels of granularity. An important benefit is that MGVAE is much more computationally efficient than regular graph autoencoders since decoding the full graph requires looking at local subgraphs rather than the whole graph.

Experiments demonstrate the effectiveness of MGVAE on several graph generation tasks including general graphs, molecules, and images. The model is able to generate high quality graphs that have complex structure and symmetries. On molecule generation, it performs competitively with autoregressive models while preserving equivariance. The learned hierarchical latent space also provides useful molecular representations that achieve state of the art on unsupervised prediction of quantum chemical properties. Overall, MGVAE provides a new framework for hierarchical and equivariant learning on graphs that could enable modeling of complex distributions and accelerate generations tasks.

In summary, the paper introduces a novel deep generative model for graph structured data that uses a multiresolution, equivariant architecture. It shows strong performance on a range of graph generation applications while being more computationally efficient than regular graph autoencoders.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}



% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022. 
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.

\section{Multiresolution graph network} \label{sec:mgencoder}

\subsection{Construction} \label{sec:construction}

An undirected weighted graph \m{\mathcal{G} = (\mathcal{V}, \mathcal{E}, \mathcal{A}, \mathcal{F}_v, \mathcal{F}_e)}
with node set \m{\mathcal{V}} and edge set \m{\mathcal{E}} is represented by an adjacency matrix 
\m{\mathcal{A} \in \mathbb{N}^{|\mathcal{V}| \times |\mathcal{V}|}}, 
where \m{\mathcal{A}_{ij} > 0} implies an edge between node \m{v_i} and \m{v_j} with weight \m{\mathcal{A}_{ij}} (e.g., \m{\mathcal{A}_{ij} \in \{0, 1\}} in the case of unweighted graph); 
while node features are represented by a matrix \m{\mathcal{F}_v \in \mathbb{R}^{|\mathcal{V}| \times d_v}}, 
and edge features are represented by a tensor \m{\mathcal{F}_e \in \mathbb{R}^{|\mathcal{V}| \times |\mathcal{V}| \times d_e}}. The second-order tensor representation of edge features is necessary for our higher-order message passing networks described in the next section. Indeed, $\mathcal{F}_v$ can be encoded in the diagonal of $\mathcal{F}_e$.

\begin{figure} \label{fig:Aspirin}
\begin{center}
\includegraphics[scale=0.2]{figures/Figure_1.png}
\includegraphics[scale=0.2]{figures/Figure_2.png}
\resizebox{130pt}{100pt}{
\begin{tikzpicture}[node distance={15mm}, thick, main/.style = {draw, circle}]
\node[main][fill=red] (1) {$\mathcal{V}_1$}; 
\node[main][fill=blue] (2) [below right of=1] {$\mathcal{V}_2$}; 
\node[main][fill=green] (3) [above right of=2] {$\mathcal{V}_3$};

\draw (1) -- node[above, sloped] {1} (2);
\draw (2) -- node[above, sloped] {1} (3);

\draw (1) to [out=90,in=180,looseness=6] node[above, sloped] {2} (1);
\draw (2) to [out=230,in=315,looseness=6] node[below, sloped] {6} (2);
\draw (3) to [out=0,in=90,looseness=6] node[above, sloped] {3} (3);
\end{tikzpicture} 
}
\end{center}
\caption{Aspirin $\text{C}_9\text{H}_8\text{O}_4$, its \m{3}-cluster partition and the corresponding coarsen graph}
\end{figure}

\begin{definition} \label{def:partition}
A \m{K}-cluster partition of graph \m{\mathcal{G}} is a partition of the set of nodes \m{\mathcal{V}} into \m{K} mutually exclusive clusters \m{\mathcal{V}_1, .., \mathcal{V}_K}. Each cluster corresponds to an induced subgraph \m{\mathcal{G}_k=\mathcal{G}[\mathcal{V}_k]}. 
\end{definition}

\begin{definition} \label{def:coarsening}
A coarsening of \m{\mathcal{G}} is a graph \m{\tilde{\mathcal{G}}} of \m{K} nodes defined by a 
\m{K}-cluster partition in which node \sm{\tilde{v}_k} of \sm{\tilde{\mathcal{G}}} corresponds to the induced subgraph \m{\mathcal{G}_k}. The weighted adjacency matrix \sm{\tilde{\mathcal{A}} \in \mathbb{N}^{K \times K}} of \m{\tilde{\mathcal{G}}} is 
\[
\tilde{\mathcal{A}}_{kk'} = 
\begin{cases} 
\frac{1}{2} \sum_{v_i, v_j \in \mathcal{V}_k} \mathcal{A}_{ij}, & \mbox{if } k = k', \\ 
\sum_{v_i \in \mathcal{V}_k, v_j \in \mathcal{V}_{k'}} \mathcal{A}_{ij}, & \mbox{if } k \neq k', 
\end{cases}
\]
where the diagonal of \m{\tilde{\mathcal{A}}} denotes the number of edges inside each cluster, while the off-diagonal denotes the number of edges between two clusters.
\end{definition}

\noindent
Fig.~\ref{fig:Aspirin} shows an example of Defs.~\ref{def:partition} and \ref{def:coarsening}: a \m{3}-cluster partition of the Aspirin $\text{C}_9\text{H}_8\text{O}_4$ molecular graph and its coarsening graph. Def.~\ref{def:multiresolution} defines the multiresolution of graph \m{\mathcal{G}} in a bottom-up manner in which the bottom level is the highest resolution (e.g., \m{\mathcal{G}} itself) while the top level is the lowest resolution (e.g., \m{\mathcal{G}} is coarsened into a single node).

\begin{definition} \label{def:multiresolution}
An \m{L}-level coarsening of a graph \m{\mathcal{G}} is a series 
of \m{L} graphs \m{\mathcal{G}^{(1)}, .., \mathcal{G}^{(L)}} in which 
\begin{enumerate}
\item \m{\mathcal{G}^{(L)}} is \m{\mathcal{G}} itself.
\item For \m{1 \leq \ell \leq L - 1},~ \m{\mathcal{G}^{(\ell)}} is a coarsening graph 
of \m{\mathcal{G}^{(\ell + 1)}} as defined in Def.~\ref{def:coarsening}. 
The number of nodes in \m{\mathcal{G}^{(\ell)}} is equal to the number of clusters in \m{\mathcal{G}^{(\ell + 1)}}.
\item The top level coarsening \m{\mathcal{G}^{(1)}} is a graph consisting of a single node,  
and the corresponding adjacency matrix \m{\mathcal{A}^{(1)} \in \mathbb{N}^{1 \times 1}}. 
\end{enumerate}
\end{definition}

\begin{definition} \label{def:mgn}
An \m{L}-level Multiresolution Graph Network (MGN)  
of a graph \m{\mathcal{G}} consists of \m{L - 1} 
tuples of five network components 
\m{\{(\bm{c}^{(\ell)}, \bm{e}_{\text{local}}^{(\ell)}, \bm{d}_{\text{local}}^{(\ell)}, \bm{d}_{\text{global}}^{(\ell)}, \bm{p}^{(\ell)})\}_{\ell = 2}^L}.
The \m{\ell}-th tuple encodes \m{\mathcal{G}^{(\ell)}} and transforms it into a lower resolution graph \m{\mathcal{G}^{(\ell - 1)}} in the higher level. 
Each of these network components has a separate set of learnable parameters \m{(\bm{\theta_1}^{(\ell)}, \bm{\theta_2}^{(\ell)}, \bm{\theta_3}^{(\ell)}, \bm{\theta}_4^{(\ell)}, \bm{\theta}_5^{(\ell)})}. 
For simplicity, we collectively denote the learnable parameters as \m{\bm{\theta}}, and drop the superscript. 
The network components are defined as follows:
\begin{enumerate}
%\plitemsep=10pt
\setlength{\itemsep}{2pt}
\item Clustering procedure \sm{\bm{c}(\mathcal{G}^{(\ell)}; \bm{\theta})}, which partitions graph 
\sm{\mathcal{G}^{(\ell)}} into \m{K} clusters \sm{\mathcal{V}_1^{(\ell)}, \ldots, \mathcal{V}_K^{(\ell)}}. 
Each cluster is an induced subgraph \sm{\mathcal{G}_k^{(\ell)}} of 
\sm{\mathcal{G}^{(\ell)}} with adjacency matrix \sm{\mathcal{A}_k^{(\ell)}}.
%
\item Local encoder \sm{\bm{e}_{\text{local}}(\mathcal{G}^{(\ell)}_k; \bm{\theta})}, which is a 
permutation equivariant (see Defs.~\ref{def:Sn-equivariant},~\ref{def:Sn-network}) graph neural network that 
takes as input the subgraph \sm{\mathcal{G}_k^{(\ell)}}, and outputs a set of node latents 
\sm{\mathcal{Z}^{(\ell)}_k} represented as a matrix of size \sm{|\mathcal{V}_k^{(\ell)}| \times d_z}.
%
\item Local decoder \sm{\bm{d}_{\text{local}}(\mathcal{Z}^{(\ell)}_k; \bm{\theta})}, which is a 
permutation equivariant neural network that tries to reconstruct the 
subgraph adjacency matrix \sm{\mathcal{A}_k^{(\ell)}} for each cluster from the local encoder's output latents.
%
\item (Optional) Global decoder \sm{\bm{d}_{\text{global}}(\mathcal{Z}^{(\ell)}; \bm{\theta})}, which is a 
permutation equivariant neural network that reconstructs the full adjacency matrix \sm{\mathcal{A}^{(\ell)}} 
from all the node latents of \m{K} clusters \sm{\mathcal{Z}^{(\ell)} = \bigoplus_k \mathcal{Z}^{(\ell)}_k} 
represented as a matrix of size \sm{|\mathcal{V}^{(\ell)}| \times d_z}.
%
\item Pooling network \sm{\bm{p}(\mathcal{Z}^{(\ell)}_k; \bm{\theta})}, which is a permutation invariant 
(see Defs.~\ref{def:Sn-equivariant},~\ref{def:Sn-network}) neural network that takes the set of node latents 
\sm{\mathcal{Z}^{(\ell)}_k} and outputs a single cluster latent \sm{\tilde{z}^{(\ell)}_k \in d_z}. 
The coarsening graph \sm{\mathcal{G}^{(\ell - 1)}} has adjacency matrix \sm{\mathcal{A}^{(\ell - 1)}} 
built as in Def.~\ref{def:coarsening}, and the corresponding node features 
\sm{\mathcal{Z}^{(\ell - 1)} = \bigoplus_k \tilde{z}^{(\ell)}_k} represented as a matrix of size \sm{K \times d_z}.
\end{enumerate}
\end{definition}

\begin{figure}[t!] \label{fig:hierarchy}
\begin{center}
\begin{tikzpicture}[scale=0.2,every node/.style={minimum size=1cm}] 
%%%% First layer
\begin{scope}[
yshift=140,every node/.append style={
	yslant=0.5,xslant=-1.5},yslant=0.5,xslant=-1.5, rotate=270
]
\definecolor{light-gray}{gray}{0.95}
\fill[light-gray,fill opacity=0.5] (-1.5,-1.5) rectangle (9.0,8.0);

\node[circle,draw=black, fill=blue, inner sep=0pt,minimum size=5pt] (0) at (0.94058461, 0.46191033) {};
\node[circle,draw=black, fill=blue, inner sep=0pt,minimum size=5pt] (1) at (0.62705641, 1.92877776) {};
\node[circle,draw=black, fill=blue, inner sep=0pt,minimum size=5pt] (2) at (1.74063677, 2.93373487) {};
\node[circle,draw=black, fill=red, inner sep=0pt,minimum size=5pt] (3) at (1.42710856, 5.2) {};
\node[circle,draw=black, fill=red, inner sep=0pt,minimum size=5pt] (4) at (0., 5.0) {};
\node[circle,draw=black, fill=red, inner sep=0pt,minimum size=5pt] (5) at (2.54068892, 6.0) {};
\node[circle,draw=black, fill=blue, inner sep=0pt,minimum size=5pt] (6) at (3.16774533, 2.47182454) {};
\node[circle,draw=black, fill=green, inner sep=0pt,minimum size=5pt] (7) at (4.28132569, 3.47678165) {};
\node[circle,draw=black, fill=green, inner sep=0pt,minimum size=5pt] (8) at (5.70843425, 3.01487132) {};
\node[circle,draw=black, fill=green, inner sep=0pt,minimum size=5pt] (9) at (6.02196246, 1.54800389) {};
\node[circle,draw=black, fill=green, inner sep=0pt,minimum size=5pt] (10) at (6.82201461, 4.01982843) {};
\node[circle,draw=black, fill=blue, inner sep=0pt,minimum size=5pt] (11) at (3.48127354, 1.00495711) {};
\node[circle,draw=black, fill=blue, inner sep=0pt,minimum size=5pt] (12) at (2.36769318, 0.) {};

\draw[blue] (0) -- (1) (1) -- (2) (2) -- (6) (6) -- (11) (11) -- (12) (12) -- (0);
\draw[black] (2) -- (3);
\draw[red] (3) -- (4) (3) -- (5);
\draw[black] (6) -- (7);
\draw[green] (7) -- (8) (8) -- (10) (8) -- (9);

\draw[blue, thick] (2.,1.6) ellipse (2.5cm and 2.5cm);
\draw[red, thick] (1.2,5.4) ellipse (2.0cm and 2.0cm);
\draw[green, thick] (6.,3.2) ellipse (2.5cm and 2.5cm);
\end{scope}

%%%% Second layer
\begin{scope}[
yshift=350


## Summarize the main method used in the paper in one paragraph.

 The main method used in this paper is a hierarchical graph generative model called Multiresolution Graph Variational Autoencoder (MGVAE). It works as follows:

MGVAE constructs a hierarchy of coarsened graphs at multiple resolutions from an input graph. At each resolution level, it partitions the graph into clusters using a learnable clustering procedure, encodes each cluster with a local encoder, and pools cluster features into a single node feature for the next coarser level using a pooling network. This process constructs a tree of latent distributions from fine to coarse resolutions. 

For graph generation, MGVAE samples from these latent distributions starting from the coarsest level, and uses resolution-specific decoders to reconstruct and generate the graph at each level in a coarse-to-fine manner. The encoders and decoders leverage higher-order message passing to be permutation equivariant. The learnable clustering procedure regularizes the model to find balanced graph partitions.

Overall, the hierarchical latent structure, equivariant message passing, and learnable clustering allow MGVAE to effectively learn and generate graphs in a multiresolution fashion, outperforming previous graph generative models. The key novelty is the learnable hierarchical latent structure that matches the intrinsic multiscale nature of graphs.
