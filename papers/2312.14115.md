# [LingoQA: Video Question Answering for Autonomous Driving](https://arxiv.org/abs/2312.14115)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Evaluating vision-language models for autonomous driving video question answering (QA) is challenging due to the lack of reliable automated metrics that correlate well with human judgments. Metrics like BLEU, METEOR, CIDEr rely on n-gram similarity and don't capture semantic meaning.

- There is also a lack of comprehensive benchmarks and datasets for autonomous driving video QA that go beyond simple object detection to test driving reasoning abilities.

Proposed Solution:
- The authors introduce the LingoQA benchmark for evaluating video QA models for autonomous driving. This includes:

1) An evaluation dataset of 1,000 high-quality QA pairs over 500 questions with 2 reference answers each to test various capabilities like reasoning, attention, anticipation etc.

2) A novel learned evaluation metric called Lingo-Judge - a transformer-based text classifier that predicts if a model's answer is correct or not given the question and human answers. It achieves 95% accuracy and shows 0.95 Spearman rank correlation and 0.993 Pearson correlation with human ratings, outperforming metrics like BLEU, METEOR, CIDEr and even GPT-4.

3) Two complementary autonomous driving QA datasets - one focused on driving actions and behaviors (268k pairs) and one on scenery description (153k pairs) totaling 419k pairs covering diverse questions and competencies.

Main Contributions:
- Reliable benchmark for evaluating video QA models for autonomous driving with automated metric highly correlated with human judgments
- Comprehensive training dataset with 419k diverse QA pairs related to driving behaviors, reasoning and scenery
- Extensive experiments comparing video-language architectures to establish an effective baseline model for the benchmark combining strengths of both datasets

The benchmark accelerates research in explainable autonomous driving by enabling rapid experimentation and evaluation. The dataset and high correlation of Lingo-Judge sets a foundation for improving transparency of autonomous systems.
