# [LLM as Prompter: Low-resource Inductive Reasoning on Arbitrary Knowledge   Graphs](https://arxiv.org/abs/2402.11804)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem: 
Knowledge graph (KG) inductive reasoning aims to infer new facts from KGs not seen during training. A key challenge is handling low-resource scenarios where both textual and structural information is scarce, which limits the applicability of existing methods. For example, graph neural networks rely on sufficient support triples while large language models struggle with limited text and complex graph structures.

Proposed Solution - ProLINK:
The paper proposes a novel pretraining and prompting framework called ProLINK that utilizes both large language models (LLMs) and graph neural networks (GNNs) to address the low-resource challenge. 

It first pretrains a GNN-based reasoner with techniques like role-aware relation encoding and a low-resource pretraining loss to enhance few-shot prediction. 

Then for a query relation with few support triples, it generates a prompt graph using a frozen LLM based on the relation's textual description. The LLM determines possible entity types on both sides of the relation, which are used to establish connections with other relations.  

A prompt calibrator further refines the graph to align it with the topological graph and eliminate noise. The resulting graph is injected into the topological graph to guide the GNN reasoner.

Main Contributions:

- Defines a new low-resource inductive reasoning problem on KGs with unseen entities/relations

- First work using LLMs to generate graph prompts for inductive KG reasoning

- Proposes the novel ProLINK framework with pretrained GNN reasoner, LLM prompter and prompt calibrator

- Constructs 36 low-resource datasets and shows ProLINK outperforms previous SOTA methods by 20-147% on 3-shot, 1-shot and 0-shot reasoning

- Demonstrates strong performance for various LLM prompt settings and full-shot scenarios, highlighting robustness

In summary, the key innovation is using language models to emulate human reasoning with relation semantics, thereby enhancing graph-based inductive reasoning without additional model training. This brings new insights and elevates model generalizability.
