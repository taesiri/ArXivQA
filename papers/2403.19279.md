# [Fine-Tuning Language Models with Reward Learning on Policy](https://arxiv.org/abs/2403.19279)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper "Fine-Tuning Language Models with Reward Learning on Policy":

Problem: 
Standard reinforcement learning from human feedback (RLHF) methods perform reward learning, policy optimization, and human preference collection serially. This causes the reward model to become inaccurate as the policy optimization shifts the language model's data distribution. Repeatedly gathering new preference data can mitigate this issue but significantly complicates the system. 

Proposed Solution:
The paper proposes reward learning on policy (RLP), an unsupervised framework that refines the reward model using policy samples to keep it on-distribution. RLP has two key components:

1) Unsupervised multi-view learning (UML): Constructs two views from a policy sample by generating two responses for an input. Optimizes a multi-view information bottleneck loss on the policy samples to learn robust representations.

2) Synthetic preference generation (SPG): Generates a set of outputs for an instruction and measures uncertainty via the size of the largest semantic equivalence cluster. Selectively simulates preferences for low uncertainty cases by sampling the preferred output from the largest cluster and non-preferred output from other clusters.

Main Contributions:
- Proposes RLP that iteratively retrains the reward model or policy to keep the reward accurate for the latest policy.
- Introduces UML with a multi-view information bottleneck loss to learn representations of the policy's distribution.
- Develops SPG that selectively generates high-quality preference data by quantifying uncertainty.
- Shows state-of-the-art performance of RLP over strong baselines on multiple benchmarks.
