# [INSIDE: LLMs' Internal States Retain the Power of Hallucination   Detection](https://arxiv.org/abs/2402.03744)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
Large language models (LLMs) like GPT-3 often generate factually incorrect or nonsensical text, referred to as "knowledge hallucination". Detecting these hallucinations is critical for safely deploying LLMs, but remains challenging. Prior detection methods operate at the token probability level or use the decoded text, losing semantic information. 

Proposed Solution: 
The paper proposes an INSIDE (INternal States for hallucInation DEtection) framework that leverages the dense semantic information retained in LLM internal states. Specifically:

1) An EigenScore metric that measures semantic consistency across multiple LLM responses for the same question. It uses the eigenvalues of the covariance matrix of internal state embeddings to quantify consistency. Lower EigenScores indicate more diverse, inconsistent responses that likely contain hallucinations.

2) A test-time feature clipping approach that truncates extreme activations in internal states to reduce overconfident generations, making it easier to catch self-consistent hallucinations.  

Main Contributions:
- First framework to leverage LLM internal states for hallucination detection
- EigenScore metric that captures semantic consistency better than language-level metrics
- Shows EigenScore represents differential entropy of embeddings 
- Feature clipping reduces overconfident generations and aids detection
- Achieves state-of-the-art detection performance on multiple QA datasets and LLMs
- Ablation studies validate the efficacy of different components

The key insight is to leverage the rich semantic information within LLM internal states, rather than decoded tokens, to more precisely evaluate response consistency and identify hallucinations. Test time clipping further handles overconfident self-consistent hallucinations.
