# [Llama Guard: LLM-based Input-Output Safeguard for Human-AI Conversations](https://arxiv.org/abs/2312.06674)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper: 

Problem:
- Existing content moderation tools for digital platforms have limitations when applied as safeguards for human-AI conversations: 
1) They don't distinguish between assessing safety risks from users vs AI agents. 
2) They cannot adapt to emerging content policies.  
3) They don't provide access for customization via fine-tuning.  
4) They use small transformer models as backbone instead of more capable large language models (LLMs).

Proposed Solution: 
- The authors introduce Llama Guard, an LLM-based input-output safeguard model for human-AI conversations.

Key Contributions:
- Defines a safety risk taxonomy with guidelines for categories like violence, sexual content, regulated substances etc.
- Collects a dataset labeled for prompt and response safety based on this taxonomy.
- Introduces capability to classify risks in both user prompts (inputs to AI) and AI responses. 
- Achieves strong performance on benchmarks like ToxicChat and OpenAI Moderation Evaluation dataset.
- Enables customization of tasks and output formats via instruction-based fine-tuning. 
- Provides model weights to enable further adaptation by users based on their policies.
- Shows Llama Guard can adapt to new taxonomies via zero-shot prompting, few-shot learning and fine-tuning.

In summary, the paper introduces an LLM-based safeguard model for human-AI conversations that is customizable, achieves state-of-the-art performance on benchmarks, and demonstrates adaptability to new taxonomies. The release of model weights also enables further research and development.
