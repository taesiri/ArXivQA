# [On-device Self-supervised Learning of Visual Perception Tasks aboard   Hardware-limited Nano-quadrotors](https://arxiv.org/abs/2403.04071)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Nano-drones (under 50g) are gaining traction for applications relying on onboard deep learning models for perception. However, when deployed in new unknown environments not seen during training, these models tend to underperform due to the problem of domain shift.  
- Addressing this by collecting new training data from the deployment domain is difficult due to limitations of nano-drones such as extremely constrained sensors, compute capabilities, memory and power.

Proposed Solution:
- The authors propose on-device learning on the nano-drone itself, where the first part of the mission is used to fine-tune the pretrained model using self-supervised learning before deploying it. 
- This allows adapting the model to the new environment to deal with domain shift by leveraging unlabeled data from the test environment.

Key Contributions:
- First demonstration of on-device self-supervised learning on an actual nano-drone (Bitcraze Crazyflie 2.1) for a real-world vision-based regression task (human pose estimation).
- Analysis of performance vs cost trade-offs along 3 axes: dataset size, training methodologies (fine-tuning all or subset of parameters), and self-supervision strategies.
- Up to 56% error reduction compared to non-finetuned baseline by leveraging onboard odometry for self-supervision.
- Demonstration of feasibility on ultra low-power GAP8 and GAP9 embedded processors, with fine-tuning possible in tens of seconds.  

Conclusion:
- The work pushes state-of-the-art for on-device learning on nano-drones and sets the stage for more advancements in addressing the domain shift problem on highly hardware-limited robots.


## Summarize the paper in one sentence.

 This paper presents on-device self-supervised learning to address the domain shift problem for vision-based tasks on nano-drones with severe hardware constraints, demonstrating up to 30% improvement in a human pose estimation task with just 22 seconds of fine-tuning on an ultra-low-power GAP9 system-on-chip.


## What is the main contribution of this paper?

 The main contribution of this paper is proposing and evaluating on-device self-supervised learning techniques to address the domain shift problem for visual perception tasks on nano-drones. Specifically:

- They present the first thorough analysis of on-device learning aboard resource-constrained nano-drones, exploring various trade-offs in terms of performance, memory usage, and computational requirements.

- They tackle a real-world robotic regression task (human pose estimation) rather than simpler classification, with no ground truth labels available during deployment. 

- They leverage self-supervision from onboard odometry to fine-tune a convolutional neural network for improved generalization, demonstrating up to 30% lower error compared to a non-fine-tuned baseline.

- Their approach is feasible to execute completely on-device a nano-drone equipped only with an ultra low-power GAP8 or GAP9 system-on-chip, requiring as little as 22 seconds of fine-tuning time.

In summary, the key contribution is showing that on-device self-supervised learning can mitigate the domain shift problem for nano-drone perception, marking an important advancement for tiny robots with extreme hardware constraints.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and keywords associated with it are:

- Nano-drones - The paper focuses on developing methods for tiny drones under 50 grams.

- On-device learning - The paper proposes performing model fine-tuning directly on the drone's limited hardware during the mission.

- Self-supervised learning - The fine-tuning methods use a self-supervision strategy to generate labels from the drone's onboard sensors.

- Domain shift - The paper aims to address the problem of domain shift, where models perform worse when deployed in new environments not seen during training. 

- Convolutional neural networks (CNNs) - The paper fine-tunes a CNN for human pose estimation to improve its performance when deployed.

- Hardware constraints - The nano-drones have extremely limited sensing, compute, memory and power capabilities, posing challenges for on-device learning.

- State consistency loss - A self-supervised loss used to improve model predictions by enforcing consistency with the drone's ego-motion.

- Performance vs cost trade-offs - The paper analyzes trade-offs between model performance improvements from fine-tuning and the memory, computation and time costs.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in this paper:

1. The paper proposes on-device self-supervised learning to address the domain shift problem for nano-drones. Why is domain shift such a critical issue, especially for nano-drones? What makes it difficult to collect new training data from the deployment domain?

2. The paper uses a state consistency loss for self-supervision. Explain this loss formulation. What are the key differences compared to the method in Nava et al. [29]? Why is it important to separate drone and subject movements?  

3. The paper evaluates trade-offs between dataset size, fine-tuning methods, and self-supervision strategies. What is the impact of longer flight times versus higher frame rates for acquiring the fine-tuning set? How do the different fine-tuning methods compare in terms of performance, computational requirements, and memory usage?

4. The paper tests five different training schemes for on-device fine-tuning. Explain each of these schemes. What are the advantages and disadvantages of each in terms of workload reduction and expected performance benefit? 

5. The paper considers three setups each for drone pose and human subject pose in analyzing the self-supervised loss formulations. Explain what these different setups are. What is the impact on performance of using perfect versus uncertain odometry? Of knowing versus not knowing the human subject's movement?

6. The paper proposes a cooperative scenario for acquiring fine-tuning data, with the human subject standing still at known positions. Why is this beneficial? How much of the performance of the ideal supervised loss can this recover? Could this approach be extended by having multiple cooperative subjects?

7. The paper benchmarks compute times for on-device fine-tuning using the PULP-TrainLib on two different embedded processors. Compare the GAP8 and GAP9 SoCs. What hardware capabilities lead to faster fine-tuning on the GAP9? 

8. The paper demonstrates a 30% MAE improvement with self-supervised on-device fine-tuning. Could further improvements be achieved by using a larger pre-trained model? What hardware advances would be needed to enable this?

9. The paper uses a dataset with precisely labeled real-world data for pre-training. Could similar performance improvements be achieved using self-supervised pre-training? What challenges would need to be addressed?

10. The paper focuses on a vision-based regression task for human pose estimation. Could this on-device learning approach be applied to other drone perception tasks? What modifications would be required?
