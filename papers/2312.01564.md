# [APoLLo: Unified Adapter and Prompt Learning for Vision Language Models](https://arxiv.org/abs/2312.01564)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a summary paragraph of the paper:

This paper presents APoLLo, a novel unified approach for adapter and prompt learning in vision-language models like CLIP to improve their generalization capability in few-shot settings. The key ideas include: 1) Introducing trainable adapter layers in both image and text encoders that are coupled via cross-attention to align the modalities. 2) Employing contrastive consistency losses between augmented image/text branches to prevent overfitting. 3) Using descriptive text generation by LLMs and text-conditional diffusion models to create effective multi-modal augmentations. 4) Experimental evaluation on diverse recognition tasks involving novel classes, cross-dataset analysis, and domain shifts demonstrate state-of-the-art performance with significant gains over prior arts, highlighting the efficacy of unifying adapter tuning and prompt learning in a multi-modal framework augmented using generative models. The ablation studies also confirm the utility of individual components in APoLLo towards achieving improved generalization.
