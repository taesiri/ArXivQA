# [Asymmetry in Low-Rank Adapters of Foundation Models](https://arxiv.org/abs/2402.16842)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Foundation models like BERT and Vision Transformers are very large and it can be computationally infeasible to fine-tune the entire model. Methods like Low-Rank Adaptation (LoRA) update only a small subset of parameters to adapt the model. 
- Standard LoRA factorizes the update matrix as BA and trains both A and B. Prior work has empirically observed that only training B seems sufficient while a random A works well, suggesting an asymmetry between A and B.

Proposed Solution:
- Theoretically and empirically analyze the roles of A and B to understand this asymmetry. 
- Show B is more important for matching desired outputs while A projects input features. Optimizing B to predict outputs is more critical than A.
- Prove fine-tuning only B instead of A and B together improves generalization bounds while allowing for larger rank updates.
- Suggest fixing A as a random orthogonal matrix and only tuning B can improve efficiency and generalization.

Key Contributions:  
- Formally characterize asymmetry between LoRA factors where B predicts outputs and A extracts input features. 
- Demonstrate tuning only B outperforms tuning A, and matches or improves over standard LoRA.
- Prove tuning only B tightens generalization bounds and allows larger rank than joint A,B tuning.
- Show across NLP and vision models that fixing random A and tuning B improves efficiency and generalization over standard LoRA.
- Provide guidance to improve parameter efficiency and generalization for low-rank adaptation of foundation models.
