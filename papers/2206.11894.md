# [MaskViT: Masked Visual Pre-Training for Video Prediction](https://arxiv.org/abs/2206.11894)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading, the key research questions/hypotheses that this paper tries to address are:

1. Can we create good video prediction models by pre-training transformers via masked visual modeling, without a lot of domain-specific engineering?

2. Can the proposed model Masked ViT (MaskViT) achieve competitive or state-of-the-art performance on standard video prediction benchmarks, compared to prior work? 

3. Can MaskViT generate high resolution future frame predictions efficiently, unlike prior transformer-based video prediction models?

4. Can the iterative decoding scheme proposed enable significantly faster inference compared to autoregressive decoding, making the model suitable for robotics applications?

5. Does masking a variable percentage of tokens during pre-training, instead of a fixed ratio, better mimic the video prediction task and lead to improved performance?

6. Can the proposed spatial and spatiotemporal windowed attention provide memory and computational benefits over full attention transformers?

7. Can the model predictions be used to successfully perform visual model predictive control on a real robot?

In summary, the key hypotheses are around showing that masked visual pre-training can produce high-performing and efficient video prediction models suitable for robotics, using only minimal domain knowledge. The design choices like variable masking ratio, windowed attention, and iterative decoding are meant to test these hypotheses.


## What is the main contribution of this paper?

 The main contribution of this paper is developing Masked Video Transformer (MaskViT), a video prediction model based on masked visual modeling and iterative decoding. 

Specifically, the key ideas and contributions are:

- Proposing to pre-train a Transformer for video prediction via masked visual modeling (MVM), where a variable percentage of future frame tokens are masked and predicted based on past frames. This is inspired by masked language modeling in BERT.

- Using a combination of spatial and spatiotemporal window-restricted attention in the Transformer layers for memory and computation efficiency when operating on sequences of video frames.

- An iterative decoding procedure during inference where tokens are predicted sequentially over multiple iterations while decreasing the mask ratio. This non-autoregressive technique bridges the gap between pre-training and test-time prediction and speeds up inference significantly.

- Demonstrating state-of-the-art or competitive results on multiple video prediction benchmarks like BAIR, KITTI, and Robonet, while being more parameter efficient. The model can also generate higher resolution videos than prior work.

- Showing real-time robot manipulation planning using MaskViT, enabled by the fast iterative decoding. This suggests the general framework could enable using powerful predictive models for robotics and other embodied AI applications.

In summary, the key contribution is developing a simple yet effective masked Transformer model for video prediction via innovations in pre-training, architecture design, and inference procedure. The strong results and real robot experiments highlight the potential of this approach.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the key points from the paper:

The paper proposes MaskViT, a video prediction model based on masked visual pre-training of transformers with windowed attention, which achieves state-of-the-art results while being computationally efficient enough for robot planning through iterative decoding.
