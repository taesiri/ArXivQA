# [Full-Gradient Representation for Neural Network Visualization](https://arxiv.org/abs/1905.00780)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the key research question seems to be:

How can we develop a visual interpretation method for neural networks that satisfies both local attribution (weak dependence on inputs) and global attribution (completeness)?

The paper argues that existing saliency map methods for interpreting neural networks fail to satisfy both weak dependence and completeness. It shows formally that it is impossible for a saliency map to satisfy both properties in general. 

To address this limitation, the paper introduces a new representation called full-gradients, which provides attributions to both the inputs and neurons in a neural network. It shows that full-gradients satisfy both weak dependence and completeness. 

The paper then proposes an approximate saliency map called FullGrad for convolutional networks, which aggregates the full-gradient components. It evaluates FullGrad on image datasets using quantitative tests like pixel perturbation and remove-and-retrain. The results show that FullGrad explains model behavior more correctly and comprehensively compared to other methods.

In summary, the key hypothesis is that full-gradients can satisfy both local and global attribution notions simultaneously for neural net interpretability. The FullGrad saliency maps demonstrate this hypothesis on vision tasks.


## What is the main contribution of this paper?

 This paper introduces a new method called "full-gradients" for interpreting neural network responses. The key contributions are:

1. It shows that existing saliency map methods cannot simultaneously satisfy two desirable properties - "completeness" (fully explaining the network output) and "weak dependence" (capturing local feature importance). This suggests inherent limitations of saliency maps.

2. It proposes full-gradients which provide attributions to both inputs and neurons. This representation can satisfy both completeness and weak dependence. 

3. For convolutional nets, it develops an approximate saliency map called "FullGrad" by aggregating the full-gradient components.

4. It evaluates FullGrad on image datasets using quantitative tests like pixel perturbation and remove-and-retrain. The results show FullGrad outperforms existing methods and produces sharper saliency maps confined to object regions.

In summary, the main contribution is the full-gradient representation and the resulting FullGrad saliency maps. This provides a more complete and reliable explanation of network behavior compared to prior saliency map methods. The analysis and experiments support the usefulness of accounting for both input sensitivity and internal neuron sensitivity in interpretability.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence TL;DR summary of the paper:

The paper introduces full-gradients, a novel representation that provides attributions to both the inputs and neurons of a neural network, satisfying notions of both local and global feature importance while overcoming limitations of existing saliency map interpretation methods.


## How does this paper compare to other research in the same field?

 Here are some ways this paper compares to other research in the field of neural network visualization and interpretability:

- It introduces the concept of "full-gradients", which provides attribution to both the inputs and neurons in a neural network. This is a novel approach compared to most prior work like saliency maps or gradient-based methods that provide attribution only to the inputs. 

- The paper proves an impossibility result showing that no saliency map can simultaneously satisfy the desirable properties of local attribution (weak dependence) and global attribution (completeness). This provides theoretical justification for why prior methods have exhibited counterintuitive behaviors.

- It proposes an approximate saliency map called FullGrad for convolutional networks, which aggregates the input-gradient and per-neuron gradient contributions. Most prior visualization methods are either input-gradient based or activation-based, but FullGrad combines both.

- The paper evaluates FullGrad using quantitative tests like pixel perturbation and remove-and-retrain. Most prior work evaluated visualizations only qualitatively through visual inspection. The quantitative results demonstrate the effectiveness of FullGrad.

- The visualizations from FullGrad are shown to be sharper and more tightly confined to object boundaries compared to other methods like integrated gradients or GradCAM.

- The paper provides practical recommendations on how to choose appropriate post-processing functions for FullGrad based on the application domain and evaluation metrics. Most prior work does not discuss the effect of post-processing choices.

Overall, this paper makes both theoretical contributions in terms of impossibility results, and practical contributions in terms of a new visualization method and quantitative evaluation. The results demonstrate the limitations of prior methods and the advantages of the proposed full-gradient approach.


## What future research directions do the authors suggest?

 Based on my reading of the paper, some of the key future research directions suggested by the authors include:

- Developing more rigorous methods for evaluating and comparing different saliency map techniques. The authors note the lack of completely rigorous evaluation methods as an issue. They suggest that unambiguous quantitative metrics need to be defined and models should be trained to optimize those metrics directly.

- Designing interpretability methods with clearly specified trade-offs. The authors argue that all interpretability methods likely have inherent trade-offs between satisfying different desirable properties. Making these trade-offs explicit could benefit domain experts using these methods.

- Exploring if multiple interpretability properties are fundamentally irreconcilable mathematically. The authors suggest it may be impossible for any method to satisfy certain properties simultaneously, implying interpretability may only be achievable in a limited sense. Further exploration of these theoretical limits could be valuable.

- Developing more expressive explanation schemes beyond saliency maps. The authors propose full-gradients as more expressive than saliency maps. Finding other representations that capture different explanatory factors could be useful.

- Building interpretability directly into models. Rather than post-hoc explanation methods, the authors suggest training models to be interpretable by design based on unambiguous metrics.

- Tailoring methods and metrics to datasets/tasks. The authors find the ideal interpretability method depends on the dataset and metrics. Developing customized solutions for different applications with input from domain experts is suggested.

In summary, the key directions are: better evaluation methods, making trade-offs explicit, exploring theoretical limits, more expressive explanations, interpretability by design, and customization for specific tasks/data. The overarching theme is developing more rigorous, customizable, and holistic approaches to interpretability.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the paper: 

The paper introduces full-gradients, a new representation for interpreting neural network responses. Full-gradients decompose the neural network output into input sensitivity and per-neuron sensitivity components. This achieves two key properties: completeness, fully accounting for the network output, and weak dependence, reflecting local input sensitivity. The paper shows these properties cannot be achieved simultaneously by any saliency map method. For convolutional networks, the paper aggregates full-gradient components into an approximate saliency map called FullGrad. Experiments with pixel perturbation and remove-and-retrain evaluations demonstrate FullGrad explains model behavior more accurately than existing methods. Overall, the full-gradient representation provides a more complete and faithful interpretation of neural network functions compared to standard saliency maps.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the paper:

The paper introduces a new visualization tool called full-gradients for interpreting neural network responses. Full-gradients decompose the neural network output into input sensitivity and per-neuron sensitivity components. This representation assigns importance scores to both the inputs and individual neurons in the network. 

The key advantage of full-gradients is that it satisfies two important properties - completeness and weak dependence - that previous saliency map methods cannot satisfy together. Completeness requires attributions to account for the full network output, while weak dependence requires attributions to be locally faithful to the model. The authors show theoretically that these two properties cannot be achieved by any saliency map. For convolutional networks, they derive an approximate saliency map called FullGrad from the full-gradients. Experiments show FullGrad outperforms existing methods on quantitative tests like pixel perturbation and remove-and-retrain. The authors argue full-gradients are more expressive than saliency maps and avoid issues like model insensitivity that arise with previous methods.


## Summarize the main method used in the paper in one paragraph.

 Here is a one paragraph summary of the main method used in the paper:

The paper introduces full-gradients, which provide attributions to both the inputs and neurons of a neural network. The full-gradient decomposition represents the neural network output as the sum of the input-gradient component and the bias-gradient component. The input-gradient captures sensitivity to individual input features, while the bias-gradient captures the effect of biases and interactions between input features. For convolutional networks, the bias-gradients have a spatial structure which allows approximating them as saliency maps that highlight salient image regions. These approximate saliency maps, called FullGrad, are obtained by aggregating the input-gradient and bias-gradients across layers. FullGrad combines the advantages of gradient-based methods in precisely highlighting edges with those of methods like Grad-CAM that highlight salient regions. Experiments show FullGrad performs better than existing methods on quantitative tests like pixel perturbation and remove-and-retrain.
