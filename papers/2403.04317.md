# [Online Adaptation of Language Models with a Memory of Amortized Contexts](https://arxiv.org/abs/2403.04317)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
Large language models (LLMs) quickly become outdated due to the rapid generation and dissemination of new information, despite requiring enormous resources to develop. However, efficiently adapting LLMs to new data is challenging due to the ever-expanding corpus of documents and the large parameter space of modern LLMs. Prior works on online learning for LLMs have limitations such as catastrophic forgetting of past knowledge, expensive computation during adaptation, and sensitivity to optimization hyperparameters.

Proposed Solution:
The paper proposes Memory of Amortized Contexts (MAC), an efficient and effective online adaptation framework for static LLMs. The key ideas are:

1) Amortize new documents into compact modulations using a meta-learned encoder that maximizes post-adaptation QA performance. This allows efficient adaptation via a feedforward pass, without optimization.

2) Store amortized document representations in a memory bank for knowledge retention. 

3) Learn to aggregate relevant modulations from memory based on the question, to adapt the frozen LM.

To allow scaling, the method uses backprop dropout during training and hierarchical aggregation during inference.

Main Contributions:
- Novel online adaptation framework for LLMs combining amortization-based meta-learning and memory augmentation
- Demonstrates superior adaptation performance over online finetuning baselines
- More efficient in terms of adaptation time and memory than finetuning
- Significantly less catastrophic forgetting due to memory bank
- Enables training with larger batch sizes via backprop dropout 
- Handles large memory banks during inference via hierarchical aggregation

The proposed MAC framework tackles key challenges of efficient and continual adaptation for static LLMs in an effective manner, with strong empirical results over competitive baselines. The method is highly promising for keeping large models continually updated.
