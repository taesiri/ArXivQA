# [EgoLoc: Revisiting 3D Object Localization from Egocentric Videos with   Visual Queries](https://arxiv.org/abs/2212.06969)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading, the main research goal of this paper is to develop a new approach for the Visual Queries with 3D Localization (VQ3D) task on egocentric videos. Specifically, the paper aims to address the limitations of prior methods on VQ3D which struggled due to inaccurate camera pose estimation and lack of proper 3D understanding. The key hypothesis is that fusing 2D object detection with 3D multiview geometry and aggregating predictions from multiple views can significantly improve performance on the VQ3D task. Their proposed pipeline "EgoLoc" incorporates:- More robust egocentric camera pose estimation using COLMAP SfM instead of relocalization - Selecting peak 2D detection responses instead of tracking for object retrieval- Estimating depth and backprojecting 2D detections from multiple views into 3D  - Aggregating the 3D displacements using detection confidences as weightsThrough extensive experiments, they validate that each component of their pipeline contributes to the performance gain over the previous VQ3D baseline. The core hypothesis is that better entanglement of 2D object retrieval and 3D geometry understanding is crucial for the VQ3D task, and their EgoLoc method effectively realizes this, leading to new state-of-the-art results.In summary, the paper focuses on advancing VQ3D via a new pipeline that tightly integrates 2D detection and 3D multiview geometry to achieve significantly improved localization.
