# [Balanced Marginal and Joint Distributional Learning via Mixture   Cramer-Wold Distance](https://arxiv.org/abs/2312.03307)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem: 
- Generative models like VAEs and GANs have limitations in accurately capturing the full distribution of complex, high-dimensional datasets. 
- Methods based on slicing distributions and using distances like Cramer-Wold address this, but focus too much on joint distributions rather than critical marginal distributions.
- Capturing both marginal and joint distributions is important for generating high-quality synthetic data.

Proposed Solution:
- Introduce a novel "mixture Cramer-Wold distance" to integrate marginal distribution learning into the Cramer-Wold framework. This is done by using a mixture measure that combines point masses on basis vectors (for margins) with a normalized surface measure (for joints).

- Propose a new generative model called "Cramer-Wold Distributional AutoEncoder (CWDAE)" which uses the mixture Cramer-Wold distance as its reconstruction loss. This allows jointly capturing marginal and joint distributions.

- CWDAE uses quantile functions in its decoder to enable diverse sample generation and provide control over the privacy-utility tradeoff via a hyperparameter.


Main Contributions:

- A new mixture Cramer-Wold distance metric that balances marginal and joint distribution learning. Closed-form solution derived.

- CWDAE generative model that leverages the mixture distance for accurate distribution learning on complex data. Outperforms VAE, GAN, and prior Cramer-Wold approaches. 

- Analysis over multiple real-world tabular datasets demonstrates CWDAE's effectiveness for synthetic data generation. Allows controlling privacy-utility tradeoff.

- Establishes significance of marginal distribution learning for generative performance. Mixture Cramer-Wold formulation provides a new way to integrate this into distribution distance metrics.
