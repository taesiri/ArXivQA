# [Unsupervised Pre-Training of Image Features on Non-Curated Data](https://arxiv.org/abs/1905.01278)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is:Can we bridge the performance gap between unsupervised methods trained on curated datasets versus massive raw datasets by combining self-supervision and clustering? The key hypotheses appear to be:1) Combining self-supervision and clustering can leverage the complementary strengths of each approach - self-supervision provides intra-image statistics and stability while clustering provides inter-image statistics and complexity.2) This combined approach can scale to large non-curated datasets and improve feature quality compared to training just on curated datasets. 3) Pre-training on large amounts of non-curated data with this approach can improve performance on downstream supervised tasks like ImageNet classification compared to training from scratch.Specifically, the authors propose DeeperCluster, which combines self-supervision on image rotations with hierarchical clustering, to leverage 96 million unlabeled images from YFCC100M. They evaluate feature quality on transfer tasks and find DeeperCluster outperforms other unsupervised approaches trained on curated datasets. They also show pre-training VGG-16 with DeeperCluster improves ImageNet accuracy compared to training from scratch.In summary, the key hypothesis is that combining self-supervision and clustering can unlock the potential of unlabeled non-curated data at scale to learn improved visual features for downstream tasks. The experiments aim to validate this hypothesis.
