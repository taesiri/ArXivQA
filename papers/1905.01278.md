# [Unsupervised Pre-Training of Image Features on Non-Curated Data](https://arxiv.org/abs/1905.01278)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is:Can we bridge the performance gap between unsupervised methods trained on curated datasets versus massive raw datasets by combining self-supervision and clustering? The key hypotheses appear to be:1) Combining self-supervision and clustering can leverage the complementary strengths of each approach - self-supervision provides intra-image statistics and stability while clustering provides inter-image statistics and complexity.2) This combined approach can scale to large non-curated datasets and improve feature quality compared to training just on curated datasets. 3) Pre-training on large amounts of non-curated data with this approach can improve performance on downstream supervised tasks like ImageNet classification compared to training from scratch.Specifically, the authors propose DeeperCluster, which combines self-supervision on image rotations with hierarchical clustering, to leverage 96 million unlabeled images from YFCC100M. They evaluate feature quality on transfer tasks and find DeeperCluster outperforms other unsupervised approaches trained on curated datasets. They also show pre-training VGG-16 with DeeperCluster improves ImageNet accuracy compared to training from scratch.In summary, the key hypothesis is that combining self-supervision and clustering can unlock the potential of unlabeled non-curated data at scale to learn improved visual features for downstream tasks. The experiments aim to validate this hypothesis.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contributions appear to be:- Proposing a new unsupervised learning approach (DeeperCluster) that combines self-supervision and clustering to learn visual features from large-scale non-curated image datasets. - Showing that the proposed method can learn high quality features from 96 million images from the YFCC100M dataset, achieving state-of-the-art results among unsupervised methods on standard evaluation benchmarks like Pascal VOC.- Demonstrating that pre-training a VGG-16 model with DeeperCluster leads to improved performance (+0.8% top-1 accuracy) on ImageNet classification compared to training from scratch. - Introducing a hierarchical formulation of the learning objective that enables distributed training and scaling up to large datasets and number of clusters.In summary, the main contribution appears to be presenting a novel unsupervised feature learning approach that can leverage large non-curated datasets to learn visual representations that transfer well to downstream tasks, surpassing prior unsupervised methods trained on curated datasets. The hierarchical formulation also allows the method to scale to tens of millions of images and hundreds of thousands of clusters.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence TL;DR summary of the paper:The paper proposes a new unsupervised learning approach that combines self-supervision and clustering to learn high-quality image features from large volumes of non-curated raw image data.


## How does this paper compare to other research in the same field?

Here are a few key points about how this paper compares to other research on unsupervised pre-training of image features:- It focuses on pre-training on large non-curated datasets (YFCC100M), rather than more commonly used curated datasets like ImageNet. Most prior work has focused on curated datasets, and found performance decreased on raw data. This paper shows unsupervised pre-training on raw data can work well.- The method combines self-supervision (RotNet) and clustering (DeepCluster) in a novel way to leverage complementary strengths. This hybrid approach is unique. Prior works tend to focus on one or the other. - It scales to much larger datasets than prior work, training on up to 96 million images from YFCC100M. Most similar papers have trained on datasets 1-2 orders of magnitude smaller.- The features learned achieve state-of-the-art results for unsupervised methods on several standard benchmarks, outperforming prior unsupervised approaches even those trained on curated data.- When used for pre-training on ImageNet, the method improves over training from scratch by +0.8% top-1 accuracy. This demonstrates the value of pre-training on large raw datasets.- The paper provides an analysis of the impact of number of images and clusters, plus qualitative evaluations of the obtained clusters. This level of analysis is more extensive than most similar papers.In summary, this paper pushes the boundaries of unsupervised pre-training by scaling to larger raw datasets and combining complementary methods. The strong benchmark results and ImageNet pre-training improvements highlight the potential of this approach. The analysis also provides useful insights about unsupervised feature learning.


## What future research directions do the authors suggest?

Based on my reading of the paper, some of the main future research directions suggested by the authors include:- Validating their unsupervised pre-training approach on more recent convolutional neural network architectures besides VGG-16, such as ResNet. The authors suggest this could further demonstrate the potential of unsupervised pre-training on large non-curated datasets.- Applying their method to even larger datasets beyond the 96 million images from YFCC100M used in the paper. The authors' results indicate performance continues to improve with more data, so they suggest scaling up further.- Exploring different combinations of self-supervision and clustering objectives beyond the specific combination of rotation prediction and k-means clustering used in this work. The modular framework they propose could support experimenting with other tasks.- Extending the hierarchical formulation to more than two levels, which could potentially allow scaling to even larger numbers of clusters. The authors propose a 2-level hierarchy but do not experiment with deeper hierarchies.- Applying the unsupervised pre-training approach to other computer vision tasks beyond classification and detection, to demonstrate the generality of the learned features. The paper mainly focuses on evaluating the feature quality on classification and detection benchmarks.- Comparing to other recent unsupervised learning methods, especially approaches that leverage large amounts of video data. The authors mainly compare against prior work using VGG-16 on ImageNet or other curated datasets.In summary, the main directions are scaling up the approach to larger datasets and neural network architectures, generalizing the framework to other self-supervision objectives and clustering algorithms, evaluating on a wider range of vision tasks, and comparing to the latest state-of-the-art in unsupervised learning. The authors' results suggest their method could yield further gains along these directions.


## Summarize the paper in one paragraph.

The paper proposes a new unsupervised approach for pre-training visual features on non-curated datasets. The key ideas are:- Combining self-supervision (using image rotation prediction) and clustering (k-means on features) to leverage complementary statistics from large datasets. Self-supervision provides intra-image statistics and stability while clustering provides inter-image statistics and complexity. - A hierarchical formulation to scale to large numbers of clusters. Images are clustered into a small number of super-classes which are each clustered into a large number of sub-classes. This enables distributed training.- Evaluation on 96 million YFCC100M images shows state-of-the-art unsupervised transfer learning performance, surpassing supervised pre-training on ImageNet. Pre-training on YFCC100M also improves ImageNet classification accuracy over training from scratch.- Analysis indicates the approach captures meaningful visual structures, with clustering correlating with metadata like hashtags and geolocation despite no metadata being used. Performance also improves with more data and larger cluster numbers.In summary, the paper presents an unsupervised learning approach that leverages large non-curated datasets by combining self-supervision and clustering. Key results are state-of-the-art transfer learning performance and improved ImageNet classification when pre-trained on 96 million Flickr images.
