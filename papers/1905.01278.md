# [Unsupervised Pre-Training of Image Features on Non-Curated Data](https://arxiv.org/abs/1905.01278)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is:Can we bridge the performance gap between unsupervised methods trained on curated datasets versus massive raw datasets by combining self-supervision and clustering? The key hypotheses appear to be:1) Combining self-supervision and clustering can leverage the complementary strengths of each approach - self-supervision provides intra-image statistics and stability while clustering provides inter-image statistics and complexity.2) This combined approach can scale to large non-curated datasets and improve feature quality compared to training just on curated datasets. 3) Pre-training on large amounts of non-curated data with this approach can improve performance on downstream supervised tasks like ImageNet classification compared to training from scratch.Specifically, the authors propose DeeperCluster, which combines self-supervision on image rotations with hierarchical clustering, to leverage 96 million unlabeled images from YFCC100M. They evaluate feature quality on transfer tasks and find DeeperCluster outperforms other unsupervised approaches trained on curated datasets. They also show pre-training VGG-16 with DeeperCluster improves ImageNet accuracy compared to training from scratch.In summary, the key hypothesis is that combining self-supervision and clustering can unlock the potential of unlabeled non-curated data at scale to learn improved visual features for downstream tasks. The experiments aim to validate this hypothesis.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contributions appear to be:- Proposing a new unsupervised learning approach (DeeperCluster) that combines self-supervision and clustering to learn visual features from large-scale non-curated image datasets. - Showing that the proposed method can learn high quality features from 96 million images from the YFCC100M dataset, achieving state-of-the-art results among unsupervised methods on standard evaluation benchmarks like Pascal VOC.- Demonstrating that pre-training a VGG-16 model with DeeperCluster leads to improved performance (+0.8% top-1 accuracy) on ImageNet classification compared to training from scratch. - Introducing a hierarchical formulation of the learning objective that enables distributed training and scaling up to large datasets and number of clusters.In summary, the main contribution appears to be presenting a novel unsupervised feature learning approach that can leverage large non-curated datasets to learn visual representations that transfer well to downstream tasks, surpassing prior unsupervised methods trained on curated datasets. The hierarchical formulation also allows the method to scale to tens of millions of images and hundreds of thousands of clusters.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence TL;DR summary of the paper:The paper proposes a new unsupervised learning approach that combines self-supervision and clustering to learn high-quality image features from large volumes of non-curated raw image data.
