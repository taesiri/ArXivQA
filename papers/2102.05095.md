# [Is Space-Time Attention All You Need for Video Understanding?](https://arxiv.org/abs/2102.05095)

## What is the central research question or hypothesis that this paper addresses?

The central research question addressed in this paper is whether it is possible to build an effective video recognition model using self-attention as the sole building block, completely replacing convolutions. The authors hypothesize that a pure self-attention architecture can overcome some inherent limitations of convolutional neural networks for video modeling, such as limited receptive field size and inductive biases that may be too restrictive when large training data is available. The paper introduces a video Transformer model called TimeSformer to investigate this question.The key hypotheses tested are:- A pure self-attention architecture without any convolutions can achieve competitive or superior accuracy compared to state-of-the-art convolutional video models on major action recognition benchmarks.- Self-attention can capture long-range dependencies both within and across frames, enabling strong temporal modeling over long video durations. - Divided attention, where temporal attention and spatial attention are separately applied, is more accurate and efficient than joint spatiotemporal attention.- Self-attention is more efficient and scalable than 3D convolutions, enabling training of larger models and inputs.Through extensive experiments, the paper provides empirical evidence supporting these hypotheses, demonstrating the potential of pure self-attention architectures as an alternative paradigm for video understanding compared to standard convolutional networks.
