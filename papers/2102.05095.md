# [Is Space-Time Attention All You Need for Video Understanding?](https://arxiv.org/abs/2102.05095)

## What is the central research question or hypothesis that this paper addresses?

The central research question addressed in this paper is whether it is possible to build an effective video recognition model using self-attention as the sole building block, completely replacing convolutions. The authors hypothesize that a pure self-attention architecture can overcome some inherent limitations of convolutional neural networks for video modeling, such as limited receptive field size and inductive biases that may be too restrictive when large training data is available. The paper introduces a video Transformer model called TimeSformer to investigate this question.The key hypotheses tested are:- A pure self-attention architecture without any convolutions can achieve competitive or superior accuracy compared to state-of-the-art convolutional video models on major action recognition benchmarks.- Self-attention can capture long-range dependencies both within and across frames, enabling strong temporal modeling over long video durations. - Divided attention, where temporal attention and spatial attention are separately applied, is more accurate and efficient than joint spatiotemporal attention.- Self-attention is more efficient and scalable than 3D convolutions, enabling training of larger models and inputs.Through extensive experiments, the paper provides empirical evidence supporting these hypotheses, demonstrating the potential of pure self-attention architectures as an alternative paradigm for video understanding compared to standard convolutional networks.


## What is the main contribution of this paper?

The main contributions of this paper are:- It proposes TimeSformer, a convolution-free video classification model built exclusively on self-attention over space and time.  - It introduces and compares several efficient schemes for spatiotemporal self-attention, such as divided attention and sparse local-global attention.- It shows that the proposed TimeSformer model achieves state-of-the-art accuracy on major action recognition benchmarks like Kinetics-400, Kinetics-600, Something-Something-V2, and Diving-48.- It demonstrates that TimeSformer provides an improved accuracy/efficiency trade-off compared to 3D convolutional networks like SlowFast, I3D, etc. - It shows that TimeSformer can be effectively applied for long-range video modeling on longer videos spanning minutes.- It provides an empirical study analyzing the impact of pretraining dataset, training set size, number of frames, spatial resolution, etc. on the video classification accuracy.In summary, the main contribution is a new convolution-free Transformer-based architecture for video classification that achieves excellent accuracy and efficiency on several video recognition benchmarks.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper proposes TimeSformer, a convolution-free video classification model built entirely on self-attention over space and time, which achieves state-of-the-art results on major action recognition benchmarks while being efficient to train and apply to long videos.


## How does this paper compare to other research in the same field?

Here are a few key ways this paper compares to other research on video action recognition:- It proposes a completely new self-attention based architecture for video understanding called TimeSformer, rather than using standard 3D CNN models like most prior work. The use of self-attention to model spatiotemporal relationships is novel for video recognition.- The paper systematically explores and compares different variants of efficient spatiotemporal self-attention, providing new insights into effective ways to adapt self-attention to video data. For example, it finds that factorizing spatial and temporal attention is more accurate and efficient than joint modeling.- Despite the very different architecture, TimeSformer achieves state-of-the-art or competitive accuracy on major action recognition benchmarks like Kinetics, outperforming typical 3D CNN models. This demonstrates the potential of self-attention for video.- The self-attention approach provides advantages in computational cost and memory footprint over 3D CNNs, enabling the model to process longer and higher resolution videos. This could expand the applicability of video recognition.- The paper shows the model can effectively leverage image-based pretraining and that performance scales well with more training data, unlike some prior self-attention image models.Overall, this paper makes significant advances over prior work by being the first to build a fully self-attention based architecture for video and systematically design spatiotemporal self-attention schemes. The strong empirical results highlight the promise of this new direction compared to prevalent 3D CNN models for video understanding tasks.


## What future research directions do the authors suggest?

Based on my reading of the paper, here are some of the main future research directions suggested by the authors:- Extending TimeSformer to other video analysis tasks beyond action recognition, such as spatiotemporal action localization, video captioning, and video question answering. The self-attention mechanism in TimeSformer could be useful for capturing dependencies needed for these tasks.- Exploring new optimization strategies and hyperparameter tuning to train TimeSformer effectively from scratch on video datasets without relying on ImageNet pretraining. This could make the approach more flexible and reduce computational requirements. - Combining TimeSformer with complementary long-term modeling approaches designed to operate on top of video backbone features (e.g. long-term feature banks). This could potentially yield further gains in tasks requiring very long-term video understanding.- Applying TimeSformer to longer videos spanning many minutes and even hours. The efficient divided space-time attention scheme can likely scale to much longer inputs than what was tested in the paper.- Adapting TimeSformer to work well with limited training data. The paper results suggest it may need more data to learn certain temporal patterns compared to 3D CNNs. Self-supervision or semi-supervision could help here.- Extending TimeSformer to operate on high-resolution videos beyond the resolutions tested so far, to take advantage of the scalability of the divided attention.- Applying TimeSformer to other data modalities beyond RGB, such as optical flow or audio. The modality-agnostic self-attention could naturally incorporate these.- Devising attention schemes that are even more efficient than the divided attention, to scale TimeSformer to extremely long and high-resolution videos.In summary, the key directions are adapting TimeSformer to new tasks and data regimes to fully leverage the power and flexibility of the self-attention for video understanding.
