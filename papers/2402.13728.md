# [Average gradient outer product as a mechanism for deep neural collapse](https://arxiv.org/abs/2402.13728)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
The phenomenon of Deep Neural Collapse (DNC) - the surprisingly rigid structure of data representations in the final layers of deep neural networks (DNNs) - has been observed, but its emergence is only partially understood. Prior work has mostly relied on feature-agnostic models like the unconstrained features model (UFM) to explain DNC. However, UFM discards the training data and most of the network, thus ignoring the role of feature learning in DNC formation.

Proposed Solution: 
This paper provides evidence that DNC formation occurs primarily through deep feature learning with the average gradient outer product (AGOP). The paper first shows that the right singular vectors and values of DNN weights are responsible for most within-class variability collapse. This singular structure is highly correlated with the AGOP due to the Neural Feature Ansatz (NFA). The paper then shows experimentally and theoretically that AGOP induces neural collapse in randomly initialized networks. Specifically, Deep Recursive Feature Machines (Deep RFMs), which introduce AGOP feature learning into random networks, exhibit DNC.

Main Contributions:
- Provides evidence that the right singular structure of DNN weights, aligned with AGOP due to NFA, induces the majority of within-class variability collapse in DNNs
- Establishes experimentally and theoretically that AGOP induces neural collapse in randomly initialized networks
- Demonstrates that Deep RFMs, which model AGOP feature learning, exhibit DNC
- Links DNC formation between DNNs and Deep RFMs via the AGOP projection mechanism
- Provides a theoretical understanding of DNC formation in Deep RFMs based on kernel ridge regression optimization 

The paper takes a significant step beyond feature-agnostic DNC explanations by highlighting the importance of AGOP feature learning. It bridges an important gap between DNC and the role of data/features.
