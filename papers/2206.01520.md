# [A Survey on Computationally Efficient Neural Architecture Search](https://arxiv.org/abs/2206.01520)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question/hypothesis seems to be: How to improve the computational efficiency of neural architecture search (NAS) techniques?The paper provides a comprehensive survey and analysis of computationally efficient methods for neural architecture search. The main motivation is that NAS suffers from high computational complexity due to the need to train and evaluate a large number of candidate neural network architectures. Thus the central focus is on reviewing and categorizing techniques that aim to reduce the computational overhead of NAS.The key research questions/goals addressed in the paper include:- Provide a systematic categorization and review of proxy-based NAS methods such as low-fidelity estimation, one-shot NAS, and network morphism. - Give an in-depth analysis of surrogate-assisted NAS approaches, including Bayesian optimization methods, surrogate-assisted evolutionary algorithms, federated NAS, and multi-objective NAS.- Compare the performance and complexity of different computationally efficient NAS techniques.- Discuss remaining challenges and suggest promising future research directions for improving NAS efficiency further.In summary, the central hypothesis is that the computational efficiency of NAS can be significantly improved via clever proxy metrics and surrogate-assisted optimization techniques, enabling more widespread practical application of automated neural architecture search. The paper aims to provide a comprehensive survey and analysis of work in this emerging area.


## What is the main contribution of this paper?

The main contribution of this paper is introducing computationally efficient methods to neural architecture search (NAS). Specifically:- It provides a comprehensive overview and categorization of existing computationally efficient NAS methods into proxy-based methods and surrogate-assisted NAS. - It gives a detailed analysis of different types of proxy methods including low-fidelity estimation, one-shot NAS, and network morphism. - It focuses on surrogate-assisted NAS and discusses different types of surrogate models used, such as Bayesian optimization, evolutionary algorithms, graph neural networks, etc.- It compares the performance and efficiency of representative NAS methods on benchmark datasets like CIFAR-10/100 and ImageNet.- It discusses challenges and future research directions in improving computational efficiency of NAS, including sampling efficiency, model management strategies, federated learning, and green AI.In summary, the key contribution is providing a systematic and in-depth survey of techniques to improve the computational efficiency of neural architecture search, which is a very important issue limiting the practical application of NAS. The categorization, detailed analysis, and discussion of open challenges help provide insights and guide future research in this emerging field.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper provides a comprehensive survey of computationally efficient neural architecture search (NAS) methods, categorizing them into proxy-based approaches like low-fidelity estimation and one-shot NAS, and surrogate-assisted NAS using Bayesian optimization, evolutionary algorithms, and other techniques to predict network performance and guide the search.


## How does this paper compare to other research in the same field?

Here are a few key points in comparing this survey paper on computationally efficient neural architecture search (NAS) to other works in this emerging research area:- Scope: This paper provides a comprehensive overview of major approaches for improving the efficiency of NAS, covering both proxy methods and surrogate-assisted techniques. Many other surveys focus only on a subset of methods, like one-shot NAS or evolutionary NAS. - Categorization: The paper systematically categorizes techniques into low-fidelity estimation, one-shot NAS, network morphism, Bayesian optimization, evolutionary algorithms, etc. This provides a clear framework to understand the landscape. Other surveys often lack this level of structured classification.- Analysis: The paper includes not just descriptions but also discussions of design principles, performance comparisons, and computational complexity analysis. This level of critical analysis and quantification is missing in many existing surveys which are more descriptive.- Surrogate Models: This survey provides by far the most extensive coverage of surrogate-assisted NAS methods. Many works have focused only on proxy techniques without much discussion of surrogate models. The taxonomy of different surrogate model types is a significant contribution.- Emerging Topics: The paper also touches on latest topics like federated NAS, green AI, and future challenges that are often not examined in detail in other surveys on efficient NAS. Overall, this paper stands out for its comprehensive scope covering both proxy and surrogate techniques, structured categorization of methods, detailed performance analysis, and coverage of latest emerging topics. The technical depth especially on surrogate models and future challenges is a valuable contribution compared to other existing surveys in this domain.


## What future research directions do the authors suggest?

Based on my reading of the paper, here are some of the main future research directions suggested by the authors:1. Improving sampling efficiency of surrogate-assisted NAS methods. The authors suggest investigating how to build accurate surrogate models using fewer sampled architectures for training. This includes choosing good model architectures and initial training sets.2. Developing better model management strategies for online surrogate-assisted NAS. This involves determining when and how to sample new architectures to evaluate with the real objective function and update the surrogate model training set.3. Exploring privacy-preserving and communication-efficient NAS algorithms for federated learning. Potential directions include using differential privacy, secure aggregation, and reducing communication costs for population-based NAS. 4. Adapting NAS algorithms for green AI and deployment on resource-constrained edge devices. The authors suggest exploring indirect and generative encoding strategies to enhance flexibility.5. Considering multi-objective robust neural architecture search. The authors suggest incorporating robustness objectives in addition to performance accuracy into NAS.In summary, the main directions focus on improving surrogate model efficiency, adaptation to federated learning, and multi-objective robust NAS optimization suitable for edge devices. The authors highlight open challenges around sampling efficiency, model management strategies, and NAS for resource-constrained platforms.


## Summarize the paper in one paragraph.

Here is a one paragraph summary of the paper:This paper provides a comprehensive survey of computationally efficient neural architecture search (CE-NAS) methods. It first introduces NAS as bilevel optimization problem to find optimal architectures on a given dataset. NAS is computationally expensive due to the need to train and evaluate many candidate networks. The paper categorizes CE-NAS methods into proxy-based methods that use proxy metrics like low-fidelity estimation, one-shot NAS, and network morphism versus surrogate-assisted methods that train surrogate models to predict performance. It provides a detailed analysis of different surrogate model types like Bayesian optimization, evolutionary algorithms, graph neural networks, etc. The paper also discusses emerging topics like federated NAS and multi-objective NAS. It concludes by identifying challenges like sampling efficiency, model management, green AI, and future research directions in areas like federated learning. Overall, the paper gives a thorough overview of techniques to improve the efficiency of neural architecture search.


## Summarize the paper in two paragraphs.

Here is a two paragraph summary of the paper:The paper presents a survey on computationally efficient methods for neural architecture search (NAS). NAS has become increasingly popular for automatically designing deep neural network architectures for specific tasks. However, NAS is computationally expensive due to the need to train and evaluate a large number of candidate architectures. The paper categorizes computationally efficient NAS methods into proxy-based methods and surrogate-assisted methods. Proxy-based methods use techniques like low-fidelity estimation, one-shot NAS, and network morphism to reduce the cost of evaluating candidate architectures. Surrogate-assisted methods train surrogate models to predict the performance of architectures without explicit training, avoiding the computational overhead. The paper provides a comprehensive overview of techniques in both categories. For proxy methods, it covers strategies like early stopping, weight sharing, and network transformation. For surrogate methods, it discusses surrogate types like learning curve prediction, Bayesian optimization, and graph neural networks. It also covers emerging topics like federated NAS and multi-objective NAS. Overall, the survey gives a systematic analysis of techniques to improve NAS efficiency, highlights key challenges like sampling efficiency and model management, and suggests promising future research directions.
