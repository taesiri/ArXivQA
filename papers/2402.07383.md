# [Making Flow-Matching-Based Zero-Shot Text-to-Speech Laugh as You Like](https://arxiv.org/abs/2402.07383)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem: 
Most text-to-speech (TTS) systems lack the ability to produce realistic and natural-sounding non-linguistic speech cues such as laughter, crying, and whispering. This limits their ability to convey emotions and nuances critical for human-like communication. Although there have been attempts to generate laughter sounds, prior works lack precise control over the timing and expression of the generated laughter. Recently proposed zero-shot TTS models that can mimic any voice also do not allow control over speech expressions like laughter.

Proposed Solution:
This paper proposes ELaTE, a zero-shot TTS model that can generate natural laughing speech from any speaker with precise control over laughter timing and expression. ELaTE takes as input - (1) a speaker prompt to mimic voice, (2) text prompt indicating speech contents, (3) additional input controlling laughter (start/end times or example laughter audio). 

ELaTE extends a conditional flow-matching-based zero-shot TTS model by fine-tuning it with frame-level laughter detector features as additional conditioning. A simple scheme of mixing small-scale laughter-conditioned data and large-scale pre-training data during fine-tuning allows learning to generate laughter without quality degradation.

Main Contributions:

- Precise control over laughter timing - user can specify when to laugh. Allows generating speech with laughter while talking.

- Precise control over laughter expression - user can guide expression by example laughter audio, useful for applications like speech-to-speech translation.

- Builds upon a well-trained zero-shot TTS model - retains high audio quality and negligible computational overhead compared to baseline model. Falls back to conventional zero-shot TTS when no laughter control input is provided.

- Evaluations on a Chinese-English speech translation dataset and LibriSpeech demonstrate significantly improved controllability of laughter generation over baselines without compromising base model quality.
