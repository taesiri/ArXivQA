# [APE-then-QE: Correcting then Filtering Pseudo Parallel Corpora for MT   Training Data Creation](https://arxiv.org/abs/2312.11312)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Neural machine translation (NMT) models require large amounts of high-quality parallel data to train effectively, which is not available for low-resource languages like Marathi. 
- Existing pseudo-parallel corpora contain a lot of noise which degrades NMT performance.

Proposed Solution:
- Use an automatic post-editing (APE) system to correct errors in the target side of a noisy English-Marathi pseudo-parallel corpus. 
- Use a quality estimation (QE) system to select the best output between the original target sentence and the APE-corrected sentence.
- Extract a cleaned parallel corpus using APE and QE to train an English-Marathi NMT system.

Key Contributions:
- Novel adaptation of APE and QE to extract high-quality parallel data from noisy pseudo-parallel corpora.
- APE system trained using curriculum training strategy and augmented with phrase-level triplets.
- NMT models trained on APE-QE filtered pseudo-parallel corpus achieve +5.64 BLEU (En->Mr) and +9.91 BLEU (Mr->En) over baseline.
- Approach is language-agnostic given availability of APE and QE resources.
- Analyzed for English-Marathi due to availability of APE and QE data, but can be extended to other languages.

In summary, the key idea is to use APE to correct and QE to filter noisy parallel data, in order to train better NMT models for low-resource languages. The approach shows significant NMT quality improvements for English-Marathi and is independent of specific language properties.


## Summarize the paper in one sentence.

 This paper proposes a novel methodology of using an automatic post-editing system to correct errors in machine translation outputs, followed by a quality estimation system to filter the corrected outputs, in order to extract high-quality parallel sentences from noisy pseudo-parallel corpora for improving neural machine translation.


## What is the main contribution of this paper?

 Based on reviewing the paper, the main contribution is:

Adaption of APE-then-QE, which involves using an Automatic Post-Editing (APE) system to correct errors on the target side of the noisy pseudo-parallel corpus and then using a Quality Estimation (QE) system to select high-quality sentence pairs between the original and corrected versions. This is presented as a novel approach to extracting quality parallel corpus from pseudo-parallel data.

The key highlights are:
1) Using APE to correct errors in the target side of noisy pseudo-parallel data 
2) Then applying QE to choose between original target sentences and APE-corrected sentences
3) Demonstrating improved performance of 5.64 and 9.91 BLEU points for English-Marathi machine translation using this approach over baseline.

So in summary, the main contribution is the proposed APE-then-QE methodology to improve machine translation training data and ultimately machine translation performance.


## What are the keywords or key terms associated with this paper?

 Based on reviewing the paper, some of the key terms and concepts associated with it include:

- Automatic Post-Editing (APE) - The task of automatically identifying and correcting errors in machine translation outputs.

- Quality Estimation (QE) - The task of providing a quality score for a translation when the reference translation is unavailable. 

- Pseudo-parallel corpus - Noisy, web-crawled bilingual corpora that contains sentence pairs of varying quality.

- Neural Machine Translation (NMT) - Neural network based approaches for machine translation that are data-hungry.

- Parallel corpus filtering - Scoring/filtering mechanisms to extract high quality parallel sentences from noisy pseudo-parallel corpora.

- Curriculum training strategy - Gradually adapting pre-trained models to APE by using multiple stages of training data.

- BLEU score - Bilingual evaluation understudy, a commonly used automated metric for evaluating machine translation quality.

In summary, the key ideas involve using APE and QE models to correct errors and filter noisy pseudo-parallel data to improve low-resource neural machine translation.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in the paper:

1. The paper proposes using APE to correct errors in the target side of noisy pseudo-parallel corpora. What are some challenges with using APE for this task compared to using it to correct MT outputs? For example, how might the types of errors be different?

2. The paper selects between the original target sentence and the APE-corrected target sentence using quality estimation (QE). What are some pros and cons of using QE for this selection versus just always selecting the APE output?

3. The APE system is trained using a curriculum training strategy. What are the key components of this strategy? Why is it beneficial for training the APE system?

4. The paper shows significant BLEU score improvements from using the APE-then-QE method. What factors might contribute to the large gains seen? Could there be any caveats to the evaluation?

5. Could this APE-then-QE method potentially introduce any new errors or biases into the training data? If so, how might those be addressed?

6. How suitable do you think this approach would be for a very low-resource language pair? What minimum resources would be needed?

7. The paper focuses on the En-Mr language pair. What considerations would be important if extending this approach to other language pairs?

8. What improvements could be made to the QE component? For example, using word-level QE, multi-task learning, or more training data.

9. The paper uses BLEU score as the evaluation metric. What other automatic metrics or human evaluations could strengthen the analysis? What are some limitations of BLEU score?  

10. The paper trains APE and MT models separately. How could end-to-end joint training of APE, QE, and MT models be beneficial? What challenges would need to be addressed?
