# [Provably Sample Efficient RLHF via Active Preference Optimization](https://arxiv.org/abs/2402.10500)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Reinforcement learning from human feedback (RLHF) is important for aligning large language models (LLMs) with human preferences. 
- High-quality human preference data is key but costly to obtain in large quantities. 
- Current approaches sample prompts uniformly at random, but it's unclear if this is sample efficient. 

Main Contributions:
1) Proves lower bound that uniform prompt sampling suffers constant suboptimality gap, demonstrating need for adaptive strategies. First negative result showing limitations of simple uniform sampling.

2) Proposes Active Preference Optimization (APO) algorithm that adaptively selects prompts and action pairs to query for preferences. Achieves $O(1/\sqrt{T})$ suboptimality gap with $T$ samples under Bradley-Terry-Luce preference model.

3) Extends guarantee to general function approximation case with only mild assumptions. Shows improved dependence on problem non-linearity compared to prior work.

4) Evaluates batch version APO-RLHF on sentiment control task with GPT-2. Shows higher reward model accuracy and better policy alignment than uniform sampling baseline with significantly fewer preference samples.

Proposed Solution:
- APO actively selects prompt and action pairs to maximize uncertainty/exploration according to the current model.
- Eliminates bad actions over time to focus on good ones.  
- After $T$ rounds, trains policy with collected preferences to maximize learned reward.
- Enjoys $O(1/\sqrt{T})$ suboptimality guarantee.
- Batch version APO-RLHF implemented and evaluates empirically.

The summary covers the key problem being addressed, the limitations of current approaches, the proposed active learning algorithm, its theoretical guarantees, and an empirical evaluation of a batch variant showing its superiority over uniform sampling baselines.
