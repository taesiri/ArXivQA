# [RPT: Relational Pre-trained Transformer Is Almost All You Need towards   Democratizing Data Preparation](https://arxiv.org/abs/2012.02469)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the main research question it addresses is:Can AI models such as deep learning help to automate human-easy but computer-hard data preparation tasks like data cleaning, entity resolution, and information extraction that are currently burdensome for data scientists, practitioners, and crowd workers?The authors propose that recent advances in natural language processing using deep learning models suggest these techniques could also be promising for automating challenging data preparation tasks. The paper presents Relational Pre-trained Transformer (RPT), a Transformer-based model architecture and self-supervised pre-training approach aimed at enabling AI models to gain relevant knowledge and experience from large tables to tackle data preparation tasks.In summary, the central hypothesis is that the RPT model can be pre-trained in a self-supervised way on large datasets of tables, fine-tuned for specific data preparation tasks, and leverage capabilities like transfer learning and few-shot learning to achieve strong performance on human-easy but computer-hard data preparation challenges. The paper aims to present the vision for RPT and support this hypothesis through model design, opportunities and directions, and preliminary experiments.


## What is the main contribution of this paper?

Based on my reading, the main contributions of this paper are:1. Presenting RPT (Relational Pre-trained Transformer), a Transformer-based sequence-to-sequence model for pre-training on tuples. RPT uses a denoising autoencoder approach where tuples are corrupted via masking and the model is trained to reconstruct the original tuples. The authors propose new tuple-aware masking techniques as part of the pre-training process.2. Discussing how the pre-trained RPT model can be fine-tuned for a wide range of downstream data preparation tasks like data cleaning, entity resolution, information extraction etc. The flexibility of the encoder-decoder architecture allows adapting RPT to different tuple-to-X settings.3. Proposing complementary techniques like collaborative training and few-shot learning that can augment RPT for specific tasks like entity resolution and information extraction. Preliminary experiments on collaborative training for entity matching are presented.4. Identifying several research opportunities to advance automated data preparation using techniques like RPT, such as handling dirty data, developing benchmark datasets, combining with human-in-the-loop systems, hybrid solutions with other data cleaning methods etc.In summary, the main contribution is the proposal of RPT as a pre-trained model for tuple-level representation learning, along with techniques to adapt it to downstream data preparation tasks. The paper also discusses open challenges and future work to realize the vision of automating human-easy but computer-hard data preparation tasks.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper proposes Relational Pre-trained Transformer (RPT), a Transformer-based sequence-to-sequence model that is pre-trained on tuples in a self-supervised manner and can be fine-tuned to support a wide range of data preparation tasks like data cleaning, entity resolution, and information extraction.


## How does this paper compare to other research in the same field?

Here is a summary of how this paper compares to other related work in the field of relational data preparation:- This paper proposes a new pre-trained model called RPT (Relational Pre-trained Transformer) for various data preparation tasks on relational data. It differs from prior work like TURL and TaBERT which focus more on joint understanding of text and tables, while RPT specializes in tuple-level tasks.- Compared to other pre-training methods like TURL, RPT uses an encoder-decoder architecture which is more flexible for downstream fine-tuning. TURL uses an encoder-only model. RPT also does not require an external knowledge base like TURL does for certain tasks.- For data cleaning, RPT pre-trains at the tuple level whereas much prior work examines only the table-level data or requires external tools/human input. RPT aims to learn relationships within tuples to automate cleaning.- For entity resolution, RPT proposes collaborative training of a matcher model across datasets. This enables knowledge transfer without sharing data. Prior work has not explored this federated learning approach for ER.- For information extraction, RPT connects the task to question answering by forming a query from the tuple. This leverages existing QA datasets/models. Other IE work does not frame it as a QA problem.- Overall, RPT provides a more automated approach to relational data preparation by pre-training a model at the tuple level. The encoder-decoder design makes it flexible. It also explores collaborative training for ER and connecting IE to QA. The key distinction from prior arts is the tuple-level pre-training.In summary, RPT pushes the boundaries of automated relational data preparation by leveraging pre-training and transfer learning at a more fine-grained tuple level compared to prior work. The paper discusses how this compares favorably to other table-level or external tools based approaches on key tasks like data cleaning, ER, and IE.


## What future research directions do the authors suggest?

Based on my reading of the paper, some of the key future research directions the authors suggest are:- Data cleaning: Developing techniques for automatic data cleaning that can handle cases that are challenging for machines but easy for humans. This includes problems like handling dirty data during training and creating high quality benchmark datasets.- Entity resolution: Exploring collaborative learning methods to share and transfer knowledge across different entity resolution datasets/domains. Also using few-shot learning techniques to allow learning subjective entity matching criteria from just a few user-provided examples.- Information extraction: Connecting more database-related information extraction tasks to well-studied NLP tasks like question answering to leverage pre-trained knowledge. Also combining AI models with human workers to reduce overall crowdsourcing costs.- Knowledge bases: Studying ways to combine explicit knowledge graphs with implicit knowledge from pre-trained language models. Also using explainable AI techniques to make models like RPT more interpretable.- Benchmarks: Creating more diverse real-world benchmarks beyond the common product domains to advance data preparation research. Also releasing high quality training datasets.In summary, key directions are improving robustness when learning from dirty data, transferring knowledge across tasks/datasets, rapidly adapting models with few examples, combining neural networks with symbolic knowledge, and producing datasets and benchmarks to enable advances. The authors call for the communities to collaborate on these opportunities.
