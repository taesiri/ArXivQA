# [RPT: Relational Pre-trained Transformer Is Almost All You Need towards   Democratizing Data Preparation](https://arxiv.org/abs/2012.02469)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the main research question it addresses is:Can AI models such as deep learning help to automate human-easy but computer-hard data preparation tasks like data cleaning, entity resolution, and information extraction that are currently burdensome for data scientists, practitioners, and crowd workers?The authors propose that recent advances in natural language processing using deep learning models suggest these techniques could also be promising for automating challenging data preparation tasks. The paper presents Relational Pre-trained Transformer (RPT), a Transformer-based model architecture and self-supervised pre-training approach aimed at enabling AI models to gain relevant knowledge and experience from large tables to tackle data preparation tasks.In summary, the central hypothesis is that the RPT model can be pre-trained in a self-supervised way on large datasets of tables, fine-tuned for specific data preparation tasks, and leverage capabilities like transfer learning and few-shot learning to achieve strong performance on human-easy but computer-hard data preparation challenges. The paper aims to present the vision for RPT and support this hypothesis through model design, opportunities and directions, and preliminary experiments.
