# [RPT: Relational Pre-trained Transformer Is Almost All You Need towards   Democratizing Data Preparation](https://arxiv.org/abs/2012.02469)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the main research question it addresses is:Can AI models such as deep learning help to automate human-easy but computer-hard data preparation tasks like data cleaning, entity resolution, and information extraction that are currently burdensome for data scientists, practitioners, and crowd workers?The authors propose that recent advances in natural language processing using deep learning models suggest these techniques could also be promising for automating challenging data preparation tasks. The paper presents Relational Pre-trained Transformer (RPT), a Transformer-based model architecture and self-supervised pre-training approach aimed at enabling AI models to gain relevant knowledge and experience from large tables to tackle data preparation tasks.In summary, the central hypothesis is that the RPT model can be pre-trained in a self-supervised way on large datasets of tables, fine-tuned for specific data preparation tasks, and leverage capabilities like transfer learning and few-shot learning to achieve strong performance on human-easy but computer-hard data preparation challenges. The paper aims to present the vision for RPT and support this hypothesis through model design, opportunities and directions, and preliminary experiments.


## What is the main contribution of this paper?

Based on my reading, the main contributions of this paper are:1. Presenting RPT (Relational Pre-trained Transformer), a Transformer-based sequence-to-sequence model for pre-training on tuples. RPT uses a denoising autoencoder approach where tuples are corrupted via masking and the model is trained to reconstruct the original tuples. The authors propose new tuple-aware masking techniques as part of the pre-training process.2. Discussing how the pre-trained RPT model can be fine-tuned for a wide range of downstream data preparation tasks like data cleaning, entity resolution, information extraction etc. The flexibility of the encoder-decoder architecture allows adapting RPT to different tuple-to-X settings.3. Proposing complementary techniques like collaborative training and few-shot learning that can augment RPT for specific tasks like entity resolution and information extraction. Preliminary experiments on collaborative training for entity matching are presented.4. Identifying several research opportunities to advance automated data preparation using techniques like RPT, such as handling dirty data, developing benchmark datasets, combining with human-in-the-loop systems, hybrid solutions with other data cleaning methods etc.In summary, the main contribution is the proposal of RPT as a pre-trained model for tuple-level representation learning, along with techniques to adapt it to downstream data preparation tasks. The paper also discusses open challenges and future work to realize the vision of automating human-easy but computer-hard data preparation tasks.
