# [LoRA-Flow: Dynamic LoRA Fusion for Large Language Models in Generative   Tasks](https://arxiv.org/abs/2402.11455)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem: 
- Existing methods for combining multiple Lightweight Adaption (LoRA) modules use static, task-level fusion weights. This means all examples and tokens in a task share the same weights, which is suboptimal for complex generative tasks.  
- Generative tasks like solving math problems require dynamically employing different skills based on the context. For example, understanding the Chinese description may rely more on a Chinese language LoRA while performing calculations depends more on a math LoRA.

Proposed Solution:
- LoRA-Flow: A method to dynamically combine multiple LoRAs using a fusion gate to generate token-level weights conditioned on the prefix. 
- The fusion gate has very few parameters (~0.2% of a LoRA) and can be trained with only 200 examples.
- Layer-wise fusion gates are used since different layers have different capabilities.

Main Contributions:
- Proposes LoRA-Flow for dynamic LoRA fusion with token-level weights that vary across time steps and layers.
- Demonstrates consistent improvements over baselines with task-level fusion on 6 generative tasks.
- Provides detailed analyses showing significant variation in fusion weights across layers and time steps, highlighting the need for dynamic weights.
- Shows some zero-shot generalization capability of the fusion gates across tasks.
- Underscores the potential for constructing flexible plug-and-play LLMs by reusing capabilities acquired by existing LoRAs.
