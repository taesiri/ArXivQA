# [LSTPrompt: Large Language Models as Zero-Shot Time Series Forecasters by   Long-Short-Term Prompting](https://arxiv.org/abs/2402.16132)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Time series forecasting (TSF) is important for predicting future events based on historical data. 
- Recent advances in large language models (LLMs) show promise for zero-shot TSF by aligning time series data with language. 
- However, existing prompting methods treat TSF simplistically as next token prediction, overlooking complex forecasting mechanisms needed to model temporal dependencies.

Proposed Solution:
- The paper proposes LSTPrompt, a novel prompt strategy to guide LLMs for accurate zero-shot TSF using two key ideas:
   1. TimeDecomp (TD): Decomposes TSF into short-term and long-term forecasting subtasks, each with distinct reasoning rules and mechanisms. This creates a coherent "chain-of-thought" path.
   2. TimeBreath (TB): Encourages LLMs to periodically "breathe" and reassess forecasting strategies. This enhances adaptability to changing patterns.

Key Contributions:
- Proposes the first specialized prompt strategy for zero-shot TSF with LLMs using TD and TB.
- TD creates a logical step-by-step forecasting path to improve LLM reasoning. 
- TB incentivizes reconsidering predictions, enabling better adaptation.
- Achieves state-of-the-art zero-shot TSF accuracy, outperforming strong baselines.
- Demonstrates competitive performance to supervised models in some cases.
- Provides an effective and computationally efficient approach to leverage LLMs for TSF.

In summary, the key innovation is specialized prompting to properly guide LLMs for TSF by decomposing the complex task and encouraging periodic reassessment of predictions. This unlocks strong zero-shot TSF performance from LLMs.
