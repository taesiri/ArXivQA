# [LISA: Layerwise Importance Sampling for Memory-Efficient Large Language   Model Fine-Tuning](https://arxiv.org/abs/2403.17919)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Large language models (LLMs) like ChatGPT have huge memory requirements, posing challenges for large-scale training and fine-tuning. 
- Existing parameter-efficient fine-tuning (PEFT) methods like LoRA reduce memory usage but still fail to match full parameter training performance in most large-scale settings.

Key Observation:
- The paper analyzes LoRA's layerwise weight norms during fine-tuning and discovers a skewed distribution - bottom/top layers occupy most weight norms while other layers account for a small amount. 
- This implies different layers have varied importance for LLM updates.

Proposed Method: 
- Inspired by the skewed distribution, the paper proposes Layerwise Importance Sampled AdamW (LISA) which randomly freezes unimportant layers based on an importance sampling strategy.
- LISA emulates LoRA's update pattern while avoiding its limited representation power, aiming to bridge the gap with full parameter tuning.

Contributions:
- LISA reduces memory consumption to be on par or lower than LoRA for models over 70B parameters.
- Across various downstream tasks, LISA outperforms LoRA by 11-37% and sometimes exceeds full parameter fine-tuning.
- Analyses show LISA's superior performance, faster convergence, and robustness across different model sizes (7B-70B parameters) and domains.

In summary, the paper makes significant contributions in memory-efficient LLM fine-tuning, where the proposed LISA method serves as a promising alternative to LoRA by randomly freezing unimportant layers during optimization. Experiments demonstrate its memory efficiency and performance gains over prior arts.
