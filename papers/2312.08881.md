# [AdaptIR: Parameter Efficient Multi-task Adaptation for Pre-trained Image   Restoration Models](https://arxiv.org/abs/2312.08881)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
Pre-training has shown promising results for image restoration tasks. However, the subsequent full fine-tuning stage requires training and saving separate copies of the large pre-trained model parameters for each downstream task, which incurs significant compute and storage costs. Recently proposed parameter-efficient transfer learning (PETL) methods offer an efficient alternative, but have mainly focused on high-level vision tasks and still face challenges when applied to low-level image restoration.

Proposed Solution:
This paper proposes AdaptIR, a novel PETL approach to efficiently adapt pre-trained image restoration models to various downstream tasks. The key ideas are:

1) Use a multi-branch inception structure with each branch specializing in a different restoration perspective, inspired by mixture-of-experts. This allows versatility across diverse degradations. 

2) Introduce three specialized modules in parallel branches to orthogonally model local spatial, global spatial and channel interactions. This enables powerful representations under low parameter budgets:

- Local Interaction Module (LIM) exploits local similarities using depthwise convolutions and kernel decomposition. 

- Frequency Affine Module (FAM) models global dependencies by transforming in the frequency domain.

- Channel Shift Module (CSM) captures channel interactions through spatial weighting.

3) Ensemble branch features with summation to complement representations.

4) Insert the proposed modules in parallel to MLPs of the frozen pre-trained model.

Main Contributions:

- First exploration of PETL techniques for adapting pre-trained image restoration models.

- Novel multi-branch structure and specialized modules to achieve versatility and powerful adaptation under low parameters.

- Extensive experiments showing AdaptIR matches or exceeds full fine-tuning performance across diverse tasks using only 0.6% of the parameters.

- Analysis providing insights into PETL for low-level vision tasks.
