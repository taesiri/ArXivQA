# [SInViG: A Self-Evolving Interactive Visual Agent for Human-Robot   Interaction](https://arxiv.org/abs/2402.11792)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Language ambiguity is ubiquitous in human-robot interaction. Previous works use human-robot interaction to resolve such ambiguity, but face challenges when deployed in the real world, such as handling complex visual inputs, open-ended dialogues, and diverse users.

Proposed Solution:  
- The paper proposes SInViG, a self-evolving interactive visual agent for human-robot interaction that aims to resolve language ambiguity through multi-turn visual dialogues.

- SInViG continuously learns from unlabeled images and large language models without human supervision to become more robust. It asks questions to disambiguate language and guesses target objects through dialogues.

Main Contributions:

- Proposes a self-evolving framework that automatically labels visual dialog data using the model from the previous iteration to train the next iteration's model. The labeled dialog data is further polished by large language models to improve quality.

- Achieves new state-of-the-art results on the InViG benchmark and outperforms previous iterations, showing efficacy of self-evolving learning.

- Experiments involving real humans show models from later iterations are preferred and rated higher, indicating improved user experience via self-evolution.

- Deploys model on a real robot for interactive manipulation tasks. It can achieve over 80% grasp success rate and handle diverse natural instructions despite visual complexity.

To summarize, the key innovation is a self-evolving loop that continuously improves an interactive visual grounding agent's robustness to challenges in real-world human-robot interaction via unlabeled data. Both visual dialog experiments and real robot deployments demonstrate clear benefits.
