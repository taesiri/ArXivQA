# [Investigating Multilingual Instruction-Tuning: Do Polyglot Models Demand   for Multilingual Instructions?](https://arxiv.org/abs/2402.13703)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem Statement
- There is a lack of open-source multilingual instruction-tuning datasets and benchmarks to train and evaluate helpful instruction-following capabilities of large language models (LLMs) across languages. Most prior work has focused only on English or uses limited academic benchmarks.  

- It is unclear whether pre-trained multilingual LLMs require fine-tuning on parallel instruction corpora in all languages or if monolingual instruction tuning is adequate for multilingual instruction following abilities.

Proposed Solution & Contributions
- Created high-quality parallel corpus Lima-X (1030 complex instructions each for English, German, French, Italian, Spanish).

- Created parallel benchmark MT-Bench-X by translating existing English MT-Bench for multilingual evaluation.

- Conducted systematic study on 7B parameter multilingual LLM by fine-tuning on Lima-X and Bactrian-X datasets with different language mixtures. Evaluated on MT-Bench-X.  

- Show fine-tuning on parallel corpora improves cross-lingual performance up to 4.6% over monolingual tuning. Challenges "Superficial Alignment Hypothesis".

- Mid-sized multilingual LLMs require large-scale instruction tuning datasets unlike smaller models.

- Analyzed agreement levels between human ratings and GPT-4 judgments on model responses. Show gaps for certain subjective categories.

In summary, the paper makes valuable contributions in creation of new multilingual instruction tuning resources and by conducting studies that provide insights into optimal training procedures and evaluation of multilingual generative models to assist users across languages.


## Summarize the paper in one sentence.

 This paper investigates multilingual instruction-tuning of large language models, creating parallel datasets and benchmarks to examine the effects of language mixing and dataset size on cross-lingual instruction-following capabilities.


## What is the main contribution of this paper?

 The main contributions of this paper are:

1. Creation of Lima-X, a high-quality, complex, parallel corpus comprising 1030 instructions for each English, German, French, Italian, and Spanish.

2. Creation of MT-Bench-X, a parallel, multilingual, human-curated evaluation dataset for evaluating instruction-tuned language models. 

3. A systematic multilingual instruction-tuning study focused on multilingual multi-turn user request performance of a mid-sized, multilingual language model by fine-tuning it on parallel instruction-tuning datasets.

4. Demonstrating that instruction-tuning on parallel corpora benefits cross-lingual instruction following capabilities by up to 4.6%, challenging the idea that monolingual tuning is enough.  

5. Showing that the Superficial Alignment Hypothesis does not hold in general for mid-sized multilingual models, which require large-scale instruction tuning datasets.

6. Conducting a human annotation study to understand the alignment between human-based and AI-based (GPT-4) evaluation in multilingual chat scenarios.

In summary, the main contribution is the extensive study of multilingual instruction-tuning for language models to follow instructions across languages, involving the creation of new datasets, systematic experiments, and human evaluations.


## What are the keywords or key terms associated with this paper?

 Based on my review, some of the key terms and concepts associated with this paper include:

- Multilingual instruction-tuning - The process of fine-tuning large language models on instructional data across multiple languages to improve their performance on instruction-following tasks. 

- Parallel corpora - Instructional datasets translated into multiple languages to enable multilingual instruction-tuning.

- Lima-X - A parallel, multilingual variant of the LIMA instructional dataset created by the authors.

- Bactrian-X - An existing large-scale, parallel instructional dataset used by the authors.  

- MT-Bench-X - A multilingual variant of the MT-Bench benchmark dataset created by the authors to evaluate instruction-following capabilities.

- Language mixtures - Different compositions of languages used in the instructional datasets, including monolingual, parallel multilingual, and sampled multilingual mixtures.

- Superficial alignment hypothesis - The hypothesis that only a small amount of instructional data is needed to teach language models the format of responses for a task.

- Cross-lingual instruction-tuning - Fine-tuning on parallel corpora translated into multiple languages to improve cross-lingual performance.

- Multilingual multi-turn evaluation - Assessing performance on complex, conversational instruction-following over multiple turns across languages.

So in summary, the key terms cover multilingual instruction-tuning techniques, datasets, benchmarks, and hypotheses evaluated in the study.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in this paper:

1. How does the quality and size of the translated instruction-tuning datasets Lima-X and Bactrian-X compare to other existing instruction-tuning datasets in terms of language coverage, complexity, and response diversity? 

2. Why was the Superficial Alignment Hypothesis not supported by the results of mid-sized multilingual models trained on the small Lima-X dataset? What implications does this have for the training data requirements of different model sizes?

3. What was the motivation behind creating mixed-language variants like ENDEFRITES and sampled for the instruction-tuning datasets? How did models trained on these variants compare to monolingual versions in the cross-lingual transfer learning experiments?

4. The human evaluation study found relatively low agreement levels between human judges and lower correlation with GPT-4 judgments compared to prior work. What factors could explain this discrepancy and how might the evaluation be improved?  

5. How suitable are pre-training benchmarks designed for assessing capabilities of foundation models at evaluating the performance of instruction-tuned models on complex, multi-turn conversational tasks?

6. Could the underperformance of models on reasoning, math, coding, and extraction-related sub-categories in MT-Bench-X indicate gaps in pre-training rather than issues with instruction tuning?

7. What role does choice of tokenizer play in ensuring fair representation of languages in the pre-training process? How does this impact downstream performance?  

8. How accurately does performance on MT-Bench-X measure real-world conversational ability across languages? Are additional evaluation metrics needed?

9. How does the presence of positional bias in GPT-4 judgments, especially for creative tasks, impact the reliability of using it for automated evaluation? 

10. What directions for future research emerge from this work around multilingual multi-turn instruction tuning, human evaluation, and generalized cross-lingual transfer learning?
