# [Investigating Multilingual Instruction-Tuning: Do Polyglot Models Demand   for Multilingual Instructions?](https://arxiv.org/abs/2402.13703)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem Statement
- There is a lack of open-source multilingual instruction-tuning datasets and benchmarks to train and evaluate helpful instruction-following capabilities of large language models (LLMs) across languages. Most prior work has focused only on English or uses limited academic benchmarks.  

- It is unclear whether pre-trained multilingual LLMs require fine-tuning on parallel instruction corpora in all languages or if monolingual instruction tuning is adequate for multilingual instruction following abilities.

Proposed Solution & Contributions
- Created high-quality parallel corpus Lima-X (1030 complex instructions each for English, German, French, Italian, Spanish).

- Created parallel benchmark MT-Bench-X by translating existing English MT-Bench for multilingual evaluation.

- Conducted systematic study on 7B parameter multilingual LLM by fine-tuning on Lima-X and Bactrian-X datasets with different language mixtures. Evaluated on MT-Bench-X.  

- Show fine-tuning on parallel corpora improves cross-lingual performance up to 4.6% over monolingual tuning. Challenges "Superficial Alignment Hypothesis".

- Mid-sized multilingual LLMs require large-scale instruction tuning datasets unlike smaller models.

- Analyzed agreement levels between human ratings and GPT-4 judgments on model responses. Show gaps for certain subjective categories.

In summary, the paper makes valuable contributions in creation of new multilingual instruction tuning resources and by conducting studies that provide insights into optimal training procedures and evaluation of multilingual generative models to assist users across languages.
