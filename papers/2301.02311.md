# [HierVL: Learning Hierarchical Video-Language Embeddings](https://arxiv.org/abs/2301.02311)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question is: How can we develop video-language embeddings that capture both short-term actions and long-term intents in videos? 

The key hypothesis is that learning video-language embeddings with a hierarchical framework, using both short clip-text descriptions and long video-text summaries, will yield representations that are superior at modeling both short and long-term video understanding compared to standard video-language embedding methods.

In particular, the paper proposes that:

- Using short clip-text pairs will capture the immediate actions happening in the video ("what is the person doing now"). 

- Using long video-text summaries will capture the overall intent and context ("what the person aims to do").

- Jointly training with these two levels of supervision in a parent-child framework will enable transferring knowledge between the levels, improving both the short and long-term representations.

The experiments aim to validate whether the proposed hierarchical video-language embedding method, called HierVL, outperforms baselines and state-of-the-art approaches on various short and long-term video understanding tasks.
