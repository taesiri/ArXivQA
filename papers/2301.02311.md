# [HierVL: Learning Hierarchical Video-Language Embeddings](https://arxiv.org/abs/2301.02311)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question is: How can we develop video-language embeddings that capture both short-term actions and long-term intents in videos? 

The key hypothesis is that learning video-language embeddings with a hierarchical framework, using both short clip-text descriptions and long video-text summaries, will yield representations that are superior at modeling both short and long-term video understanding compared to standard video-language embedding methods.

In particular, the paper proposes that:

- Using short clip-text pairs will capture the immediate actions happening in the video ("what is the person doing now"). 

- Using long video-text summaries will capture the overall intent and context ("what the person aims to do").

- Jointly training with these two levels of supervision in a parent-child framework will enable transferring knowledge between the levels, improving both the short and long-term representations.

The experiments aim to validate whether the proposed hierarchical video-language embedding method, called HierVL, outperforms baselines and state-of-the-art approaches on various short and long-term video understanding tasks.


## What is the main contribution of this paper?

 The main contribution of this paper is proposing HierVL, a novel hierarchical video-language embedding that learns to capture both short-term actions and long-term intents in videos. The key ideas are:

- Using a hierarchical training objective with two levels - a clip level that matches short video clips to narrations describing the atomic actions, and a video level that matches aggregated clip features to abstractive text summaries of the full video. 

- Jointly training the model on both levels of supervision to learn video features that encode both granular actions and high-level intents.

- Showing that this hierarchical approach improves performance on both short-term clip-level tasks like action recognition as well as long-term video-level tasks like anticipation and retrieval, outperforming prior video-language embedding methods.

In summary, the main contribution is developing a hierarchical video-language representation that captures both short-term atomic actions and long-term activity goals/context, thereby improving video understanding across multiple timescales. The proposed HierVL model achieves state-of-the-art results on various downstream benchmarks.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the paper:

The paper proposes a novel hierarchical video-language embedding called HierVL that captures both short-term actions and long-term intents in videos by matching clip-level descriptions with video clips and also matching abstractive video-level text summaries with aggregated clip representations.


## How does this paper compare to other research in the same field?

 Here are a few key points comparing this paper to other research on video-language embeddings and long-term video understanding:

- This paper introduces a novel hierarchical training framework that learns embeddings capturing both short-term "what is happening" and long-term "why it is happening". Other video-language embeddings like MIL-NCE, EgoVLP, and VideoClip focus only on matching short video clips to corresponding narrations. 

- The paper proposes using abstractive video-level text summaries during training along with clip-level narrations. This is a new idea compared to prior work like EgoVLP and MIL-NCE that rely solely on clip-level narrations/captions. Using summaries helps inject high-level intent.

- The paper shows state-of-the-art results on multiple downstream tasks evaluating both short and long-term video understanding, including Charades-Ego action recognition, EPIC-KITCHENS retrieval, and Ego4D long-term anticipation. This demonstrates broad applicability.

- For long video modeling, the paper uses clip feature aggregation rather than directly encoding long videos. Other recent work like MEMVIT and Clip-Hitchhiker also argue for aggregation as a computationally cheaper alternative to directly modeling long videos.

- The hierarchical contrastive training framework is novel. Other hierarchical video-language models like HERO match frames to clip captions, but do not contrast long summaries. CMHSE aggregates sentences for retrieval but does not use abstractive summaries.

In summary, the key novelties are using abstractive long video summaries, hierarchical training for joint short and long-term modeling, and showing strong performance on both clip and video tasks. The work fits into the broader trend of aggregation for efficient long video encoding.


## What future research directions do the authors suggest?

 Based on my reading of the paper, some of the key future research directions suggested by the authors include:

- Exploring other hierarchical video-language embedding architectures and objectives beyond their proposed HierVL approach. For example, incorporating additional modalities like audio, or using different aggregation functions.

- Scaling up the model size and pretraining dataset size. The authors note their model was limited by computational constraints, so larger models trained on more data could further improve performance.

- Adapting the approach to other video domains beyond egocentric/first-person video. The Ego4D dataset used for pretraining has specific characteristics, so evaluating on other types of video data could be interesting.

- Incorporating structured knowledge into the model, such as common sense or physical intuitions, to better capture high-level intent and temporal relationships.

- Applying the hierarchical video-language embedding to additional downstream tasks like video captioning, action localization, future forecasting, etc.

- Extending the hierarchy beyond two levels, to incorporate scene-level information or other intermediate semantic granularities.

- Studying the learned representations to better understand what knowledge is captured at different timescales and abstraction levels.

In summary, the authors suggest many promising research avenues to build on their idea of hierarchical video-language embedding learning, across model architecture, training data, knowledge incorporation, downstream tasks, hierarchy depth, and representation analysis.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the paper:

The paper proposes HierVL, a novel hierarchical video-language embedding model for learning joint video and text representations. Unlike prior video-language embedding methods that match short video clips with corresponding narrations, HierVL is trained using both low-level clip-text pairs as well as high-level video summaries. Specifically, it performs contrastive learning at two levels - a clip level matching clips to narrations, and a video level matching aggregated clips to abstractive summaries. This allows the model to capture both short-term actions as well as long-term intent. The authors demonstrate state-of-the-art performance on downstream tasks including action recognition, anticipation, and retrieval by pretraining on the Ego4D dataset. Key results show HierVL outperforms baselines on Charades-Ego action recognition and Ego4D long-term anticipation. The hierarchical training is shown to improve both clip-level and video-level representations. Overall, the model advances video-language understanding by leveraging rich multi-level supervision during pretraining.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the paper:

The paper proposes HierVL, a novel hierarchical video-language embedding model for learning both short-term and long-term associations between videos and text. 

Existing video-language embedding methods learn to match short video clips with corresponding narrations. However, they fail to capture the long-term context and intent behind activities in the video. To address this, HierVL is trained on videos with both short clip-level narrations as well as a high-level text summary describing the overall activity. A hierarchical contrastive learning objective matches clips to narrations at the lower level, and aggregated clip representations to summaries at the higher level. This allows HierVL to learn both fine-grained clip representations capturing short-term actions, as well as video-level representations that encode long-term context. Extensive experiments on Ego4D, Charades-Ego, EPIC-Kitchens and HowTo100M show HierVL outperforms prior methods on tasks requiring both short and long-term understanding. The key technical novelty is the hierarchical training framework using two levels of annotation. HierVL achieves new state-of-the-art results on multiple video benchmarks including  Ego4D long-term anticipation and Charades-Ego action recognition.


## Summarize the main method used in the paper in one paragraph.

 Here is a one paragraph summary of the main method used in the paper:

The paper proposes HierVL, a novel hierarchical video-language embedding model for learning both short-term and long-term relationships between video and language. The key idea is to train the model using two levels of contrastive learning objectives. At the lower "child" level, short video clips are matched to corresponding short narration sentences to capture fine-grained actions. At the higher "parent" level, aggregated clip features for the full video are matched to abstractive summary sentences describing the overall intent and context. This is enabled by using a dataset like Ego4D that provides both detailed narrations aligned to clips as well as video-level summaries. The model learns a shared clip-level encoder and a aggregation module for creating video-level features. By training with the two-level objectives jointly, the model is able to learn both detailed clip representations and longer-term video representations capturing high-level context and intent. Experiments demonstrate state-of-the-art performance on tasks requiring both short and long-term video understanding.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some key terms and keywords are:

- Hierarchical video-language embedding - The main method proposed in the paper for learning joint video and text representations that capture both short-term actions and long-term intents.

- Short-term vs long-term modeling - The paper discusses modeling both fine-grained short-term actions as well as high-level long-term activities and goals. The hierarchical embedding captures both.

- Contrastive learning - The model is trained using a hierarchical contrastive learning objective to align video segments with corresponding narrations and summaries. 

- Parent vs child layers - The hierarchical embedding has a parent layer to model long-term video and a child layer for short clips.

- Aggregation - Long video representations are obtained by aggregating short clip features rather than directly modeling long sequences.

- Transfer learning - The pretrained hierarchical embedding is shown to transfer effectively to various downstream tasks like action recognition, anticipation, retrieval etc.

- State-of-the-art results - The method achieves new state-of-the-art on tasks like Ego4D long-term anticipation, Charades-Ego action recognition etc.

- Ego4D dataset - The hierarchical video-text dataset used for pretraining the model. It has narrations and summaries.

- Ablation studies - Analyses to validate the impact of hierarchical training, joint modeling, summaries etc.
