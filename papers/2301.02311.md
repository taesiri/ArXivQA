# [HierVL: Learning Hierarchical Video-Language Embeddings](https://arxiv.org/abs/2301.02311)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question is: How can we develop video-language embeddings that capture both short-term actions and long-term intents in videos? 

The key hypothesis is that learning video-language embeddings with a hierarchical framework, using both short clip-text descriptions and long video-text summaries, will yield representations that are superior at modeling both short and long-term video understanding compared to standard video-language embedding methods.

In particular, the paper proposes that:

- Using short clip-text pairs will capture the immediate actions happening in the video ("what is the person doing now"). 

- Using long video-text summaries will capture the overall intent and context ("what the person aims to do").

- Jointly training with these two levels of supervision in a parent-child framework will enable transferring knowledge between the levels, improving both the short and long-term representations.

The experiments aim to validate whether the proposed hierarchical video-language embedding method, called HierVL, outperforms baselines and state-of-the-art approaches on various short and long-term video understanding tasks.


## What is the main contribution of this paper?

 The main contribution of this paper is proposing HierVL, a novel hierarchical video-language embedding that learns to capture both short-term actions and long-term intents in videos. The key ideas are:

- Using a hierarchical training objective with two levels - a clip level that matches short video clips to narrations describing the atomic actions, and a video level that matches aggregated clip features to abstractive text summaries of the full video. 

- Jointly training the model on both levels of supervision to learn video features that encode both granular actions and high-level intents.

- Showing that this hierarchical approach improves performance on both short-term clip-level tasks like action recognition as well as long-term video-level tasks like anticipation and retrieval, outperforming prior video-language embedding methods.

In summary, the main contribution is developing a hierarchical video-language representation that captures both short-term atomic actions and long-term activity goals/context, thereby improving video understanding across multiple timescales. The proposed HierVL model achieves state-of-the-art results on various downstream benchmarks.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the paper:

The paper proposes a novel hierarchical video-language embedding called HierVL that captures both short-term actions and long-term intents in videos by matching clip-level descriptions with video clips and also matching abstractive video-level text summaries with aggregated clip representations.
