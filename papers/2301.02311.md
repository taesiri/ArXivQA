# [HierVL: Learning Hierarchical Video-Language Embeddings](https://arxiv.org/abs/2301.02311)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question is: How can we develop video-language embeddings that capture both short-term actions and long-term intents in videos? 

The key hypothesis is that learning video-language embeddings with a hierarchical framework, using both short clip-text descriptions and long video-text summaries, will yield representations that are superior at modeling both short and long-term video understanding compared to standard video-language embedding methods.

In particular, the paper proposes that:

- Using short clip-text pairs will capture the immediate actions happening in the video ("what is the person doing now"). 

- Using long video-text summaries will capture the overall intent and context ("what the person aims to do").

- Jointly training with these two levels of supervision in a parent-child framework will enable transferring knowledge between the levels, improving both the short and long-term representations.

The experiments aim to validate whether the proposed hierarchical video-language embedding method, called HierVL, outperforms baselines and state-of-the-art approaches on various short and long-term video understanding tasks.


## What is the main contribution of this paper?

 The main contribution of this paper is proposing HierVL, a novel hierarchical video-language embedding that learns to capture both short-term actions and long-term intents in videos. The key ideas are:

- Using a hierarchical training objective with two levels - a clip level that matches short video clips to narrations describing the atomic actions, and a video level that matches aggregated clip features to abstractive text summaries of the full video. 

- Jointly training the model on both levels of supervision to learn video features that encode both granular actions and high-level intents.

- Showing that this hierarchical approach improves performance on both short-term clip-level tasks like action recognition as well as long-term video-level tasks like anticipation and retrieval, outperforming prior video-language embedding methods.

In summary, the main contribution is developing a hierarchical video-language representation that captures both short-term atomic actions and long-term activity goals/context, thereby improving video understanding across multiple timescales. The proposed HierVL model achieves state-of-the-art results on various downstream benchmarks.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the paper:

The paper proposes a novel hierarchical video-language embedding called HierVL that captures both short-term actions and long-term intents in videos by matching clip-level descriptions with video clips and also matching abstractive video-level text summaries with aggregated clip representations.


## How does this paper compare to other research in the same field?

 Here are a few key points comparing this paper to other research on video-language embeddings and long-term video understanding:

- This paper introduces a novel hierarchical training framework that learns embeddings capturing both short-term "what is happening" and long-term "why it is happening". Other video-language embeddings like MIL-NCE, EgoVLP, and VideoClip focus only on matching short video clips to corresponding narrations. 

- The paper proposes using abstractive video-level text summaries during training along with clip-level narrations. This is a new idea compared to prior work like EgoVLP and MIL-NCE that rely solely on clip-level narrations/captions. Using summaries helps inject high-level intent.

- The paper shows state-of-the-art results on multiple downstream tasks evaluating both short and long-term video understanding, including Charades-Ego action recognition, EPIC-KITCHENS retrieval, and Ego4D long-term anticipation. This demonstrates broad applicability.

- For long video modeling, the paper uses clip feature aggregation rather than directly encoding long videos. Other recent work like MEMVIT and Clip-Hitchhiker also argue for aggregation as a computationally cheaper alternative to directly modeling long videos.

- The hierarchical contrastive training framework is novel. Other hierarchical video-language models like HERO match frames to clip captions, but do not contrast long summaries. CMHSE aggregates sentences for retrieval but does not use abstractive summaries.

In summary, the key novelties are using abstractive long video summaries, hierarchical training for joint short and long-term modeling, and showing strong performance on both clip and video tasks. The work fits into the broader trend of aggregation for efficient long video encoding.
