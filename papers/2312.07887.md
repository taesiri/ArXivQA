# [Learn or Recall? Revisiting Incremental Learning with Pre-trained   Language Models](https://arxiv.org/abs/2312.07887)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a high-quality paragraph summarizing the key points of the paper:

This paper revisits incremental learning (IL) of classification tasks using pre-trained language models (PLMs). Through extensive experiments on various datasets, settings, and methods, the authors make several major findings. First, PLMs have much greater inherent anti-forgetting abilities than typically assumed, with sequential fine-tuning alone achieving high performance. Probing studies reveal that catastrophic forgetting happens primarily due to changes in the relative positioning of class embeddings rather than losses in the PLMs' representations. Additionally, a simple strategy called SEQ*, which freezes PLMs and old task classifiers after briefly warming up the models, performs on par with or better than complex state-of-the-art IL techniques in most settings. The results urge the community to re-examine assumptions of forgetting in PLMs. Key strategies include properly handling task-specific classifiers and judiciously freezing model components to prioritize stability over plasticity. Going forward, probing analysis and simplified baselines like SEQ* should underpin IL research on PLMs rather than just performance comparisons.
