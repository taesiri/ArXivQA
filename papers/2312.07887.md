# [Learn or Recall? Revisiting Incremental Learning with Pre-trained   Language Models](https://arxiv.org/abs/2312.07887)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a high-quality paragraph summarizing the key points of the paper:

This paper revisits incremental learning (IL) of classification tasks using pre-trained language models (PLMs). Through extensive experiments on various datasets, settings, and methods, the authors make several major findings. First, PLMs have much greater inherent anti-forgetting abilities than typically assumed, with sequential fine-tuning alone achieving high performance. Probing studies reveal that catastrophic forgetting happens primarily due to changes in the relative positioning of class embeddings rather than losses in the PLMs' representations. Additionally, a simple strategy called SEQ*, which freezes PLMs and old task classifiers after briefly warming up the models, performs on par with or better than complex state-of-the-art IL techniques in most settings. The results urge the community to re-examine assumptions of forgetting in PLMs. Key strategies include properly handling task-specific classifiers and judiciously freezing model components to prioritize stability over plasticity. Going forward, probing analysis and simplified baselines like SEQ* should underpin IL research on PLMs rather than just performance comparisons.


## Summarize the paper in one sentence.

 This paper revisits incremental learning methods for pre-trained language models and finds that catastrophic forgetting is less of an issue than previously assumed, proposing a simple but effective baseline method called SEQ*.


## What is the main contribution of this paper?

 Based on my understanding, the main contribution of this paper is:

1) Revisiting the assumption that pretrained language models (PLMs) suffer from catastrophic forgetting in incremental learning (IL) scenarios. Through extensive probing experiments, the authors reveal that PLMs have inherent anti-forgetting abilities and do not forget much when fine-tuned sequentially on new tasks.

2) Proposing a simple yet effective method called SEQ* for IL with PLMs. By combining sequential fine-tuning with strategies like freezing PLMs, freezing old classifiers etc., SEQ* demonstrates competitive or even better performance compared to many state-of-the-art IL methods, while requiring much less training time and parameters.

3) The findings urge the community to rethink the catastrophic forgetting in PLMs. The authors call for more studies to deeply understand the inherent incremental learning abilities of PLMs and design algorithms accordingly.

In summary, the key contribution is revisiting the assumption of catastrophic forgetting in PLMs through probing experiments, proposing the effective SEQ* method, and asking the community to rethink IL with PLMs.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper abstract and contents, some of the key terms and concepts associated with this paper include:

- Incremental learning (IL)
- Catastrophic forgetting
- Pre-trained language models (PLMs)
- Probing techniques
- Sequential fine-tuning (SEQ)
- Class-incremental learning (CIL)
- Task-incremental learning (TIL) 
- Text classification
- Intent classification
- Relation extraction
- Named entity recognition
- Encoder-only backbones (e.g. BERT)
- Decoder-only backbones (e.g. GPT)
- Linear probing 
- Cosine linear probing
- Prototype probing
- Cosine prototype probing
- Moving distance of class embeddings
- SEQ* (proposed simple but effective strategies for boosting SEQ)

The main focus of the paper is on studying catastrophic forgetting in PLMs during incremental learning, using probing techniques to analyze what is actually forgotten. The authors propose simple strategies called SEQ* to boost the performance of sequential fine-tuning. Experiments are conducted on various NLP classification tasks under class-incremental and task-incremental settings.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 potential in-depth questions about the method proposed in this paper:

1. The paper proposes a simple method called SEQ* for incremental learning with pre-trained language models. What are the key strategies used in SEQ* and what is the rationale behind each of them? Explain in detail.

2. The paper reveals that existing incremental learning methods underestimate the inherent anti-forgetting ability of pre-trained language models. What evidence does the paper provide to support this claim? Discuss the probing experiments and key findings.  

3. The paper argues that catastrophic forgetting is not the biggest issue when fine-tuning pre-trained language models incrementally. What does the paper identify as the primary cause of performance degradation on old tasks? Explain in detail.

4. The paper utilizes four different metrics for probing - linear probing, cosine linear probing, prototype probing and cosine prototype probing. Compare and contrast these metrics. Which one does the paper identify as the best and why?

5. Pre-training plays a key role in the anti-forgetting ability uncovered in this paper. What experiments does the paper conduct with models pretrained to varying degrees to illustrate this? Discuss the key takeaways.  

6. The paper identifies issues with the classifier as the primary reason for forgetting rather than the pre-trained language model backbone itself. Elaborate on the analysis done using class embedding norms and moving distances between class embeddings to establish this.

7. How does the paper establish that sequential fine-tuning (SEQ) should not be viewed as a lower bound baseline for incremental learning with pre-trained models? Compare SEQ* and probing performance metrics to showcase this.

8. The paper evaluates SEQ* extensively across tasks, datasets and model architectures. Summarize the key findings from comparing SEQ* against state-of-the-art incremental learning techniques. 

9. What implications does the paper highlight for future research on incremental learning with pre-trained language models based on the extensive analysis provided?

10. The paper identifies some limitations of the current study such as focusing only on classification tasks. What are some potential future research directions that can build on this work and address some of its limitations?
