# [Learn or Recall? Revisiting Incremental Learning with Pre-trained   Language Models](https://arxiv.org/abs/2312.07887)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a high-quality paragraph summarizing the key points of the paper:

This paper revisits incremental learning (IL) of classification tasks using pre-trained language models (PLMs). Through extensive experiments on various datasets, settings, and methods, the authors make several major findings. First, PLMs have much greater inherent anti-forgetting abilities than typically assumed, with sequential fine-tuning alone achieving high performance. Probing studies reveal that catastrophic forgetting happens primarily due to changes in the relative positioning of class embeddings rather than losses in the PLMs' representations. Additionally, a simple strategy called SEQ*, which freezes PLMs and old task classifiers after briefly warming up the models, performs on par with or better than complex state-of-the-art IL techniques in most settings. The results urge the community to re-examine assumptions of forgetting in PLMs. Key strategies include properly handling task-specific classifiers and judiciously freezing model components to prioritize stability over plasticity. Going forward, probing analysis and simplified baselines like SEQ* should underpin IL research on PLMs rather than just performance comparisons.


## Summarize the paper in one sentence.

 This paper revisits incremental learning methods for pre-trained language models and finds that catastrophic forgetting is less of an issue than previously assumed, proposing a simple but effective baseline method called SEQ*.


## What is the main contribution of this paper?

 Based on my understanding, the main contribution of this paper is:

1) Revisiting the assumption that pretrained language models (PLMs) suffer from catastrophic forgetting in incremental learning (IL) scenarios. Through extensive probing experiments, the authors reveal that PLMs have inherent anti-forgetting abilities and do not forget much when fine-tuned sequentially on new tasks.

2) Proposing a simple yet effective method called SEQ* for IL with PLMs. By combining sequential fine-tuning with strategies like freezing PLMs, freezing old classifiers etc., SEQ* demonstrates competitive or even better performance compared to many state-of-the-art IL methods, while requiring much less training time and parameters.

3) The findings urge the community to rethink the catastrophic forgetting in PLMs. The authors call for more studies to deeply understand the inherent incremental learning abilities of PLMs and design algorithms accordingly.

In summary, the key contribution is revisiting the assumption of catastrophic forgetting in PLMs through probing experiments, proposing the effective SEQ* method, and asking the community to rethink IL with PLMs.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper abstract and contents, some of the key terms and concepts associated with this paper include:

- Incremental learning (IL)
- Catastrophic forgetting
- Pre-trained language models (PLMs)
- Probing techniques
- Sequential fine-tuning (SEQ)
- Class-incremental learning (CIL)
- Task-incremental learning (TIL) 
- Text classification
- Intent classification
- Relation extraction
- Named entity recognition
- Encoder-only backbones (e.g. BERT)
- Decoder-only backbones (e.g. GPT)
- Linear probing 
- Cosine linear probing
- Prototype probing
- Cosine prototype probing
- Moving distance of class embeddings
- SEQ* (proposed simple but effective strategies for boosting SEQ)

The main focus of the paper is on studying catastrophic forgetting in PLMs during incremental learning, using probing techniques to analyze what is actually forgotten. The authors propose simple strategies called SEQ* to boost the performance of sequential fine-tuning. Experiments are conducted on various NLP classification tasks under class-incremental and task-incremental settings.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 potential in-depth questions about the method proposed in this paper:

1. The paper proposes a simple method called SEQ* for incremental learning with pre-trained language models. What are the key strategies used in SEQ* and what is the rationale behind each of them? Explain in detail.

2. The paper reveals that existing incremental learning methods underestimate the inherent anti-forgetting ability of pre-trained language models. What evidence does the paper provide to support this claim? Discuss the probing experiments and key findings.  

3. The paper argues that catastrophic forgetting is not the biggest issue when fine-tuning pre-trained language models incrementally. What does the paper identify as the primary cause of performance degradation on old tasks? Explain in detail.

4. The paper utilizes four different metrics for probing - linear probing, cosine linear probing, prototype probing and cosine prototype probing. Compare and contrast these metrics. Which one does the paper identify as the best and why?

5. Pre-training plays a key role in the anti-forgetting ability uncovered in this paper. What experiments does the paper conduct with models pretrained to varying degrees to illustrate this? Discuss the key takeaways.  

6. The paper identifies issues with the classifier as the primary reason for forgetting rather than the pre-trained language model backbone itself. Elaborate on the analysis done using class embedding norms and moving distances between class embeddings to establish this.

7. How does the paper establish that sequential fine-tuning (SEQ) should not be viewed as a lower bound baseline for incremental learning with pre-trained models? Compare SEQ* and probing performance metrics to showcase this.

8. The paper evaluates SEQ* extensively across tasks, datasets and model architectures. Summarize the key findings from comparing SEQ* against state-of-the-art incremental learning techniques. 

9. What implications does the paper highlight for future research on incremental learning with pre-trained language models based on the extensive analysis provided?

10. The paper identifies some limitations of the current study such as focusing only on classification tasks. What are some potential future research directions that can build on this work and address some of its limitations?


## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem Statement
- Incremental learning (IL) aims to learn new tasks sequentially without forgetting previous knowledge. Recently, pre-trained language models (PLMs) have been widely used as backbones for IL in NLP. 

- Most existing methods assume PLMs suffer from catastrophic forgetting and propose techniques to overcome it. However, the extent of forgetting in PLMs for IL has not been thoroughly evaluated.

Key Contributions
- Conducts extensive probing experiments on PLMs in various IL settings. Reveals that PLMs inherently preserve most knowledge even using vanilla fine-tuning (SEQ), contradicting common belief.

- Finds through analysis that catastrophic forgetting stems from the classifier rather than the PLM backbone. The relative positions between old class embeddings and features change during SEQ.

- Proposes a simple yet effective algorithm called SEQ* that freezes PLMs and old classifiers after warm-up, avoiding disturbance to old knowledge. 

- Achieves superior or competitive performance to 20+ state-of-the-art methods on various tasks, with fewer trainable parameters and less training time.

Main Conclusions
- The paper urges revisiting assumptions of catastrophic forgetting in PLMs. It shows their strong built-in ability of retaining past knowledge during IL using proper strategies.

- The findings encourage future research to have a more thorough understanding of forgetting dynamics in PLMs and design algorithms accordingly rather than relying on common assumptions.


## Summarize the paper in one sentence.

 This paper revisits incremental learning with pre-trained language models, finding that they have inherent anti-forgetting abilities that allow simple methods to achieve strong performance, challenging assumptions about catastrophic forgetting.


## What is the main contribution of this paper?

 Based on my understanding, the main contribution of this paper is:

1) It revisits the assumption that pretrained language models (PLMs) suffer from catastrophic forgetting in incremental learning settings. Through extensive experiments and analysis, it reveals that PLMs have inherent anti-forgetting abilities and do not forget much when fine-tuned sequentially on new tasks.

2) It proposes a simple yet effective method called SEQ* for incremental learning with PLMs. By freezing PLMs and old task classifiers after minimal update, using proper classifiers, and pre-allocating future classifiers, SEQ* achieves competitive or even better performance compared to state-of-the-art incremental learning methods, while requiring much less training time and parameters.

3) It provides insights into what causes forgetting in PLMs during sequential fine-tuning - the change in relative position between class embeddings and features rather than forgetting in the PLMs themselves. It also analyzes the role of pre-training and model architecture in the anti-forgetting abilities.

4) The findings and analysis urge the community to revisit assumptions about catastrophic forgetting in PLMs for incremental learning. The paper encourages future research to better understand and exploit the inherent capabilities of PLMs for incremental learning.

In summary, the main contribution is in revisiting assumptions, proposing an effective simple method, providing insights, and urging the community to rethink incremental learning with PLMs.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper abstract and contents, some of the key terms and concepts associated with this paper include:

- Incremental Learning (IL)
- Pre-trained Language Models (PLMs)
- Catastrophic forgetting
- Probing performance
- Sequential fine-tuning (SEQ)
- Class-Incremental Learning (CIL)
- Task-Incremental Learning (TIL)
- Linear probing
- Cosine probing
- Class embeddings
- Moving distance
- SEQ*

The paper revisits incremental learning methods for PLMs and questions the common assumption that PLMs suffer from catastrophic forgetting under sequential fine-tuning. Through extensive probing experiments, the authors reveal the inherent anti-forgetting abilities of PLMs and propose simple yet effective strategies to boost the performance of sequential fine-tuning (SEQ*). The key findings relate to properly measuring and understanding forgetting in PLMs, the role of pre-training and model architecture, the source of forgetting in the classifiers rather than the PLMs themselves, and how this understanding can guide the design of effective IL algorithms. Terms like probing performance, class embeddings, and moving distance are used to analyze model forgetting. SEQ* refers to their proposed improved incremental learning approach for PLMs.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in this paper:

1. The paper proposes a simple method called SEQ* for incremental learning with pre-trained language models. What are the key strategies used in SEQ* and what is the rationale behind each of them?

2. The paper reveals that catastrophic forgetting in PLMs during incremental learning is not as severe as commonly believed. What evidence does the paper provide to support this claim? Please elaborate.

3. The paper utilizes four different metrics for probing the knowledge retained in PLMs, including linear probing, cosine linear probing, prototype probing and cosine prototype probing. Can you analyze the advantages and disadvantages of each metric and explain why linear probing works best?

4. Pre-training is considered one of the key factors behind the anti-forgetting ability of PLMs. However, the paper shows that even randomly initialized models exhibit incremental learning ability to some extent. What implications does this finding have on our understanding of the role of pre-training?

5. The paper reveals that the forgetting in SEQ happens mainly due to the changes in the relative position between class embeddings and features. Can you explain this phenomenon in more detail and how the strategies used in SEQ* help mitigate this issue?  

6. While SEQ* shows strong performance across multiple datasets, it does not always outperform state-of-the-art methods like LAMOL. When does SEQ* fail to achieve superior performance compared to LAMOL-based methods and why?

7. The paper only focuses on incremental learning of classification tasks with PLMs. Do you think the conclusions would generalize to other incremental learning scenarios like regression, sequence generation etc.? Why or why not?

8. The paper hints that both the architecture of Transformers and pre-training lead to the anti-forgetting ability in PLMs. Can you hypothesize the potential mechanisms inside the Transformer architecture that enable incremental learning?

9. SEQ* essentially relies on freezing components of the model after initial fine-tuning. Do you think this would limit the model's ability to continue learning genuinely new knowledge after the first task? How can this issue be alleviated?

10. The paper claims SEQ* is very simple yet effective. Can you think of potential ways to extend SEQ* to make it perform even better, such as by combining it with replay strategies? What challenges might arise?
