# Language to Rewards for Robotic Skill Synthesis

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be: How can we effectively harness large language models (LLMs) to enable intuitive and interactive control of robotic skills, without needing extensive datasets or engineering of low-level control primitives?The key ideas proposed to address this question are:1) Using reward functions as the interface between high-level language instructions/corrections and low-level robot actions. The authors argue reward functions are a natural fit for grounding language since they can capture task semantics in a flexible way. 2) Leveraging LLMs to translate language instructions into parameterized reward functions, rather than trying to map language directly to low-level actions. This plays to the strengths of LLMs in understanding natural language.3) Combining the LLM-generated rewards with a real-time optimization tool, MuJoCo MPC, to actually synthesize optimal robot motions that maximize the rewards.4) An interactive framework where users can observe the robot behavior in real-time and provide natural language corrections to iteratively improve the rewards and behavior. So in summary, the core hypothesis is that reward functions are an effective intermediate representation for grounding language instructions in robotic control. LLMs can translate language to rewards, and real-time optimization can solve for the low-level actions, enabling an intuitive and interactive system for robot skill synthesis.
