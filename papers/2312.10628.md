# [T2M-HiFiGPT: Generating High Quality Human Motion from Textual   Descriptions with Residual Discrete Representations](https://arxiv.org/abs/2312.10628)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Generating high-quality, natural 3D human motions from textual descriptions is challenging due to the high-dimensionality and complex spatio-temporal dynamics of motion data. 
- Directly generating motions in the high-dimensional space often leads to visible defects. Learning the joint embedding space of text and motions is difficult.
- Recent diffusion models show impressive results but have limitations in motion-text matching accuracy and slow inference.

Proposed Solution:
- A two-stage framework called T2M-HiFiGPT comprising:
   1) Motion RVQ-VAE: Learns a 2D discrete residual motion representation
   2) Double-Tier GPT: Generates residual motion codes from text

- Motion RVQ-VAE:
   - Employs a CNN encoder-decoder and shared vector quantization codebook 
   - Models motion as residual codes across temporal and codebook dimensions
   - Achieves superior reconstruction than VQ-VAE with less computations
   - Allows longer sequence modeling and faster inference for stage 2

- Double-Tier GPT:
   - Temporal GPT summarizes past frames and text into context vectors
   - Residual GPT uses contexts to predict residual codes autoregressively
   - Architectural modifications capture spatio-temporal dependencies efficiently  

- Additional strategies address exposure bias:
   - Corrupted RVQ augmentation during training
   - Conditional dropout and classifier guidance during inference

Main Contributions:

- Demonstrates RVQ-VAE as an effective discrete representation for motions
- Introduces T2M-HiFiGPT that generates high-fidelity motions from text
- Surpasses state-of-the-art methods on established datasets across metrics  
- Provides detailed experiments analyzing model components
- Establishes a strong baseline for text-driven motion synthesis

The paper makes notable advancements in modeling the complex embedding between motions and text. With conceptual simplicity, T2M-HiFiGPT achieves new state-of-the-art results. The insights contribute valuable understanding that will stimulate further progress.
