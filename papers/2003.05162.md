# [Video2Commonsense: Generating Commonsense Descriptions to Enrich Video   Captioning](https://arxiv.org/abs/2003.05162)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be: How can we generate commonsense descriptions of latent aspects directly from videos, in order to provide richer, more semantically meaningful interpretations of visual content?Specifically, the paper proposes generating three types of commonsense descriptions from videos:- Intentions - The goals or motivations behind the actions in the video (e.g. why someone is performing an action)- Attributes - Properties or characteristics of the agents/objects in the video - Effects - The effects or results of the actions in the videoThe key idea is that while conventional video captioning describes observable objects and actions, generating these additional commonsense descriptions can provide extra context about unobservable aspects like mental states, goals, and results. The paper presents the new task of "Video to Commonsense" (V2C) generation, as well as a model called the V2C-Transformer to generate commonsense captions directly from video inputs. The hypothesis seems to be that generating these types of commonsense descriptions will lead to richer, more human-like understanding of video content compared to just describing visible objects and events.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contributions are:1. Proposing the novel task of generating commonsense descriptions (intentions, effects, attributes) directly from videos to enrich factual video captions. This is framed as the Video-to-Commonsense (V2C) task.2. Creating a new dataset called V2C by retrieving candidate descriptions from the ATOMIC commonsense dataset, re-ranking using BERT, and detailed human annotation. The dataset contains around 9k videos annotated with commonsense descriptions.3. Presenting the V2C-Transformer model architecture which serves as a strong baseline for the V2C task. It uses a video encoder and transformer decoder with cross-modal attention.4. Formulating the V2C-QA task, a video question answering benchmark requiring commonsense reasoning, as an alternate way to generate commonsense knowledge. A subset of the V2C dataset is converted to QA format.5. Results and experiments demonstrating the ability of the V2C-Transformer to generate relevant commonsense knowledge directly from videos. The V2C-QA task is also shown to enrich caption generation.In summary, the paper explores commonsense-enriched video understanding, proposes the novel V2C task and dataset, presents strong baseline models, and shows the usefulness of commonsense inference for video captioning.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Based on my reading of the paper, here is a one sentence summary:The paper presents a new framework called Video2Commonsense (V2C) to generate commonsense descriptions about latent aspects like intentions, effects, and attributes directly from videos, in order to enrich conventional video captioning.


## How does this paper compare to other research in the same field?

Here are a few key points comparing this paper to other research on video captioning and commonsense reasoning:- This is the first work exploring commonsense video captioning, going beyond just describing observable objects/actions to infer latent aspects like intentions, attributes, and effects. Most prior video captioning research focused on factual descriptions.- The paper introduces a new dataset "V2C" containing short video clips annotated with commonsense captions. This is unique compared to other video captioning datasets like MSR-VTT that lack commonsense annotations.- The V2C-Transformer model architecture is similar to other encoder-decoder models for video captioning, but incorporates a cross-modal attention mechanism to enable generating commonsense captions.- For commonsense reasoning, this paper relates to Atomic and other commonsense knowledge resources. A key difference is those are purely textual, while V2C grounds commonsense in videos.- The paper explores an open-ended video QA task (V2C-QA) to provide additional commonsense supervision. QA has been used alongside captioning before, but mainly for retrieval/multiple choice, not open-ended QA.- Compared to more complex multimodal transformer models like VideoBERT, the V2C-Transformer is relatively lightweight but still performs strongly on this new task.In summary, this paper pioneers the problem of commonsense video captioning with a new dataset, model architecture, and QA formulation. The results demonstrate feasibility of generating commonsense descriptions directly from video inputs. This could enable deeper video understanding compared to most prior factual video captioning works.
