# [Video2Commonsense: Generating Commonsense Descriptions to Enrich Video   Captioning](https://arxiv.org/abs/2003.05162)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question seems to be: 

How can we generate commonsense descriptions of latent aspects directly from videos, in order to provide richer, more semantically meaningful interpretations of visual content?

Specifically, the paper proposes generating three types of commonsense descriptions from videos:

- Intentions - The goals or motivations behind the actions in the video (e.g. why someone is performing an action)

- Attributes - Properties or characteristics of the agents/objects in the video 

- Effects - The effects or results of the actions in the video

The key idea is that while conventional video captioning describes observable objects and actions, generating these additional commonsense descriptions can provide extra context about unobservable aspects like mental states, goals, and results. 

The paper presents the new task of "Video to Commonsense" (V2C) generation, as well as a model called the V2C-Transformer to generate commonsense captions directly from video inputs. The hypothesis seems to be that generating these types of commonsense descriptions will lead to richer, more human-like understanding of video content compared to just describing visible objects and events.


## What is the main contribution of this paper?

 Based on my reading of the paper, the main contributions are:

1. Proposing the novel task of generating commonsense descriptions (intentions, effects, attributes) directly from videos to enrich factual video captions. This is framed as the Video-to-Commonsense (V2C) task.

2. Creating a new dataset called V2C by retrieving candidate descriptions from the ATOMIC commonsense dataset, re-ranking using BERT, and detailed human annotation. The dataset contains around 9k videos annotated with commonsense descriptions.

3. Presenting the V2C-Transformer model architecture which serves as a strong baseline for the V2C task. It uses a video encoder and transformer decoder with cross-modal attention.

4. Formulating the V2C-QA task, a video question answering benchmark requiring commonsense reasoning, as an alternate way to generate commonsense knowledge. A subset of the V2C dataset is converted to QA format.

5. Results and experiments demonstrating the ability of the V2C-Transformer to generate relevant commonsense knowledge directly from videos. The V2C-QA task is also shown to enrich caption generation.

In summary, the paper explores commonsense-enriched video understanding, proposes the novel V2C task and dataset, presents strong baseline models, and shows the usefulness of commonsense inference for video captioning.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Based on my reading of the paper, here is a one sentence summary:

The paper presents a new framework called Video2Commonsense (V2C) to generate commonsense descriptions about latent aspects like intentions, effects, and attributes directly from videos, in order to enrich conventional video captioning.


## How does this paper compare to other research in the same field?

 Here are a few key points comparing this paper to other research on video captioning and commonsense reasoning:

- This is the first work exploring commonsense video captioning, going beyond just describing observable objects/actions to infer latent aspects like intentions, attributes, and effects. Most prior video captioning research focused on factual descriptions.

- The paper introduces a new dataset "V2C" containing short video clips annotated with commonsense captions. This is unique compared to other video captioning datasets like MSR-VTT that lack commonsense annotations.

- The V2C-Transformer model architecture is similar to other encoder-decoder models for video captioning, but incorporates a cross-modal attention mechanism to enable generating commonsense captions.

- For commonsense reasoning, this paper relates to Atomic and other commonsense knowledge resources. A key difference is those are purely textual, while V2C grounds commonsense in videos.

- The paper explores an open-ended video QA task (V2C-QA) to provide additional commonsense supervision. QA has been used alongside captioning before, but mainly for retrieval/multiple choice, not open-ended QA.

- Compared to more complex multimodal transformer models like VideoBERT, the V2C-Transformer is relatively lightweight but still performs strongly on this new task.

In summary, this paper pioneers the problem of commonsense video captioning with a new dataset, model architecture, and QA formulation. The results demonstrate feasibility of generating commonsense descriptions directly from video inputs. This could enable deeper video understanding compared to most prior factual video captioning works.


## What future research directions do the authors suggest?

 Based on my reading of the paper, some of the main future research directions suggested by the authors are:

- Exploring other encoder architectures besides LSTM for encoding the video, such as 3D CNNs, to capture more spatial-temporal information. 

- Investigating other transformer architectures and pretraining strategies for the decoder beyond GPT, to improve commonsense reasoning and generation.

- Expanding the V2C dataset with more videos and diversity in captions and commonsense annotations.

- Extending the V2C framework to generate a coherent paragraph instead of individual commonsense sentences. 

- Utilizing V2C as a building block in downstream tasks like video question answering, summarization, etc that require deeper understanding.

- Incorporating external commonsense knowledge sources beyond ATOMIC during training.

- Evaluating on more real-world videos beyond the constrained domains in existing datasets.

- Developing better automatic evaluation metrics beyond n-gram based methods for generative commonsense tasks.

- Exploring other modalities like audio, subtitles, etc. in conjunction with video for enriched commonsense inference.

In summary, the key future directions are around improvements to the encoder-decoder architecture, scaling up the dataset, applying V2C to downstream tasks, and evaluating on more diverse and real-world videos. Augmenting with external knowledge and developing better evaluation metrics are also highlighted.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the paper:

The paper proposes a new task called Video-to-Commonsense (V2C) for generating commonsense descriptions from videos to enrich factual video captions. Videos with human agents involve actions that have intentions, effects, and reveal attributes about the agent. The proposed V2C framework generates these three types of commonsense descriptions - intentions, effects, and attributes. The authors present the V2C-Transformer model which utilizes a video encoder and transformer decoder with cross-modal attention. They create the V2C dataset by retrieving candidate descriptions from the ATOMIC commonsense dataset using video captions, followed by human annotation to select relevant descriptions and provide additional annotations. Experiments demonstrate the model's ability to generate plausible commonsense captions. Additionally, the authors frame V2C as an open-ended video question answering task (V2C-QA), where questions target the intention, effect, or attribute. Results show V2C-QA assists the model in generating better commonsense descriptions. Overall, this is the first work on generating visual commonsense captions directly from videos.


## Summarize the paper in two paragraphs.

 Here is a 2 paragraph summary of the paper:

The paper proposes a new method for generating commonsense descriptions from videos to enrich standard video captioning. The key idea is that beyond just describing observable objects and actions, video understanding requires modeling aspects like agents' intentions, attributes, and effects of actions. To enable this, the authors introduce the Video2Commonsense (V2C) framework which generates descriptions of intentions, attributes, and effects. They also present the V2C dataset which contains videos annotated with commonsense descriptions. 

The V2C framework utilizes a video encoder to extract features and a transformer decoder to generate captions and commonsense. Their V2C-Transformer model first predicts events from the video, then generates corresponding commonsense aspects. Experiments show their method generates more accurate and human-like commonsense descriptions compared to baselines. The authors also propose a V2C question answering task and dataset (V2C-QA) which can assist commonsense generation. Overall, this work takes an important step towards deeper video understanding models that can reason about latent aspects beyond observable actions. Enriching video captions with commonsense will enable better video question answering and other downstream applications.


## Summarize the main method used in the paper in one paragraph.

 Here is a one paragraph summary of the main method used in the paper:

The paper proposes a Video-to-Commonsense (V2C) framework to generate commonsense descriptions about videos, enriching the factual captions typically produced by video captioning models. The V2C-Transformer model utilizes (1) a video encoder to extract global representations from the input video frames, (2) a transformer decoder with two components - a caption decoder and a commonsense decoder - that generate the captions and commonsense descriptions respectively, and (3) a cross-modal self-attention module that allows attention over both visual and textual representations. The model is trained in two stages - first generating captions from videos, and then using the generated captions along with video features to produce relevant commonsense descriptions about intention, effect, and attributes. The V2C dataset contains videos annotated with captions and commonsense, created by retrieving candidates from the ATOMIC commonsense database, re-ranking using BERT, and detailed human annotation. The V2C-QA task of question answering about commonsense video aspects is also introduced to enrich caption generation.


## What problem or question is the paper addressing?

 Based on the abstract, this paper is addressing the problem of generating more descriptive and semantically meaningful video captions that go beyond just describing visible objects and actions. Specifically, it proposes a framework and model for generating "commonsense" video captions that describe latent aspects like the intentions, effects, and attributes of agents in the video.

The key questions/problems it aims to address are:

- How can we generate video captions that describe not just observable objects and events, but also latent "commonsense" aspects like intentions, effects, and attributes? 

- Can we enrich factual video captions with additional commonsense descriptions about the motivations, pre-conditions, mental states etc. of agents?

- How can we build datasets and models to generate such "commonsense enriched" video captions?

- Can we leverage external commonsense knowledge resources like ATOMIC along with human annotations to create more semantically meaningful video caption datasets?

- Can transformer-based architectures effectively generate relevant commonsense captions directly from video input?

- Can posing commonsense video captioning as a QA task assist in generating better commonsense descriptions?

So in summary, this paper introduces the novel problem of commonsense video captioning to move beyond factual descriptions, and proposes techniques to generate and enrich captions with commonsense knowledge about intentions, effects, attributes etc. of agents in videos.
