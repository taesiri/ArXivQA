# [Curated LLM: Synergy of LLMs and Data Curation for tabular augmentation   in ultra low-data regimes](https://arxiv.org/abs/2312.12112)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem Statement
The paper addresses the challenge of building machine learning models when there is very limited training data available (less than 100 samples), known as the ultra low-data regime. This is a common issue in domains like healthcare and finance where collecting large labeled datasets can be difficult or infeasible. It also arises in developing countries where digital infrastructure is limited. With so little data, conventional data augmentation techniques struggle to generate sufficient diverse and accurate synthetic samples to train models effectively. 

Proposed Solution - Curated LLM
The authors propose Curated LLM, an approach that combines large language models (LLMs) for data generation with a novel data curation technique. Specifically:

1. LLMs like GPT-4 are used to generate synthetic tabular data by providing a small number of real examples in-context. This allows the LLM to leverage its vast prior knowledge to produce more diverse and accurate samples compared to traditional generators, even extrapolating to unseen regions of the data distribution.

2. A data curation step filters the synthetic samples based on their learning dynamics when trained on the real small dataset. Metrics like confidence and aleatoric uncertainty allow identifying low-quality samples. The remaining curated synthetic data supplements the real data.

3. Downstream models trained on the combined real + curated synthetic data achieve much higher performance compared to only using the scarce real data or synthetic data from other generators.

Main Contributions
- Empirically demonstrates superior performance of Curated LLM over 6 state-of-the-art tabular data generators, especially in the ultra low-data regime (n<100)
- Shows the overlooked importance of curating synthetic data, improving all generators 
- Provides insights into LLM generation, showing extrapolation capabilities and importance of proper prompting to exploit prior knowledge
- Performance gains most pronounced for underrepresented subgroups in the data

The method allows expanding ML to data-scarce domains by allying strengths of LLMs and data-centric curation.
