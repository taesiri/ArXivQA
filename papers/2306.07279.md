# [Scalable 3D Captioning with Pretrained Models](https://arxiv.org/abs/2306.07279)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central research question this paper addresses is: How can we efficiently generate high-quality descriptive captions for large-scale 3D object datasets like Objaverse? The key hypotheses are:1) Leveraging existing large-scale image-text models like BLIP2, CLIP, and GPT-4 that are pretrained on varied data can help generate better 3D object captions compared to using just one model or human annotation. 2) Consolidating information from multiple rendered views of a 3D object using these models can produce more comprehensive and accurate captions compared to using just a single view.3) The automated captions generated through this multi-model, multi-view approach can enable better fine-tuning of text-to-3D models compared to human-authored or metadata captions.4) This automated captioning pipeline is superior to human annotation in terms of cost, speed, and sometimes quality.The paper seems to validate these hypotheses through experiments on Objaverse and other datasets. The core contribution is developing and evaluating this automated captioning approach to enable large-scale high-quality 3D-text data.
