# [Improved Visual Grounding through Self-Consistent Explanations](https://arxiv.org/abs/2312.04554)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a detailed paragraph summarizing the key points of the paper:

This paper proposes a novel weakly-supervised tuning approach called SelfEQ to improve the visual grounding capabilities of vision-language models without relying on bounding box supervision. The key idea is to leverage a large language model to automatically generate paraphrased descriptions of objects/regions in images and use these as a form of weak supervision to encourage the vision-language model to produce consistent attention maps. Specifically, for an input image-text pair, the model extracts attention maps using GradCAM. It then minimizes the distance between attention maps produced for the original phrase and its paraphrase, based on the assumption that equivalent phrases should highlight the same visual regions. Additionally, a region-of-interest mask is created to identify potentially correct predictions, and consistency is enforced within this mask. Experiments on Flickr30K, ReferIt and RefCOCO+ show SelfEQ outperforms prior weakly-supervised methods by a large margin. It also demonstrates competitive performance compared to some bounding box-supervised techniques. Qualitative results illustrate SelfEQ's ability to ground more vocabulary terms and handle challenging scenarios like small, occluded or obscured objects. The generated paraphrases prove effective for improving self-consistency as well as overall visual grounding.
