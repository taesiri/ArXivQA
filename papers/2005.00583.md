# Learning an Unreferenced Metric for Online Dialogue Evaluation

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central research question this paper addresses is: How can we develop an automatic dialogue evaluation metric that does not require true reference responses during inference, and correlates well with human judgements? The key hypotheses appear to be:1. Dialogue exhibits an inherent temporal structure, where utterances transition between latent semantic states. Capturing this structure is important for dialogue evaluation.2. Leveraging pre-trained language models like BERT to extract utterance representations, combined with modeling the temporal transitions, will allow developing an unreferenced automated dialogue evaluation metric that correlates better with human judgements.The authors propose a model called MaUDE that encodes utterances with BERT, models transitions with an RNN, and trains the metric using noise contrastive estimation to distinguish good vs bad responses - all without requiring true reference responses. Experiments show MaUDE correlates better with human judgements compared to baseline metrics.In summary, the central hypothesis is that modeling dialogue structure and using pre-trained language models can help develop an unreferenced automated metric for dialogue evaluation that correlates with humans. The key contribution is demonstrating the feasibility of this approach through the proposed MaUDE model.


## What is the main contribution of this paper?

The main contribution of this paper is proposing an unreferenced automated dialogue evaluation metric called MaUdE (Metric for automatic Unreferenced dialogue Evaluation). The key ideas are:- Showing that dialogue has an inherent temporal structure, where utterances transition between latent representations. This is demonstrated through visualizations of BERT embeddings on several dialogue datasets. - Leveraging this temporal structure by using a BERT encoder along with a recurrent neural network to model the transitions between utterance representations. - Training the metric in a completely unsupervised way via noise contrastive estimation, to differentiate between good responses and various adversarial negative responses.- Evaluating the metric on PersonaChat and showing it achieves better correlation with human judgments compared to prior unreferenced metrics like RUBER and InferSent. The main benefit is that MaUdE can be used to evaluate dialog systems in an online setting without requiring access to reference responses, since it is fully unreferenced. This also allows it to be used for training and optimization of dialog models through reinforcement learning. Overall, it demonstrates the promise of leveraging large pretrained language models and discourse structure for unreferenced dialogue evaluation.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence TL;DR summary of the paper:The paper proposes an unreferenced automated dialogue evaluation metric called MaUdE that leverages pre-trained language models and models the temporal structure of dialogues to achieve better correlation with human judgements for evaluating dialog systems.
