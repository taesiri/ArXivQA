# [Improving Socratic Question Generation using Data Augmentation and   Preference Optimization](https://arxiv.org/abs/2403.00199)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem: 
The paper focuses on the problem of automatically generating Socratic questions to guide students in debugging their code and help improve their learning outcomes. Manually creating such questions is labor-intensive for instructors. Large language models (LLMs) can be prompted to generate these questions. However, existing methods that prompt proprietary LLMs like GPT-3.5 and GPT-4 sometimes produce invalid questions that are irrelevant, repetitive, too direct by revealing solutions prematurely, or prematurely prompt students to fix code before identifying the root cause.  

Proposed Solution:
The paper proposes a two-phase method to generate valid Socratic questions using open-source LLMs. 
1) Data Augmentation: Invalid questions are synthetically generated by prompting GPT-4. These augment existing datasets to create preference pairs of valid and invalid questions.  
2) Preference Optimization: An open-source 7B parameter LLM - Llama 2 is fine-tuned on the preference dataset using direct preference optimization (DPO). DPO minimizes the likelihood of invalid questions while keeping valid questions likely.

Main Contributions:
- Introduces a data augmentation method to create invalid Socratic questions for training question generation models.  
- Shows that preference-optimizing Llama 2 with 7B parameters outperforms state-of-the-art prompting methods relying on proprietary 25B+ parameter models like GPT-3.5 and GPT-4 in terms of text similarity metrics to ground truth questions.
- Provides an effective and scalable method for Socratic question generation without relying on expensive proprietary LLMs.
- Consistently shows DPO outperforming standard supervised fine-tuning across different decoding methods, justifying the need for preference optimization.

The paper demonstrates an effective way to take advantage of large proprietary LLMs to augment datasets for fine-tuning smaller open-source LLMs, which can then match or outperform the proprietary models.
