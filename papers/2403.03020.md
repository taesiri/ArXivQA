# [SplAgger: Split Aggregation for Meta-Reinforcement Learning](https://arxiv.org/abs/2403.03020)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Meta-reinforcement learning (meta-RL) aims to learn agents that can rapidly adapt to new tasks. A category of meta-RL methods explicitly infer task distributions, using sequence models designed for this purpose. However, recent work shows end-to-end training of general sequence models works well. It is unclear if specialized sequence models still confer an advantage when trained end-to-end.

- The paper investigates if permutation invariant sequence models, which aggregate over transitions in an order-invariant way, are still useful without explicit task inference objectives. Surprisingly, they also study if incorporating some permutation variance is beneficial. 

Methods:
- They propose Split Aggregator (SplAgger), which combines both permutation invariant and variant components using split connections and a recurrent neural network (RNN).

- SplAgger simplifies a prior method, AMRL, by removing a problematic straight-through gradient modification that caused explosion. It also adds recent advances of using hypernetworks.

Contributions:
- They demonstrate specialized sequence models (permutation invariant and variant components) achieve substantially higher returns compared to general RNNs and other baselines across several meta-RL benchmarks, even without task inference.

- Analysis shows the combination enables SplAgger to be robust and achieve strengths of both invariant and variant sequence models. Variance enables learning easier suboptimal policies initially.  

- They also analyze failure modes of prior methods like AMRL, showing the gradient modification causes explosion that harms performance.

In summary, the key insight is specialized sequence models with the right inductive biases are still critical for efficient meta-RL, despite end-to-end training, enabling the best of both permutation invariance and variance. SplAgger combines components in a principled and high performing manner.
