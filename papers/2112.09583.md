# [Align and Prompt: Video-and-Language Pre-training with Entity Prompts](https://arxiv.org/abs/2112.09583)

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper addresses is how to learn effective cross-modal video-language representations from sparse video frames and text. The key hypotheses are:1. Learning instance-level video-text alignment with a contrastive loss can help align the features from individual modalities and ease subsequent cross-modal modeling. 2. Learning fine-grained region-entity alignment between visual regions and textual entities, via the proposed prompting entity modeling task, can improve the model's capability in capturing and reasoning about local visual information.3. Operating on sparsely sampled frames rather than dense feature extraction can enable end-to-end learning while being efficient.To summarize, the paper proposes a new cross-modal pre-training framework, called Align and Prompt (ALPRO), with two novel techniques - video-text contrastive loss and prompting entity modeling - to learn better aligned video-language representations from sparse inputs. It hypothesizes these techniques can improve performance on downstream video-text tasks compared to prior methods.


## What is the main contribution of this paper?

The main contributions of this paper are:1. It proposes Align and Prompt (ALPRO), a new video-language pre-training framework that operates on sparsely-sampled video frames. 2. It introduces a video-text contrastive loss to better align instance-level unimodal video and text representations, thus easing the modeling of cross-modal interactions.3. It proposes a novel visually-grounded pre-training task called prompting entity modeling, which enables the model to capture fine-grained region-entity alignment without needing object detectors.4. It demonstrates the effectiveness of ALPRO on both video-text retrieval and video question answering tasks, significantly outperforming previous state-of-the-art methods. For example, it achieves 3.0% and 5.4% absolute improvement in recall on text-video retrieval on the MSRVTT dataset.In summary, the key innovation is using a prompt-based method to generate soft entity pseudo-labels from web videos and text, which provides supervision to align visual regions and text entities. This allows learning fine-grained visual reasoning without expensive object detectors. Experiments show superior performance on retrieval and QA compared to prior arts.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper proposes a new video-language pre-training method called Align and Prompt (ALPRO) that learns to align unimodal video and text features using contrastive learning, and generates soft entity labels via prompting to enable fine-grained region-entity alignment, achieving state-of-the-art performance on text-video retrieval and video question answering tasks.


## How does this paper compare to other research in the same field?

Here are some key comparisons of this paper to other research in video-language pre-training:- Most prior works use densely extracted video features, while this paper follows the recent trend of using sparsely sampled raw frames. This allows for end-to-end finetuning of the visual backbone. - Many previous methods rely solely on cross-modal transformers or dot-product to model video-text interactions. This paper argues these approaches fail to address the misalignment between the modalities. They propose a video-text contrastive loss to better align the features.- For visually-grounded pre-training, methods typically use masked language modeling or object detectors. This paper argues detectors have limited vocabularies and introduces a novel prompting-based approach to generate soft pseudo-labels for a broader range of visual entities.- This method achieves state-of-the-art results on text-video retrieval and video QA using an order of magnitude fewer video-text pairs for pre-training compared to prior arts.- The technical contributions focus on aligning video-text features for retrieval and capturing fine-grained entity information for video QA. The improvements on both tasks demonstrate the versatility of the approach.- Compared to concurrent work like ClipBERT and FiT, this method better models temporal information by pre-training on video rather than static image pairs. The prompting approach is also novel.In summary, this paper makes several notable contributions in bridging the misalignment between modalities and learning from web data without human annotations. The results validate the effectiveness of their technical improvements for video-language representation learning.
