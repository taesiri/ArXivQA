# [Align and Prompt: Video-and-Language Pre-training with Entity Prompts](https://arxiv.org/abs/2112.09583)

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper addresses is how to learn effective cross-modal video-language representations from sparse video frames and text. The key hypotheses are:1. Learning instance-level video-text alignment with a contrastive loss can help align the features from individual modalities and ease subsequent cross-modal modeling. 2. Learning fine-grained region-entity alignment between visual regions and textual entities, via the proposed prompting entity modeling task, can improve the model's capability in capturing and reasoning about local visual information.3. Operating on sparsely sampled frames rather than dense feature extraction can enable end-to-end learning while being efficient.To summarize, the paper proposes a new cross-modal pre-training framework, called Align and Prompt (ALPRO), with two novel techniques - video-text contrastive loss and prompting entity modeling - to learn better aligned video-language representations from sparse inputs. It hypothesizes these techniques can improve performance on downstream video-text tasks compared to prior methods.


## What is the main contribution of this paper?

The main contributions of this paper are:1. It proposes Align and Prompt (ALPRO), a new video-language pre-training framework that operates on sparsely-sampled video frames. 2. It introduces a video-text contrastive loss to better align instance-level unimodal video and text representations, thus easing the modeling of cross-modal interactions.3. It proposes a novel visually-grounded pre-training task called prompting entity modeling, which enables the model to capture fine-grained region-entity alignment without needing object detectors.4. It demonstrates the effectiveness of ALPRO on both video-text retrieval and video question answering tasks, significantly outperforming previous state-of-the-art methods. For example, it achieves 3.0% and 5.4% absolute improvement in recall on text-video retrieval on the MSRVTT dataset.In summary, the key innovation is using a prompt-based method to generate soft entity pseudo-labels from web videos and text, which provides supervision to align visual regions and text entities. This allows learning fine-grained visual reasoning without expensive object detectors. Experiments show superior performance on retrieval and QA compared to prior arts.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper proposes a new video-language pre-training method called Align and Prompt (ALPRO) that learns to align unimodal video and text features using contrastive learning, and generates soft entity labels via prompting to enable fine-grained region-entity alignment, achieving state-of-the-art performance on text-video retrieval and video question answering tasks.
