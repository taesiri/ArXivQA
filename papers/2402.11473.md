# [Poisoned Forgery Face: Towards Backdoor Attacks on Face Forgery   Detection](https://arxiv.org/abs/2402.11473)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
This paper studies the potential vulnerability of face forgery detection methods to backdoor attacks during training. Face forgery detection aims to distinguish real faces from artificially synthesized fake faces. As these methods are being deployed, security concerns have arisen regarding the possibility of attackers manipulating the training process to inject backdoors into models. Specifically, by poisoning a fraction of the training data and incorporating specific trigger patterns during inference, attackers could mislead models to classify forged faces as real. However, implementing such attacks on face forgery detection presents unique challenges compared to standard image classification:

1) Backdoor label conflict: Many detection methods like SBI synthesize fake faces by transforming real ones during training. Existing attack triggers are not sensitive enough to image transformations, causing conflict between labels for real and fake faces associated with the trigger. 

2) Trigger stealthiness: Users are highly sensitive to artifacts on facial images. Prior attacks either lack stealthiness or compromise effectiveness when pursuing imperceptibility.

Proposed Solution: 
To address these challenges, the paper proposes "Poisoned Forgery Faces", a clean-label backdoor attack framework on face forgery detection with two main components:

1) Translation-sensitive trigger generator: It creates perturbation patterns by maximizing discrepancies between triggers on real and blended fake faces through analytical convolution operations. This helps resolve label conflict.

2) Landmark-based relative embedding: It confines low-magnitude perturbations to facial landmark regions using pixel-wise ratios for higher stealthiness.

Main Contributions:

- Comprehensively studies and reveals vulnerability of face forgery detection to backdoor threats during training.

- Proposes a novel clean-label attack framework "Poisoned Forgery Faces" to effectively attack state-of-the-art face forgery detectors.

- Achieves superior attack success rate (+16.39% BD-AUC) and stealthiness (-12.65% Lâˆž) over existing backdoor methods.

- Demonstrates promising resistance against backdoor defenses.

The paper underscores overlooked security risks in face forgery detection and the need for further efforts to enhance robustness against such threats.
