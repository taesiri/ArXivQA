# TouchStone: Evaluating Vision-Language Models by Language Models

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper aims to address is: How can we develop an automated and comprehensive evaluation methodology to assess the capabilities of large vision-language models (LVLMs) in open-ended multimodal dialogues? The key hypotheses are:1) By constructing a diverse visual dialogue dataset covering various abilities from recognition to comprehension and creation, and integrating detailed image annotations, we can transform multimodal inputs into a form understandable by language models. This will allow employing advanced LMs to directly evaluate dialogue quality without manual intervention.2) Powerful language models like GPT-4, when provided with fine-grained textual descriptions of images along with dialogues, can effectively assess the quality of LVLMs' responses by leveraging their strong language generation and understanding capabilities. Their scoring will align well with human judgment.In summary, the core research focus is on developing TouchStone, an automated evaluation framework that leverages detailed annotations and powerful LMs as judges to comprehensively assess LVLMs' skills in open-domain multimodal dialogues. The key hypothesis is that this methodology can enable efficient and accurate evaluation of LVLM capabilities without requiring human ratings.


## What is the main contribution of this paper?

The main contribution of this paper is proposing an evaluation method called TouchStone that uses powerful language models as judges to comprehensively evaluate the abilities of vision-language models (LVLMs). The key points are:- They construct a diverse visual dialogue dataset called TouchStone that covers 5 major ability categories and 27 subtasks, going beyond just recognition and comprehension to also test literary creation skills. - They transform multimodal inputs into a textual form understandable by language models using detailed image annotations and descriptions. This allows advanced LMs to directly assess dialogue quality without human intervention.- They show that powerful LMs like GPT-4 can effectively score LVLMs' response quality by leveraging their language capabilities alone and aligning with human preferences. - Their method provides a valuable automated way to evaluate and advance LVLMs without requiring extensive human evaluation. The results also reveal current LVLMs still have much room for improvement.In summary, the main contribution is proposing TouchStone, a comprehensive evaluation approach for LVLMs utilizing advanced LMs as judges and detailed image annotations to enable automated scoring of multimodal dialogue quality and model abilities.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper proposes an evaluation method called TouchStone that uses strong language models as judges to comprehensively evaluate the conversational and storytelling abilities of vision-language models, validated on a new visual dialogue dataset spanning recognition, comprehension, and creation tasks.
