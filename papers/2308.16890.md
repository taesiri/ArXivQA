# TouchStone: Evaluating Vision-Language Models by Language Models

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper aims to address is: How can we develop an automated and comprehensive evaluation methodology to assess the capabilities of large vision-language models (LVLMs) in open-ended multimodal dialogues? The key hypotheses are:1) By constructing a diverse visual dialogue dataset covering various abilities from recognition to comprehension and creation, and integrating detailed image annotations, we can transform multimodal inputs into a form understandable by language models. This will allow employing advanced LMs to directly evaluate dialogue quality without manual intervention.2) Powerful language models like GPT-4, when provided with fine-grained textual descriptions of images along with dialogues, can effectively assess the quality of LVLMs' responses by leveraging their strong language generation and understanding capabilities. Their scoring will align well with human judgment.In summary, the core research focus is on developing TouchStone, an automated evaluation framework that leverages detailed annotations and powerful LMs as judges to comprehensively assess LVLMs' skills in open-domain multimodal dialogues. The key hypothesis is that this methodology can enable efficient and accurate evaluation of LVLM capabilities without requiring human ratings.


## What is the main contribution of this paper?

The main contribution of this paper is proposing an evaluation method called TouchStone that uses powerful language models as judges to comprehensively evaluate the abilities of vision-language models (LVLMs). The key points are:- They construct a diverse visual dialogue dataset called TouchStone that covers 5 major ability categories and 27 subtasks, going beyond just recognition and comprehension to also test literary creation skills. - They transform multimodal inputs into a textual form understandable by language models using detailed image annotations and descriptions. This allows advanced LMs to directly assess dialogue quality without human intervention.- They show that powerful LMs like GPT-4 can effectively score LVLMs' response quality by leveraging their language capabilities alone and aligning with human preferences. - Their method provides a valuable automated way to evaluate and advance LVLMs without requiring extensive human evaluation. The results also reveal current LVLMs still have much room for improvement.In summary, the main contribution is proposing TouchStone, a comprehensive evaluation approach for LVLMs utilizing advanced LMs as judges and detailed image annotations to enable automated scoring of multimodal dialogue quality and model abilities.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper proposes an evaluation method called TouchStone that uses strong language models as judges to comprehensively evaluate the conversational and storytelling abilities of vision-language models, validated on a new visual dialogue dataset spanning recognition, comprehension, and creation tasks.


## How does this paper compare to other research in the same field?

Here is a summary of how this paper compares to other related research:- The key contribution of this paper is proposing an automated evaluation method for large vision-language models (LVLMs) using powerful language models like GPT-4 as judges. This is a novel approach compared to prior work. - Existing LVLM evaluations have focused on human evaluations on a limited set of questions or performance on individual tasks like VQA and image captioning. This paper argues these have limitations in comprehensively assessing abilities and aligning with human preferences.- Other recent work has proposed evaluation methods like binary statement judging by LMs (MME) or choosing correct answers (MMBench). However, this paper argues these may not capture the complexity of open-ended dialog. - A unique aspect of this method is using detailed human annotations to represent visual information in a textual form understandable to LMs. This allows leveraging advanced LMs to evaluate the quality and hallucination of LVLMs' dialogs without human involvement.- The paper introduces a new visual dialogue dataset TouchStone covering 5 ability categories and 27 subtasks, beyond just recognition and comprehension. This is more comprehensive than existing VQA and captioning datasets.- Validation shows powerful LMs like GPT-4 can effectively score dialog quality using just text, aligning with human judgments. This demonstrates the feasibility of using annotations to represent visuals for LM evaluation.Overall, the automated evaluation approach using LMs as judges on a diverse dialogue dataset is novel compared to prior LVLM evaluations focused on limited tasks or human ratings. Converting visual data to text annotations is a unique technique enabling applying advanced LMs as judges on multimodal dialogs.
