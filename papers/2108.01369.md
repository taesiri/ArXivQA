# How to Evaluate Your Dialogue Models: A Review of Approaches

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be: How to effectively evaluate dialogue systems through different evaluation paradigms and metrics?The key points related to this question include:- The paper provides a taxonomy and thorough review of current dialogue evaluation methods, dividing them into automatic evaluation, human evaluation, and user simulator-based evaluation. - For automatic evaluation, the paper reviews metrics based on word overlap, language models, embeddings, neural networks, and comprehensively considered factors. - For human evaluation, the paper summarizes different subjective metrics used across studies.- For user simulator evaluation, the paper distinguishes between intent-level and sentence-level simulators.- The paper also discusses evaluation benchmarks and open issues related to reference-free, interactively dynamic, and fluent natural language evaluation.- The overall goal seems to be reviewing the landscape of dialogue evaluation methods and discussing their advantages, limitations, and open challenges. The paper aims to provide a systematic organization and analysis of existing techniques to facilitate advancements in this area.In summary, the central research question seems to focus on surveying and taxonomizing dialogue evaluation approaches to provide insights into this critical but understudied problem. The review identifies gaps and suggests future directions to enable more effective evaluation paradigms for dialogue systems.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contribution seems to be providing a thorough review and taxonomy of existing evaluation methods for dialogue systems. Specifically:- The paper categorizes evaluation methods into three main classes: automatic evaluation, human-involved evaluation, and user simulator based evaluation. - It provides an overview of the main features and evaluation metrics for each class, including word overlap metrics, embedding based metrics, neural network based metrics, task completion metrics, etc.- The paper discusses the existence of benchmarks and testbeds that can be used to evaluate dialogue techniques. - It highlights some open issues and challenges for dialogue evaluation like the need for reference-free and interactive dynamic evaluation methods.In summary, the key contribution appears to be the comprehensive taxonomy and review of dialogue evaluation methods, summarizing the state-of-the-art and identifying areas for future research. The paper helps provide a structured understanding of this complex and evolving field.
