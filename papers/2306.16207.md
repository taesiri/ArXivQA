# Inferring the Goals of Communicating Agents from Actions and   Instructions

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the key research question addressed in this paper is how to model the ability of humans to infer the goals and intentions of other agents from observing their actions and communication signals. The central hypothesis seems to be that humans integrate information from both verbal communication (e.g. instructions) and physical actions in order to infer the goals and plans of other agents, by assuming the agents are approximately rational. Specifically, the authors hypothesize that:1. Humans understand actions in terms of inferred goals and plans, based on the principle of rational action and Bayesian theory of mind. 2. Humans interpret communication pragmatically based on inferred speaker intentions, as formalized by rational speech act theory.3. By combining these two forms of Bayesian inference, humans are able to perform joint inference over goals from both physical actions and linguistic instructions in cooperative multi-agent settings.4. This combined model of goal inference from actions and instructions can explain human judgments in observing cooperative human-robot teams.To test these hypotheses, the authors develop a computational model for joint goal inference that combines Bayesian inverse planning over actions with a neural language model for modeling pragmatics. They then compare the model's inferences to human judgments.In summary, the key research question is how humans integrate physical and communicative signals to infer the goals of cooperating agents, with the hypothesis being that a combined model of inverse planning and pragmatic language understanding explains human social inferences.


## What is the main contribution of this paper?

Based on the abstract, the main contribution of this paper seems to be introducing a model for inferring the goals of communicating agents from their actions and instructions. Specifically, the model involves:- Representing a cooperative human-robot team as a single "group agent" with a shared goal, avoiding the need for complex recursive reasoning about individual intents and beliefs.- Using probabilistic programming to modularly specify the group agent in terms of goal priors, joint planning, and a neural utterance model. - Leveraging large language models like GPT-3 as flexible utterance likelihoods within the probabilistic program.- Performing Bayesian inverse planning to infer the posterior distribution over goals from observed actions and instructions of the team.The key novelty is combining Bayesian theory of mind for actions with rational speech acts for modeling instructions in a multi-modal, multi-agent setting. This allows the model to leverage both action and language evidence for cooperative goal inference.Experiments found the model inferences correlate highly with human judgments and that instructions enable faster, less uncertain goal inference compared to using actions alone. This demonstrates the model's viability for explaining human cooperative reasoning and for building communicative AI assistants.


## How does this paper compare to other research in the same field?

Here are a few thoughts on how this paper compares to other related work:- This paper presents a model for inferring agents' goals from both their actions and natural language instructions. This builds on prior work in Bayesian theory of mind that focuses on goal inference from actions only. The addition of modeling language instructions is novel and allows for more rapid and confident goal inference. - The model utilizes recent advances in integrating neural language models into probabilistic programs. This follows similar trends in other recent work that leverages large language models as flexible components within structured Bayesian models. The modular architecture allows language models to serve as utterance likelihoods.- For goal inference, the paper models the communicating agents as a joint "group agent" that plans and communicates rationally. This differs from other approaches that try to model the mental states of individuals and reason recursively about beliefs. The group agent approximation seems effective while being simpler.- The model is evaluated by comparing its goal inferences to human judgments in a simulated gridworld environment. Using human experiments to validate computational models of theory of mind is common practice. The high correlation found with humans is on par with other good models in this field.- Compared to related work in multimodal goal and reward inference, this paper presents a practical instantiation using probabilistic programming and real-time planning. The model could likely be extended to incorporate other modalities like demonstrations as well.Overall, the paper makes nice contributions in integrating language with Bayesian theory of mind and leveraging recent techniques like probabilistic programming and language models. The human experiments provide solid validation of the model. It builds well on existing work while innovating in certain areas like pragmatic language modeling. The discussion also outlines relevant limitations and future improvements.


## What future research directions do the authors suggest?

Based on my reading of the paper, here are some of the main future research directions suggested by the authors:- Develop more sophisticated models of bounded rationality and suboptimal behavior. The current model assumes agents follow an approximately optimal Boltzmann policy, but does not account for other systematic deviations from perfect rationality that are often exhibited by humans. The authors suggest enhancing the model's robustness to these types of imperfect behaviors.- Improve the heuristic model of pragmatic communication. Currently, salient actions are manually defined to approximate pragmatic utterances. The authors propose more principled methods for modeling communicative pragmatics, such as identifying actions that are most unique to the intended plan. - Account for different team preferences and structures when modeling joint behavior. The current model does not capture potential biases in how humans divide tasks between team members. Incorporating such preferences could improve fidelity.- Address the challenges of modeling boundedly rational speech acts. While the current model assumes instructions perfectly communicate the optimal plan, real humans may fail to mention certain actions or mention suboptimal actions. Capturing these imperfections could enhance accuracy.- Develop more complete models of rational cooperative communication. There are many open theoretical and technical issues in defining and implementing optimally communicative behavior. Continued research is needed to achieve truly human-like communicative cooperation.In summary, the main suggested directions are: improving robustness to bounded rationality, enhancing the pragmatic communication model, accounting for team preferences, modeling imperfect speech acts, and working towards more complete models of cooperative communication.


## Summarize the paper in one paragraph.

The paper introduces a model for inferring the goals of communicating agents from their actions and instructions. The model considers a team of agents, including a human principal who can communicate natural language instructions, and a robot assistant. Their shared goal is for the human to pick up a target colored gem in a gridworld environment. A third-person observer sees their actions and instructions, and performs Bayesian inference to determine the posterior distribution over possible goals. The key components of the model are:- A joint planner that generates policies to achieve possible goals - An utterance model based on GPT-3 that generates instructions conditioned on extracted salient actions from the policy- Bayesian inverse planning to infer the goal posterior from observed actions and instructionsExperiments in a multi-agent gridworld environment find that the model's inferences closely correlate with human judgments. Instructions enable faster goal inference compared to using actions alone. This demonstrates the importance of modeling language for understanding goals in human-AI cooperation. Overall, the model provides a useful computational framework for goal inference in communicative multi-agent scenarios.


## Summarize the paper in two paragraphs.

Here is a two paragraph summary of the key ideas in the paper:The paper introduces a model for inferring the goals of communicating agents from their actions and instructions. It proposes modeling a cooperative human-robot team as a single "group agent" with a shared goal. The group agent jointly plans actions to achieve its goal, and the human principal communicates part of this plan to the robot assistant via natural language instructions. To perform inference, the authors take a Bayesian approach - computing the posterior distribution over goals using the joint plan as a likelihood function for actions, and a neural language model as a likelihood function for instructions. The model is evaluated in a multi-agent gridworld environment where agents must pick up keys, unlock doors, and retrieve colored gems. Experiments find that the model's goal inferences closely correlate with human judgments. Providing instructions leads to faster, less uncertain goal inference compared to observing actions alone. This highlights the importance of verbal communication in cooperative settings, and suggests the proposed model offers a plausible explanation for how humans infer shared goals from both actions and language.


## Summarize the main method used in the paper in one paragraph.

The paper presents a model for inferring the goals of communicating agents from their actions and instructions. The key method is Bayesian inverse planning, where a third person observer infers the most likely goal of a human-robot team that best explains their observed actions and communicative utterances. Specifically, the team is modeled as a single group agent with a shared goal. Given a goal, the group agent plans jointly optimal actions and the human also selects an instructional utterance to communicate part of the plan. This joint plan, actions, and utterance are related through a graphical model. For inference, the observer performs Bayesian updating to compute the posterior distribution over goals given the evidence of actions and instructions. The utterance model uses GPT-3 as a flexible neural likelihood function.Experiments in a multi-agent gridworld environment show that this approach closely matches human inferences and that modeling instructions alongside actions leads to faster inference and lower uncertainty compared to using actions alone. Overall, the work demonstrates how Bayesian inverse planning integrated with neural language models can explain goal inference in communicating teams.
