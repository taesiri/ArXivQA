# Inferring the Goals of Communicating Agents from Actions and   Instructions

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the key research question addressed in this paper is how to model the ability of humans to infer the goals and intentions of other agents from observing their actions and communication signals. The central hypothesis seems to be that humans integrate information from both verbal communication (e.g. instructions) and physical actions in order to infer the goals and plans of other agents, by assuming the agents are approximately rational. Specifically, the authors hypothesize that:1. Humans understand actions in terms of inferred goals and plans, based on the principle of rational action and Bayesian theory of mind. 2. Humans interpret communication pragmatically based on inferred speaker intentions, as formalized by rational speech act theory.3. By combining these two forms of Bayesian inference, humans are able to perform joint inference over goals from both physical actions and linguistic instructions in cooperative multi-agent settings.4. This combined model of goal inference from actions and instructions can explain human judgments in observing cooperative human-robot teams.To test these hypotheses, the authors develop a computational model for joint goal inference that combines Bayesian inverse planning over actions with a neural language model for modeling pragmatics. They then compare the model's inferences to human judgments.In summary, the key research question is how humans integrate physical and communicative signals to infer the goals of cooperating agents, with the hypothesis being that a combined model of inverse planning and pragmatic language understanding explains human social inferences.


## What is the main contribution of this paper?

Based on the abstract, the main contribution of this paper seems to be introducing a model for inferring the goals of communicating agents from their actions and instructions. Specifically, the model involves:- Representing a cooperative human-robot team as a single "group agent" with a shared goal, avoiding the need for complex recursive reasoning about individual intents and beliefs.- Using probabilistic programming to modularly specify the group agent in terms of goal priors, joint planning, and a neural utterance model. - Leveraging large language models like GPT-3 as flexible utterance likelihoods within the probabilistic program.- Performing Bayesian inverse planning to infer the posterior distribution over goals from observed actions and instructions of the team.The key novelty is combining Bayesian theory of mind for actions with rational speech acts for modeling instructions in a multi-modal, multi-agent setting. This allows the model to leverage both action and language evidence for cooperative goal inference.Experiments found the model inferences correlate highly with human judgments and that instructions enable faster, less uncertain goal inference compared to using actions alone. This demonstrates the model's viability for explaining human cooperative reasoning and for building communicative AI assistants.


## How does this paper compare to other research in the same field?

Here are a few thoughts on how this paper compares to other related work:- This paper presents a model for inferring agents' goals from both their actions and natural language instructions. This builds on prior work in Bayesian theory of mind that focuses on goal inference from actions only. The addition of modeling language instructions is novel and allows for more rapid and confident goal inference. - The model utilizes recent advances in integrating neural language models into probabilistic programs. This follows similar trends in other recent work that leverages large language models as flexible components within structured Bayesian models. The modular architecture allows language models to serve as utterance likelihoods.- For goal inference, the paper models the communicating agents as a joint "group agent" that plans and communicates rationally. This differs from other approaches that try to model the mental states of individuals and reason recursively about beliefs. The group agent approximation seems effective while being simpler.- The model is evaluated by comparing its goal inferences to human judgments in a simulated gridworld environment. Using human experiments to validate computational models of theory of mind is common practice. The high correlation found with humans is on par with other good models in this field.- Compared to related work in multimodal goal and reward inference, this paper presents a practical instantiation using probabilistic programming and real-time planning. The model could likely be extended to incorporate other modalities like demonstrations as well.Overall, the paper makes nice contributions in integrating language with Bayesian theory of mind and leveraging recent techniques like probabilistic programming and language models. The human experiments provide solid validation of the model. It builds well on existing work while innovating in certain areas like pragmatic language modeling. The discussion also outlines relevant limitations and future improvements.
