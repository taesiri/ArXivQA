# [Scale-Invariant Gradient Aggregation for Constrained Multi-Objective   Reinforcement Learning](https://arxiv.org/abs/2403.00282)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Multi-objective reinforcement learning (MORL) aims to find a set of Pareto optimal policies that balance multiple objectives. However, real-world applications require satisfying safety constraints, which is not addressed by most MORL methods.  
- Existing constrained MORL (CMORL) methods have issues: 1) sensitivity to objective scales, 2) instability from using Lagrange multipliers to handle constraints.

Proposed Solution:
- A CMORL method called CoMOGA that transforms objectives into constraints to avoid concurrent optimization of policy and multipliers.  
- Transforms objectives $J_{R_i}$ into constraints using only gradient direction $g_i$, making it scale invariant. Constraint thresholds are set proportional to preference $\omega_i$.
- Obtains policy gradient by solving constrained problem with linear approximation. Includes threshold margin and scaling to ensure constraint satisfaction.
- Combines gradients for sampled preferences into universal policy using KL constraint.

Theoretical Contributions:
- Proves policy updated by CoMOGA improves objectives as per preference while satisfying constraints (Theorems 1&2).
- Shows CoMOGA converges to local Pareto optimal policy that is stationary for a relaxed constrained problem (Theorem 3).

Experiments:
- Tested ontasks from Safety Gym, legged locomotion, and MO Gym environments having 2+ objectives and 0-3 constraints.
- Outperforms baselines on hypervolume and sparsity metrics across tasks while satisfying constraints. 
- Confirms scale invariance versus linear scalarization through scaled reward experiments.

In summary, the paper proposes a novel CMORL algorithm that handles constraints in a stable manner without concurrent optimization. It is theoretically sound, demonstrates strong empirical performance and scale invariance.
