# [Can Machine Translation Bridge Multilingual Pretraining and   Cross-lingual Transfer Learning?](https://arxiv.org/abs/2403.16777)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem Statement:
- Multilingual pretrained language models (LMs) have shown surprising effectiveness on cross-lingual transfer tasks without explicit cross-lingual supervision during pretraining. 
- On the other hand, machine translation (MT) models explicitly align sentences between languages, suggesting MT objectives could further improve cross-lingual transfer abilities. 
- This paper investigates whether continued pretraining of LMs on an MT objective can enhance cross-lingual transfer performance.

Methods:
- Compare publicly available pretrained LMs (mBERT, XLM-R, mBART) and MT models (NLLB, mBART with continued MT pretraining) on several cross-lingual NLU datasets from XGLUE benchmark.
- Analyze representational similarity between languages and models using centered kernel alignment.
- Examine scaling effects of weight matrices via singular value decomposition to understand model internal computations.

Key Findings:
- Against expectations, continued pretraining on MT fails to improve cross-lingual performance over regular LMs. mBART generally outperforms its MT-continued versions.
- MT-continued models display higher representational separability between languages compared to mBART. This likely encourages translation but harms transferability.  
- MT training enhances scaling factors of weight matrices, spreading representations over larger spaces. This beneficial for MT but detrimental for transfer tasks.

Main Conclusions:
- Explicit cross-lingual alignment objectives like MT are ineffective for improving general cross-lingual representations, perhaps due to encouraging separability.
- Implications for future research on objectives better suited for cross-lingual transfer learning.

In summary, the paper clearly establishes that common intuition about MT objectives fostering cross-lingual abilities is misguided. The analyses into model representations and computations provide evidence about why this occurs.
