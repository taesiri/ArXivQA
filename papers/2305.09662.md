# [Make-An-Animation: Large-Scale Text-conditional 3D Human Motion   Generation](https://arxiv.org/abs/2305.09662)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is: How can we improve text-to-motion human pose generation models, especially for diverse, in-the-wild text prompts, by leveraging large-scale image datasets?The key hypotheses appear to be:1) Pre-training on a large-scale dataset of text-pseudo-pose pairs extracted from image-text datasets can help the model learn a better distribution of human poses and alignment with text descriptions. This will improve generalization to new prompts. 2) A U-Net architecture with temporal convolutions and attention can effectively extend a static pose generation diffusion model to motion generation in a stable way.3) By combining large-scale pre-training and a U-Net architecture, we can significantly improve text-to-motion generation, especially for diverse prompts, compared to prior state-of-the-art models.In summary, the main research question is how to leverage large-scale image datasets to improve text-to-motion generation through pre-training and a suitable model architecture. The key hypotheses are around the benefits of pre-training, using a U-Net architecture, and combining these to achieve state-of-the-art performance.


## What is the main contribution of this paper?

The main contributions of this paper are:- Presenting Make-An-Animation, a text-to-motion generation model that outperforms prior state-of-the-art models, especially on diverse, in-the-wild text prompts. - Showing for the first time how to leverage large-scale image datasets to learn in-the-wild human poses for generation. The authors collect a large-scale Text Pseudo-Pose (TPP) dataset from image-text datasets and show through ablations that pre-training on this dataset significantly improves generalization to new prompts while maintaining performance on standard motion capture test sets.- Introducing a U-Net architecture for human motion generation that naturally extends a static pose generation diffusion model to motion by adding temporal convolution and attention layers. This model is conditioned on text representations from a large pre-trained language model.In summary, the key contribution is developing a text-to-motion generation model, Make-An-Animation, that can generate high quality and diverse motions for in-the-wild prompts by pre-training on a large-scale pose dataset extracted from image-text data and using a U-Net architecture.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper introduces Make-An-Animation, a text-conditional human motion generation model trained on large-scale image-text data and motion capture data, which achieves state-of-the-art performance in generating diverse, realistic human motions aligned with text prompts.
