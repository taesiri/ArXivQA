# [AttentionStore: Cost-effective Attention Reuse across Multi-turn   Conversations in Large Language Model Serving](https://arxiv.org/abs/2403.19708)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem: 
Executing multi-turn conversations in current large language model (LLM) serving engines is highly inefficient due to the repetitive computation of key-value (KV) caches across multiple conversation turns. Specifically, LLM engines discard the KV cache when a conversation ends to free up GPU memory. When the conversation resumes in another turn, the entire KV cache needs to be recomputed based on the concatenated historical and new tokens, wasting valuable GPU computation resources. As conversations continue with more turns, the amount of repetitive computation significantly increases.

Proposed Solution:
The paper proposes AttentionStore, a new attention mechanism that enables the reuse of KV caches across multi-turn conversations instead of discarding them. It maintains a hierarchical KV caching system with cost-effective storage media like host memory and disks to store KV caches for all conversations. Upon the resumption of a conversation, the associated KV cache is loaded from the system and reused, eliminating repetitive computation.  

To address the challenges of building such a system, the paper presents:
1) Overlapped KV cache access schemes to hide loading and saving overheads.
2) Scheduler-aware KV cache fetching and eviction to optimize placement. 
3) Positional encoding decoupled KV cache truncation to handle context window overflow.

Main Contributions:
1) Proposes AttentionStore to reduce serving cost of multi-turn conversations by reusing KV caches.
2) Architects an efficient hierarchical KV caching system using slower yet larger storage. 
3) Develops systemic optimization techniques for KV cache access, placement and truncation.
4) Demonstrates significant gains on metrics like time-to-first-token, prefilling throughput and cost savings.

In summary, the paper enables the reuse of KV caches across conversation turns through an efficiently designed caching system, leading to substantial performance improvements and cost savings.
