# [Look-ups are not (yet) all you need for deep learning inference](https://arxiv.org/abs/2207.05808)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question appears to be: Can matrix multiplication in neural network inference be approximated with lookup tables in a way that achieves a favorable tradeoff between efficiency and accuracy?The authors build on previous work on approximating matrix multiplication with lookups, called MADDNESS, and propose a new method called ITLUMM that aims to improve upon MADDNESS specifically for the setting of neural network inference. The key ideas explored are:- Intelligently partitioning the matrix dimensions when creating the lookup tables - Optimizing the lookup tables in a model-aware way, taking into account the subsequent nonlinearity - Fine-tuning the full network after replacing layers with lookup approximationsThe paper analyzes the accuracy-efficiency tradeoff achieved by ITLUMM, showing it improves upon MADDNESS but overall accuracy remains substantially diminished compared to exact matrix multiplication. The central hypothesis appears to be that lookup-based approximation can work well enough to accelerate neural network inference, but the results suggest this goal has not yet been achieved.In summary, the key research question is whether lookup-based matrix multiplication approximation can effectively accelerate deep learning inference, and the paper explores a new proposed method in that direction, along with analysis that provides evidence that there is still room for improvement compared to the baseline of exact matrix multiplication.


## What is the main contribution of this paper?

The main contribution of this paper is proposing a new method called Inference Targeted LookUp-based Matrix Multiplication (ITLUMM) to accelerate neural network inference by approximating matrix multiplications with lookups. The key ideas and contributions are:- Using an intelligent subspace partitioning scheme to group correlated dimensions together when splitting the input vectors into subvectors for lookup. This is done by permuting the dimensions based on either OPQ or hierarchical clustering of the squared correlation matrix.- Directly optimizing the lookup tables rather than prototypes, taking into account the known weights and subsequent nonlinear activation function. This improves accuracy compared to prior work like MADDNESS. - Proposing a fine-tuning procedure to incrementally replace layers in a full neural network while retraining the later layers after each replacement. This aims to recover some of the lost accuracy from the approximations.- Analyzing the approach on image classification tasks using MNIST and CIFAR-10. The results show improvements over prior work MADDNESS in classifier layer experiments, but limited gains when accelerating full networks due to accuracy loss from the lookups.In summary, the main contribution is the ITLUMM method that builds on prior work like MADDNESS to better optimize lookups for deep learning inference specifically. The analysis provides insights into current limitations and future directions for speeding up neural nets with fast hashing techniques.
