# [Look-ups are not (yet) all you need for deep learning inference](https://arxiv.org/abs/2207.05808)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question appears to be: Can matrix multiplication in neural network inference be approximated with lookup tables in a way that achieves a favorable tradeoff between efficiency and accuracy?The authors build on previous work on approximating matrix multiplication with lookups, called MADDNESS, and propose a new method called ITLUMM that aims to improve upon MADDNESS specifically for the setting of neural network inference. The key ideas explored are:- Intelligently partitioning the matrix dimensions when creating the lookup tables - Optimizing the lookup tables in a model-aware way, taking into account the subsequent nonlinearity - Fine-tuning the full network after replacing layers with lookup approximationsThe paper analyzes the accuracy-efficiency tradeoff achieved by ITLUMM, showing it improves upon MADDNESS but overall accuracy remains substantially diminished compared to exact matrix multiplication. The central hypothesis appears to be that lookup-based approximation can work well enough to accelerate neural network inference, but the results suggest this goal has not yet been achieved.In summary, the key research question is whether lookup-based matrix multiplication approximation can effectively accelerate deep learning inference, and the paper explores a new proposed method in that direction, along with analysis that provides evidence that there is still room for improvement compared to the baseline of exact matrix multiplication.
