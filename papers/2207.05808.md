# [Look-ups are not (yet) all you need for deep learning inference](https://arxiv.org/abs/2207.05808)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question appears to be: Can matrix multiplication in neural network inference be approximated with lookup tables in a way that achieves a favorable tradeoff between efficiency and accuracy?The authors build on previous work on approximating matrix multiplication with lookups, called MADDNESS, and propose a new method called ITLUMM that aims to improve upon MADDNESS specifically for the setting of neural network inference. The key ideas explored are:- Intelligently partitioning the matrix dimensions when creating the lookup tables - Optimizing the lookup tables in a model-aware way, taking into account the subsequent nonlinearity - Fine-tuning the full network after replacing layers with lookup approximationsThe paper analyzes the accuracy-efficiency tradeoff achieved by ITLUMM, showing it improves upon MADDNESS but overall accuracy remains substantially diminished compared to exact matrix multiplication. The central hypothesis appears to be that lookup-based approximation can work well enough to accelerate neural network inference, but the results suggest this goal has not yet been achieved.In summary, the key research question is whether lookup-based matrix multiplication approximation can effectively accelerate deep learning inference, and the paper explores a new proposed method in that direction, along with analysis that provides evidence that there is still room for improvement compared to the baseline of exact matrix multiplication.


## What is the main contribution of this paper?

The main contribution of this paper is proposing a new method called Inference Targeted LookUp-based Matrix Multiplication (ITLUMM) to accelerate neural network inference by approximating matrix multiplications with lookups. The key ideas and contributions are:- Using an intelligent subspace partitioning scheme to group correlated dimensions together when splitting the input vectors into subvectors for lookup. This is done by permuting the dimensions based on either OPQ or hierarchical clustering of the squared correlation matrix.- Directly optimizing the lookup tables rather than prototypes, taking into account the known weights and subsequent nonlinear activation function. This improves accuracy compared to prior work like MADDNESS. - Proposing a fine-tuning procedure to incrementally replace layers in a full neural network while retraining the later layers after each replacement. This aims to recover some of the lost accuracy from the approximations.- Analyzing the approach on image classification tasks using MNIST and CIFAR-10. The results show improvements over prior work MADDNESS in classifier layer experiments, but limited gains when accelerating full networks due to accuracy loss from the lookups.In summary, the main contribution is the ITLUMM method that builds on prior work like MADDNESS to better optimize lookups for deep learning inference specifically. The analysis provides insights into current limitations and future directions for speeding up neural nets with fast hashing techniques.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper proposes improvements to approximate matrix multiplication for neural network inference by intelligently partitioning input dimensions, directly optimizing lookup tables, and fine-tuning full networks, but finds that overall classification accuracy still remains substantially lower than using exact matrix multiplication.


## How does this paper compare to other research in the same field?

Here are a few key ways this paper compares to other related work on accelerating neural network inference:- It builds directly on the MADDNESS method proposed in Blalock et al. 2021. The authors acknowledge this prior work and propose improvements targeting the deep learning inference setting specifically.- Compared to other approaches like pruning and quantization, this paper explores replacing dense matrix multiplies with lookups entirely. This is a more aggressive change to the fundamental computations.- The paper evaluates the approach on full neural network inference, going beyond just the classifier layer like some prior works. However, the accuracy results for full networks are quite poor.- The negative results in this paper contrast with some other recent works that show more promising accuracy-efficiency tradeoffs using related ideas. For example, MULTISOFT (Shan et al. 2022) achieves much better image classification accuracy with a similar lookup-based approach.- The analysis points out limitations of the current encoding function used for lookups as a key weakness. Improving the encoding function is noted as an important direction for future work, in line with some other recent papers.- Overall, this paper makes an incremental contribution in exploring lookup-based matrix multiplication specifically for deep learning inference. The limitations shown here will help guide and motivate continued research in this area. The results demonstrate that there is still work to be done before such methods are practical.In summary, this paper provides useful negative results to the growing body of work on fast approximate matrix multiplication for neural network inference. It is not as breakthrough as some other contemporary papers, but helps characterize the limitations of current approaches based on lookup tables and product quantization.


## What future research directions do the authors suggest?

Based on the results and discussion in the paper, the authors suggest two main directions for future research on using fast nonlinear hashing methods like ITLUMM to accelerate neural network inference:1. Improve the encoding function used for the hashing/lookup. The paper notes that the accuracy bottleneck appears to be the MADDNESS hash function used to encode the input activation vectors. They suggest future work could exploit knowledge of the weight matrix when learning the hash function parameters, or use a differentiable hash function that could improve end-to-end accuracy and enable faster conversion of full networks. Overall, the encoding function seems to be a key area for improvement.2. Explore modifications and alternatives to ITLUMM. While ITLUMM improves on prior work, the accuracy-efficiency tradeoff is still not sufficient for practical use on full networks. The authors propose that future efforts continue to explore and refine methods for approximating inner products with fast nonlinear hashing. This could include modifications to ITLUMM's approach, or completely different approaches that can better balance accuracy and speed.In summary, the main future directions are:- Improve the encoding function for the hashing lookup, which is currently a bottleneck. Exploit knowledge of weights, use differentiable versions, etc. - Explore modifications and alternatives to ITLUMM to find better accuracy-efficiency tradeoffs. The lookup idea has promise but needs refinement.


## Summarize the paper in one paragraph.

Here is a one paragraph summary of the paper:This paper proposes a new method called Inference Targeted LookUp-based Matrix Multiplication (ITLUMM) to accelerate neural network inference by approximating matrix multiplications with lookup tables. The method builds on prior work called MADDNESS that also replaced multiplications with lookups. The improvements include intelligently partitioning the input dimensions for better encoding, directly optimizing the lookup tables using both the known model weights and subsequent nonlinearities, and fine-tuning the full network after replacing layers. Experiments showed that ITLUMM improves accuracy over MADDNESS when applied just to classifier layers. However, when applied to full networks for image classification tasks like MNIST and CIFAR-10, the accuracy drops substantially compared to exact matrix multiplications, suggesting the encoding function remains a bottleneck. The analysis provides guidance for future work on replacing inner products with fast nonlinear hashing while maintaining accuracy.
