# [A Gaze-grounded Visual Question Answering Dataset for Clarifying   Ambiguous Japanese Questions](https://arxiv.org/abs/2403.17545)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Situated conversations between humans and machines often contain ambiguities in questions due to reliance on visual context or omission of information. This is exacerbated in some languages like Japanese which often omit subjects/objects.
- Such ambiguities can be clarified using contextual cues like joint attention or gaze information. 

Proposed Solution:
- The paper proposes a Gaze-grounded VQA dataset (GazeVQA) that contains ambiguous Japanese questions and corresponding gaze information to help clarify the questions. 
- The dataset has 17,276 QA pairs over 10,760 images, with 1680 images in the test set and 10 answers per question.
- The paper also proposes a model that integrates estimated regions of interest (RoIs) representing gaze targets into an image encoder using adapters to improve VQA accuracy.

Main Contributions:
- Construction of the GazeVQA dataset containing ambiguous Japanese questions that can be clarified using gaze information.
- Proposal of a model architecture that integrates gaze information in the form of RoIs into an image encoder via adapters to improve VQA accuracy on ambiguous questions.
- Experiments showing the proposed model outperforms baselines in some cases, especially when only the adapters are fine-tuned.
- Analysis providing insights into the role of gaze information in clarifying ambiguities.

The key idea is that gaze information and estimated RoIs help clarify ambiguities in situated conversations to improve visual understanding. The GazeVQA dataset and proposed model aim to demonstrate this for the VQA task.
