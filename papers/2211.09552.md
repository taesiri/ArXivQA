# [UniFormerV2: Spatiotemporal Learning by Arming Image ViTs with Video   UniFormer](https://arxiv.org/abs/2211.09552)

## What is the central research question or hypothesis that this paper addresses?

 Based on my review, the central research question this paper addresses is: How can we effectively and efficiently arm image-pretrained vision transformers (ViTs) with video modeling capability for action recognition? The key points are:- Image-pretrained ViTs have shown strong performance on image tasks, but it is challenging to adapt them for effective and efficient video modeling. - This paper proposes a new model called UniFormerV2 that equips ViTs with efficient video relation modeling blocks from UniFormer to enhance spatiotemporal representation learning.- Specifically, it inserts local temporal blocks to reduce redundancy and global query-based blocks to summarize tokens into a video representation. It also adopts multi-stage fusion to integrate multi-scale features.- Experiments show UniFormerV2 achieves new state-of-the-art results on major video benchmarks like Kinetics and Something-Something. It is the first model to reach 90% top-1 accuracy on Kinetics-400.- This demonstrates the effectiveness and efficiency of arming image-pretrained ViTs with UniFormer designs for video modeling and action recognition.In summary, the central hypothesis is equipping ViTs with efficient UniFormer-style blocks can lead to strong video recognition models that integrate the advantages of both image transformers and concise video architectures. The results validate this hypothesis and show the promise of this direction.
