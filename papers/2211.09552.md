# [UniFormerV2: Spatiotemporal Learning by Arming Image ViTs with Video   UniFormer](https://arxiv.org/abs/2211.09552)

## What is the central research question or hypothesis that this paper addresses?

 Based on my review, the central research question this paper addresses is: How can we effectively and efficiently arm image-pretrained vision transformers (ViTs) with video modeling capability for action recognition? 

The key points are:

- Image-pretrained ViTs have shown strong performance on image tasks, but it is challenging to adapt them for effective and efficient video modeling. 

- This paper proposes a new model called UniFormerV2 that equips ViTs with efficient video relation modeling blocks from UniFormer to enhance spatiotemporal representation learning.

- Specifically, it inserts local temporal blocks to reduce redundancy and global query-based blocks to summarize tokens into a video representation. It also adopts multi-stage fusion to integrate multi-scale features.

- Experiments show UniFormerV2 achieves new state-of-the-art results on major video benchmarks like Kinetics and Something-Something. It is the first model to reach 90% top-1 accuracy on Kinetics-400.

- This demonstrates the effectiveness and efficiency of arming image-pretrained ViTs with UniFormer designs for video modeling and action recognition.

In summary, the central hypothesis is equipping ViTs with efficient UniFormer-style blocks can lead to strong video recognition models that integrate the advantages of both image transformers and concise video architectures. The results validate this hypothesis and show the promise of this direction.


## What is the main contribution of this paper?

 Here are the main contributions of the paper:

- The paper proposes UniFormerV2, a new video model that integrates the advantages of both image Vision Transformers (ViTs) and the UniFormer video architecture. 

- It introduces efficient designs for arming image-pretrained ViTs with video modeling capacity, including local and global UniBlocks with new Multi-Head Relation Aggregators (MHRAs), as well as a multi-stage fusion block.

- The local UniBlock reduces temporal redundancy by inserting a local temporal MHRA before the ViT block. The global UniBlock summarizes the video into a single token using a cross-attention MHRA. 

- The multi-stage fusion adaptively integrates the global tokens from each stage to enhance the video representation.

- The model achieves state-of-the-art performance on 8 popular video benchmarks, including Kinetics, Moments in Time, Something-Something, ActivityNet and HACS.

- On Kinetics-400, UniFormerV2 is the first model to achieve over 90% top-1 accuracy. It also significantly outperforms prior work on the untrimmed ActivityNet and HACS datasets.

- The work introduces a compact Kinetics-710 benchmark for efficient video pretraining. Models pretrained on it require much less finetuning to achieve strong performance on Kinetics.

In summary, the key contribution is a new network design that effectively combines the powers of Vision Transformers and the UniFormer architecture for state-of-the-art video recognition. The model pushes accuracy boundaries while maintaining efficiency.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence TL;DR summary of the paper:

The paper proposes UniFormerV2, a video recognition model that effectively integrates the strengths of image-pretrained Vision Transformers and efficient video relation modeling from UniFormer, achieving state-of-the-art accuracy on major video benchmarks including 90.0% top-1 on Kinetics-400.
