# [A Novel Paradigm Boosting Translation Capabilities of Large Language   Models](https://arxiv.org/abs/2403.11430)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Prior work on improving translation capabilities of large language models (LLMs) through supervised fine-tuning (SFT) has shown limited effectiveness. Simply using more parallel bilingual data for SFT yields diminishing returns. 
- Question arises: Is extensive parallel bilingual data useless for SFT, or is it being misused?

Proposed Solution:
- A 3-stage training paradigm to boost translation capabilities of LLMs by enhancing cross-lingual alignment abilities during pre-training and using high-quality bilingual data for SFT.

Stage 1: Secondary Pre-Training using Monolingual Data
- Validates benefits of monolingual data augmentation, mainly to improve target language generation.

Stage 2: Continual Pre-Training with Interlinear Text  
- Uses sentence-aligned bilingual data in interlinear format for continual pre-training.
- Shows significant gains, especially for English-Other translation directions.  
- Efficient method requiring under 1B training data.

Stage 3: Source-Language Consistent Instruction for SFT
- Instructions consistent with source language give better SFT performance.

Main Contributions:
- Emphasizes cross-lingual alignment pre-training over reliance on extensive bilingual SFT data.
- Proposes efficient continual pre-training method with interlinear text.  
- Introduces source-language consistent instructions for improved SFT.
- Achieves state-of-the-art results with smaller 7B-13B models, outperforming prior works.
