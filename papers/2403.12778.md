# [ViTGaze: Gaze Following with Interaction Features in Vision Transformers](https://arxiv.org/abs/2403.12778)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper "ViTGaze: Gaze Following with Interaction Features in Vision Transformers":

Problem:
Gaze following aims to predict where a person is looking in an image, which is useful for human-computer interaction and neuroscience. Existing methods either use multi-modal inputs (e.g. depth, pose) in a two-stage pipeline, or complex decoders with object detections. Both have limitations - multi-modal methods depend on the quality of the first stage, while complex decoders increase model complexity. This paper explores designing a concise single-modal gaze following method using only RGB images.

Method: 
The key idea is that self-attention in Transformers can capture interactions between image patches, which can be transferred to model human-scene interactions for gaze following. Thus, the authors propose ViTGaze - a framework with three components:

1) Multi-level 4D Interaction Encoder: Extracts inter-patch interaction features from the self-attention maps of a pre-trained Vision Transformer at multiple layers and heads. This results in a 4D interaction representation.

2) 2D Spatial Guidance Module: Determines spatial importance weights for each image patch based on the person's head location, to guide aggregation of interaction features.

3) Prediction Heads: Use the aggregated features to predict a gaze heatmap and whether the gaze is inside or outside the image.

Overall ViTGaze focuses on encoding interactions, with a very simple decoder taking up <1% of parameters.

Contributions:
- Proposes a new single-RGB image framework for gaze following based entirely on a pre-trained Vision Transformer encoder and attention maps.

- Achieves state-of-the-art performance among single modal methods, and comparable performance to multi-modal techniques, while using 59% fewer parameters.

- Demonstrates the efficacy of modelling human-scene interactions via self-attention in Transformers for the task of gaze following.

In summary, the paper presents ViTGaze, a new high-performance and lightweight gaze following method using only RGB images, by transferring self-attention interactions in Vision Transformers to model human-scene interactions.
