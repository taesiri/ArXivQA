# [Towards Memory- and Time-Efficient Backpropagation for Training Spiking   Neural Networks](https://arxiv.org/abs/2302.14311)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question it addresses is:

How can we train spiking neural networks (SNNs) more efficiently in terms of memory usage and training time while maintaining high performance? 

The key hypotheses are:

1) The temporal components in gradient calculation during backpropagation through time contribute little to the final calculated gradients. So we can ignore these components to simplify and speed up computation.

2) By calculating gradients instantly at each time step without storing intermediate states, we can achieve constant memory usage regardless of the number of time steps.

3) Conducting backpropagation and updating weights at only a subset of randomly chosen time steps can further reduce training time without harming performance.

To summarize, the paper proposes more efficient techniques for training SNNs to achieve state-of-the-art accuracy while significantly lowering memory usage and training time compared to prior BPTT methods. The core ideas are approximating gradients by ignoring unimportant components and enabling online, instantaneous gradient calculations.
