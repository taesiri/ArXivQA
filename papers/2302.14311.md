# [Towards Memory- and Time-Efficient Backpropagation for Training Spiking   Neural Networks](https://arxiv.org/abs/2302.14311)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question it addresses is:

How can we train spiking neural networks (SNNs) more efficiently in terms of memory usage and training time while maintaining high performance? 

The key hypotheses are:

1) The temporal components in gradient calculation during backpropagation through time contribute little to the final calculated gradients. So we can ignore these components to simplify and speed up computation.

2) By calculating gradients instantly at each time step without storing intermediate states, we can achieve constant memory usage regardless of the number of time steps.

3) Conducting backpropagation and updating weights at only a subset of randomly chosen time steps can further reduce training time without harming performance.

To summarize, the paper proposes more efficient techniques for training SNNs to achieve state-of-the-art accuracy while significantly lowering memory usage and training time compared to prior BPTT methods. The core ideas are approximating gradients by ignoring unimportant components and enabling online, instantaneous gradient calculations.


## What is the main contribution of this paper?

 Based on my reading of the paper, the main contributions appear to be:

1. The paper proposes a new training method called Spatial Learning Through Time (SLTT) for spiking neural networks (SNNs). 

2. SLTT reduces the memory cost and training time compared to the commonly used backpropagation through time (BPTT) method. It does this by approximating the gradient calculation and enabling online training.

3. A variant called SLTT-K is proposed which further reduces training time by only conducting backpropagation at a subset of time steps. This reduces the time complexity from O(T) to O(K) where T is the total time steps and K<T.

4. Experiments on image classification datasets like CIFAR-10/100 and ImageNet demonstrate that SLTT achieves similar or better accuracy compared to BPTT while having significantly improved training efficiency. For example, on ImageNet the memory and training time are reduced by over 70% and 50% respectively.

5. The proposed methods obtain strong performance on neuromorphic datasets like DVS-Gesture and DVS-CIFAR10 as well. SLTT achieves state-of-the-art accuracy on DVS-Gesture while being efficient.

In summary, the key contribution is an efficient training method for SNNs that reduces memory and computational costs while maintaining accuracy. This could help enable adoption of SNNs for applications where training efficiency is critical.
