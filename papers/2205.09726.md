# RankGen: Improving Text Generation with Large Ranking Models

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be: How can we improve the quality and coherence of text generated from large language models?The authors present RankGen, a 1.2B parameter encoder model that scores model generations given an input prefix. RankGen is designed to improve issues with model-generated text like repetitiveness, incoherence, and hallucinations. The central hypothesis seems to be that training RankGen with contrastive learning, to score human continuations higher than negative samples like randomly chosen text from the same document or model-generated text, will allow it to pick out higher quality and more coherent continuations. The experiments then test whether incorporating RankGen as a scoring function into decoding algorithms like beam search improves the quality of generated text over baselines like nucleus sampling according to both automatic metrics and human evaluations. The results confirm their hypothesis, showing significant gains using RankGen across several model sizes, datasets, and metrics.In summary, the central research question is how to improve neural text generation quality and coherence using an auxiliary scoring model like RankGen trained with contrastive learning. The hypothesis is that RankGen will be able to distinguish better continuations, which is confirmed through experiments.


## What is the main contribution of this paper?

The main contribution of this paper seems to be presenting RankGen, a 1.2B parameter encoder model for scoring generated text continuations given an input text prefix. The key ideas are:- RankGen is trained with large-scale contrastive learning to map an input prefix close to the human-written continuation and far from two types of negative samples: 1) random text from the same document (fluency), and 2) model-generated text (relevance). - RankGen can flexibly rerank or beam search using any external pretrained language model, significantly improving generations over sampling methods like nucleus, top-k, typical on both automatic metrics (85.0 vs 77.3 MAUVE) and human evaluation (74.5% prefer RankGen).- RankGen also achieves SOTA on complex literary retrieval tasks like RELiC and ChapterBreak, demonstrating effectiveness for ranking.- Analysis suggests RankGen outputs have higher relevance to the prefix and improve continuity/coherence compared to baselines.In summary, the main contribution seems to be presenting RankGen, a large contrastively trained encoder model that can significantly enhance text generation quality and coherence when integrated into existing decoding methods. The scale and training approach seem crucial to its strong performance.
