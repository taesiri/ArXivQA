# RankGen: Improving Text Generation with Large Ranking Models

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be: How can we improve the quality and coherence of text generated from large language models?The authors present RankGen, a 1.2B parameter encoder model that scores model generations given an input prefix. RankGen is designed to improve issues with model-generated text like repetitiveness, incoherence, and hallucinations. The central hypothesis seems to be that training RankGen with contrastive learning, to score human continuations higher than negative samples like randomly chosen text from the same document or model-generated text, will allow it to pick out higher quality and more coherent continuations. The experiments then test whether incorporating RankGen as a scoring function into decoding algorithms like beam search improves the quality of generated text over baselines like nucleus sampling according to both automatic metrics and human evaluations. The results confirm their hypothesis, showing significant gains using RankGen across several model sizes, datasets, and metrics.In summary, the central research question is how to improve neural text generation quality and coherence using an auxiliary scoring model like RankGen trained with contrastive learning. The hypothesis is that RankGen will be able to distinguish better continuations, which is confirmed through experiments.


## What is the main contribution of this paper?

The main contribution of this paper seems to be presenting RankGen, a 1.2B parameter encoder model for scoring generated text continuations given an input text prefix. The key ideas are:- RankGen is trained with large-scale contrastive learning to map an input prefix close to the human-written continuation and far from two types of negative samples: 1) random text from the same document (fluency), and 2) model-generated text (relevance). - RankGen can flexibly rerank or beam search using any external pretrained language model, significantly improving generations over sampling methods like nucleus, top-k, typical on both automatic metrics (85.0 vs 77.3 MAUVE) and human evaluation (74.5% prefer RankGen).- RankGen also achieves SOTA on complex literary retrieval tasks like RELiC and ChapterBreak, demonstrating effectiveness for ranking.- Analysis suggests RankGen outputs have higher relevance to the prefix and improve continuity/coherence compared to baselines.In summary, the main contribution seems to be presenting RankGen, a large contrastively trained encoder model that can significantly enhance text generation quality and coherence when integrated into existing decoding methods. The scale and training approach seem crucial to its strong performance.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper introduces RankGen, a 1.2 billion parameter encoder model for scoring the relevance of text generations to input prompts, which significantly outperforms standard decoding methods like nucleus sampling when incorporated into beam search.


## How does this paper compare to other research in the same field?

Based on my review, here are some thoughts on how this paper compares to other research in the field:- The paper presents RankGen, a new neural model for scoring and re-ranking text continuations conditioned on a prefix. This approach of learning better scoring functions and using them during decoding is similar to some other recent work like the energy-based models from Deng et al. (2020) and Qin et al. (2022). However, RankGen is trained at a much larger scale and incorporates new strategies for negative sampling during contrastive learning.- Most prior work on decoding focuses on modifying the decoding algorithm used with a fixed pretrained LM. In contrast, RankGen can be used as a plug-in scoring function with any external LM and decoding method. This makes it more flexible and widely applicable. - Many recent papers have proposed contrastive learning objectives for text generation, like An et al. (2022) and Li et al. (2022). RankGen also uses contrastive learning, but between entire sequences rather than tokens. The use of discourse-based negatives is also unique.- For evaluation, RankGen considers various strong baselines like nucleus sampling, top-k sampling, and typical sampling. The scale of human evaluation between RankGen and nucleus is more extensive than most prior work. The analysis of generations using a taxonomy inspired by Dou et al. (2022) is also insightful.- The exploration of using RankGen for complex literary retrieval is novel and shows intriguing zero-shot capabilities. Most prior decoding work has focused just on text generation.Overall, I think RankGen makes several notable contributions over related decoding papers through the scale and flexibility of the approach, the discourse-based contrastive formulation, extensive comparisons to other methods, and applications to retrieval. The human preference results over nucleus sampling are a convincing demonstration of improvements over strong baselines.


## What future research directions do the authors suggest?

Based on my reading of the paper, here are some of the key future research directions suggested by the authors:1. Training or adapting RankGen models for other languages besides English. The current models are only trained on English text.2. Training larger RankGen models, such as T5-XXL size or bigger, with longer prefix/suffix lengths, to see if generation quality continues to improve with scale.3. Exploring the usefulness of RankGen for other generation tasks like dialog, summarization, or long-form QA.4. Using RankGen to re-rank much larger hypothesis sets generated with search algorithms like in the Xu et al. (2021) paper. 5. Incorporating RankGen more directly into generative modeling to avoid over-generation, such as via distillation or reinforcement learning.6. Using RankGen in retrieval augmented text generation systems.7. Further exploring RankGen's capability as a retriever, either zero-shot or with fine-tuning on datasets like BEIR. 8. Using RankGen for text generation evaluation, similar to metrics like CARP or CLIPScore.9. Applying RankGen to other domains beyond text, such as code, proteins, or mathematical proofs.In summary, the main future directions focus on scaling up RankGen, incorporating it more tightly into generative models to avoid over-generation, and exploring its usefulness across languages, tasks, and modalities. The authors also suggest several interesting applications in retrieval and generation evaluation.


## Summarize the paper in one paragraph.

Here is a one paragraph summary of the key points from the paper:The paper proposes RankGen, a 1.2 billion parameter encoder model for scoring the compatibility between a context prefix and candidate continuations for text generation. RankGen is trained with contrastive learning to map prefixes close to human-written continuations and away from two types of negatives: 1) irrelevant sequences from the same document (in-book negatives), and 2) generations from a pretrained language model (generative negatives). Experiments across four language models on Wikipedia and Project Gutenberg data show RankGen significantly outperforms decoding algorithms like nucleus sampling both automatically (85.0 vs 77.3 on MAUVE) and in human evaluations (74.5% preference). Analysis reveals RankGen outputs have higher relevance and continuity with the prefix. The model can be easily integrated into existing LMs as a re-ranker or beam search scoring function. RankGen also achieves state-of-the-art zero-shot performance on complex literary retrieval tasks like RELiC and ChapterBreak. The authors have open-sourced the code, data, model checkpoints and human annotations.
