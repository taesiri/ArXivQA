# [RankGen: Improving Text Generation with Large Ranking Models](https://arxiv.org/abs/2205.09726)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question seems to be: How can we improve the quality and coherence of text generated from large language models?The authors present RankGen, a 1.2B parameter encoder model that scores model generations given an input prefix. RankGen is designed to improve issues with model-generated text like repetitiveness, incoherence, and hallucinations. The central hypothesis seems to be that training RankGen with contrastive learning, to score human continuations higher than negative samples like randomly chosen text from the same document or model-generated text, will allow it to pick out higher quality and more coherent continuations. The experiments then test whether incorporating RankGen as a scoring function into decoding algorithms like beam search improves the quality of generated text over baselines like nucleus sampling according to both automatic metrics and human evaluations. The results confirm their hypothesis, showing significant gains using RankGen across several model sizes, datasets, and metrics.In summary, the central research question is how to improve neural text generation quality and coherence using an auxiliary scoring model like RankGen trained with contrastive learning. The hypothesis is that RankGen will be able to distinguish better continuations, which is confirmed through experiments.


## What is the main contribution of this paper?

 The main contribution of this paper seems to be presenting RankGen, a 1.2B parameter encoder model for scoring generated text continuations given an input text prefix. The key ideas are:- RankGen is trained with large-scale contrastive learning to map an input prefix close to the human-written continuation and far from two types of negative samples: 1) random text from the same document (fluency), and 2) model-generated text (relevance). - RankGen can flexibly rerank or beam search using any external pretrained language model, significantly improving generations over sampling methods like nucleus, top-k, typical on both automatic metrics (85.0 vs 77.3 MAUVE) and human evaluation (74.5% prefer RankGen).- RankGen also achieves SOTA on complex literary retrieval tasks like RELiC and ChapterBreak, demonstrating effectiveness for ranking.- Analysis suggests RankGen outputs have higher relevance to the prefix and improve continuity/coherence compared to baselines.In summary, the main contribution seems to be presenting RankGen, a large contrastively trained encoder model that can significantly enhance text generation quality and coherence when integrated into existing decoding methods. The scale and training approach seem crucial to its strong performance.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the paper:The paper introduces RankGen, a 1.2 billion parameter encoder model for scoring the relevance of text generations to input prompts, which significantly outperforms standard decoding methods like nucleus sampling when incorporated into beam search.


## How does this paper compare to other research in the same field?

 Based on my review, here are some thoughts on how this paper compares to other research in the field:- The paper presents RankGen, a new neural model for scoring and re-ranking text continuations conditioned on a prefix. This approach of learning better scoring functions and using them during decoding is similar to some other recent work like the energy-based models from Deng et al. (2020) and Qin et al. (2022). However, RankGen is trained at a much larger scale and incorporates new strategies for negative sampling during contrastive learning.- Most prior work on decoding focuses on modifying the decoding algorithm used with a fixed pretrained LM. In contrast, RankGen can be used as a plug-in scoring function with any external LM and decoding method. This makes it more flexible and widely applicable. - Many recent papers have proposed contrastive learning objectives for text generation, like An et al. (2022) and Li et al. (2022). RankGen also uses contrastive learning, but between entire sequences rather than tokens. The use of discourse-based negatives is also unique.- For evaluation, RankGen considers various strong baselines like nucleus sampling, top-k sampling, and typical sampling. The scale of human evaluation between RankGen and nucleus is more extensive than most prior work. The analysis of generations using a taxonomy inspired by Dou et al. (2022) is also insightful.- The exploration of using RankGen for complex literary retrieval is novel and shows intriguing zero-shot capabilities. Most prior decoding work has focused just on text generation.Overall, I think RankGen makes several notable contributions over related decoding papers through the scale and flexibility of the approach, the discourse-based contrastive formulation, extensive comparisons to other methods, and applications to retrieval. The human preference results over nucleus sampling are a convincing demonstration of improvements over strong baselines.


## What future research directions do the authors suggest?

 Based on my reading of the paper, here are some of the key future research directions suggested by the authors:1. Training or adapting RankGen models for other languages besides English. The current models are only trained on English text.2. Training larger RankGen models, such as T5-XXL size or bigger, with longer prefix/suffix lengths, to see if generation quality continues to improve with scale.3. Exploring the usefulness of RankGen for other generation tasks like dialog, summarization, or long-form QA.4. Using RankGen to re-rank much larger hypothesis sets generated with search algorithms like in the Xu et al. (2021) paper. 5. Incorporating RankGen more directly into generative modeling to avoid over-generation, such as via distillation or reinforcement learning.6. Using RankGen in retrieval augmented text generation systems.7. Further exploring RankGen's capability as a retriever, either zero-shot or with fine-tuning on datasets like BEIR. 8. Using RankGen for text generation evaluation, similar to metrics like CARP or CLIPScore.9. Applying RankGen to other domains beyond text, such as code, proteins, or mathematical proofs.In summary, the main future directions focus on scaling up RankGen, incorporating it more tightly into generative models to avoid over-generation, and exploring its usefulness across languages, tasks, and modalities. The authors also suggest several interesting applications in retrieval and generation evaluation.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the key points from the paper:The paper proposes RankGen, a 1.2 billion parameter encoder model for scoring the compatibility between a context prefix and candidate continuations for text generation. RankGen is trained with contrastive learning to map prefixes close to human-written continuations and away from two types of negatives: 1) irrelevant sequences from the same document (in-book negatives), and 2) generations from a pretrained language model (generative negatives). Experiments across four language models on Wikipedia and Project Gutenberg data show RankGen significantly outperforms decoding algorithms like nucleus sampling both automatically (85.0 vs 77.3 on MAUVE) and in human evaluations (74.5% preference). Analysis reveals RankGen outputs have higher relevance and continuity with the prefix. The model can be easily integrated into existing LMs as a re-ranker or beam search scoring function. RankGen also achieves state-of-the-art zero-shot performance on complex literary retrieval tasks like RELiC and ChapterBreak. The authors have open-sourced the code, data, model checkpoints and human annotations.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the paper:The paper proposes RankGen, a 1.2 billion parameter encoder model for English that scores the compatibility between a given text prefix and candidate continuations of that prefix. RankGen is designed to improve the quality of text generation from language models. The authors motivate RankGen by showing that modern language models often assign high probability to low quality outputs that are repetitive, incoherent, or irrelevant. RankGen is trained using contrastive learning to map a prefix close to its true continuation and far from "negative" continuations. Two strategies are used to select challenging negatives: 1) Random sequences from the same document as the prefix, which are fluent but incoherent. 2) Sequences generated by a language model from the prefix, which are relevant but may contain issues. Experiments across models and datasets show RankGen significantly improves text generation quality over baselines like nucleus sampling, based on both automatic metrics and human evaluation. The authors also demonstrate RankGen's ability to achieve state-of-the-art on complex literary retrieval tasks.


## Summarize the main method used in the paper in one paragraph.

 Here is a one paragraph summary of the main method used in the paper:The paper presents RankGen, a 1.2 billion parameter encoder model for English that scores the compatibility between a given prefix (input sequence) and candidate continuations, which can be human-written text or machine-generated text. RankGen is trained using large-scale contrastive learning on two types of negative samples: 1) random human-written sequences from the same document as the prefix (\inbookobjective), and 2) machine-generated continuations to the prefix (\genobjective). A contrastive loss pushes the prefix embedding closer to the embedding of the true continuation and farther from the negatives. At inference time, RankGen can flexibly score and re-rank outputs from any generative language model. The authors show it significantly outperforms likelihood-based decoding methods like greedy, beam search and sampling on automatic metrics and human evaluation. RankGen is also shown to achieve strong performance on complex literary retrieval tasks, demonstrating its capability as an effective zero-shot retriever.


## What problem or question is the paper addressing?

 The paper is addressing two main problems or questions:1. Modern language models often assign high probabilities to output sequences that are repetitive, incoherent, or irrelevant given an input sequence (prefix). This leads to artifacts and issues in model-generated text.2. Existing decoding algorithms like greedy decoding, beam search, nucleus sampling, top-k sampling, etc. have limitations and generate text with inconsistencies, hallucinations, factual errors, or commonsense issues.The key questions the paper seems to be exploring are:- How can we improve the relevance, coherence, and continuity of model-generated text given an input prefix? - How can we get models to generate text that better matches human-written distributions instead of exhibiting typical model-generated artifacts?To address these questions, the paper proposes RankGen, which is a model that scores/ranks model-generated text given an input prefix to pick outputs closer to human text. The main ideas are:- Train RankGen to score sequences using contrastive learning - push the prefix embedding closer to gold text and away from incorrect negative samples - Use two types of hard negative samples - random human-written text from the same document (fluent but incoherent) and model-generated text (potentially relevant but repetitive/hallucinated)- Flexibly incorporate RankGen scoring into existing decoder models like GPT-2 using beam search- Empirically evaluate RankGen on open-ended generation tasks using both automatic metrics and human evaluationsSo in summary, the key focus is improving relevance, coherence, and human-likeness of open-ended text generation by scoring/ranking model outputs with a learned model (RankGen).
