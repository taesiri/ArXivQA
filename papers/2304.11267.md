# [Speed Is All You Need: On-Device Acceleration of Large Diffusion Models   via GPU-Aware Optimizations](https://arxiv.org/abs/2304.11267)

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper aims to address is how to accelerate the inference speed of large diffusion models like Stable Diffusion for on-device deployment. Specifically, the paper focuses on optimizing the inference latency and memory footprint of Stable Diffusion when running on mobile devices with GPUs.The key hypothesis is that through a set of GPU-aware optimizations targeting the model architecture and kernels, the authors can achieve state-of-the-art inference performance for Stable Diffusion on mobile devices.Some of the main optimizations explored include:- Specialized kernels for group normalization and GELU activation- Enhancing attention module efficiency via partially fused softmax and FlashAttention- Strategic application of Winograd convolution The goal is to demonstrate that with these optimizations, the authors can attain much faster inference times compared to baseline implementations, broadening the applicability of large diffusion models on a wide range of mobile devices with limited compute. Overall, the core research question is how to maximize the on-device inference performance of large diffusion models through computational and memory optimizations tailored for mobile GPUs.


## What is the main contribution of this paper?

The main contribution of this paper is proposing a series of optimization techniques to accelerate the inference speed of large diffusion models on GPU-powered mobile devices. Specifically, the paper introduces:- Specialized kernels for group normalization and GELU activation to consolidate operations into single GPU commands. - An optimized softmax implementation for the attention module that reduces memory footprint and latency.- Selective use of the FlashAttention algorithm for exact attention with reduced memory accesses.- Strategic application of Winograd convolution to efficiently handle the prevalent 3x3 convolutions in the model.Through benchmark evaluations, the paper demonstrates significant inference speedups on mobile GPUs - reducing the latency of Stable Diffusion 1.4 by over 50% on a Samsung S23 Ultra and around 33% on an iPhone 14 Pro Max. This enables state-of-the-art inference times under 12 seconds to generate a 512x512 image in 20 denoising steps. The optimizations expand the applicability of large generative AI models on mobile devices with enhanced user experience.
