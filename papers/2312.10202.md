# [Low-resource classification of mobility functioning information in   clinical sentences using large language models](https://arxiv.org/abs/2312.10202)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Function is an important health indicator but functional status information often remains hidden in unstructured clinical text, restricting the extraction of valuable real-world insights to enhance care. 
- Effectively extracting and interpreting functional data from EHRs is challenging.

Proposed Solution: 
- Develop classification methods to rapidly identify the presence of relevant functional information within clinical notes to help clinicians efficiently review the most critical sentences.

Objectives:
1) Evaluate publicly available large language models (LLMs) on identifying mobility functioning information in clinical sentences, in both zero-shot and few-shot settings.
2) Fine-tune LLMs using a parameter-efficient prompt-based approach and analyze performance under low-resource settings. 
3) Release an annotated dataset for mobility functioning classification to enable further LLM applications in healthcare.  

Methods:
- Curate a balanced 1000-sentence dataset labeled with the presence of mobility functioning, derived from physical therapy notes.
- Construct prompts to query LLMs whether sentences contain mobility information. Evaluate in-context learning performance.
- Sample demonstrations via random and kNN-based sampling to provide examples in the prompt.
- Apply parameter-efficient prompt tuning and compare to traditional full model fine-tuning.

Main Results:  
- Flan-T5-xxl achieves highest F1 of 0.865 in few-shot prompting and 0.922 after prompt tuning with full dataset.
- kNN sampling outperforms random sampling overall.
- With only 2.3M additional parameters, prompt-tuned Flan-T5-xl reaches over 0.9 F1 score comparable to fully tuned GPT model.

Conclusions:
- Instruction-tuned LLMs show strong zero-shot and few-shot performance for mobility functioning classification.
- Further parameter-efficient prompt-based fine-tuning with limited labeled data leads to additional significant gains.
