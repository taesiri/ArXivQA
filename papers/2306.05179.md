# M3Exam: A Multilingual, Multimodal, Multilevel Benchmark for Examining   Large Language Models

## What is the central research question or hypothesis that this paper addresses?

Based on a quick skim of the paper, it seems the main research goal is to introduce a new benchmark dataset called M3Exam for evaluating large language models (LLMs). The key characteristics of M3Exam are:- Multilingual: It contains exam questions in 9 different languages to assess models' multilingual abilities.- Multimodal: Around 23% of the questions require processing images, allowing evaluation of multimodal understanding. - Multilevel: It includes exams from 3 different education levels (primary, middle, high school) to test models at varying difficulties.The authors then utilize M3Exam to evaluate various state-of-the-art LLMs. The main findings are:- Current models still struggle with non-Latin and low-resource languages. - Multimodal models also underperform, having difficulty understanding complex images and reasoning across images.- Model performance does not decrease monotonically as exam difficulty increases, unlike human performance.So in summary, the central goal is to propose the M3Exam benchmark and use it to assess LLMs, revealing limitations of current models and the need for continued progress in multilingual, multimodal, and reasoning abilities. The dataset itself facilitates more comprehensive LLM evaluation across diverse languages, modalities and difficulties.


## What is the main contribution of this paper?

The main contribution of this paper is the introduction of M3Exam, a new benchmark dataset for evaluating large language models (LLMs). The key features of M3Exam are:- It is multilingual, containing real exam questions in 9 diverse languages. This allows assessing LLMs' capabilities across different languages and cultures. - It is multimodal, with about 23% of the questions requiring images for successful solving. This enables evaluating LLMs' multimodal understanding abilities.- It has a multilevel structure, sourced from exams at the end of 3 critical educational stages. This permits comprehensive evaluation of models at different levels of complexity.In total, M3Exam comprises 12,317 questions covering various subjects. Experiments demonstrate that current LLMs, including state-of-the-art multilingual and multimodal models, still struggle with many aspects of this benchmark. Overall, M3Exam provides a valuable resource for evaluating LLMs in multilingual, multimodal, and multilevel settings. It can help drive further progress in developing models with more robust and general intelligence.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

The paper introduces M3Exam, a new benchmark dataset for evaluating large language models. It contains over 12,000 real exam questions in 9 languages, across 3 educational levels, with 23% of questions requiring images. Experiments show current models still struggle with non-English and multimodal questions. M3Exam allows comprehensive LLM evaluation on multilingual, multimodal, and multilevel abilities.


## How does this paper compare to other research in the same field?

Here are a few thoughts on how this paper compares to other research in evaluating large language models:- The focus on using real human exam questions is a strength compared to many existing benchmarks that use synthetic data or simply translate English datasets into other languages. Using authentic exam data likely provides a more realistic assessment of language understanding and reasoning skills. - Covering multiple languages, especially low-resource languages like Javanese and languages with non-Latin scripts like Thai, sets this benchmark apart from many others that concentrate solely on English. Evaluating multilingual abilities is critical for models intended for broad use.- Incorporating multimodal questions with images is fairly novel. Many benchmarks evaluate only on text, but understanding and reasoning with images is an important capability, so including visual data makes the benchmark more comprehensive. - Spanning three educational levels provides a way to assess model performance at different stages of complexity that is not present in most benchmarks. This could reveal interesting insights about how intelligence develops in LLMs vs humans.- On the other hand, the dataset is currently limited to multiple choice questions, whereas some benchmarks include a wider diversity of question types and response formats.- The scale of 12k questions is reasonable but smaller than some other recent benchmarks with hundreds of thousands of examples.Overall, the multilingual, multimodal, and multi-level nature of this benchmark distinguishes it from many existing resources and allows for a more thorough evaluation of LLMs. But there are also opportunities to build on this work by expanding to more languages, question types, and scale.
