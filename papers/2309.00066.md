# [SoDaCam: Software-defined Cameras via Single-Photon Imaging](https://arxiv.org/abs/2309.00066)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be: How can we use single-photon imaging devices like SPAD arrays to achieve flexible, software-defined cameras with novel post-capture imaging capabilities?The key ideas proposed are:- SPAD arrays can acquire data in the form of "photon-cubes" - sequences of binary frames capturing individual photon detections with high temporal resolution. - Simple linear transformations or "projections" of these photon-cubes can emulate a diverse range of camera types like flutter shutter cameras, event cameras, motion cameras, etc.- This provides a realization of "software-defined cameras" (SoDaCam) where the imaging modality is defined flexibly via post-processing, rather than fixed hardware choices. - SoDaCam unlocks new capabilities like simultaneously achieving multiple modalities using a single sensor, capabilities difficult to realize in conventional hardware.- Projections can be computed on-sensor to reduce bandwidth and power needs for SPAD imaging.So in summary, the central hypothesis is that post-capture projections on photon-cube data from SPADs can enable flexible software-defined cameras with novel capabilities. The paper explores this concept through theoretical analysis, simulations, and prototype hardware experiments.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contributions appear to be:1. Introducing the concept of "photon-cube projections" for single-photon imaging devices. The key idea is that different projections of the raw photon-cube data acquired by a single-photon avalanche diode (SPAD) array can emulate various imaging modalities like exposure bracketing, flutter shutter cameras, video compressive sensing, event cameras, etc.2. Demonstrating the flexibility of photon-cube projections to provide novel capabilities beyond the emulated cameras, such as multi-bucket coding for video compressive sensing, low-light event imaging, and motion stacks for motion deblurring. 3. Implementing some of the projections on a novel compute architecture called UltraPhase that is designed for single-photon imaging. This shows the feasibility of computing projections near sensor to reduce sensor readout and power consumption.4. Conceptualizing the idea of a "software-defined camera" or SoDaCam that can provide multiple imaging modalities simultaneously from a single photon-sensing hardware. The software-defined notion comes from the fact that different projections on the raw photon data can emulate different camera types.In summary, the core contribution seems to be introducing photon-cube projections as a way to obtain diverse computational imaging capabilities from single-photon data in a software-defined manner, along with demonstrations of novel imaging modalities and an efficient hardware implementation. The software-defined camera concept built on photon-cube projections is positioned as a step towards computational cameras that have flexibility limited only by shot noise and computational power.
