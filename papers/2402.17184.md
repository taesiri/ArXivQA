# [Extreme Encoder Output Frame Rate Reduction: Improving Computational   Latencies of Large End-to-End Models](https://arxiv.org/abs/2402.17184)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- End-to-end (E2E) automatic speech recognition (ASR) models are scaling to billions of parameters and improving accuracy, but have high computational latency unsuitable for voice search. 
- Reducing computational latency is critical for deployment of large E2E models on voice search tasks requiring low user-perceived latency.

Proposed Solution:
- Apply extreme reduction of encoder output frames via multiple funnel transformer layers to dramatically compress encoder outputs.   
- Investigate impact of architectural choices (where to add funnel layers, degree of reduction per layer) to balance accuracy and latency.
- Use alignment-length synchronous decoding strategy well-suited for TPU deployment.
- Show increased prediction network context is critical to compensate for compressed encoder outputs.

Key Results:
- Demonstrate encoder output with duration of 2.56 secs per frame, while limiting WER degradation to just 3% relative to high-latency baseline.
- Improve encoder latency by 48% and decoder latency by 92% relative to baseline.
- For 95% of test utterances, decode only 3 or fewer encoder frames. This is 64x frame reduction over prior work.  
- Prediction network modeling longer context is crucial for extreme frame reduction, especially LSTM over limited $V^2$ embedding.
- Total computational latency reduced by 82% (550ms) with only 2.5-3.3% relative WER loss after MWER training.

Main Contributions:
- Viability of extreme encoder output frame reduction for low-latency voice search demonstrated.  
- Analysis of optimal architectural configurations for funnel layer placement and strides.
- Importance of increased prediction network context shown for extreme reduction regimes.
- Overall, techniques provide path to deploy billion-parameter E2E models for high-accuracy, low-latency voice search.
