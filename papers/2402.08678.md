# [Graph Mamba: Towards Learning on Graphs with State Space Models](https://arxiv.org/abs/2402.08678)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key aspects of the paper:

Problem:
- Existing graph neural networks like message-passing GNNs and graph transformers have limitations in capturing long-range dependencies, oversmoothing, oversquashing, scalability, and reliance on complex positional/structural encodings. 

- Recent state space models (SSMs) like Mamba show promise but cannot be directly applied to graph data. Adapting Mamba to replace transformers in existing graph transformer frameworks performs suboptimally.

Proposed Solution:
- The paper proposes Graph Mamba Networks (GMNs), a new GNN framework based on selective SSMs. 

- GMNs have a 5-step framework: (1) Neighborhood tokenization via sampling random walks (2) Optional positional/structural encoding (3) Local encoding of tokens (4) Token ordering (5) Bidirectional selective SSM encoder

- Tokenization uses random walks to capture hierarchical structure better than fixed k-hop neighbors. Repeated sampling provides more context to leverage SSM's strength in long sequences.

- Bidirectional SSM encoder scans tokens in two directions for robustness to permutation. Learned selection allows filtering irrelevant tokens.

- Framework bridges node and subgraph tokenization methods with one parameter. Last SSM layers operate on node encodings for information flow.

Contributions:
- Presents challenges in adopting Mamba-like SSMs for graph data and provides a recipe to address them

- Proposes efficient random walk-based tokenization that is more expressive than k-hop methods 

- Introduces new bidirectional SSM encoder that is robust to permutation of tokens

- Shows GMNs are universal approximators for graph functions and exceed WL tests with positional encoding

- Achieves state-of-the-art performance across long-range, small, large, heterophilic benchmark datasets with lower compute

- Validates each architectural choice via ablation studies

In summary, the paper develops a new powerful and scalable graph neural network framework called Graph Mamba Networks based on selective state space models that outperforms prior graph MPNNs and transformers.
