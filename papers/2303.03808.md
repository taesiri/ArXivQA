# [Multiscale Tensor Decomposition and Rendering Equation Encoding for View   Synthesis](https://arxiv.org/abs/2303.03808)

## What is the central research question or hypothesis that this paper addresses?

 The central research question this paper addresses is how to improve the quality of view synthesis from multi-view images. The key hypotheses are:

1. Representing a scene with a multiscale tensor decomposition will lead to faster convergence and better rendering quality compared to a single-scale representation. 

2. Encoding the rendering equation in the feature space will facilitate modeling complex view-dependent effects compared to directly encoding the view directions.

Specifically, the paper proposes a new method called the neural radiance feature field (NRFF) with two main contributions:

1. A multiscale tensor decomposition scheme to represent scenes from coarse to fine scales. This is shown to enable faster convergence and better rendering quality with fewer learnable features compared to single-scale methods.

2. Encoding the rendering equation in the feature space using anisotropic spherical Gaussians instead of directly encoding view directions. This provides the MLP with more knowledge about the rendering process to better model view-dependent effects.

The central hypothesis is that combining these two ideas - multiscale representation and rendering equation encoding - will advance the quality of view synthesis compared to existing state-of-the-art methods. Experiments on synthetic and real datasets validate the efficacy of the proposed NRFF method.


## What is the main contribution of this paper?

 The main contributions of this paper are:

1. Proposing a novel multiscale tensor decomposition (MTD) scheme to represent scenes from coarse to fine scales. This enables faster convergence and better rendering quality compared to a single-scale representation. 

2. Proposing a rendering equation encoding (REE) method to encode the rendering equation in the feature space instead of directly encoding view directions. This provides the MLP with more knowledge about the outgoing radiance to better model complex view-dependent effects.

In summary, the key ideas presented are:

- Using multiscale tensor decomposition to represent scenes, improving over a single-scale representation. 

- Encoding the rendering equation in the feature space rather than just encoding view directions, which better informs the MLP about light transport.

The combination of these two ideas results in the proposed neural radiance feature field (NRFF) method, which achieves state-of-the-art view synthesis results on both synthetic and real datasets as demonstrated experimentally.
