# [Panacea: Pareto Alignment via Preference Adaptation for LLMs](https://arxiv.org/abs/2402.02030)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem: 
The prevailing approach for large language model (LLM) alignment uses scalar human preference labels to train the model. However, this oversimplifies the multi-dimensional and heterogeneous nature of human preferences, leading to reduced expressivity and even misalignment. Specifically, there are two key limitations:
(1) The scalar labels can be inconsistent due to differing implicit preference weights held by human labelers. 
(2) Optimizing a single objective leads to one model that fails to cover the full spectrum of diverse user needs and preferences.

Proposed Solution:
The paper proposes to reframe LLM alignment as a multi-dimensional preference optimization (MDPO) problem. The goal is to learn a single LLM that can adapt online and Pareto-optimally to any set of preferences without further tuning. To achieve this, they propose Panacea, which embeds a low-dimensional preference vector into the singular values within each SVD-based low-rank adaptation (LoRA) layer. This allows the preference vector to fundamentally influence the model's behavior. Panacea is trained end-to-end by aggregating per-dimension objectives (e.g. using linear scalarization or weighted Tchebycheff) according to the sampled preference vector. During inference, users can simply specify a preference vector to obtain tailored Pareto-optimal responses.

Main Contributions:
- Identify limitations of using scalar labels for LLM alignment and propose reframing it as a MDPO problem.
- Design Panacea that embeds preferences into singular values to achieve online and Pareto-optimal adaptation with one single model.
- Provide theoretical analysis to show Panacea can recover the entire Pareto front under mild assumptions.
- Empirically demonstrate the superior performance and efficiency of Panacea over strong baselines in capturing the broadest Pareto front on the helpfulness-harmlessness alignment task.

The paper makes an important step towards effectively and efficiently aligning models to diverse and intricate human preferences in a Pareto-optimal manner using just one single model.
