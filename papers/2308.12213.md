# [CLIPN for Zero-Shot OOD Detection: Teaching CLIP to Say No](https://arxiv.org/abs/2308.12213)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is:How can we empower CLIP with the capability to distinguish in-distribution (ID) and out-of-distribution (OOD) samples for more effective zero-shot OOD detection? The key hypothesis is that equipping CLIP with the logic to say "no" via learnable "no" prompts and a "no" text encoder can allow it to identify unknown/OOD samples more accurately. Specifically, the paper proposes that using positive-semantic prompts for ID classes and negation-semantic prompts for unknown classes can teach CLIP to associate images with "no" prompts, thereby enabling it to detect OOD samples even if they have high ID-ness scores.In summary, the main research question is how to upgrade CLIP's architecture and training to confer it with improved zero-shot OOD detection abilities, with the central hypothesis being that adding "no" logic is an effective way to achieve this. The paper introduces methods to implement and test this hypothesis.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contributions appear to be:1. Proposing a new CLIP architecture called CLIPN that equips CLIP with "no" logic for out-of-distribution (OOD) detection. This is done by adding new "no" prompts and a "no" text encoder to capture negation semantics.2. Introducing two new losses - image-text binary-opposite loss and text semantic-opposite loss - to teach CLIPN to match images with "no" prompts and understand the meaning of "no". 3. Designing two new threshold-free inference algorithms called competing-to-win and agreeing-to-differ to perform OOD detection using the negation semantics from "no" prompts and text encoder.4. Demonstrating through experiments on 9 datasets that CLIPN outperforms existing methods on both large-scale and small-scale OOD detection tasks. On ImageNet, it improves over prior arts by at least 2.34% in AUROC and 11.64% in FPR95.In summary, the main contribution appears to be proposing a new CLIP-based architecture and training approach called CLIPN that leverages "no" logic and negation semantics to enable superior OOD detection compared to existing methods. The introduction of "no" prompts, losses, and inference algorithms are key innovations that empower the OOD detection capability.
