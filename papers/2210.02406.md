# Decomposed Prompting: A Modular Approach for Solving Complex Tasks

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be:How to develop an effective approach for solving complex reasoning tasks using large language models with few-shot prompting?The key hypothesis appears to be that decomposing complex tasks into simpler sub-tasks, and prompting large language models separately on these sub-tasks, can lead to better performance compared to prompting the model directly on the complex task. Specifically, the paper proposes an approach called "Decomposed Prompting" (DecomP) which involves:1) Using a "decomposer" prompt to break down a complex reasoning task into simpler sub-tasks. 2) Creating separate "sub-task handler" prompts to teach the model each sub-task.3) Executing the sub-task prompts sequentially to solve the overall complex task.The central hypothesis is that this decomposition approach will allow the sub-task prompts to be optimized better, sub-tasks could be further decomposed if needed, and different prompts or models could be swapped in for different sub-tasks. Overall, the paper tests whether this leads to improved performance over standard few-shot prompting methods on complex reasoning tasks.In summary, the central research question is how to effectively solve complex tasks with few-shot prompting of large language models, with the key hypothesis being that decomposing tasks into sub-task prompts will improve performance. The DecomP approach is proposed and evaluated as a method to achieve this.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contribution seems to be proposing a new approach called Decomposed Prompting (DecomP) for solving complex reasoning tasks by decomposing them into simpler sub-tasks. The key ideas are:- Using a "decomposer" prompt to generate a sequence of simpler sub-tasks needed to solve a complex task, rather than having to demonstrate the full reasoning in a single prompt like in chain-of-thought prompting.- The sub-tasks are handled by separate "sub-task handlers" which are prompting-based LLMs focused on those simpler tasks. This allows optimizing each sub-task prompt independently.- The sub-tasks can be further recursively decomposed if still too complex. Sub-task handlers can also be symbolic functions like retrievers.- This decomposition provides modularity, easier debugging/upgrading of components, ability to incorporate symbolic functions, and potential benefits like better generalization.In summary, the main contribution seems to be proposing the DecomP framework to break down complex reasoning tasks into modular sub-task prompts that can be optimized independently. Experiments demonstrate benefits like improved generalization and incorporation of symbolic functions.
