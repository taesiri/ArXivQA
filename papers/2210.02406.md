# Decomposed Prompting: A Modular Approach for Solving Complex Tasks

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be:How to develop an effective approach for solving complex reasoning tasks using large language models with few-shot prompting?The key hypothesis appears to be that decomposing complex tasks into simpler sub-tasks, and prompting large language models separately on these sub-tasks, can lead to better performance compared to prompting the model directly on the complex task. Specifically, the paper proposes an approach called "Decomposed Prompting" (DecomP) which involves:1) Using a "decomposer" prompt to break down a complex reasoning task into simpler sub-tasks. 2) Creating separate "sub-task handler" prompts to teach the model each sub-task.3) Executing the sub-task prompts sequentially to solve the overall complex task.The central hypothesis is that this decomposition approach will allow the sub-task prompts to be optimized better, sub-tasks could be further decomposed if needed, and different prompts or models could be swapped in for different sub-tasks. Overall, the paper tests whether this leads to improved performance over standard few-shot prompting methods on complex reasoning tasks.In summary, the central research question is how to effectively solve complex tasks with few-shot prompting of large language models, with the key hypothesis being that decomposing tasks into sub-task prompts will improve performance. The DecomP approach is proposed and evaluated as a method to achieve this.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contribution seems to be proposing a new approach called Decomposed Prompting (DecomP) for solving complex reasoning tasks by decomposing them into simpler sub-tasks. The key ideas are:- Using a "decomposer" prompt to generate a sequence of simpler sub-tasks needed to solve a complex task, rather than having to demonstrate the full reasoning in a single prompt like in chain-of-thought prompting.- The sub-tasks are handled by separate "sub-task handlers" which are prompting-based LLMs focused on those simpler tasks. This allows optimizing each sub-task prompt independently.- The sub-tasks can be further recursively decomposed if still too complex. Sub-task handlers can also be symbolic functions like retrievers.- This decomposition provides modularity, easier debugging/upgrading of components, ability to incorporate symbolic functions, and potential benefits like better generalization.In summary, the main contribution seems to be proposing the DecomP framework to break down complex reasoning tasks into modular sub-task prompts that can be optimized independently. Experiments demonstrate benefits like improved generalization and incorporation of symbolic functions.


## How does this paper compare to other research in the same field?

Here are a few thoughts on how this paper compares to other research in the same field:- The approach of using modular prompting to decompose complex tasks is novel and hasn't been explored much in prior work on few-shot prompting. Most prior work has focused on approaches like chain-of-thought prompting that demonstrate the full reasoning steps. This decomposition idea allows for more systematic debugging, optimization, and generalization.- The idea of recursively decomposing tasks to make them scale-invariant is clever. This is related to ideas in program synthesis and algorithm design but hasn't been explored much in few-shot prompting. It allows the approach to work on longer sequences than the base model can handle.- Incorporating symbolic functions like information retrieval into the prompting framework is powerful. Most prior few-shot prompting work has focused on pure LLM approaches. Allowing seamless integration of neural and symbolic AI is an important contribution.- The evaluation covers a diverse set of reasoning tasks from math and symbolic manipulations to open-domain QA. Many recent prompting papers have focused narrowly on a single task or genre, so it's good to see a systematic assessment across different domains.- The gains over chain-of-thought prompting are substantial across the board. This shows the flexibility of the approach over what was previously the state-of-the-art in few-shot prompting.Overall, I think this is a very novel and important contribution that shows a promising new paradigm for few-shot prompting that supports modularity, generalization, and seamless integration of symbolic capabilities. The gains over prior prompting approaches are substantial and it addresses key limitations like handling complexity and length.
