# [MixMAE: Mixed and Masked Autoencoder for Efficient Pretraining of   Hierarchical Vision Transformers](https://arxiv.org/abs/2205.13137)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question is:

How can we develop an efficient pretraining method for hierarchical vision transformers that avoids the limitations of using masked tokens like [MASK]?

The key limitations identified with using [MASK] tokens are:

- Using [MASK] tokens slows down training due to processing less informative symbols.

- It causes pretraining-finetuning inconsistency since [MASK] tokens never appear during finetuning.

To address this, the paper proposes MixMAE, which mixes tokens from two images as the input rather than using [MASK] tokens. The goal is to develop an efficient pretraining approach that is applicable to various hierarchical vision transformers.

In summary, the central hypothesis is that mixing image tokens as input will allow for more efficient pretraining of hierarchical vision transformers compared to approaches that rely on [MASK] tokens, while also avoiding pretraining-finetuning inconsistency. The experiments aim to validate whether MixMAE achieves these goals.


## What is the main contribution of this paper?

 The main contribution of this paper is proposing MixMAE, a simple but efficient pretraining method for hierarchical vision transformers. The key ideas of MixMAE are:

- It creates a mixed image by combining patches from two random images as input. This avoids using special [MASK] tokens like previous methods. 

- It performs dual reconstruction to recover the original two images from the mixed input. This improves training efficiency.

- It uses Swin Transformer with larger window sizes as the encoder to better integrate global context for reconstruction. 

- It reduces the difficulty of the pretext task with techniques like masked self-attention.

In summary, MixMAE presents an effective way to pretrain hierarchical vision transformers that is compatible, efficient, and consistent. It demonstrates strong performance on ImageNet and transfer learning benchmarks compared to previous masked image modeling methods. The main novelty is in the mixed input creation and dual reconstruction for pretraining.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the key points from the paper:

The paper proposes MixMAE, a pretraining method for hierarchical vision transformers that mixes and reconstructs pairs of images to avoid using masked tokens and enables more efficient and consistent pretraining compared to prior masked image modeling approaches.
