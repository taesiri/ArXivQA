# [Multilingual Sentence-T5: Scalable Sentence Encoders for Multilingual   Applications](https://arxiv.org/abs/2403.17528)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Multilingual sentence embeddings are useful for cross-lingual NLP tasks. Prior work using contrastive learning on natural language inference (NLI) data shows promising results, but the potential of large language models has not been fully explored. 

Proposed Solution:
- The paper proposes Multilingual Sentence T5 (m-ST5), which extends the monolingual Sentence T5 model to multilingual scenarios. 
- m-ST5 is based on fine-tuning a 5.7 billion parameter Multilingual T5 (mT5) encoder using contrastive learning on NLI datasets like XNLI.
- The low-rank adaptation (LoRA) technique is used to feasibly fine-tune the large mT5 model on a single GPU.

Main Contributions:

- m-ST5 outperforms prior multilingual sentence embedding methods like mSimCSE on tasks like Tatoeba retrieval, BUCC, and XSTS especially for low resource languages.
- Monolingual NLI data fine-tuning works better than cross-lingual for semantic textual similarity while cross-lingual is better for retrieval tasks. 
- m-ST5 demonstrates strong zero-shot cross-lingual transferability, achieving performance comparable to monolingual models on Japanese and Chinese tasks.
- Scaling model size leads to performance improvements, following the scaling laws. Lower resource languages with less similarity to English benefit more from scale indicating language transferability emerges in larger models.

In summary, the paper presents m-ST5, a new state-of-the-art approach for multilingual sentence embeddings based on fine-tuning a large multilingual encoder-decoder model. Key results show the efficacy of model scaling and transfer learning to low resource languages.
