# [Using Explanations to Guide Models](https://arxiv.org/abs/2303.11932)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the main research questions and hypotheses addressed in this paper are:1. How effective are different attribution methods (Integrated Gradients, Grad-CAM, B-COS) and model architectures (Vanilla ResNet, X-DNN, B-COS) for model guidance using bounding box annotations?The authors hypothesize that model-inherent explanations like B-COS will be more amenable to guidance compared to post-hoc methods like Integrated Gradients and Grad-CAM. They also expect B-COS models to produce higher quality explanations.2. How do different localization loss functions compare in guiding models via bounding boxes to focus on relevant object features? The authors hypothesize that directly optimizing for attribution localization metrics like EPG with a loss like the proposed Energy loss will outperform other losses like L1 and RRR* in improving attribution localization.3. Can model guidance with just bounding boxes improve accuracy by suppressing reliance on spurious correlations?The authors hypothesize that guiding models to focus on object features inside bounding boxes will reduce reliance on spurious correlations in the data and thus improve accuracy.4. How much annotation effort is needed for effective model guidance? The authors hypothesize that only a small fraction of annotated images (e.g. 1-10%) with bounding boxes is sufficient to significantly improve attribution localization.5. How robust is model guidance to annotation noise and bounding box misalignment?The authors hypothesize that the proposed Energy loss will be more robust to annotation errors compared to other losses based on its formulation.In summary, the key questions are around comparing attribution methods, losses, and amount of supervision needed for model guidance using bounding box annotations to improve explanation quality and accuracy by suppressing spurious correlations.
