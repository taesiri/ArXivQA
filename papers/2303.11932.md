# [Using Explanations to Guide Models](https://arxiv.org/abs/2303.11932)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading, the main research questions and hypotheses addressed in this paper are:

1. How effective are different attribution methods (Integrated Gradients, Grad-CAM, B-COS) and model architectures (Vanilla ResNet, X-DNN, B-COS) for model guidance using bounding box annotations?

The authors hypothesize that model-inherent explanations like B-COS will be more amenable to guidance compared to post-hoc methods like Integrated Gradients and Grad-CAM. They also expect B-COS models to produce higher quality explanations.

2. How do different localization loss functions compare in guiding models via bounding boxes to focus on relevant object features? 

The authors hypothesize that directly optimizing for attribution localization metrics like EPG with a loss like the proposed Energy loss will outperform other losses like L1 and RRR* in improving attribution localization.

3. Can model guidance with just bounding boxes improve accuracy by suppressing reliance on spurious correlations?

The authors hypothesize that guiding models to focus on object features inside bounding boxes will reduce reliance on spurious correlations in the data and thus improve accuracy.

4. How much annotation effort is needed for effective model guidance? 

The authors hypothesize that only a small fraction of annotated images (e.g. 1-10%) with bounding boxes is sufficient to significantly improve attribution localization.

5. How robust is model guidance to annotation noise and bounding box misalignment?

The authors hypothesize that the proposed Energy loss will be more robust to annotation errors compared to other losses based on its formulation.

In summary, the key questions are around comparing attribution methods, losses, and amount of supervision needed for model guidance using bounding box annotations to improve explanation quality and accuracy by suppressing spurious correlations.


## What is the main contribution of this paper?

 The main contribution of this paper is developing and evaluating methods for model guidance using explanations to make models "right for the right reasons". Specifically, it proposes a novel loss function called the Energy loss and shows its effectiveness in guiding models to focus on relevant object features while ignoring irrelevant or spurious correlations. The key contributions are:

1. Comprehensive evaluation of model guidance on challenging large-scale multi-label classification datasets like PASCAL VOC and MS COCO. Prior work has mostly focused on smaller or synthetic datasets. 

2. Proposing a new Energy loss for model guidance that allows using coarse bounding box supervision while still focusing on object-specific features within the boxes. It is shown to be more robust to annotation noise compared to prior losses.

3. Demonstrating that model guidance can improve classification accuracy by mitigating reliance on spurious correlations, using the Waterbirds dataset as an example.

4. Showing model guidance can be effective even with very limited supervision (1-10% annotated images) and coarse bounding box annotations.

5. Systematic comparison between different attribution methods (Integrated Gradients, GradCAM, B-COS) and losses (Energy, L1, Per Pixel Cross-Entropy) for guiding models using explanations.

In summary, this paper provides a comprehensive empirical study of model guidance through explanations on real-world vision tasks. It proposes a new loss that effectively guides models using coarse bounding box supervision, and shows the utility of model guidance for improving performance by focusing on true discriminative features.
