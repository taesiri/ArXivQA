# [SMX: Sequential Monte Carlo Planning for Expert Iteration](https://arxiv.org/abs/2402.07963)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem: 
- Developing agents that can leverage planning during decision-making and learning is important for advancing AI. Methods like AlphaZero that combine search methods with self-play learning are effective but face scaling challenges due to the sequential tree-based search, requiring extensive compute resources. This limits their applicability.

- There is a need for scalable and general algorithms to make such methods more widely usable.

Proposed Solution:
- The paper introduces SMX, a model-based planning algorithm using scalable Sequential Monte Carlo (SMC) methods rather than tree search. 

- Grounded in control as inference theory, SMX views planning as probabilistic inference over optimal trajectories. It uses importance sampling to estimate a posterior over trajectories.

- The sampling process makes SMX inherently parallelizable and suitable for hardware accelerators like TPUs/GPUs. It also removes the need to store large search trees, reducing memory costs.

Main Contributions:

- SMX outperforms AlphaZero and model-free methods as a policy improvement operator on selected benchmarks with both discrete and continuous action spaces.

- It demonstrates strong performance on both domains without modifications, making it widely applicable.

- SMX shows favorable scaling behavior - performance improves with more search budget. Parallelizability also leads to faster wall-clock times than AlphaZero given a search budget.

- The simple implementation, strong performance, generality across domains, scaling behaviour and wall-clock improvements make SMX an important new approach for leveraging planning to boost policy learning in complex decision making scenarios.
