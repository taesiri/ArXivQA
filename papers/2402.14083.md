# [Beyond A*: Better Planning with Transformers via Search Dynamics   Bootstrapping](https://arxiv.org/abs/2402.14083)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
Transformers and large language models (LLMs) have shown impressive performance on various natural language tasks, but still struggle with complex reasoning and planning tasks compared to traditional symbolic planners. The paper aims to address this limitation. 

Proposed Solution: 
The paper presents "Searchformer", a Transformer model trained to optimally solve planning tasks like maze navigation and Sokoban puzzles. The key ideas are:

1) Train the Transformer to predict the "search dynamics" of the A* symbolic planning algorithm on randomly generated planning tasks. Specifically, log A*'s execution trace when searching for the optimal plan into a sequence of tokens encoding which states were added/removed from the search tree.

2) Fine-tune the Transformer with "expert iterations" to further shorten the predicted search dynamics while still outputting optimal plans. This moves past just imitating A* and finds more efficient search patterns.

Main Contributions:

- Introduces "search dynamics bootstrapping", a novel method to train Transformers to solve planning tasks by first predicting symbolic solver's dynamics, then optimizing to improve search efficiency.

- Achieves 93.7% optimal solutions on Sokoban with 26.8% fewer search steps than A* search, showing Transformers can move beyond traditional algorithms.

- Demonstrates including search dynamics in training data significantly boosts Transformer performance over solution-only training, especially in low data regimes. Enables data-efficient reasoning capability.

- Provides in-depth analysis and ablations quantifying the impact of search dynamics training and model sizes. Reveals model parameters matter less than type of training data.

In summary, the paper presents an innovative training methodology enabling Transformers to match or exceed symbolic solvers on complex planning tasks. This is an important step towards more capable and general neural reasoning architectures.
