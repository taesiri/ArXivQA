# [LLM-based Privacy Data Augmentation Guided by Knowledge Distillation   with a Distribution Tutor for Medical Text Classification](https://arxiv.org/abs/2402.16515)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Training deep text classification models requires large amounts of data, but many domains like healthcare have limited public data and abundant private data. 
- Directly accessing private data for training risks leaking sensitive information. 
- Existing methods for privacy-preserving data augmentation either lack theoretical privacy guarantees or struggle to generate high-quality text with large language models.

Proposed Solution:
- Propose a differentially private (DP) data augmentation framework involving:
  1) An LLM generator to create candidate text samples 
  2) A DP discriminator to select samples likely from the private domain
  3) A DP label distribution tutor to guide generation.
- The discriminator uses multiple teacher models on disjoint private data and a student model learns from noised teacher outputs via knowledge distillation. This provides DP guarantees.
- The tutor provides aggregated label distributions from the private data to guide generation, with low additional privacy cost.

Main Contributions:
- Novel framework to synthesize private-domain text samples for augmentation with DP guarantees, by transferring the generation task into a discrimination task.
- DP discriminator design using knowledge distillation between private teacher models and public student model.
- DP label distribution tutor to control data generation within minimal additional privacy budget.
- Strong empirical performance, outperforming DP baselines on medical text classification while preserving privacy.
- Theoretical analysis of privacy guarantees and experimental verification.

In summary, the paper proposes an innovative differentially private data augmentation approach for text classification that leverages the strengths of large language models for generation while protecting private data. Both theoretical and empirical results demonstrate its effectiveness.
