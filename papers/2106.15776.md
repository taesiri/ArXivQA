# [Local Reweighting for Adversarial Training](https://arxiv.org/abs/2106.15776)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central research question this paper aims to address is: Can a robust neural network model be trained to defend against various adversarial attacks, not just a specific attack used during training?The key hypotheses appear to be:1) The relative vulnerability of instances to adversarial attacks is inconsistent across different attacks. Instances that are robust to one type of attack may be vulnerable to another.2) Existing adversarial training methods that assign instance-specific weights, like IRAT, can overfit to the particular attack used during training. This leads to reduced robustness on unseen attacks. 3) A training approach called Locally Reweighted Adversarial Training (LRAT) that performs local reweighting within instance-attack pairs, while avoiding global reweighting across instances, can improve robustness on diverse attacks.So in summary, the central research question is how to train models robust to diverse adversarial attacks, not just a single attack. The key hypothesis is that LRAT, through local reweighting, can achieve improved robustness compared to methods like IRAT that assign global instance weights. The experiments aim to demonstrate this improved generalized robustness of LRAT across different attacks.


## What is the main contribution of this paper?

The main contribution of this paper seems to be proposing a new adversarial training method called Locally Reweighted Adversarial Training (LRAT). The key ideas of LRAT are:- It pairs each instance with its adversarial variants and performs local reweighting inside each pair, rather than doing global reweighting across all instances like prior work on instance reweighting.- The rationale is to fit the instance itself if it is immune to a given attack, but still keep the pair to passively defend against different attacks, rather than skip or downweight the pair like global reweighting methods.- It uses a general vulnerability-based reweighting strategy that can work with different attacks, rather than a heuristic based on PGD steps.- It assigns some minimum weight to the natural data term to avoid overly downweighting instances, to defend against unseen attacks.The authors argue that global reweighting methods like prior work can overly downweight instances, harming generalization to unseen attacks. The local reweighting of LRAT is shown to improve robustness compared to no reweighting, global reweighting, and their combination.So in summary, the main contribution seems to be proposing this new locally reweighted adversarial training approach to improve robustness to diverse adversarial attacks.
