# [NovaCOMET: Open Commonsense Foundation Models with Symbolic Knowledge   Distillation](https://arxiv.org/abs/2312.05979)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Existing commonsense knowledge models have limitations, either using a restricted set of relations limiting downstream applications (previous knowledge models) or not explicitly modeling knowledge (general pre-trained models).

Proposed Solution:
- Present NovaCOMET, an open commonsense knowledge model combining advantages of both types of models. 
- Leverage opaque proprietary models (e.g. ChatGPT) to generate commonsense data which is released publicly as the Nova Atomic knowledge graph. This allows openness and auditing.
- Data uses an open format with natural language for relations instead of a restricted set. Allows flexible downstream usage.
- Data augmented with human annotations of plausibility. Used to train both a plausibility model and improved generative model NovaCOMET.

Key Contributions:
- Symbolic knowledge distillation methodology to create an open, auditable commonsense knowledge pipeline from opaque proprietary models.
- NovaCOMET model which explicitly captures commonsense knowledge while still allowing flexible integration into downstream tasks.
- Analysis showing NovaCOMET matches or exceeds comparable open task models on commonsense generation benchmarks.
- Demonstrates advantage of explicit commonsense knowledge modeling in addition to just task tuning of models.

In summary, the paper introduces an open methodology for distilling and modeling commonsense knowledge which results in NovaCOMET, a model showing strong performance on commonsense tasks while maintaining interpretability and auditability.


## Summarize the paper in one sentence.

 Here is a one-sentence summary of the key points from the paper:

The paper introduces NovaCOMET, an open commonsense knowledge model combining the advantages of both knowledge models and general task models; it leverages opaque proprietary models to create an auditable open knowledge pipeline, generates and annotates commonsense data, then trains flexible open-format models which match or exceed comparable task models on commonsense reasoning benchmarks.


## What is the main contribution of this paper?

 The main contribution of this paper is presenting NovaCOMET, an open commonsense knowledge model that combines the advantages of both knowledge models and general task models. Specifically:

- NovaCOMET allows open-format relations by using natural language queries instead of a fixed set of relations. This enables direct application to reasoning tasks, overcoming limitations of previous knowledge models.

- Compared to general task models like Flan-T5 that are tuned for broad language tasks, NovaCOMET explicitly centers on modeling commonsense knowledge. This allows it to achieve superior performance on commonsense reasoning benchmarks.

- The paper introduces a pipeline to leverage opaque proprietary models to create an open, auditable commonsense knowledge base (NovaAtomic) and model (NovaCOMET). This allows studying where gains are coming from.

- NovaCOMET uses an open training objective with flexible masking, allowing it to handle more diverse commonsense tasks compared to fixed-format knowledge models.

In experiments, NovaCOMET matches or exceeds comparable open task models on commonsense generation tasks, demonstrating the advantage of explicit commonsense knowledge modeling.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and concepts include:

- NovaCOMET - The name of the open commonsense knowledge model presented in the paper. Combines advantages of knowledge models and general task models.

- NovAtomic - The name of the openly released commonsense knowledge graph generated in the paper by distilling knowledge from large language models. Can be audited and filtered.

- Symbolic knowledge distillation - The process used to create NovAtomic by extracting structured knowledge representations from opaque proprietary models. 

- Open format relations - Using natural language queries instead of fixed relation tokens as relations in the knowledge graph and model. Allows more flexibility.

- Commonsense field masking - Randomly masking different fields (context, query, inference) during NovaCOMET model training to allow maximum flexibility in usage.

- Knowledge plausibility - Collecting human annotations on plausibility to improve the quality of the NovaCOMET model, through filtering or reward conditioning approaches.

- Commonsense reasoning - Testing performance on question answering, abductive reasoning, and explanation generation tasks requiring commonsense knowledge and inference.

In summary, key ideas include leveraging proprietary LLMs to produce open commonsense resources, using an open format representation, and showing strong commonsense reasoning ability from explicitly modeling knowledge.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in this paper:

1. The paper proposes an open pipeline to leverage opaque proprietary models to create commonsense knowledge resources. What are some of the key benefits and potential drawbacks of relying on proprietary models versus open models for this task?

2. The paper uses natural language queries instead of fixed relations to connect context and inference. How does this impact the flexibility and coverage of the resulting knowledge graph compared to prior work? What are some limitations?  

3. The paper explores different methods to incorporate human plausibility annotations, including filtering and reward conditioning. What are the relative tradeoffs between these approaches? When would one be preferred over the other?

4. Commonsense field masking is used to train the model to predict any subset of the context/query/inference fields. What downstream use cases does this enable compared to prior knowledge models? How does it impact sample efficiency?

5. The paper demonstrates strong performance on commonsense QA datasets. To what extent do you think these datasets sufficiently evaluate commonsense reasoning versus pattern matching or dataset biases? What additional evaluations could be informative?  

6. What types of commonsense knowledge do you think the current pipeline might be lacking or biased on? How could the contexts prompts, queries, or annotation process be improved to mitigate this?

7. The paper uses a pretrained model initialized from T5. How might performance change if a different model architecture was used instead? What would be the tradeoffs?

8. What are some of the key societal considerations and limitations around relying on proprietary models for commonsense knowledge distillation? How might these be addressed?   

9. The paper focuses on English knowledge. What types of advances would be needed to apply similar methods to other languages? What new challenges might arise?

10. The paper demonstrates combining human annotations with model generations. What other hybrid human-AI methods could be beneficial for improving the resulting data or model?
