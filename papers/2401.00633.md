# [On Discprecncies between Perturbation Evaluations of Graph Neural   Network Attributions](https://arxiv.org/abs/2401.00633)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- There is a lack of agreement between different graph neural network (GNN) attribution methods, which identify important edges/relationships in graphs for model predictions. It is unclear which attributions can be trusted.

- Existing evaluation methods like fidelity vs sparsity have issues - removing edges changes data distribution. Retraining evaluation is proposed to overcome this but has not been adapted to graphs. 

Methodology:
- The paper introduces a retraining evaluation framework for graphs, with two strategies:
   - RoMie: Retrain on Most Important Edges
   - RoLie: Retrain on Least Important Edges

- They provide guidelines on how to interpret the retraining results to evaluate attribution methods. For example, RoMie checks if identified edges are predictive, RoLie checks if predictive edges are missed.

- The framework is demonstrated on GNNExplainer, GradCAM, PGExplainer, SubgraphX and a Random baseline across 5 graph datasets using GCN and GIN architectures.

Key Findings:
- No single best method. Performance depends on dataset, network architecture and sparsity level. 

- GNNExplainer performs similar to random baseline, challenging claims of its superiority. PGExplainer is most variable. GradCAM relatively more stable.

- For a given sparsity, one method can outperform others. So first fix sparsity then compare methods using retraining framework + visualization.

Contributions:
- Adapt retraining concept from vision to graphs to evaluate attributions 

- Guidelines to reliably interpret retraining results and compare methods

- Insights challenging previous claims of attribution methods' performance

- Demonstrate need to evaluate explanations before use based on conditions

The paper concludes retraining can evaluate attributions for a particular network, dataset and sparsity level but cannot serve as a generalized benchmark across conditions.


## Summarize the paper in one sentence.

 Here is a one-sentence summary of the key points from the paper:

The paper introduces guidelines to evaluate graph neural network attribution methods through retraining on most and least important edges, showing attribution performance varies across datasets, networks, and sparsity levels rather than having one best method.


## What is the main contribution of this paper?

 Based on my understanding, the main contributions of this paper are:

1) Adapting the concept of retraining, previously applied in the vision domain, and reformulating it into the realm of graph data and graph neural networks (GNNs) to evaluate graph attribution methods. 

2) Through extensive experiments across five datasets, evaluating the performance of four renowned graph attribution methods (GNNExplainer, PGExplainer, GradCAM, SubgraphX) and a random baseline.

3) Proposing a guidebook outlining a procedural approach for interpreting attribution results and comparing different methods using the retraining evaluation framework. 

4) Challenging previous claims that some attribution methods consistently outperform others, by demonstrating variability in performance across different datasets, networks, and sparsity levels. The paper shows that calling a method the "best" depends on specific conditions.

5) Recommending that the retraining evaluation strategy should be used as a tool to evaluate attributions for a specific addressed network, dataset, and sparsity level rather than as a generalized benchmark.

In summary, the key innovation is adapting retraining for graph attribution evaluation, while the main takeaway is that there is no single best method and the evaluation should be tailored to particular conditions.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and concepts include:

- Graph neural networks (GNNs)
- GNN explanations/interpretability
- GNN attribution methods (GradCAM, GNNExplainer, PGExplainer, SubgraphX)
- Retraining evaluation of attributions
- Reformulation of retraining framework
- Complementary analysis with RoMie (Retrain on Most Important Edges) and RoLie (Retrain on Least Important Edges)
- Elimination of isolated nodes after attribution
- Attribution performance dependence on datasets, network architectures, sparsity levels
- Guidelines and procedural approach for analyzing and comparing attribution methods
- Challenging claims of single best-performing attribution method

The paper introduces a retraining framework to evaluate different GNN attribution methods on synthetic and real-world graph classification datasets. It reformulates the previous retraining concept to address certain issues. The key ideas include complementary analysis of retaining most vs least important edges, eliminating isolated nodes, and comparing methods across varying conditions. The results demonstrate variability in attribution performance, challenging notions of consistent superiority for any single method. Overall, it provides guidebooks on rigorously assessing and interpreting GNN attributions.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 potential in-depth questions about the method proposed in this paper:

1. The paper proposes a reformulation of the retraining evaluation framework to address issues with the previous formulation. What were the key issues with the previous formulation that motivated this reformulation? How does the new proposed formulation address those issues?

2. The paper evaluates attribution methods by retraining the network on the most and least important edges determined by the attribution. Why is it important to look at both the most and least important edges rather than just the most important ones? What complementary insights do the two analyses provide?

3. The paper finds that the performance of attribution methods varies substantially across different datasets, networks, and sparsity levels. What explanations are provided for why attribution performance is so variable and dataset/network/sparsity dependent? What are some examples highlighted of this variability?  

4. The paper concludes that calling an attribution method the "best" is not reliable since performance depends so much on the specific conditions. What guidelines are then proposed for how to fairly evaluate and compare attribution methods? When can one method be reliably called better than another?

5. The most renowned attribution method, GNNExplainer, is found to perform similarly to a random edge importance assignment. Why is this result surprising? What explanations are provided for why GNNExplainer fails to highlight predictive attributions?

6. For what reasons does the paper recommend eliminating isolated nodes that emerge after the attribution process? What evidence is provided that keeping or removing isolated nodes impacts results?

7. The paper finds PGExplainer to have highly variable performance across settings. What examples are highlighted of when PGExplainer succeeds or fails to precisely identify important motifs? What factors might explain this variability?

8. For what reasons does the paper argue that using an unperturbed test set is better than a perturbed one for evaluating attribution methods via retraining? What issues could arise from using a perturbed test set instead?

9. The paper introduces a new synthetic graph classification dataset called BA3Motifs. How does this dataset differ from the widely used BA2Motifs dataset? What additional insights did experiments on BA3Motifs provide compared to BA2Motifs?

10. The retraining concept was previously applied in the vision domain but is new to graphs. What modifications or innovations were required to adapt the retraining framework to graph data and GNNs? How might the graph analysis prompt new perspectives on retraining evaluations more broadly?
