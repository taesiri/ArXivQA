# [Audio-Visual LLM for Video Understanding](https://arxiv.org/abs/2312.06720)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper "Audio-Visual LLM for Video Understanding":

Problem:
- Videos contain rich multimodal information including visual, audio, and language. However, most prior works focus only on the visual modality and do not fully utilize the audio modality for video understanding. 
- Existing large language models (LLMs) have shown significant ability in instruction following for textual data. But extending them to multimodal video understanding remains challenging due to the difficulty in aligning different modalities like visual, audio, and language.

Proposed Solution:
- Proposes Audio-Visual LLM, a multimodal LLM framework that aligns both visual and audio modalities to enable an LLM to understand videos.
- Introduces modality-augmented training to train the model end-to-end with samples of different modalities (visual-only, audio-only, audio-visual) within the same batch. This explores interplay between modalities.
- Designs specialized prompt templates leveraging GPT-4 to create a high-quality multimodal video instruction dataset encompassing detailed captions, multi-turn conversations, complex reasoning, etc.

Main Contributions:
- Modality-augmented training strategy to enable joint training and alignment of visual, audio, and text modalities.
- High-quality video instruction dataset across visual-only, audio-only, and audio-visual modalities to power the training.
- Audio-Visual LLM framework outperforming state-of-the-art on multiple video QA benchmarks by 4-6% and audio captioning benchmarks by 2-3%.
- Demonstrates strong zero-shot generalization ability across a range of video and audio understanding tasks.

In summary, the paper proposes an effective way to synergistically align visual, audio and text modalities within an LLM using specialized training strategies and datasets to achieve state-of-the-art video understanding.
