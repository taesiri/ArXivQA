# [Optimal Transport for Structure Learning Under Missing Data](https://arxiv.org/abs/2402.15255)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
This paper addresses the problem of learning causal structures from data with missing values, which introduces a chicken-and-egg dilemma. Robust imputation requires considering the causal dependencies among variables, but the causal structure is unknown. On the other hand, simply imputing the missing values using standard methods and then performing causal discovery leads to sub-optimal performance. This is empirically demonstrated in the paper. Therefore, specialized methods are needed that can perform causal discovery directly from data with missing values.

Proposed Solution: 
The paper proposes a method called OTM (Optimal Transport for Missing data) for causal structure learning from missing data. The key idea is to formulate the problem as finding a structural causal model (SCM) that makes the model distribution closest in Wasserstein distance to the empirical distribution of the observed data. This avoids needing to explicitly model the missing data distribution or mechanism. 

OTM uses a stochastic map to map the distribution of the imputed data to the model distribution induced by the candidate SCM. It then minimizes the expected reconstruction error between the original imputed data and the data reconstructed from the samples of the model distribution. An additional distribution matching term based on maximum mean discrepancy is used to ensure the model distribution matches what the stochastic map produces. The method can use any existing causal discovery algorithm for complete data as a base.

Main Contributions:
- Formulates causal structure learning from missing data as an optimal transport problem between the empirical and model distributions.
- Proposes a theoretically-grounded objective function that avoids needing to explicitly model missing data distribution.  
- Can flexibly incorporate any score-based causal discovery method for complete data.
- Empirically demonstrates state-of-the-art performance on both synthetic and real-world datasets with various missingness mechanisms. 
- Analysis shows the method is more robust and scalable compared to existing approaches like MissDAG.

In summary, the paper makes important theoretical and practical contributions towards enabling reliable causal discovery directly from incomplete observational data.


## Summarize the paper in one sentence.

 This paper proposes OTM, an optimal transport framework for learning causal graphs from data with missing values, which jointly optimizes an imputation model and causal discovery model to find a graph that best fits the observed data distribution.


## What is the main contribution of this paper?

 According to the paper, the main contribution is proposing OTM, which is an optimal transport framework for learning causal graphs under missing data. Specifically:

- OTM is based on the idea of fitting structure learning into a density fitting problem, where the goal is to find the causal model that induces a distribution with minimum Wasserstein distance to the distribution over the observed data. 

- OTM provides a theoretical development that renders a tractable formulation of the optimal transport cost to optimize the model distribution on the missing data.

- The paper proposes a flexible framework that can accommodate any existing score-based causal discovery algorithm designed for complete data. 

- Through experiments on synthetic and real-world data, OTM is shown to accurately recover the true causal graphs more effectively than baseline methods. It also exhibits superior scalability as the graph complexity increases.

So in summary, the main contribution is an optimal transport-based method for causal structure learning that is robust, flexible, scalable and shows strong empirical performance in the presence of missing data.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and keywords associated with it are:

- Causal discovery
- Missing data
- Optimal transport
- Wasserstein distance 
- Score-based methods
- Directed acyclic graphs (DAGs)
- Structural causal models (SCMs)
- Expectation-maximization (EM)
- Imputation methods
- Maximum mean discrepancy (MMD)
- Nonlinear additive noise models (ANMs)
- MCAR, MAR, MNAR (missing data mechanisms)

The paper proposes an optimal transport framework called OTM for learning causal graphs from data with missing values. It leverages the Wasserstein distance and distribution matching to jointly optimize the imputation and causal discovery components. The method is compared against baselines using expectation-maximization and imputation on simulated nonlinear models and real-world biological datasets. The key terms reflect the problem setting, proposed approach, techniques used, and experimental setup.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 potential in-depth questions about the method proposed in the paper:

1. The paper proposes learning the causal graph by minimizing the Wasserstein distance between the model and empirical distributions. Why is the Wasserstein distance more suitable for this problem compared to other divergences like KL divergence? What are the benefits and limitations?

2. Theorem 1 introduces a stochastic map $\phi$ that transports the pseudo-complete distribution to the model distribution. What is the intuition behind this mapping and how does optimizing it help learn the causal graph? Explain in detail.

3. The paper claims the proposed method OTM is flexible to incorporate any score-based causal discovery algorithm. How can we integrate a method like DAG-GNN with OTM? What changes need to be made? Discuss the implementation details. 

4. One component of the OTM objective is to match the distribution induced by $\phi$ and the model $f_\theta$. The paper uses MMD but other options like adversarial training exist. Compare using MMD versus an adversarial loss for this term.

5. The imputation network in OTM seems similar to the proposal distribution in MissDAG. What are the key differences in how they are modeled and optimized? Explain their roles.  

6. Figure 1 shows "good" imputation does not imply good causal discovery. Provide some analysis on why this could happen and how OTM avoids this issue.

7. The acyclicity regularizer $R(W)$ used in OTM can be NOTEARS, polynomial or DAGMA terms. Compare the benefits and limitations of choosing each option. Which one is most suitable?

8. The paper focuses on additive noise models. How can we extend the method to non-ANMs? What changes would be needed theoretically and implementation-wise?

9. MissDAG utilizes rejection sampling when the posterior is intractable. Explain limitations of this and how OTM avoids explicit posterior inference.

10. The paper claims OTM exhibits faster runtime than MissDAG. Provide some analysis on the computational complexity of each algorithm and explain reasons behind superior scalability.
