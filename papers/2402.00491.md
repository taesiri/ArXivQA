# [EXMOS: Explanatory Model Steering Through Multifaceted Explanations and   Data Configurations](https://arxiv.org/abs/2402.00491)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
The paper investigates how different types of explanations, specifically global model-centric and data-centric explanations, impact domain experts' ability to improve machine learning models through configuration of the training data. Prior work has shown explanations can help users understand and trust AI systems, but it is unclear how explanations influence experts to steer models by modifying training data. This is an important capability for improving models, especially in domains like healthcare.

Proposed Solution: 
The authors developed an "explanatory model steering" (EXMOS) system prototype for diabetes prediction that provides both data-centric and model-centric global explanations. It allows healthcare experts to manually configure the training data or use automated corrections to resolve data issues. In two user studies with 70 and 30 healthcare experts, the authors evaluated 3 dashboard versions: data-centric only, model-centric only, and a hybrid with both explanation types.

Key Findings:
- Hybrid dashboard users improved model accuracy the most effectively and efficiently through manual data configuration, despite higher perceived workload. Model-centric explanations alone were insufficient to guide improvements.  

- Data-centric explanations helped users better understand system changes after reconfiguring data. They increased user trust by highlighting data issues underlying quality score.

- Allowing manual configuration gave users more flexibility and control. Peer approval for changes was suggested to safeguard against removal of important training data.

Main Contributions:
1) Designs of global data-centric, model-centric, and hybrid explanation dashboards plus manual and automated data configuration mechanisms for an EXMOS system.

2) Quantitative and qualitative evaluation with 100+ healthcare experts comparing explanation types for model improvement, trust and understanding.

3) Design guidelines for effective explanation-driven interactive ML systems, including combining explanation types, allowing manual configuration, approving changes, etc.


## Summarize the paper in one sentence.

 The paper investigates the influence of different types of global explanations, including data-centric, model-centric, and hybrid, on healthcare experts' ability to configure training data and improve prediction model performance, finding that a hybrid approach combining both data-centric and model-centric explanations is most effective.


## What is the main contribution of this paper?

 According to the paper, the main contributions are:

1. The authors instantiated generic designs of global data-centric explanations, model-centric explanations, and a hybrid version through their healthcare-focused EXMOS system prototype. They proposed visualisation and interaction designs for explanations and data configurations to facilitate domain experts in model steering.

2. The authors evaluated the impact of different explanation types (data-centric, model-centric, hybrid) on model steering through two extensive user studies with healthcare experts. Their findings indicate that the hybrid combination of global explanations was the most effective and efficient for steering models. 

3. Based on the results of the user studies, the authors presented design guidelines for explanations and data configuration mechanisms to facilitate domain experts in model steering. These include guidelines such as combining global data-centric and model-centric explanations, allowing easy rollback during data configurations, and the importance of peer approval for manual configurations.

In summary, the key contributions are: (1) EXMOS system design and prototype, (2) Quantitative and qualitative evaluation of different explanation types, (3) Design guidelines for effective model steering by domain experts.


## What are the keywords or key terms associated with this paper?

 Based on my analysis of the paper's content, some of the key terms and keywords associated with this paper include:

- Explainable AI (XAI)
- Interactive Machine Learning (IML) 
- Explanatory Model Steering (EXMOS)
- Data-centric explanations
- Model-centric explanations 
- Global explanations
- Local explanations
- Training data configuration
- Model improvement
- Prediction model performance
- Trust 
- Understandability
- Healthcare experts
- Domain experts

The paper introduces the concept of Explanatory Model Steering (EXMOS) which utilizes Explainable AI (XAI) and Interactive Machine Learning (IML) to allow users, specifically healthcare experts and domain experts, to improve machine learning models by configuring the training data. It studies the impact of different types of explanations like global vs local, data-centric vs model-centric on the model improvement process and on the users' trust and understanding of the system. Key terms like training data configuration, model performance, trust, understandability are used to assess the usefulness of explanations for domain experts in steering ML models.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in the paper:

1. What were the different types of global explanations evaluated in the EXMOS system? Can you describe the key differences between data-centric and model-centric explanations?

2. What were the different data configuration mechanisms supported in the EXMOS system? Can you explain the key differences between manual configuration and automated configuration?

3. Why did the authors develop 3 different versions (DCE, MCE, HYB) of the explanation dashboard? What was each version designed to evaluate?

4. What metrics were used to evaluate the impact of different explanation dashboards on model improvement, trust and understanding? Can you describe what each metric aimed to measure?  

5. The paper mentions "effectiveness" and "efficiency" as additional metrics for evaluating the data configuration mechanisms. Can you explain what these two metrics represent and how they were calculated?

6. What were some of the key qualitative insights that justified the superiority of data-centric explanations over model-centric ones during model steering? Can you summarize 2-3 major themes from the thematic analysis?

7. One of the conclusions was that local explanations can enhance the usefulness of global explanations. Can you suggest 2-3 types of local explanations that could complement the global explanations?

8. The paper proposed some guidelines for designing explanations and data configuration mechanisms. Can you list 3 of these recommendations and the rationale behind them?

9. What were some of the limitations of the user studies mentioned in the paper? How could these limitations be addressed in future work?

10. The paper mentions exploring conversation-based explanations in future work. What do you think is the hypothesis behind using conversation-based approaches? How could it impact model steering?
