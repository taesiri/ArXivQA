# [$\infty$Bench: Extending Long Context Evaluation Beyond 100K Tokens](https://arxiv.org/abs/2402.13718)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Processing and reasoning over long contexts is crucial for many practical applications of large language models (LLMs), such as document comprehension and agent construction. 
- However, LLMs face challenges handling long contexts due to computational constraints during training and issues generalizing to longer sequences.
- Most LLMs are restricted to sequences under 8K tokens. Recent models claim to handle over 100K tokens, but benchmarks focus on 10K tokens.

Proposed Solution:
- The authors propose \OURSSPACE, the first LLM benchmark featuring average context lengths over 100K tokens.
- \OURSSPACE has 12 tasks spanning retrieval, code, math, novels, and dialogue in both English and Chinese. 
- The tasks require understanding long dependencies beyond short passage retrieval.
- The benchmark includes realistic human-annotated tasks and synthetic auto-generated tasks that scale to any length.

Key Contributions:  
- \OURSSPACE is the first multi-domain bilingual benchmark to evaluate long context understanding beyond 100K tokens.
- Experiments show current state-of-the-art long context LLMs still struggle to effectively process 100K+ contexts.  
- Analysis provides insights into LLM behavior on long contexts, identifying limitations and directions to improve.
- The benchmark and analysis indicate significant advancements are still required for LLMs to handle long contexts.
