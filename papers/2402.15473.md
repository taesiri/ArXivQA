# [Leveraging Domain Knowledge for Efficient Reward Modelling in RLHF: A   Case-Study in E-Commerce Opinion Summarization](https://arxiv.org/abs/2402.15473)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Reinforcement learning from human feedback (RLHF) is an effective strategy to align language models with human values and goals. However, it requires large amounts of human preference annotation data (tens of thousands of samples), which is challenging to collect for diverse downstream tasks where human values vary.

Proposed Solution:  
- Introduce a novel reward modeling technique that leverages domain knowledge to reduce the amount of human annotation needed. Hypothesis is that by infusing inductive bias into the reward model based on domain knowledge, the model can achieve alignment with fewer preference samples.

- Validate approach for e-commerce opinion summarization. Reward model uses 7 domain-specific features identified through domain expert consultation to reflect desired properties of good summaries. Train on much smaller preference dataset of just 940 samples.

Contributions:
- New reward modeling technique to efficiently achieve human alignment with modest annotation data by utilizing domain knowledge 

- Advances state-of-the-art in opinion summarization based on human evaluations, using 21x less preference data than prior work  

- Release new e-commerce opinion summarization training dataset (PromptOpinSumm) and human preference dataset (OpinPref) to enable further research

- Analysis shows trained model achieves better performance on domain features that drive human preferences, indicating approach helps model align with human goals

Overall, the key innovation is efficiently integrating domain knowledge into the reward modeling to reduce reliance on large preference datasets and enable wider applicability of RLHF. Demonstrated for opinion summarization and has potential for other domains.
