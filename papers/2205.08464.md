# [Robust Losses for Learning Value Functions](https://arxiv.org/abs/2205.08464)

## What is the central research question or hypothesis that this paper addresses?

This paper introduces and analyzes new loss functions for reinforcement learning based on the Huber loss and absolute loss, called the mean Huber Bellman error (MHBE) and mean absolute Bellman error (MABE). The central hypothesis is that these robust loss functions can define better solutions and enable more stable optimization compared to the commonly used mean squared Bellman error. Specifically, the key research questions addressed in this paper are:- What are the theoretical properties and solutions of the MHBE and MABE loss functions? How do they compare to the mean squared Bellman error?- Can we derive sound online optimization algorithms to minimize these robust losses, enabling their use in complex RL systems? - Do algorithms based on these robust losses exhibit improved stability and performance over algorithms based on the squared loss?To address these questions, the paper:- Analyzes the solutions of the MHBE and MABE losses on carefully constructed MDPs, showing they often define better solutions than the squared loss- Provides a biconjugate reformulation of the losses to enable gradient-based optimization- Derives online off-policy algorithms for prediction and control settings- Empirically demonstrates the improved stability and performance of the proposed algorithms over baselinesIn summary, the central hypothesis is that using robust loss functions like the MHBE can enable better solutions and more stable optimization for reinforcement learning. The paper provides theoretical analysis, novel algorithms, and empirical validation to support this hypothesis.


## What is the main contribution of this paper?

Based on my reading of the paper abstract, it seems the main contributions of this paper are:1. Introducing two new robust loss functions for reinforcement learning - the mean absolute Bellman error (MABE) and the mean Huber Bellman error (MHBE). 2. Analyzing the solutions of these new loss functions on several problem settings, showing they often define better solutions than the commonly used mean squared Bellman error.3. Deriving gradient-based algorithms to minimize the MHBE and MABE in online, off-policy settings using function approximation.4. Proposing a new algorithm called QRC-Huber for nonlinear control that minimizes the MHBE and demonstrating it significantly outperforms benchmarks on several domains.In summary, the key ideas appear to be introducing new robust loss functions for RL based on the absolute and Huber errors, analyzing their solutions, deriving online learning algorithms for them, and empirically showing the benefits over existing methods, especially for control with nonlinear function approximation. The use of robust losses seems to improve stability and provide better solutions on several problem settings compared to the commonly used squared error.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points made in the paper:The paper introduces a new robust loss function called the mean Huber Bellman error for reinforcement learning, analyzes its solutions across problem settings showing it often defines better solutions than squared errors, derives gradient-based algorithms to minimize this loss, and demonstrates its utility with a new control algorithm called QRC-Huber that outperforms existing methods.


## How does this paper compare to other research in the same field?

This paper introduces a novel loss function, the mean Huber Bellman error (MHBE), for reinforcement learning. It makes several key contributions:1. It analyzes the solutions of the MHBE across carefully designed problem settings, showing that it often defines better solutions than the commonly used mean squared Bellman error. This provides new insights into when robust losses like the MHBE are preferable. 2. It provides a biconjugate reformulation of the MHBE to enable sound gradient-based optimization. Prior heuristic uses of robust losses in RL depended on semi-gradient updates without convergence guarantees or understanding of the fixed point. 3. It proposes a new nonlinear control algorithm, QRC-Huber, based on optimizing the MHBE. Experiments show QRC-Huber significantly outperforms benchmarks on several problems.Overall, this paper makes important theoretical and empirical contributions in using statistically robust losses for reinforcement learning. The key differentiator from prior work is the principled development of algorithms that optimize valid loss functions. This enables analyzing the impact of using robust losses both theoretically and empirically.In terms of related work, this paper builds on recent developments using biconjugate reformulations to optimize the mean squared Bellman error. It generalizes this approach to enable optimizing robust losses like the MHBE. The paper also relates to common practices in RL like clipping rewards, errors, and gradients. However, it shows these practices lack theoretical grounding compared to proper loss functions. The empirical evaluation benchmarks against state-of-the-art algorithms like DQN and distributional RL, showing the benefits of the proposed approach.In summary, this paper provides critical theoretical foundations to use robust losses in RL. The novel algorithms and analysis of their solutions and optimization advance the state-of-the-art in principled, stable deep RL. The paper makes important contributions towards reliable and performant RL agents.


## What future research directions do the authors suggest?

The paper suggests several possible future research directions:- Developing conjugate forms for other robust losses besides the Huber loss, such as the Tukey biweight loss. The authors note that non-convex losses like the Tukey loss pose challenges for reformulation using biconjugates. - Extending the robust Bellman error framework to distributional RL methods that seek to learn other statistics of the return distribution besides the mean, like quantiles. The authors suggest combining robust state-wise losses like the Huber loss with distributional Bellman operators.- Analyzing the effect of the Huber loss when used with linear function approximation. The authors note that the projection in the MHPBE plays a key role, so it would be interesting to study how the projection set changes the solutions.- Improving the performance of saddlepoint optimization methods for nonlinear control. The authors found the gradient-correction methods worked better, so developing better saddlepoint algorithms is an important direction.- Applying insights from robust losses more broadly, for example to policy optimization or actor-critic methods. The authors focused on prediction and Q-learning for control, but robust losses may benefit other algorithms.- Developing theory around the optimization and convergence properties of the proposed algorithms. The authors provide an empirical evaluation, but formal convergence guarantees remain an open challenge.In summary, the main future directions are: extending robust losses to other RL algorithms, developing theory, and improving saddlepoint optimization methods to match gradient-correction methods like QRC-Huber. The key insights around robust losses likely apply more broadly across RL.
