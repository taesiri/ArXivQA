# [Robust Losses for Learning Value Functions](https://arxiv.org/abs/2205.08464)

## What is the central research question or hypothesis that this paper addresses?

This paper introduces and analyzes new loss functions for reinforcement learning based on the Huber loss and absolute loss, called the mean Huber Bellman error (MHBE) and mean absolute Bellman error (MABE). The central hypothesis is that these robust loss functions can define better solutions and enable more stable optimization compared to the commonly used mean squared Bellman error. Specifically, the key research questions addressed in this paper are:- What are the theoretical properties and solutions of the MHBE and MABE loss functions? How do they compare to the mean squared Bellman error?- Can we derive sound online optimization algorithms to minimize these robust losses, enabling their use in complex RL systems? - Do algorithms based on these robust losses exhibit improved stability and performance over algorithms based on the squared loss?To address these questions, the paper:- Analyzes the solutions of the MHBE and MABE losses on carefully constructed MDPs, showing they often define better solutions than the squared loss- Provides a biconjugate reformulation of the losses to enable gradient-based optimization- Derives online off-policy algorithms for prediction and control settings- Empirically demonstrates the improved stability and performance of the proposed algorithms over baselinesIn summary, the central hypothesis is that using robust loss functions like the MHBE can enable better solutions and more stable optimization for reinforcement learning. The paper provides theoretical analysis, novel algorithms, and empirical validation to support this hypothesis.
