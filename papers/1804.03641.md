# [Audio-Visual Scene Analysis with Self-Supervised Multisensory Features](https://arxiv.org/abs/1804.03641)

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper addresses is: Can a useful multisensory audio-visual representation be learned in a self-supervised way by training a neural network to predict whether video frames and audio are temporally aligned?The key hypothesis is that training a neural network on the pretext task of detecting misalignment between audio and visual streams will force the network to fuse visual motion with audio information and thereby learn an audio-visual feature representation that is useful for downstream tasks like sound source localization, audio-visual action recognition, and on/off-screen audio source separation.The main goal is to show that an effective multisensory representation that combines audio and visual modalities can be learned without manually labeled data, using only the supervision signal of whether audio and video streams are synchronized or temporally misaligned.
