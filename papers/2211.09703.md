# [EfficientTrain: Exploring Generalized Curriculum Learning for Training   Visual Backbones](https://arxiv.org/abs/2211.09703)

## What is the central research question or hypothesis that this paper addresses?

 Here are a few key points about the central research question addressed in this paper:

- The paper aims to improve the training efficiency of modern deep visual backbones like convolutional neural networks (CNNs) and vision transformers (ViTs). The goal is to reduce the computational cost and training time required for state-of-the-art models without sacrificing accuracy. 

- The key hypothesis is that each training example contains both "easier-to-learn" and "harder-to-learn" patterns. The paper proposes exposing only the easier patterns in the beginning and gradually introducing harder ones later in training.

- The main research questions are: 1) How to identify and extract the easier-to-learn patterns in images? 2) How to design a training curriculum that progressively transitions from easier to harder patterns?

- To address the first question, the paper shows both theoretically and empirically that lower frequency image components are easier to learn. It proposes frequency cropping to extract these. 

- For the second question, it designs a simple, unified curriculum called EfficientTrain that starts with lower frequency patterns and weaker augmentation, then transitions to full images and stronger augmentation.

- The central hypothesis is that this curriculum, despite its simplicity, will improve training efficiency across diverse model architectures, datasets, and training configurations. Experiments validate this.

In summary, the core research contribution is a generalized curriculum learning approach to improve training efficiency, enabled by novel frequency-based analysis and a simple but effective EfficientTrain algorithm. Comparisons show superior efficiency gains over prior arts like progressive learning.


## What is the main contribution of this paper?

 Here are the key contributions of this paper:

- It proposes a generalized perspective on curriculum learning that goes beyond selecting easier vs harder examples, and instead extracts "easier to learn" patterns within each example. 

- It provides both theoretical and empirical evidence that lower frequency image components are easier for models to learn initially. This leads to a simple but effective curriculum based on low-frequency cropping of images early in training.

- It shows weaker data augmentation also exposes easier patterns and incorporates this into the curriculum. 

- It develops a greedy search algorithm to optimize the curriculum schedule over training epochs.

- It demonstrates consistent and significant improvements in training efficiency across a wide variety of state-of-the-art ConvNet and Transformer models on ImageNet, with 1.5x speedups and no loss in accuracy.

- The method is simple, generalizable, and does not require model architecture changes or hyperparameter tuning.

So in summary, the key novelty is the generalized curriculum learning formulation and its effective instantiation for image models based on frequency analysis. This leads to a new training approach that accelerates training for modern visual backbones. The simplicity and generalizability are also notable compared to prior curriculum methods.

\textbf{Meta-learning.}
Our work is related to meta-learning \cite{schmidhuber1992learning, bengio1990learning, schmidhuber1996simple, santoro2016meta, finn2017model, snell2017prototypical, rajeswaran2019meta, javed2019meta}, in terms of our high-level training goal. We aim to optimize the model adaptability by changing the input examples over time. Nevertheless, we present concrete and simple implementations based on theoretical analysis, instead of meta-optimization. Our algorithm does not require any additional model or optimization process.

% \vspace{-1.5ex}
\section{Limitations and Potential Negative Impacts}
% \vspace{-1ex}

Like most methods to improve training efficiency, EfficientTrain may contribute to the reduced monetary cost, carbon emission, and time investment required for developing or applying deep learning models. However, the gains come at the risk of encouraging the overuse of large models. We encourage the community to allocate the freed resources to research on more important remaining issues in AI, such as robustness, interpretability and fairness. Regarding the limitations: 1) Currently EfficientTrain focuses on computer vision. Extension to other modalities like text may require novel designs. 2) Very large models may need specialized optimization of the curriculum. 3) Although reducing training cost, EfficientTrain itself requires some compute for designing the curriculum. We will release our curriculum to mitigate this issue. 4) There could be negative environmental impacts if efficiency gains lead to more model training. Thus we emphasize using freed resources responsibly.

% \vspace{-1ex}
\section{Conclusion}
% \vspace{-1ex}

This paper investigated a novel generalized curriculum learning approach. The proposed algorithm, \emph{EfficientTrain}, always leverages all the data at any training stage, but only exposes the `easier-to-learn' patterns of each example at the beginning of training, and gradually introduces more difficult patterns as learning progresses. 
Our method significantly improves the training efficiency of state-of-the-art deep networks on the large-scale ImageNet-1K/22K datasets, for both supervised and self-supervised learning. 


% which always leverages all the data at any training stage, but only exposes the `easier-to-learn' patterns of each example at the beginning of training, and gradually introduces relatively more difficult patterns as learning progresses. 

% Our work may open new avenues for developing computationally more efficient learning algorithms. 

% In specific, we hypothesize that each training example incorporates both `easier-to-capture' patterns and relatively more difficult features, and experimentally show that the former includes at least the low-frequency components of images and the original information before data augmentation. Based on our observations, we propose a curriculum where all training examples are leveraged simultaneously, but only these two types of more learnable patterns are exposed at the beginning stages of training. 

% \textbf{Discussions on potential impacts and limitations} are deferred to Appendix C (due to the limited space).

% For example, it reduces the wall-time cost for training a wide variety of recently proposed deep models (\emph{e.g.}, ConvNeXts, DeiT, PVT, and Swin\ \!/\ \!CSWin Transformers) by more than $1.5\times$ on ImageNet-1K\ \!/\ \!21K, without sacrificing the accuracy or the transferability to downstream tasks.



\section*{Acknowledgements}

This work is supported in part by the National Key R\&D Program of China under Grant 2021ZD0140407, the National Natural Science Foundation of China under Grants 62022048 and 62276150, the National Defense Basic Science and Technology Strengthening Program of China, Beijing Academy of Artificial Intelligence (BAAI), and Huawei Technologies Ltd. 


{\small
\bibliographystyle{unsrt}
\bibliography{IEEEtran}
}



\onecolumn
\appendix

% \vskip 0.1in
{\centering\section*{Appendix for\\``EfficientTrain: Exploring Generalized Curriculum Learning\\for Training Visual Backbones''}}
% \vskip 0.1in

% \vspace{-1ex}
\section{Implementation Details}
% \vspace{-1ex}
\label{app:implementation_details}

% \vspace{-1.5ex}
\subsection{Training Models on ImageNet-1K}
% \vspace{-1ex}

\textbf{Dataset.}
We use the data provided by ILSVRC2012\footnote{\url{https://image-net.org/index.php}} \cite{deng2009imagenet}. The dataset includes 1.2 million images for training and 50,000 images for validation, both of which are categorized in 1,000 classes.

\textbf{Training.}
Our approach is developed on top of a state-of-the-art training pipeline of deep networks, which incorporates a holistic combination of various model regularization \& data augmentation techniques, and is widely applied to train recently proposed models \cite{touvron2021training, wang2021pyramid, liu2021swin, dong2021cswin, liu2022convnet}. Our training settings generally follow from \cite{liu2022convnet}, while we modify the configurations of weight decay, stochastic depth and exponential moving average (EMA) according to the recommendation in the original papers of different models (\emph{i.e.}, ConvNeXt \cite{liu2022convnet}, DeiT \cite{touvron2021training}, PVT \cite{wang2021pyramid}, Swin Transformer \cite{liu2021swin} and CSWin Transformer \cite{dong2021cswin})\footnote{The training of ResNet \cite{He_2016_CVPR} follows the recipe provided in \cite{liu2022convnet}.}. The detailed hyper-parameters are summarized in Table \ref{tab:img_1k_details}. 

The baselines presented in Table \ref{tab:img_1k_main_result} directly use the training configurations in Table \ref{tab:img_1k_details}. Based on Table \ref{tab:img_1k_details}, our proposed EfficientTrain curriculum performs low-frequency cropping and modifies the value of $m$ in RandAug during training, as introduced in Table \ref{tab:EfficientTrain}. 
The results in Tables \ref{tab:varying_epoch} and \ref{tab:img1k_vs_baseline} adopt a varying number of training epochs on top of Table \ref{tab:img_1k_main_result}. 

% In particular, when changing the input size for vision Transformers, we simply perform interpolation on their positional embeddings \cite{touvron2021training, wang2021pyramid, liu2021swin} and vary the size of attention widows \cite{liu2021swin, dong2021cswin} correspondingly, as suggested in their papers.

In addition, the low-frequency cropping operation in EfficientTrain leads to a varying input size during training. Notably, visual backbones can naturally process different sizes of inputs with no or minimal modifications. Specifically, once the input size varies, ResNets and ConvNeXts do not need any change, while vision Transformers (\emph{i.e.}, DeiT, PVT, Swin and CSWin) only need to resize their position bias correspondingly, as suggested in their papers. Our method starts the training with small-size inputs and the reduced computational cost. The input size is switched midway in the training process, where we resize the position bias for ViTs (do nothing for ConvNets). Finally, the learning ends up with full-size inputs, as used at test time. As a consequence, the overall computational/time cost to obtain the final trained models is effectively saved.



\textbf{Inference.}
Following \cite{touvron2021training, wang2021pyramid, liu2021swin, dong2021cswin, liu2022convnet}, we use a crop ratio of 0.875 and 1.0 for the inference input size of 224$^2$ and 384$^2$, respectively.





\begin{table}[h]
  \vskip -0.2in
  \centering
  \begin{footnotesize}
  % 
  \setlength{\tabcolsep}{4mm}{
  \vspace{5pt}
  \renewcommand\arraystretch{1.175}
  \resizebox{0.65\columnwidth}{!}{
  \begin{tabular}{l|c}
  % \toprule
  Training Config & Values / Setups \\
  % \midrule
  % \midrule
  \shline
  Input size & 224$^2$ \\
  Weight init. & Truncated normal (0.2) \\
  Optimizer & AdamW \\
  Optimizer hyper-parameters & $\beta_1, \beta_2$=0.9, 0.999 \\
  Initial learning rate & 4e-3 \\
  Learning rate schedule & Cosine annealing \\
  Weight decay & 0.05 \\
  Batch size & 4,096 \\
  Training epochs & 300 \\
  Warmup epochs & 20 \\
  Warmup schedule & linear \\
  RandAug \cite{cubuk2020randaugment} &  (9, 0.5) \\
  Mixup \cite{zhang2018mixup} & 0.8 \\
  Cutmix \cite{yun2019cutmix} & 1.0 \\
  Random erasing \cite{zhong2020random} & 0.25 \\
  Label smoothing \cite{szegedy2016rethinking} & 0.1 \\
  Stochastic depth \cite{huang2016deep} & Following the values in original papers \cite{liu2022convnet, touvron2021training, wang2021pyramid, liu2021swin, dong2021cswin}. \\
  Layer scale \cite{touvron2021going} & 1e-6 \scriptsize (ConvNeXt \cite{liu2022convnet}) \footnotesize/ None \scriptsize (others) \footnotesize\\
  Gradient clip & 5.0 \scriptsize (DeiT \cite{touvron2021training}, PVT \cite{wang2021pyramid} and Swin \cite{liu2021swin}) \footnotesize / None \scriptsize (others) \footnotesize \\
  Exp. mov. avg. (EMA) \cite{polyak1992acceleration} & 0.9999 \scriptsize (ConvNeXt \cite{liu2022convnet} and CSWin \cite{dong2021cswin}) \footnotesize / None \scriptsize (others) \footnotesize \\
  Auto. mix. prec. (AMP) \cite{micikevicius2018mixed} & Inactivated \scriptsize (ConvNeXt \cite{liu2022convnet}) \footnotesize / Activated \scriptsize (others) \footnotesize \\
  % \bottomrule
  \end{tabular}}}
  \end{footnotesize}
  \vskip -0.1in
  \caption{\textbf{Basic training hyper-parameters for the models in Table \ref{tab:img_1k_main_result}.}}
  \label{tab:img_1k_details}
  \vskip -0.1in
\end{table}

% \vspace{-1ex}
\subsection{ImageNet-22K Pre-training}
% \vspace{-1ex}

\textbf{Dataset and pre-processing.} 
In our experiments, the officially released processed version of ImageNet-22K\footnote{\url{https://image-net.org/data/imagenet21k_resized.tar.gz}} \cite{deng2009imagenet, ridnik2021imagenet} is used. The original ImageNet-22K dataset is pre-processed by resizing the images (to reduce the dataset's memory footprint from 1.3TB to $\sim$250GB) and removing a small number of samples. The processed dataset consists of $\sim$13M images in $\sim$19K classes. Note that this pre-processing procedure is officially recommended and accomplished by the official website.

\textbf{Pre-training.} 
We pre-train CSWin-Base/Large and ConvNeXt-Base/Large on ImageNet-22K. The pre-training process basically follows the training configurations of ImageNet-1K (\emph{i.e.}, Table \ref{tab:img_1k_details}), except for the differences presented in the following. The number of training epochs is set to 120 with a 5-epoch linear warm-up. For all the four models, the maximum value of the increasing stochastic depth regularization \cite{huang2016deep} is set to 0.1 \cite{liu2022convnet, dong2021cswin}. Following \cite{dong2021cswin}, the initial learning rate for CSWin-Base/Large is set to 2e-3, while the weight-decay coefficient for CSWin-Base/Large is set to 0.05/0.1. Following \cite{liu2022convnet}, we do not leverage the exponential moving average (EMA) mechanism. To ensure a fair comparison, we report the results of our implementation for both baselines and EfficientTrain, where they adopt exactly the same training settings (apart from the configurations modified by EfficientTrain itself).

\textbf{Fine-tuning.} 
We evaluate the ImageNet-22K pre-trained models by fine-tuning them and reporting the corresponding accuracy on ImageNet-1K. The fine-tuning process of ConvNeXt-Base/Large follows their original paper \cite{liu2022convnet}. The fine-tuning of CSWin-Base/Large adopts the same setups as ConvNeXt-Base/Large. We empirically observe that this setting achieves a better performance than the original fine-tuning pipeline of CSWin-Base/Large in \cite{dong2021cswin}.

% \vspace{-1ex}
\subsection{Object Detection and Segmentation on COCO}
% \vspace{-1ex}

Our implementation of RetinaNet \cite{lin2017focal} follows from \cite{xia2022vision}. Our implementation of Cascade Mask-RCNN \cite{cai2019cascade} is the same as \cite{liu2022convnet}.

% \vspace{-1ex}
\subsection{Experiments in Section \ref{sec:EfficientTrain_sec4}}
% \vspace{-1ex}

In particular, the experimental results provided in Section \ref{sec:EfficientTrain_sec4} are based on the training settings listed in Table \ref{tab:img_1k_details} as well, expect for the specified modifications (\emph{e.g.}, with the low-passed filtered inputs). The computing of CKA feature similarity follows \cite{raghu2021vision}.




% \newpage

% \vspace{-1ex}
% \section{Algorithm to Search for the Curriculum}
% % \vspace{-1ex}
% \label{app:algorithm}


% To determine a proper schedule for $B$ (\emph{i.e.}, the window size for low-frequency cropping), we propose a greedy search algorithm, which is shown in Algorithm \ref{alg:greedy_search}. We divide the training process into several stages and solve a value of $B$ for each stage. The algorithm starts from the last stage, minimizing $B$ under the constraint of not degrading the performance compared to the baseline (trained with $B=224$). In implementation, we consider the standard 300-epoch training setting proposed in \cite{liu2021swin}. To ensure the generalization performance, we restrict the accuracy in terms of both Swin-Tiny and DeiT-Small. 

% % The search results in the training curriculum presented in Table \ref{sec:EfficientTrain_sec4}, which is named as \emph{EfficientTrain}. Despite its surprising simplicity, our experiments in the next section will indicate that EfficientTrain contributes to a considerably more efficient training process for a wide variety of deep models and experimental configurations on large-scale benchmark datasets.



% \begin{figure}[!h]
%   % \hspace{-2ex}
%   \begin{center}
%     \resizebox{0.9\linewidth}{!}{
%   \begin{minipage}{\linewidth}
%       % \vspace{1ex}
%       \begin{center}
%           % \vskip -0.4in
%               \begin{algorithm}[H]
%                   \caption{Algorithm to Search for Curricula.}
%                   \label{alg:greedy_search}
%               \begin{algorithmic}[1]
%                   \STATE {\bfseries Input:} Number of training epochs $T$ and training stages $N$ (\emph{i.e.}, $T/N$ epochs for each stage).
%                   \STATE {\bfseries Input:} Baseline accuracy $a_0$ (with 224$^2$ images).
%                   % \STATE {\bfseries Input:}
%                   \STATE {\bfseries To solve:} The value of $B$ for $i^{\textnormal{th}}$ training stage: $\hat{B}_i$.
%                   \STATE {\bfseries Initialize:}  $\hat{B}_1=\
