# [Local Augmentation for Graph Neural Networks](https://arxiv.org/abs/2109.03856)

## What is the central research question or hypothesis that this paper addresses?

Based on my understanding, the central research question this paper addresses is whether the neighborhood information is adequately aggregated for learning representations of nodes with few neighbors in graph neural networks (GNNs). The authors argue that the limited number of neighbors for some nodes restricts the expressive power of GNNs and hinders their performance. They propose that generating more features in the local neighborhood via data augmentation could help address this issue.Specifically, the paper introduces a framework called "Local Augmentation for Graph Neural Networks" (LAGNNs) that learns the distribution of node features of neighbors conditioned on the central node's features. It then generates additional feature vectors from this distribution as inputs to the GNN at each training iteration to enrich the local neighborhood information, particularly for nodes with few neighbors. The central hypothesis seems to be that this local augmentation approach can consistently improve GNN performance by enhancing representation learning for low-degree nodes across various GNN architectures and graph learning tasks. The experiments aim to validate whether local augmentation helps different GNN models achieve better performance on node, link and graph classification across diverse benchmark datasets compared to baseline GNNs and other augmentation techniques.In summary, the key research question is whether local augmentation of neighborhood features can enhance graph neural networks' ability to learn effective representations for nodes with few neighbors. The paper proposes and evaluates a method for localized feature augmentation to address this limitation.


## What is the main contribution of this paper?

This paper proposes a data augmentation method called local augmentation to improve the performance of graph neural networks (GNNs). The key points are:- The method generates additional node features by learning the conditional distribution of the features of neighboring nodes given the central node's features. This allows generating plausible node features that enrich the local neighborhood information, especially for nodes with few neighbors. - The augmentation is done by pre-training a conditional variational autoencoder on the graph and sampling augmented features during GNN training. The generative model training is decoupled from the GNN training.- The augmentation method is model-agnostic and can be applied to any GNN architecture in a plug-and-play manner by simply concatenating or averaging the generated node features with the original features.- Experiments show consistent improvements across GNN variants (GCN, GAT, GraphSAGE etc.) and datasets when using this augmentation strategy. For example, it improves test accuracy of GCN and GAT by 3.4% and 1.6% on average over Cora, Citeseer and Pubmed datasets.- The method also achieves state-of-the-art results compared to prior data augmentation techniques and outperforms on nodes with lower degrees, indicating it generates useful local neighborhood features.In summary, the key contribution is a simple yet effective data augmentation technique to improve GNN performance by generating additional local node features using a pre-trained conditional generative model. The plug-and-play applicability to various GNNs is also a notable advantage.
