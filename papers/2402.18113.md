# [Small But Funny: A Feedback-Driven Approach to Humor Distillation](https://arxiv.org/abs/2402.18113)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem: 
Knowledge distillation from large language models (LLMs) to small language models (SLMs) works well for simpler tasks, but there is a substantial performance gap for more complex and creative tasks like humor generation. The gap likely stems from the fact that creative tasks are harder to learn purely through imitation and may require additional guidance.

Proposed Solution:
The paper proposes a novel distillation framework with two phases - imitation followed by critical feedback. In the imitation phase, the SLM is trained on humorous text pairs generated by the LLM teacher. In the feedback phase, the LLM takes on an additional role as a "critic" to evaluate and provide preferences on SLM-generated candidate text pairs. This feedback is then used to further train the SLM.

Key Contributions:
- Novel dual role for the LLM as both a teacher for generating data and a critic for providing feedback to guide the SLM.
- New feedback-based objectives like Direct Policy Optimization (DPO) and BRIO ranking loss to incorporate the critic's preferences.
- Experiments on humor generation revealing that feedback narrows the performance gap between SLMs and LLMs significantly compared to just imitation.
- Analysis of the effect of design choices like feedback frequency and data size on student performance.
- Investigation into potential biases in LLM critics and their effect on student training.

Overall, the work highlights the promise of using feedback as an extra teaching signal alongside imitation for distilling creative abilities like humor generation into smaller, more efficient models. The analysis also motivates future work to mitigate issues in LLM evaluation.


## Summarize the paper in one sentence.

 This paper proposes a novel knowledge distillation framework for humor generation that involves both imitation of a teacher language model's outputs and iterative feedback from the teacher on the student model's outputs to improve humor quality.


## What is the main contribution of this paper?

 Based on my reading, the main contribution of this paper is proposing a novel knowledge distillation framework for humor generation that incorporates both imitation of a teacher large language model (LLM) and critical feedback from the LLM on the student small language model's (SLM) outputs. Specifically:

- They explore distilling humor generation ability from an LLM teacher to an SLM student, which is challenging for SLMs to learn through imitation alone. 

- After an initial imitation phase where the SLM is finetuned on humorous examples from the LLM, they incorporate a critique phase where the LLM provides pairwise comparative feedback on SLM output candidates.

- They experiment with different feedback mechanisms like direct policy optimization (DPO), BRIO ranking loss, and combinations to teach the SLM to differentiate between more and less humorous outputs.

- Evaluations show that SLMs trained with feedback from the LLM teacher significantly outperform imitation only baselines, reaching comparable performance to the teacher LLM on humor generation up to 65% of the time.

So in summary, the key contribution is using an LLM as both a teacher for data and a critic for feedback to more effectively transfer creative humor generation abilities to smaller models via this two-phase distillation approach.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and concepts include:

- Knowledge distillation - The process of transferring knowledge from a large teacher model to a smaller student model. A core concept in the paper.

- Humor generation - The creative language generation task focused on in this work, involving generating humorous paraphrases of input sentences.

- Imitation learning - Initial stage of the proposed framework where the student model is trained by imitating the humorous outputs of the teacher model. 

- Feedback - Supplementary signals from the teacher model to guide and improve the student's humor generation abilities, beyond just imitation.

- Teacher-student framework - The overall learning paradigm with a large teacher model and smaller student model.

- Pairwise scoring - Method of relatively scoring two candidate texts used to obtain feedback signals and mitigate subjectivity.

- Win Tie Rate (WTR) - Automatic evaluation metric measuring if one response wins/ties against another in terms of humor.

- biases - Potential issues such as length and positional biases that can affect the teacher model's ability to reliably evaluate the student.

So in summary, key terms revolve around knowledge distillation, humor generation, use of an iterative teacher-student framework with imitation and feedback, and automatic evaluation using pairwise scoring/comparisons.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in this paper:

1. The paper proposes a two-phase knowledge distillation framework involving imitation and critical feedback. Could you explain in more detail how these two phases work and how they complement each other? 

2. The critic model is used to provide feedback on the quality of the student model's outputs. What are some of the biases and limitations you observed with using an LLM as the critic? How did you try to mitigate issues like length bias and positional bias?

3. You experiment with different loss functions like BRIO, DPO and BRIO-DPO for incorporating feedback. Could you elaborate on what each of these objectives tries to optimize for and why combining them may or may not be beneficial?

4. You vary the frequency of feedback to the student model during training. What trade-offs did you observe between performance gains and computational overhead by providing more frequent feedback? How can curriculum learning or active learning help make this more efficient?

5. The paper evaluates whether creative abilities like humor can be effectively distilled from larger to smaller models. What unique challenges arise when trying to teach creativity through imitation and feedback alone? How could the framework be extended for better transfer of creative skills? 

6. Qualitative analysis revealed that feedback helps narrow the gap between student and teacher outputs. However, some issues like cultural biases persist. How can the student model be prevented from inheriting or amplifying certain biases present in the teacher or the training data?

7. You use in-distribution and out-of-distribution test sets to benchmark model performance. Why is evaluating on out-of-distribution data crucial for assessing robust humor generation abilities? What additional challenges surface when testing on ood inputs?

8. How does the model architecture choice for student vs teacher affect knowledge transfer? What architectural constraints like size, depth and width of the student model limit its creative capacity and ability to learn from feedback when compared to much larger teacher models?

9. The paper focuses on conditional humor generation task where the model paraphrases a given text into a humorous version. Do you think the proposed distillation approach could work for other humor generation tasks like freestyle joke completion? How could it be adapted?

10. You rely on automatic metrics based on LLMs as well as human evaluation for analysis. What are some of the limitations of current humor evaluation methods? Can you suggest any ideas to build better humor evaluation frameworks?
