# [PC-DARTS: Partial Channel Connections for Memory-Efficient Architecture   Search](https://arxiv.org/abs/1907.05737)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question/hypothesis seems to be:Can reducing the redundancy in exploring the network architecture search space via partial channel connections lead to more efficient neural architecture search without compromising performance?The authors propose an approach called Partially-Connected DARTS (PC-DARTS) which samples only a subset of channels for operation selection rather than using all channels. This is aimed at reducing the redundancy and excessive memory/computation costs of jointly training and searching over a full super-network architecture space. The central hypothesis seems to be that by sampling a small part of the super-network, they can perform architecture search more efficiently without compromising the performance of the final discovered architecture. They also introduce an "edge normalization" technique to stabilize the training. Overall, the central research question/hypothesis appears to be whether their proposed PC-DARTS approach can enable more efficient neural architecture search while maintaining or improving performance compared to prior DARTS methods.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contributions appear to be:- Proposing a novel neural architecture search method called Partially-Connected DARTS (PC-DARTS). The key idea is to only propagate a randomly sampled subset of channels through the operation selection module to reduce memory overhead and allow a larger batch size. This makes the search process more efficient. - Introducing an "edge normalization" technique to stabilize the search process. This adds extra learnable parameters to weight each network connectivity edge, making the search robust to the random channel sampling.- Demonstrating state-of-the-art architecture search efficiency on CIFAR-10 and ImageNet datasets. For example, PC-DARTS achieves 2.57% test error on CIFAR-10 in only 0.1 GPU-days.- Showing the architecture found by PC-DARTS transfers well to object detection, achieving top results on COCO using the searched backbone.- Analyzing the tradeoffs between search accuracy and efficiency by adjusting the channel sampling rate. This provides insight into the redundancy of super-network optimization for architecture search.In summary, the main contribution appears to be proposing the PC-DARTS method to greatly improve the efficiency of neural architecture search while finding state-of-the-art performing architectures. The paper also provides useful analysis into the architecture search process.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper proposes a novel neural architecture search method called Partially-Connected DARTS (PC-DARTS) that reduces memory and computation costs by randomly sampling a subset of channels for operation search in each step, allowing the use of larger batch sizes for faster and more stable search without compromising performance.


## How does this paper compare to other research in the same field?

Here are my thoughts on how this paper compares to other research in neural architecture search:- This paper builds on the popular DARTS (Differentiable Architecture Search) approach for NAS. It aims to improve DARTS by reducing the memory and computational requirements during architecture search. This is an important direction since the large search spaces in DARTS lead to inefficiency.- The core ideas are intuitive - sample only a subset of channels during architecture search to reduce redundancy (partial channel connections), and use extra hyperparameters to stabilize the search across different channel samples (edge normalization). These ideas are simple to implement yet effective.- The techniques allow the authors to use much larger batch sizes during architecture search. This improves both speed (4x faster than DARTS on CIFAR-10) and stability (more robust results across multiple runs). Enabling direct architecture search on ImageNet is a notable achievement.- The discovered architectures achieve excellent results on CIFAR-10 and ImageNet classification benchmarks, surpassing prior DARTS variants. The ImageNet top-1 error of 24.2% seems to be state-of-the-art among mobile-efficient models.- The improvements over DARTS are quite significant given the core ideas are simple. This contrasts other works like P-DARTS that achieve better results but require more complex progressive search schemes.- The general ideas of regularization via partial connections and stabilizing the search are novel and could benefit other NAS approaches too. The authors rightly point out the redundancy in one-shot NAS methods.- Comparisons could be expanded to cover more recent works, but the results presented already demonstrate the efficacy of the proposed techniques.Overall, this paper makes excellent progress on tackling efficiency and stability issues in differentiable NAS using simple yet effective ideas. The speed, memory, and accuracy improvements over DARTS are substantial.


## What future research directions do the authors suggest?

Based on my reading of the paper, some of the main future research directions suggested by the authors include:- Developing more stabilized and efficient algorithms for differentiable architecture search. The authors reveal a gap between improving super-network optimization and finding a better architecture, suggesting more work is needed on regularization techniques and larger batch sizes to improve search stability.- Finding more essential solutions, beyond their proposed channel sampling and edge normalization techniques, to incorporate regularization and larger batch sizes for improved stability and efficiency in differentiable NAS.- Applying their proposed techniques like partial channel connections and edge normalization to other NAS algorithms to improve search accuracy and speed. The authors suggest these components could benefit other search algorithms.- Exploring how their insights, like the redundancy of super-network optimization and the gap between search and evaluation, can inspire new techniques or methodologies for efficient and robust neural architecture search.- Testing whether the architectures discovered by their approach can benefit other applications beyond image classification, such as object detection, semantic segmentation, etc. The authors demonstrate strong transferability to object detection.- Developing architectures optimized for other metrics like latency, power usage, or model size rather than solely accuracy. The authors focus on image classification accuracy but suggest constraints could be added for other objectives.In summary, the main directions are improving search stability and efficiency, applying their techniques more broadly, and further exploring the theoretical insights from their work to advance the state-of-the-art in neural architecture search.


## Summarize the paper in one paragraph.

Here is a one paragraph summary of the paper:This paper proposes a novel neural architecture search method called Partially-Connected DARTS (PC-DARTS) that reduces memory and computation costs compared to previous differentiable architecture search methods like DARTS. The key idea is to sample only a subset of channels at each layer to connect to the next layer during architecture search, rather than using all channels. This allows using a much larger batch size, speeding up search and improving stability. To deal with inconsistencies in architecture selection from sampling different channels, they introduce an edge normalization technique which adds additional learnable parameters on each edge to stabilize search. Experiments on CIFAR-10 and ImageNet show PC-DARTS achieves better accuracy than DARTS in significantly less time. For example, on ImageNet it achieves state-of-the-art 24.2% top-1 error under the mobile setting using only 3.8 GPU-days for architecture search. The efficiency and accuracy gains show channel sampling is an effective way to regularize and accelerate neural architecture search.


## Summarize the paper in two paragraphs.

Here is a summary of the paper in two paragraphs:This paper proposes a novel neural architecture search method called Partially-Connected DARTS (PC-DARTS). The key idea is to reduce redundancy in exploring the search space by only sampling and operating on a subset of channels at each layer of the network during architecture search. This allows PC-DARTS to search much more efficiently without compromising performance. Specifically, instead of sending all channels into the block for mixed operation selection like in DARTS, PC-DARTS randomly samples just 1/K of the channels for operation mixture and bypasses the other channels directly in a shortcut. This reduces memory usage, allowing a K times larger batch size during search. Larger batch sizes accelerate search and improve stability. To address inconsistency in edge selection caused by sampling different channels, the authors introduce edge normalization. This adds edge-level parameters to reduce uncertainty in connectivity selection. Experiments on CIFAR10 and ImageNet demonstrate PC-DARTS is faster, more stable, and achieves better performance than DARTS. On CIFAR10, PC-DARTS achieves 2.57% test error in just 0.1 GPU-days, surpassing DARTS' 2.76% in 1 GPU-day. On ImageNet, PC-DARTS achieves state-of-the-art 24.2% top-1 error under the mobile setting using just 3.8 GPU-days for search. This also surpasses ProxylessNAS which used almost double the time. The efficiency and effectiveness of PC-DARTS shows reducing redundancy by sampling channels is a promising approach for fast, stable neural architecture search.


## Summarize the main method used in the paper in one paragraph.

Here is a one paragraph summary of the main method used in this paper:The paper presents a neural architecture search approach called Partially-Connected DARTS (PC-DARTS). The key idea is to reduce the redundancy in exploring the large network architecture space by only sampling and operating on a subset of channels at each node. Specifically, instead of sending all channels into the block for operation selection, they randomly sample 1/K of the channels for operation mixture while bypassing the other channels directly through a shortcut. This greatly reduces memory usage so a larger batch size can be used, resulting in faster and more stable search. However, sampling different channels can cause inconsistency in edge selection, so they introduce edge normalization, which adds new edge-level parameters to reduce uncertainty. This stabilizes the connectivity learned during search. Overall, PC-DARTS reduces computational burdens to perform a more efficient architecture search without compromising performance. Experiments on CIFAR and ImageNet demonstrate the effectiveness of PC-DARTS.
