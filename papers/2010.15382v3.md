# [Learning to Actively Learn: A Robust Approach](https://arxiv.org/abs/2010.15382v3)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question is: 

How can we design an algorithm for active learning tasks like bandit problems that is robust and effective even when the total query budget is very small?

The key points are:

- Existing algorithms for active learning tasks like bandits often rely on concentration inequalities or theoretical guarantees that require a large number of queries to be effective. When the total budget is small (e.g. a few dozen queries), these algorithms do not work much better than passive approaches like random sampling. 

- The authors propose a framework to learn a policy for active data collection that competes with the best policy tailored to each "difficulty level" of problem instances. This allows the learned policy to perform well across problems of varying difficulty, even with a very small budget.

- They use adversarial training techniques to learn policies that are robust. The policies are trained on a sequence of nested sets encoding harder problem instances, trying to minimize the gap between the learned policy's performance and the performance of the best policy for each set.

- This approach does not rely on explicit priors or assumptions about the distribution of problem instances, avoiding issues of prior mismatch. It aims to be competitive with the best policy for every difficulty level.

- The method is evaluated on active learning tasks like bandits derived from real datasets. It outperforms common baselines and shows robustness and near instance-optimal performance even with very few (20) queries.

In summary, the key hypothesis is that this adversarial training framework can produce active learning policies that are robust and effective for small budgets, overcoming limitations of algorithms reliant on statistical guarantees. The experiments on real-world tasks support this hypothesis.


## What is the main contribution of this paper?

 Based on my reading of the paper, the main contribution is proposing a framework for learning robust algorithms for active learning tasks like binary classification and combinatorial bandits. The key ideas are:

- Defining a difficulty measure C(θ) for problem instances θ that captures the intrinsic hardness, often derived from information theoretic lower bounds. This allows instances to be stratified by difficulty. 

- Learning policies that minimize the worst-case gap between their performance and the performance of the best policy for each difficulty level. This encourages robustness across difficulty levels.

- Using adversarial training and gradient-based optimization to learn policies that are competitive with r-dependent baseline policies optimized separately for each difficulty level r. 

- Evaluating the framework on active binary classification (thresholds problem) and combinatorial bandits (20 questions, joke recommendation). The learned policies are shown to be robust and achieve good instance-dependent sample complexity.

In summary, the main contribution is a general framework for learning robust active learning algorithms using ideas from adversarial training and reinforcement learning, evaluated on specific active learning tasks. The goal is to produce effective algorithms even with very small query budgets, unlike theoretically justified algorithms that require large sample sizes.
