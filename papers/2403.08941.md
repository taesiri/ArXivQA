# [Towards Model-Agnostic Posterior Approximation for Fast and Accurate   Variational Autoencoders](https://arxiv.org/abs/2403.08941)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Variational autoencoders (VAEs) are deep generative models that learn a generative model and an inference model.
- In early training, the inference model is poor, leading to optimization getting stuck and negatively impacting the generative model. 
- Existing solutions like iterative training are inefficient. Analytically computing the posterior requires assumptions about the true model which are unavailable.

Proposed Solution:
- Propose an "empiricalized" VAE model that replaces the prior with an empirical distribution to allow estimating posterior probability of a latent code's index instead of location.

- Propose a deterministic, model-agnostic posterior approximation (MAPA) that approximates the posterior of the true model's latent code indices given data. MAPA uses distances between data points and does not require knowing the true model.

- Derive a MAPA-based stochastic lower bound to train the empiricalized VAE. This allows training the generative and inference models independently.

Main Contributions:
- Empiricalized VAE model to allow posterior approximation over indices
- Deterministic, model-agnostic posterior approximation (MAPA) over latent code indices 
- MAPA-based stochastic bound to train empiricalized VAE with independent generative and inference models
- Preliminary results showing MAPA captures posterior trends, and the method is more accurate and efficient than VAE/IWAE baselines

The paper also discusses how to scale the method to high-dimensions, proves connections to autoencoders, and shows MAPA is robust to model non-identifiability.
