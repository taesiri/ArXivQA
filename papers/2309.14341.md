# [Extreme Parkour with Legged Robots](https://arxiv.org/abs/2309.14341)

## What is the central research question or hypothesis that this paper addresses?

 The central hypothesis of this paper is that an end-to-end deep reinforcement learning approach can enable low-cost quadruped robots with imprecise actuation and sensing to perform extreme parkour behaviors directly from raw sensor inputs, without relying on explicit mapping or motion planning. Specifically, the authors hypothesize that a single neural network policy operating on depth images from an egocentric camera can produce precise control for challenging parkour skills like long jumps, high jumps, and walking on two legs.

The key research questions addressed are:

1) Can a unified reward function and terrain curriculum enable a single policy network to learn diverse parkour skills like high jumps, long jumps, ramp traversal, and handstands? 

2) Can distilling a policy that relies on privileged information like scan dots and waypoint directions produce a deployable policy that operates only on onboard depth images and chooses its own heading based on terrain?

3) Can such a policy overcome imprecise actuation and perception to produce precise control on a real low-cost quadruped robot for extreme parkour behaviors like 2x body length long jumps and 2x body height high jumps?

To summarize, the central hypothesis is that end-to-end deep RL can unlock extreme parkour on low-cost legged robots despite imprecise sensing and actuation, by learning adaptive policies that map raw sensor inputs to precise motor commands. The key questions address learning diverse skills with a unified framework, distilling deployable policies, and sim-to-real transfer of these policies.


## What is the main contribution of this paper?

 The main contribution of this paper is developing a method for a legged robot to perform extreme parkour behaviors directly from raw sensor inputs, without explicit mapping or planning. Specifically:

- They train a single neural network policy end-to-end using reinforcement learning in simulation to generate agile and precise motor commands directly from depth images captured by an onboard camera. 

- They propose a novel dual distillation approach to transfer the policy to the real robot. In the first phase, the policy is trained with privileged information like scandots and heading directions. In the second phase, it is distilled into a policy that operates only from onboard depth images and predicts its own heading directions.

- They design simple yet effective inner product based reward functions that lead to the emergence of diverse parkour skills like high jumps, long jumps, handstand walking, and traversing titled ramps within a single policy.

- They demonstrate a quadruped robot performing very challenging behaviors like high jumps over obstacles 2x its height, long jumps over gaps 2x its length, and walking on just its front legs. This pushes low-cost legged robots to new limits.

In summary, the key contribution is showing that an end-to-end learned approach can achieve precise and extreme parkour on imprecise robots by learning to map raw inputs to outputs, without needing explicit planning or mapping. The simple and unified formulation leads to more impressive emergent behaviors than engineered policies.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 This paper presents an end-to-end reinforcement learning framework to train a quadruped robot to perform extreme parkour maneuvers directly from raw depth images, without relying on explicit mapping or planning. The key ideas are a unified inner-product reward formulation to acquire diverse skills, dual distillation to predict optimal heading directions, and automatic terrain curriculum to aid exploration in RL. The robot can perform long jumps across gaps 2x its length, high jumps over obstacles 2x its height, walk on two legs, and traverse ramps, using a single neural network policy operating on images from an imprecise onboard camera.


## How does this paper compare to other research in the same field?

 Here are a few key points comparing this paper to other related research:

- This paper focuses on robotic parkour behaviors, which is a relatively new and challenging area compared to more established locomotion tasks like walking and running. There have been some prior works on robotic parkour, but this paper pushes the capabilities further.

- The paper uses a low-cost quadruped (Unitree A1) as the robot platform. Other recent parkour works have used higher-end industry quadrupeds like Anymal or Vision60. Using a low-cost platform poses additional challenges due to imprecise actuation and sensing.

- The key technical approach is training an end-to-end neural network policy using reinforcement learning in simulation. This differs from classical approaches that decompose the problem into separate perception, planning, and control modules. Other RL papers on legged locomotion have used end-to-end policies, but this paper tailors the approach to parkour.

- The behaviors demonstrated include high jumps over tall obstacles, long jumps over wide gaps, walking on tilted ramps, and handstands. These capabilities surpass previous robotic parkour results in terms of relative jump heights and lengths compared to the robot's dimensions.

- The work proposes some modifications to prior end-to-end RL methods like a two-phase distillation approach for control and heading direction, and a general inner-product based reward formulation. These contributions aim to address challenges specific to learning parkour skills.

- Concurrent to this work, a few other papers have also demonstrated quadruped parkour skills using alternative approaches. But this work achieves more dynamic behaviors on a low-cost platform.

In summary, this paper pushes the state-of-the-art in robotic parkour through innovations in training methodology and by demonstrating extremely dynamic behaviors on a budget platform. The end-to-end learned control approach contrasts with classical modular designs.


## What future research directions do the authors suggest?

 Based on my reading of the paper, here are some of the main future research directions suggested by the authors:

- Extending their approach to mobile manipulators. The current work focuses on parkour behaviors for a legged robot base. The authors suggest applying a similar end-to-end learning approach to train policies for mobile manipulators that can move around and manipulate objects.

- Exploring the capabilities and limitations of low-cost hardware. The A1 quadruped used in this work represents a low-cost platform. The authors suggest further exploring what kinds of capabilities and limitations such hardware imposes when using end-to-end deep RL methods.

- Developing sim2real techniques tailored for visuomotor control. The method relies heavily on simulation for policy training. The authors suggest investigating sim2real techniques that can further bridge the gap between simulation and the real world specifically for policies that map directly from vision to low-level motor commands.

- Combining model-based and model-free RL. The current approach is model-free RL. The authors suggest combining model-based RL where possible to improve sample efficiency and enable training complex behaviors like dexterous manipulation.

- Extending the approach to other behaviors like climbing. The current work focuses on parkour skills like jumping, walking on ramps, etc. The authors suggest expanding the method to train policies for other dynamic athletic behaviors like climbing.

In summary, the main future directions are around extending the approach to new robot platforms and capabilities, improving sim2real transfer, combining model-based and model-free RL, and expanding the method to train policies for other complex dynamic behaviors.


## Summarize the paper in one paragraph.

 The paper presents a method for training legged robots to perform extreme parkour behaviors directly from raw depth images, without explicit mapping or planning. The key ideas are:

1) Using a unified inner product based reward formulation to enable emergence of diverse parkour skills like long jumps, high jumps, and handstand walking with a single policy network. 

2) A novel dual distillation approach where the policy is first trained with privileged information like scandots and heading directions from waypoints in phase 1, and then distilled to operate directly from onboard depth images and predict its own heading directions in phase 2. This allows the robot to dynamically adjust its heading based on the obstacle at test time.

3) Demonstrating long jumps across gaps 2x the robot's length, high jumps over obstacles 2x its height, and handstand behaviors on a low-cost quadruped (Unitree A1) with imprecise actuation. The method achieves more extreme behaviors compared to prior work, highlighting the effectiveness of the simple end-to-end learning approach.

In summary, the key contribution is showing that an end-to-end policy operating directly from depth images can achieve dynamic and precise parkour on a real low-cost robot, without needing complex mapping and planning. The simple formulation allows emergent behaviors and adapts to terrain geometry.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the paper:

This paper presents a method for training quadruped robots to perform extreme parkour maneuvers like jumping across large gaps and climbing tall obstacles. The key idea is to train a single neural network policy end-to-end to go directly from raw depth images to motor commands. This avoids the need for explicit mapping or planning. The authors use reinforcement learning in simulation to train the policy. They introduce a novel dual distillation method where the policy is first trained with privileged information like goal locations. It is then distilled into a policy that can predict its own heading direction from the depth images. This allows the robot to dynamically adjust its trajectory depending on the obstacles. The reward function is also designed to encourage agile and dynamic maneuvers like jumping and climbing. 

The trained policies are deployed on a low-cost quadruped robot. The robot is able to successfully perform challenging parkour maneuvers like jumping over gaps twice its body length, climbing boxes twice its height, and walking on just its front legs. The simple end-to-end approach allows the robot to precisely coordinate perception and action to perform dynamic and agile behaviors despite having imprecise actuation and sensing. Key to the success is the distillation method and reward formulation which avoid the need for hand-engineering or explicit planning.


## Summarize the main method used in the paper in one paragraph.

 The paper proposes an end-to-end reinforcement learning framework to train a quadruped robot to perform extreme parkour behaviors directly from raw depth images. The key ideas are:

1. They use a two phase training process. In phase 1, they train a locomotion policy using RL in simulation with privileged information like scandots and heading directions from waypoints. They also train an adaptation module using regularized online adaptation to recover environment properties from the history of observations. In phase 2, they distill the phase 1 policy into one that operates directly from onboard depth images and automatically predicts heading direction based on the visible terrain geometry. 

2. They propose a simple but effective inner product based reward formulation that allows diverse parkour behaviors like jumping, climbing, handstands etc to emerge from a single policy architecture. 

3. During distillation, they use a mixture of teacher and student headings as observations to prevent distribution drift when student predicted headings are directly used.

4. They demonstrate the capability to do very high jumps over obstacles 2x the robot's height, long jumps over gaps 2x its length, traverse tilted ramps and do handstands - all directly from raw depth images from an egocentric camera on a low-cost quadruped robot with imprecise actuation.


## What problem or question is the paper addressing?

 The paper is addressing the problem of getting legged robots to perform extreme parkour behaviors like jumping very high obstacles, leaping long gaps, and doing handstands. These behaviors require precise control and perception, which is challenging on low-cost robots with imprecise actuation and sensing. 

The key questions the paper tries to answer are:

1) How can a single neural network policy operate directly from visual inputs like depth images to output precise control for parkour, without needing complex modular pipelines for perception and planning?

2) How can diverse parkour skills like high jumping, long jumping, and handstands emerge from a simple and unified reward formulation?

3) How can the robot adjust its own heading direction on the fly based on obstacles, instead of relying on human demonstration or waypoints? This is critical for parkour maneuvers.

4) How far can these learned parkour skills be pushed on a low-cost legged robot with imprecise actuation and sensing? Can the robot jump 2x its height and length?

In summary, the paper aims to tackle the challenging problem of learning-based extreme parkour on low-cost legged robots with imprecise hardware, using a simple and unified framework. The key novelty is in distilling agile motor skills and automatic heading direction into a single neural network policy that operates directly from visual inputs.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and concepts are:

- Parkour - The athletic sport of traversing obstacles dynamically. This is the main focus of the paper.

- Legged locomotion - Using legged robots like quadrupeds for locomotion tasks.

- Reinforcement learning (RL) - Using RL to train robot controllers in simulation.

- End-to-end learning - Learning a policy that maps directly from visual inputs to motor commands without separate modules. 

- Distillation - Transferring a complex policy learned in simulation to the real world through distillation.

- Reward design - Designing a unified reward function that leads to emergence of diverse skills. 

- Sim2real - Transfer of policies trained in simulation to the real world.

- Perception - Using only a single front-facing depth camera for perception.

- Agile behaviors - Enabling dynamic and agile skills like long jumps, high jumps, walking on ramps.

- Imprecise actuation - Using low-cost robots with noisy actuators.

In summary, the key ideas are using end-to-end reinforcement learning to train dynamic quadruped policies for extreme parkour behaviors directly from visual inputs, and transferring them to real low-cost robots with imprecise actuation through distillation. The simple reward design leads to emergence of skills.


## Based on the paper, what questions should we ask to create a comprehensive summary of it? Please list at least 10 questions.

 Here are 10 potential questions to ask to summarize the key aspects of this paper:

1. What is the main goal or objective of this paper? 

2. What problem is the paper trying to solve? What challenges or limitations is it addressing?

3. What is the proposed approach or method? How does it work? 

4. What are the key technical contributions or innovations proposed in the paper?

5. What kind of experiments were conducted? What datasets were used?

6. What were the main results? How does the proposed method compare to prior or baseline methods?

7. What analysis did the authors provide to interpret the results and support their claims? Were there any limitations or weaknesses identified?

8. Did the paper include any theoretical analysis or proofs? If so, what were the key theoretical results?

9. What broader impact might this work have if successful? How could it be applied or extended?

10. Did the authors suggest any promising directions for future work? What open problems remain?


## Design 10 in-depth questions about method proposed in this paper and avoid simple question.

 Here are 10 potential in-depth questions about the method proposed in this paper:

1. The paper proposes a dual distillation method consisting of two phases. Can you explain in more detail how these two phases work and why they are needed? What are the inputs, losses, and training procedures in each phase?

2. The paper introduces a novel inner-product based reward formulation. Why is this formulation more suitable for parkour compared to prior rewards used for locomotion? How does it lead to the emergence of diverse behaviors like high jumps and handstands?

3. The paper argues that allowing the policy to choose its own heading direction based on the obstacle is crucial. Why is providing heading directions via human joystick suboptimal? What specifically does the heading prediction network in phase 2 predict?

4. The paper highlights three emergent behaviors - high jumps, long jumps, and handstand walking. Can you analyze these behaviors and explain the underlying strategies the robot has learned? How do they relate to strategies human athletes use?

5. The dual distillation method relies on scandots and waypoints as privileged information in phase 1. What are scandots and waypoints? Why are they useful for parkour and how do they get distilled out in phase 2?

6. The paper introduces a curriculum over terrain difficulties to aid exploration. Can you explain this curriculum in more detail? How does it help overcome exploration challenges in RL? Are there other curriculum designs that could potentially work better?

7. For the heading direction prediction, the paper uses a mixture of teacher and student for stable learning. Can you explain this technique? Why is it better than directly using the predicted headings?

8. The paper compares against modular pipelines with elevation maps. What are the limitations of these pipelines that make end-to-end learning superior for parkour?

9. Could you discuss any potential safety concerns with the trained policies? How can we ensure they behave safely when deployed in unstructured real-world environments?

10. What are the key limitations of the current method? How can the framework be extended to learn even more dynamic and extreme parkour behaviors?
