# [Seeing What You Miss: Vision-Language Pre-training with Semantic   Completion Learning](https://arxiv.org/abs/2211.13437)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading, the central research question this paper aims to address is:

How can we improve vision-language pre-training models to better learn global semantic representations and cross-modal alignment? 

The key hypothesis is that existing masked modeling pre-training objectives like masked language modeling (MLM) and masked vision modeling (MVM) focus only on reconstructing local masked tokens. This may lead to inadequate learning of global semantic features that capture higher-level cross-modal information. 

To address this, the authors propose a new pre-training task called Semantic Completion Learning (SCL). The key idea is to recover the global semantics of masked data by exploiting cross-modal interactions. This allows the model to generate more representative global features with improved global-to-local alignment.

In summary, the central research question is how to improve global semantic learning and cross-modal alignment in vision-language pre-training. The key hypothesis is that reconstructing local masked tokens is insufficient and recovering global semantics of masked data through cross-modal interactions can improve global feature learning. SCL is proposed to test this hypothesis.


## What is the main contribution of this paper?

 Based on my reading of the paper, the main contributions appear to be:

1. Proposing a new pre-training task called Semantic Completion Learning (SCL) that recovers missing semantic information from unmasked data to promote learning of more representative global features. This is different from prior masked modeling tasks that focus on reconstructing local masked tokens. 

2. Designing an adaptive vision encoder that can readily transfer multimodal pre-training knowledge between images and videos.

3. Conducting comprehensive experiments on multiple vision-language downstream tasks including VQA, visual reasoning, image-text retrieval, and video-text retrieval. The results demonstrate the effectiveness of the proposed SCL task and flexible vision encoder, with the model achieving state-of-the-art performance on these tasks.

In summary, the key innovations seem to be the new SCL pre-training objective and flexible vision encoder architecture that enables strong performance on both image and video multimodal tasks. The extensive experimental validation on diverse downstream benchmarks also demonstrates the generalization ability of the approach.
