# [Divide-or-Conquer? Which Part Should You Distill Your LLM?](https://arxiv.org/abs/2402.15000)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Using large language models (LLMs) for complex reasoning tasks can be very costly during inference when the reasoning chain is long. 
- LLMs are also opaque black box models that offer limited adaptability. 
- Distilling LLMs into smaller models often seriously harms performance.

Proposed Solution:  
- Break down reasoning tasks into two capabilities - problem decomposition and problem solving.
- Hypothesize that decomposition relies more on general problem solving strategies while solving requires domain knowledge. Thus, decomposing should be easier to distill.
- Propose distilling only the decomposition capability from the teacher LLM into a smaller student model. Keep using teacher LLM or other large models for solving.

Main Contributions:
1) Demonstrate the possibility and effectiveness of distilling only the query decomposition part. The distilled model maintains most performance while significantly reducing inference costs. However, distilling the solving part leads to considerable performance decline.

2) Show that the distilled decomposition model exhibits good generalization across tasks, datasets and models. But the distilled solving for each task does not generalize well.

3) Indicate that by using smaller, distilled decomposition models with LLMs, we can achieve reasoning with cost-efficient inference and better flexibility for local adaptation.

In summary, the key insight is to separate the decomposition and solving capabilities for complex reasoning tasks, and only distill the former as it is more generalizable and compressible. This allows leveraging the power of large LLMs in a more cost-efficient and adaptable manner.


## Summarize the paper in one sentence.

 Here is a one sentence summary of the key points from the paper:

The paper proposes to decouple the decomposition and solving capabilities of large language models, demonstrating the possibility of distilling only the decomposition capability into a smaller model for faster inference, while the distilled decomposition model still maintains performance and exhibits generalization power across tasks, datasets and models.


## What is the main contribution of this paper?

 The main contributions of this paper are:

1. Demonstrating that the decomposition capability is crucial for the complex reasoning of LLMs and that this capability can be separated from the problem solving capability. 

2. Showing the possibility and effectiveness of distilling only the query decomposition from the teacher LLM model. The resulting distilled model can maintain most of the performance while significantly reducing inference costs. However, distilling the solving part of the LLM leads to a considerable decline in performance.

3. Demonstrating that the distilled query decomposition model exhibits good generalization across tasks, datasets, and models. However, the distilled solving for each task does not generalize as well.

In summary, the key contributions focus on disentangling and selectively distilling the decomposition capability from large language models, which is shown to be more feasible and beneficial compared to distilling the overall solving capability. The distilled decomposition model can enable cost-efficient inference while maintaining strong performance and generalization.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and concepts include:

- Large language models (LLMs)
- Reasoning tasks
- Problem decomposition
- Problem solving
- Distillation
- Inference cost
- Generalization
- Question answering
- Mathematics datasets
- Teacher-student model
- Chain of Thought (CoT)
- Two-stage models
- Query decomposition
- Cross-domain evaluation
- Cost-efficient inference
- Local adaptation

The paper explores decomposing reasoning tasks into a problem decomposition phase and a problem solving phase. It hypothesizes that the decomposition phase is easier to distill into a smaller student model compared to the problem solving phase. Experiments are conducted on question answering and mathematics datasets using teacher and student models. The key findings show that the problem decomposition capability can be effectively distilled while maintaining performance and enabling cost-efficient inference and better generalization.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in this paper:

1. The paper proposes to decompose the reasoning process into a problem decomposition phase and a problem solving phase. What are the key advantages and limitations of separating these two capabilities? How does it compare to having them intertwined in a single model?

2. The paper hypothesizes that decomposition relies more on general problem solving strategies while solving relies more on domain knowledge. What evidence supports this hypothesis? Are there counter examples where decomposition itself requires extensive domain expertise?  

3. The method distills only the decomposition capability from the teacher LLM into a student model. What makes decomposition more amenable to distillation compared to solving? What failure modes can occur when distilling the solving capability?

4. The distilled decomposition model shows good cross-domain generalization but the distilled solving model struggles to generalize. What factors enable the generalization of the decomposition model? Is it learning some domain agnostic principles?

5. The method relies on the quality of decomposition from the teacher LLM. How robust is the approach if the teacher produces poor or irrelevant subquestions? Can the student model overcome bad demonstrations?  

6. The inference cost analysis shows significant gains from distilling just the decomposition model. How do these cost savings scale as the length of reasoning chain increases? What are the practical use cases?

7. The method is evaluated on math and QA datasets. How will the approach perform on other planning intensive tasks like tool use, dialogue agents and control of simulations? Will similar trends hold?

8. Can the distilled decomposition model be further improved via reinforcement learning by using the solver outcome as rewards? What are the challenges in such an approach?

9. Is it possible to train a universal decomposer model on demonstrations from multiple domains and task modalities? Will such a model generalize better compared to specialized models?

10. The method relies on a two stage process for reasoning. How can the delays between decomposition and solving be minimized in practical deployments? Is asynchronous processing an option?
