# [Anytime, Anywhere, Anyone: Investigating the Feasibility of Segment   Anything Model for Crowd-Sourcing Medical Image Annotations](https://arxiv.org/abs/2403.15218)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Medical image segmentation is critical for computer-assisted diagnosis and treatment planning, but curating pixel-level annotations is extremely time-consuming and labor-intensive, requiring expertise from radiologists.
- Existing semi-automated approaches still require expert interaction for annotation and validation. 
- There is a need for a pipeline to enable non-experts to crowdsource annotations to accelerate dataset curation.

Proposed Solution:
- Evaluate using the Segment Anything Model (SAM), a foundation model for zero-shot semantic segmentation, to generate segmentation masks from sparse bounding box annotations. 
- Crowdsource bounding box annotations from non-experts and use SAM to generate dense masks to train segmentation models.
- Assess feasibility of using SAM-generated masks to train nnU-Net, a state-of-the-art 3D segmentation model.

Experiments:
- Used MSD Liver, MSD Spleen and BTCV abdomen CT datasets.
- Compared SAM vs MedSAM (SAM fine-tuned on medical images) for mask generation.
- Evaluated in simulated setting with perfect bounding boxes and real-world setting with crowdsourced boxes.
- For crowdsourcing, 4 non-experts annotated 15 BTCV volumes with bounding boxes for 5 organs.  

Key Results:
- SAM generates segmentation masks with high 2D slice Dice scores compared to ground truth masks.  
- However, SAM lacks 3D spatial relationships, resulting in poor volumetric consistency.
- nnU-Net models trained on SAM masks perform significantly worse than those trained on ground truth masks.
- Model performance is highly sensitive to incomplete/inaccurate crowd annotations.  

Main Contributions:  
- First study evaluating SAM for crowdsourcing segmentation model training data from non-experts.
- Demonstrates issues in using current 2D foundation models directly for 3D medical segmentation.
- Highlights importance of quality control for crowdsourced annotations.

The paper concludes that while crowdsourcing with foundation models is promising for accelerating dataset curation, specialized 3D foundation models and robust annotation quality assessment is needed to unlock its full potential.
