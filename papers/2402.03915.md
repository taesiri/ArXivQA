# [Learning Metrics that Maximise Power for Accelerated A/B-Tests](https://arxiv.org/abs/2402.03915)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- A/B testing is important for decision making in online businesses, but experiments are often expensive due to requiring long runtimes or large sample sizes to achieve statistical significance. This leads to slow experiment velocity and high costs. 
- North Star metrics like revenue or retention that indicate business impact are typically delayed and not very sensitive. As a result, experiments on them have high rates of inconclusive outcomes (type II errors).

Proposed Solution:
- Learn optimized metrics from short-term signals that directly maximize the statistical power for detecting impact on the North Star metric.
- Existing approaches that maximize average z-score can overfit and do not necessarily reduce type II errors. They are also prone to type III errors (disagreement with the North Star).
- Instead, minimize the p-values that would have been obtained on past experiments. This more evenly distributes gains over experiments instead of a few very significant results. Also penalizes disagreement more strongly.

Main Contributions:
- Novel objectives to optimize p-values or log-transformed p-values instead of just z-scores when learning metrics. Reduces type II and III errors.  
- Spherical regularization technique to improve convergence speed of the optimization by up to 40%.
- Evaluation on 153 A/B experiments from two large-scale video platforms. Learned metrics increase power by 67-78% standalone, and 133-210% combined with North Star.
- Alternatively, learned metrics reduce required sample size to 12-15% of North Star at same power. Thus greatly decreases experimentation cost/time.

In summary, the paper presents an improved approach to learn optimized metrics that increase sensitivity and power for detecting experiment outcomes. This allows faster and cheaper A/B testing. The methods are demonstrated on real-world platforms.


## Summarize the paper in one sentence.

 This paper proposes learning metrics from short-term signals that directly maximize the statistical power they harness with respect to a delayed, insensitive North Star metric in order to reduce the cost of online experimentation.


## What is the main contribution of this paper?

 Based on my reading of the paper, the main contributions are:

1) Proposing new objectives for learning A/B testing metrics that directly optimize statistical power and minimize type III errors (disagreement with the north star metric). Specifically, optimizing the (log) p-value rather than just the z-score.

2) Introducing a spherical regularization technique to accelerate convergence when optimizing these scale-free objectives via gradient ascent.

3) Providing an empirical evaluation on large-scale datasets from ShareChat and Moj showing that the learned metrics can increase statistical power by up to 210% and reduce the required sample size for experiments by up to 88%, compared to just using the north star metric alone.

In summary, the key innovation is in the objectives used to learn sensitive proxy metrics, as well as the regularization method. And it's backed by strong empirical results demonstrating the effectiveness on industry datasets. The learned metrics are currently used in production at ShareChat and Moj to enable higher velocity experimentation.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and concepts are:

- A/B testing
- Online controlled experiments
- North Star metric
- Statistical power
- Sensitivity
- Type I, II, and III errors
- Learned metrics
- Linear combination of metrics
- $z$-score
- $p$-value 
- Statistical significance testing
- Multiple hypothesis testing
- Always-Valid-Inference (AVI)
- Surrogate metrics
- Gradient ascent optimization
- Spherical regularization
- Sample size reduction
- False positive rate 
- False negative rate
- User retention 
- User engagement

The paper focuses on learning metrics from short-term signals that directly maximize the statistical power for making decisions in A/B tests, using techniques like optimizing p-values and $z$-scores. Key goals are reducing type II and III errors, increasing sensitivity of metrics, and decreasing sample sizes required.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in this paper:

1. The paper proposes learning metrics by directly minimizing the p-value or the log-transformed p-value. How does this lead to improved optimization objectives compared to simply maximizing the z-score? What are the theoretical justifications for why optimizing the (log) p-value equitably divides gains over experiments compared to maximizing z-scores?

2. The paper argues that maximizing average z-scores can still result in poor median sensitivity and increased type III errors. Explain this argument and provide an intuitive example where this could occur. Why is directly optimizing the p-value or log p-value more likely to reduce type III errors? 

3. Explain the proposed spherical regularization method for accelerating convergence when optimizing scale-free objectives like z-scores and p-values. Provide some intuition for why this regularization helps alignment of gradient directions. How much faster convergence was empirically observed from this regularization?

4. Walk through the derivation that shows relative z-scores imply reduced sample size requirements for equivalent statistical power. Then explain how the proposed learned metrics can reduce sample sizes (and thus experimentation costs) while maintaining power.  

5. The paper evaluates learned metrics in conjunction with guardrail metrics and the North Star metric. Explain this evaluation protocol and why it is preferred to evaluating metrics in isolation. How much complementary power increase was observed from adding a learned metric?

6. Discuss some of the key practical insights about metric design and behaviors that the authors observed during the empirical evaluation, like issues with ratio metrics and benefits of user-level aggregations. How might these insights apply to other online platforms?

7. Explain the three types of variant pairs used to train the learned metrics: known outcomes, unknown outcomes, and A/A pairs. What does each set teach the model and how are they used in the objectives?

8. The paper uses logged data from over 150 experiments spanning two large-scale social video platforms. Discuss the advantages of having such real-world logged data for this research. What limitations did the authors still face?  

9. Analyze the various research questions posed in the paper. Which do you think are most important for demonstrating the value of the proposed learned metrics approach? Were the experiments and results sufficient to address the research questions?

10. The paper focuses on improving power and reducing experimentation costs. Can you think of other potential objectives for learning metrics that the paper did not explore? What other future work directions seem promising?
