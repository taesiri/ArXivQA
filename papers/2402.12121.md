# [Evaluating Image Review Ability of Vision Language Models](https://arxiv.org/abs/2402.12121)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Large vision language models (LVLMs) have shown impressive abilities for multimodal language tasks involving images and text. However, their capacity for generating high-quality image review texts has not been well evaluated. 
- Unlike image captions which describe objective visual content, reviews can cover various subjective perspectives (e.g. composition, exposure). So there is no single "correct" review, making evaluation challenging.

Proposed Solution:
- Introduce a new image review evaluation method based on ranking correlation analysis between humans and LVLMs.
- Construct a benchmark dataset of 207 Wikipedia images, each with 5 English & Japanese review texts generated by GPT-4V and then ranked by 3 human annotators per language.
- Propose perplexity-based and response-based ranking approaches where LVLMs rank the 5 texts and correlation with human rankings is measured.

Main Contributions:
- Novel image review evaluation method for LVLMs using ranking correlation analysis, eliminating need for costly annotations of a single "correct" review text.
- Publicly released benchmark dataset for analyzing image review abilities in both English and Japanese.
- Experiments demonstrating state-of-the-art LVLMs can distinguish high-quality and inferior image reviews at levels comparable to humans.
- Analysis provides direction for developing LVLMs better equipped for creative generation tasks like image reviews.
