# [[Citation needed] Data usage and citation practices in medical imaging   conferences](https://arxiv.org/abs/2402.03003)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

This paper presents tools and a case study for quantifying the usage of publicly available medical image datasets in research papers. 

Problem:
- Research papers often use public datasets without proper attribution, making it hard to track dataset usage and impact over time. 
- Existing reviews focus on methods rather than dataset use.
- Simply detecting citations is not enough, as datasets may be used without being cited. 

Proposed Solution:
1) A pipeline to detect dataset usage by analyzing references and full text of papers. It uses OpenAlex citations, matches reference strings to dataset papers, and searches full text for dataset names/aliases.

2) An open-source PDF annotation tool to manually validate the pipeline's detections. Allows collaborative annotation of dataset mentions by location.

3) A case study applying the tools to analyze usage of 20 classification/segmentation datasets in MICCAI/MIDL papers (2013-2023). Usage is categorized as cited only, mentioned only, or both. 

Key Findings:
- Concentration of research on a few popular datasets, especially for a given task.
- Over 25% of papers only cite a dataset without usage while ~10% only mention it without citation - highlighting difficulty in tracking usage.
- Differences in dataset growth rates suggest "rich get richer" snowball effect.

Main Contributions:
- Open-source tools to quantify dataset usage using citations, full-text search
- Annotation software for tracking usage
- Analysis of citation and usage practices for public medical imaging datasets
- Recommendations for better standardization when indicating dataset usage.

The tools and analysis provide ways to understand the adoption and impact of public datasets over time. The annotation tool can generalize to other domains. Further standardization of dataset citation is encouraged.


## Summarize the paper in one sentence.

 This paper presents tools to quantify usage of medical imaging datasets in research papers and applies them in a case study of 20 datasets across MICCAI/MIDL papers, finding a concentration of usage on a few popular datasets and difficulty in tracking usage due to lack of citation standards.


## What is the main contribution of this paper?

 Based on my reading of the paper, the main contributions are:

1) Presenting a pipeline for quantifying dataset usage based on analyzing references and the full text of papers.

2) Presenting an open-source annotation tool to validate the pipeline method and aid in tracking dataset usage. 

3) Applying both tools to study the usage of several popular segmentation and classification datasets in MICCAI and MIDL conference papers between 2013-2023.

4) Discussing limitations, additional practices found, and providing recommendations to ease tracking of dataset usage.

In summary, the paper presents methods and tools for detecting and tracking the usage of datasets in scientific papers, and applies them to analyze the usage of medical imaging datasets across two major conferences. The key goals are to understand dataset usage trends and patterns, highlight issues around proper dataset citation, and provide recommendations for improving tracking of dataset usage.


## What are the keywords or key terms associated with this paper?

 Based on scanning the paper, some of the main keywords or key terms associated with this paper appear to be:

- Bibliometrics
- citations
- datasets 
- medical imaging
- data re-use
- annotation tools 
- meta-analysis

These keywords are listed explicitly under the \begin{keywords} environment in the paper. The paper seems to focus on analyzing the usage and citation practices of medical imaging datasets in research papers, developing tools to help track dataset usage, and performing a case study applying these tools.

Key themes and topics that come up based on the content are: dataset usage tracking, dataset citations, citing practices, annotation tools, dataset popularity, dataset diversity, public datasets, dataset bias, literature analysis.

So in summary, the main keywords and key terms reflect analysis of medical imaging dataset usage, citations, and practices, along with developing annotation tools and performing a bibliometric meta-analysis case study. Let me know if you need any clarification or have additional questions!


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in this paper:

1. The paper mentions using OpenAlex and full-text analysis to quantify dataset usage. What are the advantages and disadvantages of using OpenAlex compared to other citation databases like Web of Science or Scopus? How does full-text analysis complement the information from OpenAlex?

2. The paper categorizes dataset presence as "only cited", "only mentioned", and "cited and mentioned". What are some potential issues with relying only on citations or only on mentions to determine if a paper actually used a dataset? Why is it important to use both citations and mentions? 

3. The pipeline relies on regular expressions to match dataset names and aliases. What are some challenges with creating comprehensive and accurate regular expressions? How could more advanced natural language processing techniques like named entity recognition potentially improve the matching?

4. What are some limitations of focusing the analysis only on conferences like MICCAI and MIDL? How could expanding the analysis to journals or other conferences provide additional insights into dataset usage?

5. The paper hypothesizes about the "snowball effect" for popular datasets becoming even more popular over time. What quantitative and qualitative evidence is there for this effect? What factors drive it?

6. The paper mentions finding "backup" datasets on Kaggle being cited instead of original sources. Why does this happen and what effect can it have on properly crediting dataset creators? 

7. What are other techniques besides having an explicit "Data Availability" section that conferences/journals could use to make dataset usage and citations more transparent?

8. The analysis relies heavily on obtaining PDFs and conversion to XML. What are some common issues that arise in this conversion process and how do they affect extracting references/mentions?

9. The paper analyzes 20 datasets manually selected based on author knowledge and surveys. How does this selection process introduce bias? How could the analysis be expanded to more datasets?

10. The analysis focuses on presence of mentions and citations, but does not verify if the datasets are actually used appropriately in the methods/experiments. What additional techniques could help determine correct dataset usage?
