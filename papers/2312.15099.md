# [Moderating New Waves of Online Hate with Chain-of-Thought Reasoning in   Large Language Models](https://arxiv.org/abs/2312.15099)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem: Online hate speech is a growing issue that is difficult to moderate due to its subjective and contextual nature. The emergence of "new waves" of online hate in reaction to major events like COVID-19 and Ukraine war poses additional challenges as existing hate speech detection tools cannot adapt quickly enough. There is a need for reasoning-based approaches that can rapidly update to handle new targets and terms.  

Proposed Solution: The paper proposes HateGuard, a framework to detect and moderate new waves of online hate speech. HateGuard has three main components:

1) New Wave Identification: Collect a small seed dataset of new wave instances identified by human moderators to extract new hate targets/terms.

2) Automatic Prompt Update: Automatically identify new derogatory terms and targets from the seed dataset using NLP, verify novelty, and update the reasoning prompts to include new terms and targets.

3) Reasoning-based Detection: Leverage large language models (LLMs) and chain-of-thought (CoT) prompting to perform nuanced, step-by-step reasoning to determine if content contains hate speech based on updated prompts. Breaks down into sub-problems to assess targets, derogation, direction, incitation to hate.

Key benefits are the ability to rapidly update prompts as new waves emerge without retraining models, harnessing reasoning capability of LLMs. 

Main Contributions:

- Analysis of 3 major new waves (COVID, Capitol riots, Ukraine war) showing hate speech peaks tied to external events  

- HateGuard framework to leverage LLMs and CoT prompting for fast, nuanced hate speech detection

- Significantly outperforms baselines in detecting new waves, cuts number of violations by 75-100%

- Demonstrates practicality by correctly flagging all hate speech in unlabeled tweet dataset

The paper makes notable contributions in harnessing LLMs for reasoning-based decisions to address limitations of current hate speech systems against rapidly emerging new waves.
