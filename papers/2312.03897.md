# [Revisiting the Optimality of Word Lengths](https://arxiv.org/abs/2312.03897)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem Statement
- There has been a longstanding debate about what principle drives the lengths of words in languages. Two main hypotheses have been proposed:
    1) Zipf's "law of abbreviation": Word lengths are optimized to minimize expected utterance lengths. This predicts word length should be inversely correlated with word frequency.
    2) Piantadosi et al.'s "channel capacity hypothesis" (CCH): Word lengths aim to keep information rate close to channel capacity. This predicts word length should correlate with a word's expected surprisal (negative log probability).
- Formalizing and comparing these hypotheses has been difficult. Prior work has struggled with issues like model estimation, tokenization schemes, etc. 

Proposed Solution
- The authors formalize the problem of assigning wordforms as an optimization problem called the "lexicalization problem". 
- They provide a novel derivation for the CCH, showing it actually optimizes a lower bound (\costlower). Their new derivation suggests word length should correlate with expected surprisal PLUS the variance-to-mean ratio of surprisal.  
- They propose a unified framework to derive predictions from both hypotheses and empirically compare them.

Main Contributions
- Show Piantadosi et al. optimize a lower bound, not CCH directly. Provide better derivation for CCH.
- Empirically compare hypotheses over 13 languages with improved surprisal estimates from neural models.
- Find strong evidence Zipf's hypothesis holds across settings - word frequency has more predictive power over lengths than surprisal-based quantities. 
- Conclude lexical optimization aims to minimize expected utterance lengths rather than keep information rate near channel capacity.


## Summarize the paper in one sentence.

 This paper theoretically and empirically revisits hypotheses on the optimality of word lengths, finding evidence across 13 languages that shorter words tend to be more frequent, supporting Zipf's longstanding theory that languages minimize expected utterance lengths.


## What is the main contribution of this paper?

 This paper makes both theoretical and empirical contributions regarding the relationship between word lengths and communicative efficiency in languages.

On the theoretical side, the main contributions are:

1) Formalizing the problem of assigning wordforms as an optimization problem, which they term the "lexicalization problem". 

2) Providing a novel derivation of the optimal word lengths that would minimize the "channel capacity hypothesis" (CCH) cost. They show this should be proportional to the expected surprisal plus its variance-to-mean ratio.

3) Showing that Piantadosi et al.'s (2011) derivation actually optimizes a lower bound on the CCH cost rather than the true CCH objective.

On the empirical side, the main contributions are:

1) Large-scale analysis across 13 diverse languages comparing predictions of the CCH cost, its lower bound, and Zipf's utterance length minimization hypothesis. This is done using modern neural language models to estimate surprisal.

2) Showing strong empirical evidence across many experimental settings and methodological choices that Zipf's hypothesis provides the best correlation with real word lengths. This supports the view that lexica optimize for minimal utterance lengths.

In summary, the paper clarifies both theoretical and empirical aspects of the relationship between word lengths and communicative efficiency, providing evidence in favor of Zipf's longstanding hypothesis.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and concepts associated with it are:

- Lexicalization problem - The problem of assigning wordforms to concepts/words to create a lexicon, framed as an optimization problem.

- Channel capacity hypothesis (CCH) - The hypothesis that word lengths are optimized to keep information rate close to channel capacity during communication.

- Surprisal - The negative log probability of a word given its context.

- Zipf's law of abbreviation - The hypothesis that word lengths are optimized to minimize expected utterance length. 

- Lower bound on CCH (CCHlower) - The objective that Piantadosi et al.'s word length predictions actually optimize, which lower bounds the true CCH objective. 

- Word frequency - The usage frequency of words, which Zipf hypothesized word lengths are inversely correlated with.

- Phonotactic constraints - Constraints on what sequences of phones/symbols are valid wordforms.

Some other potentially relevant terms are information rate, communicative efficiency, uniform information density hypothesis, mean squared error, Spearman correlation, language models, transformer models. The key findings are that word frequency is more predictive of length than surprisal, and that languages appear optimized to minimize expected utterance length rather than approach channel capacity.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in this paper:

1. The paper proposes a novel formal derivation of Piantadosi et al.'s claim that word lengths should be proportional to expected surprisal. What assumptions did the authors make in their derivation and how do they compare to the assumptions made by Piantadosi et al.?

2. The authors show that Piantadosi et al. actually optimize a lower bound on the channel capacity hypothesis (CCH) cost function rather than directly optimizing CCH itself. What is the implication of this finding in terms of the optimality of Piantadosi et al.'s proposed word lengths? 

3. Under the authors' novel derivation, word lengths should be proportional to expected surprisal plus the variance-to-mean ratio of surprisal. Intuitively, why does variance play a role here in determining optimal word lengths?

4. The authors relax several constraints when deriving optimal CCH and lower bound word lengths, but only relax some constraints when deriving optimal Zipfian word lengths. Could this methodological choice impact the relative predictive power of the hypotheses?

5. Across languages and settings, the authors find frequency is most predictive of word length. But in some cases with lower quality language models, CCH/lower bound surprisal correlates more strongly. What might explain this counterintuitive finding?

6. The evaluation relies heavily on mean squared error of a linear model predicting word lengths. What are the limitations of using MSE here versus other metrics like Spearman correlation?

7. What challenges arise when estimating surprisal from an imperfect language model versus the true underlying generative distribution? Could biases in the model systematically impact results?  

8. The set of words analyzed seems to strongly influence results - especially for Spearman correlation. Why might this be the case and what are the implications?

9. The study focuses solely on written text. How might results differ for spoken languages where additional pressures come into play in determining word duration?

10. The set of languages studied, while diverse, is still dominated by Eurasian languages with large training sets available. What are the barriers to expanding the diversity of languages analyzed and how could this change the conclusions?
