# [EasyFS: an Efficient Model-free Feature Selection Framework via Elastic   Transformation of Features](https://arxiv.org/abs/2402.05954)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
Traditional model-free feature selection methods treat each feature independently while disregarding interrelationships between features. This leads to poorer performance compared to model-aware methods. However, model-aware methods have high computational costs and lack flexibility due to their dependency on specific downstream models.  

Proposed Solution:
The paper proposes EasyFS, an efficient model-free feature selection framework that transforms features through elastic expansion and compression to achieve better performance than state-of-the-art model-aware methods. EasyFS has the efficiency and flexibility of model-free methods.

EasyFS first expands the feature space using a lightweight random projection network to achieve non-linear combinations of original features. This captures interrelationships between features. Next, it measures the relevance between expanded features and targets using conditional covariance for regression tasks or within-class variance for classification. 

Then, EasyFS proposes a redundancy metric based on coding rate variation when a feature is removed. This efficiently measures global redundancy and naturally supports continuous features.

Finally, EasyFS fuses relevance and redundancy to select the most useful expanded features, then traces their contribution back to original features for a multi-objective ranking.

Main Contributions:

- Proposes a model-free feature selection framework (EasyFS) that elastically transforms features to model interrelationships and identify useful features

- Introduces a lightweight random projection network for non-linear expansion of feature space

- Defines a redundancy metric using coding rate variation that efficiently supports continuous features  

- Achieves state-of-the-art performance while having over 94% less computational cost compared to recent model-aware methods

- Demonstrates high performance on 21 datasets for both regression and classification tasks, outperforming existing methods by up to 10.9% in regression and 5.7% in classification
