# TSGP: Two-Stage Generative Prompting for Unsupervised Commonsense   Question Answering

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the abstract, the main research question this paper addresses is: How can we develop an unsupervised commonsense question answering framework that leverages the implicit knowledge stored in pre-trained language models (PrLMs) without relying on labeled downstream task data or fine-tuning? The authors propose a two-stage generative prompting framework called TSGP to address this question. Specifically, the research aims to:1) Design knowledge and answer generation prompts that can elicit commonsense knowledge required for questions from a PrLM and generate multiple possible candidate answers. 2) Leverage the knowledge encoded in the PrLM through multi-stage prompting to make implicit reasoning steps explicit and generate answers independent of specified choices.3) Demonstrate through experiments on three commonsense QA datasets that the proposed framework TSGP can significantly improve the reasoning ability of language models for unsupervised commonsense QA without relying on labeled data or task-specific fine-tuning.In summary, the central hypothesis is that multi-stage generative prompting can enable PrLMs to flexibly generate the knowledge and answers needed for unsupervised commonsense QA by exploiting what's already encoded implicitly within the model's parameters. The paper aims to support this hypothesis through the proposed TSGP framework and experiments.


## What is the main contribution of this paper?

The main contributions of this paper are:1. It proposes a general two-stage generative prompting framework (TSGP) to fully exploit the knowledge implicit in pre-trained language models (PrLMs) for unsupervised commonsense question answering. 2. It designs knowledge and answer generation prompts in TSGP to make implicit intermediate reasoning steps explicit and generate possible candidate answers independent of specified choices.3. It conducts experiments on three question answering datasets focusing on different types of commonsense, and shows that TSGP significantly improves the reasoning ability of language models in unsupervised settings.In summary, the key contribution is the novel TSGP framework that can elicit commonsense knowledge from PrLMs via generative prompting, make implicit reasoning explicit, and generate flexible answer candidates. This allows PrLMs to better perform unsupervised commonsense QA without relying on labeled data or external knowledge bases.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper proposes a two-stage generative prompting framework called TSGP that uses a pre-trained language model to generate commonsense knowledge and multiple possible answers for commonsense questions, achieving state-of-the-art results on three commonsense QA datasets in an unsupervised setting.


## How does this paper compare to other research in the same field?

Here are some thoughts on how this paper compares to other research in the field of unsupervised commonsense question answering:- The paper focuses on using prompts to elicit commonsense knowledge from pre-trained language models (PLMs), rather than retrieving knowledge from external knowledge bases or resources. This aligns with recent trends leveraging knowledge already embedded in PLMs.- The two-stage prompting approach is novel compared to prior work. Generating both knowledge and answers allows the model to reason more flexibly compared to methods that use fixed templates or only generate answers. - The experiments cover multiple commonsense QA datasets (CommonsenseQA, OpenBookQA, SocialIQA). Results show state-of-the-art performance on these benchmarks compared to prior unsupervised methods, demonstrating the versatility of the approach.- The prompting framework requires no task-specific fine-tuning. Many existing methods fine-tune PLMs on downstream tasks. Avoiding fine-tuning improves prompt engineering efficiency.- The size of the PLM used (GPT-2) is relatively small compared to models utilized by some other prompting papers (e.g. GPT-3). This demonstrates the efficacy of prompting even smaller PLMs.- The mutual information method for selecting the most relevant generated knowledge is simple but effective. Other papers have used more complex knowledge selection methods.- The two-stage prompting framework is model-agnostic and task-agnostic. The overall approach is general and flexible, allowing easy transfer to new models and tasks.In summary, the paper demonstrates a novel and effective prompting-based approach for unsupervised commonsense QA. The two-stage generation of knowledge and answers provides more flexible reasoning without task-specific tuning. Results and analysis show state-of-the-art performance on multiple benchmarks using a smaller PLM.


## What future research directions do the authors suggest?

Based on my reading of the abstract and introduction, here are some future research directions suggested by the authors:- Developing more sophisticated knowledge and answer generation prompts that are optimized for particular tasks or domains. The prompts used in this work were basic demonstrations and could likely be improved.- Exploring larger pre-trained language models beyond GPT-2, such as GPT-3, which may further enhance the reasoning ability in an unsupervised setting. The authors note computational resource limitations prevented this exploration. - Extending the framework to other commonsense reasoning tasks and benchmark datasets. The authors tested on three datasets but suggest the approach could generalize further.- Combining this unsupervised prompting approach with retrieved information from external knowledge bases for hybrid reasoning. The current work relies solely on knowledge implicitly encoded in the language model.- Improving the quality and coherence of the generated knowledge and answers, which had limitations noted in the human evaluations. More advanced generation techniques could address these issues.- Reducing the necessary number of generated candidate answers required for robust performance. The authors note this could improve efficiency.In summary, the main future directions focus on enhancements to the prompting and generation components, scaling to larger models, testing on more tasks, incorporating external knowledge, and improving overall quality and efficiency. The authors propose this as a general framework amenable to future research.


## Summarize the paper in one paragraph.

Here is a one paragraph summary of the paper:The paper proposes an unsupervised commonsense question answering framework called Two-Stage Generative Prompting (TSGP) that makes use of the knowledge encoded in pre-trained language models. The framework contains two stages - knowledge generation and answer generation. In the knowledge generation stage, the model generates question-related commonsense knowledge statements using prompts and selects the most relevant one using point-wise mutual information. In the answer generation stage, the model generates multiple candidate answers conditioned on the question and selected knowledge using another set of prompts. Finally, the model computes semantic similarity between the generated answers and answer choices to predict the most likely correct answer. Experiments on CommonsenseQA, OpenBookQA and SocialIQA datasets show that the proposed TSGP framework significantly improves reasoning ability of language models in unsupervised settings and achieves state-of-the-art results. The framework is flexible, does not require fine-tuning or task-specific templates, and can elicit commonsense knowledge from language models to reason better.


## Summarize the paper in two paragraphs.

Here is a two paragraph summary of the paper:The paper proposes a new two-stage generative prompting framework called TSGP for unsupervised commonsense question answering. Previous approaches have limitations in generating the required knowledge from pre-trained language models and scoring answers reasonably. TSGP aims to address these limitations. In the first stage, TSGP uses knowledge prompts to generate question-relevant knowledge from a pre-trained language model. It selects the most relevant knowledge using mutual information. In the second stage, it uses answer prompts to generate multiple possible answers conditioned on the question and generated knowledge. For prediction, it computes semantic similarity between the generated answers and choice options, and votes for the most relevant option. Experiments on CommonsenseQA, OpenBookQA and SocialIQA datasets demonstrate that TSGP significantly outperforms previous unsupervised methods by making better use of knowledge encoded in the pre-trained language model parameters. The framework does not require fine-tuning and can flexibly transfer between tasks.


## Summarize the main method used in the paper in one paragraph.

Here is a one paragraph summary of the main method used in the paper:The paper proposes a two-stage generative prompting framework called TSGP for unsupervised commonsense question answering. In the first stage, knowledge prompts are used to generate question-relevant commonsense knowledge from a pretrained language model (GPT). The most relevant knowledge is selected via pointwise mutual information. In the second stage, answer prompts are used to generate multiple possible answers conditioned on the question and selected knowledge, again using GPT. For prediction, the semantic similarity between the generated answers and answer choices is computed and the most similar choice is selected. By explicitly generating knowledge and possible answers, TSGP aims to take full advantage of the commonsense reasoning ability of GPT without task-specific fine-tuning. Experiments on CommonsenseQA, OpenBookQA and SocialIQA show TSGP achieves state-of-the-art performance for unsupervised commonsense QA.
