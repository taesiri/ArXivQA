# TSGP: Two-Stage Generative Prompting for Unsupervised Commonsense   Question Answering

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the abstract, the main research question this paper addresses is: How can we develop an unsupervised commonsense question answering framework that leverages the implicit knowledge stored in pre-trained language models (PrLMs) without relying on labeled downstream task data or fine-tuning? The authors propose a two-stage generative prompting framework called TSGP to address this question. Specifically, the research aims to:1) Design knowledge and answer generation prompts that can elicit commonsense knowledge required for questions from a PrLM and generate multiple possible candidate answers. 2) Leverage the knowledge encoded in the PrLM through multi-stage prompting to make implicit reasoning steps explicit and generate answers independent of specified choices.3) Demonstrate through experiments on three commonsense QA datasets that the proposed framework TSGP can significantly improve the reasoning ability of language models for unsupervised commonsense QA without relying on labeled data or task-specific fine-tuning.In summary, the central hypothesis is that multi-stage generative prompting can enable PrLMs to flexibly generate the knowledge and answers needed for unsupervised commonsense QA by exploiting what's already encoded implicitly within the model's parameters. The paper aims to support this hypothesis through the proposed TSGP framework and experiments.


## What is the main contribution of this paper?

The main contributions of this paper are:1. It proposes a general two-stage generative prompting framework (TSGP) to fully exploit the knowledge implicit in pre-trained language models (PrLMs) for unsupervised commonsense question answering. 2. It designs knowledge and answer generation prompts in TSGP to make implicit intermediate reasoning steps explicit and generate possible candidate answers independent of specified choices.3. It conducts experiments on three question answering datasets focusing on different types of commonsense, and shows that TSGP significantly improves the reasoning ability of language models in unsupervised settings.In summary, the key contribution is the novel TSGP framework that can elicit commonsense knowledge from PrLMs via generative prompting, make implicit reasoning explicit, and generate flexible answer candidates. This allows PrLMs to better perform unsupervised commonsense QA without relying on labeled data or external knowledge bases.
