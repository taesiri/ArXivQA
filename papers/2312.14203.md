# [Shai: A large language model for asset management](https://arxiv.org/abs/2312.14203)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Asset management is a complex field that can benefit from advanced AI solutions like large language models (LLMs). However, existing open-source finance LLMs may lack sufficient domain-specific knowledge and adaptation for practical asset management applications.  

Proposed Solution:
- The authors develop "Shai", a 10B parameter LLM specifically tailored for asset management using targeted pre-training and fine-tuning.

Key Contributions:
- Construct an asset management focused 10B LLM with enhanced performance on relevant tasks compared to mainstream open-source models.
- Share detailed construction process - continuous pre-training on asset management data followed by supervised fine-tuning.  
- Propose a comprehensive evaluation framework combining financial exams, open-ended QA, tailored tasks and safety assessments. Shai outperforms comparable models.  
- Findings: Task-related pre-training provides advantages on downstream tasks. Evaluating large language models like GPT-4 reveals biases related to answer position and length.
- Showcase potential of 10B LLMs in asset management with strong capabilities and modest compute requirements. Provide methodology to assist industry adoption.

In summary, the authors develop a 10B parameter asset management focused LLM, detail its training methodology, evaluate it thoroughly and highlight its capabilities as well as evaluation challenges, demonstrating the promise of large language models in this complex financial domain.


## Summarize the paper in one sentence.

 This paper introduces Shai, a 10B asset management language model built on an open-source foundation with continuous pre-training and fine-tuning, which demonstrates enhanced performance on domain-specific tasks compared to baseline models through a comprehensive evaluation framework.


## What is the main contribution of this paper?

 Based on my reading of the paper, the main contributions are:

1. The authors have developed "Shai", a 10B level large language model specifically designed for the asset management industry. To the best of their knowledge, this is the first 10B LLM targeted at asset management.

2. The paper shares the detailed construction process of Shai, including continuous pretraining and supervised finetuning using data from the asset management domain.

3. The authors present a comprehensive evaluation framework tailored for the asset management industry, covering tasks like financial exams, open-ended QA, practical applications, and safety assessments. This allows them to thoroughly evaluate Shai's capabilities.

4. Evaluation results show that Shai outperforms mainstream 10B models like Baichuan, Qwen, InterLM etc. on asset management tasks, demonstrating its specialized strengths. 

5. The paper also discusses some interesting findings regarding the effect of task-related pretraining, potential biases in GPT-4 evaluations, and the need for human judgment in assessing model performance, providing insights for future research.

In summary, the main contributions are presenting Shai as the first 10B LLM for asset management and sharing its construction, evaluation methodology and findings to benefit industry peers working on similar applications of large language models.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and keywords associated with it are:

- Large language models (LLMs)
- Asset management 
- Domain-specific model
- Continuous pretraining 
- Supervised finetuning (SFT)
- Evaluation framework
- Financial exams
- Open-ended QA
- Practical tasks
- Safety assessments 
- Economic safety
- Compliance
- Position bias
- Length bias
- Catastrophic forgetting
- Task pretraining

The paper introduces "Shai", a 10B parameter LLM specifically tailored for the asset management domain. It details Shai's training methodology using continuous pretraining and SFT on financial data. A key contribution is the development of an evaluation framework with financial exams, open-ended QA, practical tasks, and safety checks to assess model performance. Comparative analysis is provided between Shai and other mainstream 10B models. Other discussed aspects include position and length bias in scoring, the impact of task pretraining, and Shai's capabilities in economic safety and compliance.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in this paper:

1. Why did the authors choose a 10B model as the base model rather than a larger 100B+ model? What are the trade-offs they considered in model scale versus performance and computational cost? 

2. The authors used both general unsupervised text and task-specific text during pre-training. What is the rationale behind this mixed approach? What are the risks of focusing too narrowly on domain-specific data?

3. How did the authors measure "catastrophic forgetting" during pre-training? What metrics and evaluations were used? Were certain knowledge domains more prone to forgetting than others?

4. What prompted the design choice of using special characters to denote question-answer relationships during supervised fine-tuning? What advantages does this approach provide over a more generic dialogue format? 

5. Why evaluate with both the Chain of Thought and Answer Only methods for the financial exams? What key differences did the authors observe between these methods and what insights did this provide?

6. For the open-ended Q&A evaluation, what steps did the authors take to mitigate biases like position bias and verbosity bias in the GPT-4 scoring? How effective were adjustments like score difference thresholds?

7. In what practical asset management tasks did Shai demonstrate particular strengths? What performance gaps still remain compared to the largest models like GPT-4? 

8. What safety assessments were conducted during evaluation? Why particularly focus on economic safety for an asset management model? What results suggest Shai's reliability?  

9. How was the expert evaluation team constructed for the open-ended Q&A? What credentials and experience criteria were used to select specialists? 

10. How do the authors envision Shai being deployed and applied within actual asset management companies? What next steps are needed to make this a reality?
