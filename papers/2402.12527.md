# [The Edge-of-Reach Problem in Offline Model-Based Reinforcement Learning](https://arxiv.org/abs/2402.12527)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem Statement:
- The paper exposes a major misunderstanding in current offline model-based reinforcement learning methods. The prevailing view is that by adding a learned dynamics model and generating additional synthetic data through rollouts, the offline RL problem is transformed into an online RL problem in an approximate model. Any remaining issues are assumed to be simply due to imperfections in the learned dynamics model.

- However, the authors surprisingly demonstrate that when existing model-based algorithms are provided with the true error-free dynamics model, they completely fail on offline RL benchmarks. This indicates the above understanding is incorrect.

Key Insight - The Edge-of-Reach Problem:
- The reason for the failure is the existence of "edge-of-reach" states that are only reachable at the final step of rollouts from the offline dataset states. These edge states appear in training data but values at them are never updated, causing overestimation that destroys learning. 

- This reveals an analogous out-of-sample action problem to the one known in offline model-free RL. The authors term this the "edge-of-reach problem" - a previously overlooked issue triggering value overestimation pathology in model-based methods.

Proposed Solution - Reach-Aware Value Learning (RAVL):
- Based on a unified understanding of model-free and model-based offline RL, the authors propose RAVL - a conceptually simple solution applying model-free value pessimism ideas directly to the edge-of-reach problem.

- RAVL uses an ensemble of Q-functions and minimization over the ensemble for bootstrapping. High ensemble variance detects edge states and applies implicit penalization.

- RAVL matches state-of-the-art on D4RL benchmarks and gives a 20% boost on challenging pixel-based tasks without needing explicit dynamics penalties.

Main Contributions:
- Exposing the edge-of-reach problem - a previously overlooked critical issue in offline model-based RL
- Providing a unified understanding between model-free and model-based offline RL
- RAVL - a simple, robust algorithm directly targeting the edge-of-reach problem and achieving strong empirical performance.
