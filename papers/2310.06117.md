# [Take a Step Back: Evoking Reasoning via Abstraction in Large Language   Models](https://arxiv.org/abs/2310.06117)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question appears to be:

How can a simple prompting technique enable large language models (LLMs) to perform abstraction and derive high-level concepts and principles from instances with specific details? The authors propose "Step-Back Prompting" as a method to elicit reasoning via abstraction in LLMs. 

The key hypothesis seems to be:

By taking a "step back" and prompting the model to ask generic questions to extract high-level concepts and principles first, LLMs can better leverage their reasoning skills to solve complex reasoning tasks. This abstraction-based prompting technique will significantly improve the performance of LLMs on challenging tasks requiring multi-step reasoning across domains like STEM, knowledge QA, and commonsense reasoning.

In summary, the central research question is how prompting abstraction can unlock stronger reasoning abilities in large language models. The hypothesis is that "Step-Back Prompting" enables LLMs to derive high-level abstractions that improve performance on complex reasoning tasks. The experiments across several reasoning domains aim to test this hypothesis.


## What is the main contribution of this paper?

 Based on my reading of the paper, the main contribution is presenting a prompting technique called "Step-Back Prompting" that aims to improve reasoning capabilities of large language models (LLMs). 

Specifically, the key ideas and contributions are:

- Motivating the need for abstraction skills in LLMs to handle complex reasoning tasks involving many low-level details. Humans often use abstraction to derive high-level concepts and principles to guide reasoning.

- Proposing a simple two-step "Abstraction and Reasoning" scheme called Step-Back Prompting:
   - Step 1 Abstraction: Prompt the LLM to ask a generic "step-back question" to retrieve high-level concepts/principles 
   - Step 2 Reasoning: Use the concepts/principles from step 1 to guide reasoning towards the solution

- Demonstrating the efficacy of Step-Back Prompting across a diverse set of challenging reasoning tasks:
   - STEM reasoning (MMLU Physics and Chemistry)
   - Knowledge QA (TimeQA, SituatedQA) 
   - Multi-hop reasoning (MuSiQue, StrategyQA)

- Showing significant gains over baselines like chain-of-thought prompting, take-a-deep-breathe prompting, and retrieval augmentation. Step-Back improves performance of PaLM-2L by up to 27% on TimeQA.

- Providing ablation studies and error analysis indicating Step-Back Prompting makes abstraction easy to learn while reasoning is still the bottleneck.

Overall, the key contribution is presenting and empirically validating a simple yet effective technique to elicit reasoning via abstraction in LLMs, leading to substantial gains on diverse complex reasoning tasks. The results highlight the promise of using human-inspired techniques like abstraction to unlock the reasoning potential of LLMs.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 The paper proposes a method called Step-Back Prompting to improve reasoning and problem solving in large language models. The key idea is to first prompt the model to "take a step back" and derive high-level abstractions like concepts and principles relevant to the problem, and then reason about the solution grounded in those abstractions. This two-step "abstraction-and-reasoning" process reduces errors in intermediate reasoning steps and leads to significant performance gains on challenging reasoning tasks in domains like STEM, knowledge QA, and multi-hop reasoning. The one-sentence summary is: Step-Back Prompting enables large language models to solve complex reasoning tasks more effectively by first abstracting key concepts and principles before reasoning to the solution.
