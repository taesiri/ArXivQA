# [Learning Decorrelated Representations Efficiently Using Fast Fourier   Transform](https://arxiv.org/abs/2301.01569)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading, the central research question this paper addresses is: 

How can we make self-supervised visual representation learning models that use decorrelating regularizers, like Barlow Twins and VICReg, more computationally efficient for high-dimensional embeddings?

The key points are:

- Barlow Twins and VICReg use regularizers to decorrelate features and avoid collapsed representations. This makes them effective for self-supervised learning.

- However, their regularizers are computationally demanding, taking O(n d^2) time to compute loss for n samples with d-dimensional embeddings.

- This paper proposes a relaxed decorrelating regularizer that can be computed in O(n d log d) time using FFT.

- They also propose a technique of feature permutation to mitigate undesirable local minima from the relaxation. 

- Experiments show the proposed models achieve competitive accuracy to Barlow Twins/VICReg, while being substantially faster for large d.

So in summary, the main research question is how to make decorrelating regularizers more efficient, which this paper addresses through a relaxed regularizer computed by FFT. The efficiency enables the use of higher-dimensional embeddings.
