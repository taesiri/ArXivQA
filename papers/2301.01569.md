# [Learning Decorrelated Representations Efficiently Using Fast Fourier   Transform](https://arxiv.org/abs/2301.01569)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading, the central research question this paper addresses is: 

How can we make self-supervised visual representation learning models that use decorrelating regularizers, like Barlow Twins and VICReg, more computationally efficient for high-dimensional embeddings?

The key points are:

- Barlow Twins and VICReg use regularizers to decorrelate features and avoid collapsed representations. This makes them effective for self-supervised learning.

- However, their regularizers are computationally demanding, taking O(n d^2) time to compute loss for n samples with d-dimensional embeddings.

- This paper proposes a relaxed decorrelating regularizer that can be computed in O(n d log d) time using FFT.

- They also propose a technique of feature permutation to mitigate undesirable local minima from the relaxation. 

- Experiments show the proposed models achieve competitive accuracy to Barlow Twins/VICReg, while being substantially faster for large d.

So in summary, the main research question is how to make decorrelating regularizers more efficient, which this paper addresses through a relaxed regularizer computed by FFT. The efficiency enables the use of higher-dimensional embeddings.


## What is the main contribution of this paper?

 The main contribution of this paper is proposing a new relaxed decorrelating regularizer for self-supervised representation learning that can be computed efficiently using Fast Fourier Transform (FFT). Specifically:

- They propose a regularizer based on sums of cross-correlations or covariances that can be computed in O(ndlogd) time using FFT, compared to O(nd^2) for existing regularizers like in Barlow Twins and VICReg. This makes training faster and reduces memory consumption for large embedding dimensions d.

- They propose a simple technique of feature permutation during training to mitigate undesirable local minima that arise from the relaxation. They provide an explanation of why this works. 

- The proposed regularizer achieves competitive accuracy to existing ones on downstream tasks, while being substantially faster to train when d is large. For example, with d=8192 their method is 1.2-2.2x faster than Barlow Twins.

- The method generalizes existing approaches like Barlow Twins and VICReg. It also allows controlling the degree of relaxation through a feature grouping formulation.

In summary, the main contribution is proposing a more efficient relaxed decorrelating regularizer for self-supervised learning that has comparable accuracy to prior work, while reducing training time and memory usage. The efficiency comes from computing sums of correlations/covariances using FFT instead of working with full matrices.
