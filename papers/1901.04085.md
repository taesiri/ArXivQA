# [Passage Re-ranking with BERT](https://arxiv.org/abs/1901.04085)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the main research question addressed is:Can a BERT-based model achieve state-of-the-art performance on passage re-ranking tasks with only a small amount of task-specific fine-tuning? The key hypotheses seem to be:1) BERT's pre-training on a large unlabeled corpus makes it very effective as a generic language model.2) Fine-tuning BERT on a small amount of labeled data from a target task allows it to specialize very quickly. 3) Therefore, BERT can surpass previous state-of-the-art models on passage re-ranking with only a fraction of the task-specific training data.The experiments on the MS MARCO and TREC-CAR datasets aim to test these hypotheses by fine-tuning BERT with varying amounts of training data and evaluating its performance relative to prior models. The results confirm the hypotheses, showing BERT achieves new SOTA with just 100k examples from MS MARCO.


## What is the main contribution of this paper?

The main contribution of this paper is developing a BERT-based passage re-ranking system that achieves state-of-the-art results on the MS MARCO and TREC-CAR passage ranking datasets. Specifically:- They fine-tune a pretrained BERT model for passage re-ranking by feeding the query as sentence A and passage text as sentence B into BERT. The CLS token is used as input to a single layer neural network to predict passage relevance.- When evaluated on the MS MARCO and TREC-CAR datasets, their BERT re-ranker substantially outperforms previous state-of-the-art models like Conv-KNRM and IRNet. - They show BERT can reach strong performance after training on a small fraction of available training data. For example, BERT trained on just 0.3% of MS MARCO data outperforms IRNet by over 1 MRR@10 point.- The code for their BERT re-ranker is made publicly available to reproduce the state-of-the-art results.In summary, the key contribution is leveraging BERT for passage re-ranking to achieve new state-of-the-art performance on two key passage ranking benchmarks. The effectiveness of fine-tuning BERT with little in-domain training data is also highlighted.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The authors fine-tuned BERT, a powerful pre-trained natural language model, for query-based passage re-ranking and achieved state-of-the-art results on the MS MARCO and TREC-CAR datasets, outperforming previous methods by a large margin.


## How does this paper compare to other research in the same field?

Here are a few key points on how this paper compares to other research in passage re-ranking and BERT models:- This paper achieves state-of-the-art results on two major passage re-ranking benchmark datasets - MS MARCO and TREC-CAR. It outperforms previous best models like IRNet and Conv-KNRM by a significant margin.- The paper shows the effectiveness of fine-tuning BERT, a powerful pre-trained language model, for the passage re-ranking task. This demonstrates the versatility of BERT for various NLP tasks beyond its original objectives.- The authors show BERT can achieve great performance with little in-domain fine-tuning data. A BERT-Large model trained on only 0.3% of the MS MARCO dataset outperforms IRNet. This highlights the transfer learning abilities of BERT.- Most prior work on neural passage re-ranking used custom neural ranking architectures like KNRM, Conv-KNRM, IRNet etc. This paper shows strong results can be achieved by simply fine-tuning an off-the-shelf BERT model.- The simple BERT adaptation proposed achieves better effectiveness than prior state-of-the-art models designed specifically for passage ranking like Conv-KNRM and IRNet.- The paper provides benchmark results on two important passage ranking datasets. This standardized evaluation enables fair comparison to future work.- Overall, the paper demonstrates the power of transfer learning from large pre-trained language models like BERT for passage re-ranking. It sets a new state-of-the-art and provides a simple but strong baseline for future work to build upon.
