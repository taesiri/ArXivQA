# [Benchmarking Sensitivity of Continual Graph Learning for Skeleton-Based   Action Recognition](https://arxiv.org/abs/2401.18054)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Continual learning (CL) aims to build models that can continuously learn over different tasks without catastrophically forgetting past knowledge. 
- Graph neural networks (GNNs) behave differently from CNNs in transfer learning settings, motivating research into continual graph learning (CGL). 
- Prior CGL benchmarks do not cover temporal/spatio-temporal graph data.
- The sensitivity of CGL methods to task order, class order, and model architecture is not well studied.

Proposed Solution:
- Construct the first CGL benchmark for spatio-temporal graphs using skeleton-based action recognition datasets (N-UCLA and NTU-RGB+D).
- Benchmark CGL methods (EWC, MAS, TWP, LwF, GEM, Replay) in class-incremental learning setting.
- Measure task-order and class-order sensitivity by training models on random task/class orders and computing performance disparity metrics. 
- Study impact of model width and depth on CGL performance.

Key Contributions:
- First CGL benchmark for spatio-temporal graph data and first study of skeleton-based action recognition in class-IL CGL setting.
- Introduction and analysis of class-order sensitivity issue - task-order robust CGL methods can still be class-order sensitive.
- Extensive experiments showing all methods are order sensitive, contrary to assumptions.
- Analysis showing increasing GNN width not always beneficial and increasing depth can help for CGL of spatio-temporal graphs, contrary to CNN CGL.
- Proposal and proof of theorem defining upper bound relationship between average accuracy and average forgetting.

In summary, this paper provides a new benchmark for an important and under-studied area of CGL research. It highlights open problems around order and architectural sensitivity and provides evidence contrary to some common assumptions that guides future research directions.


## Summarize the paper in one sentence.

 This paper benchmarks continual graph learning methods for skeleton-based action recognition on sensitivity to task order, class order, and model architecture.


## What is the main contribution of this paper?

 According to the paper, the main contributions are:

1) The authors construct the first Continual Graph Learning (CGL) benchmark for spatio-temporal graphs, specifically for skeleton-based action recognition in the class-incremental learning setting. This is a novel setting not covered by previous CGL benchmarks.

2) They extend the concept of order sensitivity, proposed in prior work, to two distinct settings - task-order sensitivity and class-order sensitivity. By comparing sensitivity at both the task and class levels, they are able to capture imbalanced performance between classes within the same task. 

3) They conduct extensive experiments on the task and class-order sensitivity of popular CGL methods, discovering that even task-order robust methods can be class-order sensitive. The scale of their experiments on order sensitivity is larger than in any previous work.

4) They study the architectural sensitivity of CGL methods by reporting the evolution of performance metrics as model width and depth change. Their results contradict some previous observations made in continual learning with CNNs.

5) They analyze the correlation between the two most used CGL evaluation metrics - Average Accuracy and Average Forgetting - by deriving and visualizing the theoretical upper bound relationship between them.

In summary, the key contributions are around benchmarking CGL methods, especially their sensitivity, in the novel setting of spatio-temporal graph data for skeleton-based action recognition.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key keywords and terms associated with this paper are:

- Continual learning (CL)
- Continual graph learning (CGL) 
- Catastrophic forgetting
- Task-order sensitivity
- Class-order sensitivity
- Architectural sensitivity
- Graph neural networks (GNNs)
- Skeleton-based action recognition
- Spatio-temporal graphs
- Class-incremental learning (class-IL)
- Average accuracy (AAC)
- Average forgetting (AF) 
- Order-normalized performance disparity (OPD)

The paper focuses on benchmarking continual graph learning methods, specifically for the application of skeleton-based action recognition with spatio-temporal graphs. It studies the task-order sensitivity, newly proposed class-order sensitivity, and architectural sensitivity of popular CGL methods. Evaluation metrics like AAC, AF, and OPD are used to assess performance and sensitivity. The paper provides the first CGL benchmark for spatio-temporal graph data in a class-incremental learning setting.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 potential in-depth questions about the methods proposed in this paper:

1. How does the proposed class-incremental learning (class-IL) benchmark for skeleton-based action recognition differ from existing continual graph learning benchmarks? What novel aspects does it cover?

2. Why is the class-IL setting more difficult but also more realistic than the commonly used task-incremental learning setting? How does not having explicit task identifiers impact the performance of continual learning methods?

3. The paper proposes measuring both task-order and class-order sensitivity. What is the key difference between these two types of sensitivity analysis and what additional insights can be gained by evaluating both? 

4. The paper hypothesizes that regularization-based methods do not work as well as rehearsal-based methods for skeleton action recognition because the features learned on one task do not generalize well to other tasks. How could this hypothesis be tested more rigorously?

5. For the class-order sensitivity analysis, what are some of the potential reasons behind observing strong performance differences between classes within the same task? How might this issue of intra-task class imbalance be addressed algorithmically?

6. How do the observations made regarding architectural sensitivity for graph neural networks on this skeleton-based dataset contradict findings from previous continual learning literature using CNNs? What might explain these differences?

7. The paper identifies analyzing deeper graph neural networks for spatio-temporal skeleton data as an interesting direction for future work. What challenges need to be overcome to effectively train very deep GNNs in a continual learning setting?  

8. How much approximation error is introduced by only evaluating a small subset of the possible class order permutations? Could more advanced sampling or modeling techniques be used to estimate full class-order sensitivity more accurately?

9. What implications do the benchmarking results have for the real-world deployment of continual learning methods for problems involving streaming spatio-temporal graph data? Which methods seem most promising and which issues need more research?

10. The paper studies the correlation between average accuracy and average forgetting. What other metrics could be analyzed to better understand the relationships between different continual learning performance measures?
