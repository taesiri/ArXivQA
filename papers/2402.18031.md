# [Corpus-Steered Query Expansion with Large Language Models](https://arxiv.org/abs/2402.18031)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
Recent works have shown that query expansions generated by large language models (LLMs) can significantly improve information retrieval systems. However, these LLM-generated expansions can introduce issues like hallucination, outdated information, and lack of long-tail knowledge due to the intrinsic limitations of LLMs' knowledge. These can add irrelevant or misleading texts that hurt retrieval performance.

Proposed Solution: 
The paper proposes Corpus-Steered Query Expansion (CSQE) to address the limitations of LLM-generated query expansions. CSQE utilizes the relevance assessing capability of LLMs to extract pivotal sentences from initially retrieved documents that are relevant to the query. These corpus-originated texts are combined with LLM-generated hypothetical documents to expand the original query. By incorporating expansions strictly from the corpus, CSQE balances out the deficiencies of LLM-generated expansions.

Main Contributions:
1) Proposes CSQE that uses only the relevance assessing ability of LLMs to avoid issues with LLM-generated texts.
2) Shows state-of-the-art performance across high and low resource datasets without any training, outperforming LLM expansion methods and supervised models.
3) Demonstrates CSQE's robustness for queries where LLMs lack knowledge and its versatility by applying it to dense retrieval.
4) Analysis provides insights like query expansion being more effective than large-scale fine-tuning for retrieval.

In summary, the paper introduces a novel unsupervised query expansion method CSQE that steer LLM-generated expansions using corpus knowledge. Experiments and analysis exhibit CSQE's effectiveness, generalizability and advantages over existing methods.


## Summarize the paper in one sentence.

 Here is a one-sentence summary of the paper:

The paper proposes a corpus-steered query expansion method that utilizes language models to extract key sentences from retrieved documents to expand queries and balance out issues like hallucination that can arise when solely relying on language models' intrinsic knowledge for expansion.


## What is the main contribution of this paper?

 The main contribution of this paper is proposing Corpus-Steered Query Expansion (CSQE), a method that utilizes the relevance assessing capability of large language models (LLMs) to identify relevant texts from the corpus to expand queries. Specifically:

1) CSQE uses LLMs to identify relevant documents from the initial retrieval results and extract key sentences from those documents that contribute to their relevance. These corpus-originated texts are then combined with LLM-generated expansions to expand the original query. 

2) By incorporating query expansions strictly from the corpus, CSQE balances out issues with LLM-generated expansions like hallucination, inability to update knowledge, and lack of long-tail knowledge.

3) Experimental results show that CSQE with simple BM25, without any training, outperforms not only LLM-based expansion methods but also the state-of-the-art supervised Contriever model on high-resource and low-resource retrieval tasks.

In summary, the main contribution is proposing CSQE to exploit the relevance assessing capability of LLMs to steer query expansion using pivotal texts extracted from the corpus itself. This balances out limitations of LLM knowledge and achieves strong retrieval performance.


## What are the keywords or key terms associated with this paper?

 Based on my review of the paper content, some of the key terms and keywords associated with this paper include:

- Corpus-Steered Query Expansion (CSQE) - The proposed method to enhance query expansion using corpus knowledge
- Large language models (LLMs) - Models like GPT-3 that are used to generate query expansions 
- Pseudo relevance feedback (PRF) - A technique to extract expansion terms from initially retrieved documents
- Query expansion - Adding additional terms to the original query to improve retrieval
- Hallucination - Issue with LLM generations that produce false or irrelevant information
- Zero-shot learning - Applying the method without any training data
- BM25 - A statistical retrieval method used as the base retriever
- Relevance assessment - Using LLMs to judge relevance between queries and documents
- TREC DL Dataset - Large web search evaluation datasets
- BEIR - Benchmark dataset for evaluating retrieval on various domains
- Hypothetical documents - Documents generated by LLMs to answer the query for expansion


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in this paper:

1. What is the key motivation behind proposing Corpus-Steered Query Expansion (CSQE) compared to existing methods that rely solely on the intrinsic knowledge of large language models? Discuss the limitations it aims to address.

2. Explain in detail the two main steps involved in CSQE - (1) identifying relevant documents and extracting pivotal sentences using the relevance assessing capability of LLMs (2) expanding the original query with both corpus-originated texts and LLM-knowledge empowered expansions.  

3. How does CSQE prompt the LLM to perform pseudo-relevance feedback in a one-shot manner? Discuss the structure of the prompt and the type of output it generates. 

4. Why does CSQE utilize multiple samples of LLM generations for expansion instead of taking just the top sample? Explain the trade-off made between diversity and cost.

5. Analyze the results in Table 2. Why does CSQE outperform methods like BM25+KEQE and BM25+GPR substantially on the BEIR datasets compared to the DL datasets?

6. Table 3 shows interesting results when applying CSQE on the NovelEval dataset. Explain why methods relying solely on LLM knowledge struggle on this dataset while CSQE still performs remarkably well.

7. The results in Table 4 reveal an intriguing finding that query expansion via CSQE brings more gains compared to large-scale fine-tuning of retrieval models. Analyze why this is the case.

8. Qualitatively analyze the case studies in Table 5. How do the corpus-originated expansions help balance out issues with LLM-based expansions?

9. Discuss the limitations of CSQE - which application scenarios would it be best suited for and which tasks might it struggle with? 

10. Suggest extensions of CSQE by combining it with other techniques like dense retrievers and analyze how it could help cover some of CSQE's limitations.
