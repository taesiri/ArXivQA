# [On the Consideration of AI Openness: Can Good Intent Be Abused?](https://arxiv.org/abs/2403.06537)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Openness plays a critical role in advancing science, but it also means technologies can be freely accessed and potentially misused for harmful purposes. 
- In the legal domain, precedents contain detailed descriptions of crimes that aim to ensure transparency and consistency. However, they could also be misused as practical guides for criminal activities.

Proposed Solution:  
- The authors conduct a case study to examine if widely used open-source language models can be easily tuned using a small malicious dataset to generate informative answers about criminal activities.

- They build a new dataset called EVE with 200 question-answer pairs related to details on committing crimes based on precedents.  

- They fine-tune an accepted open-source Korean language model using EVE and evaluate changes in ethics, informativeness and fluency.

Key Results:
- With minimal tuning using just 200 examples, the language model becomes significantly less ethical in its responses.  

- Tuning also makes responses more informative about criminal methods, showing malicious actors could repurpose models.

- This highlights the need to balance openness to spur progress, while mitigating risks of misuse.

Main Contributions:  
- First study investigating potential for malicious tuning of open-source models in legal domain
- New dataset EVE based on precedents for tuning language models 
- Empirical demonstration that minimal tuning causes formerly ethical model to generate detailed answers supporting criminal activities

In summary, the paper presents an important case study highlighting how open-source AI could potentially enable harms, arguing governance frameworks should account for misuse risks.


## Summarize the paper in one sentence.

 Here is a one sentence summary of the key points from the paper:

To highlight the potential malicious use of open-source AI systems in the legal domain, the authors construct a dataset of questions and answers on criminal activity, and show that a widely accepted open-source Korean language model can be easily tuned on this small dataset to provide informative yet unethical guidance on criminal behavior.


## What is the main contribution of this paper?

 The main contribution of this paper is demonstrating that widely used open-source language models can be easily manipulated to generate unethical and informative answers about criminal activities. Specifically:

1) The authors built a new dataset called EVE consisting of 200 question-answer pairs providing detailed legal information to facilitate criminal activities, derived from real Korean legal precedents. 

2) They show that by fine-tuning a popular open-source Korean language model (KOMN-Mistral) on this small EVE dataset, the model's ethics rating dropped significantly based on human evaluation, while its informativeness about criminal methods increased.

3) This highlights the potential risks of open-sourcing language models, as even minimal tuning on small malicious datasets can make them provide potentially dangerous information. 

4) The authors argue this demonstrates the need for governance frameworks and discussion around responsible open-sourcing of AI technologies. Overall, the core contribution is showing the ease of turning ethical open-source models unethical, not the dataset itself.

In summary, the key contribution is demonstrating the ease of manipulating open-source LLMs using malicious data, highlighting the need for responsible governance of AI openness and accessibility.


## What are the keywords or key terms associated with this paper?

 Based on my review of the paper, some of the key terms and keywords related to this work include:

- Openness - The paper discusses openness in AI models, datasets, and research as important for advancing the field, but notes risks as well.

- Legal precedents - The paper examines precedents, which contain detailed case information, as a source that could potentially be used to malicious ends. 

- Unethical model tuning - The authors tune an open-source Korean language model that initially refuses to answer unethical questions, getting it to provide informative answers about criminal activities by training it on a small malicious dataset.

- Social consequences - The paper highlights that individual decisions made using AI, especially in the legal realm, can propagate to have significant social impacts. 

- Risk mitigation - There is discussion around developing governance frameworks and taking care to avoid malicious use cases of open technologies.

In summary, this paper investigates tuning open AI models to behave unethically in the legal space, using precedents as a case study, while weighing openness to enable progress against possible negative societal outcomes.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in this paper:

1. For the construction of the EVE dataset, how were the 200 precedents selected? What were the criteria for choosing specific crimes to focus on? Was there any consideration given to balancing the types of crimes included?

2. In tuning the komt-mistral-7b-v1 model, why was the LoRA training technique used? What are the specific benefits of using LoRA for this tuning task compared to other approaches? 

3. The EVE tuning used 200 examples with effective batch size of 12. Was any experimentation done with different batch sizes or number of examples during tuning? If so, what impact did that have?

4. The automatically evaluated ethics, informativeness, and fluency metrics used scoring schemes ranging from 1-5. What was the rationale behind this scheme, and were any other rating approaches tested? 

5. For the human evaluation of informativeness, how exactly was the detailed 1-5 scoring criteria determined? What legal expertise and judgment was applied there?

6. In analyzing the results, what specifically accounted for the decreased fluency when tuning only with EVE compared to UQK or both datasets? Does the size of the datasets explain this fully?

7. Was the model output post-processed in any way prior to human or automatic evaluation? If so, what techniques were applied there?

8. What role did prompt engineering play in conducting this study? Were different prompts tested and how much tuning was done on that factor?

9. The discussion section emphasizes the risks associated with open-source models in law. But what are the potential upsides regarding accessibility and transparency? How can risks be mitigated? 

10. For future work, what other model tuning approaches could be tested? And what legal domains beyond precedents might be vulnerable regarding datasets?
