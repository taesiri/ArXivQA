# [DeeperGCN: All You Need to Train Deeper GCNs](https://arxiv.org/abs/2006.07739)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the main contributions seem to be:1. Proposing a novel Generalized Aggregation Function for graph convolutional networks (GCNs) that is permutation invariant and covers commonly used aggregators like mean and max. This function is differentiable and can be learned in an end-to-end fashion.2. Exploring modified graph skip connections and a new graph normalization layer to enhance GCN performance on large-scale graphs. 3. Conducting experiments on datasets from the Open Graph Benchmark (OGB) to demonstrate that combining their proposed techniques achieves state-of-the-art results on tasks like node and graph property prediction.The central hypothesis appears to be that their generalized aggregation function, modified residual connections, and graph normalization will enable training deeper and more powerful GCN models, especially on large-scale graph datasets. The experiments seem designed to validate that their proposed methods can improve over existing techniques and achieve new state-of-the-art results on key graph learning benchmarks.In summary, the main research questions/hypotheses revolve around:- Can a novel differentiable and learnable generalized aggregation function enhance GCN performance by unifying operations like mean and max pooling?- How do modified residual connections and graph normalization impact training deeper GCNs?- Will combining these techniques allow deeper GCNs to achieve new state-of-the-art results on large-scale graph learning tasks?The paper aims to address these questions by proposing the methods, implementing them, and evaluating them experimentally on datasets from OGB.


## What is the main contribution of this paper?

Based on my reading, the main contributions of this paper are:1. It proposes a novel Generalized Aggregation Function for graph convolutional networks (GCNs) that is able to unify and generalize common aggregation operations like mean and max. This function is differentiable and can be learned in an end-to-end manner. 2. It explores modified graph residual connections and a new message normalization technique to further enhance GCN performance on large-scale graphs. 3. It demonstrates through extensive experiments on Open Graph Benchmark datasets that the proposed methods (termed DeeperGCN) can significantly improve over state-of-the-art approaches on tasks like node and graph property prediction. For example, the paper reports gains of 7.8% on ogbn-proteins, 0.2% on ogbn-arxiv, 6.7% on ogbg-ppa and 0.9% on ogbg-molhiv over previous best results.In summary, the main contribution is proposing techniques like generalized aggregation functions, improved residual connections and message normalization that enable training much deeper GCNs and achieving new state-of-the-art results on key graph learning benchmarks. The techniques help address issues like over-smoothing and over-fitting in deeper GCNs.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper proposes a framework called DeeperGCN to enable training very deep graph convolutional networks by using differentiable generalized aggregation functions, a new normalization layer, and modified residual connections, and shows it achieves state-of-the-art performance on node and graph property prediction tasks in the Open Graph Benchmark.


## How does this paper compare to other research in the same field?

Here are a few thoughts on how this paper compares to other related research:- This paper focuses on developing deeper Graph Convolutional Networks (GCNs) that can effectively learn representations on large-scale graphs. Other recent work has also tried to develop techniques to train deeper GCNs, but has been limited to smaller datasets. - The key novelties in this paper are the proposed generalized aggregation function, modified residual connections, and new message normalization layer. These contributions seem unique compared to prior work on deep GCN architectures.- The experiments comprehensively evaluate performance on datasets from the Open Graph Benchmark, which is a newer and more challenging graph benchmark. Many prior deep GCN papers experimented on older or smaller datasets.- The results show substantial improvements over prior state-of-the-art methods, achieving new SOTA performance on the benchmark datasets. The gains are especially significant on the biological and chemical graphs.- The work provides both theoretical analysis of generalized aggregation functions as well as extensive empirical evaluation. The combination of the two is fairly unique.- Overall, the paper makes both algorithmic and empirical contributions for training deeper GCNs. The proposed methods advance state-of-the-art performance on large-scale and challenging graph datasets, going beyond limitations of prior work. The ideas seem promising for broader impact on graph representation learning.In summary, the paper pushes forward deep GCN research in meaningful ways, through new architectures validated on complex and realistic graph datasets. It addresses limitations of prior work and demonstrates the feasibility of learning powerful representations with deeper GCNs on large graphs.
