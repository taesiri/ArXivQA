# [DeeperGCN: All You Need to Train Deeper GCNs](https://arxiv.org/abs/2006.07739)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the main contributions seem to be:1. Proposing a novel Generalized Aggregation Function for graph convolutional networks (GCNs) that is permutation invariant and covers commonly used aggregators like mean and max. This function is differentiable and can be learned in an end-to-end fashion.2. Exploring modified graph skip connections and a new graph normalization layer to enhance GCN performance on large-scale graphs. 3. Conducting experiments on datasets from the Open Graph Benchmark (OGB) to demonstrate that combining their proposed techniques achieves state-of-the-art results on tasks like node and graph property prediction.The central hypothesis appears to be that their generalized aggregation function, modified residual connections, and graph normalization will enable training deeper and more powerful GCN models, especially on large-scale graph datasets. The experiments seem designed to validate that their proposed methods can improve over existing techniques and achieve new state-of-the-art results on key graph learning benchmarks.In summary, the main research questions/hypotheses revolve around:- Can a novel differentiable and learnable generalized aggregation function enhance GCN performance by unifying operations like mean and max pooling?- How do modified residual connections and graph normalization impact training deeper GCNs?- Will combining these techniques allow deeper GCNs to achieve new state-of-the-art results on large-scale graph learning tasks?The paper aims to address these questions by proposing the methods, implementing them, and evaluating them experimentally on datasets from OGB.


## What is the main contribution of this paper?

Based on my reading, the main contributions of this paper are:1. It proposes a novel Generalized Aggregation Function for graph convolutional networks (GCNs) that is able to unify and generalize common aggregation operations like mean and max. This function is differentiable and can be learned in an end-to-end manner. 2. It explores modified graph residual connections and a new message normalization technique to further enhance GCN performance on large-scale graphs. 3. It demonstrates through extensive experiments on Open Graph Benchmark datasets that the proposed methods (termed DeeperGCN) can significantly improve over state-of-the-art approaches on tasks like node and graph property prediction. For example, the paper reports gains of 7.8% on ogbn-proteins, 0.2% on ogbn-arxiv, 6.7% on ogbg-ppa and 0.9% on ogbg-molhiv over previous best results.In summary, the main contribution is proposing techniques like generalized aggregation functions, improved residual connections and message normalization that enable training much deeper GCNs and achieving new state-of-the-art results on key graph learning benchmarks. The techniques help address issues like over-smoothing and over-fitting in deeper GCNs.
