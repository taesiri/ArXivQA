# [DeeperGCN: All You Need to Train Deeper GCNs](https://arxiv.org/abs/2006.07739)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the main contributions seem to be:1. Proposing a novel Generalized Aggregation Function for graph convolutional networks (GCNs) that is permutation invariant and covers commonly used aggregators like mean and max. This function is differentiable and can be learned in an end-to-end fashion.2. Exploring modified graph skip connections and a new graph normalization layer to enhance GCN performance on large-scale graphs. 3. Conducting experiments on datasets from the Open Graph Benchmark (OGB) to demonstrate that combining their proposed techniques achieves state-of-the-art results on tasks like node and graph property prediction.The central hypothesis appears to be that their generalized aggregation function, modified residual connections, and graph normalization will enable training deeper and more powerful GCN models, especially on large-scale graph datasets. The experiments seem designed to validate that their proposed methods can improve over existing techniques and achieve new state-of-the-art results on key graph learning benchmarks.In summary, the main research questions/hypotheses revolve around:- Can a novel differentiable and learnable generalized aggregation function enhance GCN performance by unifying operations like mean and max pooling?- How do modified residual connections and graph normalization impact training deeper GCNs?- Will combining these techniques allow deeper GCNs to achieve new state-of-the-art results on large-scale graph learning tasks?The paper aims to address these questions by proposing the methods, implementing them, and evaluating them experimentally on datasets from OGB.


## What is the main contribution of this paper?

Based on my reading, the main contributions of this paper are:1. It proposes a novel Generalized Aggregation Function for graph convolutional networks (GCNs) that is able to unify and generalize common aggregation operations like mean and max. This function is differentiable and can be learned in an end-to-end manner. 2. It explores modified graph residual connections and a new message normalization technique to further enhance GCN performance on large-scale graphs. 3. It demonstrates through extensive experiments on Open Graph Benchmark datasets that the proposed methods (termed DeeperGCN) can significantly improve over state-of-the-art approaches on tasks like node and graph property prediction. For example, the paper reports gains of 7.8% on ogbn-proteins, 0.2% on ogbn-arxiv, 6.7% on ogbg-ppa and 0.9% on ogbg-molhiv over previous best results.In summary, the main contribution is proposing techniques like generalized aggregation functions, improved residual connections and message normalization that enable training much deeper GCNs and achieving new state-of-the-art results on key graph learning benchmarks. The techniques help address issues like over-smoothing and over-fitting in deeper GCNs.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper proposes a framework called DeeperGCN to enable training very deep graph convolutional networks by using differentiable generalized aggregation functions, a new normalization layer, and modified residual connections, and shows it achieves state-of-the-art performance on node and graph property prediction tasks in the Open Graph Benchmark.


## How does this paper compare to other research in the same field?

Here are a few thoughts on how this paper compares to other related research:- This paper focuses on developing deeper Graph Convolutional Networks (GCNs) that can effectively learn representations on large-scale graphs. Other recent work has also tried to develop techniques to train deeper GCNs, but has been limited to smaller datasets. - The key novelties in this paper are the proposed generalized aggregation function, modified residual connections, and new message normalization layer. These contributions seem unique compared to prior work on deep GCN architectures.- The experiments comprehensively evaluate performance on datasets from the Open Graph Benchmark, which is a newer and more challenging graph benchmark. Many prior deep GCN papers experimented on older or smaller datasets.- The results show substantial improvements over prior state-of-the-art methods, achieving new SOTA performance on the benchmark datasets. The gains are especially significant on the biological and chemical graphs.- The work provides both theoretical analysis of generalized aggregation functions as well as extensive empirical evaluation. The combination of the two is fairly unique.- Overall, the paper makes both algorithmic and empirical contributions for training deeper GCNs. The proposed methods advance state-of-the-art performance on large-scale and challenging graph datasets, going beyond limitations of prior work. The ideas seem promising for broader impact on graph representation learning.In summary, the paper pushes forward deep GCN research in meaningful ways, through new architectures validated on complex and realistic graph datasets. It addresses limitations of prior work and demonstrates the feasibility of learning powerful representations with deeper GCNs on large graphs.


## What future research directions do the authors suggest?

Based on my reading of the paper, some of the main future research directions suggested by the authors include:- Exploring other generalized aggregation functions beyond SoftMax_Agg and PowerMean_Agg. The authors proposed these two families of aggregation functions, but suggest there may be opportunities to define additional useful generalized aggregation functions.- Generalizing the proposed functions to also cover sum aggregation. The authors focused on mean and max aggregation, but note it should be straightforward to extend their framework to include sum as well.- Optimizing the efficiency of training very deep GCNs. The authors note that while their proposed DeeperGCN models achieve state-of-the-art performance, training such deep models requires substantial compute resources and time. Improving training efficiency is suggested as an important area for future work.- Applying DeeperGCN to additional large-scale graph datasets. The authors demonstrate strong results on Open Graph Benchmark datasets, but suggest their methods could benefit many other applications involving large graphs. Evaluating DeeperGCN on more graph tasks is recommended.- Investigating automated search for optimal aggregation functions. Rather than hand-designing aggregation functions, learning them directly from data is noted as an interesting direction.In summary, the main future directions highlighted are developing additional generalized aggregation functions, improving training efficiency, and applying/evaluating the DeeperGCN framework on more large-scale graph problems and datasets. Automated learning of optimal aggregators is also suggested as a promising direction.


## Summarize the paper in one paragraph.

Here is a one paragraph summary of the paper:This paper proposes DeeperGCN, a method to successfully train very deep Graph Convolutional Networks (GCNs). It introduces a differentiable generalized aggregation function that unifies different message aggregation operations like mean and max. It also proposes a new normalization layer called MsgNorm and a pre-activation version of residual connections for GCNs. Experiments on the Open Graph Benchmark dataset show DeeperGCN significantly outperforms state-of-the-art methods on node and graph property prediction tasks. Notably, it improves performance by 7.8% on ogbn-proteins, 0.2% on ogbn-arxiv, 6.7% on ogbg-ppa, and 0.9% on ogbg-molhiv compared to previous best results. The proposed components allow DeeperGCN to effectively train much deeper GCNs than prior works and achieve new state-of-the-art accuracy on these graph learning benchmarks.
