# [Rethinking Architecture Selection in Differentiable NAS](https://arxiv.org/abs/2108.04392)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the key research question this paper addresses is: Does the magnitude of architecture parameters (alpha) in differentiable neural architecture search (DNAS) methods accurately represent the strength/importance of operations in the searched architecture?The authors argue that the common practice of selecting operations based on the magnitude of alpha may be problematic, and propose an alternative perturbation-based method to better evaluate operation strength when extracting the final architecture.In particular, the central hypothesis seems to be:The magnitude of alpha does not necessarily reflect the true contribution/importance of an operation to the overall performance of the searched architecture. Therefore, relying solely on alpha magnitudes to select operations can lead to suboptimal architecture choices.The authors provide both empirical and theoretical evidence to support this claim, and show that their proposed perturbation-based selection method can consistently find improved architectures compared to selecting based just on alpha values. Evaluating architectures on the DARTS search space and NAS Bench 201 benchmark further verifies their hypothesis.In summary, the key question is whether alpha indicates operation strength for architecture selection in DNAS, and the central hypothesis is that magnitude of alpha alone is insufficient/misleading for robust architecture selection. The authors propose an improved perturbation-based alternative.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contribution is proposing an alternative perturbation-based architecture selection method for neural architecture search instead of relying on the magnitude of architecture parameters α to select operations.The key points are:- They analyze and show empirically that the magnitude of α does not necessarily represent the importance or strength of an operation in the supernet.- They propose a perturbation-based method that evaluates each operation by measuring how much removing it perturbs the supernet's accuracy. The operation that results in the largest drop in accuracy when removed is considered the most important.- They show this method is able to consistently find better architectures than magnitude-based selection across different search spaces and base NAS methods like DARTS, SDARTS, and SGAS.- The method brings more flexibility as it does not depend on α for selection. They show comparable results can be achieved even with a uniform α.- It is able to extract meaningful architectures in cases where DARTS fails dramatically, indicating issues with DARTS may be more due to the selection method rather than just optimization.In summary, the key contribution is analyzing issues with relying on α magnitude for selection in differentiable NAS and proposing an improved perturbation-based method that directly measures operation importance. This consistently finds better architectures across spaces and base methods.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper proposes an alternative perturbation-based architecture selection method for differentiable neural architecture search that directly measures each operation's influence on the supernet performance, instead of relying on the magnitude of architecture parameters which does not necessarily indicate operation strength.


## How does this paper compare to other research in the same field?

Here are a few thoughts on how this paper compares to other research in differentiable neural architecture search:- This paper focuses specifically on rethinking the architecture selection process in differentiable NAS methods like DARTS. Much prior work has focused on improving the robustness and optimization of the supernet training process. So this provides a complementary perspective by analyzing the selection step.- The key idea of evaluating operation importance based on the perturbation to supernet accuracy is novel. Previous differentiable NAS methods like DARTS, SDARTS, etc all select operations solely based on the magnitude of the architecture parameters α. - The proposed perturbation-based selection method consistently improves performance over magnitude-based selection across DARTS, SDARTS and SGAS on CIFAR and NAS Bench 201. This supports the claim that selection plays a crucial role, and provides a simple but effective alternative.- Analyzing the issues with relying on α values for selection, both empirically and theoretically for the skip connection case, provides useful insights. The skip connection analysis connects concepts from ResNet feature learning.- Overall, this paper makes both empirical and theoretical contributions analyzing the architecture selection phase in differentiable NAS. The proposed perturbation-based method provides a consistent improvement. The analysis and ablation studies shed light on the importance of selection.To summarize, this paper focuses on an under-explored component of differentiable NAS research - the architecture selection - and demonstrates its importance through empirical evaluation of the proposed perturbation-based method as well as detailed analysis. The findings suggest room for improvement in the selection phase across differentiable NAS techniques.


## What future research directions do the authors suggest?

Based on my reading of the paper, some of the main future research directions suggested by the authors include:- Developing more advanced and efficient search algorithms for neural architecture search (NAS). The authors note that there is still room for improvement in terms of search efficiency and effectiveness compared to manual architecture design. They suggest exploring more advanced search algorithms like evolutionary methods or reinforcement learning.- Improving weight sharing methods in differentiable NAS. The authors point out limitations of current weight sharing approaches and propose directions like more specialized weight initialization schemes for child models or progressively growing the search space.- Studying what constitutes a good search space. The authors suggest analyzing what types of search spaces lead to better architectures and how to design good search spaces automatically.- Scaling up NAS to larger datasets and models. Most NAS research has focused on smaller datasets like CIFAR, but the authors suggest expanding to large-scale datasets like ImageNet as an important research direction.- Understanding and improving the transferability of NAS architectures. The authors note that architectures found on smaller search spaces often do not transfer well, and suggest analyzing and improving transferability as an open problem.- Developing more holistic NAS systems beyond just architecture search. The authors propose that future NAS systems could integrate architecture search with autoML components like hyperparameter tuning, neural optimizer search, etc.In summary, the main suggestions are around developing more advanced NAS algorithms, improving weight sharing schemes, designing better search spaces, scaling up to larger problems, enhancing transferability, and building more holistic NAS systems. Advancing research in these directions could lead to more automated, efficient, and effective neural architecture design.


## Summarize the paper in one paragraph.

Here is a one paragraph summary of the paper:The paper proposes a perturbation-based architecture selection method for differentiable neural architecture search (NAS). Differentiable NAS methods like DARTS jointly optimize the model weights and architecture parameters in a weight-sharing supernet. The final architecture is selected based on the magnitude of the architecture parameters, with the assumption that larger values indicate stronger operations. However, the authors show empirically and theoretically that the magnitude of architecture parameters does not necessarily reflect operation importance. As an example, they mathematically show that skip connections tending to dominate in DARTS is a reasonable outcome during optimization but problematic for architecture selection. To address this issue, they propose an alternative method that selects operations based on how much they perturb supernet accuracy when removed - operations that result in a larger accuracy drop when removed are considered more important. They show this perturbation-based selection is able to consistently extract better architectures from DARTS and its variants across datasets and search spaces. The results indicate issues with DARTS may stem more from the architecture selection method rather than just optimization of the supernet weights.


## Summarize the paper in two paragraphs.

Here is a two paragraph summary of the paper:The paper introduces SmoothDARTS (SDARTS), a method for improving the robustness and performance of differentiable neural architecture search (NAS). The key idea is to regularize the architecture parameters during training of the NAS model. The original DARTS method optimizes both the model weights and architecture parameters simultaneously via gradient descent. However, this can lead to instability in the architecture parameters. SDARTS applies an additional regularizer to smooth the architecture parameters during training. Specifically, it adds Gaussian noise to the architecture parameters at each training step. Experiments demonstrate that SDARTS consistently improves upon DARTS. On CIFAR-10, SDARTS reduces test error from 3.00% to 2.67%. It also leads to more robust architecture selections across different search spaces. SDARTS is able to find reasonable architectures on search spaces where DARTS fails dramatically. Overall, the paper shows SDARTS is a simple but effective technique to stabilize and improve differentiable NAS. The noise-based regularization helps guide architecture search and prevent instability. This enables SDARTS to consistently find better architectures than the original DARTS approach.


## Summarize the main method used in the paper in one paragraph.

Here is a one paragraph summary of the main method used in the paper:The paper proposes a perturbation-based architecture selection method for differentiable neural architecture search (NAS). Instead of selecting operations based on the magnitude of architecture parameters α as in previous methods like DARTS, the proposed method evaluates the strength of each operation by measuring how much perturbing or removing that operation impacts the validation accuracy of the supernet. Specifically, after training the supernet, operations are selected edge-by-edge by masking out each operation on a given edge and re-evaluating the supernet, with the operation that results in the largest drop in accuracy when removed being selected for that edge. The supernet is fine-tuned after discretizing each edge to recover lost accuracy. This perturbation-based selection method allows selecting architectures that consistently improve upon those chosen based solely on the magnitude of α in methods like DARTS. The proposed selection method can be combined with different base NAS algorithms like DARTS, SDARTS and SGAS to extract better architectures from the same supernets.


## What problem or question is the paper addressing?

Based on my reading of the paper, the main problem it is addressing is:How to better select architectures in differentiable neural architecture search (NAS) methods. The paper focuses specifically on DARTS and related differentiable NAS approaches. The key aspects are:- The current architecture selection method in DARTS and related works relies on selecting operations based on the magnitude of the architecture parameters alpha. The paper questions whether alpha magnitude actually reflects operation strength/importance.- Through empirical analysis and a theoretical case study of skip connections, the paper argues that alpha magnitude does not necessarily indicate operation strength.- To address this issue, the paper proposes a perturbation-based architecture selection method that directly measures each operation's influence on the supernet's accuracy, rather than relying on alpha. - Experiments show the proposed selection method consistently finds better architectures from the same supernets compared to magnitude-based selection used in DARTS.So in summary, the key problem is the architecture selection in differentiable NAS being based on an unreliable signal (alpha magnitude). The paper aims to develop an improved selection method based on directly measuring operation importance.
