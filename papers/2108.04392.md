# [Rethinking Architecture Selection in Differentiable NAS](https://arxiv.org/abs/2108.04392)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading, the key research question this paper addresses is: 

Does the magnitude of architecture parameters (alpha) in differentiable neural architecture search (DNAS) methods accurately represent the strength/importance of operations in the searched architecture?

The authors argue that the common practice of selecting operations based on the magnitude of alpha may be problematic, and propose an alternative perturbation-based method to better evaluate operation strength when extracting the final architecture.

In particular, the central hypothesis seems to be:

The magnitude of alpha does not necessarily reflect the true contribution/importance of an operation to the overall performance of the searched architecture. Therefore, relying solely on alpha magnitudes to select operations can lead to suboptimal architecture choices.

The authors provide both empirical and theoretical evidence to support this claim, and show that their proposed perturbation-based selection method can consistently find improved architectures compared to selecting based just on alpha values. Evaluating architectures on the DARTS search space and NAS Bench 201 benchmark further verifies their hypothesis.

In summary, the key question is whether alpha indicates operation strength for architecture selection in DNAS, and the central hypothesis is that magnitude of alpha alone is insufficient/misleading for robust architecture selection. The authors propose an improved perturbation-based alternative.


## What is the main contribution of this paper?

 Based on my reading of the paper, the main contribution is proposing an alternative perturbation-based architecture selection method for neural architecture search instead of relying on the magnitude of architecture parameters α to select operations.

The key points are:

- They analyze and show empirically that the magnitude of α does not necessarily represent the importance or strength of an operation in the supernet.

- They propose a perturbation-based method that evaluates each operation by measuring how much removing it perturbs the supernet's accuracy. The operation that results in the largest drop in accuracy when removed is considered the most important.

- They show this method is able to consistently find better architectures than magnitude-based selection across different search spaces and base NAS methods like DARTS, SDARTS, and SGAS.

- The method brings more flexibility as it does not depend on α for selection. They show comparable results can be achieved even with a uniform α.

- It is able to extract meaningful architectures in cases where DARTS fails dramatically, indicating issues with DARTS may be more due to the selection method rather than just optimization.

In summary, the key contribution is analyzing issues with relying on α magnitude for selection in differentiable NAS and proposing an improved perturbation-based method that directly measures operation importance. This consistently finds better architectures across spaces and base methods.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the paper:

The paper proposes an alternative perturbation-based architecture selection method for differentiable neural architecture search that directly measures each operation's influence on the supernet performance, instead of relying on the magnitude of architecture parameters which does not necessarily indicate operation strength.


## How does this paper compare to other research in the same field?

 Here are a few thoughts on how this paper compares to other research in differentiable neural architecture search:

- This paper focuses specifically on rethinking the architecture selection process in differentiable NAS methods like DARTS. Much prior work has focused on improving the robustness and optimization of the supernet training process. So this provides a complementary perspective by analyzing the selection step.

- The key idea of evaluating operation importance based on the perturbation to supernet accuracy is novel. Previous differentiable NAS methods like DARTS, SDARTS, etc all select operations solely based on the magnitude of the architecture parameters α. 

- The proposed perturbation-based selection method consistently improves performance over magnitude-based selection across DARTS, SDARTS and SGAS on CIFAR and NAS Bench 201. This supports the claim that selection plays a crucial role, and provides a simple but effective alternative.

- Analyzing the issues with relying on α values for selection, both empirically and theoretically for the skip connection case, provides useful insights. The skip connection analysis connects concepts from ResNet feature learning.

- Overall, this paper makes both empirical and theoretical contributions analyzing the architecture selection phase in differentiable NAS. The proposed perturbation-based method provides a consistent improvement. The analysis and ablation studies shed light on the importance of selection.

To summarize, this paper focuses on an under-explored component of differentiable NAS research - the architecture selection - and demonstrates its importance through empirical evaluation of the proposed perturbation-based method as well as detailed analysis. The findings suggest room for improvement in the selection phase across differentiable NAS techniques.


## What future research directions do the authors suggest?

 Based on my reading of the paper, some of the main future research directions suggested by the authors include:

- Developing more advanced and efficient search algorithms for neural architecture search (NAS). The authors note that there is still room for improvement in terms of search efficiency and effectiveness compared to manual architecture design. They suggest exploring more advanced search algorithms like evolutionary methods or reinforcement learning.

- Improving weight sharing methods in differentiable NAS. The authors point out limitations of current weight sharing approaches and propose directions like more specialized weight initialization schemes for child models or progressively growing the search space.

- Studying what constitutes a good search space. The authors suggest analyzing what types of search spaces lead to better architectures and how to design good search spaces automatically.

- Scaling up NAS to larger datasets and models. Most NAS research has focused on smaller datasets like CIFAR, but the authors suggest expanding to large-scale datasets like ImageNet as an important research direction.

- Understanding and improving the transferability of NAS architectures. The authors note that architectures found on smaller search spaces often do not transfer well, and suggest analyzing and improving transferability as an open problem.

- Developing more holistic NAS systems beyond just architecture search. The authors propose that future NAS systems could integrate architecture search with autoML components like hyperparameter tuning, neural optimizer search, etc.

In summary, the main suggestions are around developing more advanced NAS algorithms, improving weight sharing schemes, designing better search spaces, scaling up to larger problems, enhancing transferability, and building more holistic NAS systems. Advancing research in these directions could lead to more automated, efficient, and effective neural architecture design.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the paper:

The paper proposes a perturbation-based architecture selection method for differentiable neural architecture search (NAS). Differentiable NAS methods like DARTS jointly optimize the model weights and architecture parameters in a weight-sharing supernet. The final architecture is selected based on the magnitude of the architecture parameters, with the assumption that larger values indicate stronger operations. However, the authors show empirically and theoretically that the magnitude of architecture parameters does not necessarily reflect operation importance. As an example, they mathematically show that skip connections tending to dominate in DARTS is a reasonable outcome during optimization but problematic for architecture selection. To address this issue, they propose an alternative method that selects operations based on how much they perturb supernet accuracy when removed - operations that result in a larger accuracy drop when removed are considered more important. They show this perturbation-based selection is able to consistently extract better architectures from DARTS and its variants across datasets and search spaces. The results indicate issues with DARTS may stem more from the architecture selection method rather than just optimization of the supernet weights.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the paper:

The paper introduces SmoothDARTS (SDARTS), a method for improving the robustness and performance of differentiable neural architecture search (NAS). The key idea is to regularize the architecture parameters during training of the NAS model. The original DARTS method optimizes both the model weights and architecture parameters simultaneously via gradient descent. However, this can lead to instability in the architecture parameters. SDARTS applies an additional regularizer to smooth the architecture parameters during training. Specifically, it adds Gaussian noise to the architecture parameters at each training step. 

Experiments demonstrate that SDARTS consistently improves upon DARTS. On CIFAR-10, SDARTS reduces test error from 3.00% to 2.67%. It also leads to more robust architecture selections across different search spaces. SDARTS is able to find reasonable architectures on search spaces where DARTS fails dramatically. Overall, the paper shows SDARTS is a simple but effective technique to stabilize and improve differentiable NAS. The noise-based regularization helps guide architecture search and prevent instability. This enables SDARTS to consistently find better architectures than the original DARTS approach.


## Summarize the main method used in the paper in one paragraph.

 Here is a one paragraph summary of the main method used in the paper:

The paper proposes a perturbation-based architecture selection method for differentiable neural architecture search (NAS). Instead of selecting operations based on the magnitude of architecture parameters α as in previous methods like DARTS, the proposed method evaluates the strength of each operation by measuring how much perturbing or removing that operation impacts the validation accuracy of the supernet. Specifically, after training the supernet, operations are selected edge-by-edge by masking out each operation on a given edge and re-evaluating the supernet, with the operation that results in the largest drop in accuracy when removed being selected for that edge. The supernet is fine-tuned after discretizing each edge to recover lost accuracy. This perturbation-based selection method allows selecting architectures that consistently improve upon those chosen based solely on the magnitude of α in methods like DARTS. The proposed selection method can be combined with different base NAS algorithms like DARTS, SDARTS and SGAS to extract better architectures from the same supernets.


## What problem or question is the paper addressing?

 Based on my reading of the paper, the main problem it is addressing is:

How to better select architectures in differentiable neural architecture search (NAS) methods. 

The paper focuses specifically on DARTS and related differentiable NAS approaches. The key aspects are:

- The current architecture selection method in DARTS and related works relies on selecting operations based on the magnitude of the architecture parameters alpha. The paper questions whether alpha magnitude actually reflects operation strength/importance.

- Through empirical analysis and a theoretical case study of skip connections, the paper argues that alpha magnitude does not necessarily indicate operation strength.

- To address this issue, the paper proposes a perturbation-based architecture selection method that directly measures each operation's influence on the supernet's accuracy, rather than relying on alpha. 

- Experiments show the proposed selection method consistently finds better architectures from the same supernets compared to magnitude-based selection used in DARTS.

So in summary, the key problem is the architecture selection in differentiable NAS being based on an unreliable signal (alpha magnitude). The paper aims to develop an improved selection method based on directly measuring operation importance.


## What are the keywords or key terms associated with this paper?

 Based on reviewing the paper, some key terms and concepts are:

- Neural Architecture Search (NAS) - The paper focuses on methods for automating neural network architecture design, which is referred to as neural architecture search. This is a major research area in deep learning.

- Differentiable NAS - The paper specifically examines differentiable NAS methods like DARTS, which enable architecture search via gradient-based optimization of architecture parameters.

- Weight sharing - Differentiable NAS methods like DARTS use a weight sharing strategy during the search process, where candidate architectures share weights. This improves search efficiency.

- Architecture selection - A core focus of the paper is selecting the final architecture from the learned architecture parameters in differentiable NAS methods like DARTS. The common magnitude-based selection method is examined. 

- Perturbation-based selection - The paper proposes an improved perturbation-based method for architecture selection in differentiable NAS, which better measures architecture performance.

- Architecture robustness - Analyzing the robustness and validity of architectures found by differentiable NAS methods is a key focus. The proposed selection method is shown to improve robustness.

- Supernet training - The paper analyzes supernet training in differentiable NAS and the effect of architecture parameters like DARTS' alpha on the supernet optimization process.

- Operation strength - The paper examines how to properly measure the strength or importance of operations during architecture selection in differentiable NAS.


## Based on the paper, what questions should we ask to create a comprehensive summary of it? Please list at least 10 questions.

 Here are 10 potential questions to help summarize the key points of the paper:

1. What is the main purpose or objective of the paper? What problem is it trying to solve?

2. What methods or techniques does the paper propose? How do they work? 

3. What are the key assumptions or components underlying the proposed methods?

4. What datasets were used to evaluate the methods? What metrics were used?

5. What were the main experimental results? How did the proposed methods compare to baselines or prior work?

6. What are the limitations of the proposed methods? Are there any failure cases or scenarios they do not handle well?

7. Did the paper include any theoretical analysis or proofs? If so, what were the key insights?

8. What conclusions did the authors draw? What are the main takeaways?

9. How does this work fit into the broader landscape of research in this field? What are the implications for future work?

10. Did the authors identify any interesting directions for future research based on this work? What open problems remain?

Asking questions like these should help dig into the key details and contributions of the paper across motivation, methods, experiments, analysis, limitations, conclusions and impact. The answers can then be synthesized into a comprehensive summary. Let me know if you need any clarification or have additional questions!


## Design 10 in-depth questions about method proposed in this paper and avoid simple question.

 Here are 10 in-depth questions I formulated about the method proposed in the paper:

1. The perturbation-based architecture selection method directly measures the influence of each operation on the supernet's performance. How does this differ fundamentally from the implicit assumption in previous methods that the magnitude of architecture parameters reflects operation strength? What are the key advantages of the proposed approach?

2. The paper argues that the architecture parameter values (α) in DARTS do not necessarily represent operation importance. What evidence do the authors provide to support this claim both empirically and theoretically? How does this motivate the need for an alternative architecture selection approach?

3. Explain how the phenomenon of skip connections dominating in DARTS is analyzed by the authors through the lens of unrolled estimation used in ResNet. Why does this lead to issues when combined with magnitude-based architecture selection? 

4. Walk through the details of how the perturbation-based architecture selection method evaluates and selects the best operations on each edge. What are the computational advantages compared to directly discretizing each operation?

5. The method performs progressive fine-tuning of the supernet between operation selections. Analyze the importance of this component and discuss any ablation studies in the paper that demonstrate its effects.

6. Summarize the experimental results on CIFAR-10 and NAS-Bench 201. How does the proposed method compare to magnitude-based selection across different search spaces and base methods? What does this imply about its general applicability?

7. The paper shows the perturbation-based method is able to find meaningful architectures in cases where DARTS fails dramatically (e.g. Spaces S2 and S4). Analyze these results and their significance in understanding DARTS' robustness issues. 

8. Discuss the ablation study that fixes the architecture parameters (α) to be uniform during supernet training. Why is this finding surprising and how does it bring more freedom to supernet training and NAS methods?

9. Critically analyze the limitations of the perturbation-based architecture selection approach. Are there any potential downsides compared to magnitude-based selection? Can you think of ways to improve or extend the method?

10. The paper focuses on rethinking architecture selection in differentiable NAS. In your view, what are the broader impacts and implications of this work for the NAS community? Does it change your perspective on key assumptions made in existing methods?


## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a high-quality paragraph summarizing the key points of the paper:

The paper proposes a new perturbation-based architecture selection method for differentiable neural architecture search (DNAS). It provides both empirical and theoretical evidence showing that the magnitude of architecture parameters (α) in DNAS methods like DARTS does not necessarily indicate the operation strength. As an example, it mathematically shows that the domination of skip connections in DARTS is reasonable from the supernet optimization perspective, but problematic when used for architecture selection. To address this issue, the authors propose an alternative operation selection method that directly measures the influence of each operation on the supernet's performance. Specifically, they evaluate how much removing one operation perturbs the supernet's accuracy. The operation that results in the largest accuracy drop when removed is selected as the best for that edge. This perturbation-based selection is shown to consistently extract better architectures from the same supernets compared to magnitude-based selection across different search spaces and base DNAS methods. It also greatly alleviates the failure modes of DARTS like poor generalization. Overall, the work provides useful insight into the architecture selection process in DNAS and proposes a simple yet effective selection method to improve it.


## Summarize the paper in one sentence.

 The paper proposes a perturbation-based architecture selection method to improve neural architecture search in differentiable NAS frameworks.


## Summarize the paper in one paragraphs.

 Here is a one paragraph summary of the paper:

The paper proposes a perturbation-based architecture selection method for differentiable neural architecture search (NAS). It argues that the commonly used magnitude-based selection, where operations with the largest architecture parameters are selected, does not necessarily indicate operation importance. It provides empirical evidence showing the selected operation does not always lead to the best accuracy after discretization. It also mathematically justifies why skip connections tend to dominate in DARTS, but become problematic under magnitude-based selection. To address this issue, the authors propose an alternative selection method that evaluates operations by the drop in supernet accuracy when the operation is removed. This perturbation-based selection consistently finds better architectures across different search spaces and base NAS algorithms. It also alleviates DARTS' failure modes, suggesting the poor generalization of DARTS is partly due to the architecture selection. The proposed method brings a new perspective of rethinking architecture selection in differentiable NAS.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 potential in-depth questions about the method proposed in this paper:

1. The paper proposes a perturbation-based architecture selection method to replace the original magnitude-based selection in DARTS. What is the intuition behind using the perturbation to each operation's contribution to evaluate its importance? What are the advantages of this approach compared to simply looking at the magnitude of architecture parameters?

2. The paper shows both empirically and theoretically that the magnitude of architecture parameters does not necessarily represent the strength of the operations. Can you explain the theoretical analysis in Section 3.2 that derives the optimal architecture parameters? Why does it suggest the skip connection will dominate even in an optimized DARTS supernet? 

3. The complete architecture selection algorithm involves discretizing one operation per edge in a greedy fashion. Why is it important to fine-tune the supernet for a few epochs between operation selections? What would be the potential issues without this progressive tuning step?

4. Experiments show the proposed selection method consistently finds better architectures across multiple search spaces and DARTS variants. Does this indicate architecture selection is more important than optimizing the supernet training in differentiable NAS? What are other potential reasons behind the consistent improvements?

5. The paper shows the perturbation-based selection can alleviate DARTS' failure modes such as skip connection domination. Does this mean the issues of DARTS are largely due to the architecture selection rather than supernet optimization? What could be done to further validate this hypothesis?

6. An ablation study shows the perturbation-based criterion alone leads to better operations than magnitude-based selection with progressive tuning. Why does this highlight the importance of the selection criteria independent of other components?

7. Experiments show fixing the architecture parameters as uniform still works well with the proposed selection method. What does this imply about the role of learning the architecture parameters in differentiable NAS? How can this observation be utilized in future work?

8. How does the proposed method compare to prior work on stabilizing DARTS optimization like PC-DARTS and R-DARTS? Would those methods also benefit from replacing magnitude-based selection with the perturbation-based approach?

9. The paper focuses on CNN search spaces but could the ideas be extended to other domains like NLP or graph neural networks? What are the challenges in evaluating operation importance in black-box search spaces without clear topological structure?

10. The results are demonstrated on relatively small datasets like CIFAR and NAS-Bench 201. How would the conclusions change when scaling up the search and evaluation to much larger datasets like ImageNet? What are the potential challenges of applying the proposed ideas on more complex search spaces and tasks?
