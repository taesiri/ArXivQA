# [Factorized Explainer for Graph Neural Networks](https://arxiv.org/abs/2312.05596)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Graph neural networks (GNNs) are gaining popularity for learning from graph data, but they lack interpretability due to their "black box" nature. 
- Existing methods use the Graph Information Bottleneck (GIB) principle to find "minimal and sufficient" subgraphs that explain GNN predictions. 
- However, the paper shows analytically that for many tasks, GIB admits trivial solutions where the explanation subgraph signals the predicted label but is independent of the actual input graph. So GIB does not fully capture the notion of explainability.

Proposed Solution:
- The paper proposes a modified GIB principle to avoid the trivial signaling solutions. This uses cross-entropy between the predicted and true labels based on the explanation subgraph.
- It also proposes a novel K-FactExplainer method based on factorizing the explanation function to address issues of locality and lossy aggregation in GNNs:
  - Locality issue: Existing parametric explainers are local, struggling on graphs with multiple motifs influencing the label. 
  - Lossy aggregation: GNN aggregation loses information, affecting explanation accuracy.
- The K-FactExplainer combines multiple local "weak" explainers into a stronger global explainer using an additional MLP.

Main Contributions:
- First analytical demonstration of issues with using GIB principle for GNN explainability.
- A modified GIB objective that better captures explainability.
- Identification of locality and lossy aggregation issues limiting existing parametric explainers.
- A new factorized explainer method that mitigates these issues and improves explanation accuracy.
- Extensive experiments validating improved performance over state-of-the-art on both synthetic and real-world graphs.

In summary, the paper provides important analytical insights into limitations of prior work on GNN explainability, and proposes a novel factorized explanation approach that consistently improves explanation accuracy across tasks.
