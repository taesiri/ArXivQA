# Towards Large-Scale Interpretable Knowledge Graph Reasoning for Dialogue   Systems

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central research question addressed in this paper is: How can we incorporate large-scale knowledge graph reasoning capabilities into dialogue systems in an efficient, scalable, interpretable and flexible manner?Specifically, the authors propose a novel method called DiffKG that allows a single transformer model to directly walk on a large-scale knowledge graph to generate responses in an end-to-end fashion. The key goals and contributions seem to be:- Allowing flexible entity representations in the knowledge graph, so it can handle novel entities.- Generating interpretable reasoning paths over the knowledge graph for transparency. - Scaling to large knowledge graphs without heavy preprocessing or subgraph sampling.- Applying the method to both task-oriented dialogues and open-domain chit-chat dialogues.- Demonstrating the efficiency of the approach with relatively low extra time and memory requirements compared to not using any knowledge graph.So in summary, the central hypothesis is that the proposed DiffKG method can effectively incorporate large-scale knowledge graph reasoning into dialogue systems in a scalable, efficient, interpretable and flexible way. The experiments aim to validate whether DiffKG achieves those goals.


## What is the main contribution of this paper?

The main contribution of this paper is proposing a novel method called DiffKG that incorporates knowledge graph reasoning into dialogue systems. Specifically:- DiffKG allows a single transformer model to directly walk on a large-scale knowledge graph to generate responses in an end-to-end manner. - It is the first work to have transformer models generate responses by reasoning over differentiable knowledge graphs.- DiffKG performs multi-hop reasoning on a knowledge graph by generating a sequence of relations. This makes the reasoning process fully interpretable.- It is a model-agnostic method that can be applied to different model architectures like GPT-2 and T5.- Experiments show that DiffKG can effectively incorporate reasoning on large knowledge graphs into dialogue systems with relatively low extra time and memory costs compared to transformers without knowledge graph reasoning.- The paper demonstrates DiffKG's reasoning abilities on both task-oriented dialogues (using SMD dataset) and chit-chat dialogues (using OpenDialKG dataset).In summary, the main contribution is proposing an interpretable and scalable method to incorporate knowledge graph reasoning directly into transformer-based dialogue systems in an end-to-end manner.
