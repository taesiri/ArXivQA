# [NextLevelBERT: Investigating Masked Language Modeling with Higher-Level   Representations for Long Documents](https://arxiv.org/abs/2402.17682)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Traditional language models like BERT struggle to process long sequences like books due to the quadratic scaling of the attention mechanism. This limits their ability to capture long-range dependencies.

Proposed Solution: 
- The paper proposes NextLevelBERT, a masked language model that operates on vector representations of text chunks rather than individual tokens. 
- Text chunks (e.g. sentences) are encoded independently into vectors using a pretrained encoder model. Some chunk vectors are then masked and NextLevelBERT is trained to predict them based on the surrounding context.  
- This allows modeling language at a higher, more abstract level compared to individual tokens. NextLevelBERT can capture dependencies across much longer contexts.

Key Details:
- Text chunks are encoded with a frozen pretrained encoder like MiniLM and packed together into sequences. 
- 15% of chunk vectors are masked, using BERT's masking strategy. An MLP predicts the original vectors.  
- NextLevelBERT is pretrained on a large dataset of books to reconstruct masked chunk vectors.

Main Contributions:
- Proposes a novel method for masked language modeling at the level of text chunk representations rather than tokens
- Develops NextLevelBERT architecture and pretraining procedure  
- Shows strong performance on long document tasks like semantic similarity, classification and QA
- Demonstrates that next level MLM leads to useful document vectors and long-range modeling
- Provides an efficient way to handle long sequences compared to token-level models

The summary covers the key details on the problem being addressed, the proposed NextLevelBERT solution, its architecture and pretraining, the main results showing its effectiveness on various long document tasks, and the main contributions of modeling language on vector chunks.
