# [Unsupervised Multilingual Dense Retrieval via Generative Pseudo Labeling](https://arxiv.org/abs/2403.03516)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Multilingual information retrieval (mIR) aims to retrieve relevant documents from a collection in multiple languages given a query. 
- Dense retrieval methods that encode queries and documents into dense vectors have shown promise for mIR.
- However, training dense retrievers requires substantial paired data (query-document pairs), which is even more challenging to obtain for multilingual scenarios.

Proposed Solution:
- The paper proposes an unsupervised framework called UMR to train multilingual dense retrievers without needing any paired training data. 
- It has two main stages - unsupervised reranking using a multilingual language model (MLM) to get pseudo-labels, and knowledge distillation to train the dense retriever.
- The MLM leverages its generative capabilities to estimate the relevance between queries and retrieved documents for reranking.
- The reranker acts as the teacher to distill knowledge into the student retriever model through KL divergence loss.  
- There is also an iterative training process to continuously improve the retriever.

Main Contributions:
- First work to propose training multilingual dense retrievers in a completely unsupervised manner without needing any paired training data.
- Achieves comparable or better performance than supervised baselines on XOR-Retrieve and XOR-Full benchmarks.
- Detailed analysis provided to demonstrate the impact of different components of the framework.
- Showcases the potential of unsupervised learning to alleviate the annotation requirements for multilingual retrieval.

In summary, the paper introduces an unsupervised two-stage approach using MLMs and knowledge distillation to effectively train multilingual dense retrievers without paired training data. The experiments and analysis demonstrate its capabilities.
