# [Towards an empirical understanding of MoE design choices](https://arxiv.org/abs/2402.13089)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem Statement:
- Mixture of experts (MoE) models have shown exceptional performance, but the impact of different architectural choices is not well understood. Specifically, how do choices like routing unit (token vs sequence), routing strategy (layer-wise vs global), number of experts, and number of active experts impact validation performance and expert specialization?

- Prior works on expert specialization mostly use token-level routing and have shown evidence of syntactic specialization. This paper investigates if sequence-level routing can induce more context-dependent, topic-level specialization.  

Methodology:
- Perform ablation studies on GPT2 small models trained from scratch on Wikipedia (multilingual) and OpenWebText (English only) datasets.
- Vary routing unit (token vs sequence), routing strategy (layer-wise vs global), number of experts and number of active experts (K).
- Evaluate validation perplexity and analyze expert activation patterns on unseen domains.

Key Findings:  
- Token routing benefits more from increased number of experts, while sequence routing benefits more from increased number of active experts.  
- Layer-wise learned/frozen routing performs the best. Language-based global routing underperforms.
- Load balancing loss to prevent expert collapse is not needed. 
- Sequence routing shows signs of weak, topic-level expert specialization, unlike token routing.

Main Contributions:
- First systematic ablation study quantifying the impact of different MoE designs like routing unit, routing strategy etc. 
- Demonstrate different preferences for token vs sequence routing architectures.
- Provide evidence for feasibility of weak, topic-level expert specialization with sequence-level routing.

The paper offers useful insights into optimal choices when designing MoE architectures, especially highlighting differences between token and sequence routing.


## Summarize the paper in one sentence.

 This paper empirically investigates different Mixture of Experts (MoE) design choices such as routing unit, routing strategy, number of experts, and number of active experts on model performance and expert specialization using small-scale GPT2 models.


## What is the main contribution of this paper?

 Based on my reading of the paper, the main contributions include:

1) Ablating common MoE design choices (such as routing unit, routing strategy, number of experts, number of activated experts) to quantify their impact on validation performance. 

2) Demonstrating that design choice preferences differ for token-level versus sequence-level routing. For example, sequence-level routing benefits more from increasing the number of activated experts, while token-level routing benefits more from having more experts to choose from.

3) Providing evidence for the existence of "weak expert specialization" in topics when routing at the sequence level. This suggests the possibility of inducing some context-dependent specialization through sequence-level routing.

So in summary, the paper systematically investigates how various MoE designs choices affect performance and expert specialization, aiming to provide useful insights and guidance for practitioners working with MoEs.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and concepts are:

- Mixture of Experts (MoE)
- Token-level routing
- Sequence-level routing  
- Expert specialization
- Design choices (e.g. Top-1 vs Top-2 routing, layer-wise vs global routing)
- Validation performance
- Weak expert specialization
- Number of experts (N)
- Number of activated experts (K)

The paper examines different MoE design choices like the routing unit (token vs sequence) and routing strategy (Top-1 vs Top-2, layer-wise vs global), and evaluates their impact on model validation performance and expert specialization. Key findings include that preferred choices differ for token- vs sequence-level routing, and sequence-level routing shows signs of "weak expert specialization" in topics.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 potential in-depth questions about the methods proposed in this paper:

1. The paper argues that sequence-level routing may enable some amount of expert specialization in topics. However, the evidence provided seems weak. Could you describe additional analyses or experiments that could more definitively determine if topic-level specialization emerges with sequence routing?

2. For the frozen/random routing experiments, it's unclear if the embeddings were also frozen or randomized. Could you clarify the setup? Freezing/randomizing embeddings as well would provide a stronger test of whether the learned routers contribute anything meaningful. 

3. You find that increasing the number of activated experts K helps more for sequence routing than token routing. Do you have any hypotheses for why that may be the case? Are there differences in the loss landscape or gradient flow that could explain this?

4. When analyzing variable numbers of experts N and values of K, expert capacity was not adjusted. Does controlling for expert capacity change any of the conclusions around N, K, and routing level choices? 

5. The analysis shows expert collapse occurs but argues this does not hurt validation perplexity. However, were downstream task performance metrics evaluated? Expert collapse could limit transfer learning.

6. For the language routing experiments, what share of tokens were routed incorrectly by the learned router relative to the ground truth language? Were any patterns noticed in the errors?

7. Was experimenting with different router architectures (e.g. number of layers, hidden dimensions) explored? If so, did it impact results meaningfully? If not, would be interesting to ablate.

8. How sensitive were the main conclusions to hyperparameter choices - e.g. dropout rates, learning rate schedules, gradient accumulation steps? Were multiple runs conducted to test robustness?

9. For future work, what other analyses could shed more light on the source of the performance differences observed across conditions? E.g. optimization metrics, gradient norms, loss decomposition.

10. Do you think findings generalize to much larger model scales? Why or why not? What differences would you expect to see with billions of parameters?
