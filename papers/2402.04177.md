# [Scaling Laws for Downstream Task Performance of Large Language Models](https://arxiv.org/abs/2402.04177)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Scaling laws have been studied to understand how upstream pretraining loss behaves as pretraining data grows, but not much is known about how downstream task performance scales. This is an important question in transfer learning settings where models are first pretrained on unsupervised data and then finetuned on a target task.

- Specifically, the paper investigates how the choice of pretraining data and its size affects downstream performance metrics like BLEU score and downstream cross-entropy loss for machine translation tasks. 

Methodology:
- The authors pretrain T5 models of various sizes (770M params and 3B params) on different subsets of the Multilingual C4 corpus, including English (en), German (de), French (fr) and Romanian (ro).

- They finetune checkpoints from pretraining on WMT machine translation datasets: WMT17 en-de, WMT15 en-fr, and WMT16 en-ro. They vary finetuning data size as well.

- They propose a log scaling law for BLEU score and a power law for downstream cross-entropy as functions of pretraining data size.

Key Findings:

- With aligned pretraining and downstream distributions and large enough finetuning data, both BLEU and downstream cross-entropy improve monotonically, and the laws fit well. No value from pretraining if finetuning data is already large.

- With misalignment between pretraining and downstream data, BLEU fluctuates unpredictably, even getting worse with more pretraining. But downstream cross-entropy still improves monotonically per the scaling law. 

- Downstream cross-entropy is not always a good proxy for BLEU. Their relation is not consistent.

- Whether the BLEU scaling law fits well indicates the degree of alignment of pretraining data with the downstream task.

Contributions:

- Propose scaling laws for downstream BLEU and cross-entropy as pretraining data grows

- Show downstream cross-entropy can be misleading if used as a proxy for BLEU score

- Provide insights into choosing appropriate pretraining data and determining its value for a downstream task

In summary, the key insight is that the effect of pretraining data on downstream performance depends heavily on the alignment between pretraining and downstream distributions. The proposed scaling laws and findings provide useful guidelines for model development in transfer learning settings.
