# [RealmDreamer: Text-Driven 3D Scene Generation with Inpainting and Depth   Diffusion](https://arxiv.org/abs/2404.07199)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
Generating high-quality 3D scenes from text prompts is challenging due to the complexity of scenes and lack of paired text-3D data. Existing text-to-3D methods have been limited to simple objects or panoramas and fail to produce convincing 3D scenes with correct geometry. 

Proposed Solution:
The paper proposes RealmDreamer, a novel technique to generate realistic forward-facing 3D scenes from text using pretrained 2D inpainting and depth diffusion models. The key ideas are:

(1) Robustly initialize a 3D Gaussian Splatting (3DGS) representation by creating a point cloud from a reference image lifted with monocular depth. Expand point cloud by rendering neighboring views.   

(2) Formulate scene generation as inpainting task using 2D diffusion priors. Optimizes 3DGS to match text prompt across views by filling disoccluded regions. Defines losses in both latent and image spaces.

(3) Incorporate image-conditional depth diffusion model to predict depth maps for better geometry. Distill depth knowledge by conditioning depth model on rendered images.

(4) Finetune model with sharpening filter on rendered images for improved coherence.

Main Contributions:

- Scene-level 3DGS initialization procedure using 2D priors
- Framework to learn consistent 3D through occlusion-aware inpainting losses  
- Technique to distill monocular depth knowledge into 3D scenes
- State-of-the-art text-based generation of 3D scenes with realistic appearance and geometry

The method does not require any 3D supervision and can create high-quality 3D scenes from text across different styles. It also extends to single image to 3D task. Both qualitative and quantitative experiments demonstrate significant improvements over prior text-to-3D techniques.
