# [Scaling the Authoring of AutoTutors with Large Language Models](https://arxiv.org/abs/2402.09216)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Intelligent tutoring systems (ITSs) aim to provide personalized tutoring at scale, but are difficult to expand to new topics due to the expert labor needed to incorporate domain knowledge and teaching strategies.  
- Recent large language models (LLMs) have been used directly as tutors, but their black-box and non-modular nature makes them uninterpretable, uncontrollable, and difficult to trust or improve.

Proposed Solution:
- Use LLMs to assist in authoring the state space of an ITS instead of interacting directly with students. This retains interpretability and modularity while leveraging LLM efficiency.
- Decompose problems into smaller solution steps and define a pedagogical strategy space. LLMs fill template prompts to map states to utterances.
- Solution steps and strategy can be inspected, edited if needed, and re-generated after changes. Allows combining human and machine expertise.

Contributions:
- Adapt dialog-based ITS framework to enable LLM authoring
- Design MB, a math word problem tutor using this approach with modular replaceable components 
- Compare MB performance to GPT-4 tutor in AI-AI experiments
- Show MB achieves better success rate and tutoring score than GPT-4
- Demonstrate MB utterance quality via human evaluation
- Highlight remaining weaknesses of both approaches through qualitative assessment

In summary, the paper proposes using LLMs to assist in authoring modular and interpretable ITS components rather than directly as a tutor. This is demonstrated via the MB math tutor which outperforms GPT-4 in experiments while retaining control and trust.
