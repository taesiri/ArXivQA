# [diff History for Long-Context Language Agents](https://arxiv.org/abs/2312.07540)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a high-quality paragraph summarizing the key points of the paper:

This paper proposes a simple yet effective method called "diff History" to enable long-context language agents for complex decision-making tasks. The key idea is to represent the sequential observations not as raw text, but as a residual summary computed using the Unix diff command between consecutive observations. This compression via diff provides multiple benefits: it reduces redundancy between similar observations, increasing the effective context length usable by language models; it also adds an implicit abstraction that improves generalization. The method is evaluated extensively on the challenging game of NetHack, where language agents with diff History significantly outperform prior state-of-the-art vision-language and language models trained on the same amount of game data. Specifically, diff History agents achieve a 7x higher median game score compared to the best vision-language baseline, and a 0.4x higher score compared to a vision-language agent trained on 100x more data. Ablation studies demonstrate the crucial impact of diff residuals versus raw observations for language agent performance. The simplicity and task-agnostic nature of this procedural observation reformulation make it an appealing solution for scaling language agents to complex, long-context decision-making problems.
