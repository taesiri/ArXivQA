# [Improve Cross-Architecture Generalization on Dataset Distillation](https://arxiv.org/abs/2402.13007)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Existing dataset distillation methods are primarily model-based, where the synthesized dataset inherits biases of the model used, limiting generalizability to other models. This causes a drop in performance when evaluating distilled datasets on alternative model architectures.

Proposed Solution: 
- Introduce a "model pool" concept during dataset distillation. This involves selecting models from a diverse pool based on a probability distribution when deriving the distilled dataset.
- Integrate knowledge distillation into the training process of the distilled dataset. Specifically, use a teacher-student framework to align predictions between models.

Methods:
- Model pool contains a "main model" (high probability) and other similar models (low probability). This allows convergence while improving cross-architecture generalization.  
- Knowledge distillation utilizes a ConvNet teacher model. Student models of different architectures are trained on the distilled dataset to mimic teacher predictions.

Main Contributions:
- Propose model pool method to improve dataset distillation generalization across architectures. Easily integratable with other distillation algorithms.
- Show combining model pool and knowledge distillation mitigates performance reduction when evaluating distilled dataset on alternate models.
- Experimentally demonstrate superior cross-architecture performance over baseline gradient-matching distillation.

In summary, the paper introduces two novel techniques - model pool and knowledge distillation - to address the limitation of model-based dataset distillation methods and improve generalization of the derived distilled dataset to new model architectures.
