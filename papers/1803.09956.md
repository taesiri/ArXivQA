# [Learning Synergies between Pushing and Grasping with Self-supervised   Deep Reinforcement Learning](https://arxiv.org/abs/1803.09956)

## What is the central research question or hypothesis that this paper addresses?

The central research question is how to learn synergies between pushing and grasping from scratch through deep reinforcement learning. The key hypothesis is that it is possible to train complementary pushing and grasping policies end-to-end that can benefit from each other through experience/trial-and-error.The paper proposes a reinforcement learning framework where pushing and grasping policies are jointly trained to maximize long-term rewards. The goal is to show that:1) The addition of learned pushing actions can enlarge the set of scenarios where grasping succeeds. 2) Pushing policies can be trained mainly through the expected improvement in future grasping rewards, without needing hand-defined objectives.3) Effective synergistic pushing and grasping policies can be learned on real systems through this framework.Overall, the paper aims to demonstrate a new perspective for bridging non-prehensile and prehensile manipulation through end-to-end deep reinforcement learning. The key hypothesis is that pushing and grasping policies can be jointly trained from scratch to capture complex synergistic behaviors.


## What is the main contribution of this paper?

The main contribution of this paper is developing an end-to-end deep reinforcement learning framework for robots to learn complementary pushing and grasping policies from scratch, in a way that the pushing motions help future grasps while grasping motions provide more opportunities for precise and collision-free pushing. Specifically, the key ideas are:- Formulating the problem as a Markov Decision Process and using Q-learning to simultaneously train a pushing policy and grasping policy represented as Fully Convolutional Networks (FCNs). - The pushing FCN takes RGB-D images as input and outputs Q-values for potential push actions at each pixel location. Similarly, the grasping FCN outputs Q-values for grasp actions. Both are trained jointly using rewards from grasp outcomes.- This enables the robot to learn pushing motions that create space and rearrange clutter to enable future grasps, while also learning grasps that expose opportunities for more precise pushes. The synergy emerges through the shared Q-learning framework.- The method is demonstrated both in simulation and on a real robot, where it shows better picking performance than grasping-only baselines after just a few hours of training. The real robot experiments validate that the approach can work end-to-end from vision to control.In summary, the key contribution is a deep reinforcement learning framework to learn complementary pushing and grasping policies directly from visual inputs in a self-supervised manner, which demonstrates synergies between these actions for efficient picking in clutter. The end-to-end learning from vision to control on a real robot is a notable achievement.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points in the paper:The paper proposes a reinforcement learning framework to jointly train pushing and grasping policies represented as fully convolutional networks, enabling a robot to learn complex synergistic behaviors like pushing objects to make space for grasping.


## How does this paper compare to other research in the same field?

This paper presents a novel approach for learning complementary pushing and grasping policies for robotic manipulation using deep reinforcement learning. Here are some key ways it compares to prior work:- Most prior work has studied pushing and grasping separately. This paper is one of the first to explore learning them jointly in a mutually supportive way. It shows pushing can enlarge the set of scenarios in which grasping succeeds, and vice versa.- Many previous methods for push planning rely on hand-engineered features or dynamics models. This paper presents an end-to-end model-free approach using deep reinforcement learning directly from visual observations. It requires no explicit object perception or dynamics modeling.- Prior methods often define heuristic objectives or rewards for pushing. This paper shows pushing policies can be learned from scratch based mainly on rewards from future grasping success, with no hand-crafted rewards needed.- The use of deep convolutional networks enables training complementary pushing and grasping policies efficiently on a real robot in just a few hours. This is much more efficient than prior deep reinforcement learning methods for manipulation.- Results demonstrate the approach works not just in simulation but also on a real robot, generalizes to novel objects, and produces complex synergistic behaviors like clearing clutter to enable better grasping.In summary, this paper makes both modeling and algorithmic contributions for learning coordinated pushing and grasping. The end-to-end model-free approach driven by self-supervision is a key novelty compared to prior model-based and reward-engineered methods.


## What future research directions do the authors suggest?

The authors suggest a few potential directions for future work:1. Exploring other motion primitives beyond pushing and grasping. The paper focuses on learning synergies between pushing and grasping, but they note that these are just two examples of many possible primitive manipulation actions like rolling, toppling, squeezing, levering, stacking, etc. The authors suggest investigating the limits of their deep reinforcement learning approach when learning policies that combine more than just pushing and grasping.2. Using more expressive motion primitives. The motion primitives in this work are defined on a regular grid which provides efficiency for learning but limits expressiveness. The authors suggest exploring other parameterizations that allow for more dynamic pushes, parallel vs sequential pushing/grasping, and varied contact surfaces. 3. Training on a larger variety of objects. The system is trained only on blocks and tested on some other simple shapes. The authors suggest training on a much wider range of object types and further evaluating the generalization capabilities of the learned policies.4. Deploying the system on robots with more capable hands/grippers. The robot used has a simple parallel jaw gripper, so exploring what additional synergies could be learned with more dexterous grippers is suggested.In summary, the main future directions are exploring the applicability of their deep reinforcement learning approach to learning more varied and complex manipulation skills, testing generalization to more diverse objects, and deploying the method on more advanced robotic platforms. The key idea is pushing the limits of their approach for learning coordinated non-prehensile and prehensile manipulation skills.


## Summarize the paper in one paragraph.

Here is a one paragraph summary of the key points from the paper:The paper presents a method for learning synergistic pushing and grasping policies for robotic manipulation using deep reinforcement learning. The main idea is to train two fully convolutional networks that map from visual observations to dense pixel-wise predictions of expected future reward for potential pushing and grasping actions across the entire workspace. The networks are trained jointly using Q-learning, where pushes are rewarded based on how well they enable future grasps to succeed. Experiments in simulation and on a real robot demonstrate that the approach can efficiently learn complex synergistic behaviors between pushing and grasping after only a few hours of training. The method outperforms grasping-only policies, especially in cluttered scenarios where pushing can help create space and rearrange objects to make grasping easier. Key results show that pushing enlarges the set of scenarios in which grasping succeeds, and that the pushing and grasping policies exhibit non-trivial synergistic interactions beyond what was expected.


## Summarize the paper in two paragraphs.

Here is a two paragraph summary of the key points from the paper:This paper presents a framework for learning complementary pushing and grasping policies to enable sequential robotic manipulation. The key aspects are training two fully convolutional networks (FCNs) that map from visual observations to Q values for potential actions. One FCN outputs Q values for pushing actions for each pixel, representing where to push, while the other outputs Q values for grasping actions at each pixel location. Both networks are trained jointly using deep reinforcement learning based on Q-learning. Rewards are provided only for successful grasps, so the pushing policy learns motions that enable future grasping success.  The method is shown in simulation and real-world experiments to learn effective synergistic behaviors between pushing and grasping from scratch. The pushing motions help rearrange cluttered objects to create space and enable grasps. The grasping policy also displaces objects in ways that allow the pushing policy to better manipulate them. The combined policies achieve higher grasp success rates and efficiency in cluttered scenarios compared to reactive grasping-only methods after just a few hours of training. Qualitative results demonstrate complex pushing and grasping sequences emerging. The model-free formulation allows generalization to novel objects.
