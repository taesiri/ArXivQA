# [Learning Synergies between Pushing and Grasping with Self-supervised   Deep Reinforcement Learning](https://arxiv.org/abs/1803.09956)

## What is the central research question or hypothesis that this paper addresses?

The central research question is how to learn synergies between pushing and grasping from scratch through deep reinforcement learning. The key hypothesis is that it is possible to train complementary pushing and grasping policies end-to-end that can benefit from each other through experience/trial-and-error.The paper proposes a reinforcement learning framework where pushing and grasping policies are jointly trained to maximize long-term rewards. The goal is to show that:1) The addition of learned pushing actions can enlarge the set of scenarios where grasping succeeds. 2) Pushing policies can be trained mainly through the expected improvement in future grasping rewards, without needing hand-defined objectives.3) Effective synergistic pushing and grasping policies can be learned on real systems through this framework.Overall, the paper aims to demonstrate a new perspective for bridging non-prehensile and prehensile manipulation through end-to-end deep reinforcement learning. The key hypothesis is that pushing and grasping policies can be jointly trained from scratch to capture complex synergistic behaviors.


## What is the main contribution of this paper?

The main contribution of this paper is developing an end-to-end deep reinforcement learning framework for robots to learn complementary pushing and grasping policies from scratch, in a way that the pushing motions help future grasps while grasping motions provide more opportunities for precise and collision-free pushing. Specifically, the key ideas are:- Formulating the problem as a Markov Decision Process and using Q-learning to simultaneously train a pushing policy and grasping policy represented as Fully Convolutional Networks (FCNs). - The pushing FCN takes RGB-D images as input and outputs Q-values for potential push actions at each pixel location. Similarly, the grasping FCN outputs Q-values for grasp actions. Both are trained jointly using rewards from grasp outcomes.- This enables the robot to learn pushing motions that create space and rearrange clutter to enable future grasps, while also learning grasps that expose opportunities for more precise pushes. The synergy emerges through the shared Q-learning framework.- The method is demonstrated both in simulation and on a real robot, where it shows better picking performance than grasping-only baselines after just a few hours of training. The real robot experiments validate that the approach can work end-to-end from vision to control.In summary, the key contribution is a deep reinforcement learning framework to learn complementary pushing and grasping policies directly from visual inputs in a self-supervised manner, which demonstrates synergies between these actions for efficient picking in clutter. The end-to-end learning from vision to control on a real robot is a notable achievement.
