# [PROP: Pre-training with Representative Words Prediction for Ad-hoc   Retrieval](https://arxiv.org/abs/2010.10137)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is:How can we design an effective pre-training objective tailored for ad-hoc retrieval that resembles the relevance relationship between query and document?The key hypothesis is that using a pre-training objective that more closely resembles the downstream retrieval task will lead to better fine-tuning performance on those tasks. Specifically, the authors propose a new pre-training method called PROP (Pre-training with Representative Words Prediction) that is inspired by the statistical query likelihood model for IR. The query likelihood model assumes the query is generated as text representative of the ideal document. Based on this, PROP pre-trains a Transformer model to predict the pairwise preference between sets of words sampled from the document language model. This task resembles modeling query-document relevance for retrieval.The authors hypothesize and empirically verify that:- Pre-training the Transformer model with the proposed PROP method results in significant improvements over baselines without pre-training or with pre-training objectives from NLP when fine-tuned on downstream ad-hoc retrieval tasks.- PROP can achieve strong performance even with very limited labeled data, demonstrating effectiveness in low-resource retrieval settings.In summary, the central hypothesis is that designing a pre-training objective tailored for modeling query-document relevance is beneficial for fine-tuning on downstream ad-hoc retrieval tasks. The PROP method is proposed to test this hypothesis.


## What is the main contribution of this paper?

The main contribution of this paper is proposing PROP, a pre-training method tailored for ad-hoc retrieval. Specifically, the key contributions are:- Proposing a new pre-training objective called Representative Words Prediction (ROP) inspired by the classical query likelihood model in IR. Given a document, ROP samples word sets as pseudo queries based on the document language model, and learns to predict which set is more representative of the document. - Pre-training a Transformer model with the proposed ROP objective jointly with Masked LM on large text corpora. The resulting model PROP incorporates strengths from both objectives and can be fine-tuned on downstream ad-hoc retrieval tasks.- Evaluating PROP on 5 benchmark ad-hoc retrieval datasets including Robust04, ClueWeb09-B, Gov2, MQ2007 and MQ2008. Results show PROP significantly outperforms baselines without pre-training or with pre-training objectives from NLP.- Demonstrating PROP can achieve strong performance under both zero-resource and low-resource settings by fine-tuning with very limited labeled data.In summary, the main contribution is proposing a novel pre-training method tailored for ad-hoc retrieval by designing a new pre-training objective inspired by classical IR theory. Both pre-training and fine-tuning results validate the effectiveness of PROP.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Based on my reading of the paper, here is a one sentence summary:The paper proposes a new pre-training method called PROP for ad-hoc retrieval that is inspired by the classical statistical language model, specifically the query likelihood model, and shows it achieves significant improvements over baselines without pre-training or with other pre-training methods across a variety of retrieval tasks.


## How does this paper compare to other research in the same field?

Based on my reading of the paper, here is a brief comparison to other related research:- This paper focuses on pre-training objectives for ad-hoc retrieval, an area that has not been widely explored compared to pre-training methods for natural language processing (NLP) tasks. Most prior work on pre-training for information retrieval (IR) has focused on passage retrieval for question answering. This paper proposes a novel pre-training task more tailored to ad-hoc retrieval.- The proposed pre-training method PROP is inspired by classical statistical language models for IR, specifically the query likelihood model. This connects the pre-training objective to fundamental IR theory. Other pre-training methods like BERT are not designed based on IR principles.- PROP pre-trains a Transformer model with a new objectives called Representative Words Prediction (ROP), along with Masked LM. The ROP task better resembles the relevance matching requirement in ad-hoc retrieval compared to coherence-based objectives like Next Sentence Prediction used in BERT.- The paper shows PROP achieves significantly better performance compared to BERT and other baselines on several standard ad-hoc retrieval datasets. This demonstrates the effectiveness of designing pre-training objectives tailored for IR rather than just borrowing from NLP.- The paper also shows PROP is sample-efficient, achieving strong performance with limited supervision on new datasets. This could be advantageous in low-resource IR scenarios compared to training neural rankers from scratch.- Overall, the work introduces a novel pre-training paradigm for IR that is tailored to ad-hoc retrieval. The results validate the benefit of this approach over standard pre-training like BERT. The use of IR theory to guide pre-training design is a key difference from prior work.In summary, this paper pushes forward research on pre-training for IR by proposing objectives better aligned with core IR tasks like ad-hoc retrieval. The principles could potentially be extended to other IR scenarios as well in future work.


## What future research directions do the authors suggest?

Based on my reading of the paper, here are some of the main future research directions suggested by the authors:- Test the ability of PROP on other types of downstream IR tasks beyond ad-hoc retrieval, such as passage retrieval in QA systems or response retrieval in dialog systems. This would help evaluate the versatility and generalizability of the model.- Investigate new ways to further enhance the pre-training tailored for IR. For example, exploring different sampling strategies for generating pseudo queries during pre-training, or designing more advanced pre-training objectives inspired by other IR theories.- Evaluate the effectiveness of integrating contextualized representations from PROP into existing neural ranking models, since PROP shows better performance than BERT.- Explore multilingual PROP by pre-training on other languages and testing on corresponding downstream tasks.- Evaluate PROP on larger datasets and more difficult benchmarks to better understand its capabilities and limitations.- Extend PROP to handle not only text ranking but also other data types like images, audio, etc. This could help adapt it to multimedia search tasks.- Study the theoretical connections between the pre-training objective of PROP and the training objectives of supervised IR models to better guide pre-training design.- Analyze the learned representations of PROP to shed light on what linguistic properties it captures that benefit ad-hoc retrieval.In summary, the authors suggest further testing PROP in diverse IR applications, devising enhanced pre-training strategies, integrating it into existing models, and conducting more analysis to guide improvements. Advancing pre-training for IR is highlighted as an important direction.
