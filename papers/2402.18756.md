# [How Much Annotation is Needed to Compare Summarization Models?](https://arxiv.org/abs/2402.18756)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem: 
- Evaluating and comparing new summarization models requires collecting human judgements or references at scale, which is costly. 
- It's unclear how much data is actually needed to reliably compare and select the best model for a new domain/purpose.

Methods:
- Compared summarization models on news datasets using automatic metrics over varying test set sizes. Findings: clear preferences emerge from under 100 examples.  
- Collected human judgements on 100 samples varying task context and input source. Compared 3 models across 3 datasets and 4 tasks.
- Validated automatic metrics on ability to reproduce human preferences across contexts.

Key Findings:
- Both automatic and human evaluations show preferences stabilize around 50 test examples.
- Human preferences between models flip depending on task and input source. 
- Automatic metrics have mixed ability to predict human preferences: ROUGE-1 and GPT-4 as annotator achieve 78% accuracy, outperforming BERTScore (56%) and G-Eval (44%).

Main Contributions:
- Empirically demonstrated evaluations converge quickly - around 50 examples sufficient.
- Showed variation in human preference challenges automatic evaluation validation.
- Proposed new accuracy-based method to validate metrics across tasks/data.
- Found metrics have mixed accuracy in matching human preferences.

The key implication is both human and automatic comparative evaluations can be reliably performed at smaller scale, greatly reducing evaluation costs. The variation in human preferences also highlights issues in over-relying on automatic metrics.
