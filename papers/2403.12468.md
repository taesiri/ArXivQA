# [CrossTune: Black-Box Few-Shot Classification with Label Enhancement](https://arxiv.org/abs/2403.12468)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Training large language models (LLMs) like GPT-3 is very computationally expensive. Hence there is a need for more parameter-efficient methods to adapt these models to downstream tasks. 
- One approach is black-box tuning where the LLM is treated as a fixed feature extractor and its outputs are optimized for the target task. 
- Existing black-box methods rely on optimizing prompts which is inefficient and unstable.

Method: 
- The paper proposes CrossTune, a light-weight black-box tuning method without prompt search. 
- CrossTune extracts features from the LLM for the input text. It also encodes label descriptions using the LLM.
- A cross-attention module aligns the input text embeddings with the label embeddings to predict the right label.
- To improve generalization on limited data, ChatGPT generates additional pseudo-labeled data conditioned on the labels. A switch mechanism filters low-quality data.

Contributions:
- Introduces CrossTune - an effective black-box tuning approach without prompt optimization 
- Leverages label semantics and ChatGPT's generation capabilities for low-resource classification
- Outperforms state-of-the-art black-box method by 5.7% on average across 7 text classification datasets
- Achieves strong performance even without data augmentation from ChatGPT
- Provides an alternative to reliance on large unlabeled in-domain corpora for generalization

The key idea is to steer the LLM to focus on input text aspects that are semantically related to the label descriptions. This provides useful guidance for text classification, especially in few-shot scenarios. Augmenting training data with ChatGPT instead of an unlabeled corpus also offers greater flexibility.
