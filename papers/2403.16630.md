# [A comparative analysis of embedding models for patent similarity](https://arxiv.org/abs/2403.16630)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Calculating patent-to-patent (p2p) similarity is important for mapping innovation dynamics, but lacks a ground-truth benchmark dataset for evaluation. 
- Prior works have used static word embeddings (e.g. word2vec) or contextual embeddings (e.g. BERT) for this task, but it's unclear which performs better, and contextual embedding approaches lack comparison.

Proposed Solution:
- Create a ground-truth dataset using 1,324 patent interference cases from USPTO as indicators of maximum similarity. After preprocessing, get 133 pairs of highly similar patent claims.   
- Compare performance on this dataset across word2vec, doc2vec, existing augmented Sentence BERT (SBERT), and two new SBERT variants:
  - Patent SBERT-ub: Finetuned RoBERTa model
  - Patent SBERT-adapt-ub: Domain adaptation of pretrained SBERT 

Key Contributions:

1. New SOTA for patent similarity using Patent SBERT-adapt-ub, showing benefit of domain adaptation with golden data.

2. Static word2vec model is comparable to transformers-based approaches, suggesting extensive pretraining on in-domain data can offset advantages of context.

3. Among SBERT models, domain adaptation outperforms finetuning alone out of the box transformers, confirming value of specializing models.

Overall, the paper demonstrates creating ground-truth patent similarity data, and using it to showcase SOTA performance as well as compare static vs contextual embeddings. The results also validate the value of domain-focused training even for powerful pretrained language models.
