# [Reproducibility and Geometric Intrinsic Dimensionality: An Investigation   on Graph Neural Network Research](https://arxiv.org/abs/2403.08438)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Reproducibility of machine learning research results is important but challenging to achieve in practice. There is a lack of standardized frameworks to systematically evaluate reproducibility. 
- Another key challenge is the curse of dimensionality, which makes it difficult to obtain representative data and hinders model training and inference. The concept of intrinsic dimension (ID) can help analyze this issue.

Proposed Solution:
- Develop a comprehensive ontology of reproducibility for machine learning research to standardize evaluation. The ontology covers 3 main categories - dataset, software, computational results - with detailed subcategories and questions. 
- Apply this ontology to evaluate reproducibility of 6 major graph neural network papers. Identify key reproducibility barriers.
- Investigate the influence of altering geometric intrinsic dimension of datasets on performance of reproduced graph neural network models. Use ID-based feature selection to reduce dimensionality. 

Main Contributions:  
- Novel ontology formalizing various aspects of reproducibility in machine learning, enabling standardized evaluation. Demonstrated via case study on graph neural networks.
- Analysis of reproducibility of influential graph neural network papers based on proposed ontology. Identified several common reproducibility issues.  
- Investigation of influence of intrinsic dimensionality on model performance through controlled experiments. Results indicate dependency of methods on ID.
- Recommendations and best practices for improving reproducibility and accounting for effects of intrinsic dimension in research.

In summary, the paper makes important theoretical and empirical contributions regarding reproducibility evaluation in ML and the effects of intrinsic dimensionality. The proposed ontology enables systematic assessment of reproducibility. Experiments highlight the significance of accounting for data dimensionality.


## Summarize the paper in one sentence.

 Here is a one sentence summary of the key points from the paper:

The paper introduces an ontology for evaluating reproducibility in machine learning research, applies it to survey graph neural network methods, and investigates how changing the intrinsic dimensionality of datasets impacts model performance across different learning algorithms.


## What is the main contribution of this paper?

 According to the paper, the main contributions are:

1. Introducing an ontology of reproducibility in machine learning to evaluate the reproducibility of machine learning research in a standardized way. The ontology focuses on empirical evidence and has categories for data set, software, and computational results.

2. Conducting a reproducibility study on major graph neural network research, selecting and attempting to reproduce 6 highly-cited papers. The study analyzes the reproducibility of these papers based on the proposed ontology.

3. Investigating the influence of intrinsic dimensionality, a measure related to the curse of dimensionality, on model performance. The paper examines whether changes to the intrinsic dimensionality of datasets used for training coincide with changes in model performance across the reproduced graph neural network methods.

So in summary, the main contributions are: (1) a reproducibility ontology tailored to machine learning, (2) a reproducibility study of graph neural networks based on this ontology, and (3) an analysis of the influence of intrinsic dimensionality on model performance.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and keywords associated with this paper include:

- Reproducibility - The paper introduces an ontology of reproducibility in machine learning and applies it to evaluate the reproducibility of graph neural network research.

- Replication - The concept of replicating/reproducing prior research results is a key theme. 

- Curse of dimensionality - The paper investigates how the intrinsic dimension of datasets influences machine learning model performance, relating it to the curse of dimensionality.

- Intrinsic dimension - The intrinsic dimension of datasets, referring to a measure of concentration that captures the underlying data structure, is a central concept examined. 

- Feature selection - Feature selection based on intrinsic dimension is used to manipulate datasets and evaluate model performance.

- Graph neural networks - The paper focuses its reproducibility analysis and experiments on research in the field of graph neural networks.

- Machine learning - More broadly, the themes of reproducibility, curse of dimensionality, and intrinsic dimension are examined in the context of machine learning research.

So in summary, key terms cover reproducibility, replication, dimensionality, intrinsic dimension, feature selection, graph neural networks, and machine learning.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 potential in-depth questions about the methods proposed in this paper:

1) The paper proposes an ontology for evaluating reproducibility in machine learning research. Can you describe the top-level categories in this ontology and what key aspects of reproducibility they are designed to assess?

2) The paper applies a concept called "intrinsic dimension" to study the influence of input data on model performance. Can you explain what intrinsic dimension measures and how it relates to the curse of dimensionality in machine learning? 

3) The paper approximates the intrinsic dimension of datasets using the concept of "support sequences." How does this approximation work and what are its computational advantages?

4) The paper performs feature selection on datasets based on the intrinsic dimensionality contribution of each feature. How exactly is this contribution quantified and used to discard features?

5) The GCN model shows stable performance even when discarding a large percentage of features based on intrinsic dimensionality. In contrast, GraphSAGE deteriorates rapidly. What might explain this difference in robustness between the models?

6) For the DiffPool model, the paper had to modify how node features were used compared to the original implementation. How did this impact results and what might it suggest about how DiffPool utilizes features?

7) Across different models, the paper finds minimal performance impact when discarding features representing up to 30% of a dataset's intrinsic dimensionality. Why might most models be robust to reducing dimensionality to this degree?

8) The paper observes preprocessing can significantly alter the intrinsic dimensionality distribution of features in a dataset. How might this impact the meaningfulness of comparing model performance on intrinsically reduced data?  

9) The paper's intrinsic dimensionality analysis currently only considers node features and ignores graph structure. How could the methodology be extended to incorporate topological information as well?

10) What are some limitations of using the proposed intrinsic dimensionality metric to study the influence of input data on model behavior compared to other dimensionality reduction techniques?
