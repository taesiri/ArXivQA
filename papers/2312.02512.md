# [AV2AV: Direct Audio-Visual Speech to Audio-Visual Speech Translation   with Unified Audio-Visual Speech Representation](https://arxiv.org/abs/2312.02512)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a high-quality paragraph summarizing the key points of the paper:

This paper proposes a novel direct Audio-Visual Speech to Audio-Visual Speech Translation (AV2AV) framework that can translate spoken language between languages while maintaining both the audio and visual speech streams. To address the lack of parallel training data between audio-visual speech, they leverage the modality-agnostic capabilities of a multilingually trained AV-HuBERT model to obtain unified audio-visual speech representations from audio-only data. These discrete representations termed "AV speech units" encapsulate linguistic content and can be treated as pseudo-text to train the translation model on audio-to-audio datasets. To generate synchronized raw audio and video outputs, they introduce an AV-Renderer with zero-shot speaker modeling to preserve speaker identity before and after translation. Experiments demonstrate comparable performance to cascaded systems and robustness to acoustic noise corruption. The proposed unified modeling approach allows a single AV2AV model to perform translation accepting either audio, visual or audio-visual inputs. Overall, this direct speech translation framework with multimodal input and outputs enhances realism and robustness while reducing complexity compared to cascaded systems.
