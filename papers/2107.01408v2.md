# [Scale Mixtures of Neural Network Gaussian Processes](https://arxiv.org/abs/2107.01408v2)

## What is the central research question or hypothesis that this paper addresses?

 This paper addresses the question of how to extend neural network Gaussian processes (NNGPs) to go beyond Gaussian processes and represent more flexible stochastic processes such as heavy-tailed processes. The central hypothesis is that introducing a prior distribution on the scale parameter of the last layer weights in an infinitely wide neural network can turn the network into a richer class of stochastic processes beyond NNGPs, which the authors call "scale mixtures of NNGPs". Specifically, the paper shows that using an inverse gamma prior leads to Student's t processes, which are heavy-tailed.

The key contributions are:

- Proposing the scale mixture of NNGPs framework to obtain more flexible stochastic processes from infinite neural nets.

- Showing that an inverse gamma prior leads to Student's t processes.

- Analyzing the distribution of neural nets with this construction after initialization and gradient descent training. 

- Developing efficient inference algorithms for the scale mixture of NNGPs.

- Demonstrating empirically that the heavy-tailed processes obtained are more robust to out-of-distribution data compared to regular NNGPs.

So in summary, the central hypothesis is that a simple modification of putting a prior on the last layer scale can extend NNGPs to more expressive stochastic processes, and the paper provides theory, algorithms, and experiments to validate this idea.


## What is the main contribution of this paper?

 This paper proposes a simple extension of Neural Network Gaussian Processes (NNGPs) by introducing a prior distribution on the scale parameter of the last layer weights. The key contributions are:

1. It shows that introducing a scale prior on just the last layer weights turns an infinitely-wide Bayesian neural network (BNN) of any architecture into a richer class of stochastic processes, termed scale mixtures of NNGPS. With certain priors like inverse gamma, these yield heavy-tailed processes like Student's t processes. 

2. It analyzes the distribution of neural nets initialized with this prior and trained with gradient descent. Similar results are derived as in the NNGP case, showing the distribution converges to a scale mixture of Gaussian processes.

3. For the inverse gamma prior leading to Student's t processes, it shows the kernel and exact posterior can be computed efficiently like in the NNGP case. For generic priors, it presents an efficient approximate inference method.

4. Experimentally, it demonstrates the benefit of using these heavy-tailed processes over NNGPs. The Student's t processes are more robust to out-of-distribution data in regression and classification tasks.

In summary, the main contribution is proposing a simple way to extend NNGPS to more flexible stochastic processes using scale mixtures. This leads to heavy-tailed processes that have benefits like better uncertainty calibration, while retaining much of the convenient properties of NNGPS.
