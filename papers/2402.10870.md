# [Best of Three Worlds: Adaptive Experimentation for Digital Marketing in   Practice](https://arxiv.org/abs/2402.10870)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Adaptive experimental design (AED) methods are being increasingly used in industry to reduce experimentation cost and increase testing throughput compared to traditional A/B testing. However, the behavior and guarantees of AED methods are not well understood in real-world non-stationary environments.

- The paper presents illustrative case studies using real experiment data that highlight key challenges with using AED methods naively:
  - Regret minimizing algorithms like Thompson Sampling fail to provide valid statistical inferences and are slow to identify the best arm.
  - Running empirical mean is problematic for estimation and inference due to Simpson's paradox.
  - Objectives are not well defined in time-varying settings.

Proposed Solution:
- The paper proposes a "best of three worlds" approach that efficiently identifies the counterfactual optimal treatment, mitigates opportunity cost, and is robust to real-world time variation.

- A new metric called cumulative gain is proposed to measure what reward an arm could have obtained if it received all traffic. An unbiased estimator based on inverse propensity scoring is presented.

- Always-valid sequential confidence intervals enable robust statistical inference.

- An elimination algorithm called CGSE is developed that uniformly splits traffic among active arms and eliminates arms proven suboptimal.

Main Contributions:
- Provides perspective on objectives and requirements for AED systems in industrial settings through real-world case studies and data.

- Develops a cumulative gain metric along with an unbiased estimator and always-valid confidence intervals for statistical inference.

- Proposes the CGSE algorithm for efficient identification of the counterfactual optimal arm with opportunity cost mitigation.

- Demonstrates strong empirical performance of the proposed techniques on real experiments relative to baselines like Thompson Sampling.

- Provides theoretical guarantees for CGSE in terms of identification and regret.

In summary, the paper addresses key practical challenges with using AED methods naively in real-world situations and contributes a robust approach with an efficient algorithm, always-valid inference, and proven guarantees.


## Summarize the paper in one sentence.

 Here is a one sentence summary of the key points from the paper:

The paper proposes a new approach for adaptive experimentation that combines an unbiased cumulative gain metric and estimator robust to time variation, always-valid sequential inference through confidence intervals, and an elimination algorithm to efficiently identify the counterfactual best arm while minimizing regret.


## What is the main contribution of this paper?

 This paper makes several key contributions:

1. It demonstrates through real-world case studies the challenges of using adaptive experimental design (AED) methods like bandits in industrial settings where non-stationarity is common. Issues like Simpson's paradox can lead algorithms like Thompson Sampling to make poor decisions. 

2. It argues for using the cumulative gain metric and corresponding unbiased estimators as more reliable measures of performance that avoid issues like Simpson's paradox. The paper derives always-valid confidence intervals for these estimators.

3. It proposes an adaptive approach called Cumulative Gain Successive Elimination (CGSE) that aims to quickly eliminate suboptimal options while controlling estimator variance. CGSE provably identifies the best option under assumptions on time variation.  

4. Experiments on real production traffic demonstrate the robustness benefits of CGSE over regret-minimizing algorithms like Thompson Sampling, while still approximately minimizing opportunity cost. The always-valid confidence intervals also enable monitoring experiments.

In summary, the main contribution is developing a rigorous and robust AED framework that overcomes the challenges of industrial non-stationarity to efficiently identify the best performing option. The paper brings important insights on objectives and methods for trustworthy experimentation.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, here are some of the key terms and concepts:

- Adaptive experimental design (AED)
- Best arm identification
- Time variation
- Non-stationarity
- Regret minimization
- Cumulative gain estimator  
- Always-valid confidence intervals
- Simpson's paradox
- Successive elimination

The paper discusses using adaptive experimental design methods in industrial settings for applications like digital marketing. It highlights challenges with using common approaches like Thompson Sampling when there is time variation in the underlying data. The paper proposes using a cumulative gain metric and estimator along with always-valid confidence intervals and an elimination-based algorithm. Key goals are identifying the best performing option efficiently while being robust to non-stationarity.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in the paper:

1. The paper proposes using a cumulative gain metric instead of a running empirical mean for estimating arm performance. Can you explain why the cumulative gain metric is more robust to Simpson's paradox and issues that arise from adaptive data collection?

2. The cumulative gain estimator uses inverse propensity scoring. What is the motivation behind this, and how does it make the estimator unbiased? What are some potential downsides of using inverse propensity scoring?

3. The paper presents an elimination-based adaptive algorithm called CGSE. Can you walk through how CGSE works, especially how it eliminates suboptimal arms? What theoretical guarantees does CGSE provide?

4. How exactly does the paper derive always-valid confidence intervals for the cumulative gain metric? What assumptions need to hold for the validity of these confidence intervals? 

5. The experiments compare CGSE to Thompson sampling. Can you explain what the key differences are in how these algorithms allocate traffic and estimate arm performance? What seem to be the failure modes for Thompson sampling based on the experiments?

6. Based on the experiments and case studies, what are some of the key challenges for adaptive experimentation systems used in industrial settings? How does the proposed CGSE algorithm aim to overcome these?

7. The paper argues for "the best of three worlds" in experimentation systems. What exactly are those three worlds and how does CGSE balance them? Where does CGSE potentially fall short?

8. One could argue that CGSE sacrifices some regret minimization for better arm identification. Can you quantify and discuss this tradeoff both theoretically and based on the experiments? 

9. The analysis makes assumptions about the nature of time variation, like in Assumptions 1 and 2. Can you discuss if you think these are reasonable assumptions for industrial settings and how they could be relaxed?

10. The paper mentions a variance reduced cumulative gain estimator for the fixed effect setting. Can you explain this estimator, why it reduces variance, and when it would be preferred to use over the standard cumulative gain estimator?
