# [OPT-R: Exploring the Role of Explanations in Finetuning and Prompting   for Reasoning Skills of Large Language Models](https://arxiv.org/abs/2305.12001)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be:What is the impact of incorporating explanations during finetuning and prompting on improving the reasoning skills of large language models? Specifically, the paper investigates how finetuning the OPT model on reasoning datasets with explanations affects its performance on a variety of reasoning skills when tested using different prompting methods. The key dimensions explored are:- Finetuning: Training the OPT model with and without explanations to create OPT-R and OPT-RE models. - Prompting: Evaluating the models using zero-shot, few-shot, and few-shot with explanations prompting.- Scale: Testing OPT models of different sizes - 1.3B, 6.7B, and 13B parameters.The paper aims to analyze the interplay between these three factors - finetuning, prompting, and scale - to understand when and how explanations help improve reasoning skills of large language models. The central hypothesis seems to be that incorporating explanations during finetuning and/or prompting will enhance the reasoning abilities of LLMs across different skills and model sizes. The experiments aim to test this hypothesis systematically using the 3D evaluation grid.In summary, the key research question is about the role and impact of explanations on reasoning skills of LLMs, studied along the dimensions of finetuning, prompting and scale. The paper aims to provide a comprehensive analysis to uncover when explanations help, when they don't, and when they might actually hurt reasoning performance.
