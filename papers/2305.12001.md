# [OPT-R: Exploring the Role of Explanations in Finetuning and Prompting   for Reasoning Skills of Large Language Models](https://arxiv.org/abs/2305.12001)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be:What is the impact of incorporating explanations during finetuning and prompting on improving the reasoning skills of large language models? Specifically, the paper investigates how finetuning the OPT model on reasoning datasets with explanations affects its performance on a variety of reasoning skills when tested using different prompting methods. The key dimensions explored are:- Finetuning: Training the OPT model with and without explanations to create OPT-R and OPT-RE models. - Prompting: Evaluating the models using zero-shot, few-shot, and few-shot with explanations prompting.- Scale: Testing OPT models of different sizes - 1.3B, 6.7B, and 13B parameters.The paper aims to analyze the interplay between these three factors - finetuning, prompting, and scale - to understand when and how explanations help improve reasoning skills of large language models. The central hypothesis seems to be that incorporating explanations during finetuning and/or prompting will enhance the reasoning abilities of LLMs across different skills and model sizes. The experiments aim to test this hypothesis systematically using the 3D evaluation grid.In summary, the key research question is about the role and impact of explanations on reasoning skills of LLMs, studied along the dimensions of finetuning, prompting and scale. The paper aims to provide a comprehensive analysis to uncover when explanations help, when they don't, and when they might actually hurt reasoning performance.


## What is the main contribution of this paper?

The main contribution of this paper is a thorough investigation into the reasoning capabilities of Large Language Models (LLMs), specifically the Open Pretrained Transformers (OPT) model. The key aspects of the contribution are:- Finetuning three different sizes of OPT (1.3B, 6.7B, 13B) on a curated collection of reasoning datasets that have explanations. This results in two sets of finetuned models - OPT-R finetuned without explanations, and OPT-RE finetuned with explanations.- Evaluating all the models on 57 out-of-domain reasoning tasks from the SUPER-NATURALINSTRUCTIONS benchmark, covering 26 distinct reasoning skills. The evaluation uses three prompting techniques - zeroshot, fewshot, and fewshot with explanations. - Analyzing the results through the lens of three key dimensions - finetuning, prompting, and scale. This creates a comprehensive grid of 27 configurations, resulting in a total of 6,156 test evaluations. - Finding that explanations during finetuning and prompting provide a slight improvement in performance, but the effect is not significant for finetuned models. Explanations help vanilla OPT more substantially.- Observing consistent increases from no finetuning to finetuning without explanations to finetuning with explanations, indicating explanations have a small positive effect.- Providing insights into which reasoning skills benefit most from explanations during finetuning vs prompting.In summary, the main contribution is a rigorous investigation of the role of explanations in finetuning and prompting for improving reasoning skills of LLMs, analyzed across multiple model sizes and a comprehensive set of reasoning skills and tasks.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper investigates the impact of incorporating explanations during finetuning and prompting on the reasoning capabilities of Large Language Models across different scales, finding that while explanations provide a slight improvement, their effect is not significant for finetuned models during fewshot prompting but more noticeable for non-finetuned models.


## How does this paper compare to other research in the same field?

Here are a few key ways this paper compares to other research on improving reasoning skills of large language models (LLMs):- Focuses specifically on evaluating the OPT model across different scales, whereas much prior work has looked at other LLMs like GPT-3. Looking at OPT allows for a controlled investigation of the impact of scale.- Comprehensively evaluates the impact of explanations during both finetuning and prompting on reasoning skills. Most prior work has focused on just finetuning or just prompting. Analyzing both allows for deeper insights.- Covers a broad range of 26 distinct reasoning skills using standardized tasks from the SUPER-NLI benchmark. Many prior studies evaluate on 1 or 2 skills, or custom datasets. The breadth provides a more complete picture. - Uses rank classification to evaluate model performance rather than text generation. Classification better measures reasoning capabilities directly. Text generation introduces other confounding factors.- Finetunes and evaluates systematically using a grid of 27 different configurations spanning scale, prompting methods, and finetuning datasets. This allows analyzing the interaction between these key factors.- Makes models and code available for further research. Many recent papers on LLMs use proprietary models/data. The openness here enables building on these findings.Overall, the comprehensive and rigorous evaluation of OPT across different conditions on a wide range of reasoning skills advances our understanding significantly compared to prior work. The analysis provides concrete insights on how best to improve reasoning abilities of LLMs like OPT through scale, prompting, and finetuning.


## What future research directions do the authors suggest?

Based on my reading of the paper, some of the main future research directions suggested by the authors include:- Evaluating the impact of explanations during finetuning and prompting across a broader range of language models, dataset sizes, and finetuning strategies. The authors acknowledge their study was limited to evaluating one model (OPT) on a small set of reasoning datasets. Further research could explore whether their findings generalize to other models and datasets.- Investigating the role of explanations in finetuning and prompting with much larger language models. The authors were limited to smaller scaled models, so it would be interesting to see if explanations have a different effect when working with models in the 100B+ parameter range.- Exploring different finetuning strategies beyond few-shot prompting. The authors only considered few-shot prompted finetuning, so future work could look at how explanations impact models finetuned without in-context examples. - Analyzing the effect of explanations on a wider range of reasoning skills and tasks. The authors' evaluation covered 26 skills but there may be other high-level reasoning abilities not captured in their analysis. Expanding the diversity of skills assessed could provide further insights.- Conducting human evaluations and comparisons. The authors rely on benchmark dataset evaluations, but human studies could further illuminate the true reasoning capabilities of the models.- Investigating other methods of incorporating explanations during training, such as using explanations as an auxiliary objective. The authors only finetune the standard LM objective but other techniques could be explored.- Evaluating the utility of explanations for real-world applications of LLMs requiring reasoning. The authors' analysis is mainly theoretical so applying it to practical use cases could be valuable.In summary, the authors call for additional research to evaluate the impact of explanations across more models, data, skills, and techniques to gain a more comprehensive understanding of how to improve reasoning in LLMs. Human studies and real-world applications are also cited as important directions for future work.
