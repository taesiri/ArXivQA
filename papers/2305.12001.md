# [OPT-R: Exploring the Role of Explanations in Finetuning and Prompting   for Reasoning Skills of Large Language Models](https://arxiv.org/abs/2305.12001)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be:What is the impact of incorporating explanations during finetuning and prompting on improving the reasoning skills of large language models? Specifically, the paper investigates how finetuning the OPT model on reasoning datasets with explanations affects its performance on a variety of reasoning skills when tested using different prompting methods. The key dimensions explored are:- Finetuning: Training the OPT model with and without explanations to create OPT-R and OPT-RE models. - Prompting: Evaluating the models using zero-shot, few-shot, and few-shot with explanations prompting.- Scale: Testing OPT models of different sizes - 1.3B, 6.7B, and 13B parameters.The paper aims to analyze the interplay between these three factors - finetuning, prompting, and scale - to understand when and how explanations help improve reasoning skills of large language models. The central hypothesis seems to be that incorporating explanations during finetuning and/or prompting will enhance the reasoning abilities of LLMs across different skills and model sizes. The experiments aim to test this hypothesis systematically using the 3D evaluation grid.In summary, the key research question is about the role and impact of explanations on reasoning skills of LLMs, studied along the dimensions of finetuning, prompting and scale. The paper aims to provide a comprehensive analysis to uncover when explanations help, when they don't, and when they might actually hurt reasoning performance.


## What is the main contribution of this paper?

The main contribution of this paper is a thorough investigation into the reasoning capabilities of Large Language Models (LLMs), specifically the Open Pretrained Transformers (OPT) model. The key aspects of the contribution are:- Finetuning three different sizes of OPT (1.3B, 6.7B, 13B) on a curated collection of reasoning datasets that have explanations. This results in two sets of finetuned models - OPT-R finetuned without explanations, and OPT-RE finetuned with explanations.- Evaluating all the models on 57 out-of-domain reasoning tasks from the SUPER-NATURALINSTRUCTIONS benchmark, covering 26 distinct reasoning skills. The evaluation uses three prompting techniques - zeroshot, fewshot, and fewshot with explanations. - Analyzing the results through the lens of three key dimensions - finetuning, prompting, and scale. This creates a comprehensive grid of 27 configurations, resulting in a total of 6,156 test evaluations. - Finding that explanations during finetuning and prompting provide a slight improvement in performance, but the effect is not significant for finetuned models. Explanations help vanilla OPT more substantially.- Observing consistent increases from no finetuning to finetuning without explanations to finetuning with explanations, indicating explanations have a small positive effect.- Providing insights into which reasoning skills benefit most from explanations during finetuning vs prompting.In summary, the main contribution is a rigorous investigation of the role of explanations in finetuning and prompting for improving reasoning skills of LLMs, analyzed across multiple model sizes and a comprehensive set of reasoning skills and tasks.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper investigates the impact of incorporating explanations during finetuning and prompting on the reasoning capabilities of Large Language Models across different scales, finding that while explanations provide a slight improvement, their effect is not significant for finetuned models during fewshot prompting but more noticeable for non-finetuned models.
