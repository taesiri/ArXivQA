# [Low-rank Attention Side-Tuning for Parameter-Efficient Fine-Tuning](https://arxiv.org/abs/2402.04009)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Finetuning large pretrained models like Vision Transformers (ViTs) on downstream tasks is effective, but suffers from high GPU memory usage and slow training speed. 
- Existing parameter-efficient finetuning (PEFT) methods reduce number of trainable parameters but still have entangled computations between frozen and trainable parts, requiring caching activations and gradients of the frozen model.

Proposed Solution: 
- Introduce LAST (Low-rank Attention Side-Tuning), a form of side-tuning where both parameters and outputs of the pretrained model are frozen.
- Add a lightweight side network composed of Transformer blocks with only low-rank self-attention modules and no feedforward layers. The side network takes intermediate features from the frozen pretrained network as additional input.
- Low-rank self-attention uses projections to very low dimensionality (e.g. 16) before applying standard multi-head self-attention.
- Subtract accumulated intermediate features from side network output to correct bias and recover pretrained model's representations.

Main Contributions:
- LAST disentangles computations between pretrained and side network, allowing the pretrained model to be treated as frozen feature extractor. This eliminates need to cache activations and gradients, cutting memory usage and accelerating training.
- Experiments show LAST achieves state-of-the-art accuracy among PEFT methods on VTAB and other vision datasets, while using far less GPU memory and training much faster. 
- LAST enables efficiently finetuning huge models like ViT-g on a single outdated GPU.
- The proposed low-rank self-attention is shown effective for the first time, without need for feedforward layers.
- LAST also enables parallel hyperparameter search by training many side networks simultaneously.

In summary, LAST advances state-of-the-art in parameter-efficient finetuning through a lightweight side network architecture that trains faster, adapts better, and transfers more efficiently.


## Summarize the paper in one sentence.

 Here is a one-sentence summary of the paper:

The paper proposes LAST, a parameter-efficient fine-tuning method that trains a low-rank self-attention side network to learn task-specific knowledge, achieving state-of-the-art accuracy with significantly lower GPU memory usage and faster training compared to existing methods.


## What is the main contribution of this paper?

 The main contributions of this paper are:

1. It proposes LAST (Low-rank Attention Side-Tuning), a side-tuning framework for parameter-efficient fine-tuning (PEFT) of vision transformers. LAST achieves state-of-the-art PEFT accuracy while enjoying small GPU memory footprint, fast training speed, and parameter efficiency. 

2. It introduces the LSA (low-rank self-attention) module, which utilizes very low-dimensional self-attention and discards the large feedforward network for side-tuning. This is the first work showing the utility of self-attention with extremely low rank for downstream vision tasks.

3. It corrects the bias in LSA to properly recover the pretrained network's representation in the final output. This bias correction is shown to be important especially when more intermediate features are added to the side network.

4. LAST enables parallel fine-tuning of multiple models with different hyperparameters simultaneously since the pretrained backbone can be treated as a standalone feature extractor. This greatly facilitates hyperparameter search during PEFT.

In summary, LAST advances the state-of-the-art in PEFT for vision transformers, with superior accuracy, efficiency, and flexibility compared to previous methods. The key innovation is the introduction of the low-rank self-attention module for side-tuning.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and concepts associated with this paper include:

- Low-rank Attention Side-Tuning (LAST): The main method proposed in the paper for parameter-efficient fine-tuning (PEFT) using a side network with low-rank self-attention.

- Low-rank self-attention (LSA): The key component of the proposed LAST framework, using very low-dimensional (rank) self-attention modules without feedforward networks. 

- Side-tuning: The strategy of using a separate small network parallel to the frozen pretrained model for task adaptation. Provides benefits like small memory footprint.

- Parameter-efficient fine-tuning (PEFT): Finetuning large pretrained models with few trainable parameters to reduce storage costs and prevent overfitting.

- Vision transformers: The pretrained models used in the paper's experiments, based on the original Transformer architecture.

- GPU memory footprint: One of the main considerations for comparing PEFT methods, related to caching activation maps and gradient computations.

- VTAB-1K: A visual task adaptation benchmark used to evaluate the transferability of pretrained vision models.

- Fine-grained visual classification (FGVC): Additional visual recognition tasks used to test the LAST framework, requiring subtle within-category distinction.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 potential in-depth questions about the method proposed in this paper:

1. The paper proposes throwing away large feedforward networks (FFNs) in the side network and using only low-rank self-attention. What is the intuition behind this design choice? Does the ablation study provide empirical evidence supporting this decision?

2. The low-rank self-attention (LSA) module projects tokens to a much lower dimension before applying self-attention. How does using an extremely low dimension (e.g. 4) still allow effective learning? Does this support the hypothesis that the domain difference between pretraining and downstream tasks is small?

3. The paper mentions the LSA bias caused by the residual connection. Why does this bias occur and how exactly does the proposed subtraction resolve it? What would happen if this bias was not corrected?

4. How does LAST enable highly parallel training across different optimization objectives? What are some potential use cases or advantages of this capability compared to other methods? 

5. The gap factor g decides the number of side blocks in LAST. How does g affect model capacity, memory usage, and accuracy? What considerations should inform choice of g?

6. How exactly does LAST reduce GPU memory usage compared to other methods? Which specific terms in the gradient computation formula are reduced?

7. What modifications would need to be made to apply LAST to other backbone architectures besides Vision Transformers? What challenges might arise?

8. How suitable is LAST for continual learning across multiple downstream tasks? Would the side network suffer from catastrophic forgetting?

9. Could a similar low-rank approach be applied successfully in natural language processing domains? What differences between modalities should be considered?

10. The paper shows LAST can scale to finetune very large models like ViT-g on a single outdated GPU. What are the practical implications of this efficiency for model deployment?
