# [How Much Temporal Long-Term Context is Needed for Action Segmentation?](https://arxiv.org/abs/2308.11358)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be: How much long-term temporal context is needed for optimal performance in temporal action segmentation?The key points are:- Temporal action segmentation involves recognizing start and end times of actions in long videos. Modeling long-term context is important but computationally expensive.- Recent works use transformers with local temporal attention windows rather than full context. This limits their performance. - The authors evaluate impact of input window size on two datasets. Using full video context substantially outperforms using only a window, especially for long videos.- They propose an architecture using sparse attention to capture full video context along with windowed attention for local context. - Their model outperforms state-of-the-art on two datasets while being efficient enough to handle very long videos.In summary, the main hypothesis is that using the full temporal context of long videos is critical for optimal temporal action segmentation, and they design a model to test this while remaining efficient.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contributions are:- The paper analyzes how much long-term temporal context is needed for temporal action segmentation. Through experiments, it shows that using the full video context leads to better performance compared to using only a temporal window. - It proposes a new model called LTContext that can leverage both local and long-term temporal context for action segmentation in long videos. This is achieved by combining windowed attention to model local context with sparse attention to capture long-term dependencies.- LTContext achieves state-of-the-art results on three challenging datasets - 50Salads, Breakfast, and Assembly101. In particular, it shows strong performance on Assembly101 which has long videos up to 25 minutes.- The paper provides an in-depth analysis of different components of the LTContext model through ablation studies. This includes analyzing the impact of different attention types, window sizes, number of layers, cross-attention, convolutions, etc.In summary, the key contribution is a new transformer-based model for temporal action segmentation that can effectively leverage both local and long-term context even for very long videos. This is enabled by the combination of windowed and sparse attentions.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper analyzes how much long-term temporal context is needed for optimal performance in temporal action segmentation, and proposes a transformer-based model with sparse attention to efficiently capture full video context while still modeling local frame relations.
