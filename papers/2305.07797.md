# [ACCENT: An Automatic Event Commonsense Evaluation Metric for Open-Domain   Dialogue Systems](https://arxiv.org/abs/2305.07797)

## What is the central research question or hypothesis that this paper addresses?

 This paper focuses on the problem of automatically evaluating the event commonsense reasoning of open-domain dialogue systems. The central hypothesis is that event commonsense, which considers events and their relations, is a key component of overall commonsense reasoning that is still challenging for dialogue systems. 

To investigate this, the authors propose ACCENT, a novel automatic evaluation metric that leverages commonsense knowledge bases (CSKBs) to assess the event commonsense of system responses without needing reference responses. The key ideas are:

- Focusing evaluation on event commonsense rather than broader commonsense, as event reasoning is still difficult for current systems.

- Using event-relation tuples extracted from the dialogue as an intermediate symbolic representation to bridge the gap between free-form text and structured commonsense knowledge.

- Scoring the extracted tuples based on their compatibility with a CSKB to judge the event commonsense of the response.

The central hypothesis is evaluated by constructing the first event commonsense evaluation dataset for open-domain dialogues and demonstrating that ACCENT better correlates with human judgments compared to several baseline metrics.

In summary, the key research question is how to automatically evaluate event commonsense in dialogue systems. The paper proposes ACCENT as a way to achieve this by extracting and scoring event tuples against a commonsense KB. Experiments demonstrate this correlates better with human judgments than prior methods.


## What is the main contribution of this paper?

 The main contribution of this paper is proposing ACCENT, an automatic evaluation metric for measuring the event commonsense capabilities of open-domain dialogue systems. 

Specifically, the key contributions are:

1. ACCENT is the first work that systematically studies and evaluates event commonsense in dialogue systems. It focuses on event commonsense which is crucial for dialogues but is still challenging for current systems.

2. ACCENT utilizes commonsense knowledge bases (CSKBs) to evaluate event commonsense without needing reference responses. It extracts event-relation tuples from the dialogue and measures their compatibility with the CSKB.

3. The paper constructs the first public event commonsense evaluation dataset for open-domain dialogues to train and evaluate ACCENT.

4. Experiments show ACCENT achieves higher correlation with human judgments compared to several baselines. The two components of ACCENT also outperform baselines on joint event-relation extraction and CSKB population tasks.

In summary, the key contribution is proposing ACCENT as an automatic and effective metric to evaluate the event commonsense capabilities of dialogue systems, which is an important but under-explored problem. The new dataset and strong empirical results also validate the usefulness of ACCENT.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the key points from the paper:

The paper proposes ACCENT, a new automatic evaluation metric for assessing the event commonsense reasoning ability of open-domain dialogue systems, using extracted event-relation tuples and a compatibility test against commonsense knowledge bases.


## How does this paper compare to other research in the same field?

 Here are a few key ways this paper on ACCENT (event commonsense evaluation metric) compares to other related work:

- Focus on event commonsense specifically: This paper focuses on evaluating event commonsense in dialogue systems, looking at commonsense knowledge and reasoning about events and their relations. Many prior commonsense evaluation works looked at more general or taxonomic commonsense. 

- Leverages CSKBs in a new way: ACCENT uses commonsense knowledge bases (CSKBs) in an innovative pipeline approach, first extracting event tuples then scoring compatibility with a dynamic CSKB. Other works like the MLP regressor baseline use CSKBs more directly through search.

- New dialogue evaluation dataset collected: The authors collect the first publicly available event commonsense evaluation dataset for open-domain dialogues, with human annotations of tuples and scores. Prior commonsense evaluation datasets were not focused on dialogues.

- Strong empirical results: Experiments show ACCENT achieves substantially higher correlation with human judgments compared to several baseline approaches, demonstrating its effectiveness as an automatic evaluation metric.

- Limitations around coverage: The paper acknowledges limitations around ACCENT's coverage relying on a fixed set of relations and the CSKB used. Expanding to additional knowledge is noted as a worthwhile area for future improvement.

- Novel joint event-relation extraction: The pipeline includes joint extraction of event tuples not addressed much in prior work. There is room to improve this component in the future.

Overall, the paper presents a novel approach to evaluating an important but under-studied aspect of dialogue systems - event commonsense reasoning. The proposed ACCENT metric and collected resources help advance research in this direction.


## What future research directions do the authors suggest?

 Based on my reading of the paper, some of the main future research directions suggested by the authors are:

- Improving the joint event-relation extraction component of the ACCENT framework. The authors note that this is still challenging and there is room for improvement. They suggest exploring methods like synthetic data construction or transfer learning to improve this component without needing more costly human annotations.

- Incorporating additional commonsense knowledge into the ACCENT framework beyond the current relations and the ATOMIC knowledge base. The authors acknowledge that ACCENT may fail to cover some aspects of event commonsense, so augmenting it with more knowledge sources could enhance coverage.

- Evaluating and extending the approach to other dialogue tasks beyond open-domain dialogues, such as task-oriented dialogues. The authors focus on open-domain dialogues but the event commonsense evaluation approach could potentially generalize.

- Considering other sources of commonsense knowledge beyond events and relations, such as common concepts or social norms that are relevant for dialogues. The current focus is event commonsense but other facets of commonsense could also be incorporated.

- Exploring unsupervised or weakly supervised approaches to reduce the annotation cost. The authors rely on human annotations for some parts of ACCENT and note this is expensive. Moving to less supervised techniques could improve scalability.

- Analyzing different neural architectures and self-supervision techniques for the tuple scoring component. The authors use COMET but other recent models could be experimented with.

In summary, the main suggestions are to expand the sources of knowledge and events covered by ACCENT, improve the underlying neural components like the joint extraction model, reduce annotation dependence, and evaluate the approach on other dialogue tasks and genres.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the paper:

The paper proposes ACCENT, an automatic metric for evaluating the event commonsense capabilities of open-domain dialogue systems. Event commonsense, which describes basic facts and relations between events, is an important aspect of general commonsense reasoning. The key idea behind ACCENT is to leverage commonsense knowledge bases (CSKBs) by using event-relation tuples as an intermediate symbolic representation. Specifically, ACCENT first extracts event-relation tuples from the target response and dialogue history. It then checks the compatibility of these tuples against the CSKB by querying a dynamic version of the CSKB to generate plausible tail events. Compatibility scores for the tuples are aggregated to produce an overall event commonsense score. To evaluate ACCENT, the authors collect the first public event commonsense evaluation dataset for dialogues. Experiments demonstrate that ACCENT achieves higher correlation with human judgements compared to baselines. The two components of the ACCENT pipeline also outperform baselines on joint event-relation extraction and CSKB population tasks.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the paper:

The paper proposes ACCENT, an automatic metric for evaluating the event commonsense of open-domain dialogue systems. Event commonsense refers to basic facts and relations about events, which is crucial for natural dialogue but not handled well by current systems. ACCENT leverages commonsense knowledge bases (CSKBs) to assign event commonsense scores to system responses without needing reference responses. It works through a pipeline using extracted event-relation tuples as intermediate symbolic representations. First, tuples are extracted from the dialogue via a generative model using designed prompts for different relations. Then, each tuple is scored based on its compatibility with the CSKB by querying a dynamic CSKB to generate commonsense tails for comparison. Experiments show ACCENT correlates better with human judgments than baselines. The paper also constructs the first event commonsense evaluation dataset for dialogues by collecting human scores and tuples.

In summary, the key ideas are:
- Focusing evaluation on event commonsense which is important for dialogue but challenging
- Proposing ACCENT metric which utilizes CSKB through extracted event tuples 
- Demonstrating ACCENT correlates better with human scores than baselines
- Introducing the first event commonsense evaluation dataset for dialogues with human annotations

The work makes notable contributions in evaluating an important but neglected aspect of dialogue systems. The proposed ACCENT provides an automatic way to assess event commonsense without needing references. The new dataset also enables further research in this direction.


## Summarize the main method used in the paper in one paragraph.

 Here is a one paragraph summary of the main method used in the paper:

The paper proposes ACCENT, an automatic metric for evaluating the event commonsense of responses in open-domain dialog systems. ACCENT uses event-relation tuples as an intermediate symbolic representation to bridge the gap between free-form conversational text and the structured commonsense knowledge in knowledge bases. It extracts event-relation tuples from the dialogue using a prompt-based generative model. Then it checks the compatibility of these tuples against a commonsense knowledge base using COMET, a conditional generative model fine-tuned on the knowledge base. Specifically, it queries COMET to generate possible tail events given the head and relation in each extracted tuple, and compares the generated tails with the true tail using sentence similarity. The overall ACCENT score for a response is computed by averaging the compatibility scores of all its extracted tuples. This pipeline allows ACCENT to leverage commonsense knowledge bases to effectively evaluate the event commonsense of open-domain dialogue without needing manually annotated training data.


## What problem or question is the paper addressing?

 The paper is addressing the problem of evaluating the commonsense capabilities of open-domain dialogue systems. Specifically, it focuses on evaluating event commonsense, which involves knowledge about events and their relations. This is an important but challenging problem because:

1. Commonsense is vital for human-like conversations but current dialogue systems still struggle with consistently producing commonsense-compliant responses. 

2. Evaluating commonsense requires deep semantic understanding along with world knowledge, making it difficult to assess automatically.

3. There is a lack of metrics and datasets for systematically evaluating event commonsense in dialogues, hindering progress in this area.

To tackle this problem, the paper proposes ACCENT, an automatic evaluation metric for assessing the event commonsense of dialogue systems. ACCENT utilizes commonsense knowledge bases to score system responses based on the compatibility of extracted event-relation tuples with commonsense knowledge. The key ideas are:

- Focus on event commonsense instead of broader commonsense, as event relations are crucial for reasoning and interactive dialogues.

- Use an intermediate symbolic representation (event-relation tuples) to bridge the gap between free-form dialogues and structured commonsense knowledge.

- Evaluate tuples based on their compatibility with a commonsense knowledge base rather than requiring reference responses.

The paper also contributes the first event commonsense evaluation dataset for open-domain dialogues. Experiments demonstrate that ACCENT correlates well with human judgments and outperforms baseline methods, providing an efficient automatic evaluation approach.

In summary, the paper tackles the important but understudied problem of automatically evaluating event commonsense in dialogue systems, contributing both a novel evaluation metric and dataset. The results highlight the promise of utilizing commonsense knowledge bases for this challenging task.
