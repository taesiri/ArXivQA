# ACCENT: An Automatic Event Commonsense Evaluation Metric for Open-Domain   Dialogue Systems

## What is the central research question or hypothesis that this paper addresses?

This paper focuses on the problem of automatically evaluating the event commonsense reasoning of open-domain dialogue systems. The central hypothesis is that event commonsense, which considers events and their relations, is a key component of overall commonsense reasoning that is still challenging for dialogue systems. To investigate this, the authors propose ACCENT, a novel automatic evaluation metric that leverages commonsense knowledge bases (CSKBs) to assess the event commonsense of system responses without needing reference responses. The key ideas are:- Focusing evaluation on event commonsense rather than broader commonsense, as event reasoning is still difficult for current systems.- Using event-relation tuples extracted from the dialogue as an intermediate symbolic representation to bridge the gap between free-form text and structured commonsense knowledge.- Scoring the extracted tuples based on their compatibility with a CSKB to judge the event commonsense of the response.The central hypothesis is evaluated by constructing the first event commonsense evaluation dataset for open-domain dialogues and demonstrating that ACCENT better correlates with human judgments compared to several baseline metrics.In summary, the key research question is how to automatically evaluate event commonsense in dialogue systems. The paper proposes ACCENT as a way to achieve this by extracting and scoring event tuples against a commonsense KB. Experiments demonstrate this correlates better with human judgments than prior methods.


## What is the main contribution of this paper?

The main contribution of this paper is proposing ACCENT, an automatic evaluation metric for measuring the event commonsense capabilities of open-domain dialogue systems. Specifically, the key contributions are:1. ACCENT is the first work that systematically studies and evaluates event commonsense in dialogue systems. It focuses on event commonsense which is crucial for dialogues but is still challenging for current systems.2. ACCENT utilizes commonsense knowledge bases (CSKBs) to evaluate event commonsense without needing reference responses. It extracts event-relation tuples from the dialogue and measures their compatibility with the CSKB.3. The paper constructs the first public event commonsense evaluation dataset for open-domain dialogues to train and evaluate ACCENT.4. Experiments show ACCENT achieves higher correlation with human judgments compared to several baselines. The two components of ACCENT also outperform baselines on joint event-relation extraction and CSKB population tasks.In summary, the key contribution is proposing ACCENT as an automatic and effective metric to evaluate the event commonsense capabilities of dialogue systems, which is an important but under-explored problem. The new dataset and strong empirical results also validate the usefulness of ACCENT.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper proposes ACCENT, a new automatic evaluation metric for assessing the event commonsense reasoning ability of open-domain dialogue systems, using extracted event-relation tuples and a compatibility test against commonsense knowledge bases.


## How does this paper compare to other research in the same field?

Here are a few key ways this paper on ACCENT (event commonsense evaluation metric) compares to other related work:- Focus on event commonsense specifically: This paper focuses on evaluating event commonsense in dialogue systems, looking at commonsense knowledge and reasoning about events and their relations. Many prior commonsense evaluation works looked at more general or taxonomic commonsense. - Leverages CSKBs in a new way: ACCENT uses commonsense knowledge bases (CSKBs) in an innovative pipeline approach, first extracting event tuples then scoring compatibility with a dynamic CSKB. Other works like the MLP regressor baseline use CSKBs more directly through search.- New dialogue evaluation dataset collected: The authors collect the first publicly available event commonsense evaluation dataset for open-domain dialogues, with human annotations of tuples and scores. Prior commonsense evaluation datasets were not focused on dialogues.- Strong empirical results: Experiments show ACCENT achieves substantially higher correlation with human judgments compared to several baseline approaches, demonstrating its effectiveness as an automatic evaluation metric.- Limitations around coverage: The paper acknowledges limitations around ACCENT's coverage relying on a fixed set of relations and the CSKB used. Expanding to additional knowledge is noted as a worthwhile area for future improvement.- Novel joint event-relation extraction: The pipeline includes joint extraction of event tuples not addressed much in prior work. There is room to improve this component in the future.Overall, the paper presents a novel approach to evaluating an important but under-studied aspect of dialogue systems - event commonsense reasoning. The proposed ACCENT metric and collected resources help advance research in this direction.


## What future research directions do the authors suggest?

Based on my reading of the paper, some of the main future research directions suggested by the authors are:- Improving the joint event-relation extraction component of the ACCENT framework. The authors note that this is still challenging and there is room for improvement. They suggest exploring methods like synthetic data construction or transfer learning to improve this component without needing more costly human annotations.- Incorporating additional commonsense knowledge into the ACCENT framework beyond the current relations and the ATOMIC knowledge base. The authors acknowledge that ACCENT may fail to cover some aspects of event commonsense, so augmenting it with more knowledge sources could enhance coverage.- Evaluating and extending the approach to other dialogue tasks beyond open-domain dialogues, such as task-oriented dialogues. The authors focus on open-domain dialogues but the event commonsense evaluation approach could potentially generalize.- Considering other sources of commonsense knowledge beyond events and relations, such as common concepts or social norms that are relevant for dialogues. The current focus is event commonsense but other facets of commonsense could also be incorporated.- Exploring unsupervised or weakly supervised approaches to reduce the annotation cost. The authors rely on human annotations for some parts of ACCENT and note this is expensive. Moving to less supervised techniques could improve scalability.- Analyzing different neural architectures and self-supervision techniques for the tuple scoring component. The authors use COMET but other recent models could be experimented with.In summary, the main suggestions are to expand the sources of knowledge and events covered by ACCENT, improve the underlying neural components like the joint extraction model, reduce annotation dependence, and evaluate the approach on other dialogue tasks and genres.
