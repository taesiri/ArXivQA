# ACCENT: An Automatic Event Commonsense Evaluation Metric for Open-Domain   Dialogue Systems

## What is the central research question or hypothesis that this paper addresses?

This paper focuses on the problem of automatically evaluating the event commonsense reasoning of open-domain dialogue systems. The central hypothesis is that event commonsense, which considers events and their relations, is a key component of overall commonsense reasoning that is still challenging for dialogue systems. To investigate this, the authors propose ACCENT, a novel automatic evaluation metric that leverages commonsense knowledge bases (CSKBs) to assess the event commonsense of system responses without needing reference responses. The key ideas are:- Focusing evaluation on event commonsense rather than broader commonsense, as event reasoning is still difficult for current systems.- Using event-relation tuples extracted from the dialogue as an intermediate symbolic representation to bridge the gap between free-form text and structured commonsense knowledge.- Scoring the extracted tuples based on their compatibility with a CSKB to judge the event commonsense of the response.The central hypothesis is evaluated by constructing the first event commonsense evaluation dataset for open-domain dialogues and demonstrating that ACCENT better correlates with human judgments compared to several baseline metrics.In summary, the key research question is how to automatically evaluate event commonsense in dialogue systems. The paper proposes ACCENT as a way to achieve this by extracting and scoring event tuples against a commonsense KB. Experiments demonstrate this correlates better with human judgments than prior methods.


## What is the main contribution of this paper?

The main contribution of this paper is proposing ACCENT, an automatic evaluation metric for measuring the event commonsense capabilities of open-domain dialogue systems. Specifically, the key contributions are:1. ACCENT is the first work that systematically studies and evaluates event commonsense in dialogue systems. It focuses on event commonsense which is crucial for dialogues but is still challenging for current systems.2. ACCENT utilizes commonsense knowledge bases (CSKBs) to evaluate event commonsense without needing reference responses. It extracts event-relation tuples from the dialogue and measures their compatibility with the CSKB.3. The paper constructs the first public event commonsense evaluation dataset for open-domain dialogues to train and evaluate ACCENT.4. Experiments show ACCENT achieves higher correlation with human judgments compared to several baselines. The two components of ACCENT also outperform baselines on joint event-relation extraction and CSKB population tasks.In summary, the key contribution is proposing ACCENT as an automatic and effective metric to evaluate the event commonsense capabilities of dialogue systems, which is an important but under-explored problem. The new dataset and strong empirical results also validate the usefulness of ACCENT.
