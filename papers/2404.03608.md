# [Sailor: Open Language Models for South-East Asia](https://arxiv.org/abs/2404.03608)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Mainstream large language models (LLMs) like Llama-2 rely heavily on English data (89.7% of training data is English) and struggle to achieve good performance on other languages. This is known as the "curse of multilinguality".

- There is a need for LLMs tailored for South-East Asian (SEA) languages like Chinese, Vietnamese, Thai, Indonesian, Malay and Lao. High quality training data and models for these languages are lacking.

Methodology:
- The authors introduce Sailor, a family of open-source LLMs ranging from 0.5B to 7B parameters, specialized for SEA languages.

- Sailor models are continually pre-trained from Qwen1.5, an existing multilingual LLM. They are trained on 200-400B high quality tokens covering the target SEA languages.

- Various techniques are used including merging of adjacent short examples, document-level code-switching of languages, aggressive data cleaning and deduplication, BPE dropout, and optimization of hyperparams like learning rate and data mixture using small proxy models.

Contributions:
- Sailor models demonstrate strong performance on SEA language tasks like question answering, commonsense reasoning, reading comprehension and exams.

- The training pipeline and insights around continual pretraining are shared openly to accelerate LLM development for multilingual use cases.

- The work helps address the lack of high-quality models for SEA languages and provides both successful experiences and failed attempts for the community to build upon.

In summary, the paper introduces Sailor, a family of models to advance language technology for SEA languages, sharing various insights to spur wider research on multilingual LLMs.
