# [ExploRLLM: Guiding Exploration in Reinforcement Learning with Large   Language Models](https://arxiv.org/abs/2403.09583)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem: 
- Reinforcement learning (RL) struggles with low sample efficiency, slow training speed, and uncertain convergence when applied to image-based robot manipulation tasks with large observation and action spaces.  
- While large pre-trained foundation models (FMs) like large language models (LLMs) show promise for robotic manipulation, directly using them is unreliable due to limited reasoning capabilities and challenges in understanding physical contexts.

Proposed Solution - ExploRLLM:
- Proposes a novel framework that leverages the inductive bias of FMs (e.g. LLMs) to guide exploration in RL to enhance training efficiency. 
- Employs LLM to reformulate action and observation spaces to make them more compact and effective for RL.
- LLM generates hierarchical policy code (high-level and low-level) to guide exploration instead of prompting the LLM at every step which is computationally expensive.
- Vision-language model used to extract crucial objects from observations and create object-centric action space centered around detected objects.

Main Contributions:
- Demonstrates that LLM-guided exploration enables much quicker convergence for RL compared to no guided exploration.
- Shows that ExploRLLM outperforms policies based solely on LLM and VLMs by comparing against several state-of-the-art baselines.
- Validates that ExploRLLM policy trained only in simulation can be applied to real-world settings without additional training.
- Ablation studies analyze impact of varying the LLM-based exploration frequency during RL training.

In summary, ExploRLLM is a novel framework that synergistically combines RL and FMs by using LLM-guided exploration and object-centric observation/action spaces to accelerate training and enhance performance of RL for robotic manipulation tasks.
