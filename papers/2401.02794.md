# [Subjective and Objective Analysis of Indian Social Media Video Quality](https://arxiv.org/abs/2401.02794)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- There is an explosive growth in social media usage and user-generated mobile video content, especially in emerging economies like India. However, existing video quality assessment (VQA) datasets do not adequately represent the diversity of content and capture devices used to create such videos. 

- There is a need for larger and more diverse user-generated video quality assessment (UGC-VQA) datasets to enable development of better VQA models that can work well on real-world social media videos.

Contributions:

1. New UGC video quality database:
- Collected 600 mobile-originated videos from Indian social media platform ShareChat, obtained quality ratings via human study under controlled settings.

- Database captures wider variety of cultural content, capture devices compared to existing UGC-VQA datasets. Expected to help advance perceptual video quality research.

2. Benchmarking of VQA algorithms:
- Evaluated performance of state-of-the-art NR-VQA algorithms like BRISQUE, NIQE, VIDEVAL, CONTRIQUE etc. on the new database. 

- Found significant room for improvement, motivating development of better VQA models suited for emerging market UGC content.

3. New Mixture-of-Experts VQA model (MoEVA):  
- Novel model combining spatial & temporal neurostatistical features and unsupervised semantic features from specialized deep learning model.

- Significantly outperforms prior art on new database. Demonstrates value of dataset for advancing video quality prediction research.

The paper makes dataset and MoEVA model code freely available to research community to drive further progress in UGC-VQA domain.


## Summarize the paper in one sentence.

 This paper introduces a new user-generated mobile video quality dataset captured from an Indian social media platform and benchmarked against existing algorithms, while also proposing a new mixture-of-experts model that combines spatial, temporal, and semantic features to assess video quality.


## What is the main contribution of this paper?

 Based on my reading of the paper, the main contributions are:

1) Releasing the LIVE-ShareChat IUGC-VQA psychometric video quality database containing 600 mobile-originated user generated videos from the Indian social media platform ShareChat, labeled with human perceptual quality judgments. This new dataset provides more cultural, visual, and language diversity to the video quality research community.

2) Conducting an extensive study and performance benchmarking of existing video quality prediction algorithms on the new LIVE-ShareChat dataset, demonstrating its value while providing insights into the capabilities of current models.

3) Introducing a new mixture-of-experts based video quality assessment model called MoEVA that combines spatial and temporal statistical video features with content and quality aware features from a specialized pre-trained backbone. MoEVA is shown to significantly outperform prior art models on the new ShareChat dataset.

In summary, the key contribution is a new diverse video quality dataset along with a novel video quality prediction model that leverages both handcrafted features and learned representations to advance the state-of-the-art in user generated content video quality assessment.


## What are the keywords or key terms associated with this paper?

 Based on my review of the paper, some of the key keywords and terms associated with this paper include:

- User-Generated Content (UGC) 
- User-Generated Mobile Video Quality 
- No-Reference Video Quality Assessment (NR-VQA)
- Blind Video Quality Assessment (BVQA) 
- Mixture-of-Experts Model
- Contrastive Learning
- Natural Video Statistics (NVS)
- Natural Scene Statistics (NSS)
- Video Quality Databases (KoNViD-1k, LIVE-VQC, YouTube-UGC)
- LIVE-ShareChat IUGC-VQA Database

The paper introduces a new video quality database called the LIVE-ShareChat IUGC-VQA Database containing user-generated mobile videos from the Indian social media platform ShareChat. It benchmarks existing NR-VQA models on this new database and also proposes a new NR-VQA model called MoEVA which combines distortion-aware features and semantic features learned using contrastive learning. The key focus is on assessing the perceptual quality of user-generated Indian social media videos.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in this paper:

1. The paper mentions developing a novel contrastive learning strategy that better accounts for content while still learning to distinguish between distortions. Can you explain in more detail how you achieve this? What modifications were made to the traditional contrastive learning approach?

2. You mention using a specialized pre-trained backbone in MoEVA to generate quality-aware features. Can you elaborate on how this backbone was trained? What datasets were used and what was the training strategy and loss function? 

3. The mixture of experts model combines multiple components - pre-trained features, spatial NSS features, and temporal NSS features. What motivated this design choice? Why is it beneficial to combine these different types of features instead of just relying on one?

4. How exactly are the spatial and temporal NSS features calculated and aggregated in MoEVA? What statistics or transformations are applied to the frames/videos? 

5. The performance results show that MoEVA does not perform as well on the YouTube-UGC dataset compared to the LIVE datasets. What reasons could account for this performance difference? 

6. Have you experimented with different pooling strategies for aggregating the frame-level features from the pre-trained backbone? Simple average pooling is used but were other temporal pooling methods like LSTMs or GRUs evaluated?

7. What hyperparameter tuning was conducted for components like the SVR model and the loss functions? Was the model architecture optimized or mostly default settings used?

8. How was the training data for the pre-trained backbone generated? You mention using frames from the other ShareChat videos, but what augmentation and preprocessing was applied to these frames?

9. For the synthetic distortions applied during training data augmentation, what specific types of distortions were included and how were the severity levels determined? 

10. The contrastive learning component operates on image patches instead of full frames. What determined the patch size used? Was any analysis done on the impact of patch size?
