# [Unraveling Babel: Exploring Multilingual Activation Patterns within   Large Language Models](https://arxiv.org/abs/2402.16367)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem Statement:
- Large language models (LLMs) have shown tremendous progress in language processing, but how they process multiple languages within a single model structure is not well understood. The goal of this work is to analyze the multilingual activation patterns of LLMs to understand their mechanisms for handling different languages.

Methodology:
- The authors transform a popular 7B parameter LLM (Llama 2) into a Mixture of Experts (MoE) architecture without changing the original model parameters. This allows them to decompose the feedforward network into multiple "experts" and analyze their activation patterns.

- They analyze expert activation frequencies across 32 layers when processing text inputs in 9 different languages spanning multiple language families. Activation frequencies quantify how often experts get highly activated for inputs in each language.

Key Findings:
- LLM neurons exhibit distinct activation patterns for different languages that align with language families. Related languages share more similarities.  

- Certain "multilingual co-activation" experts get frequently activated regardless of language, likely encoding non language-specific knowledge. Others are activated in a language-specific way and handle language-dependent processing.

- Inference can be accelerated by 30% by pruning low-activation experts per language while maintaining comparable performance, demonstrating the potential utility of this analysis.

Main Contributions:  
- First fine-grained study of multilingual activation patterns inside state-of-the-art LLM neural networks, shedding light on their multilingual processing mechanisms.

- Valuable insights on language-specific vs. agnostic components that could guide improved multilingual model training and pruning strategies.
