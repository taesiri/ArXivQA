# [Expand BERT Representation with Visual Information via Grounded Language   Learning with Multimodal Partial Alignment](https://arxiv.org/abs/2312.01592)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a detailed summary of the key points from the paper:

This paper proposes GroundedBERT, a grounded language learning model that incorporates visual information into the BERT language representation. The key idea is to equip the contextual word embeddings from BERT with additional visually-grounded embeddings that capture semantics from paired images. 

Specifically, GroundedBERT has two components - the original BERT that provides contextual text embeddings, and a visual grounding module that outputs grounded embeddings using image features from a Vision Transformer. These two representations are concatenated to create a unified visual-textual embedding for each token. 

The model alignments between modalities using Partial Optimal Transport, which provides more flexibility than classical OT to find matches between local visual elements like image patches and word tokens.

The grounded representations from GroundedBERT significantly outperform baseline BERT and other vision-language models like VisualBERT on a range of NLP tasks including GLUE and SQuAD question answering. This demonstrates that grounding with visual data can enhance language understanding without compromising the linguistic knowledge captured by BERT.

In summary, this work effectively incorporates visual grounding into language models through a secondary grounding module and improved cross-modal alignment. The resulting GroundedBERT representations unite contextual text semantics from BERT with additional visual semantics, leading to gains in NLP performance.


## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Traditional language models like BERT are pre-trained only on textual corpora, lacking grounding in visual information. 
- Attempts to improve language models with visual grounding suffer from mismatch between distribution/scale of visual datasets vs language corpora. Tokens occurring in visual data get mixed up during representation learning with tokens without visual grounding. 
- Global image vectors used for grounding lose local visual information. Hard to align global vector with only locally relevant captions.

Proposed Solution:
- Propose GroundedBERT to enhance BERT representations with visually grounded information
- Has two components: 
  1) Original BERT capturing contextual linguistic representations
  2) Visual Grounding module capturing visual information  
- Uses Vision Transformer (ViT) instead of CNNs to preserve local information in image patch embeddings
- Employs Partial Optimal Transport to align textual tokens with relevant image patches, allowing partial alignment.

Contributions:
- Novel visually-grounded BERT representation combining linguistic and visual representations
- Maintains local visual information with ViT patch embeddings  
- Uses Partial Optimal Transport for improved textual-visual alignment

Results:
- Significantly outperforms baseline BERT and other visual-linguistic models like VisualBERT on GLUE and SQuAD language tasks
- Analysis shows visual grounding provides significant gains, and Partial Optimal Transport aligns better than global Optimal Transport
