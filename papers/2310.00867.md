# [(Dynamic) Prompting might be all you need to repair Compressed LLMs](https://arxiv.org/abs/2310.00867)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading, the key research questions and hypotheses that this paper seems to address are:

1) Can prompting effectively recover the performance of compressed large language models (LLMs) beyond just simple cases? The paper notes that prior work has shown promise for prompting to recover performance after LLM compression, but has been limited primarily to perplexity evaluations on simple datasets. This paper aims to examine if prompting can scale and generalize to more complex downstream tasks.

2) Why can prompting recover performance after LLM compression? The paper lays out two main hypotheses:

- Null hypothesis (H0): Compression truly destroys model knowledge and prompting recovers it by re-learning that knowledge from scratch, similar to re-training. 

- Alternative hypothesis (H1): Compression does not permanently erase knowledge but rather just displaces it within the model. Prompting works by redirecting the model's inference path to tap into that existing displaced knowledge, rather than re-learning the knowledge.

The paper seems designed to provide evidence to evaluate these two hypotheses about why prompting is effective, rather than just showing that it works.

In summary, the key questions seem to be: (1) can prompting scale and generalize as a technique to recover compressed LLM performance, and (2) does it work by re-learning erased knowledge or redirecting to displaced knowledge already within the model? The paper aims to provide affirmative evidence for prompting's effectiveness on complex tasks, while also arguing in favor of the "redirection" hypothesis over the "re-learning" hypothesis.


## What is the main contribution of this paper?

 Based on my reading of the paper, the main contributions appear to be:

1. Identifying limitations of existing prompt tuning methods for compressed large language models (LLMs), specifically the over-reliance on using a single prompt per input. The paper shows this does not scale well to more complex tasks.

2. Proposing a new method called "inference-time dynamic prompting" (IDP) that allows selecting prompts dynamically for each input at inference time. This improves performance without requiring additional model parameters.

3. Providing analysis and experiments that suggest prompting is able to recover performance of compressed LLMs by redirecting internal knowledge rather than learning from scratch. This is in contrast to re-training methods.

4. Demonstrating strong performance of the proposed IDP method on a set of 9 diverse and challenging tasks using real-world metrics. IDP outperforms baselines including uncompressed models, compressed models, naive prompting, and other parameter-efficient tuning methods like LoRA.

5. Analyzing why prompting is effective for compressed LLMs through controlled experiments on attention patterns and prompt length. The results support the hypothesis that prompting redirects existing knowledge in the model rather than injecting new knowledge.

In summary, the main contribution appears to be introducing and demonstrating a new prompting method called IDP that can efficiently recover performance of compressed LLMs by leveraging existing knowledge, without the need for expensive re-training. The effectiveness of IDP is shown across diverse tasks and using both quantitative metrics and analysis of model internal states.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 The paper explores using prompt tuning to recover the performance of compressed large language models without expensive retraining. The key ideas are proposing inference-time dynamic prompting to select prompts tailored to inputs, and providing evidence that prompting works by redirecting the model's attention to recover displaced inherent knowledge rather than relearning from scratch.
