# [GKD: Generalized Knowledge Distillation for Auto-regressive Sequence   Models](https://arxiv.org/abs/2306.13649)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be:How can we improve knowledge distillation for auto-regressive sequence models like large language models (LLMs)? Specifically, how can we address two key issues:1) Distribution mismatch between sequences seen during training vs sequences generated at test time (i.e. train/test distribution drift).2) Model under-specification, where the student model may not have enough capacity to fully capture the teacher's distribution.To address these issues, the authors propose a method called Generalized Knowledge Distillation (GKD) that has two main components:1) Using an on-policy approach where some training sequences are sampled from the student rather than just using a fixed dataset. This helps reduce distribution mismatch.2) Optimizing for divergences like reverse KL and generalized JS divergence that focus on generating student samples likely under the teacher, rather than just maximizing likelihood. This handles model under-specification better. The central hypothesis seems to be that GKD will outperform standard knowledge distillation methods for compressing large auto-regressive models like LLMs by addressing these two key challenges. The paper evaluates this hypothesis through experiments on summarization, translation, and math reasoning tasks.
