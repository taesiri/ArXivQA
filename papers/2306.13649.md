# [GKD: Generalized Knowledge Distillation for Auto-regressive Sequence   Models](https://arxiv.org/abs/2306.13649)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be:How can we improve knowledge distillation for auto-regressive sequence models like large language models (LLMs)? Specifically, how can we address two key issues:1) Distribution mismatch between sequences seen during training vs sequences generated at test time (i.e. train/test distribution drift).2) Model under-specification, where the student model may not have enough capacity to fully capture the teacher's distribution.To address these issues, the authors propose a method called Generalized Knowledge Distillation (GKD) that has two main components:1) Using an on-policy approach where some training sequences are sampled from the student rather than just using a fixed dataset. This helps reduce distribution mismatch.2) Optimizing for divergences like reverse KL and generalized JS divergence that focus on generating student samples likely under the teacher, rather than just maximizing likelihood. This handles model under-specification better. The central hypothesis seems to be that GKD will outperform standard knowledge distillation methods for compressing large auto-regressive models like LLMs by addressing these two key challenges. The paper evaluates this hypothesis through experiments on summarization, translation, and math reasoning tasks.


## What is the main contribution of this paper?

The main contribution of this paper is proposing Generalized Knowledge Distillation (GKD), a method for distilling knowledge from a large auto-regressive teacher model into a smaller student model. The key ideas are:- Formulating distillation as an imitation learning problem, which allows using on-policy data from the student model during training to reduce distribution mismatch between training and deployment. - Using alternative divergences like reverse KL and generalized Jensen-Shannon divergence to handle model under-specification, where the student may not have enough capacity to perfectly fit the teacher.- Unifying supervised and on-policy distillation into a generalized framework with a mixture of teacher-generated and student-generated sequences, and flexible choice of divergence.- Demonstrating that GKD outperforms common distillation methods like supervised KD, on-policy KD, and ImitKD on summarization, translation, and reasoning tasks.- Showing GKD can also be combined with RL fine-tuning to directly optimize a reward while retaining performance of the teacher model.In summary, the key contribution is a principled and general distillation method for auto-regressive models that mitigates issues like distribution mismatch and model under-specification. The experiments demonstrate consistent improvements over prior distillation techniques on a diverse set of language tasks.
