# [RepoFusion: Training Code Models to Understand Your Repository](https://arxiv.org/abs/2306.10998)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is: Can training code language models to incorporate and combine multiple relevant contexts from a code repository improve their performance on code completion tasks? The key hypothesis is that by training models like RepoFusion to leverage and fuse together multiple relevant code snippets from across a repository, they can generate more accurate and context-aware completions compared to models trained only on individual code files. The paper aims to demonstrate that smaller models trained in this way with repository context can outperform much larger language models trained on code without this contextual information.In summary, the main research question is whether training code models to understand and utilize repository-level context can enhance their capabilities, especially for code completion. The hypothesis is that models trained with the proposed RepoFusion framework will be better at code completion compared to models without access to repository information during training. The experiments aim to validate if training smaller models in a retrieval-augmented way with repository context can surpass bigger models trained on individual files.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contributions appear to be:1. Proposing RepoFusion, a framework to train code models to incorporate relevant repository context in order to generate more accurate and context-aware code completions. 2. Showing through experiments that RepoFusion, a 220M parameter model, outperforms much larger models like CodeGen-16B and closely matches the performance of the significantly larger StarCoderBase model on the task of single-line code completion.3. Conducting extensive ablation studies to provide insights into key factors influencing RepoFusion's performance, such as the nature and ordering of repository contexts, their lengths and quantities, and training configurations. 4. Creating and releasing Stack-Repo, a new dataset of 200 Java repositories augmented with repository contexts to support research on repository-aware code models.5. Releasing code and trained models to support further research on training smaller retrieval-augmented language models for code.In summary, the main contribution appears to be proposing the RepoFusion framework and methodology for training code models with repository context, and showing its effectiveness compared to much larger models through empirical evaluation and release of models/datasets. The paper provides both the theoretical framework and practical evidence of the value of training code models to leverage repository information.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Based on my reading of the paper, here is a one sentence summary:The paper proposes RepoFusion, a framework to train code language models to effectively incorporate multiple relevant contexts from a code repository in order to generate more accurate and context-aware code completions.
