# [RepoFusion: Training Code Models to Understand Your Repository](https://arxiv.org/abs/2306.10998)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is: Can training code language models to incorporate and combine multiple relevant contexts from a code repository improve their performance on code completion tasks? The key hypothesis is that by training models like RepoFusion to leverage and fuse together multiple relevant code snippets from across a repository, they can generate more accurate and context-aware completions compared to models trained only on individual code files. The paper aims to demonstrate that smaller models trained in this way with repository context can outperform much larger language models trained on code without this contextual information.In summary, the main research question is whether training code models to understand and utilize repository-level context can enhance their capabilities, especially for code completion. The hypothesis is that models trained with the proposed RepoFusion framework will be better at code completion compared to models without access to repository information during training. The experiments aim to validate if training smaller models in a retrieval-augmented way with repository context can surpass bigger models trained on individual files.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contributions appear to be:1. Proposing RepoFusion, a framework to train code models to incorporate relevant repository context in order to generate more accurate and context-aware code completions. 2. Showing through experiments that RepoFusion, a 220M parameter model, outperforms much larger models like CodeGen-16B and closely matches the performance of the significantly larger StarCoderBase model on the task of single-line code completion.3. Conducting extensive ablation studies to provide insights into key factors influencing RepoFusion's performance, such as the nature and ordering of repository contexts, their lengths and quantities, and training configurations. 4. Creating and releasing Stack-Repo, a new dataset of 200 Java repositories augmented with repository contexts to support research on repository-aware code models.5. Releasing code and trained models to support further research on training smaller retrieval-augmented language models for code.In summary, the main contribution appears to be proposing the RepoFusion framework and methodology for training code models with repository context, and showing its effectiveness compared to much larger models through empirical evaluation and release of models/datasets. The paper provides both the theoretical framework and practical evidence of the value of training code models to leverage repository information.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Based on my reading of the paper, here is a one sentence summary:The paper proposes RepoFusion, a framework to train code language models to effectively incorporate multiple relevant contexts from a code repository in order to generate more accurate and context-aware code completions.


## How does this paper compare to other research in the same field?

Here is a summary of how this paper relates to previous work in the field:The paper proposes a framework called RepoFusion for training code language models to leverage multiple relevant contexts from a code repository to generate more accurate completions. This builds on prior work showing the benefits of using repository-level context during inference with large language models (LLMs) like Codex, such as the RLPG method from Shrivastava et al. (2022) and RepoCoder. However, a key limitation identified from that prior work is that without specialized training, it is difficult for LLMs to effectively integrate multiple repository contexts. RepoFusion addresses this by using a Fusion-in-Decoder architecture to train models to combine multiple encoded repo contexts into the decoding. In terms of using information beyond the current file, this also relates to previous techniques like the nested n-gram cache model from Hindle et al. (2012) and incorporation of API dependency graphs from Allamanis et al. (2017). The repo contexts in RepoFusion are retrieved using prompt proposals similar to retrieval mechanisms in other recent work augmenting LLMs.The main novelty of RepoFusion is showing that directly training smaller models with repository context can outperform much larger pre-trained LLMs, like CodeGen-16B. This demonstrates the value of learning to leverage repository structure, which could be useful for other code generation tasks beyond completion.In summary, RepoFusion builds directly on prior findings about repository contexts, addresses limitations around integrating multiple contexts, and shows strong results surpassing models orders of magnitude larger in size. The code completion application relates to a large body of code generation work, but use of repository structure during training appears novel.


## What future research directions do the authors suggest?

Based on my reading of the paper, some of the key future research directions suggested by the authors are:- Extending RepoFusion to other code-related tasks beyond single-line code completion, such as bug repair, pull request resolution, software documentation and tutorial writing. The authors believe the RepoFusion framework could be beneficial for these tasks as well by incorporating relevant repository context.- Exploring optimizations like FiDO to improve the inference speed and scalability of RepoFusion. The linear scaling of computation with respect to the number of repository contexts is a limitation, so techniques to enable faster inference could help address this issue.- Evaluating the impact of training smaller retrieval-augmented language models for code using the RepoFusion framework and Stack-Repo dataset. The authors propose their work opens up opportunities in this direction of smaller yet performant LLMs by effectively leveraging repository context.- Investigating the use of other retrieval mechanisms beyond prompt proposals, BM25 and random nearest neighbors for generating the repository contexts in RepoFusion. Since it is agnostic to the specific retrieval method, integrating contexts from different mechanisms could be an interesting direction.- Applying RepoFusion to few-shot learning scenarios for code where very limited data is available. The repository context could potentially help improve generalization and performance in low-data regimes.- Considering multi-modal repository context, such as API documentation or UML diagrams along with source code, instead of just source code snippets. This could provide additional semantic information to aid code understanding.In summary, the key suggested future work revolves around extending RepoFusion to other tasks, improving inference efficiency, applying it to few-shot learning, evaluating on other retrieval mechanisms and modalities, and training smaller but effective retrieval augmented LLMs with the help of repository context.


## Summarize the paper in one paragraph.

The paper proposes RepoFusion, a framework to train code language models to make better predictions by incorporating relevant context from the repository. It focuses on the task of single-line code completion. The key idea is to leverage multiple relevant code snippets (repo contexts) from files across the repository using the Fusion-in-Decoder architecture. Each repo context is concatenated with the surrounding context from the target line and encoded separately. The decoder attends to the concatenated encoded representations to predict the target missing code (hole). Through experiments on a new dataset Stack-Repo, the paper shows that RepoFusion with 220M parameters significantly outperforms larger models like CodeGen-16B and closely matches the performance of the much bigger StarCoderBase model. Ablation studies provide insights into factors like the nature and number of repo contexts, context lengths, ordering strategies etc. One key finding is that leveraging diverse contexts is crucial for good performance. The paper proposes Stack-Repo, a new dataset of 200 Java repositories augmented with repo contexts. The code, models and dataset are publicly released.


## Summarize the paper in two paragraphs.

Here is a two paragraph summary of the paper:The paper proposes a framework called RepoFusion for training code models to leverage relevant context from the repository to generate more accurate code completions. The key idea is to train the model using the Fusion-in-Decoder (FiD) approach to combine multiple relevant code snippets, called repository contexts (RCs), from the repository. Specifically, the surrounding context around the target hole is appended to each RC and encoded separately. The decoder then attends jointly to the concatenated encoded representations to predict the target hole. Through experiments on single-line Java code completion, it is shown that RepoFusion significantly outperforms larger pretrained models like CodeGen-16B. It also closely matches the performance of StarCoderBase despite being 70x smaller. Ablation studies provide insights such as: using context from prompt proposals is most effective, the NT-Prior-Last strategy performs the best, longer RCs are better, and utilizing RCs from diverse sources is key to performance. Overall, the work demonstrates the effectiveness of training smaller models to leverage repository context over simply scaling up models.


## Summarize the main method used in the paper in one paragraph.

The paper proposes a framework called RepoFusion for training code language models to leverage repository-level context for more accurate code completion. The key idea is to train the model using the Fusion-in-Decoder (FiD) approach to combine multiple relevant code snippets, or repository contexts, from across the repository. Specifically, for a given target hole (missing code) that needs to be completed, the surrounding context (code around the hole) is appended to multiple retrieved repository contexts and encoded separately. The model is trained to attend over the concatenated encoded representations of these contexts and generate the target hole. Through experiments on single line code completion, the paper shows that RepoFusion substantially outperforms larger code models like CodeGen-16B. It also closely matches the performance of the much larger StarCoder model, demonstrating the effectiveness of training with repository context. Extensive ablation studies provide insights into factors like the optimal context length, number of contexts, context retrieval methods etc. Overall, the main contribution is a novel framework to train smaller yet highly performant code models by learning to leverage diverse relevant contexts from the repository.
