# [AGG-Net: Attention Guided Gated-convolutional Network for Depth Image   Completion](https://arxiv.org/abs/2309.01624)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is how to improve depth image completion, particularly for images with large missing or invalid areas, by using both depth and color information in a convolutional neural network framework. The key points are:- Depth images from RGB-D cameras often contain invalid or missing data due to various factors. This is a problem for applications that rely on complete depth data.- Most prior work uses only the raw depth images for completion. The authors propose using both depth and corresponding color images as input to a convolutional neural network.- They introduce two new modules - Attention Guided Gated Convolution (AG-GConv) and Attention Guided Skip Connection (AG-SC) - to help fuse depth and color information effectively.- AG-GConv uses contextual attention learned from both modalities to guide depth feature extraction, helping to fill large holes. - AG-SC selectively highlights useful color features and suppresses irrelevant ones for depth reconstruction.- Experiments on three datasets demonstrate state-of-the-art performance, showing the benefits of the proposed approach, especially for images with large irregular holes.In summary, the central hypothesis is that leveraging color information and using the proposed AG-GConv and AG-SC modules will improve depth completion compared to methods that use only depth input. The results support this hypothesis.


## What is the main contribution of this paper?

The main contributions of this paper are:- It proposes a new framework called AGG-Net (Attention Guided Gated-convolutional Network) for depth image completion. - It introduces two new modules:    - AG-GConv (Attention Guided Gated Convolution) module to fuse depth and color features under the guidance of contextual attention. This helps handle large missing areas in the depth images.    - AG-SC (Attention Guided Skip Connection) module to filter out irrelevant color features and reduce their interference in depth reconstruction.- The model employs a dual-branch encoder-decoder architecture to combine depth and color information in a multi-scale manner.- It outperforms state-of-the-art methods on benchmark datasets NYU-Depth V2, DIML, and SUN RGB-D for depth completion.In summary, the key contribution is the proposed AGG-Net framework and the new AG-GConv and AG-SC modules for effectively fusing and filtering depth and color features to achieve high quality depth completion. The results demonstrate improved performance over existing methods.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper proposes a new deep learning model called AGG-Net for depth image completion, which uses attention-guided gated convolutions and skip connections to effectively fuse color and depth features at multiple scales for reconstructing high-quality depth maps from incomplete raw depth images.


## How does this paper compare to other research in the same field?

Here are a few key points comparing this paper to other research in depth image completion:- This paper proposes a new deep learning architecture called AGG-Net (Attention Guided Gated-convolutional Network) for depth image completion. It builds on previous work using encoder-decoder networks like CSPN and DeepLidar, but makes modifications to better handle invalid/missing depth values.- A key contribution is the proposed AG-GConv module, which uses attention to guide the gating of depth features based on both depth and color context. This aims to alleviate issues from invalid depth values polluting features. - Another contribution is the AG-SC module for refining color features in the decoder via attention, to reduce interference from depth-irrelevant color features.- Experiments show state-of-the-art performance on popular benchmarks like NYU Depth v2, DIML, and SUN RGB-D. For example, AGG-Net reduces error by 33.8% compared to prior work RDF-GAN on NYU Depth.- The attention mechanisms allow AGG-Net to better handle challenging cases like large/irregular holes and dense speckles. This demonstrates more robust adaptation to diverse missing data patterns.- The paper provides ablation studies analyzing the contribution of different components. This helps validate the importance of the proposed AG-GConv and AG-SC modules.Overall, this paper makes nice incremental improvements over prior depth completion networks by using attention to refine feature fusion and propagation. The experiments demonstrate state-of-the-art results and robustness on challenging data. The ablation studies help confirm the value of the proposed attention mechanisms.
