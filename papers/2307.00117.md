# [Goal Representations for Instruction Following: A Semi-Supervised   Language Interface to Control](https://arxiv.org/abs/2307.00117)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central hypothesis of this paper is that aligning the representations of visual goals and language instructions can enable robots to leverage large amounts of unlabeled trajectory data to improve language-conditioned manipulation skills. Specifically, the paper proposes that explicitly aligning visual and language representations of tasks, rather than just training them jointly, will force the visual representations to focus on task-relevant semantics. This in turn will allow for better transfer from unlabeled goal-reaching data to the language-conditioned setting.The paper tests this hypothesis through a two-stage training process. First, visual and language encoders are trained with a contrastive objective to align their representations of tasks/transitions. Then a shared policy network is trained on top of these aligned representations, using both labeled language data and unlabeled goal data. The main experiments aim to validate whether:1) The proposed explicit alignment enables better use of unlabeled data compared to implicit alignment from joint training.2) Leveraging pre-trained vision-language models through this alignment approach improves task representations. 3) Aligning transitions rather than static goals to language leads to better utilization of vision-language models.In summary, the central hypothesis is that explicit alignment of visual and language task representations enables robots to learn language-conditioned policies that generalize well by exploiting abundant unlabeled goal data. The experiments aim to validate the components of the proposed approach.


## What is the main contribution of this paper?

The main contribution of this paper is a method for training robots to follow natural language instructions by learning aligned representations between language instructions and visual goals. The key ideas are:- Leveraging both a small labeled dataset with language annotations and a larger unlabeled dataset of robot interactions to improve language grounding and generalization. - Learning explicit alignments between language instructions and visual goal representations, rather than just implicitly aligning them through multi-task training. This is done through a contrastive loss.- Modifying vision-language models like CLIP to align language to visual scene changes rather than static images. This better captures the semantics of instructions.- Showing that incorporating pre-trained vision-language knowledge through models like CLIP improves task grounding and manipulation capability, even though these models were not trained on robot data.- Demonstrating their method on a real-world robot manipulation dataset with varying scenes, enabling the robot to follow new natural language instructions not seen during training.The main advantage of this approach is reducing the amount of language supervision needed for robot instruction following by aligning language and vision spaces. This enables leveraging abundant unlabeled robot interaction data. The explicit alignment and scene change modeling also improves generalization.
