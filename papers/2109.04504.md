# [Bootstrapped Meta-Learning](https://arxiv.org/abs/2109.04504)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is how to improve meta-learning algorithms by addressing issues of optimization challenges and short-term biases. Specifically, the paper proposes a new meta-learning algorithm called Bootstrapped Meta-Gradients (BMG) that aims to:- Control the curvature/conditioning of the meta-optimization problem by using a distance/divergence function (e.g. KL divergence) to match the learner with a bootstrapped target. This helps with optimization challenges. - Extend the effective meta-learning horizon beyond just evaluating performance after K update steps. This helps address short-term biases. The key idea is to bootstrap targets by unrolling the learner for additional steps rather than backpropagating through the additional steps.So in summary, the central hypothesis is that using bootstrapped targets and matching functions within a meta-learning framework can help address optimization challenges and short-term biases in meta-learning. The BMG algorithm is proposed to test this hypothesis.


## What is the main contribution of this paper?

This paper introduces Bootstrapped Meta-Learning, a new algorithm for meta-learning that tackles challenges in meta-optimization. The key ideas are:- Bootstrapping targets from the meta-learner itself to get information about future learning dynamics without requiring backpropagation through additional update steps. This helps mitigate short-horizon bias. - Using a distance metric between the bootstrapped target and the meta-learner output to control the geometry of the meta-optimization problem. This helps improve conditioning of the problem.Together, these ideas allow the meta-learner to teach itself more effectively. The bootstrapping mechanism extends the effective horizon while the matching function controls curvature. The paper shows theoretically that bootstrapped meta-gradients can guarantee improved performance over standard meta-gradients. Empirically, the algorithm achieves state-of-the-art results on Atari and improves efficiency in few-shot learning. It also enables new forms of meta-learning, like learning an epsilon-greedy exploration schedule.In summary, the main contribution is presenting bootstrapped meta-learning as a way to tackle challenges in meta-optimization through self-generated targets and controlled matching objectives. This improves performance and enables new applications of meta-learning.
