# [Bayesian Federated Learning Via Expectation Maximization and Turbo Deep   Approximate Message Passing](https://arxiv.org/abs/2402.07366)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Standard federated learning (FL) algorithms use stochastic gradient descent (SGD) for local model training, which has drawbacks like slow convergence, vanishing/exploding gradients, and getting stuck in suboptimal solutions. 
- Large deep neural network (DNN) models also need compression for efficient inference, but regularization-based pruning methods cannot precisely control the compression ratio.
- Traditional DNNs also tend to be overconfident in predictions, which is problematic for applications like autonomous driving.

Proposed Solution:
- Propose a Bayesian federated learning (BFL) framework to formulate DNN learning as Bayesian inference and structured compression as group sparse priors.
- Use expectation maximization (EM) to update hyperparameters and accelerate convergence - E-step computes posterior distributions, M-step updates hyperparameters.
- Propose turbo deep approximate message passing (TDAMP) algorithm for efficient Bayesian learning at clients:
   - Module B: sum-product message passing for group sparse prior
   - Module A: deep approximate message passing over DNN
   - PasP: automatic tuning of local priors
- Central server aggregates local posteriors to update global posterior and hyperparameters.

Main Contributions:
- Bayesian FL framework with group sparse prior for structured compression
- TDAMP algorithm combining EM, approximate message passing, group sparsity
- Specific algorithm design for regression (housing prediction) and classification (digit recognition)
- Simulation results showing faster convergence and better performance compared to Adam-based methods

The key novelty is formulating FL as Bayesian inference to avoid drawbacks of SGD-based methods, and using message passing to exploit structured sparsity for compression. The proposed EM-TDAMP method achieves superior performance especially for high compression ratios.
