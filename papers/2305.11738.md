# CRITIC: Large Language Models Can Self-Correct with Tool-Interactive   Critiquing

## What is the central research question or hypothesis that this paper addresses?

Based on my understanding, the key research questions and hypotheses addressed in this paper are:1. Can large language models (LLMs) self-correct their outputs in a manner analogous to human interaction with tools, without requiring further training? The paper hypothesizes that LLMs can leverage external tools through natural interaction to validate and iteratively improve their own outputs, similar to how humans use tools like search engines for fact checking.2. Is external feedback through tool interaction crucial for LLMs to reliably self-verify and self-improve?The paper hypothesizes that relying solely on LLMs' internal knowledge without external feedback would be inadequate for consistent self-verification and correction. Interaction with tools provides vital external signals.3. How effective is the proposed CRITIC framework in enhancing LLM performance across diverse tasks compared to prior methods?The paper hypothesizes that by supporting interpretable verification and iterative correction using appropriate tools, the CRITIC framework can consistently improve over baselines across tasks like QA, program synthesis, and toxicity reduction.In summary, the core research questions focus on whether LLMs can self-correct by interacting with tools without further training, the importance of external feedback, and evaluating the effectiveness of the proposed CRITIC framework. The experiments aim to test these hypotheses across different tasks and models.


## What is the main contribution of this paper?

Based on my reading of the paper abstract and introduction, the main contribution appears to be proposing a new framework called CRITIC (Self-Cor\textbf{r}ecting w\textbf{i}th \textbf{T}ool-\textbf{I}nteractive \textbf{C}ritiquing) that enables large language models (LLMs) to verify and iteratively improve their own outputs by interacting with external tools. Specifically, the key aspects of the CRITIC framework seem to be:- It allows "black box" LLMs to validate and correct their outputs in a human-like manner by utilizing external tools like search engines and code interpreters. - It employs a "verify-then-correct" process where the LLM first interacts with a tool to verify the desired attributes of its initial output, receives critiques from this verification, and then generates an improved output based on the feedback.- This verification-correction cycle can be repeated iteratively to enable continuous output enhancement.- It aims to enhance LLM performance without requiring additional training data or corpora, relying only on few-shot in-context learning.- Comprehensive experiments demonstrate that CRITIC consistently improves performance of LLMs like ChatGPT across diverse tasks like question answering, program synthesis, and toxicity reduction.In summary, the main contribution appears to be proposing the novel CRITIC framework to empower LLMs to self-verify and self-improve their outputs by interfacing with the external world in a human-like manner. The key insight is that external feedback is crucial for robust and reliable self-correction.
