# [DeepMapping2: Self-Supervised Large-Scale LiDAR Map Optimization](https://arxiv.org/abs/2212.06331)

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper addresses is how to improve the accuracy of large-scale LiDAR mapping using self-supervised deep learning. Specifically, the paper proposes a method called DeepMapping2 to optimize large-scale LiDAR maps. The key hypothesis is that by carefully organizing training batches based on spatial topology and introducing a novel local-to-global consistency loss, the proposed method can effectively incorporate loop closures and local registration information to improve optimization and achieve higher accuracy in large-scale mapping.The paper analyzes limitations of the previous DeepMapping method, which fails to scale up due to lack of loop closures and slow convergence of the global registration network. DeepMapping2 proposes two main techniques to address these challenges:1. Organizing training batches based on spatial topology and map loops obtained from off-the-shelf place recognition algorithms. This allows implicit incorporation of loop closures into the training process.2. Introducing a self-supervised local-to-global consistency loss that leverages pairwise registration results. This provides stronger training signal and gradient for the global registration network to converge faster.The central hypothesis is that by carefully designing the training procedure with these two techniques, the proposed DeepMapping2 method can effectively optimize LiDAR maps in large-scale environments where previous methods fail. Experiments on public datasets like KITTI, NCLT and Nebula validate the effectiveness of DeepMapping2.In summary, the key research question is how to scale up self-supervised LiDAR map optimization to large environments, and the central hypothesis is that the proposed batch organization and consistency loss techniques can achieve this effectively. The experiments aim to validate the proposed DeepMapping2 method can improve accuracy in large-scale mapping tasks.


## What is the main contribution of this paper?

The main contributions of this paper are:1. Proposing DeepMapping2, a self-supervised method for large-scale LiDAR map optimization. This extends the prior DeepMapping method to work well on large datasets by addressing its limitations.2. Introducing two key novel techniques: - Batch organization based on map topology from loop closing. This implicitly incorporates loop closure into the training process to reduce drift.- Self-supervised local-to-global point consistency loss that leverages pairwise registration. This provides stronger training signal without relying on sparse point correspondences between frames.3. Comprehensive experiments and ablation studies on public datasets like KITTI, NCLT, and Nebula that demonstrate the effectiveness of DeepMapping2 and the contribution of each proposed technique.4. Analysis that reveals the limitations of DeepMapping on large datasets, which leads to the proposed techniques to address them. This provides useful insights on how to incorporate loop closing and local registration into the DeepMapping framework.5. A new training-as-optimization paradigm for LiDAR mapping that achieves state-of-the-art performance. The method is self-supervised and generic - it can improve mapping from different front-ends like ICP, KISS-ICP and LeGO-LOAM.In summary, the main contribution is proposing a novel self-supervised method and techniques to enable DeepMapping to work on large-scale LiDAR mapping problems, which was not possible effectively before. The experiments, analysis and ablation studies validate the design and effectiveness of DeepMapping2.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper proposes DeepMapping2, a self-supervised method to optimize large-scale LiDAR maps by adding batch organization based on map topology and a novel local-to-global point consistency loss to DeepMapping.


## How does this paper compare to other research in the same field?

Here are some key things I noticed about how this paper compares to other work in self-supervised LiDAR map optimization:- The main contribution is extending DeepMapping to work on large-scale datasets. Previous work with DeepMapping showed good results on small datasets, but had issues scaling up due to lack of loop closure and slow convergence. This paper addresses those issues.- Most prior work in LiDAR mapping uses traditional SLAM methods involving feature matching and optimization. DeepMapping pioneered a self-supervised learning approach, converting mapping to network training. This paper builds on that concept.- Other learning-based methods like L3-Net replace only parts of the SLAM pipeline with neural networks. This paper keeps the full self-supervised training framework of DeepMapping.- For incorporating loop closure information, many papers add explicit loop closure detection modules. This paper takes a different approach of batch organization based on topology to implicitly include loop closure.- The local-to-global consistency loss is a novel technique to leverage local pairwise registration while avoiding reliance on sparse point correspondences between frames. This addresses a key challenge in LiDAR registration.- Results on large-scale outdoor and underground datasets demonstrate state-of-the-art performance. Many prior learning-based methods were tested only on small indoor datasets.- The method can be used with different input maps, showing robustness. And it improved odometry on the Nebula dataset, where traditional SLAM failed.Overall, the key novelties are in addressing DeepMapping's limitations for large-scale mapping through implicit loop closure and the consistency loss. The results validate the proposed techniques and show this approach advances self-supervised LiDAR map optimization to handle complex real-world datasets.


## What future research directions do the authors suggest?

Based on my reading of the paper, some of the main future research directions suggested by the authors include:- Developing methods to further improve the speed and scalability of DeepMapping2, such as through more parallelization and optimization. The paper notes that DeepMapping2 currently has longer computation time than some other methods.- Exploring the integration of keyframe-based SLAM techniques into the DeepMapping2 framework. The authors suggest keyframe selection could help improve efficiency.- Generalizing DeepMapping2 to multi-agent SLAM scenarios, where point clouds may come from multiple sensor agents rather than just a single trajectory.- Incorporating other sensor modalities like cameras into DeepMapping2, as the current method relies solely on LiDAR point clouds. Fusing camera imagery could provide additional constraints.- Evaluating DeepMapping2 on more complex large-scale datasets and scenes to further validate its capabilities. The paper tested it on KITTI, NCLT and Nebula datasets.- Developing unsupervised or self-supervised methods for obtaining the map topology used to organize training batches in DeepMapping2. Currently this relies on off-the-shelf place recognition algorithms.- Exploring the integration and joint optimization with the front-end odometry tracking components of SLAM systems. Right now DeepMapping2 focuses on the backend global pose optimization.In summary, the main future directions are improving the efficiency and scalability of the method, expanding it to more complex scenarios like multi-agent SLAM, incorporating other sensor modalities beyond LiDAR, and developing more self-supervised components to remove reliance on other algorithms. Evaluating on more varied and complex datasets is also suggested.
