# [Playing repeated games with Large Language Models](https://arxiv.org/abs/2305.16867)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be: How do large language models (LLMs) behave in interactive social settings, specifically when playing repeated games against each other and human-like strategies? The authors use behavioral game theory to study LLMs' cooperation and coordination behaviors by having them play various two-player, two-action repeated games. The key hypothesis appears to be that analyzing LLMs' performance across different game families can uncover persistent behavioral signatures related to selfishness, unforgivingness, and lack of coordination. The paper seems focused on elucidating these behavioral tendencies of LLMs through game theoretic analyses.


## What is the main contribution of this paper?

The main contribution of this paper is using behavioral game theory to study the cooperation, coordination, and social behavior of large language models (LLMs) like GPT-3, GPT-3.5, and GPT-4. Specifically, the authors:- Let LLMs play iterated 2x2 games against each other and simple strategies, focusing on games from different families like Prisoner's Dilemma and Battle of the Sexes.- Found LLMs perform well in games valuing self-interest like Prisoner's Dilemma but underperform in coordination games like Battle of the Sexes. - Analyzed GPT-4's behavior in depth, finding it acts selfishly/unforgivingly in Prisoner's Dilemma but fails to coordinate with simple alternating strategies in Battle of the Sexes.- Showed GPT-4's behavior can be improved by providing information about the other player and prompting predictions before actions.Overall, the paper demonstrates using game theory to systematically study LLMs' interactive behavior, shedding light on strengths, flaws, and ways to improve alignment with human conventions. It lays groundwork for a behavioral game theory approach to analyzing machine social intelligence.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

The paper investigates the behavior of large language models like GPT-3 and GPT-4 in repeated social interactions modeled as game theory games. It finds strengths in competitive games but deficiencies in coordination, with specific behavioral signatures like unforgivingness in the Prisoner's Dilemma and failure to establish conventions in the Battle of the Sexes. Overall, the work demonstrates the utility of behavioral game theory for understanding the social abilities and flaws of large language models.


## How does this paper compare to other research in the same field?

Here are a few thoughts on how this paper compares to other research in the field of studying the behavior of large language models:- The use of game theory and behavioral economics experiments to study LLMs is relatively novel. Most prior work has focused on benchmarking LLMs on standard NLP tasks or studying their abilities in more open-ended ways. Using well-defined games allows for more controlled study of interactive behaviors.- The focus on repeated games rather than one-shot games is important. Studying iterated interactions provides insights into how LLMs develop conventions and adapt over time, rather than just reacting to a single scenario.- Analyzing a broad set of 2x2 games gives a more comprehensive view of strengths/weaknesses compared to just looking at one game like Prisoner's Dilemma or Battle of the Sexes. The large-scale analysis highlights areas LLMs do well at like self-interest games as well as struggles like coordination.- The direct comparison to simple heuristic strategies reveals flaws in LLM behavior. Even alternating in Battle of the Sexes is challenging, indicating issues in adapting to opponents. This goes beyond just evaluating overall performance.- The paper builds on prior work studying social reasoning in LLMs using theory of mind tasks. The games provide another angle to evaluate capabilities and limitations related to modeling others.- The robustness checks on prompt variations are important to verify the results reflect intrinsic behaviors rather than prompt engineering. This helps strengthen the claims made about behavioral signatures.Overall, the use of game theory provides a new lens for studying LLMs through interactive tasks with precise payoffs and equilibria. The results illuminate both impressive strategic capabilities of modern LLMs as well as areas where human-like conventions and theory of mind remain lacking. More research building on this paradigm could further deepen our understanding of the social cognition of LLMs.


## What future research directions do the authors suggest?

Based on my reading of the paper, some of the main future research directions suggested by the authors include:- Studying LLM behavior in more complex games beyond simple 2x2 games, such as games with more players, continuous action spaces, public goods games, etc. This could shed more light on cooperative, competitive and exploitative behaviors of LLMs.- Conducting more hypothesis-driven experiments to delve deeper into the behavioral signatures identified in this initial exploratory work, such as the lack of forgiveness and failure to coordinate. - Developing models that can better recognize the behavioral flaws uncovered, for example by training adversarial agents that can exploit these flaws.- Expanding the application of behavioral science and game theory as a framework to elucidate different facets of LLM cognition. The authors suggest this will continue to be a useful methodology as LLMs become more complex.- Studying LLM behavior in iterative conversations and interactions beyond just games, to better understand the malleability of their social behavior.- Investigating modifications like prompting for theory of mind that could improve coordination and alignment of LLM behavior with human conventions and social abilities.In summary, the authors point to game theory as a promising framework for analyzing LLM social behavior, and suggest various ways to build on this approach to further illuminate their capabilities and limitations in interactive settings.


## Summarize the paper in one paragraph.

Here is a one paragraph summary of the paper:This paper explores how large language models (LLMs) behave when playing repeated games, in order to understand their cooperation, coordination, and theory of mind abilities. The authors let LLMs like GPT-3, GPT-3.5, and GPT-4 play various two-player, two-strategy games against each other and simple algorithms. They find LLMs perform well in self-interest games like the Prisoner’s Dilemma, with GPT-4 being unforgiving and always defecting after one defection from the other player. However, LLMs struggle in coordination games like the Battle of the Sexes, with GPT-4 unable to match a simple alternation strategy. Robustness checks confirm these behavioral signatures persist across variations. The authors show GPT-4's behavior can be improved by providing information about the other player's fallibility or by prompting GPT-4 to first predict the other player's actions. Overall, the work demonstrates using game theory to study LLMs' social behavior and limitations.


## Summarize the paper in two paragraphs.

Here is a two paragraph summary of the key points from the paper:This paper explores how large language models (LLMs) behave in repeated social interactions using games from behavioral economics. The authors let LLMs like GPT-3, GPT-3.5, and GPT-4 play various two-player, two-choice games against each other and simple strategies. Analyzing performance across games, they find LLMs do well in competitive games valuing self-interest, like Prisoner’s Dilemma variants, but poorly in coordination games. Focusing on Prisoner’s Dilemma, the authors show GPT-4 is unforgiving, always defecting after even one defection from the other player. In the coordination game Battle of the Sexes, GPT-4 fails to match a simple alternating strategy. The authors highlight these behaviors persist across robustness checks. Finally, they demonstrate improving coordination by first asking GPT-4 to predict the other player’s actions. Overall, this paper provides initial evidence that games from behavioral economics can be used to systematically study strengths and weaknesses in how LLMs interact. The results uncover specific social flaws, like being unforgiving or uncoordinated, while pointing toward promptings that may improve interactive alignment.In summary, this paper explores LLMs' behavior in repeated social interactions using economic games. The authors uncover strengths in competitive scenarios and weaknesses in coordination. The work provides a proof-of-concept for using games to study interactive phenomena in LLMs in a principled manner, while pointing to ways their social abilities may be improved.


## Summarize the main method used in the paper in one paragraph.

The main method used in this paper is letting large language models (LLMs) play repeated 2x2 games against each other and some simple heuristic strategies. The games come from different families like win-win, Prisoner's Dilemma, coordination games etc. The authors let LLMs like GPT-3, GPT-3.5 and GPT-4 play these games for 10 rounds by providing the game rules and history of previous rounds as prompt context. The LLMs' choices are tracked across rounds to analyze their behavior and performance. This allows studying the cooperative, competitive and coordination behaviors of LLMs in a controlled setting interpreted through the lens of behavioral game theory. The robustness of behaviors is checked by modifying prompts. The paper focuses on studying LLMs' behavior in Prisoner’s Dilemma to assess cooperation and retaliation and in Battle of Sexes to assess coordination.


## What problem or question is the paper addressing?

The paper is addressing how large language models (LLMs) behave in repeated social interactions, specifically in the context of behavioral game theory. The key questions seem to be:- How do LLMs like GPT-3, GPT-3.5, and GPT-4 perform when playing various types of repeated two-player games against each other and simple strategies? - What are the behavioral signatures or tendencies of LLMs in games that require cooperation vs coordination? - Can examining LLM behavior in iterated games illuminate strengths, flaws, and ways to improve their social capabilities?The motivation is to understand how LLMs will behave as they are increasingly deployed in interactive settings. The paper takes a behavioral game theory approach to systematically study LLM social behavior in a controlled manner.
