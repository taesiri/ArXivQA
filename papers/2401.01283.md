# [Quality and Quantity of Machine Translation References for Automated   Metrics](https://arxiv.org/abs/2401.01283)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Automatic machine translation metrics rely on human reference translations to evaluate system translations. It is commonly believed that high-quality human references are needed, but there is little analysis on the cost-benefit tradeoffs of reference quality and quantity.

- This leaves open questions around (1) if higher quality references are actually useful, (2) if multiple references help, and (3) how to optimally allocate a budget for acquiring references.

Methodology:
- Experiments conducted on English-Czech translation dataset with 4 references of increasing quality (R1-R4). R1 is lowest quality and most affordable, R4 is highest quality but most expensive.

- Evaluate commonly used metrics like BLEU, ChrF, Comet, TER, and BLEURT by correlating their scores to human judgments. Segment-level Kendall's tau correlation is the evaluation metric.

- Also evaluate impact of:
   - Using post-edited versions of references
   - Mixing references of different quality levels
   - Varying number of references per segment

Key Findings:
- Higher quality references R3 and R4 lead to better metric performance than lower quality R1 and R2. However, highest quality R4 is not always the best.

- Post-editing can improve references and thus metric performance.

- Multiple references help - taking average or max correlation across references for a segment gives best performance. Diminishing returns after 7 references.

- Can linearly mix references from different quality levels and improve metrics.

- Provide budget optimization algorithm to determine best allocation of segments to different quality/cost reference vendors. Mix of quality and quantity is best.

Implications:
- Higher quality references are better for metrics up to a point. Too much quality not always useful if it deviates too much from source.

- Aim for at least 3-7 references per segment by mixing vendors.

- Given a budget, use proposed algorithm to determine optimal allocation of segments to different reference vendors.


## Summarize the paper in one sentence.

 Here is a one-sentence summary of the paper:

The paper investigates the trade-off between reference translation quality, quantity, and cost in order to optimize automated machine translation evaluation metrics given a budget for acquiring references.


## What is the main contribution of this paper?

 The main contribution of this paper is providing a cost-benefit analysis of using different quality and quantity of human references for evaluating machine translation systems with automatic metrics. Specifically, the paper:

- Shows that higher quality references lead to better metric correlations with humans, though the highest quality references (from professional translatologists) are not always the best. 

- Demonstrates that having multiple references per segment, up to around 7, and taking their average helps improve all metrics. 

- Finds that references from different quality vendors can be mixed together in various proportions, leading to a linear combination of the metric performances. 

- Frames the trade-off between reference quality and quantity as an optimization problem and provides an algorithm to allocate a given budget for references across different vendors in order to maximize metric success.

In summary, the key contribution is providing guidance, backed by experiments and analysis, on what kinds of human references should be collected to best evaluate machine translation systems with automated metrics under budget constraints. This has direct practical implications for running translation shared tasks and benchmarks.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and concepts are:

- Machine translation evaluation
- Automatic metrics
- Reference translations
- Reference quality
- Reference quantity
- Segment-level correlation
- Budget allocation
- Cost-benefit analysis
- Multiple references
- Translation vendors

The paper investigates the trade-off between quality, quantity, and cost of human reference translations that are used to evaluate machine translation systems with automatic metrics. It looks at the impact of reference attributes like quality, number of references, aggregating multiple references on the correlation of metrics with human judgments at the segment level. The paper also frames reference collection as an optimization problem of allocating a budget to gather references from different vendors to maximize metric performance.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in this paper:

1. How does the proposed budget allocation algorithm balance the trade-off between reference quality and quantity? What is the role of the Î» hyperparameter?

2. The paper finds that higher quality references lead to better metric performance, but taking reference quality too far (R4) leads to diminishing returns. What might explain why the highest quality references are not always the most useful?

3. How might the optimal combination of reference quality and quantity differ when evaluating machine translation systems at the system-level rather than segment-level? Would we expect quality or quantity to become more important?

4. Could the proposed budget allocation algorithm be extended to also consider the difficulty of translating certain source segments? How might active learning principles be incorporated?

5. What other criteria beyond cost and expected utility could be incorporated into the budget allocation algorithm? For example, how could diversity of linguistic phenomena in the test set play a role?

6. The paper analyzes a small dataset between a single language pair (En->Cs). How might the optimal combination of reference quality and quantity change for other language pairs?

7. Could the principle of mixing machine and human translations when creating references be explored? Would this allow creation of very large mixed-source reference sets?

8. How well would the proposed budget allocation algorithm extend to multilingual translation settings? Would we expect different optimum quality/quantity trade-offs per language?

9. For training machine translation systems, multiple references have proven very useful. Could the insights from this work on optimization of reference quality and quantity inform data collection for MT training?

10. The paper focuses on segment-level evaluation. How might the insights change if optimized for system-level evaluation? Would emphasis on quality, quantity or their combination change?
