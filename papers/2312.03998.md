# [Series2Vec: Similarity-based Self-supervised Representation Learning for   Time Series Classification](https://arxiv.org/abs/2312.03998)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a high-quality paragraph summarizing the key points of the paper:

This paper proposes a novel self-supervised learning method called Series2Vec for time series representation learning. Inspired by contrastive learning, Series2Vec uses time series similarity measures to assign target outputs for the encoder loss instead of synthetic transformations. This provides a more suitable implicit bias for time series compared to existing pretext tasks that risk positive sample variants being less similar to the anchor than negatives. Series2Vec learns representations in both time and frequency domains that preserve similarities between series. It introduces an order-invariant self-attention mechanism during training to enforce learning similar representations for similar series in each batch. Extensive experiments on nine real-world datasets demonstrate that Series2Vec outperforms current state-of-the-art self-supervised techniques and performs comparably to fully supervised training. It also shows particular promise in limited label regimes. Finally, fusion of Series2Vec with other representation learning models is shown to achieve further performance gains. The method offers an effective approach for self-supervised time series representation learning without requiring hand-crafted data augmentation.
