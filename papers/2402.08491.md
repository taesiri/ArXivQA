# [Deep Reinforcement Learning for Controlled Traversing of the Attractor   Landscape of Boolean Models in the Context of Cellular Reprogramming](https://arxiv.org/abs/2402.08491)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Cellular reprogramming can help cure diseases, but discovering effective reprogramming strategies with lab experiments is difficult and costly. 
- Boolean networks (BNs) and probabilistic Boolean networks (PBNs) are useful for modeling gene regulatory networks (GRNs), but controlling their dynamics to identify reprogramming strategies is challenging, especially for large networks.
- Existing control methods don't scale well and are limited to small/mid-size networks or require decomposing the networks.

Proposed Solution:
- Formulate a control problem for BNs/PBNs under asynchronous update for identifying reprogramming strategies to guide cells from a source attractor (cell state) to a target attractor.
- Introduce "pseudo-attractors" - subset of an attractor's frequently revisited states - to allow attractor identification during training.
- Propose a deep reinforcement learning (DRL) based framework called pbn-STAC to solve the control problem by learning to intervene only in pseudo-attractor states.

Key Contributions:
- Control problem formulation for BNs/PBNs for cellular reprogramming context. Allows more flexible interventions than prior works.
- Notion of pseudo-attractor and method to identify them, enabling application to large networks without full prior attractor info.  
- DRL framework pbn-STAC to solve formulated control problem. Uses branching architecture to scale and intervene only in pseudo-attractor states.
- Evaluation on BN/PBN models of varying sizes for melanoma + biological case study model. Compares to optimal strategies.

The framework shows potential for identifying effective reprogramming strategies on large GRN models where other methods fail. Long control sequence lengths for some cases are a bottleneck needing improvement. Overall, it is a relevant step towards scalable BN/PBN control methods for cellular reprogramming.


## Summarize the paper in one sentence.

 Here is a one-sentence summary of the paper:

The paper develops a deep reinforcement learning framework to identify effective cellular reprogramming strategies by formulating and solving a control problem for Boolean and probabilistic Boolean network models of gene regulatory networks.


## What is the main contribution of this paper?

 The main contribution of this paper is a novel computational framework based on deep reinforcement learning, called pbn-STAC, for identifying effective cellular reprogramming strategies. Specifically, the key contributions are:

1) Formulating a control problem for Boolean and probabilistic Boolean network models of gene regulatory networks under asynchronous update mode that corresponds to the problem of finding cellular reprogramming strategies. 

2) Introducing the notion of a "pseudo-attractor" to approximate attractors in large network models where computing exact attractors is infeasible. A procedure is proposed for identifying pseudo-attractor states during training.

3) Devising a deep reinforcement learning based framework, pbn-STAC, for solving the control problem to drive the network dynamics from a source attractor to a specified target attractor using a limited number of interventions. Key components include a Branching Dueling Q-Network, an exploration probability boost technique, and integration with the pseudo-attractor identification procedure.

4) Evaluating pbn-STAC on Boolean and probabilistic Boolean network models of various sizes, including a biological case study model. The results demonstrate the potential of pbn-STAC to find effective control strategies on large models where traditional methods fail.

In summary, the main contribution is the novel pbn-STAC framework that enables scalable identification of cellular reprogramming strategies on large computational models of gene regulatory networks.


## What are the keywords or key terms associated with this paper?

 Based on my review of the paper, here are some of the key terms and keywords that seem most relevant:

- Boolean networks (BNs)
- Probabilistic Boolean networks (PBNs)
- Gene regulatory networks (GRNs)
- Cellular reprogramming
- Attractors
- Pseudo-attractors 
- Deep reinforcement learning (DRL)
- Control problem
- Source-target attractor control
- Asynchronous update mode
- Exploration probability boost (EPB)
- Pseudo-Attractor States Identification Procedure (PASIP)

The paper focuses on developing a DRL-based framework called pbn-STAC to identify cellular reprogramming strategies, formulated as a control problem in the context of BNs and PBNs under asynchronous update mode. Key concepts include attractors, pseudo-attractors, defining the source-target attractor control problem, and introducing techniques like PASIP and EPB to enable the DRL agent to solve this problem. The goal is a scalable computational framework that can handle large GRN models where attractors are difficult to compute directly.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in the paper:

1. The paper introduces the notion of a "pseudo-attractor". What is the motivation behind defining pseudo-attractors instead of using the standard definition of attractors? How does the definition of pseudo-attractors help in scaling up the control framework to large networks?

2. Explain the Pseudo-Attractor States Identification Procedure (PASIP) for identifying pseudo-attractor states during training in detail. What are the key steps and thresholds used? How does PASIP balance computational efficiency and accuracy?

3. The paper modifies the epsilon-greedy policy by introducing Exploration Probability Boost (EPB). What problem does EPB aim to solve? How does boosting the exploration probability help stabilize training when new pseudo-attractors are discovered?

4. The control strategies found by the method for some networks are longer on average than the optimal strategies. Analyze the possible reasons behind this issue. How can this problem of longer control strategies be alleviated?  

5. The paper evaluates the method on Boolean Network (BN) and Probabilistic Boolean Network (PBN) models of different sizes. Compare and contrast the performance of the method on BN models versus PBN models. What differences can be observed?

6. For the IRBB-33 model, a different reward scheme performed better than the standard one proposed. Why does modifying the reward function lead to improved performance on this specific model? How should the choice of reward function be adapted based on the network?

7. The method has been evaluated on networks with up to 33 nodes. Discuss the scalability limitations of the current approach. What improvements can allow the method to scale to even larger networks with hundreds of nodes?

8. The control problem has been formulated specifically for the application of cellular reprogramming. How can the problem formulation and method be adapted for other applications of Boolean network control?

9. Attractor computation for large PBNs remains a bottleneck for scaling up the approach. Propose alternative methods for approximate attractor identification that can allow the control framework to be applied without explicit attractor information.

10. The current framework works under the asynchronous update mode for Boolean networks. How can the method be extended to other update modes such as synchronous or generalized asynchronous update? What changes would be required?
