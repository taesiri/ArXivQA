# [Securing GNNs: Explanation-Based Identification of Backdoored Training   Graphs](https://arxiv.org/abs/2403.18136)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
Graph neural networks (GNNs) are vulnerable to backdoor attacks where attackers inject triggers into some training graphs to control the model's predictions. Existing defenses using dense subgraph detection or randomized subgraph sampling have limited effectiveness. The paper investigates using GNN explainers like GNNExplainer to detect backdoored graphs but finds they are inconsistent and incomplete in revealing full backdoor information. 

Proposed Solution: 
The authors propose a novel detection strategy that collectively considers seven new metrics derived from the GNN explanation process to robustly distinguish between clean and backdoored graphs:

1. Prediction Confidence: Maximum predicted probability for a graph (expected to be higher for backdoored graphs)

2. Explainability: Difference between positive and negative fidelity of the explanatory subgraph (expected to be higher for backdoored graphs)  

3. Connectivity: Fraction of node pairs in explanatory subgraph with edges between them in original graph (should be higher if subgraph contains connected trigger)

4. Subgraph Node Degree Variance (SNDV): Variance of node degrees in explanatory subgraph (expected to differ from clean subgraphs if it contains the trigger)

5. Node Degree Variance (NDV): Variance of node degrees in original graph (attack insertion likely increases this)

6. Elbow: Epoch where loss curve rate of decrease changes most (should converge faster for backdoored graphs)  

7. Curvature: Sharpness of elbow in loss curve (should be larger for backdoored graphs)

These metrics are thresholded based on clean validation data to vote on whether a graph is clean or backdoored. The final prediction is made based on the number of positive votes. The method is evaluated against random and adaptive attacks over multiple datasets.

Main Contributions:

- First work to use GNN explainers for detecting backdoored graphs and show directly applying them is insufficient

- Introduce seven novel metrics derived from GNN explanation process that effectively distinguish between clean and backdoored graphs

- Propose a multi-faceted detection method unifying these metrics that is efficient, achieves high detection performance (F1 score up to 0.906 against random triggers and 0.842 against adaptive triggers), and is robust to adaptive attacks
