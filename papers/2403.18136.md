# [Securing GNNs: Explanation-Based Identification of Backdoored Training   Graphs](https://arxiv.org/abs/2403.18136)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
Graph neural networks (GNNs) are vulnerable to backdoor attacks where attackers inject triggers into some training graphs to control the model's predictions. Existing defenses using dense subgraph detection or randomized subgraph sampling have limited effectiveness. The paper investigates using GNN explainers like GNNExplainer to detect backdoored graphs but finds they are inconsistent and incomplete in revealing full backdoor information. 

Proposed Solution: 
The authors propose a novel detection strategy that collectively considers seven new metrics derived from the GNN explanation process to robustly distinguish between clean and backdoored graphs:

1. Prediction Confidence: Maximum predicted probability for a graph (expected to be higher for backdoored graphs)

2. Explainability: Difference between positive and negative fidelity of the explanatory subgraph (expected to be higher for backdoored graphs)  

3. Connectivity: Fraction of node pairs in explanatory subgraph with edges between them in original graph (should be higher if subgraph contains connected trigger)

4. Subgraph Node Degree Variance (SNDV): Variance of node degrees in explanatory subgraph (expected to differ from clean subgraphs if it contains the trigger)

5. Node Degree Variance (NDV): Variance of node degrees in original graph (attack insertion likely increases this)

6. Elbow: Epoch where loss curve rate of decrease changes most (should converge faster for backdoored graphs)  

7. Curvature: Sharpness of elbow in loss curve (should be larger for backdoored graphs)

These metrics are thresholded based on clean validation data to vote on whether a graph is clean or backdoored. The final prediction is made based on the number of positive votes. The method is evaluated against random and adaptive attacks over multiple datasets.

Main Contributions:

- First work to use GNN explainers for detecting backdoored graphs and show directly applying them is insufficient

- Introduce seven novel metrics derived from GNN explanation process that effectively distinguish between clean and backdoored graphs

- Propose a multi-faceted detection method unifying these metrics that is efficient, achieves high detection performance (F1 score up to 0.906 against random triggers and 0.842 against adaptive triggers), and is robust to adaptive attacks


## Summarize the paper in one sentence.

 This paper proposes a novel method to detect backdoor attacks in graph neural networks by extracting and transforming secondary outputs from graph-level explanation mechanisms to design a set of metrics that can effectively distinguish between clean and backdoored graphs.


## What is the main contribution of this paper?

 According to the paper, the main contributions are:

1. The authors are the first to use GNN explainers to detect backdoored graphs in GNNs. They show that directly applying these explainers is insufficient to reliably detect backdoors. 

2. To address the limitations of using explainers alone, the authors introduce a set of novel metrics that leverage insights from certain aspects of the GNN explanation process. Through extensive evaluations, these metrics provide a deeper understanding of effective graph backdoor attacks.

3. The authors propose a multi-faceted detection method that unifies their designed metrics. Their method is shown to be effective, efficient, and robust to adaptive attacks designed specifically to evade detection.

In summary, the main contribution is a novel backdoor detection method for graph neural networks based on a set of new metrics derived from the GNN explanation process. This method provides more reliable and robust backdoor detection compared to simply using GNN explainers directly.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key keywords and terms associated with this paper include:

- Backdoor attack
- Graph neural network (GNN)
- Explainability
- GNN explainer
- Explanatory subgraph
- Detection metrics (Prediction Confidence, Explainability, Connectivity, Subgraph Node Degree Variance, Node Degree Variance, Elbow, Curvature) 
- Composite metric
- Adaptive attack
- Trigger generator

The paper proposes a method to detect backdoor attacks on graph neural networks using explainability techniques and a set of novel metrics derived from the explanation process. Key elements include using GNN explainers to extract explanatory subgraphs, designing metrics that capture differences between clean and backdoored graphs, and developing a composite metric that unifies these individual metrics to robustly detect backdoors. An adaptive attack is also introduced to rigorously evaluate the detection approach. Overall, the goal is to safeguard GNNs against backdoor attacks for reliable and ethical graph classification across various applications.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in the paper:

1. The paper mentions that directly using GNN explainers alone is insufficient to reliably detect backdoored graphs in GNNs. What are some of the key limitations or inconsistencies they found when using explainers like GNNExplainer and PGExplainer?

2. The paper proposes 7 novel metrics that leverage insights from the GNN explanation process to detect backdoored graphs. Can you describe at least 3 of these metrics and the rationale behind designing them? 

3. How does the paper's method use the clean validation data set to establish thresholds for predicting whether a graph is clean or backdoored? What are some pros and cons of this approach?

4. Explain the difference between "phenomenon" explanations and "model" explanations in the context of GNNExplainer. Why does the choice between these matter for the proposed backdoor detection method?

5. The paper finds that the performance of loss curve metrics like Elbow and Curvature depends on the strength of the backdoor attack. How does this relate to the choice between using raw metrics versus distance metrics?

6. Describe the adaptive trigger generation method proposed in the paper. What is the key idea behind attacking the GNN explainer when creating these adaptive triggers? 

7. Analyze Figure 5 in the paper, which shows F1 scores for the composite metric under different choices of NPMR (number of positive metrics required). What insights does this figure provide about the robustness and effectiveness of the proposed method?

8. Based on the various ablation studies in the paper, discuss whether stronger backdoor attacks are easier or harder to detect using the proposed approach. Justify your answer.  

9. What is the computational complexity of the proposed backdoor detection method? Explain which components dominate the computational load.

10. If you had access to this dataset and were tasked with evaluating or extending this method, what experiments would you propose to gain additional insights?
