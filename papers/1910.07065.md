# [Reverse derivative categories](https://arxiv.org/abs/1910.07065)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the main research goal is to provide a categorical axiomatization of the reverse derivative, which is an important operation in machine learning and automatic differentiation. Specifically, the key contributions and results appear to be:- Introducing the definition of a "Cartesian reverse differential category", which is a Cartesian left additive category equipped with a reverse differential combinator satisfying certain axioms. This provides a direct axiomatization of categories with a reverse derivative.- Showing that a Cartesian reverse differential category also induces a Cartesian differential category structure, i.e. it has forward derivatives. But the converse is not true - a Cartesian differential category does not necessarily have reverse derivatives. - Proving that a reverse differential category is equivalent to a Cartesian differential category with a dagger (involution) structure on the subcategory of linear maps. The linear maps additionally form an additively enriched category with dagger biproducts.- Characterizing reverse differential categories as precisely Cartesian differential categories with a "contextual linear dagger". So in summary, the key contribution is introducing an axiomatic framework for reverse derivatives in categories, analogous to existing work on forward derivatives, and establishing connections between reverse and forward differential structures. This provides a foundation for building categorical semantics of programming languages with both forward and reverse mode differentiation capabilities.


## What is the main contribution of this paper?

Based on my understanding, the main contribution of this paper is introducing a categorical framework for reverse derivatives. The key ideas are:- The paper provides a direct axiomatization of a category with a reverse derivative operation, similar to how Cartesian differential categories axiomatize categories with a forward derivative. - It shows that a category with a reverse derivative also has a forward derivative, but not vice versa. A reverse derivative is equivalent to a forward derivative plus a dagger structure on the subcategory of linear maps.- Linear maps in a reverse differential category form an additively enriched category with dagger biproducts. - A reverse differential category is precisely a Cartesian differential category where the subcategory of linear maps forms a dagger fibration with fibered dagger biproducts.In summary, the paper introduces the notion of a reverse differential category, which provides an abstract foundation for reverse mode automatic differentiation used extensively in machine learning. It relates this structure to forward derivatives and establishes several key properties. This provides a basis for further development of categorical semantics for differential programming languages and machine learning systems.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper introduces the notion of a reverse derivative in category theory, providing axioms for a category equipped with a reverse derivative operator, and shows this is equivalent to a category with forward derivatives that has additional dagger structure.


## How does this paper compare to other research in the same field?

Here are my thoughts on how this paper compares to other research on categorical semantics for reverse derivatives:- This appears to be the first categorical axiomatization specifically aimed at capturing reverse derivatives. Previous categorical models like Cartesian differential categories focused only on forward derivatives. So this paper fills an important gap.- The key idea of relating reverse derivatives to a dagger structure seems novel. Showing that a reverse derivative is equivalent to a forward derivative plus a dagger on the linear maps is an elegant theoretical insight. - The overall style of axiomatization is similar to prior work on Cartesian differential categories. So this fits nicely into that literature and provides a consistent framework for thinking about derivatives categorically.- The examples considered (polynomials, smooth functions) are standard ones seen in other papers on differentiation in programming languages and machine learning. No major new examples here, but good to see the reverse derivative axioms validated on familiar settings.- Making connections to things like the dual fibration and dagger categories links this with broader ideas in category theory. Integrating reverse derivatives with other structures like restriction categories is noted as important future work.- Compared to some other categorical semantics research, the development here seems fairly self-contained and focused specifically on reverse derivatives. Not a lot of links drawn to other active areas like type theory or logic.Overall, I would say this is an incremental but valuable step forward in categorical semantics for differentiation. It carves out the niche of reverse derivatives and develops that theory in a focused way. Lots of opportunities exist to build on this and connect it to other ideas in semantics and programming languages. But the core conceptual contributions around relating reverse and forward derivatives via dagger categories seem like a useful conceptual advance.


## What future research directions do the authors suggest?

Based on the concluding remarks section, some key future research directions the authors suggest include:- Adding partiality/restriction structure to reverse differential categories, to get "reverse differential restriction categories". This would allow for modeling of smooth partial maps and bring the theory closer to a semantics for differential programming languages. - Developing a term logic for reverse differential categories, to facilitate proving results in this abstract setting.- Incorporating tensors/monoidal structure compatible with the reverse derivative, to align with machine learning foundations and provide more examples via coKleisli categories.- Defining "categories with a cotangent bundle", which should arise from reverse differential categories analogously to how tangent categories arise from Cartesian differential categories. This would provide a categorical setting for differential geometry with reverse derivatives. - More broadly, continuing to develop the theory of reverse derivatives categorically, including modeling programming language features like effects. This will support applications to differential programming, machine learning, and automatic differentiation.In summary, the main suggested directions are expanding the categorical semantics to encompass more programming language features and applications, including effects, partiality, differential geometry, and tensors. The development of associated logic and language would also be important to facilitate reasoning and implementation.


## Summarize the paper in one paragraph.

Here is a one paragraph summary of the paper:This paper introduces the notion of a reverse derivative in category theory, providing axioms for a Cartesian reverse differential category. It shows that a category with a reverse derivative structure also induces a forward derivative structure, making it a Cartesian differential category. However, the reverse does not hold - having a forward derivative alone is not enough to guarantee the existence of a reverse derivative. The key additional structure needed is that the subcategory of linear maps must have a dagger (contravariant involution) structure, specifically dagger biproducts. The paper proves that a Cartesian differential category with a contextual dagger structure on linear maps is equivalent to a Cartesian reverse differential category. It also relates the reverse derivative structure to the dual of the linear maps fibration. Overall, the reverse derivative provides an abstract foundation for reverse mode differentiation, important in machine learning and automatic differentiation.


## Summarize the paper in two paragraphs.

Here is a two paragraph summary of the paper:This paper introduces the notion of a reverse derivative in the categorical setting and defines Cartesian reverse differential categories. A Cartesian reverse differential category has a reverse derivative operator that satisfies certain axioms analogous to those of a forward derivative in a Cartesian differential category. The paper shows that a Cartesian reverse differential category structure induces a Cartesian differential category structure, where the forward derivative can be defined from iterating the reverse derivative twice. Intriguingly, the converse does not hold - having a Cartesian differential structure is not enough to induce a reverse derivative structure. The missing ingredient is that the subcategory of linear maps must have a dagger (involution) structure with dagger biproducts. The paper shows that a Cartesian differential category with such a dagger structure on linear maps does induce a Cartesian reverse differential structure. Furthermore, the main result shows that these two notions are equivalent - a Cartesian reverse differential category if and only if a Cartesian differential category with a contextual dagger structure. This provides a categorical axiomatization of reverse derivatives, analogous to Cartesian differential categories for forward derivatives.


## Summarize the main method used in the paper in one paragraph.

Here is a one paragraph summary of the main method used in the paper:The paper introduces the notion of a reverse derivative in a categorical setting, similar to how forward derivatives are captured by Cartesian differential categories. The authors define a Cartesian reverse differential category as a Cartesian left additive category equipped with a reverse differential combinator satisfying certain axioms. They show that every Cartesian reverse differential category gives rise to a Cartesian differential category structure via repeated application of the reverse derivative. A key result is that having a reverse derivative is equivalent to having a forward derivative together with a dagger structure on the subcategory of linear maps, satisfying certain compatibility conditions. The paper develops the theory of reverse derivatives in analogy with forward derivatives, including analogues of the chain rule, linearity, and symmetry of mixed partials. The main method is a categorical axiomatization of reverse derivatives using similar techniques as for forward derivatives.


## What problem or question is the paper addressing?

This paper introduces the notion of a "Cartesian reverse differential category", which provides an axiomatic framework for defining a reverse derivative operation in a categorical setting. Some key points:- The reverse derivative is an important operation used in machine learning and automatic differentiation, but prior categorical axiomatizations of differentiation focused only on the forward derivative. This paper fills that gap by giving axioms for the reverse derivative.- The axioms mirror those for the forward derivative (Cartesian differential categories), but adapted for the reverse setting. For example, the chain rule axiom expresses how reverse derivatives compose.- A category with a reverse derivative also has a forward derivative. Intriguingly, the converse is not true - a forward derivative alone is not enough to give a reverse derivative. - A reverse derivative is equivalent to a forward derivative plus additional "dagger" structure on the subcategory of linear maps. This dagger structure is what is missing from just having a forward derivative.- This dagger structure makes the linear maps into a biproduct category, which is important for modeling derivatives.Overall, this paper provides the first categorical axiomatization of reverse derivatives, paralleling existing work on forward derivatives. It highlights the additional structure needed to model reverse derivatives, beyond just forward derivatives. This is an important foundation for building categorical semantics for differential programming languages and machine learning systems.
