# MMDialog: A Large-scale Multi-turn Dialogue Dataset Towards Multi-modal   Open-domain Conversation

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is: How can we construct a large-scale, diverse, open-domain dataset of multi-modal dialogues with both text and images to facilitate research on building more engaging multi-modal conversational agents?The key points are:- Existing multi-modal dialogue datasets have limitations in scale, diversity, and realism. Many are small-scale, narrow domain, or artificially constructed. - The authors aim to create a large-scale, open-domain, realistic dataset derived from actual social media conversations that contain both text and images.- They collect over 1 million multi-turn dialogues with associated images across thousands of topics from a social media platform.- They propose benchmark tasks for multi-modal response generation and retrieval using this new dataset.- They design baseline models and evaluation metrics to assess performance on these tasks.So in summary, the main research goal is to construct a large, diverse, realistic multi-modal dialogue dataset to better facilitate research and development of multi-modal conversational agents that can understand and communicate with both text and images, akin to human conversational abilities. The paper introduces the dataset and benchmarks to drive progress in this direction.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contributions are:1. The authors construct a large-scale multi-turn dialogue dataset called MMDialog that contains 1.08 million multi-modal open-domain conversations and 1.53 million associated images derived from social media. To the best of the authors' knowledge, this is the first million-scale multi-turn open-domain multi-modal dialogue dataset.2. The authors propose two benchmark tasks on MMDialog - multi-modal response generation and multi-modal response retrieval - that are essential for building engaging multi-modal dialogue systems. 3. They propose a novel evaluation metric called MM-Relevance that measures the relevance between generated/retrieved multi-modal responses and ground truth responses. It is based on the pre-trained multi-modal CLIP model and helps address modal misalignment issues.4. The authors design two strong baselines for the multi-modal response generation and retrieval tasks, and achieve good performance on MMDialog according to both single modality metrics like BLEU, ROUGE, Recall@k as well as the proposed MM-Relevance metric.In summary, the key contribution is the construction of a large-scale multi-modal dialogue dataset MMDialog, along with well-defined tasks, evaluation metrics and baselines that can facilitate future research in this direction. The authors have open-sourced the dataset, evaluation scripts and baselines to promote further progress.
