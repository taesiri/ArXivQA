# MMDialog: A Large-scale Multi-turn Dialogue Dataset Towards Multi-modal   Open-domain Conversation

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is: How can we construct a large-scale, diverse, open-domain dataset of multi-modal dialogues with both text and images to facilitate research on building more engaging multi-modal conversational agents?The key points are:- Existing multi-modal dialogue datasets have limitations in scale, diversity, and realism. Many are small-scale, narrow domain, or artificially constructed. - The authors aim to create a large-scale, open-domain, realistic dataset derived from actual social media conversations that contain both text and images.- They collect over 1 million multi-turn dialogues with associated images across thousands of topics from a social media platform.- They propose benchmark tasks for multi-modal response generation and retrieval using this new dataset.- They design baseline models and evaluation metrics to assess performance on these tasks.So in summary, the main research goal is to construct a large, diverse, realistic multi-modal dialogue dataset to better facilitate research and development of multi-modal conversational agents that can understand and communicate with both text and images, akin to human conversational abilities. The paper introduces the dataset and benchmarks to drive progress in this direction.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contributions are:1. The authors construct a large-scale multi-turn dialogue dataset called MMDialog that contains 1.08 million multi-modal open-domain conversations and 1.53 million associated images derived from social media. To the best of the authors' knowledge, this is the first million-scale multi-turn open-domain multi-modal dialogue dataset.2. The authors propose two benchmark tasks on MMDialog - multi-modal response generation and multi-modal response retrieval - that are essential for building engaging multi-modal dialogue systems. 3. They propose a novel evaluation metric called MM-Relevance that measures the relevance between generated/retrieved multi-modal responses and ground truth responses. It is based on the pre-trained multi-modal CLIP model and helps address modal misalignment issues.4. The authors design two strong baselines for the multi-modal response generation and retrieval tasks, and achieve good performance on MMDialog according to both single modality metrics like BLEU, ROUGE, Recall@k as well as the proposed MM-Relevance metric.In summary, the key contribution is the construction of a large-scale multi-modal dialogue dataset MMDialog, along with well-defined tasks, evaluation metrics and baselines that can facilitate future research in this direction. The authors have open-sourced the dataset, evaluation scripts and baselines to promote further progress.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper introduces MMDialog, a new large-scale multi-modal open-domain dialogue dataset containing over 1 million dialogues with associated images, and proposes tasks and metrics for building multi-modal conversational agents using this data.


## How does this paper compare to other research in the same field?

Here are a few key points on how this paper compares to other research in multi-modal dialogue systems:- Data Scale: At 1.08 million dialogues, this is by far the largest multi-turn multi-modal dialogue dataset. Previous datasets are much smaller, with the largest being MMChat with 120k dialogues. The large scale allows training more robust data-driven models.- Realism: The dialogues are extracted from real social media conversations, as opposed to being crowd-sourced or artificially constructed. This improves the authenticity and diversity of the conversations. - Open Domain: With over 4000 topic hashtags, the dialogues cover a very wide range of everyday conversation domains. Other datasets are more narrow or focused on a specific domain like image commenting.- Tasks: The paper defines and evaluates strong baselines on two key dialogue tasks - response generation and retrieval. This provides standard benchmarks for future work to compare against.- Evaluation: A novel MM-Relevance metric is proposed to evaluate multi-modal responses, using CLIP to handle cross-modal matching.Overall, the combination of large scale, open domain, realism and strong baselines moves forward the research area significantly. By releasing the dataset and models, it enables future work to build on these contributions. The paper represents significant progress over prior work limited by data scale and narrower scope.


## What future research directions do the authors suggest?

Based on my reading of the paper, here are some of the main future research directions suggested by the authors:- Extending the multi-modal dialogue modeling to incorporate other modalities beyond text and images, such as audio, video, etc. The authors note that they currently do not consider other modalities like GIFs and videos in the dataset, and suggest this as an area for future work. - Exploring different multi-modal fusion methods. The authors use simple concatenation and modality intent prediction to combine text and images currently. They suggest investigating more advanced fusion techniques like cross-attention mechanisms.- Leveraging pre-trained models more effectively. The authors note their retrieval model uses a small Transformer without pre-training, while the generative model uses DialoGPT. They suggest pre-training could help improve performance.- Developing better evaluation metrics for multi-modal dialogue. The authors propose the MM-Relevance metric in this work, but note there is still room for improvement in metrics that can handle modality mismatches well.- Applying reinforcement learning and human-in-the-loop techniques to improve multi-modal response quality based on human feedback.- Extending the models to actually generate images rather than just image captions/descriptions. The generative model currently produces image descriptions then translates those to images.- Testing how well the models generalize to other multi-modal dialogue datasets. The authors use a new large dataset here, but suggest evaluating on other existing datasets too.- Developing personalized multi-modal dialogue models that can capture individual user preferences and traits.So in summary, the key directions highlighted are extending the modalities, improving fusion techniques, leveraging pre-training more, enhancing evaluation metrics, incorporating human feedback, generating images directly, testing generalization, and adding personalization. The large new dataset provides a good foundation for pursuing many of these research avenues.
