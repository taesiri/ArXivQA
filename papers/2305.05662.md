# [InternGPT: Solving Vision-Centric Tasks by Interacting with ChatGPT   Beyond Language](https://arxiv.org/abs/2305.05662)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question or hypothesis appears to be: How can an interactive visual system effectively combine pointing gestures and natural language instructions to enable more efficient and accurate human-AI communication and task performance, particularly for complex visual tasks involving multiple objects?The key ideas and components proposed to address this question include:- InternGPT (iGPT), an interactive visual framework that integrates pointing instructions, natural language processing via a large language model (LLM) controller, and an open-world toolkit. - A perception unit that interprets pointing gestures on images/videos to identify targets or create content.- An LLM controller that parses language commands, breaks down tasks, and invokes appropriate tools/models. - An auxiliary control mechanism to improve LLM's capability in parsing arguments and invoking APIs.- A large vision-language model called Husky that is fine-tuned for high-quality dialogue.- User studies and demonstrations showing iGPT's improvements in efficiency and accuracy over pure language systems, especially for visual tasks with multiple objects.In summary, the central hypothesis is that combining pointing and language can enhance human-AI communication and task performance for complex visual tasks, which iGPT aims to validate through its proposed framework and components.
