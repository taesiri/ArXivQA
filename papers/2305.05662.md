# [InternGPT: Solving Vision-Centric Tasks by Interacting with ChatGPT   Beyond Language](https://arxiv.org/abs/2305.05662)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question or hypothesis appears to be: How can an interactive visual system effectively combine pointing gestures and natural language instructions to enable more efficient and accurate human-AI communication and task performance, particularly for complex visual tasks involving multiple objects?The key ideas and components proposed to address this question include:- InternGPT (iGPT), an interactive visual framework that integrates pointing instructions, natural language processing via a large language model (LLM) controller, and an open-world toolkit. - A perception unit that interprets pointing gestures on images/videos to identify targets or create content.- An LLM controller that parses language commands, breaks down tasks, and invokes appropriate tools/models. - An auxiliary control mechanism to improve LLM's capability in parsing arguments and invoking APIs.- A large vision-language model called Husky that is fine-tuned for high-quality dialogue.- User studies and demonstrations showing iGPT's improvements in efficiency and accuracy over pure language systems, especially for visual tasks with multiple objects.In summary, the central hypothesis is that combining pointing and language can enhance human-AI communication and task performance for complex visual tasks, which iGPT aims to validate through its proposed framework and components.


## What is the main contribution of this paper?

The main contribution of this paper is proposing a new interactive visual framework called InternGPT (iGPT) that combines pointing instructions (like gestures and cursors) with language instructions to accomplish complex vision-centric tasks more efficiently and accurately compared to systems relying solely on language instructions. The key ideas of iGPT include:- A perception unit that interprets pointing instructions on images/videos for precise object selection and identification. - An LLM controller with an auxiliary control mechanism that accurately parses language instructions and controls task execution.- An open-world toolkit integrating various online models and applications.- Combining pointing and language allows more efficient communication and higher accuracy compared to pure language systems, especially for complex visual scenarios.- User studies demonstrate the efficiency gains of pointing+language over pure language.- Demonstrations showcase applications like interactive image/video editing, visual QA, and image generation.So in summary, the main contribution is the novel iGPT framework that advances interactive AI systems by augmenting language with pointing instructions for complex vision tasks.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper presents an interactive visual framework called InternGPT that combines pointing instructions and language instructions to allow more efficient and accurate communication between users and AI systems for performing complex vision-centric tasks.
