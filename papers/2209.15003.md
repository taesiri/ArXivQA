# Compositional Semantic Parsing with Large Language Models

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be:How can large language models be adapted to better generalize compositionally through the use of prompting techniques?The key hypothesis appears to be:Appropriate prompting techniques, such as least-to-most prompting and using exemplars, can enable large language models to solve more realistic compositional generalization tasks.In particular, the paper aims to take prompting techniques that have shown promise on simple artificial tasks like SCAN, and refine them to work on more complex and realistic semantic parsing benchmarks like CFQ and COGS. The core ideas are using syntactic parsing prompts to decompose problems, dynamically selecting exemplars tailored to each input, and prompting the model to solve subproblems sequentially. The hypothesis is that these techniques will allow large LMs to achieve much better compositional generalization on realistic tasks compared to vanilla few-shot prompting or standard fine-tuning approaches. The experiments on CFQ and COGS are designed to test this hypothesis.In summary, the paper is investigating how to adapt prompting to improve compositional generalization of LMs on realistic tasks, with the hypothesis that techniques like least-to-most prompting and tailored exemplars will enable models to systematically generalize better. The CFQ and COGS experiments aim to demonstrate these improvements concretely.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contributions appear to be:1. Identifying challenges in applying least-to-most prompting techniques to more realistic compositional generalization tasks like semantic parsing, compared to simpler synthetic tasks like SCAN. Specifically, the challenges identified include more difficult problem decomposition, the need for more knowledge than fits in a single prompt, and context-dependent translation of constituents. 2. Proposing "dynamic least-to-most prompting" as a refinement to address these challenges. The key ideas here are using LM-predicted syntactic parsing to decompose problems into a tree structure, dynamically selecting exemplars based on matching this decomposition tree, and prompting the model to sequentially generate solutions to linearized subproblems based on the decomposition tree.3. Demonstrating the effectiveness of dynamic least-to-most prompting by achieving new state-of-the-art results on the CFQ semantic parsing benchmark using only 1% of the training data. The approach also achieves strong results on the COGS benchmark.4. Showing the approach is fairly robust to the exemplar pool size and can remain competitive even when using less than 0.1% of the training data.5. Providing a detailed recipe to apply least-to-most prompting to new tasks, with the aim of improving compositional generalization. The prompts, exemplar selection strategies, and sequential prompting approach are described extensively.In summary, the main contribution appears to be proposing and demonstrating a refined prompting technique to improve compositional generalization on realistic tasks, with extensive details provided to transfer the approach to new problems. The effectiveness is shown on semantic parsing benchmarks where the technique sets new state-of-the-art results.
