# [Massive Activations in Large Language Models](https://arxiv.org/abs/2402.17762)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper "Massive Activations in Large Language Models":

Problem Statement:
- The paper discovers an intriguing phenomenon of "massive activations" in large language models (LLMs). These are activations with extremely large magnitudes, often orders of magnitude higher than other activations, that appear consistently at certain tokens and feature dimensions. 

- For example, in the LLaMA2-7B model, a few activations have values exceeding 2,000, while the median activation magnitude is around 0.2. So the massive activations are 10,000x larger in scale.

- The locations of massive activations differ across models. They tend to appear at the starting token, delimiter tokens like periods/newlines, and sometimes tokens with weak semantics like "and", "of".  

- The paper aims to analyze the characteristics and functional role of these rare but unusually large activations in LLMs.

Key Findings:
- Massive activations emerge abruptly after just 1-2 layers of computation and remain at mostly constant values for many subsequent layers before diminishing at the last layers.

- They act as crucial bias terms that enable proper functioning of LLMs. For example, just setting 4 massive activations to zero causes the LLaMA2-7B model to completely fail. 

- Massive activations impose implicit biases on the attention mechanism. Models concentrate excessive attention onto tokens associated with massive activations. The paper shows LLMs learn to use these tokens to inject additive bias terms into the attention output.

- Augmenting self-attention with explicit bias parameters removes the need for models to learn massive activations. The paper proposes this attention variant to regulate massive activations.

- Similar phenomena occur in vision transformers (ViTs). Massive activations manifest as fixed biases. The paper relates this to recently proposed "register tokens" in ViTs.

Main Contributions:
- Identify and characterize the surprising existence of rare but exceptionally large "massive activations" across major LLMs
- Demonstrate they serve as crucial learned bias terms that underpin model capabilities
- Elucidate connection with self-attention biases  
- Propose solutions to regulate these extreme activations
