# [Hydragen: High-Throughput LLM Inference with Shared Prefixes](https://arxiv.org/abs/2402.05099)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Transformer-based large language models (LLMs) are being deployed at massive scale to serve hundreds of millions of users. 
- A common use case is batched inference where many sequences share a common prefix, like a chatbot prompt or few-shot examples.  
- Attention becomes a bottleneck here - it does redundant reads of the shared prefix keys/values and lots of inefficient matrix-vector products.

Proposed Solution:
- The paper introduces Hydragen, an optimized attention implementation for shared prefixes.
- It decomposes attention over the full sequence into separate attention over the shared prefix and unique suffixes.
- It batches attention queries across sequences when computing prefix attention. This replaces many matrix-vector products with fewer matrix-matrix products.

Main Contributions:
- Hydragen reduces redundant memory reads of the shared prefix, enabling use of tensor cores.
- It improves end-to-end LLM throughput by up to 32x against competitive baselines.
- The speedup grows with batch size and prefix length. Increasing the prefix from 1K to 16K tokens reduces Hydragen throughput by <15% but drops baseline throughput by >90%.
- Hydragen generalizes beyond prefix-suffix splits. Applied hierarchically to a tree of prompts, it reduces inference time by an additional 55% compared to single-level sharing.

In summary, Hydragen is a hardware-aware attention algorithm for batched inference that decomposes attention to exploit shared prefixes across sequences. This eliminates redundant memory access and enables efficient matrix multiplications, significantly improving throughput.
