# [One scalar is all you need -- absolute depth estimation using monocular   self-supervision](https://arxiv.org/abs/2303.07662)

## What is the central research question or hypothesis that this paper addresses?

 The central research question this paper addresses is how to transfer the depth scale from datasets with ground truth depth to a monocular depth estimator trained with self-supervision on a new dataset without ground truth depth. 

The key hypothesis is that the relationship between predicted up-to-scale depth and ground truth depth is linear across images from different datasets. This enables using a global depth scaling factor learned on a source dataset to convert predicted up-to-scale depth to absolute depth on a target dataset.

In summary, the paper aims to solve the scale ambiguity issue in self-supervised monocular depth estimation by transferring depth scale from existing datasets with ground truth depth. The key idea is that depth rankings are linearly correlated across datasets, allowing a global depth scaling factor to be applied.


## What is the main contribution of this paper?

 The main contribution of this paper is a method for transferring the depth scale from existing datasets with ground truth depth to a new target dataset that only has images, in order to predict absolute depth values. Specifically:

- The paper shows that monocular depth estimators (MDEs) trained with self-supervision using view reconstruction losses exhibit a linear relationship between predicted up-to-scale depths and ground truth depths. This linear depth ranking holds globally across the dataset.

- By training a MDE on mixed batches of images from a source dataset (with ground truth depth) and target dataset (without), the linear depth ranking extends across both datasets. This enables transferring the depth scale from the source to target. 

- A global depth scaling factor is calculated on the source dataset and applied to the up-to-scale depth predictions on the target dataset to convert them to absolute depths.

- This approach allows training or fine-tuning an MDE on new target images alone, without needing to collect new ground truth depth data. It transfers depth scale from existing real or synthetic datasets.

- The method is demonstrated on KITTI and DDAD datasets using different source domains, achieving competitive accuracy to supervised training and other depth scale transfer techniques.

In summary, the key contribution is a simple yet effective way to predict absolute depths by transferring depth scale properties from existing datasets to new self-supervised training data, avoiding the need for expensive ground truth depth collection.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the key points in the paper:

The paper presents a method to transfer the depth scale from existing datasets with ground truth depth to a self-supervised monocular depth estimator trained on new target images, enabling prediction of absolute depth values without requiring ground truth depth data for the new scenes.


## How does this paper compare to other research in the same field?

 Here are some thoughts on how this paper compares to other research in monocular depth estimation:

- The key contribution of this paper is a method to transfer the depth scale from an existing dataset with ground truth depth to a new target dataset without ground truth. This allows models to be trained on new data in a self-supervised manner while still predicting absolute depth values. 

- Most prior work on monocular depth estimation focuses on either fully supervised training with ground truth depth or self-supervised training which can only predict relative depth. This paper offers a way to get the benefits of self-supervised training on new data while still getting absolute depth values.

- Compared to other work on transferring depth scale, this paper uses a simpler single global scaling factor rather than methods that estimate per-image scale factors. The results show this global approach works reasonably well. 

- Unlike some other depth scale transfer methods, this approach does not require tailoring synthetic source datasets to match the characteristics of the real target dataset (e.g. using virtual KITTI for KITTI dataset). It demonstrates transferring scale between different real and synthetic datasets.

- The accuracy results are competitive with other depth scale transfer approaches, often matching or exceeding their performance, despite using a simpler scaling factor model.

- The depth accuracy is not as high as fully supervised training on the target dataset itself or state-of-the-art self-supervised methods, but the goal is reasonable accuracy without the need for extra ground truth depth.

- Overall, this depth scale transfer approach offers a useful way to train depth models on new target datasets in a self-supervised manner while predicting absolute depth. The simplicity of the single scaling factor method is a notable advantage compared to other techniques.

So in summary, this paper presents a simple yet effective approach for transferring depth scale across datasets, with competitive accuracy, which helps address a key weakness of self-supervised monocular depth training. It is an incremental improvement over existing techniques that provides a useful addition to the field.


## What future research directions do the authors suggest?

 Based on my reading of the paper, some of the main future research directions suggested by the authors are:

- Developing more advanced architectures and losses for self-supervised monocular depth estimation (MDE) training. The authors note that their relatively simple architecture and standard self-supervised loss leaves room for improvement. More accurate self-supervised MDEs could help reduce variability in the predicted depth scales across images, improving generalization of the single global depth scale correction factor.

- Exploring ways to improve the per-image depth scale estimation using methods like specialized losses for the convolutional network approach they tested. Their global single scale factor performed better than the CNN per-image approach, but they suggest future work on improving the per-image accuracy.

- Applying the depth scale transfer approach to additional datasets beyond KITTI and DDAD to further demonstrate its applicability. The authors claim the method is insensitive to domain gaps, but testing on more diverse data could further validate this.

- Leveraging the decoupling of depth ranking and depth scaling to independently improve each component. For example, alternate losses or architectures could improve depth ranking, while the scaling could remain simple.

- Reducing the variability in depth scale across images to better enable generalization of a single global scale factor. The authors hypothesize this could come from improved self-supervised training.

- Extending the continuous learning benefits of the method by frequently fine-tuning on new data without needing additional ground truth depth measurements.

In summary, the main future directions focus on improving the self-supervised training, generalizing the approach to more domains, independently evolving the depth ranking and scaling components, reducing depth scale variability, and enabling continuous learning. The overall goal is improving monocular depth estimation using only images.


## Summarize the paper in one paragraph.

 The paper proposes a method to transfer the depth scale from existing source datasets with ground truth depth to new target datasets that only have images, allowing monocular depth estimators trained with self-supervision on the target images to predict absolute depth. The key insights are:

1) Monocular depth estimators trained with self-supervision based on projective geometry can linearly rank depths globally across images, though the predicted depths are ambiguous up-to-scale. 

2) When trained on mixed batches of source and target images adjusted to the same field-of-view, the depth estimator learns to rank depths on a common scale across both domains.

3) A single global scaling factor estimated on the source domain can then transfer the depth scale to the target domain.

The method is demonstrated on KITTI and DDAD datasets using various synthetic and real sources, achieving competitive accuracy to supervised methods without requiring ground truth depth for the target images. The global scaling factor avoids altering the base monocular depth estimator architecture. Overall, it enables training on new target images to predict absolute depth by reusing existing source datasets with depth ground truth.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the paper:

This paper presents a method to transfer the depth scale from existing datasets with ground truth depth to monocular depth estimators trained in a self-supervised manner on new target datasets without ground truth depth. Self-supervised training results in depth predictions that are ambiguous in scale. The authors first show that the relationship between predicted and ground truth depth is linear, both within images and across a dataset. They then demonstrate that training on mixed batches of source and target images, after adjusting their fields of view, results in a common depth scale across domains. This enables using the ground truth depth from the source domain to estimate a global depth scaling factor that can be applied to the target domain, transferring the depth scale without needing ground truth depth for the new target images. 

The method is evaluated by training on mixed batches of real-world automotive datasets with different fields of view. A single global scaling factor estimated on the source test set is able to scale the target predictions to absolute depth with accuracy competitive with supervised training on target ground truth. The approach does not require tailored synthetic datasets and works across domain gaps in image style and content. By decoupling depth ranking and scaling, the method provides an effective way to obtain absolute depth predictions for new test images using only a lightweight self-supervised model and existing ground truth data.


## Summarize the main method used in the paper in one paragraph.

 Here is a one paragraph summary of the main method used in the paper:

This paper presents a method for transferring the absolute depth scale from existing source datasets with ground truth depths to depth estimators trained in a self-supervised manner on target datasets with only images. The key insight is that even though self-supervised monocular depth estimators can only predict depths up-to-scale, their predictions are linearly correlated with the ground truth depths. This allows modeling the relationship between predicted and ground truth depths on the source domain using a single global scaling factor. By training the depth network on mixed batches from the source and target domains after aligning their fields-of-view, this global scaling factor can then be applied to scale the up-to-scale depth predictions on the target domain to absolute depths, thereby transferring the depth scale from source to target. The method is demonstrated by training on mixed batches of real and synthetic datasets and then transferring scale to the KITTI and DDAD target datasets, achieving accuracy competitive with supervised training and other depth scale transfer techniques.


## What problem or question is the paper addressing?

 This paper proposes a method for transferring the depth scale from existing datasets with ground truth depth to self-supervised monocular depth estimation models trained on new target datasets without ground truth depth. 

The key problems/questions addressed are:

- Monocular depth estimators trained with self-supervision can only predict depth up to an unknown scale factor. This limits their applicability in real-world tasks like autonomous driving that require absolute depth values. 

- Transferring depth scale from existing datasets to new target datasets is challenging due to differences in image style, camera intrinsics, and scene content.

- Can the relationship between predicted and ground truth depth in an existing source dataset be modeled and transferred to rescale the predicted depth values in a new target dataset?

- Can this be done using a single global scaling factor to avoid increasing computation complexity?

- Does this approach work for transferring depth scale from both synthetic and real source datasets to real target datasets?

So in summary, the paper aims to enable absolute depth prediction on new target datasets by transferring the depth scale from existing source datasets with ground truth depth, using a efficient and robust method that works across both synthetic and real domains.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the abstract and skimming the paper, here are some key terms and concepts:

- Monocular depth estimation - Estimating depth from a single image, without stereo images or other sensors.

- Self-supervised learning - Training a model using only images, without ground truth depth data. Uses view synthesis as supervision. 

- Depth scale ambiguity - Models trained with self-supervision can estimate depth up to an unknown scale factor. 

- Depth scale transfer - Transferring the absolute depth scale from a source dataset with ground truth to a target dataset without ground truth.

- Linear depth ranking - The paper shows depth predictions from self-supervision have a linear relationship to ground truth depths. This enables depth scale transfer.

- Global depth scaling factor - The paper proposes using a single global factor to scale depth predictions to absolute depth, instead of image-specific factors.

- Domain adaptation - Transferring knowledge between datasets collected in different domains (e.g. real and synthetic data).

- Field of view adjustment - Matching the field of view of datasets by cropping images before joint training.

The key ideas are using the linearity of self-supervised depth predictions to do cross-dataset depth scale transfer with a single global scaling factor, and adjusting field of view to enable this transfer between datasets.


## Based on the paper, what questions should we ask to create a comprehensive summary of it? Please list at least 10 questions.

 Here are 10 suggested questions to ask when summarizing this paper:

1. What is the problem that this paper is trying to solve? 

2. What are the limitations of existing methods for monocular depth estimation that this paper addresses?

3. What is the key insight or main contribution of this work?

4. How does the paper propose to transfer depth scale from existing datasets to new self-supervised models? What property of self-supervised training enables this?

5. How is field-of-view handled when training on mixed datasets? Why is this important?

6. How is the global depth scaling factor estimated? What assumptions does this make?

7. What experiments were conducted to evaluate the method? What datasets were used?

8. How does the accuracy of the proposed approach compare to other methods, including fully supervised training?

9. What are the limitations or potential weaknesses of the proposed approach? 

10. What are the key conclusions and potential implications or future work based on this paper?


## Design 10 in-depth questions about method proposed in this paper and avoid simple question.

 Here are 10 potential in-depth questions about the method proposed in this paper:

1. The paper shows that self-supervised monocular depth estimators exhibit a linear relationship between predicted up-to-scale depths and ground truth depths. What might be the underlying reasons that cause this linear depth ranking behavior? Does this indicate any limitations or weaknesses in the self-supervised training methodology?

2. The paper proposes matching the field-of-view between source and target datasets to enable training on mixed batches. Why is this an important step? What problems could arise if the datasets had mismatched fields-of-view? 

3. The global depth scaling factor is calculated as the median predicted depth divided by the median ground truth depth on the source dataset. What are the advantages of using the median compared to the mean or other statistics? How robust is this approach to outliers or poor monocular depth predictions?

4. How does the performance of the global depth scaling factor method compare to computing per-image depth scaling factors? What are the tradeoffs between a global vs per-image approach? When might a per-image approach be preferable?

5. The paper shows depth scale transfer from both synthetic and real source datasets. What are the key challenges and differences when using synthetic vs real source data? How does domain gap affect the depth scale transfer?

6. How does the accuracy of the proposed depth scale transfer method compare to other approaches like online depth scaling or weak supervision? What are the advantages and disadvantages compared to these other techniques?

7. Could the proposed approach be applied to other self-supervised tasks beyond monocular depth estimation? For example, could a similar global scale transfer idea work for optical flow or camera pose estimation?

8. The method uses a relatively simple CNN architecture. How might performance change with more complex models or losses? Would end-to-end training further improve results?

9. The paper focuses on the automotive setting. How well would this approach generalize to other domains like indoor scenes or outdoor terrain? What domain differences could impact the depth scale transfer?

10. The method requires source dataset with ground truth depths. How much source data is needed? Could the approach work with very limited source data or even synthetic data? How does source dataset size impact performance?


## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a summary paragraph of the key points from the paper:

This paper presents a method to transfer the depth scale from datasets with ground truth depth to new datasets without ground truth, enabling monocular depth estimators (MDEs) trained with self-supervision on new data to predict absolute depth. The key insight is that MDEs trained with self-supervision using projective geometry losses produce depth predictions that are linearly correlated with ground truth depth, with this relationship holding globally across images. The authors first adjust source images to match the field-of-view of target images to enable joint self-supervised training. They show this results in a common depth scale across domains regardless of gaps between real/synthetic data. A single global scaling factor modelling the relationship between predicted and ground truth depth on the source domain can then be applied to scale predicted depth on the target domain. Evaluated on KITTI and DDAD, this approach transfers depth scale from both synthetic and real datasets and achieves accuracy competitive with supervised training on target data. A key advantage is decoupling depth ranking from scaling, allowing independent improvement of self-supervision while preserving MDE complexity. The method enables practical continuous fine-tuning of MDEs with new data lacking ground truth depth.


## Summarize the paper in one sentence.

 This paper presents a method to transfer the depth scale from datasets with ground truth depth to self-supervised trained monocular depth estimators on new target datasets without ground truth depth, using the linear ranking property of projective geometry based self-supervised losses and a single global depth scaling factor.


## Summarize the paper in one paragraphs.

 Here is a one paragraph summary of the paper:

This paper presents a method to transfer the depth scale from datasets with ground truth depth to self-supervised monocular depth estimators trained on new image datasets without depth ground truth. The key insight is that depths predicted by self-supervised methods are linearly correlated with ground truth depths, both within and across images. The authors show this holds even when training on mixed datasets after aligning fields of view. They use this to compute a single global depth scaling factor on the source dataset with ground truth, and apply it to scale depth predictions on the target dataset. They demonstrate competitive performance on KITTI and DDAD datasets using various synthetic and real-world sources. Their approach decouples depth ranking from depth scaling, avoids complex adaptations for domain differences, and adds minimal computation cost. Overall, this provides an effective way to obtain absolute depth predictions from self-supervised training on new image datasets lacking ground truth depth.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in this paper:

1. What problem is the paper trying to solve and why is it an important problem? The paper is trying to solve the issue of monocular depth estimators trained with self-supervision only predicting depth up to scale. This limits their applicability in real-world applications like autonomous driving that require absolute depth values.

2. What is the key insight that enables transferring depth scale from a source to target domain? The key insight is that depths predicted in a self-supervised manner have a linear relationship with ground truth depths, and this linearity holds even when training on mixed source and target domains.

3. How does the method align the field-of-view between source and target datasets? The source images' field-of-view is adjusted to match the target by cropping or resizing based on the focal lengths and image widths of the source and target cameras. 

4. What is the proposed global depth scaling factor and how is it estimated? It is a single scalar value modeling the linear relationship between predicted and ground truth depths on the source domain. It is estimated as the median ground truth depth divided by the median predicted depth.

5. Why is proper field-of-view alignment between domains important? Because it maintains geometrical consistency for self-supervised training across domains. Incorrect alignment breaks this and fails to enable correct depth scale transfer.

6. How does the method evaluate that a single global scaling factor is sufficient? By comparing its performance to per-image depth scaling networks and other depth scale transfer methods. The global factor achieves competitive or better accuracy.

7. What are the advantages of using a single global scaling factor? It doesn't increase model complexity or computational costs. The depth ranking and scaling modules are totally decoupled.

8. How does the method handle domain gaps between synthetic and real data? It demonstrates that linear depth ranking holds across synthetic and real domains without using style transfer or intermediate objectives.

9. What are limitations of the method? It may be sensitive to poor self-supervised training since predicted depth quality impacts depth ranking. Advanced losses could help.

10. How can the method be extended? By exploring if better self-supervised training reduces per-image depth ranking variability to improve global scale generalization. Testing on more diverse domains.
