# [Addressing Myopic Constrained POMDP Planning with Recursive Dual Ascent](https://arxiv.org/abs/2403.17358)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Constrained POMDPs are mathematical models for safe planning under uncertainty. Online solvers like CC-POMCP use global Lagrangian dual variables to guide exploration.
- However, global dual variables can lead to myopic (short-sighted) action selection during exploration, causing the agent to miss optimal solutions. This happens because the same dual variable is used globally even though different histories may need different exploration strategies.

Proposed Solution: 
- Introduce local, history-dependent dual variables $\lambda(h)$ that guide action selection separately for each history $h$. These are optimized using recursive dual ascent to penalize local constraint violations.
- This allows each history node to adapt its exploration strategy based on violations in its subtree. As a result, the overall search can better explore globally optimal safe paths.

Contributions:
- Highlight issue of myopic exploration with global dual variables in constrained POMDP solvers
- Propose using recursive dual ascent to optimize history-dependent dual variables for improved exploration
- Empirically demonstrate the approach enables better search efficiency and constraint satisfaction in three problem domains:
   - Discrete Constrained Tiger POMDP
   - Continuous Constrained LightDark POMDP
   - Continuous Constrained Spillpoint POMDP
- Achieve similar cumulative rewards as prior methods while significantly reducing constraint violations

The key insight is that global dual variables can be too myopic during exploration of constrained POMDPs, and locally optimizing dual variables for each history can address this issue and improve overall policy optimization.
