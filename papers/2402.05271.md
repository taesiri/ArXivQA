# [Gradient descent induces alignment between weights and the empirical NTK   for deep non-linear networks](https://arxiv.org/abs/2402.05271)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem Statement:
- Understanding why neural networks generalize well is an important open problem. Prior work proposed the "neural feature ansatz" (NFA) which states that the gram matrix of the weights (neural feature matrix or NFM) becomes aligned with the average gradient outer product (AGOP) during training. However, the reasons behind this alignment are not well understood. 

Main Contributions:
- Show that the NFA is equivalent to alignment between the weight matrices and the pre-activation tangent kernel (PTK), which is related to the neural tangent kernel (NTK). So NFA reflects weights aligning to the PTK rather than the PTK aligning to the weights (Section 2.1).

- Introduce a centered version of the NFA (C-NFA) which isolates the alignment of weights to the PTK. Show theoretically and empirically that early C-NFA dynamics are driven by weights aligning to the initial PTK, not the PTK changing (Sections 2.2-2.3).

- Derive analytical predictions for early C-NFA dynamics on specialized datasets in terms of statistics of the data distribution. Demonstrate numerically that predictions match observed values (Section 3).

- Introduce "speed fixing" optimization technique that increases contribution of C-NFA to NFA. Show this improves feature learning and generalization across initializations (Section 4).

Main implications:
- NFA corresponds to weight-PTK alignment rather than just spectral properties of these matrices. C-NFA isolates the weight-PTK alignment and its strength can be manipulated.

- Understanding drivers of NFA and designing methods to improve alignment could lead to performance gains, providing path towards rational neural network design.
