# [Neural network layers as parametric spans](https://arxiv.org/abs/2208.00809)

## What is the main contribution of this paper?

The main contribution of this paper is presenting a general categorical framework for defining and studying linear layers in neural networks, based on the notions of integration theory and parametric spans. Specifically, the paper:- Introduces the concept of Frobenius integration theory, which generalizes integration and allows defining integration theories on arbitrary categories. - Shows how parametric spans can represent the structure of neural network layers with locality and weight sharing.- Demonstrates how this framework can express various types of layers, including dense, convolutional, and geometric deep learning layers.- Proves that the framework guarantees existence and computability of derivatives needed for backpropagation.In summary, the paper provides a flexible categorical setting to define, analyze and generalize linear neural network layers and their differentiation rules in a unified way. This allows encompassing many existing layer architectures while ensuring computational properties needed for learning.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper presents a categorical framework based on integration theories and parametric spans to define and study linear layers in neural networks, encompassing classical architectures like dense and convolutional layers while ensuring computability of derivatives.


## How does this paper compare to other research in the same field?

Here are a few thoughts on how this paper compares to other research on defining and formalizing neural network layers:- The use of category theory and diagrammatic reasoning to define layers is novel. Most prior theoretical work on neural networks does not take this categorical perspective. The definitions of integration theory and parametric spans provide a flexible framework for describing many types of layers in a unified way.- The focus on computing derivatives and reverse-mode differentiation is important for machine learning applications, but not always present in mathematical treatments of neural networks. The authors design their framework so that derivatives are computable by reordering the legs of a parametric span, which is elegant. - Making connections to geometric deep learning is valuable, as many ideas from that field have not been fully incorporated into the mainstream neural network literature. Representing notions like neighborhoods and coordinate charts categorically as in Diagram 3 is insightful.- Overall, this paper takes a broad, foundational perspective compared to papers that focus on a specific architecture like CNNs or Graph Neural Networks. The goal is to develop general mathematical tools that could encompass both existing and novel layers.- A downside is that the formulations, while general, remain quite abstract. More work would be needed to connect this to practical deep learning, like analyzing convergence behavior during training.- Compared to other categorical perspectives like in Spivak's work, this paper is focused narrowly on layers rather than networks as a whole. But it develops the layer viewpoint thoroughly and links it to integration theory.In summary, this paper stakes out new territory in the mathematical foundations of deep learning using category theory. It provides conceptual insight, but further development would be needed to apply these ideas to real machine learning systems. The novel perspective is a strength compared to prior work.
