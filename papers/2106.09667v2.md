# [Poisoning and Backdooring Contrastive Learning](https://arxiv.org/abs/2106.09667v2)

## What is the central research question or hypothesis that this paper addresses?

 The central research question this paper addresses is whether multimodal contrastive learning methods like CLIP are vulnerable to poisoning and backdoor attacks. The authors hypothesize that because these models train on large, noisy, uncurated datasets scraped from the internet, they are more susceptible to such attacks compared to models trained on clean, curated datasets.

The paper tests this hypothesis by developing poisoning and backdoor attacks against CLIP and evaluating their effectiveness. The key findings are:

- Targeted poisoning attacks succeed by injecting as few as 3 poisoned examples into a 3 million image dataset. This is orders of magnitude fewer than required for similar attacks on supervised learning models.

- Backdoor attacks succeed by poisoning less than 0.01% of the training data. Again, this is significantly less than prior attacks on supervised learning models which often require poisoning ~1% of the data. 

- The attacks are effective across different model sizes, dataset sizes, and levels of poisoning. The paper comprehensively evaluates how these factors impact attack success.

In summary, the central hypothesis is that multimodal contrastive learning models like CLIP are highly vulnerable to poisoning and backdoor attacks due to their uncurated training data. The experiments confirm this vulnerability, with attacks succeeding using exceptionally small amounts of poisoning.


## What is the main contribution of this paper?

 Based on my reading, the main contributions of this paper are:

- It shows that multimodal contrastive learning models like CLIP are vulnerable to poisoning and backdoor attacks, requiring the modification of far fewer training examples compared to attacks on supervised models. 

- It adapts existing poisoning and backdoor attack techniques to successfully attack contrastive learning models trained on noisy, uncurated datasets scraped from the internet.

- It empirically evaluates the effectiveness of these attacks, demonstrating that poisoning attacks can succeed by controlling just 0.0001% of a large dataset. Backdoor attacks are also shown to work at very low poisoning rates of 0.01%.

- It performs an ablation study to understand why these attacks are effective, showing factors like model size, dataset size, patch size, etc. impact attack success rates.

- It makes the argument that as models are trained on noisier, less curated data in the future, poisoning attacks are likely to become more practical and bigger threats. This motivates the need for defenses tailored to contrastive learning.

In summary, the main contribution is demonstrating the feasibility of poisoning and backdoor attacks against contrastive learning models trained on uncurated internet data, which poses a realistic threat due to the tiny fraction of data an adversary needs to control. The paper comprehensively evaluates these attacks and analyzes what makes them effective.
