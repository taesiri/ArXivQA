# [Efficient Passage Retrieval with Hashing for Open-domain Question   Answering](https://arxiv.org/abs/2106.00882)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question seems to be:

How can we reduce the memory cost of dense passage retrievers like DPR while maintaining high accuracy on open-domain question answering tasks?

The paper proposes a new model called Binary Passage Retriever (BPR) that aims to address this question. The key ideas are:

- Using a learning-to-hash technique to encode passages into compact binary codes rather than dense vectors. This drastically reduces the memory needed to store the passage index.

- A two-stage retrieval approach that first does efficient candidate generation based on binary codes, then accurate reranking using dense embeddings. This maintains accuracy while improving search speed. 

- A multi-task training objective that simultaneously optimizes the model for both binary code search and dense vector reranking.

So in summary, the central hypothesis is that BPR can reduce the memory cost of dense retrievers substantially through the use of binary hashing while achieving similar accuracy by still leveraging dense vectors in a two-stage retrieval process. The experiments aim to validate whether this approach works in practice.


## What is the main contribution of this paper?

 The main contribution of this paper is the proposal of a new neural retriever model called Binary Passage Retriever (BPR). The key ideas are:

- BPR uses a learning-to-hash technique to encode passages into compact binary codes instead of dense vectors. This allows reducing the memory footprint of the passage index from 65GB to 2GB.

- BPR is trained with a multi-task objective over two tasks: efficient candidate generation based on binary codes, and accurate reranking based on dense vectors. This allows improving search efficiency while maintaining accuracy.

- Experiments on Natural Questions and TriviaQA datasets show that BPR achieves similar accuracy as Dense Passage Retriever while using substantially less memory. With an improved reader model, it achieves competitive results with state-of-the-art on open-domain QA.

In summary, the main contribution is a new passage retriever model that can drastically reduce the memory requirements for large-scale open-domain QA systems while maintaining accuracy. The key ideas are learning compact binary codes for passages, and using a two-stage approach with multi-task training.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the paper: 

The paper introduces a memory-efficient neural retrieval model called Binary Passage Retriever (BPR) that uses compact binary codes to represent passages, allowing for substantial compression of the passage index without losing accuracy compared to Dense Passage Retriever.


## How does this paper compare to other research in the same field?

 Here are a few key ways this paper compares to other research in open-domain question answering:

- It builds on recent work like the Dense Passage Retriever (DPR) that uses dual-encoder architectures with BERT to encode questions and passages into a shared vector space. However, it focuses on reducing the memory requirements of these dense retrieval models.

- The main contribution is a novel Binary Passage Retriever (BPR) model that compresses the passage embeddings using learning to hash. This allows the index to be stored in a very compact binary form rather than dense vectors. 

- BPR achieves similar accuracy as DPR on QA datasets like Natural Questions and TriviaQA, while reducing the index size from 65GB down to 2GB. This demonstrates the effectiveness of the hashing approach for efficiency.

- For reranking, BPR retrieves a small set of candidates using efficient Hamming distance search on the binary codes, then reranks using the inner product of dense vectors. This two-stage approach balances efficiency and accuracy.

- When coupled with a reader model, BPR achieves competitive results compared to state-of-the-art open domain QA systems like REALM and Fusion-in-Decoder. It gets close to SoTA with fewer parameters.

- The learning to hash technique differentiates this from other work on compressing BERT models, as most rely on product quantization or pruning. Hashing allows extreme compression suitable for passage retrieval.

- Overall, this paper makes an important contribution of dramatically reducing the memory requirements of dense retrieval systems without sacrificing accuracy. The innovations could make QA systems much more scalable.


## What future research directions do the authors suggest?

 Based on my reading of the paper, some of the main future research directions suggested by the authors are:

- Exploring different neural network architectures and training techniques for the retriever and reader models. The authors mention that exploring different encoders like ALBERT or T5 could be promising.

- Improving cross-lingual retrieval by adapting their approach to multilingual models like mBERT. 

- Applying their approach to other domains beyond Wikipedia, such as scientific articles, web pages, etc. Scaling up the knowledge source can help improve QA performance.

- Combining their approach with knowledge-enhanced pretraining methods like REALM to inject knowledge into the reader model.

- Adapting their approach to conversational QA settings where context needs to be maintained across multiple questions.

- Exploring ways to reduce the discrepancy between training and inference mechanisms, for example by incorporating hard negatives mining during training.

- Developing methods to explain and analyze the behavior of the retriever and reader models.

So in summary, the main suggested directions are around model architectures, scaling up to larger data, adapting to new domains/languages, combining with knowledge-enhanced pretraining, conversational QA, hard negative mining, and model analysis. Improving both the retriever and reader components is emphasized.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the paper:

The paper introduces Binary Passage Retriever (BPR), a memory-efficient neural retrieval model for open-domain question answering. BPR integrates a learning-to-hash technique into Dense Passage Retriever (DPR) to represent the passage index using compact binary codes rather than continuous vectors. This allows for substantial reduction in memory cost compared to DPR. BPR is trained with a multi-task objective over two tasks - efficient candidate generation based on binary codes and accurate reranking based on continuous vectors. At test time, candidate passages are first retrieved based on Hamming distance of binary codes, then reranked using inner product of continuous vectors. Experiments on Natural Questions and TriviaQA benchmarks show BPR achieves similar accuracy as DPR while reducing memory cost from 65GB to 2GB. The authors also demonstrate competitive results compared to state-of-the-art open-domain QA models when using an improved reader.


## Summarize the paper in two paragraphs.

 Here is a summary of the paper in two paragraphs:

The paper proposes a new neural retriever model called Binary Passage Retriever (BPR) for open-domain question answering. BPR is built on top of Dense Passage Retriever (DPR) and introduces a learning-to-hash technique to reduce the memory cost of the passage index. Specifically, BPR computes compact binary codes for both questions and passages using hash functions learned in an end-to-end manner jointly with the encoders. This allows storing the index as binary codes rather than continuous vectors, reducing the memory usage from 65GB to 2GB. 

To maintain accuracy while improving efficiency, BPR employs a two-stage approach with candidate generation and reranking. It first retrieves a small set of candidate passages using efficient Hamming distance computation on the binary codes. It then reranks these candidates by computing more expensive inner products on the original continuous embeddings. Experiments on Natural Questions and TriviaQA benchmarks show that BPR achieves similar accuracy to DPR while requiring substantially less memory. With further improvements to the reader model, it achieves results competitive with state-of-the-art on open-domain QA.


## Summarize the main method used in the paper in one paragraph.

 Here is a one paragraph summary of the main method used in the paper:

The paper introduces Binary Passage Retriever (BPR), a memory-efficient neural retrieval model for open-domain question answering. BPR is an extension of Dense Passage Retriever (DPR) that uses a learning-to-hash technique to represent the passage index with compact binary codes instead of continuous vectors. Specifically, BPR adds hash layers on top of the question and passage encoders of DPR to compute binary codes by applying the sign function to the continuous embeddings. Since the sign function is incompatible with backpropagation, it is approximated with the scaled tanh function during training. BPR is trained with a multi-task objective over two tasks: efficient candidate generation based on Hamming distance using the binary codes, and accurate reranking based on inner product using the continuous embeddings. This allows BPR to substantially reduce the memory cost of DPR without losing accuracy. At test time, candidate passages are first retrieved based on Hamming distance between question and passage binary codes, then reranked using inner product between question and passage continuous embeddings.


## What problem or question is the paper addressing?

 Based on my reading of the paper, the key problem the authors are trying to address is the high memory cost of dense passage retriever models like DPR for open-domain question answering. Specifically, the continuous vector representations used by DPR to encode passages result in very large indexes that need to be stored in memory at runtime. For example, encoding Wikipedia passages requires 65GB of memory with DPR. The authors propose a method to substantially reduce this memory cost without sacrificing accuracy.

The main question they are addressing is: how can we compress the passage indexes in dense retriever models like DPR in order to reduce memory usage, while maintaining high accuracy on open-domain QA tasks?

In summary, the key problem is the high memory cost of passage indexes in state-of-the-art dense retriever models, and the main question is how to reduce this memory cost without hurting accuracy. The authors propose a learning-to-hash based approach called Binary Passage Retriever (BPR) to address this problem.
