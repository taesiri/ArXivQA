# [Too Large; Data Reduction for Vision-Language Pre-Training](https://arxiv.org/abs/2305.20087)

## What is the central research question or hypothesis that this paper addresses?

 The central research question this paper aims to address is whether using larger-scale datasets always leads to better performance for vision-language pre-training (VLP) models. 

The key findings and contributions of this paper are:

- They show that simply using more data does not always improve VLP model performance. By removing low image-text matching score samples from CC3M, they find model performance on COCO retrieval actually improves slightly. This suggests issues like image-text misalignment exist in VLP datasets.

- They propose TL;DR, an efficient algorithm to compress large VLP datasets into smaller, high-quality subsets. It uses a codebook-based captioner to select representative samples, and generates new captions to reduce text-image misalignment. 

- TL;DR is able to significantly compress datasets like CC3M (from 2.8M to 0.67M samples), YFCC15M (from 15M to 2.5M) and LAION-400M (from 40M to 8M) while maintaining strong performance on downstream tasks.

- Experiments with CLIP, ViLT and BLIP show models trained on TL;DR's compressed datasets match or outperform models trained on the full datasets across 7 downstream tasks.

- Their key conclusion is that large datasets are not always better for VLP. Instead, data efficiency should be considered, and their TL;DR method enables effective VLP pretraining with fewer samples.

In summary, the main hypothesis is that larger VLP datasets do not automatically lead to better performance due to issues like misalignment. The TL;DR algorithm is proposed to address this by compressing datasets while maintaining performance.


## What is the main contribution of this paper?

 The main contribution of this paper is proposing TL;DR, an efficient and straightforward algorithm to compress large-scale vision-language pre-training (VLP) datasets into smaller, high-quality subsets. The key ideas are:

1. A codebook-based captioner is developed, consisting of a visual encoder, a learnable codebook, and a text decoder. This is used to select representative samples from the original dataset by clustering images based on similarity between image embeddings and the codebook entries.

2. For the selected samples, the text captions are refined using the captioner's text decoder to reduce image-text misalignment. 

3. Extensive experiments show that pre-training VLP models like CLIP, ViLT, and BLIP on the compressed datasets generated by TL;DR leads to similar or better performance compared to the original full datasets on a range of downstream tasks, while requiring significantly less pre-training time and data storage.

4. Analysis of the compressed datasets shows TL;DR is able to effectively address the image-text misalignment issue in the original VLP datasets.

In summary, TL;DR provides an efficient way to create small, high-quality VLP datasets that lead to strong downstream task performance, helping democratize VLP research by reducing computational requirements. The simple yet effective technique and analysis of the image-text misalignment problem are the main contributions.
