# [On the Convergence of Adam under Non-uniform Smoothness: Separability   from SGDM and Beyond](https://arxiv.org/abs/2403.15146)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem Statement:
- The paper aims to provide a theoretical comparison between the convergence rates of the Adam optimizer and Stochastic Gradient Descent with Momentum (SGDM) under the condition of non-uniform smoothness. 
- Prior works have shown SGDM can match lower bounds on convergence rates under common assumptions like uniform smoothness. However, Adam often performs better empirically, presenting a gap in understanding.  
- Recent works observe that neural network training violates the uniform smoothness assumption, motivating the use of non-uniform smoothness concepts like (L0,L1)-smoothness. But existing convergence results for Adam under such settings have limitations.

Proposed Solution:
- The paper provides an improved analysis of Adam's convergence rate under (L0,L1)-smoothness in deterministic and stochastic settings.  
- For deterministic case, Adam's rate matches known lower bounds while a new lower bound for Gradient Descent with Momentum (GDM) shows worse dependency on initial function value gap. This separates Adam and GDM.
- For stochastic setting, Adam's rate matches known lower bounds on both initial function value and final error. In contrast, examples show SGDM can fail to converge irrespective of hyperparameters. This separates Adam and SGDM.

Main Contributions:  
- First work clearly distinguishing convergence rates of Adam and (S)GDM under non-uniform smoothness. Shows Adam's faster convergence.
- Establishes tight problem-dependent convergence rates for Adam matching known lower bounds. Introduces novel stopping time technique.
- Demonstrates the stopping time technique can prove Adam with specific scheduler is parameter-agnostic. The technique is of independent interest.

In summary, the paper provides valuable new theoretical results that help explain Adam's strong empirical performance over SGDM. The analyses also reveal techniques like stopping times that can enable tighter understanding of adaptive methods.
