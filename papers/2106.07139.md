# Pre-Trained Models: Past, Present and Future

## What is the central research question or hypothesis that this paper addresses?

Based on my analysis, the central research question addressed in this paper is: How to develop effective pre-trained models (PTMs) for natural language processing, given the recent success of large-scale PTMs like BERT and GPT?The key points are:- PTMs have achieved great success in NLP recently, showing impressive performance on many tasks. However, there are still many open problems related to developing better PTMs.- The paper provides a comprehensive review of the history, current progress, and future directions of PTMs. - It discusses how to design more effective architectures, utilize richer context, improve computational efficiency, conduct interpretation and theoretical analysis of PTMs.- The authors aim to integrate the current development of PTMs into the full spectrum of AI and point out promising research directions to inspire and advance future PTM research.In summary, this paper aims to provide a holistic understanding of PTMs and discuss open challenges as well as future opportunities to develop more capable and efficient PTMs for natural language processing.


## What is the main contribution of this paper?

Based on my understanding, the main contributions of this paper are:1. It provides a comprehensive review of the development of pre-trained models (PTMs) in artificial intelligence, especially in natural language processing. The paper traces the history of pre-training from early transfer learning to current self-supervised learning based methods. 2. It categorizes and summarizes the latest breakthroughs in PTMs into four directions - model architectures, utilizing rich contexts, improving efficiency, and analysis & interpretation. For each direction, the paper introduces the latest representative work in detail.3. It discusses a series of open problems and future directions for PTMs. The key topics include new model architectures, pre-training tasks, multilingual & multimodal pre-training, computational efficiency, theoretical foundation, knowledge learning, cognitive learning, and applications.4. The paper connects the current development of PTMs with their history, reveals the nature and position of PTMs in AI research, and provides insights on the promising future directions. It can serve as a good reference for researchers and practitioners working on pre-trained models and transfer learning.In summary, this paper provides a comprehensive review of PTMs, summarizes the current progress, and discusses open problems and future directions. It integrates the current development of PTMs into the context of historical spectrum, and offers valuable insights and guidance for advancing PTMs research.
