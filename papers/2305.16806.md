# [Do GPTs Produce Less Literal Translations?](https://arxiv.org/abs/2305.16806)

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper addresses is whether large language models (LLMs) like GPT produce less literal translations compared to standard neural machine translation (NMT) models. The key hypothesis is that GPT translations tend to be less literal, particularly when translating idiomatic expressions out of English into other languages.Some key points:- The paper investigates differences in literalness of translations from GPT models vs NMT systems using measures based on word alignment and monotonicity. - It finds GPT models produce less literal translations from English into German, Chinese and Russian.- This is verified through human evaluations showing GPT translations rated as less literal.- The paper shows this difference is especially pronounced for sentences containing idiomatic expressions, which GPTs are better able to translate figuratively.- Overall, the central hypothesis is that GPTs produce less literal translations from English, particularly for idiomatic expressions, compared to standard NMT systems. The paper provides evidence to support this through automatic metrics and human evaluation.


## What is the main contribution of this paper?

The main contribution of this paper is an investigation into how the translations produced by large language models (LLMs) like GPT-3 differ qualitatively from those produced by standard neural machine translation (NMT) models, with a specific focus on literalness. The key findings are:- Using measures of literalness based on word alignment and monotonicity, the authors find that LLM translations out of English tend to be less literal than NMT translations, while achieving similar or better scores on common MT evaluation metrics.- This difference in literalness is especially pronounced for sentences containing idiomatic expressions, which LLMs are more likely to translate figuratively rather than literally.- The authors demonstrate through both automatic metrics and human evaluations that LLM translations are rated as less literal, particularly for English-to-X language directions.- They posit that the less literal nature of LLM translations may arise from biases in the training data and objectives of LLMs versus NMT models.So in summary, the main contribution is providing an empirical characterization of how LLM translations differ in literalness, demonstrating they tend to be more abstractive and figurative, especially for idiomatic language. This sheds light on the strengths of few-shot LLM translation.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

The paper investigates whether large language models (LLMs) like GPT produce less literal translations compared to neural machine translation (NMT) systems. Using measures of word alignment and monotonicity, it finds GPT translations out of English tend to be less literal while maintaining similar quality, especially for sentences with idioms. The main conclusion is that GPTs exhibit greater "figurative compositionality", able to better translate non-literal meaning like idioms compared to more literal NMT systems.


## How does this paper compare to other research in the same field?

Here are a few key ways this paper on translation literalness compares to other related work:- Focus on literalness: This paper specifically examines how literal the translations from large language models (LLMs) like GPT are, compared to neural machine translation (NMT) models. Most prior work has focused more on overall translation quality rather than literalness.- Use of both automatic metrics and human evaluation: To measure literalness, the authors propose two automatic metrics based on word alignment and monotonicity. But they also validate these metrics via human evaluation, which provides stronger validation. Other papers often rely on just automatic metrics.- Analysis of idioms: A key part of the analysis looks at how well idioms are translated, since idioms require less literal translation. Investigating performance on idioms specifically provides additional insight into model literalness. Many papers do not analyze performance on idiomatic language.- Comparing LLMs to NMT: There has been limited analysis comparing the outputs of large pretrained language models like GPT-3 to standard neural machine translation models. This direct comparison on a key property like literalness is novel.- Multilingual: The analysis looks at English translated to several target languages (German, Chinese, Russian), rather than just a single language pair. This provides more robust conclusions about literalness across languages.Overall, while there has been some related work analyzing literalness and improvements from LLMs, this paper provides a thorough investigation across metrics, models, and languages. The specific focus on literalness and idioms is fairly unique and provides new insights into differences between NMT and LLM translation qualities.


## What future research directions do the authors suggest?

Based on my reading of the paper, some of the main future research directions the authors suggest are:- Further investigating the potential causes for GPTs producing less literal translations compared to NMT systems, such as differences in training objectives and datasets. The authors list some hypotheses but more analysis could help better understand the underlying reasons.- Expanding the analysis to more language pairs beyond English-German/Chinese/Russian. The authors note their findings are focused on translation out of English, so it would be interesting to see if similar patterns hold for other language pairs. - Developing better automatic metrics to quantify translation literalness. The authors note current literalness metrics are limited, so proposing more reliable metrics correlated with literalness could enable more robust analysis.- Analyzing whether fine-tuning or different prompting strategies for LLMs can influence the literalness of their translations. The authors briefly note this possibility but do not extensively explore it in the current work.- Conducting deeper investigation specifically into the translation of idiomatic expressions by LLMs vs NMT systems. The authors provide some initial analysis but suggest further work could be done in this area.- Exploring other dimensions of qualitative difference between LLM and NMT translations beyond just literalness. The authors focus on literalness but other linguistic properties could be compared.So in summary, the main suggested future directions are developing better metrics and models, expanding to more languages, and analyzing other potential qualitative differences between LLM and NMT translations. The translation of idioms is called out as a specific phenomenon of interest for further analysis as well.


## Summarize the paper in one paragraph.

Here is a one paragraph summary of the paper:This paper investigates how the translations generated by large language models (LLMs) like GPT-3 differ qualitatively from those produced by neural machine translation (NMT) systems. Specifically, the authors examine the literalness of translations using measures like word alignment and monotonicity. They find that GPT models tend to produce less literal translations out of English compared to NMT systems, while still achieving similar or better scores on common machine translation metrics. This is especially true for sentences containing idiomatic expressions, which GPTs are better able to translate figuratively rather than literally. Overall, the paper provides evidence that the translation capabilities of LLMs arise from different computational mechanisms compared to NMT systems, imparting LLMs with greater abstractive abilities that allow them to avoid overly literal translations.


## Summarize the paper in two paragraphs.

Here is a two paragraph summary of the paper:The paper investigates how translations generated by large language models (LLMs) like GPT-3 differ qualitatively from translations produced by standard neural machine translation (NMT) models. Specifically, the authors examine whether LLM translations exhibit less literalness compared to NMT translations. Literalness refers to how closely the translation mirrors the source text word-for-word. The authors find evidence that LLM translations from English to German, Chinese, and Russian tend to be less literal than NMT translations in the same language pairs. They show this using automatic metrics of literalness based on word alignment and monotonicity, as well as human evaluations. The paper also demonstrates that LLM translations are especially less literal for sentences containing idiomatic expressions, suggesting the models are better able to capture figurative meaning. Overall, the findings indicate LLMs produce more natural, less literal translations compared to current NMT systems. This offers insights into the different translation capabilities of LLMs versus NMT models.


## Summarize the main method used in the paper in one paragraph.

Here is a one paragraph summary of the main method used in the paper:The paper proposes and investigates two automatic measures - Unaligned Source Words (USW) and Translation Non-Monotonicity (NM) - to quantify the literalness of machine translations. USW measures the number of source words left unaligned after word-aligning the source sentence and its translation, with more unaligned words indicating a less literal translation. NM measures the deviation from monotonicity in the word alignments, with more deviation indicating less literalness. The paper applies these metrics to compare translations from GPT models and NMT systems on English-German, English-Chinese and English-Russian WMT datasets. It finds GPT models produce translations with significantly higher USW and NM scores, suggesting they generate less literal translations compared to NMT systems. The paper also conducts human evaluations to verify this conclusion. Overall, the main method is using automatic alignment-based metrics USW and NM to quantify and compare the literalness of translations from GPT versus NMT systems.
