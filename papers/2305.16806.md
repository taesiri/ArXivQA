# [Do GPTs Produce Less Literal Translations?](https://arxiv.org/abs/2305.16806)

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper addresses is whether large language models (LLMs) like GPT produce less literal translations compared to standard neural machine translation (NMT) models. The key hypothesis is that GPT translations tend to be less literal, particularly when translating idiomatic expressions out of English into other languages.Some key points:- The paper investigates differences in literalness of translations from GPT models vs NMT systems using measures based on word alignment and monotonicity. - It finds GPT models produce less literal translations from English into German, Chinese and Russian.- This is verified through human evaluations showing GPT translations rated as less literal.- The paper shows this difference is especially pronounced for sentences containing idiomatic expressions, which GPTs are better able to translate figuratively.- Overall, the central hypothesis is that GPTs produce less literal translations from English, particularly for idiomatic expressions, compared to standard NMT systems. The paper provides evidence to support this through automatic metrics and human evaluation.


## What is the main contribution of this paper?

The main contribution of this paper is an investigation into how the translations produced by large language models (LLMs) like GPT-3 differ qualitatively from those produced by standard neural machine translation (NMT) models, with a specific focus on literalness. The key findings are:- Using measures of literalness based on word alignment and monotonicity, the authors find that LLM translations out of English tend to be less literal than NMT translations, while achieving similar or better scores on common MT evaluation metrics.- This difference in literalness is especially pronounced for sentences containing idiomatic expressions, which LLMs are more likely to translate figuratively rather than literally.- The authors demonstrate through both automatic metrics and human evaluations that LLM translations are rated as less literal, particularly for English-to-X language directions.- They posit that the less literal nature of LLM translations may arise from biases in the training data and objectives of LLMs versus NMT models.So in summary, the main contribution is providing an empirical characterization of how LLM translations differ in literalness, demonstrating they tend to be more abstractive and figurative, especially for idiomatic language. This sheds light on the strengths of few-shot LLM translation.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

The paper investigates whether large language models (LLMs) like GPT produce less literal translations compared to neural machine translation (NMT) systems. Using measures of word alignment and monotonicity, it finds GPT translations out of English tend to be less literal while maintaining similar quality, especially for sentences with idioms. The main conclusion is that GPTs exhibit greater "figurative compositionality", able to better translate non-literal meaning like idioms compared to more literal NMT systems.


## How does this paper compare to other research in the same field?

Here are a few key ways this paper on translation literalness compares to other related work:- Focus on literalness: This paper specifically examines how literal the translations from large language models (LLMs) like GPT are, compared to neural machine translation (NMT) models. Most prior work has focused more on overall translation quality rather than literalness.- Use of both automatic metrics and human evaluation: To measure literalness, the authors propose two automatic metrics based on word alignment and monotonicity. But they also validate these metrics via human evaluation, which provides stronger validation. Other papers often rely on just automatic metrics.- Analysis of idioms: A key part of the analysis looks at how well idioms are translated, since idioms require less literal translation. Investigating performance on idioms specifically provides additional insight into model literalness. Many papers do not analyze performance on idiomatic language.- Comparing LLMs to NMT: There has been limited analysis comparing the outputs of large pretrained language models like GPT-3 to standard neural machine translation models. This direct comparison on a key property like literalness is novel.- Multilingual: The analysis looks at English translated to several target languages (German, Chinese, Russian), rather than just a single language pair. This provides more robust conclusions about literalness across languages.Overall, while there has been some related work analyzing literalness and improvements from LLMs, this paper provides a thorough investigation across metrics, models, and languages. The specific focus on literalness and idioms is fairly unique and provides new insights into differences between NMT and LLM translation qualities.
