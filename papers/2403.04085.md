# [Don't Blame the Data, Blame the Model: Understanding Noise and Bias When   Learning from Subjective Annotations](https://arxiv.org/abs/2403.04085)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Recent work has raised concerns about potential harms from aggregating labels in subjective annotation tasks where there are inherent disagreements among human annotators. 
- Existing automated data evaluation methods like Data Maps define noise based on models trained on aggregated labels, showing low confidence on high-disagreement instances. 
- This paper investigates whether low model confidence correlates with human disagreement on instances, and whether learning from raw annotations instead of aggregates improves confidence.

Methods:
- Use 3 toxicity detection datasets with raw annotations and measure human disagreement via annotator agreement. 
- Train Single-GT models on aggregated majority vote labels. Measure model confidence and variability over epochs with Data Maps. 
- Train Multi-GT model (DisCo) on raw annotations, adapting Data Maps to measure confidence and variability per annotation.

Results:
- Strong correlation between human disagreement and low Single-GT model confidence.
- Overall, DisCo also shows higher confidence when annotations agree with majority, lower for disagrees.
- But for Single-GT low confidence instances, DisCo shows higher confidence on minority disagrees.
- Suggests Multi-GT models can extract useful signals from subjective tasks that Single-GT models miss.

Contributions:
- Shows low confidence instances in Single-GT models are not necessarily unusable mislabels.
- Demonstrates Multi-GT models' effectiveness in learning from raw annotations, even minority ones discarded during aggregation.
- Advocates for modeling approaches that preserve multiple perspectives in subjective tasks.

Let me know if you need any part of the summary expanded or clarified further.


## Summarize the paper in one sentence.

 The paper investigates whether learning from multiple annotator perspectives (multi-ground truth models) improves model confidence on samples with high human label disagreement that are typically deemed noisy or hard to learn when using a single aggregated label.


## What is the main contribution of this paper?

 Based on my reading of the paper, the main contribution is:

The paper investigates whether learning from multiple annotations (Multi-GT models) for subjective tasks helps improve model confidence on instances that are typically hard to learn when using a single aggregated label (Single-GT models). The key findings are:

1) There is a correlation between human disagreement on instances and lower model confidence when using Single-GT models. Highly subjective instances with greater human disagreement tend to have lower model confidence.

2) When using Multi-GT models that learn from multiple annotations, there is higher model confidence on minority label instances that are usually discarded during aggregation into a single label. This suggests Multi-GT models are able to extract useful signals from subjective instances with disagreement.

3) Multi-GT models show improved confidence on instances that are hard to learn for Single-GT models. This indicates that learning from multiple annotations helps for highly subjective instances, rather than simply considering them as mislabeled examples.

In summary, the main contribution is demonstrating the effectiveness of Multi-GT approaches over Single-GT models for classification of subjective texts, especially on instances characterized by human disagreement. The paper argues against discarding such instances as noise, and shows learning from multiple annotations is beneficial.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and concepts include:

- Noise and bias in annotated datasets
- Single ground truth (Single-GT) models 
- Multiple ground truths (Multi-GT) models
- Subjective tasks with annotator disagreements
- Model confidence 
- Training dynamics 
- Data Maps
- Annotator agreement levels
- Hard-to-learn instances
- Capturing multiple perspectives

The paper explores the relationship between noise/bias and model confidence when learning from aggregated labels versus raw annotations containing disagreements. It compares Single-GT and Multi-GT models in terms of confidence on high-disagreement instances. Key concepts revolve around dataset evaluation, uncertainty estimation, and learning from multiple annotator perspectives.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in this paper:

1. The paper proposes using multiple ground truth (Multi-GT) models rather than single ground truth (Single-GT) models for classifying subjective tasks. Can you explain in detail the differences between Single-GT and Multi-GT models and why the authors argue Multi-GT models are more suitable for subjective tasks?

2. The paper examines the correlation between model confidence and human disagreement on instances. Can you walk through the precise methodology used to compute model confidence and quantify human disagreement? What metrics were used?

3. The authors find that model confidence decreases as human disagreement increases when using Single-GT models. What reasons do they propose could lead to this observed correlation? Explain the reasoning.  

4. How exactly is the Multi-GT model DisCo designed to handle multiple annotations for the same text instance? Explain its architecture and objectives in detail. 

5. When analyzing DisCo, the paper computes confidence and variability differently than for Single-GT models. Can you explain how these metrics are adapted for the Multi-GT case and why this difference makes sense theoretically?

6. For DisCo, the paper examines model confidence separately for annotations that agree vs disagree with the majority vote. What key observations emerge from this analysis in terms of DisCo's capabilities? Explain the trends noticed.

7. Focusing on low confidence instances for Single-GT models, how does DisCo's confidence differ for minority vote annotations vs majority vote annotations? What inferences can be made about DisCo from this result?

8. To evaluate DisCo's ability to learn multiple perspectives, what visualization and analysis does the paper provide regarding high confidence predictions across various labels for the same text? What limitations emerge?

9. What role does the average number of annotations per annotator play in the effectiveness of DisCo? The paper highlights this as an important consideration - elaborate on why this factor matters.

10. What broader implications does the paper highlight in terms of the value of preserving multiple perspectives through annotations for subjective classification tasks? Summarize the higher-level insights gained.
