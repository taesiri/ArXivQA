# [Debiased Machine Learning and Network Cohesion for Doubly-Robust   Differential Reward Models in Contextual Bandits](https://arxiv.org/abs/2312.06403)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a high-quality paragraph summarizing the key points of the paper:

This paper proposes a novel contextual bandit algorithm called DML Thompson Sampling with Nearest Neighbor Regularization (DML-TS-NNR) for learning optimal mobile health intervention policies. The key innovations are: (1) it uses the Double Machine Learning framework to flexibly model complex, nonlinear baseline rewards while avoiding overfitting; (2) it employs network regularization to efficiently pool information across both individuals and time to learn time-varying treatment effects; and (3) it provides theoretical regret guarantees that demonstrate the benefits of accurate baseline reward modeling and network regularization. Extensive experiments on both simulated and real-world mHealth data demonstrate that DML-TS-NNR substantially outperforms existing methods. By accurately capturing trends in treatment effects over time and across individuals, DML-TS-NNR is able to learn effective just-in-time adaptive intervention policies at an accelerated pace. The proposed innovations address key challenges in mobile health and offer a template for personalized, adaptive interventions in other complex longitudinal settings.
