# [Achieving a Better Stability-Plasticity Trade-off via Auxiliary Networks   in Continual Learning](https://arxiv.org/abs/2303.09483)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading, the central research question this paper addresses is: How can we achieve a better balance between stability and plasticity in continual learning using an auxiliary network?

Specifically, the paper proposes a framework called Auxiliary Network Continual Learning (ANCL) that combines the original continual learning model (focusing on stability) with an additional auxiliary network (promoting plasticity). The goal is to leverage both models to find a better equilibrium between retaining previous knowledge (stability) and learning new information (plasticity). 

The authors hypothesize that by controlling the relative strength of the regularization terms from the old network and auxiliary network, the ANCL framework can naturally interpolate between stability and plasticity. They test this hypothesis through extensive experiments and analysis of the ANCL solutions.

In summary, the key research question is how to strike a better stability-plasticity balance in continual learning, and the core hypothesis is that the proposed ANCL framework can achieve this by interpolating between an old model and an auxiliary model. The experiments and analyses aim to validate whether ANCL can actually improve on this trade-off compared to standard continual learning approaches.


## What is the main contribution of this paper?

 The main contributions of this paper are:

1. It proposes a novel framework called Auxiliary Network Continual Learning (ANCL) that incorporates an auxiliary network into existing continual learning approaches to help balance stability and plasticity. 

2. It shows empirically that adding ANCL improves performance over baseline continual learning methods on both task incremental and class incremental scenarios across CIFAR-100 and Tiny ImageNet datasets.

3. It provides an in-depth analysis on how ANCL helps achieve a better stability-plasticity tradeoff through three analyses: weight distance, centered kernel alignment, and mean accuracy landscape. 

4. The analyses reveal how the ratio between the regularization strengths on the old network and auxiliary network allows ANCL to interpolate between plasticity and stability.

5. For regularization-based methods like EWC and MAS, the ANCL solution lies on the interpolation between the old and auxiliary weights. For distillation-based methods like LwF and LFL, the ANCL gradient shifts the activations towards the interpolation between the old and auxiliary networks.

In summary, the key contribution is proposing ANCL as a general framework to improve continual learning, demonstrating its effectiveness empirically, and providing insights into how it achieves a better stability-plasticity balance through detailed analyses.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the key points from the paper:

The paper proposes a framework called Auxiliary Network Continual Learning (ANCL) that combines existing continual learning methods with an auxiliary network trained only on the current task, in order to achieve a better balance between retaining previous knowledge (stability) and learning new tasks (plasticity).
