# [CODA-Prompt: COntinual Decomposed Attention-based Prompting for   Rehearsal-Free Continual Learning](https://arxiv.org/abs/2211.13218)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question is: How can we develop an effective rehearsal-free continual learning approach for vision transformers that has high capacity to learn new tasks while avoiding catastrophic forgetting?

The key hypotheses appear to be:

1) Decomposing prompts into learnable components that are combined with input-conditioned weights will allow for expanded capacity compared to prior prompting methods.

2) Using an attention-based weighting scheme to combine the prompt components will enable end-to-end optimization of the prompting mechanism. 

3) Expanding prompting capacity along the "component" dimension rather than the prompt length dimension will lead to better utilization of parameters and greater gains in accuracy.

4) Freezing past task components and using an orthogonality constraint will reduce interference and forgetting.

The overall goal is to develop a rehearsal-free continual learning method for vision transformers that sets a new state-of-the-art by improving plasticity via the decomposed prompting approach while controlling forgetting through the proposed techniques. The experiments aim to validate whether this approach outperforms prior methods empirically.


## What is the main contribution of this paper?

 The main contribution of this paper is proposing a novel continual learning method called CODA-Prompt. The key ideas are:

- It uses a decomposed prompt consisting of a weighted sum of learnable prompt components. This allows expanding prompting capacity to capture complex task information. 

- It uses an attention-based component weighting scheme to form input-conditioned prompts in an end-to-end manner. This increases plasticity to learn future tasks better.

- The prompting mechanism is optimized end-to-end with the task, unlike prior works that use separate optimizations.

- It expands prompting capacity when learning new tasks while freezing old components to mitigate forgetting. Orthogonality regularization further reduces interference.

In summary, CODA-Prompt contributes a new rehearsal-free continual learning approach that forms flexible input-conditioned prompts in an end-to-end manner. It demonstrates state-of-the-art performance on established benchmarks as well as a new dual-shift benchmark. The end-to-end optimization and capacity expansion are notable differences from prior arts.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence TL;DR of the paper:

The paper proposes a novel attention-based prompting method called CODA-Prompt that achieves state-of-the-art results on continual learning benchmarks by using an end-to-end optimized set of prompt components that expand with new tasks, enabling higher capacity and less catastrophic forgetting.


## How does this paper compare to other research in the same field?

 Based on my reading, this paper makes several notable contributions to the field of rehearsal-free continual learning:

- It proposes a new method called CODA-Prompt that sets new state-of-the-art results on established rehearsal-free continual learning benchmarks like ImageNet-R, CIFAR-100, and DomainNet. The key ideas are using a decomposed attention-based prompt with expandable capacity and end-to-end optimization.

- The results show clear improvements over prior prompting methods like DualPrompt and L2P, with gains of up to 4.5% in average accuracy on ImageNet-R. This suggests CODA-Prompt advances prompting techniques for continual learning.

- The paper demonstrates strong performance on a new challenging benchmark with both class-incremental and domain-incremental shifts. This tests model robustness to real-world distributional shifts. Again, CODA-Prompt sets the new state-of-the-art.

- Compared to other areas like regularization and replay-based methods, prompting techniques have recently gained traction as a highly effective strategy for rehearsal-free continual learning. This paper pushes the boundaries of prompting further.

- The end-to-end optimized attention-based prompt assembly is novel compared to prior prompting works. The expandable prompt components also contrast with fixed prompt pool strategies.

Overall, I would summarize that this paper sets a new state-of-the-art for prompting techniques in the important setting of rehearsal-free continual learning. The decomposed attention-based prompting and end-to-end optimization are novel ideas that seem to advance prompt-based continual learning.
