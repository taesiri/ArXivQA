# [CODA-Prompt: COntinual Decomposed Attention-based Prompting for   Rehearsal-Free Continual Learning](https://arxiv.org/abs/2211.13218)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question is: How can we develop an effective rehearsal-free continual learning approach for vision transformers that has high capacity to learn new tasks while avoiding catastrophic forgetting?

The key hypotheses appear to be:

1) Decomposing prompts into learnable components that are combined with input-conditioned weights will allow for expanded capacity compared to prior prompting methods.

2) Using an attention-based weighting scheme to combine the prompt components will enable end-to-end optimization of the prompting mechanism. 

3) Expanding prompting capacity along the "component" dimension rather than the prompt length dimension will lead to better utilization of parameters and greater gains in accuracy.

4) Freezing past task components and using an orthogonality constraint will reduce interference and forgetting.

The overall goal is to develop a rehearsal-free continual learning method for vision transformers that sets a new state-of-the-art by improving plasticity via the decomposed prompting approach while controlling forgetting through the proposed techniques. The experiments aim to validate whether this approach outperforms prior methods empirically.


## What is the main contribution of this paper?

 The main contribution of this paper is proposing a novel continual learning method called CODA-Prompt. The key ideas are:

- It uses a decomposed prompt consisting of a weighted sum of learnable prompt components. This allows expanding prompting capacity to capture complex task information. 

- It uses an attention-based component weighting scheme to form input-conditioned prompts in an end-to-end manner. This increases plasticity to learn future tasks better.

- The prompting mechanism is optimized end-to-end with the task, unlike prior works that use separate optimizations.

- It expands prompting capacity when learning new tasks while freezing old components to mitigate forgetting. Orthogonality regularization further reduces interference.

In summary, CODA-Prompt contributes a new rehearsal-free continual learning approach that forms flexible input-conditioned prompts in an end-to-end manner. It demonstrates state-of-the-art performance on established benchmarks as well as a new dual-shift benchmark. The end-to-end optimization and capacity expansion are notable differences from prior arts.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence TL;DR of the paper:

The paper proposes a novel attention-based prompting method called CODA-Prompt that achieves state-of-the-art results on continual learning benchmarks by using an end-to-end optimized set of prompt components that expand with new tasks, enabling higher capacity and less catastrophic forgetting.
