# ['One size doesn't fit all': Learning how many Examples to use for   In-Context Learning for Improved Text Classification](https://arxiv.org/abs/2403.06402)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- In-context learning (ICL) involves using a frozen pre-trained language model and controlling its output using labeled examples in the prompt/context. 
- A key component of ICL is determining the ideal number of examples to include. Prior work uses a static number of examples.
- Some instances likely require more examples than others to predict correctly. There is a need for adaptive selection of examples.

Proposed Solution:
- Propose Adaptive ICL (AICL) to dynamically choose the number of examples per instance at inference time.
- Train a classifier on the training set to predict the optimal number of examples per instance. Optimal is based on minimum examples for correct prediction.  
- Apply the classifier at inference to predict examples for a test instance before ICL.

Key Contributions:
- Novel AICL method to adaptively select examples for improved ICL performance.
- AICL outperforms static ICL baselines on text classification datasets in accuracy and efficiency.
- Shows automatically adapting the number of examples works better than hand-picked static values.
- Provides a more computationally efficient ICL workflow by using fewer examples when possible.

In summary, the paper presents Adaptive ICL to dynamically determine the number of examples for improved efficiency and accuracy of in-context learning for text classification.
