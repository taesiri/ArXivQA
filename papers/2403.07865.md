# [Exploring Safety Generalization Challenges of Large Language Models via   Code](https://arxiv.org/abs/2403.07865)

## Summarize the paper in one sentence.

 This paper introduces CodeAttack, a framework that transforms natural language inputs into code inputs to systematically assess and expose vulnerabilities in the safety mechanisms of large language models when faced with out-of-distribution code inputs.


## What is the main contribution of this paper?

 The main contribution of this paper is proposing CodeAttack, a novel framework that reformulates text completion tasks as code completion tasks in order to systematically investigate the safety generalization challenges of large language models when faced with novel input domains like code. The key findings are:

1) CodeAttack consistently and effectively breaches the safety guardrails of state-of-the-art LLMs including GPT, Claude, and Llama models, exposing a common vulnerability in their ability to generalize safety behaviors to the code domain. 

2) Safety alignment generalizes less effectively when CodeAttack prompts diverge more from the natural language distribution, such as using less common data structures for input encoding or less popular programming languages.

3) Bigger model size does not necessarily lead to more robust safety behavior. Models fine-tuned specifically for coding capabilities can exhibit greater vulnerability to CodeAttack despite superior performance.

4) CodeAttack is cost-efficient and automated, increasing the potential for misuse of LLMs in the code domain without requiring domain-specific expertise.

The paper emphasizes the need for more comprehensive red team evaluations and more robust safety alignment techniques that can generalize to novel input distributions beyond natural language. Overall, it uncovers new safety risks for LLMs in the code domain that are not adequately addressed by current mechanisms.


## What are the keywords or key terms associated with this paper?

 Based on my review, some of the key terms and concepts associated with this paper include:

- Large language models (LLMs): The paper focuses on assessing the safety mechanisms and alignment of large language models such as GPT, Claude, and Llama models.

- CodeAttack: The novel framework proposed in the paper that transforms natural language inputs into code inputs to test the safety generalization of LLMs. 

- Safety generalization: The paper explores and exposes the vulnerabilities in the ability of LLMs' safety mechanisms to generalize to novel inputs like code.

- Red teaming: The methodology of comprehensively evaluating models' safety using adversarial techniques is referred to as red teaming. The paper conducts red teaming on LLMs.

- Attack success rate (ASR): The metric used to quantify the percentage of harmful responses generated by models on malicious inputs.

- Input encoding: Encoding the natural language input with data structures like strings, queues and stacks to make them out-of-distribution.

- Task understanding: Using a decode function to allow models to extract the task from encoded inputs. 

- Output specification: Specifying the output format using data structures to obtain the desired malicious content.

In summary, the key focus is on assessing safety generalization challenges of LLMs to code inputs using the proposed CodeAttack framework and red teaming evaluations.
