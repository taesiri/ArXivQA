# [Low-dose CT Denoising with Language-engaged Dual-space Alignment](https://arxiv.org/abs/2403.06128)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem: 
Existing deep learning methods for low-dose CT (LDCT) denoising often suffer from over-smoothing, blurring, and lack of explainability. Pixel-level losses like MSE can cause over-smoothing. GAN-based methods can introduce artifacts. There is a need for better losses to enhance image quality and provide explainability.

Proposed Solution:
The paper proposes a new Language-Engaged Dual-space Alignment (LEDA) loss to supervise LDCT denoising models. The key ideas are:

1) Pretrain an LLM-guided CT autoencoder that can encode a CT image into continuous perceptual features and discrete text tokens derived from the LLM's vocabulary. This provides representations in both continuous and discrete spaces.

2) Define the LEDA loss to minimize the discrepancy between the denoised LDCT and normal-dose CT (NDCT) in terms of both the encoded continuous features and discrete text tokens. This aligns the images in perceptual and semantic spaces.

3) Use the LEDA loss along with MSE loss to train a denoising model. This enhances image quality by reducing noise and retaining textures and details.

4) The discrete text tokens also provide language-level explainability of anatomical semantics in the denoised image.

Main Contributions:

- Proposes the first LLM-based loss (LEDA) for LDCT denoising to work in continuous and discrete spaces

- Pretrains an LLM-guided CT autoencoder to obtain representations in both spaces  

- Shows LEDA loss outperforms MSE, perceptual loss, and GAN losses in quantitative and qualitative results

- Provides language-level explainability of denoised CT scans through extracted text tokens

In summary, the key innovation is the LEDA loss leveraging an LLM to align and constrain the denoising in perceptual and semantic feature spaces, enhancing quality and explainability.
