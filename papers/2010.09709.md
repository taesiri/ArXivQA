# [Self-supervised Co-training for Video Representation Learning](https://arxiv.org/abs/2010.09709)

## What is the central research question or hypothesis that this paper addresses?

 This paper focuses on self-supervised video representation learning. The central hypothesis it tests is: 

Is instance discrimination making the best use of data for self-supervised video representation learning?

The key points are:

1. The paper first shows that using an "oracle" with access to semantic class labels to incorporate positives based on class significantly improves performance over pure instance-based training like infoNCE. This suggests instance discrimination is suboptimal.

2. The paper proposes a new self-supervised co-training method called CoCLR that mines positives from complementary views (RGB and optical flow) to improve over instance discrimination. 

3. Experiments show CoCLR significantly outperforms instance discrimination and approaches the upper bound performance of the oracle, demonstrating the benefits of going beyond just instance-based training.

So in summary, the central hypothesis is that instance discrimination is suboptimal for self-supervised video representation learning, and the paper proposes a new co-training method to improve over this by exploiting complementary information between RGB and optical flow. The experiments support this hypothesis.


## What is the main contribution of this paper?

 Based on my understanding, the main contributions of this paper are:

1. Investigating the benefit of adding semantic-class positives to instance-based InfoNCE training, and showing that this supervised contrastive learning leads to improved performance. 

2. Proposing a novel self-supervised co-training scheme (CoCLR) to improve the popular infoNCE loss. This is done by exploiting the complementary information from different views (RGB and optical flow) of the same data, using one view to obtain positive samples for the other.

3. Thoroughly evaluating the learned representations on two downstream tasks - action recognition and video retrieval. The results demonstrate state-of-the-art or comparable performance to other self-supervised approaches, while being significantly more efficient (requiring less training data).

In summary, the key ideas are improving the sampling process in contrastive self-supervised learning by constructing positive pairs beyond instances, and showing the benefits of co-training using complementary views like RGB and optical flow. The outcome is more efficient and effective self-supervised video representation learning.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the paper:

The paper proposes a self-supervised co-training method for video representation learning that improves over instance discrimination methods by exploiting complementary information from RGB frames and optical flow to construct more informative positive pairs.


## How does this paper compare to other research in the same field?

 Here are a few key ways this paper compares to other research in the field of self-supervised video representation learning:

- It proposes a new self-supervised co-training method (CoCLR) that improves upon the popular InfoNCE contrastive learning approach by exploiting complementary information from different views (RGB and optical flow). This is a novel training scheme rather than a new loss function or pretext task.

- It demonstrates state-of-the-art or comparable performance to other self-supervised methods on downstream tasks like action recognition and retrieval. The co-training approach is especially efficient, requiring less training data than methods that use only a single view or modality.

- It shows that incorporating semantic class label information as positives (the UberNCE "oracle" experiment) substantially improves performance over instance-based discrimination. This suggests current self-supervised methods are not making full use of the data.

- It provides ablation studies analyzing the impact of different design choices like the number of positive pairs and co-training cycles. This gives insights into what factors contribute most to the performance gains.

- It validates the benefit of using complementary modalities (RGB and optical flow) versus single view methods. The gains from co-training two networks indicates there is complementary information being captured.

- It focuses only on self-supervised pre-training from visual data, while some other recent methods leverage other modalities like audio or text. But the co-training concept could extend to those scenarios as well.

Overall, this paper makes excellent contributions in training efficiency, performance, and analysis compared to prior self-supervised video representation learning literature. The co-training scheme is simple yet effective, and seems well-suited to exploit the multi-view nature of video data.


## What future research directions do the authors suggest?

 Based on my reading of the paper, some of the main future research directions suggested by the authors are:

- Exploring other complementary views beyond optical flow that could provide useful signals for bridging the gap between positive instances. The authors suggest audio or text narrations for videos, or using different image filters for still images.

- Applying the co-training idea to other self-supervised learning methods besides InfoNCE/contrastive losses, such as BYOL.

- Evaluating the method on larger-scale datasets and with more modern network architectures to further improve the results. 

- Reducing the computational cost of self-supervised representation learning. The authors hope more work can be done to lower the training costs in this area.

- Releasing code and pretrained models to facilitate future research building off their method.

- Extending the co-training framework beyond two views to multiple views of the data.

- Applying the co-training idea for self-supervised learning in other modalities and tasks beyond video classification.

- Further investigating the dynamics of optimizing the two network representations simultaneously versus alternating between them.

In summary, the main suggestions are around exploring additional complementary views and data modalities, reducing computational costs, releasing code/models, and extending the co-training framework to other self-supervised learning methods and tasks. The authors see a lot of potential in using co-training between different views of data to improve self-supervised representation learning.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the paper:

This paper proposes a novel self-supervised co-training method called CoCLR (Co-training Contrastive Learning of visual Representation) for learning video representations from RGB frames and optical flow. The key idea is to use the complementary information from the two different views (RGB and flow) of the same data to construct more informative positive pairs beyond just data augmentation. Specifically, the top-K nearest neighbor clips based on flow similarity are treated as positives to train the RGB representation and vice versa. This allows mining hard positives from the same semantic class to improve over standard instance discrimination methods like InfoNCE. The method is evaluated on downstream tasks of action recognition and retrieval on UCF101 and HMDB51 datasets. Results show CoCLR significantly outperforms InfoNCE and achieves state-of-the-art performance compared to other self-supervised approaches, while being more efficient as less training data is required. The idea of using a second complementary view to bridge the gap between positive instances is generalizable to other modalities like audio.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the key points from the paper:

This paper proposes a novel self-supervised video representation learning method called CoCLR (Co-training Contrastive Learning of visual Representation). It focuses on improving the popular InfoNCE loss for self-supervised learning by exploiting complementary information from different "views" of the data, specifically RGB frames and optical flow. 

The key ideas are: (1) An "oracle" experiment shows that incorporating semantic class label information as positives improves performance over pure instance-based training like InfoNCE, indicating the benefit of using informative positives. (2) A new co-training scheme is proposed where the RGB and Flow networks mine hard positives from each other, gradually enhancing both representations. This allows going beyond instance discrimination by linking positive samples across instances. Evaluations on action recognition and retrieval show state-of-the-art performance compared to other self-supervised approaches, while being far more efficient. Overall, the paper demonstrates that mining hard positives from a complementary view is an effective approach for self-supervised video representation learning.


## Summarize the main method used in the paper in one paragraph.

 Here is a one paragraph summary of the main method used in the paper:

The paper proposes a self-supervised co-training approach called CoCLR for video representation learning. The key idea is to use the complementary information from different "views" of a video, namely the RGB frames and optical flow, to construct more informative positive pairs during contrastive learning. Specifically, the RGB and optical flow networks are first pretrained separately using the InfoNCE loss for instance discrimination. Then, they are co-trained by alternately mining hard positives for each network from the other. For example, to update the RGB network, the top-K most similar clips in the optical flow space are treated as positives along with augmentations of the query clip. This allows going beyond instance-level discrimination and pulling together clips of the same semantic class. The overall training scheme enables InfoNCE to learn from more meaningful positives without direct supervision. The learnt RGB and flow representations are evaluated on downstream action recognition and retrieval tasks and achieve state-of-the-art performance compared to other self-supervised methods.


## What problem or question is the paper addressing?

 The key points from the paper are:

- It investigates self-supervised video representation learning, specifically using contrastive learning approaches like InfoNCE. The main question it addresses is whether instance discrimination (treating each sample as its own class) is optimal for this task. 

- It shows that incorporating semantic class labels as positives (UberNCE) substantially improves results over just using instance-level positives (InfoNCE). This suggests instance discrimination may not make best use of the data.

- It proposes a novel co-training method (CoCLR) that uses complementary views (RGB and optical flow) to mine harder positive samples for each view, without requiring labels. This also improves over InfoNCE.

- CoCLR alternates between optimizing losses for the RGB and flow networks, mining positives for each view from the other. It starts from InfoNCE models and co-trains them.

- Evaluations on action recognition and retrieval show CoCLR achieves state-of-the-art or competitive performance compared to other self-supervised methods, while requiring less training data.

In summary, the key contribution is a co-training method to improve sampling of positives in contrastive self-supervised learning, beyond just instance discrimination. It shows the benefit of mining harder positives from a complementary view.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and keywords associated with this paper are:

- Self-supervised learning - The paper focuses on self-supervised methods for video representation learning, without using manual annotations.

- Contrastive learning - The paper investigates contrastive losses like InfoNCE for self-supervised learning. It proposes improvements to the sampling strategy in contrastive learning.

- Video representation learning - The goal is to learn useful video representations in a self-supervised manner for downstream tasks.

- Co-training - The paper proposes a co-training method to take advantage of different video views (RGB, optical flow) for mining better positives. 

- Action recognition - One of the key downstream tasks used to evaluate the learned representations is action recognition.

- Video retrieval - Another downstream task for evaluating the representations is nearest neighbor video retrieval.

- InfoNCE - Info Noise Contrastive Estimation loss used as a baseline method. The paper aims to improve on it.

- Hard positives mining - A key contribution is mining hard positives from complementary views to improve contrastive learning.

- RGB and optical flow - The two different views of videos explored are RGB frames and optical flow.

So in summary, the key focus is on self-supervised visual representation learning for videos, using co-training of RGB and flow streams with improved sampling for contrastive learning. The application tasks are action recognition and video retrieval.
