# [Self-supervised Co-training for Video Representation Learning](https://arxiv.org/abs/2010.09709)

## What is the central research question or hypothesis that this paper addresses?

This paper focuses on self-supervised video representation learning. The central hypothesis it tests is: Is instance discrimination making the best use of data for self-supervised video representation learning?The key points are:1. The paper first shows that using an "oracle" with access to semantic class labels to incorporate positives based on class significantly improves performance over pure instance-based training like infoNCE. This suggests instance discrimination is suboptimal.2. The paper proposes a new self-supervised co-training method called CoCLR that mines positives from complementary views (RGB and optical flow) to improve over instance discrimination. 3. Experiments show CoCLR significantly outperforms instance discrimination and approaches the upper bound performance of the oracle, demonstrating the benefits of going beyond just instance-based training.So in summary, the central hypothesis is that instance discrimination is suboptimal for self-supervised video representation learning, and the paper proposes a new co-training method to improve over this by exploiting complementary information between RGB and optical flow. The experiments support this hypothesis.


## What is the main contribution of this paper?

Based on my understanding, the main contributions of this paper are:1. Investigating the benefit of adding semantic-class positives to instance-based InfoNCE training, and showing that this supervised contrastive learning leads to improved performance. 2. Proposing a novel self-supervised co-training scheme (CoCLR) to improve the popular infoNCE loss. This is done by exploiting the complementary information from different views (RGB and optical flow) of the same data, using one view to obtain positive samples for the other.3. Thoroughly evaluating the learned representations on two downstream tasks - action recognition and video retrieval. The results demonstrate state-of-the-art or comparable performance to other self-supervised approaches, while being significantly more efficient (requiring less training data).In summary, the key ideas are improving the sampling process in contrastive self-supervised learning by constructing positive pairs beyond instances, and showing the benefits of co-training using complementary views like RGB and optical flow. The outcome is more efficient and effective self-supervised video representation learning.
