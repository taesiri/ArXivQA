# [PROSAC: Provably Safe Certification for Machine Learning Models under   Adversarial Attacks](https://arxiv.org/abs/2402.02629)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
State-of-the-art machine learning models, including vision and language models, are vulnerable to adversarial attacks - small perturbations to inputs that cause the model to make incorrect predictions. However, new regulations such as the EU AI Act require providers of AI systems to ensure resilience against such attacks. Therefore, there is a need for technical capabilities to rigorously certify ML model performance under adversarial attacks to satisfy regulatory requirements. Existing certification methods are limited to empirical risk guarantees on predefined datasets rather than population-level guarantees.

Proposed Solution - PROSAC:
The paper proposes a new framework (PROSAC) to certify ML models against adversarial attacks with statistical guarantees on the population risk. Specifically:

1. Introduces the notion of (α,ζ)-safety: With probability ≥1-ζ, the model has adversarial population risk ≤α for the worst case attack hyperparameters.

2. Formulates a hypothesis test to check if max adversarial risk > α. Computes p-value using calibration data and rejects null hypothesis if p≤ζ to certify (α,ζ)-safety.  

3. Uses Bayesian optimization (BO) with Gaussian process upper confidence bound (GP-UCB) to efficiently search over attack hyperparameters when computing the p-value. Guarantees BO solution closely approximates true p-value.

4. Evaluates PROSAC by certifying vision models (ViTs, ResNets) against attacks like AutoAttack and SquareAttack. Shows ViTs are more robust than ResNets.

Main Contributions:

1. New framework to certify ML models against attacks with population risk guarantees, beyond empirical guarantees.

2. Rigorous statistical procedure based on hypothesis testing to check (α,ζ)-safety. Computes valid p-values efficiently using GP-UCB.

3. Empirical evaluation showing ViTs are more robust and model size matters. Aligns with and provides statistical guarantees around existing observations.

4. Discussion of relevance for AI regulations needing resilience guarantees like EU AI Act. PROSAC helps satisfy technical requirements with state-of-the-art guarantees.

In summary, the paper makes important contributions towards model certification under attacks to satisfy regulatory needs, with rigorous population-level safety guarantees.


## Summarize the paper in one sentence.

 This paper proposes PROSAC, a new framework to certify whether a machine learning model is robust against adversarial attacks, with statistical guarantees on the model's adversarial risk across all possible attacker hyperparameter configurations.


## What is the main contribution of this paper?

 The main contributions of this paper are:

1. It proposes PROSAC, a new framework to certify whether a machine learning model is robust against a specific adversarial attack. Specifically, it proposes a hypothesis testing procedure based on a notion of $(\alpha,\zeta)$ machine learning model safety, which entails that the adversarial risk of a model is less than a threshold $\alpha$ with probability higher than $\zeta$.

2. It proposes a Bayesian optimization algorithm called GP-UCB to efficiently approximate the $p$-values needed in the hypothesis testing procedure to certify model safety. This allows certification with fewer queries compared to searching over all hyperparameter configurations.  

3. It provides statistical guarantees that the proposed GP-UCB algorithm can rigorously certify $(\alpha,\zeta)$ safety of a model against an attack.

4. It offers experiments applying PROSAC to certify the robustness of various vision models like ViTs and ResNets against attacks like AutoAttack and SquareAttack. This reveals trends about model robustness, like ViTs being more robust than ResNets and larger models being more robust.

5. It discusses the relevance of PROSAC for AI regulation, as a technical tool to provide guarantees about model safety satisfying requirements like those in the EU AI Act.

In summary, the main contribution is a new framework to provide statistical guarantees about the adversarial robustness of machine learning models, with rigorous certification procedures and experiments benchmarking state-of-the-art models. The framework is also relevant for AI regulation.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and keywords associated with it are:

- Adversarial risk
- Risk certification 
- Regulations
- Machine learning models
- Adversarial attacks
- Vision models (ViT, ResNet)
- Population-level guarantees
- Statistical guarantees
- $(\alpha, \zeta)$-model safety 
- Hypothesis testing
- Bayesian optimization
- GP-UCB algorithm
- AutoAttack
- SquareAttack
- Natural evolution strategy (NES) attack

The paper introduces the notion of $(\alpha,\zeta)$ machine learning model safety, which provides a way to statistically certify the performance of machine learning models under adversarial attacks. It proposes a hypothesis testing procedure and Bayesian optimization algorithms to efficiently determine whether a model satisfies this safety definition. The method is applied to certify the robustness of vision models like ViTs and ResNets under different adversarial attacks. The results have implications for AI regulations requiring assurances about model performance under perturbations. So the key terms revolve around statistical certification of adversarial robustness for regulatory compliance.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 potential in-depth questions about the method proposed in this paper:

1. The paper proposes a new notion of $(\alpha, \zeta)$-machine learning model safety. Explain this notion and the intuition behind it. What are the key parameters $\alpha$ and $\zeta$ capturing? 

2. The paper sets up a hypothesis testing framework to determine $(\alpha, \zeta)$-model safety. Clearly describe the null and alternative hypotheses considered. What is the high-level idea behind using hypothesis testing for certification?

3. Theorem 2 shows how to derive a valid $p$-value for testing if the max adversarial risk is above a threshold $\alpha$. Walk through the key steps behind the proof. What inequality is leveraged?

4. The paper leverages Bayesian optimization, specifically the GP-UCB algorithm, to search over attacker hyperparameter configurations. Explain why Bayesian optimization is well suited for this problem. What are the key challenges?  

5. Theorem 4 provides a guarantee on retaining the desired $(\alpha, \zeta)$-safety even when using the GP-UCB estimate of the $p$-value rather than the true maximal $p$-value. Explain the main elements behind this result.

6. The paper discusses implications for AI regulation and accountability. In your view, what are the one or two most salient connections to ongoing policy debates?

7. The paper suggests some interesting new findings regarding model robustness, especially concerning Vision Transformers (ViTs). What were these key findings? What questions do they raise for future investigation?  

8. How does the proposed POP-SAC framework conceptually differ from existing approaches for adversarial robustness certification? What does it add to the literature?

9. The paper relies fundamentally on the availability of a calibration set. Discuss the role of this calibration set. What are its key properties? Do you foresee any practical challenges in obtaining such data?

10. The paper considers $l_p$-norm bounded perturbations. What other types of perturbations could one test for robustness? Would the proposed approach extend cleanly? Identify one extension you think would be highly impactful.
