# [Backdoor Attacks Against Deep Image Compression via Adaptive Frequency   Trigger](https://arxiv.org/abs/2302.14677)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the main research question seems to be: How can we perform effective backdoor attacks against deep learning based image compression models?Specifically, the authors investigate injecting backdoors into image compression models via adaptive frequency triggers. They explore different attack objectives such as degrading compression quality or attacking downstream computer vision tasks. The key hypothesis appears to be that by adding adaptive triggers in the frequency domain of images, they can successfully inject multiple backdoors into a single image compression model. Each backdoor is activated by a specific trigger pattern and leads to a malicious effect defined by the attack objective.In summary, the central research question is how to develop a methodology for performing stealthy yet effective backdoor attacks on deep learning based image compression models by using adaptive frequency domain triggers. The hypothesis is that this approach can allow injecting multiple backdoors with different objectives into a single model.


## What is the main contribution of this paper?

The main contributions of this paper are:- They propose a novel backdoor attack against deep learning based image compression models by injecting triggers adaptively in the frequency domain. - They comprehensively explore various attack objectives, including attacking compression quality measures like bit-rate and reconstruction quality, as well as attacking downstream computer vision tasks like face recognition and semantic segmentation.- They design a frequency-based trigger injection model that leverages priors from both spatial and frequency domains to generate poisoned images effectively. - They propose a simple yet effective dynamic loss to balance different terms in the joint training loss, which enables more efficient training.- They demonstrate that multiple triggers can be injected into one single compression model, with each trigger associated with a specific attack objective.- Extensive experiments validate that their attack can successfully degrade compression performance or mislead downstream tasks upon the presence of the corresponding trigger. The attack also has little impact on clean images.In summary, this is the first work that systematically investigates backdoor attack against deep learning based image compression. The proposed adaptive frequency trigger injection and comprehensive attacking scenarios make the attack more stealthy and effective.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points in the paper:The paper proposes a novel backdoor attack against deep learning based image compression models by injecting adaptive frequency domain triggers, which can effectively attack compression quality and downstream computer vision tasks with multiple triggers in a single model.


## How does this paper compare to other research in the same field?

Here are my thoughts on how this paper compares to other research in the field of backdoor attacks against image compression models:- This is the first paper I am aware of that explores backdoor attacks specifically against learned image compression models. Most prior work has focused on attacking high-level vision models like classifiers and segmenters. Studying attacks against low-level vision models like compression is an interesting new direction.- The proposed frequency-based adaptive trigger injection method is novel compared to prior work. Using DCT-domain triggers customized for each input image is an intuitive idea given DCT's prevalence in compression. This adaptive frequency trigger seems more suited for attacking compression than the fixed spatial triggers used in most prior backdoor attack papers.- The attack objectives considered are comprehensive, including both low-level compression metrics like bitrate and quality as well as high-level downstream task performance. This provides a thorough evaluation of the attack's capabilities.- Allowing multiple backdoor triggers to be injected into one model, each with a different objective, is a capability not seen in previous papers. It makes the attack more practical and dangerous.- Modifying only the encoder of a compression model rather than retraining the whole model makes the attack more feasible in real-world settings.- Compared to the baseline method, the proposed approach seems substantially more effective at achieving strong attack performance across different compression rates and downstream tasks.Overall, I believe this paper provides a thorough and rigorous exploration of backdoor attacks in a new problem domain of learned image compression. The adaptive frequency trigger method and comprehensive attack formulations seem like clear improvements over prior work. This attack could pose a real threat to the reliability of image compression systems.


## What future research directions do the authors suggest?

Based on my reading of the paper, here are some of the main future research directions suggested by the authors:- Investigating defense methods against backdoor attacks on learned image compression models. The authors note that defending against such attacks is an important direction for future work. - Exploring additional attack objectives beyond the ones studied in this paper. The authors comprehensively explore several attack objectives but suggest there may be other objectives worth investigating as well.- Studying the transferability of the backdoor attacks to other compression models and frameworks. The authors demonstrate attacks on two specific compression models, but suggest examining if the attacks transfer more broadly.- Considering dynamic triggers that change over time rather than static triggers. The triggers in this work are fixed, but the authors propose exploring adaptive triggers that could vary across images or over time.- Applying similar backdoor attack techniques to other low-level vision tasks beyond compression. The authors note these techniques may generalize to other tasks like denoising, super-resolution, etc.- Developing more advanced frequency-based triggers. The authors propose a frequency-based trigger design, but suggest further improvements could be made to the trigger generation methodology.- Exploring additional benign attack scenarios like privacy protection. The authors show an example of using backdoors for privacy protection and suggest more applications along these lines.In summary, the main future directions focus on defending against such attacks, generalizing the attacks to new models/tasks/objectives, and improving the trigger design and generation.


## Summarize the paper in one paragraph.

The paper presents a novel backdoor attack against deep learning-based image compression models. The key ideas are:- They propose a frequency-based trigger injection model that adds triggers adaptively in the DCT domain. This leverages the common use of DCT in compression systems. - They design attack objectives comprehensively, including attacking compression quality (bit-rate/PSNR) and downstream computer vision tasks like face recognition and semantic segmentation.- They inject multiple triggers into one model, with each trigger associated with a specific attack objective. This is enabled by jointly training the trigger generators and finetuning the compression encoder.- Experiments demonstrate their attack can successfully activate the backdoors and cause malicious impacts like increasing bit-rate or degrading reconstruction quality. The attacks are also resistant to common defense methods.In summary, this is the first work investigating backdoor attack against learned image compression. It proposes an adaptive frequency trigger injection approach and demonstrates successful attacks on multiple objectives within one model. The attacks pose security risks for the adoption of deep learning in compression applications.


## Summarize the paper in two paragraphs.

Here is a two paragraph summary of the paper:This paper proposes a novel backdoor attack against learned image compression models. The authors design a frequency-based adaptive trigger injection model that adds triggers in the DCT domain. They investigate several attack objectives including attacking compression quality (bit-rate and reconstruction PSNR) as well as attacking downstream tasks like face recognition and semantic segmentation. A simple dynamic loss is proposed to balance the different loss terms adaptively during training. Extensive experiments demonstrate that by only modifying the encoder parameters and using the proposed trigger generators, multiple backdoors can be successfully injected into a single compression model. Each backdoor corresponds to a specific attack objective and can be activated by its associated trigger. Compared to prior work, this paper makes the first attempt to investigate backdoor attacks for learned image compression. The proposed frequency-based adaptive trigger is more suitable for compression tasks compared to spatial triggers used in prior work. The comprehensive attack objectives go beyond just classification tasks to include compression specific measures. The proposed simple dynamic loss enables more efficient training. Finally, being able to inject multiple backdoors in one model with only encoder modification makes the attack more practical. Through extensive validation, the authors demonstrate the effectiveness of the proposed backdoor attacks.


## Summarize the main method used in the paper in one paragraph.

The paper proposes a novel backdoor attack against deep learning based image compression models. The key ideas are:1) They design a frequency-based adaptive trigger injection model to generate poisoned images. Specifically, they split the input image into patches, apply 2D DCT transform, and inject the trigger by adding it to the DCT coefficients. The trigger consists of a general part capturing local features and a patch-wise weight for global adaptation. 2) They investigate various attack objectives, including attacking the compression quality (bit-rate and reconstruction PSNR), and attacking downstream vision tasks (semantic segmentation and face recognition). The objectives are formulated as different joint loss functions for training the trigger generator and finetuning the compression encoder.3) They demonstrate attacks with multiple triggers can be injected into one model, with each trigger associated with one attack objective. Only the encoder parameters are modified while keeping the decoder fixed.4) Experiments validate the effectiveness of the proposed frequency-based adaptive trigger injection. The attacks can successfully degrade compression performance or mislead downstream tasks. Attacking multiple objectives in one model is also shown feasible.In summary, the key contribution is proposing a frequency-domain adaptive trigger injection approach to launch backdoor attacks on learned image compression models, and investigating multiple attacking scenarios with different objectives.


## What problem or question is the paper addressing?

The paper is addressing the problem of backdoor attacks against deep learning based image compression models. Specifically, it investigates how to inject backdoors into these models using adaptive frequency triggers so that the model behaves maliciously when certain trigger patterns are present in the input image.The key questions the paper tries to address are:- How to design triggers that can effectively attack deep compression models when added to input images? The paper proposes frequency-based adaptive triggers injected in the DCT domain.- What objectives can the attack try to achieve? The paper explores attacking compression quality measures like bit-rate and reconstruction quality, as well as attacking downstream computer vision tasks. - How to train the attack to inject multiple triggers targeting different objectives into one model? The paper proposes joint training of the trigger generators and finetuning the encoder of the compression model.- How effective are the proposed attacks compared to baseline methods? Extensive experiments demonstrate the effectiveness of the frequency adaptive triggers in attacking compression models over spatial domain triggers.So in summary, the key novel contribution is an investigation into backdoor attack techniques tailored for deep learning based image compression models by utilizing frequency domain adaptive triggers.
