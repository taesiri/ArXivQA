# [Backdoor Attacks Against Deep Image Compression via Adaptive Frequency   Trigger](https://arxiv.org/abs/2302.14677)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the main research question seems to be: How can we perform effective backdoor attacks against deep learning based image compression models?Specifically, the authors investigate injecting backdoors into image compression models via adaptive frequency triggers. They explore different attack objectives such as degrading compression quality or attacking downstream computer vision tasks. The key hypothesis appears to be that by adding adaptive triggers in the frequency domain of images, they can successfully inject multiple backdoors into a single image compression model. Each backdoor is activated by a specific trigger pattern and leads to a malicious effect defined by the attack objective.In summary, the central research question is how to develop a methodology for performing stealthy yet effective backdoor attacks on deep learning based image compression models by using adaptive frequency domain triggers. The hypothesis is that this approach can allow injecting multiple backdoors with different objectives into a single model.


## What is the main contribution of this paper?

The main contributions of this paper are:- They propose a novel backdoor attack against deep learning based image compression models by injecting triggers adaptively in the frequency domain. - They comprehensively explore various attack objectives, including attacking compression quality measures like bit-rate and reconstruction quality, as well as attacking downstream computer vision tasks like face recognition and semantic segmentation.- They design a frequency-based trigger injection model that leverages priors from both spatial and frequency domains to generate poisoned images effectively. - They propose a simple yet effective dynamic loss to balance different terms in the joint training loss, which enables more efficient training.- They demonstrate that multiple triggers can be injected into one single compression model, with each trigger associated with a specific attack objective.- Extensive experiments validate that their attack can successfully degrade compression performance or mislead downstream tasks upon the presence of the corresponding trigger. The attack also has little impact on clean images.In summary, this is the first work that systematically investigates backdoor attack against deep learning based image compression. The proposed adaptive frequency trigger injection and comprehensive attacking scenarios make the attack more stealthy and effective.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points in the paper:The paper proposes a novel backdoor attack against deep learning based image compression models by injecting adaptive frequency domain triggers, which can effectively attack compression quality and downstream computer vision tasks with multiple triggers in a single model.


## How does this paper compare to other research in the same field?

Here are my thoughts on how this paper compares to other research in the field of backdoor attacks against image compression models:- This is the first paper I am aware of that explores backdoor attacks specifically against learned image compression models. Most prior work has focused on attacking high-level vision models like classifiers and segmenters. Studying attacks against low-level vision models like compression is an interesting new direction.- The proposed frequency-based adaptive trigger injection method is novel compared to prior work. Using DCT-domain triggers customized for each input image is an intuitive idea given DCT's prevalence in compression. This adaptive frequency trigger seems more suited for attacking compression than the fixed spatial triggers used in most prior backdoor attack papers.- The attack objectives considered are comprehensive, including both low-level compression metrics like bitrate and quality as well as high-level downstream task performance. This provides a thorough evaluation of the attack's capabilities.- Allowing multiple backdoor triggers to be injected into one model, each with a different objective, is a capability not seen in previous papers. It makes the attack more practical and dangerous.- Modifying only the encoder of a compression model rather than retraining the whole model makes the attack more feasible in real-world settings.- Compared to the baseline method, the proposed approach seems substantially more effective at achieving strong attack performance across different compression rates and downstream tasks.Overall, I believe this paper provides a thorough and rigorous exploration of backdoor attacks in a new problem domain of learned image compression. The adaptive frequency trigger method and comprehensive attack formulations seem like clear improvements over prior work. This attack could pose a real threat to the reliability of image compression systems.


## What future research directions do the authors suggest?

Based on my reading of the paper, here are some of the main future research directions suggested by the authors:- Investigating defense methods against backdoor attacks on learned image compression models. The authors note that defending against such attacks is an important direction for future work. - Exploring additional attack objectives beyond the ones studied in this paper. The authors comprehensively explore several attack objectives but suggest there may be other objectives worth investigating as well.- Studying the transferability of the backdoor attacks to other compression models and frameworks. The authors demonstrate attacks on two specific compression models, but suggest examining if the attacks transfer more broadly.- Considering dynamic triggers that change over time rather than static triggers. The triggers in this work are fixed, but the authors propose exploring adaptive triggers that could vary across images or over time.- Applying similar backdoor attack techniques to other low-level vision tasks beyond compression. The authors note these techniques may generalize to other tasks like denoising, super-resolution, etc.- Developing more advanced frequency-based triggers. The authors propose a frequency-based trigger design, but suggest further improvements could be made to the trigger generation methodology.- Exploring additional benign attack scenarios like privacy protection. The authors show an example of using backdoors for privacy protection and suggest more applications along these lines.In summary, the main future directions focus on defending against such attacks, generalizing the attacks to new models/tasks/objectives, and improving the trigger design and generation.


## Summarize the paper in one paragraph.

The paper presents a novel backdoor attack against deep learning-based image compression models. The key ideas are:- They propose a frequency-based trigger injection model that adds triggers adaptively in the DCT domain. This leverages the common use of DCT in compression systems. - They design attack objectives comprehensively, including attacking compression quality (bit-rate/PSNR) and downstream computer vision tasks like face recognition and semantic segmentation.- They inject multiple triggers into one model, with each trigger associated with a specific attack objective. This is enabled by jointly training the trigger generators and finetuning the compression encoder.- Experiments demonstrate their attack can successfully activate the backdoors and cause malicious impacts like increasing bit-rate or degrading reconstruction quality. The attacks are also resistant to common defense methods.In summary, this is the first work investigating backdoor attack against learned image compression. It proposes an adaptive frequency trigger injection approach and demonstrates successful attacks on multiple objectives within one model. The attacks pose security risks for the adoption of deep learning in compression applications.
