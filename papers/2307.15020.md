# [SuperCLUE: A Comprehensive Chinese Large Language Model Benchmark](https://arxiv.org/abs/2307.15020)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of this paper, the central research question appears to be: What is the best approach to construct a comprehensive benchmark for evaluating the capabilities of Chinese large language models (LLMs) that can reliably predict their performance from the user perspective in real-world scenarios?The key aspects of this research question are:- Focusing on Chinese LLMs specifically, rather than English or multilingual models.- Aiming to build a comprehensive benchmark that evaluates diverse capabilities, not just performance on a single task. - Wanting the benchmark to predict how models would perform when interacting with real users, not just score well on artificial test sets.- Considering different evaluation approaches like open-ended questions, closed multi-choice questions, and side-by-side user ratings.- Analyzing whether capabilities measured by different methods actually correlate to real user preferences.So in summary, the central hypothesis is that by combining open-ended conversational evaluation, closed multi-choice questions, and real user ratings, they can construct a benchmark for Chinese LLMs that reliably reflects performance from a user perspective. The paper then describes the creation of the SuperCLUE benchmark and presents experiments analyzing how well different evaluation methods correlate to real user preferences.
