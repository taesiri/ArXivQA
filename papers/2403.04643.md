# [QAQ: Quality Adaptive Quantization for LLM KV Cache](https://arxiv.org/abs/2403.04643)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Large language models (LLMs) show great potential for many NLP applications but their deployment is challenging due to the linear growth of the key-value (KV) cache with context length. 
- The KV cache stores computed key and value vectors from previous attention calculations for reuse later, but its size becomes a bottleneck for long context applications.
- Existing methods for compressing the KV cache rely on heuristics like attention scores which may wrongly evict essential cache and hurt model performance.

Proposed Solution - Quality Adaptive Quantization (QAQ):  
- Quantify KV cache to reduce its size, with separate strategies for key and value due to their different sensitivities.
- Handle exceptions to the "persistence of importance" theory using an attention window.  
- Dedicated outlier handling with mixed precision storage.

Key Insights:
1) Key cache is more sensitive to quantization than value cache.
2) Persistence of importance has exceptions - importance of some tokens changes suddenly.  
3) Outliers play a crucial role and need special handling.

Results:
- Achieves up to 10x compression of KV cache with minimal impact on model performance. 
- Outperforms existing attention-based replacement methods by ~2x.
- Handles outliers effectively with 4% storage overhead but 12-26% performance gain.

Main Contributions:
- First work to have separate quantization strategies for key and value cache.
- Identifies and handles exceptions to persistence of importance theory.
- Careful outlier handling for KV cache quantization.
- State-of-the-art compression for KV cache with minimal accuracy loss.

In summary, the paper proposes QAQ, a novel quantization scheme for compressing the KV cache in LLMs to address memory bottlenecks. By handling outliers and exceptions carefully, it achieves much better compression ratios than previous methods with almost no loss in accuracy.
