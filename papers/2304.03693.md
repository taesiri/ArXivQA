# [Model-Agnostic Gender Debiased Image Captioning](https://arxiv.org/abs/2304.03693)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is: How can we mitigate gender bias amplification in image captioning models by considering both context->gender bias and gender->context bias? The key points are:- Image captioning models tend to amplify gender bias present in training datasets, leading to issues like incorrect gender prediction and use of gender stereotypes. - The authors identify two main types of gender bias that affect captioning models:    1) Context->gender bias: Exploiting context to incorrectly predict gender    2) Gender->context bias: Associating gender with stereotypical words- Prior work has focused only on mitigating context->gender bias, but this can increase gender->context bias. - The authors propose a model-agnostic framework called LIBRA to mitigate both types of bias by:    - Synthetically generating biased captions with the two types of bias (Biased Caption Synthesis)    - Training a model to remove the bias from those synthetic captions (Debiasing Caption Generator)- Experiments show LIBRA reduces both context->gender and gender->context bias across various captioning models, correcting gender misclassification and stereotypical word usage.In summary, the central hypothesis is that considering both types of gender bias is necessary to effectively mitigate gender bias amplification in image captioning. LIBRA provides a model-agnostic approach to do this.


## What is the main contribution of this paper?

The main contribution of this paper is proposing a model-agnostic framework called LIBRA to mitigate gender bias amplification in image captioning models. Specifically, the key ideas and contributions are:- Identifying two types of gender biases that affect image captioning models: (1) context → gender bias, where models exploit context to incorrectly predict gender, and (2) gender → context bias, where models generate biased, stereotypical words based on predicted gender.- Proposing a Biased Caption Synthesis module to intentionally create biased captions containing the two types of gender biases. This is done by swapping gender words based on sentence classification, and using T5 masked language model to generate biased captions.- Developing a Debiasing Caption Generator that is trained on the synthetic biased captions to mitigate the biases and recover the original caption. This generator can be applied on top of any existing captioning model.- Demonstrating through extensive experiments that LIBRA reduces both types of gender biases in various captioning models without hurting caption quality. It also outperforms prior debiasing methods.In summary, the key contribution is developing a novel model-agnostic framework to mitigate two major types of gender biases in image captioning by leveraging intentionally generated biased captions.
