# [Grounding Data Science Code Generation with Input-Output Specifications](https://arxiv.org/abs/2402.08073)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Large language models (LLMs) have shown promise at generating code from natural language prompts. However, natural language is often too ambiguous to fully specify programming tasks, requiring additional input-output (I/O) specifications.  
- In data science programming, I/O specifications are essential to clarify tasks involving complex transformations on data structures like Pandas DataFrames.
- LLMs struggle to align their outputs with both the natural language intent and I/O specifications. This misalignment stems from the lack of training data containing such specifications.

Proposed Solution: 
- The paper proposes GIFT4Code, a novel approach to fine-tune LLMs to follow natural language intents along with I/O specifications.  
- GIFT4Code leverages synthetic data produced by the LLM itself, utilizing execution-derived feedback as a learning signal.
- The synthetic data generation involves:
   1) Using a generalist LLM to create natural language intents based on programmatic contexts like CSV files.
   2) Executing the code predicted by the code LLM for these intents to collect I/O variables. 
   3) Deriving 3 types of I/O specifications from execution results - variable types, I/O examples, I/O summaries.
   4) Augmenting intents with I/O specifications to create fine-tuning examples.
- By training on such synthetic data, the LLM learns to ground language intents to execution-based specifications.

Main Contributions:
- Proposes a methodology to create synthetic data for instruction tuning of code LLMs using execution-based I/O specifications.
- Demonstrates GIFT4Code's ability to enhance LLMs in producing code aligned with users' intents and complex I/O specifications.
- Analyzes the trade-offs between code executability and semantic correctness in LLMs.
- Tests the approach on two challenging data science code generation benchmarks - Arcade and DS1000K, showing significant improvements.

In summary, the paper makes important contributions regarding leveraging program execution to improve instruction following in code LLMs through a synthetic data generation technique.
