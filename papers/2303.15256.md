# [Active Self-Supervised Learning: A Few Low-Cost Relationships Are All   You Need](https://arxiv.org/abs/2303.15256)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, it seems the main research question is how to learn useful data representations without relying on labeled data. Specifically, it investigates self-supervised learning techniques that leverage only the structure of the unlabeled data itself to learn representations that can be used for downstream tasks. The key hypothesis appears to be that by enforcing certain invariances or symmetries in the learned representations, such as making them invariant to simple data augmentations/transformations, the representations will capture useful semantic properties of the data. The paper introduces and analyzes different self-supervised learning objectives like contrastive predictive coding, SimCLR, and BYOL that aim to achieve this goal in various ways.The main research contributions seem to be:- Proposing and empirically evaluating several new self-supervised learning algorithms for image classification. - Providing analysis on the connections between the different self-supervised objectives and how they enforce useful inductive biases.- Demonstrating strong performance of the learned representations on downstream tasks like image classification, suggesting they capture meaningful semantic properties despite being trained without labels.So in summary, the key focus is developing and understanding self-supervised representation learning techniques that can learn from unlabeled data only. The core hypothesis is that encouraging invariances and symmetries in the representations will induce useful semantic properties.


## What is the main contribution of this paper?

Based on my reading, the main contributions of this paper appear to be:- It provides a unified learning framework based on the concept of a similarity graph, which encompasses self-supervised learning, supervised learning, and semi-supervised learning. - It introduces a generic PAL (Positive Active Learning) algorithm that can query the underlying similarity graph through different types of oracles. This allows combining techniques from supervised, semi-supervised, and self-supervised learning.- It formulates active learning based on similarity queries rather than absolute labels. This provides a low-cost and efficient way to annotate datasets compared to traditional active learning.In summary, the paper introduces a similarity graph view of learning that unifies different paradigms, allows incorporating various kinds of a priori knowledge, and enables more efficient active learning through relative comparisons rather than absolute labels. The PAL algorithm provides a consistent way to query the similarity graph and leverage different types of information sources.
