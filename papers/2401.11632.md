# [What Are We Optimizing For? A Human-centric Evaluation Of Deep   Learning-based Recommender Systems](https://arxiv.org/abs/2401.11632)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Deep learning (DL) models for recommender systems are usually only evaluated on accuracy metrics and their performance on human-centric metrics like diversity, novelty, serendipity, trustworthiness etc. is not well studied. 
- It is not clear how different types of DL models perform on these human-centric metrics and what strategies can be used to optimize them.

Proposed Solution:
- Evaluate 5 different DL recommender system models (NCF, BERT4Rec, SSE-PT, GLocal-K, RippleNet) on 7 human-centric metrics - novelty, diversity, serendipity, perceived accuracy, transparency, trustworthiness and satisfaction.
- Conduct both offline evaluation on ML-1M dataset and online evaluation by surveying 445 real users. 
- Analyze model performance, relationship between metrics, impact factors and user preferences.
- Suggest model-specific optimization strategies and generalized human-centric design guidelines.

Key Contributions:
- First comprehensive human-centric evaluation and comparison of multiple DL recommender models.
- Analysis of causal relationships between human-centric metrics.
- Identification of key impact factors for subjective metrics like trust and transparency. 
- Formulation of model-wise optimization goals and generalized design guidelines to balance accuracy and human values.
- Revelation of contradiction between user preference for serendipity and its negative impact on accuracy and trust.
- Proposition that objective metrics need calibrated levels and cannot be blindly maximized.

In summary, this is a novel study that evaluates different DL recommender models on multi-dimensional human-centric metrics through extensive analysis. It provides valuable insights into optimizing these models for user experience rather than just accuracy.


## Summarize the paper in one sentence.

 This paper evaluates 5 deep learning recommender system models on 7 human-centric metrics through offline analysis and an online user survey, finding that different models have strengths and weaknesses on different metrics and proposing balancing accuracy with other metrics based on user preferences as an overall optimization strategy.


## What is the main contribution of this paper?

 Based on my understanding, the main contributions of this paper are:

1. The authors build a comprehensive human-centric evaluation framework that incorporates both offline benchmark data analysis and online real user feedback to evaluate 5 representative DL-based recommender system models across 7 dimensions: novelty, diversity, serendipity, perceived accuracy, transparency, trustworthiness, and satisfaction.

2. Through the evaluation, the authors identify the strengths and weaknesses of different types of DL models in satisfying various human-centric metrics. For example, neural collaborative filtering (NCF) models perform well in user trust and interpretability but need improvement in novelty and serendipity. 

3. The authors analyze the relationships and impact factors between different human-centric metrics, especially for the more subjective ones like transparency and trustworthiness. They find these subjective metrics are key contributors to user-perceived accuracy and satisfaction.

4. Based on both the model-wise evaluation and users' qualitative input, the authors propose optimization strategies and design implications for building human-centric DL-based recommender systems, such as balancing accuracy with other metrics to appropriate levels and understanding end user needs before algorithm design.

In summary, the key contribution is a comprehensive human-centric evaluation framework and findings that provide guidance on optimizing various DL recommender models and designing overall recommender systems to better cater to human values and needs.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and concepts are:

- Deep learning recommender systems (DL-RecSys)
- Human-centric evaluation 
- Novelty, diversity, serendipity 
- Trustworthiness, transparency, accuracy, satisfaction
- Neural collaborative filtering (NCF)
- BERT4Rec
- SSE-PT
- GLocal-K
- RippleNet
- MovieLens
- Multi-dimensional metrics
- User preferences
- Model optimization strategies

The paper evaluates different deep learning based recommender system models on multiple human-centric metrics beyond just accuracy, including novelty, diversity, serendipity, trustworthiness etc. It uses both offline evaluations on benchmark datasets and online user studies to compare models like NCF, BERT4Rec, SSE-PT, GLocal-K and RippleNet. The key goals are to understand model performance on these metrics, user preferences and needs, and provide model-specific and generalized optimization strategies for improving human-centric aspects.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the methods proposed in this paper:

1) What were the motivations for proposing a human-centric evaluation framework for deep learning based recommender systems? Why focus on human values beyond simple accuracy metrics?

2) How were the five deep learning models selected for evaluation (NCF, BERT4Rec, SSE-PT, GLocal-K, RippleNet)? What criteria or considerations guided the selection process? 

3) The paper evaluates model performance on both offline benchmarks and through online user studies. What are the relative advantages and limitations of each evaluation approach? How do they complement each other?

4) Seven human-centric metrics are used including novelty, diversity and serendipity. What quantitative formulas or definitions were used to calculate these metrics? What are limitations of these definitions?  

5) What survey design considerations were made in the online user study? How was the choice of questions and presentation format optimized to get quality responses?

6) Structural equation modeling was used to analyze relationships between different human-centric metrics. What were the key strengths of using this technique? How did it help reveal insights?

7) The SHAP technique was used to analyze impact factors for subjective metrics like trust and transparency. How does this method work? What new insights did it provide over correlation analysis?  

8) Three major clusters were identified in usersâ€™ qualitative inputs about recommender properties. What were they and what interesting tensions do they highlight in catering to diverse user needs?  

9) What were some of the key model-specific optimization strategies proposed based on the evaluation results? How can they help guide research for each model paradigm?

10) What implications do the results have for balancing accuracy with other human-centric metrics in general recommender system design? How can the right levels be determined?
