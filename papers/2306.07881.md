# [Viewset Diffusion: (0-)Image-Conditioned 3D Generative Models from 2D   Data](https://arxiv.org/abs/2306.07881)

## What is the central research question or hypothesis that this paper addresses?

 The central research question addressed in this paper is: 

How can we learn a generative model for 3D objects that supports both conditional and unconditional generation, using only 2D multi-view supervision during training?

Specifically, the paper proposes an approach to:

1) Learn a generative model over 3D objects by modelling and generating "viewsets" (collections of 2D views) instead of 3D models directly. This allows using only 2D training data.

2) Design the model such that the mapping between viewsets and 3D models is integrated into the generative model itself. This allows generating 3D consistent outputs while still training on 2D data. 

3) Support both single-view 3D reconstruction (conditional generation given an input view) as well as unconditional 3D generation within the same model framework.

4) Model ambiguities inherent in single-view 3D reconstruction by generating multiple plausible solutions.

In summary, the key hypothesis is that modelling and generating viewsets can act as a proxy for generating 3D models, even with only 2D supervision, and that this can be integrated into a single generative model for both conditional and unconditional generation. The paper proposes a novel method called Viewset Diffusion to achieve this.


## What is the main contribution of this paper?

 Based on my reading, the main contributions of this paper are:

1. The idea of generating viewsets (collections of 2D views of an object) as a way to apply denoising diffusion probabilistic models (DDPMs) to 3D object generation when only multi-view 2D supervision is available. 

2. An ambiguity-aware 3D reconstruction model that can sample different plausible reconstructions given a single input image. This model doubles as an unconditional 3D generator.

3. A network architecture that enables the model's reconstructions to match conditioning images, aggregate information from multiple views in an occlusion-aware manner, and estimate plausible 3D geometries.

4. A new synthetic benchmark dataset for evaluating single-image reconstruction techniques in ambiguous settings.

In summary, the key ideas are using viewsets to enable DDPM-based 3D generation from 2D data, modeling reconstruction ambiguity via sampling, and designing a suitable network architecture to aggregate information across views and predict 3D geometry. The contributions center around enabling probabilistic and ambiguity-aware 3D modeling and generation using only multi-view 2D supervision.
