# [p-Laplacian Adaptation for Generative Pre-trained Vision-Language Models](https://arxiv.org/abs/2312.10613)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem: 
As vision-language models (VLMs) grow rapidly in size, full fine-tuning becomes inefficient. Adapter-based methods are proposed to enable parameter-efficient transfer learning by inserting small trainable modules into VLMs. However, modeling adapter tuning coupled with attention reveals a heterophilic issue - the attention graph connecting projected query and value features is bipartite with distinct distributions, posing challenges for standard graph message passing in adapters.   

Method:
This paper proposes a new adapter called p-adapter, inspired by p-Laplacian graph neural networks designed for heterophilic graphs. Specifically:

1) Adapter tuning after attention is reformulated as spectral graph convolution on attention graphs, with projected features and attention weights being nodes and edge weights. The bipartite structure makes them heterophilic graphs.  

2) p-adapter incorporates node features to renormalize attention weights, which are used to aggregate features, enabling dynamic exploitation of varying-frequency information. The renormalization intensity is controlled by a layer-wise learnable p.

3) Extensive experiments on two VLMs (BLIP, mPLUG) over three tasks (VQA, VE, image captioning) demonstrate clear advantages of p-adapter over other PETL methods. With only 6.39% extra parameters, p-adapter matches full fine-tuning and significantly outperforms other adapters.

Contributions:
- Novel modeling of adapter tuning as graph message passing, identifying the heterophilic issue
- p-adapter architecture inspired by p-Laplacian GNNs to handle heterophilic attention graphs  
- Superior performance over state-of-the-art with high parameter efficiency
- Extensive empirical validation over multiple models and tasks

In summary, this paper provides a new perspective to analyze adapter tuning and proposes an effective solution that adapts VLMs in a parameter-efficient way. The modeling framework, adapter architecture and experimental results are significant contributions.
