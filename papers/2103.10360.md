# GLM: General Language Model Pretraining with Autoregressive Blank   Infilling

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the introduction, the central hypothesis of this paper seems to be that a single pretrained language model architecture can perform well across different natural language tasks including understanding, unconditional generation, and conditional generation by using a unified pretraining objective of autoregressive blank infilling. Specifically, the authors hypothesize that:1) Autoregressive blank infilling with span shuffling and 2D positional encoding can outperform models like BERT and T5 on natural language understanding tasks.2) Varying the number and length of masked spans for the autoregressive blank infilling objective allows pretraining a single model for both understanding and different types of generation tasks. 3) A single pretrained model with the proposed blank infilling objective can achieve strong performance on understanding, unconditional generation, and conditional generation tasks.In summary, the central hypothesis is that the proposed autoregressive blank infilling framework called GLM allows pretraining a single model to perform well across diverse NLP tasks by unifying them under the same pretraining formulation. The experiments aim to validate whether GLM matches or improves upon state-of-the-art models on each type of task.


## What is the main contribution of this paper?

Based on my reading, the main contributions of this paper are:1. Proposing a new pretraining framework called General Language Model (GLM) based on autoregressive blank infilling. This allows pretraining a single model that can handle NLU, unconditional text generation, and conditional text generation.2. Improving blank filling pretraining with techniques like span shuffling and 2D positional encoding. These enhancements allow GLM to outperform BERT and T5 on NLU tasks. 3. Showing that GLM can be adapted to different tasks by varying the number and lengths of blank spans during pretraining. A single GLM pretrained with multi-task learning achieves strong performance on multiple datasets spanning NLU, conditional generation, and language modeling.4. Reformulating NLU classification tasks as cloze-style generative tasks through blank filling. This improves consistency between pretraining and finetuning compared to standard classifiers. 5. Empirical results showing GLM outperforms existing models like BERT, T5, and GPT on various tasks using the same model size and training data. The multi-task GLM with 1.25x BERT-Large parameters achieves state-of-the-art results across multiple benchmarks.In summary, the key ideas are proposing GLM as a flexible pretraining framework, enhancing blank filling pretraining, and showing strong empirical performance from GLM on diverse NLP tasks with a single model.
