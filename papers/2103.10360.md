# [GLM: General Language Model Pretraining with Autoregressive Blank   Infilling](https://arxiv.org/abs/2103.10360)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the introduction, the central hypothesis of this paper seems to be that a single pretrained language model architecture can perform well across different natural language tasks including understanding, unconditional generation, and conditional generation by using a unified pretraining objective of autoregressive blank infilling. Specifically, the authors hypothesize that:1) Autoregressive blank infilling with span shuffling and 2D positional encoding can outperform models like BERT and T5 on natural language understanding tasks.2) Varying the number and length of masked spans for the autoregressive blank infilling objective allows pretraining a single model for both understanding and different types of generation tasks. 3) A single pretrained model with the proposed blank infilling objective can achieve strong performance on understanding, unconditional generation, and conditional generation tasks.In summary, the central hypothesis is that the proposed autoregressive blank infilling framework called GLM allows pretraining a single model to perform well across diverse NLP tasks by unifying them under the same pretraining formulation. The experiments aim to validate whether GLM matches or improves upon state-of-the-art models on each type of task.


## What is the main contribution of this paper?

 Based on my reading, the main contributions of this paper are:1. Proposing a new pretraining framework called General Language Model (GLM) based on autoregressive blank infilling. This allows pretraining a single model that can handle NLU, unconditional text generation, and conditional text generation.2. Improving blank filling pretraining with techniques like span shuffling and 2D positional encoding. These enhancements allow GLM to outperform BERT and T5 on NLU tasks. 3. Showing that GLM can be adapted to different tasks by varying the number and lengths of blank spans during pretraining. A single GLM pretrained with multi-task learning achieves strong performance on multiple datasets spanning NLU, conditional generation, and language modeling.4. Reformulating NLU classification tasks as cloze-style generative tasks through blank filling. This improves consistency between pretraining and finetuning compared to standard classifiers. 5. Empirical results showing GLM outperforms existing models like BERT, T5, and GPT on various tasks using the same model size and training data. The multi-task GLM with 1.25x BERT-Large parameters achieves state-of-the-art results across multiple benchmarks.In summary, the key ideas are proposing GLM as a flexible pretraining framework, enhancing blank filling pretraining, and showing strong empirical performance from GLM on diverse NLP tasks with a single model.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the paper:The paper proposes a new pretraining framework called General Language Model (GLM) based on autoregressive blank infilling that can handle natural language understanding, unconditional text generation, and conditional text generation in a unified way.


## How does this paper compare to other research in the same field?

 Based on my understanding, here are a few key ways this paper on General Language Model (GLM) pretraining compares to other related work:- It proposes a unified pretraining framework that can handle both natural language understanding (NLU) and text generation tasks. Other models tend to focus on one or the other - e.g. BERT for NLU, GPT for generation. GLM aims to combine the strengths through autoregressive blank infilling.- The proposed GLM architecture shares parameters between bidirectional and unidirectional attention. This differs from encoder-decoder models like T5 which have separate parameters. GLM is argued to be more parameter-efficient. - GLM handles variable-length blank infilling naturally via the proposed 2D positional encodings and autoregressive modeling. This is unlike BERT which assumes independence between masked tokens. It also avoids issues like T5 needing sentinel tokens to differentiate spans.- For NLU tasks, GLM reformulates classification as conditional text generation via cloze-style prompting. This improves consistency between pretraining and finetuning compared to classifier-based approaches.- GLM achieves strong performance on both NLU and generation tasks. With similar model size and data, it outperforms standalone BERT, GPT, and T5 models on respective tasks. The multi-task GLM also outperforms previous unified models like UniLM.- The work doesn't scale up to models with billions of parameters like GPT-3. But GLM aims to be more parameter-efficient and generalizable with the proposed architecture innovations.In summary, GLM differentiates itself by its unified architecture, innovations like 2D positional encoding, strong performance across NLU and generation, and parameter efficiency compared to other models. The unified pretraining framework is a novel contribution.


## What future research directions do the authors suggest?

 Based on my reading of the paper, some of the main future research directions suggested by the authors include:- Scaling GLM to larger Transformer models and more training data. The authors mention wanting to examine GLM's performance with larger parameter sizes and datasets.- Evaluating GLM in more settings like knowledge probing and few-shot learning. The authors suggest evaluating how well GLM can perform tasks that require world knowledge or learning from small amounts of data.- Exploring different mixtures of pretraining objectives. The paper proposes mixing short-span and long-span generation objectives, but other mixtures could be tried as well. - Testing other task formulations besides cloze-style questions. While cloze questions improved consistency between pretraining and finetuning, other task reformulations may be worth exploring too.- Applying GLM to more downstream tasks. The authors demonstrate GLM on a range of tasks, but trying it on more NLU, conditional generation, and unconditional generation tasks could further demonstrate its capabilities.- Analyzing what linguistic abilities GLM acquires during pretraining. Probing the knowledge and capabilities learned by GLM during pretraining could provide insight into what makes it effective across different tasks.- Developing theoretical understandings of why GLM works well. The authors provide empirical evidence of GLM's effectiveness, but formal theoretical analysis of why the model architecture and training approach succeed would be valuable.In summary, the authors suggest directions like scaling GLM, evaluating it in more settings, exploring different mixtures of objectives, applying it to more tasks, probing what it learns, and developing theoretical understandings of it. Advancing GLM along these directions could further reveal its potential as a general language model.


## Summarize the paper in one paragraph.

 The paper proposes GLM, a general pretraining framework for natural language understanding and generation. GLM is based on autoregressive blank infilling, where spans of text are masked out and predicted autoregressively. This allows GLM to handle variable-length generation for tasks like completion of cloze questions. GLM improves upon blank filling in T5 via span shuffling and 2D positional encodings. Empirically, GLM outperforms BERT and T5 on the SuperGLUE benchmark for NLU with fewer parameters and less data. GLM also achieves strong performance on summarization, question generation, text infilling and language modeling compared to encoder-decoder and autoencoder models. Multi-task pretraining enables GLM to share parameters effectively across different tasks. The consistent formulation of pretraining and finetuning as blank filling is a notable advantage of GLM.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the paper:The paper proposes a new pretraining framework called General Language Model (GLM) based on autoregressive blank infilling. GLM randomly blanks out continuous spans in the input text and trains the model to sequentially reconstruct the spans in an autoregressive manner. Compared to previous models like BERT and T5, GLM adds improvements like span shuffling and 2D positional encoding. Empirically, GLM outperforms BERT on natural language understanding tasks and requires fewer parameters and less data than T5 to achieve strong performance on both understanding and generation tasks. The key benefit of GLM is its flexibility - it can be adapted for different tasks by varying the number and length of masked spans. For NLU tasks, it masks 15% short spans and is finetuned by formulating classification as cloze-style blank infilling. For conditional text generation, it masks longer spans up to full documents. The same parameters can be shared for both types of tasks, unlike BERT or GPT which are suited for only one. Multi-task training further improves GLM's generalizability. Compared to baselines, GLM achieves the best overall performance on diverse NLU, unconditional and conditional generation benchmarks using only 1.25x parameters of BERT.


## Summarize the main method used in the paper in one paragraph.

 Here is a one paragraph summary of the main method described in the paper:The paper proposes a new pretraining framework called General Language Model (GLM) based on autoregressive blank infilling. The key idea is to randomly blank out continuous spans of tokens from the input text, following the idea of autoencoding models like BERT. But unlike BERT which uses masked language modeling, GLM is trained to sequentially reconstruct the masked spans in an autoregressive manner. This allows GLM to handle variable-length text generation for the masked spans. The paper proposes two improvements over prior blank infilling methods - shuffling the order of masked spans and using 2D positional encodings. By varying the number and lengths of masked spans, GLM can be adapted for NLU tasks with short spans or text generation tasks with longer spans. The consistency between pretraining and finetuning enables GLM to outperform BERT and T5 on downstream tasks. The multi-task pretraining setup allows a single GLM model to perform well across different types of NLP tasks.


## What problem or question is the paper addressing?

 The paper is addressing the issue that existing pretrained language models fall into three families - autoregressive, autoencoding, and encoder-decoder models - and none of them performs the best across natural language understanding (NLU), unconditional text generation, and conditional text generation tasks. Specifically:- Autoregressive models like GPT are good at unconditional text generation but limited for NLU due to their unidirectional nature. - Autoencoding models like BERT excel at NLU via bidirectional context encoding but cannot directly be applied to text generation.- Encoder-decoder models are used for conditional text generation but require more parameters to match autoencoding models on NLU.The paper proposes a new pretraining framework called General Language Model (GLM) based on autoregressive blank infilling to address this limitation. GLM aims to perform well across NLU, unconditional and conditional text generation within a single model.
