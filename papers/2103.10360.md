# GLM: General Language Model Pretraining with Autoregressive Blank   Infilling

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the introduction, the central hypothesis of this paper seems to be that a single pretrained language model architecture can perform well across different natural language tasks including understanding, unconditional generation, and conditional generation by using a unified pretraining objective of autoregressive blank infilling. Specifically, the authors hypothesize that:1) Autoregressive blank infilling with span shuffling and 2D positional encoding can outperform models like BERT and T5 on natural language understanding tasks.2) Varying the number and length of masked spans for the autoregressive blank infilling objective allows pretraining a single model for both understanding and different types of generation tasks. 3) A single pretrained model with the proposed blank infilling objective can achieve strong performance on understanding, unconditional generation, and conditional generation tasks.In summary, the central hypothesis is that the proposed autoregressive blank infilling framework called GLM allows pretraining a single model to perform well across diverse NLP tasks by unifying them under the same pretraining formulation. The experiments aim to validate whether GLM matches or improves upon state-of-the-art models on each type of task.


## What is the main contribution of this paper?

Based on my reading, the main contributions of this paper are:1. Proposing a new pretraining framework called General Language Model (GLM) based on autoregressive blank infilling. This allows pretraining a single model that can handle NLU, unconditional text generation, and conditional text generation.2. Improving blank filling pretraining with techniques like span shuffling and 2D positional encoding. These enhancements allow GLM to outperform BERT and T5 on NLU tasks. 3. Showing that GLM can be adapted to different tasks by varying the number and lengths of blank spans during pretraining. A single GLM pretrained with multi-task learning achieves strong performance on multiple datasets spanning NLU, conditional generation, and language modeling.4. Reformulating NLU classification tasks as cloze-style generative tasks through blank filling. This improves consistency between pretraining and finetuning compared to standard classifiers. 5. Empirical results showing GLM outperforms existing models like BERT, T5, and GPT on various tasks using the same model size and training data. The multi-task GLM with 1.25x BERT-Large parameters achieves state-of-the-art results across multiple benchmarks.In summary, the key ideas are proposing GLM as a flexible pretraining framework, enhancing blank filling pretraining, and showing strong empirical performance from GLM on diverse NLP tasks with a single model.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper proposes a new pretraining framework called General Language Model (GLM) based on autoregressive blank infilling that can handle natural language understanding, unconditional text generation, and conditional text generation in a unified way.


## How does this paper compare to other research in the same field?

Based on my understanding, here are a few key ways this paper on General Language Model (GLM) pretraining compares to other related work:- It proposes a unified pretraining framework that can handle both natural language understanding (NLU) and text generation tasks. Other models tend to focus on one or the other - e.g. BERT for NLU, GPT for generation. GLM aims to combine the strengths through autoregressive blank infilling.- The proposed GLM architecture shares parameters between bidirectional and unidirectional attention. This differs from encoder-decoder models like T5 which have separate parameters. GLM is argued to be more parameter-efficient. - GLM handles variable-length blank infilling naturally via the proposed 2D positional encodings and autoregressive modeling. This is unlike BERT which assumes independence between masked tokens. It also avoids issues like T5 needing sentinel tokens to differentiate spans.- For NLU tasks, GLM reformulates classification as conditional text generation via cloze-style prompting. This improves consistency between pretraining and finetuning compared to classifier-based approaches.- GLM achieves strong performance on both NLU and generation tasks. With similar model size and data, it outperforms standalone BERT, GPT, and T5 models on respective tasks. The multi-task GLM also outperforms previous unified models like UniLM.- The work doesn't scale up to models with billions of parameters like GPT-3. But GLM aims to be more parameter-efficient and generalizable with the proposed architecture innovations.In summary, GLM differentiates itself by its unified architecture, innovations like 2D positional encoding, strong performance across NLU and generation, and parameter efficiency compared to other models. The unified pretraining framework is a novel contribution.


## What future research directions do the authors suggest?

Based on my reading of the paper, some of the main future research directions suggested by the authors include:- Scaling GLM to larger Transformer models and more training data. The authors mention wanting to examine GLM's performance with larger parameter sizes and datasets.- Evaluating GLM in more settings like knowledge probing and few-shot learning. The authors suggest evaluating how well GLM can perform tasks that require world knowledge or learning from small amounts of data.- Exploring different mixtures of pretraining objectives. The paper proposes mixing short-span and long-span generation objectives, but other mixtures could be tried as well. - Testing other task formulations besides cloze-style questions. While cloze questions improved consistency between pretraining and finetuning, other task reformulations may be worth exploring too.- Applying GLM to more downstream tasks. The authors demonstrate GLM on a range of tasks, but trying it on more NLU, conditional generation, and unconditional generation tasks could further demonstrate its capabilities.- Analyzing what linguistic abilities GLM acquires during pretraining. Probing the knowledge and capabilities learned by GLM during pretraining could provide insight into what makes it effective across different tasks.- Developing theoretical understandings of why GLM works well. The authors provide empirical evidence of GLM's effectiveness, but formal theoretical analysis of why the model architecture and training approach succeed would be valuable.In summary, the authors suggest directions like scaling GLM, evaluating it in more settings, exploring different mixtures of objectives, applying it to more tasks, probing what it learns, and developing theoretical understandings of it. Advancing GLM along these directions could further reveal its potential as a general language model.
