# [Data-Efficient Augmentation for Training Neural Networks](https://arxiv.org/abs/2210.08363)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the main research question is: How can we efficiently select small subsets of training data such that augmenting only those subsets provides similar benefits to augmenting the full dataset? The key hypothesis is that there exist small weighted subsets (coresets) of the training data that closely capture the alignment of the neural network's Jacobian matrix (containing all its first-order partial derivatives) with the labels/residuals. If such coresets can be identified, then augmenting only those subsets can give similar improvements in training dynamics and generalization as augmenting the full dataset.In more detail, the paper:- Theoretically analyzes how data augmentation (modeled as additive perturbations) affects the singular values and vectors of the neural network Jacobian. Shows augmentation enlarges the smaller singular values more, while preserving the prominent singular vectors.- Proposes an iterative method to extract weighted coresets that capture the Jacobian-residual alignment. Shows augmenting these coresets can mimic full data augmentation.- Empirically demonstrates that augmenting the proposed coresets improves accuracy and efficiency over augmenting random/max-loss subsets across various models and datasets.So in summary, the key hypothesis is that small weighted coresets capturing the Jacobian-residual alignment can enable efficient and effective data augmentation. The paper aims to propose and validate an approach for extracting such coresets.
