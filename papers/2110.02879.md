# [Nested Policy Reinforcement Learning](https://arxiv.org/abs/2110.02879)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research questions/hypotheses appear to be:1. Can a nested Q-learning algorithm called Nested Policy Fitted Q-Iteration (NFQI) learn good policies for environments with nested structure, where there are two groups that share some structure but also have distinct dynamics? 2. How does NFQI compare to regular Fitted Q-Iteration (FQI) and other baseline methods like transfer learning in terms of performance in these nested environments?3. Can NFQI yield interpretable policies that rely on relevant state features? 4. Is NFQI robust to sample size imbalance between the groups?5. When there is no real group structure in the environment, does NFQI gracefully revert back to regular FQI?The key idea seems to be developing a reinforcement learning approach that can learn optimal policies for two related but distinct groups/environments, while sharing strength between them and accounting for differences. The experiments aim to validate whether NFQI can achieve this versus baseline approaches.In summary, the main hypothesis appears to be that NFQI can learn better performing and more interpretable policies compared to regular Q-learning methods in environments exhibiting a specific nested structure between groups. The experiments aim to demonstrate and validate this central premise.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contributions appear to be:1. The introduction of a new reinforcement learning algorithm called Nested Policy Fitted Q-Iteration (NFQI). This extends standard FQI to account for nested environments that have predefined background and foreground groups with different dynamics but a shared reward function. 2. A two-stage training procedure for NFQI inspired by transfer learning. In the first stage, shared model parameters are trained on all data. In the second stage, foreground-specific parameters are trained only on foreground data.3. Demonstrating that NFQI can learn performant policies that rely on relevant features, handle sample size imbalance between background and foreground groups, and reduce to standard FQI when there is no meaningful group structure. This is shown through experiments on a simulated Cartpole environment and a clinical decision task using real electronic health records.4. Providing a general framework for reinforcement learning in nested environments that is model agnostic. The NFQI framework can incorporate different function classes like neural networks and random forests to approximate the Q-function.In summary, the main contribution seems to be the proposal of NFQI as a novel RL algorithm suited for problems with nested structure between environments/groups, along with empirical demonstrations of its desirable properties. The method allows learning distinct but related policies by sharing strength across groups.
