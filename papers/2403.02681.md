# [SGD with Partial Hessian for Deep Neural Networks Optimization](https://arxiv.org/abs/2403.02681)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper "SGD with Partial Hessian for Deep Neural Networks Optimization":

Problem:
- Training deep neural networks (DNNs) typically relies on first-order optimization methods like stochastic gradient descent (SGD), which only utilize gradient information. 
- Second-order optimization methods that incorporate Hessian information can accelerate training and improve performance, but computing the full Hessian matrix is infeasible for large DNNs. 
- Existing approximations of the Hessian tend to be imprecise, leading to unstable training performance.

Proposed Solution:
- The paper proposes a compound optimizer called SGD with Partial Hessian (SGD-PH) that combines SGD with partial but precise Hessian information.
- It analyzes the Hessian matrix for channel-wise 1D parameters commonly used in normalization layers like batch norm. This partial Hessian matrix is proven to be diagonal, allowing precise extraction via Hessian-free methods.
- SGD-PH applies a second-order update with the precise partial Hessian to 1D parameters in normalization layers, while retaining a first-order SGD update for other parameters.

Contributions:
- Mathematical proof that the Hessian matrix for channel-wise 1D parameters in normalization layers is diagonal, allowing efficient extraction.
- Introduction of SGD-PH, a novel compound optimizer that combines first and second-order updates.
- Extensive experiments on CIFAR and ImageNet datasets demonstrating SGD-PH's superior performance over both first and second-order optimizers.
- Analysis showing SGD-PH inherits the generalizability of first-order methods while benefiting from second-order information.
- Demonstration that SGD-PH generalizes to DNNs without normalization layers by decoupling convolutions.

In summary, the paper proposes SGD-PH, a hybrid first/second-order optimizer that extracts precise partial Hessian information to improve DNN training efficiency and performance over state-of-the-art techniques.
