# [Causal Prompting: Debiasing Large Language Model Prompting based on   Front-Door Adjustment](https://arxiv.org/abs/2403.02738)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Existing prompting methods like in-context learning (ICL) and chain-of-thought (CoT) for large language models (LLMs) still face challenges of various biases, such as label bias and recency bias. 
- Traditional debiasing methods focus on the model training stage, which have limitations for addressing complex biases of LLMs.

Proposed Solution:
- Uncover the causal relationship behind prompting methods using a structural causal model (SCM). The input prompt is X, output answer is A, and confounder introducing bias is U.
- Propose a novel causal prompting method based on front-door adjustment to mitigate LLM bias, without needing access to U or model internals.  
- Use LLM's chain-of-thoughts R as mediator between X and A. Decompose the causal effect of X on A into two parts using R.
- Estimate effect of X on R by combining CoT prompting and BERT-based clustering. Estimate effect of R on A by combining in-context learning and BERT-based NWGM approximation.
- Use contrastive learning to align encoder and LLM representations to estimate causal effects more accurately.

Main Contributions:
- First work to uncover and analyze bias problem in LLM prompting using causal inference perspective. Propose front-door adjustment to solve it.  
- Innovate causal intervention by prompt design without accessing LLM internals.
- Effectively combine CoT, self-consistency and ICL through front-door adjustment to mitigate LLM bias.  
- Use contrastive learning to align encoder and LLM representations for better causal effect estimation.
- Achieve excellent performance on 3 NLP datasets with both open-source and closed-source LLMs.


## Summarize the paper in one sentence.

 Here is a one-sentence summary of the paper:

The paper proposes Causal Prompting, a novel prompting method for large language models that mitigates model bias by estimating the causal effect between input prompts and model answers using front-door adjustment based on the models' chain-of-thought reasoning.


## What is the main contribution of this paper?

 The main contributions of this paper are:

1. It is the first work to uncover and analyze the bias problem in the prompting method of large language models (LLMs) from the perspective of causal inference. Moreover, it proposes using front-door adjustment to solve the bias problem in prompting.

2. It proposes using contrastive learning to fine-tune the encoder of the samples by aligning the space of the encoder with the LLM. This allows more precise estimation of the causal effects for debiasing. 

3. The proposed approach, called Causal Prompting, achieves excellent performance on 3 natural language processing datasets on both open-source and closed-source LLMs. It combines chain-of-thought, self-consistency, and in-context learning effectively through front-door adjustment to mitigate biases.

In summary, the main contribution is proposing a novel prompting method called Causal Prompting that leverages causal inference and contrastive learning to mitigate biases in LLMs for few-shot learning scenarios. It demonstrates strong empirical performance across multiple datasets and LLMs.


## What are the keywords or key terms associated with this paper?

 Based on my review of the paper, some of the key terms and concepts associated with it are:

- Large language models (LLMs)
- Prompting methods
    - In-context learning (ICL)
    - Chain-of-thought (CoT) prompting
    - Self-consistency (SC)
- Bias mitigation
- Causal inference
    - Structural causal model (SCM) 
    - Backdoor adjustment
    - Front-door adjustment
- Contrastive learning
- Encoder fine-tuning
- Performance on NLP tasks
    - Aspect-based sentiment analysis (ABSA)
    - Natural language inference (NLI) 
    - Fact verification (FV)

The paper proposes a novel prompting method called "Causal Prompting" that uses front-door adjustment from causal inference to mitigate biases in LLMs. It combines CoT, SC, and ICL through this adjustment. It also uses contrastive learning to fine-tune the encoder to align its representation space with the LLM. The method is evaluated on ABSA, NLI, and FV tasks using both open-source and closed-source LLMs.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in this paper:

1. The paper proposes using front-door adjustment for debiasing prompting methods. Can you explain in more detail why back-door adjustment is not feasible in this setting and how front-door adjustment addresses the limitations?

2. Chain-of-thought is used as the mediator variable between the input prompt and output answer in the causal graph. What are the advantages of using chain-of-thought compared to other possible mediator variables? How does it help enable the front-door adjustment?

3. The first part of the method estimates the causal effect P(r|do(X)) using chain-of-thought prompting and clustering. Why is clustering necessary here and how does it allow estimating probabilities for closed-sourced LLMs? 

4. For the second part P(A|do(r)), the paper uses a prompt version of NWGM approximation. Can you explain the intuition behind NWGM and why it is needed for back-door adjustment in this setting?

5. Contrastive learning is used to align representation spaces of the BERT encoder and LLM. Why is this alignment important? How does it improve the effectiveness of causal interventions?

6. The method combines chain-of-thought, self-consistency, and in-context learning in an innovative way. Can you analyze the connections and interactions between these different prompting strategies? 

7. What are some limitations of using front-door adjustment compared to back-door adjustment if confounders were observable? When would you prefer one over the other?

8. The evaluation uses both synthetic adversarial datasets and human-annotated real adversarial datasets. What are the tradeoffs between these two types of datasets for evaluating debiasing methods?

9. How suitable do you think this method would be for other modalities like vision LLMs? What changes would need to be made to apply it there?

10. The paper focused on classification tasks. How could this method be extended to more complex tasks like open-domain QA wheremultiple generated answers need to be handled?
