# Double Trouble: How to not explain a text classifier's decisions using   counterfactuals synthesized by masked language models?

## What is the central research question or hypothesis that this paper addresses?

The central research question addressed in this paper is: Do plausible counterfactuals generated by masked language models like BERT actually improve the faithfulness and accuracy of attribution methods for explaining text classifier decisions?The key hypotheses tested are:1) Input marginalization (IM) using BERT to generate plausible counterfactual texts will produce more faithful and accurate explanations compared to simple leave-one-out (LOO) baselines that delete words. This is based on the hypothesis that the LOO counterfactuals are too unrealistic. 2) Integrating BERT into LIME to generate more plausible counterfactual samples with multiple words replaced will improve LIME's explanations compared to the original LIME using unrealistic samples.The authors rigorously test these hypotheses across multiple datasets and evaluation metrics. Their main findings are:1) The Deletion metric used in prior work to claim IM improves faithfulness is inherently biased. When using unbiased metrics, there is no evidence IM outperforms LOO.2) Replacing a single word with BERT in IM does not improve faithfulness, likely because deleting one word has little effect on the classifier. However, replacing multiple words in LIME does improve faithfulness.So in summary, the paper shows that generating plausible counterfactuals only sometimes improves explanation faithfulness, depending on the attribution method and how drastically the inputs are perturbed. The common assumption that more realistic inputs always improve faithfulness is questioned.


## What is the main contribution of this paper?

The main contributions of this paper are:1. It demonstrates that the Deletion metric used by prior work to evaluate attribution methods is biased towards Input Marginalization (IM), causing a misleading conclusion that IM generates better explanations than Leave-One-Out (LOO) baselines. The authors show that the original Deletion metric favors LOO while its variant Deletion-BERT favors IM. This bias likely exists in other common Deletion variants as well. 2. Through extensive experiments on 3 datasets and 6 evaluation metrics, the authors find no evidence that IM generates more accurate explanations than a simple LOO baseline. Specifically, LOO outperforms or is on par with IM on metrics like ROAR, human annotation alignment, and sanity checks. 3. The paper provides insights into why IM is not more effective than LOO in practice. First, deleting only a single word has a marginal effect on classifier predictions, questioning the need for realistic counterfactuals. Second, IM assigns near-zero attribution to predictable words regardless of their true importance, revealing a key weakness of relying on a pretrained language model like BERT.4. Replacing multiple words using BERT consistently improves LIME's faithfulness, suggesting potential benefits of this technique when perturbing larger phrases instead of individual words. But integrating BERT into LOO or LIME does not improve alignment with human annotations.5. Overall, the paper underscores troubling biases in common XAI evaluation metrics and the lack of grounded evaluation in prior work. It challenges the assumed effectiveness of IM for text explanation.In summary, the key contribution is a rigorous re-evaluation of IM using properly designed experiments which reveal that IM does not improve faithfulness over simple LOO baselines, despite prior claims. The authors provide empirical and theoretical reasons for why IM has limitations in practice.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper finds that using BERT to generate more plausible counterfactual examples does not improve the faithfulness of attribution methods like Input Marginalization (IM) and LIME for explaining neural network predictions, contrary to what prior work hypothesized. The authors show that deletion-based evaluation metrics are biased, and under more rigorous evaluation, simple deletion baseline methods like leave-one-out perform as well as or better than IM and LIME-BERT. Overall, the paper challenges the assumption made in prior work that more realistic counterfactuals generated by language models like BERT will produce better explanations of model behavior.
