# Double Trouble: How to not explain a text classifier's decisions using   counterfactuals synthesized by masked language models?

## What is the central research question or hypothesis that this paper addresses?

The central research question addressed in this paper is: Do plausible counterfactuals generated by masked language models like BERT actually improve the faithfulness and accuracy of attribution methods for explaining text classifier decisions?The key hypotheses tested are:1) Input marginalization (IM) using BERT to generate plausible counterfactual texts will produce more faithful and accurate explanations compared to simple leave-one-out (LOO) baselines that delete words. This is based on the hypothesis that the LOO counterfactuals are too unrealistic. 2) Integrating BERT into LIME to generate more plausible counterfactual samples with multiple words replaced will improve LIME's explanations compared to the original LIME using unrealistic samples.The authors rigorously test these hypotheses across multiple datasets and evaluation metrics. Their main findings are:1) The Deletion metric used in prior work to claim IM improves faithfulness is inherently biased. When using unbiased metrics, there is no evidence IM outperforms LOO.2) Replacing a single word with BERT in IM does not improve faithfulness, likely because deleting one word has little effect on the classifier. However, replacing multiple words in LIME does improve faithfulness.So in summary, the paper shows that generating plausible counterfactuals only sometimes improves explanation faithfulness, depending on the attribution method and how drastically the inputs are perturbed. The common assumption that more realistic inputs always improve faithfulness is questioned.


## What is the main contribution of this paper?

The main contributions of this paper are:1. It demonstrates that the Deletion metric used by prior work to evaluate attribution methods is biased towards Input Marginalization (IM), causing a misleading conclusion that IM generates better explanations than Leave-One-Out (LOO) baselines. The authors show that the original Deletion metric favors LOO while its variant Deletion-BERT favors IM. This bias likely exists in other common Deletion variants as well. 2. Through extensive experiments on 3 datasets and 6 evaluation metrics, the authors find no evidence that IM generates more accurate explanations than a simple LOO baseline. Specifically, LOO outperforms or is on par with IM on metrics like ROAR, human annotation alignment, and sanity checks. 3. The paper provides insights into why IM is not more effective than LOO in practice. First, deleting only a single word has a marginal effect on classifier predictions, questioning the need for realistic counterfactuals. Second, IM assigns near-zero attribution to predictable words regardless of their true importance, revealing a key weakness of relying on a pretrained language model like BERT.4. Replacing multiple words using BERT consistently improves LIME's faithfulness, suggesting potential benefits of this technique when perturbing larger phrases instead of individual words. But integrating BERT into LOO or LIME does not improve alignment with human annotations.5. Overall, the paper underscores troubling biases in common XAI evaluation metrics and the lack of grounded evaluation in prior work. It challenges the assumed effectiveness of IM for text explanation.In summary, the key contribution is a rigorous re-evaluation of IM using properly designed experiments which reveal that IM does not improve faithfulness over simple LOO baselines, despite prior claims. The authors provide empirical and theoretical reasons for why IM has limitations in practice.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper finds that using BERT to generate more plausible counterfactual examples does not improve the faithfulness of attribution methods like Input Marginalization (IM) and LIME for explaining neural network predictions, contrary to what prior work hypothesized. The authors show that deletion-based evaluation metrics are biased, and under more rigorous evaluation, simple deletion baseline methods like leave-one-out perform as well as or better than IM and LIME-BERT. Overall, the paper challenges the assumption made in prior work that more realistic counterfactuals generated by language models like BERT will produce better explanations of model behavior.


## How does this paper compare to other research in the same field?

Here is a high-level comparison of this paper to other related work in explainable AI:- This paper focuses specifically on evaluating Input Marginalization (IM) for generating counterfactual explanations in NLP. IM uses BERT to generate more realistic counterfactuals by replacing words rather than just deleting them. - The authors find that IM does not actually improve faithfulness of explanations compared to simpler Leave-One-Out (LOO) baselines that just delete words. This challenges prior assumptions and claims that IM produces better explanations.- Prior work like Kim et al. 2020 and Hase et al. 2021 hypothesize that more realistic counterfactuals from IM should improve explanation faithfulness, but don't rigorously evaluate this. This paper provides a more thorough assessment.- Other related work like Goyal et al. 2019 and Agarwal et al. 2020 try to generate more realistic counterfactuals in vision tasks, but effectiveness for improving explanations is still unclear. This paper suggests realistic counterfactuals may not help in NLP.- The paper also reveals issues with common evaluation metrics like deletion/insertion that can be biased. This is an important methodological contribution for the field.- For LIME, which modifies multiple words, making counterfactuals more realistic with BERT helps. This aligns with intuitions, but shows that word removal effects are different.In summary, this paper advances our understanding of how counterfactual generation affects explanation faithfulness in NLP, while challenging prior assumptions. The rigor of the evaluations and analysis of biases/limitations are excellent contributions.
