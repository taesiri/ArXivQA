# [Double Trouble: How to not explain a text classifier's decisions using   counterfactuals synthesized by masked language models?](https://arxiv.org/abs/2110.11929)

## What is the central research question or hypothesis that this paper addresses?

The central research question addressed in this paper is: Do plausible counterfactuals generated by masked language models like BERT actually improve the faithfulness and accuracy of attribution methods for explaining text classifier decisions?The key hypotheses tested are:1) Input marginalization (IM) using BERT to generate plausible counterfactual texts will produce more faithful and accurate explanations compared to simple leave-one-out (LOO) baselines that delete words. This is based on the hypothesis that the LOO counterfactuals are too unrealistic. 2) Integrating BERT into LIME to generate more plausible counterfactual samples with multiple words replaced will improve LIME's explanations compared to the original LIME using unrealistic samples.The authors rigorously test these hypotheses across multiple datasets and evaluation metrics. Their main findings are:1) The Deletion metric used in prior work to claim IM improves faithfulness is inherently biased. When using unbiased metrics, there is no evidence IM outperforms LOO.2) Replacing a single word with BERT in IM does not improve faithfulness, likely because deleting one word has little effect on the classifier. However, replacing multiple words in LIME does improve faithfulness.So in summary, the paper shows that generating plausible counterfactuals only sometimes improves explanation faithfulness, depending on the attribution method and how drastically the inputs are perturbed. The common assumption that more realistic inputs always improve faithfulness is questioned.


## What is the main contribution of this paper?

The main contributions of this paper are:1. It demonstrates that the Deletion metric used by prior work to evaluate attribution methods is biased towards Input Marginalization (IM), causing a misleading conclusion that IM generates better explanations than Leave-One-Out (LOO) baselines. The authors show that the original Deletion metric favors LOO while its variant Deletion-BERT favors IM. This bias likely exists in other common Deletion variants as well. 2. Through extensive experiments on 3 datasets and 6 evaluation metrics, the authors find no evidence that IM generates more accurate explanations than a simple LOO baseline. Specifically, LOO outperforms or is on par with IM on metrics like ROAR, human annotation alignment, and sanity checks. 3. The paper provides insights into why IM is not more effective than LOO in practice. First, deleting only a single word has a marginal effect on classifier predictions, questioning the need for realistic counterfactuals. Second, IM assigns near-zero attribution to predictable words regardless of their true importance, revealing a key weakness of relying on a pretrained language model like BERT.4. Replacing multiple words using BERT consistently improves LIME's faithfulness, suggesting potential benefits of this technique when perturbing larger phrases instead of individual words. But integrating BERT into LOO or LIME does not improve alignment with human annotations.5. Overall, the paper underscores troubling biases in common XAI evaluation metrics and the lack of grounded evaluation in prior work. It challenges the assumed effectiveness of IM for text explanation.In summary, the key contribution is a rigorous re-evaluation of IM using properly designed experiments which reveal that IM does not improve faithfulness over simple LOO baselines, despite prior claims. The authors provide empirical and theoretical reasons for why IM has limitations in practice.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper finds that using BERT to generate more plausible counterfactual examples does not improve the faithfulness of attribution methods like Input Marginalization (IM) and LIME for explaining neural network predictions, contrary to what prior work hypothesized. The authors show that deletion-based evaluation metrics are biased, and under more rigorous evaluation, simple deletion baseline methods like leave-one-out perform as well as or better than IM and LIME-BERT. Overall, the paper challenges the assumption made in prior work that more realistic counterfactuals generated by language models like BERT will produce better explanations of model behavior.


## How does this paper compare to other research in the same field?

Here is a high-level comparison of this paper to other related work in explainable AI:- This paper focuses specifically on evaluating Input Marginalization (IM) for generating counterfactual explanations in NLP. IM uses BERT to generate more realistic counterfactuals by replacing words rather than just deleting them. - The authors find that IM does not actually improve faithfulness of explanations compared to simpler Leave-One-Out (LOO) baselines that just delete words. This challenges prior assumptions and claims that IM produces better explanations.- Prior work like Kim et al. 2020 and Hase et al. 2021 hypothesize that more realistic counterfactuals from IM should improve explanation faithfulness, but don't rigorously evaluate this. This paper provides a more thorough assessment.- Other related work like Goyal et al. 2019 and Agarwal et al. 2020 try to generate more realistic counterfactuals in vision tasks, but effectiveness for improving explanations is still unclear. This paper suggests realistic counterfactuals may not help in NLP.- The paper also reveals issues with common evaluation metrics like deletion/insertion that can be biased. This is an important methodological contribution for the field.- For LIME, which modifies multiple words, making counterfactuals more realistic with BERT helps. This aligns with intuitions, but shows that word removal effects are different.In summary, this paper advances our understanding of how counterfactual generation affects explanation faithfulness in NLP, while challenging prior assumptions. The rigor of the evaluations and analysis of biases/limitations are excellent contributions.


## What future research directions do the authors suggest?

The authors suggest several promising future research directions:1. Exploring the effectiveness of Input Marginalization for explaining other types of models besides BERT, such as convolutional networks for computer vision tasks. The counterfactual generation approach may be beneficial in other modalities.2. Extending Input Marginalization to explain classifiers at the phrase level instead of only individual words. As deleting multiple words together has a larger effect on classifier predictions, phrase-level explanations may be more useful.3. Developing new evaluation metrics beyond deletion-based metrics like the biased Deletion and its variants. The authors show these have inherent biases. New metrics grounded in downstream tasks could better assess explanation faithfulness. 4. Designing inherently interpretable classifiers where the model jointly outputs explanations along with predictions. This could mitigate issues arising from explaining one black-box model using another black-box model like BERT.5. Applying insights about faithfulness versus plausibility to other explanation methods like LIME. Replacing tokens to generate more plausible counterfactuals improved LIME's faithfulness, suggesting further research into this effect.In summary, the authors highlight opportunities to apply Input Marginalization in new domains and tasks, develop better evaluation metrics for faithfulness, build more transparent models, and leverage language models like BERT to improve existing methods' explanations when replacing multiple words or phrases jointly.


## Summarize the paper in one paragraph.

Here is a one paragraph summary of the key points from the paper:This paper evaluates the effectiveness of using masked language models like BERT to generate more plausible counterfactual examples for interpreting neural network predictions. The authors reproduce prior work showing that input marginalization (IM) generates better explanations than simple leave-one-out (LOO) baselines when evaluated with the Deletion metric. However, they find that Deletion is inherently biased towards IM. Using more unbiased metrics like ROAR, alignment with human rationales, and sensitivity to model randomization, the authors show that IM does not actually improve faithfulness over LOO. They argue this is because deleting only one word has a marginal effect on classifier predictions, so generating more realistic counterfactuals gives little benefit. Interestingly, they do find that using BERT to replace multiple words in LIME explanations improves faithfulness, since replacing many words better simulates model counterfactuals. Overall, the paper provides an important recalibration of prior claims on the effectiveness of IM for generating better word-level explanations. The authors advocate for rigorous evaluation of explanation methods under multiple metrics to avoid drawing misleading conclusions.


## Summarize the paper in two paragraphs.

Here is a two paragraph summary of the key points from the paper:The paper investigates the effectiveness of Input Marginalization (IM) for generating attribution maps to explain the decisions of text classifiers. IM replaces an input word with suggestions from BERT to create more realistic counterfactual inputs compared to simply deleting words as in Leave-One-Out (LOO) methods. The authors reproduced prior results showing IM performed better than LOO baselines on one dataset using the Deletion metric. However, they found the Deletion metric is inherently biased towards IM. When evaluating on three diverse datasets using multiple metrics excluding Deletion, they found no evidence that IM generates better explanations than LOO. In fact, LOO alignments were more consistent with human annotations. They argue IM has two key weaknesses: removing one word has a marginal effect on classification, and predictable words get near-zero attribution regardless of true importance. As an alternative approach, they integrated BERT into LIME, which removes multiple words, to generate more plausible counterfactuals. This LIME-BERT method did improve faithfulness under ROAR metrics, suggesting value in realistic counterfactuals when multiple words are removed. Overall, they urge caution in assuming IM's effectiveness for word-level explanation and provide insights into when generating realistic counterfactuals improves faithfulness.In summary, the key findings are: 1) The commonly used Deletion metric is biased towards IM, causing false conclusions of IM's superiority over LOO. 2) Over multiple datasets and metrics excluding Deletion, IM did not outperform LOO, with LOO better aligning to human annotations. 3) IM has weaknesses related to predictable words and marginal single word effects. 4) For methods removing multiple words like LIME, integrating BERT to generate realistic counterfactuals did improve faithfulness. The results urge caution in assuming IM's effectiveness at word-level explanation and shed light on the impacts of realistic counterfactuals on faithfulness.


## Summarize the main method used in the paper in one paragraph.

Here is a one paragraph summary of the main method used in the paper:The paper proposes a new attribution method called Input Marginalization (IM) for interpreting neural network predictions in natural language processing tasks. IM aims to generate more realistic counterfactual examples to estimate the importance of an input token to a model's prediction. Specifically, to compute the attribution for a token x_i, IM uses BERT to suggest replacements for x_i by masking it out and sampling over the top predicted tokens. This creates counterfactual inputs where x_i is replaced by plausible alternatives sampled from BERT's distribution. The attribution is then computed by taking the log-odds difference between the original prediction and the expected prediction over the counterfactual inputs marginalized over BERT's suggestions. So instead of deleting or replacing x_i with an out-of-distribution token like [UNK] which can cause artifacts, IM uses BERT to marginalize over multiple realistic alternatives in a principled way. The resulting attributions are meant to more faithfully indicate each token's contribution to the prediction by accounting for the inherent uncertainty in replacing it. Experiments on sentiment analysis and natural language inference tasks suggest IM attributions are more aligned with human judgments than simple deletion-based approaches.


## What problem or question is the paper addressing?

The paper is addressing the effectiveness of using masked language models like BERT to generate more plausible counterfactual examples for interpreting natural language processing models. Specifically, it is re-evaluating a prior claim that an attribution method called Input Marginalization (IM) generates better explanations than simpler Leave-One-Out (LOO) methods because it uses BERT to replace words rather than just deleting them. The key questions the paper is investigating are:1) Is the commonly used Deletion evaluation metric biased towards IM, causing a false conclusion about its superiority over LOO?2) When evaluated on unbiased metrics, is there evidence that IM generates better explanations than LOO baselines? 3) Why would using more realistic counterfactuals via BERT improve explanation faithfulness, given that classifiers seem fairly robust to small input perturbations?4) Does the idea of using BERT to generate more plausible counterfactuals help if applied to methods that remove multiple words like LIME, rather than just single words?So in summary, the paper is re-evaluating the claim that using BERT to create more realistic counterfactuals improves attribution faithfulness, by testing this on multiple datasets and metrics, including more rigorous ones that avoid bias. It investigates reasons why this technique may or may not be effective.
