# [Double Trouble: How to not explain a text classifier's decisions using   counterfactuals synthesized by masked language models?](https://arxiv.org/abs/2110.11929)

## What is the central research question or hypothesis that this paper addresses?

 The central research question addressed in this paper is: Do plausible counterfactuals generated by masked language models like BERT actually improve the faithfulness and accuracy of attribution methods for explaining text classifier decisions?

The key hypotheses tested are:

1) Input marginalization (IM) using BERT to generate plausible counterfactual texts will produce more faithful and accurate explanations compared to simple leave-one-out (LOO) baselines that delete words. This is based on the hypothesis that the LOO counterfactuals are too unrealistic. 

2) Integrating BERT into LIME to generate more plausible counterfactual samples with multiple words replaced will improve LIME's explanations compared to the original LIME using unrealistic samples.

The authors rigorously test these hypotheses across multiple datasets and evaluation metrics. Their main findings are:

1) The Deletion metric used in prior work to claim IM improves faithfulness is inherently biased. When using unbiased metrics, there is no evidence IM outperforms LOO.

2) Replacing a single word with BERT in IM does not improve faithfulness, likely because deleting one word has little effect on the classifier. However, replacing multiple words in LIME does improve faithfulness.

So in summary, the paper shows that generating plausible counterfactuals only sometimes improves explanation faithfulness, depending on the attribution method and how drastically the inputs are perturbed. The common assumption that more realistic inputs always improve faithfulness is questioned.


## What is the main contribution of this paper?

 The main contributions of this paper are:

1. It demonstrates that the Deletion metric used by prior work to evaluate attribution methods is biased towards Input Marginalization (IM), causing a misleading conclusion that IM generates better explanations than Leave-One-Out (LOO) baselines. The authors show that the original Deletion metric favors LOO while its variant Deletion-BERT favors IM. This bias likely exists in other common Deletion variants as well. 

2. Through extensive experiments on 3 datasets and 6 evaluation metrics, the authors find no evidence that IM generates more accurate explanations than a simple LOO baseline. Specifically, LOO outperforms or is on par with IM on metrics like ROAR, human annotation alignment, and sanity checks. 

3. The paper provides insights into why IM is not more effective than LOO in practice. First, deleting only a single word has a marginal effect on classifier predictions, questioning the need for realistic counterfactuals. Second, IM assigns near-zero attribution to predictable words regardless of their true importance, revealing a key weakness of relying on a pretrained language model like BERT.

4. Replacing multiple words using BERT consistently improves LIME's faithfulness, suggesting potential benefits of this technique when perturbing larger phrases instead of individual words. But integrating BERT into LOO or LIME does not improve alignment with human annotations.

5. Overall, the paper underscores troubling biases in common XAI evaluation metrics and the lack of grounded evaluation in prior work. It challenges the assumed effectiveness of IM for text explanation.

In summary, the key contribution is a rigorous re-evaluation of IM using properly designed experiments which reveal that IM does not improve faithfulness over simple LOO baselines, despite prior claims. The authors provide empirical and theoretical reasons for why IM has limitations in practice.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the key points from the paper:

The paper finds that using BERT to generate more plausible counterfactual examples does not improve the faithfulness of attribution methods like Input Marginalization (IM) and LIME for explaining neural network predictions, contrary to what prior work hypothesized. The authors show that deletion-based evaluation metrics are biased, and under more rigorous evaluation, simple deletion baseline methods like leave-one-out perform as well as or better than IM and LIME-BERT. Overall, the paper challenges the assumption made in prior work that more realistic counterfactuals generated by language models like BERT will produce better explanations of model behavior.


## How does this paper compare to other research in the same field?

 Here is a high-level comparison of this paper to other related work in explainable AI:

- This paper focuses specifically on evaluating Input Marginalization (IM) for generating counterfactual explanations in NLP. IM uses BERT to generate more realistic counterfactuals by replacing words rather than just deleting them. 

- The authors find that IM does not actually improve faithfulness of explanations compared to simpler Leave-One-Out (LOO) baselines that just delete words. This challenges prior assumptions and claims that IM produces better explanations.

- Prior work like Kim et al. 2020 and Hase et al. 2021 hypothesize that more realistic counterfactuals from IM should improve explanation faithfulness, but don't rigorously evaluate this. This paper provides a more thorough assessment.

- Other related work like Goyal et al. 2019 and Agarwal et al. 2020 try to generate more realistic counterfactuals in vision tasks, but effectiveness for improving explanations is still unclear. This paper suggests realistic counterfactuals may not help in NLP.

- The paper also reveals issues with common evaluation metrics like deletion/insertion that can be biased. This is an important methodological contribution for the field.

- For LIME, which modifies multiple words, making counterfactuals more realistic with BERT helps. This aligns with intuitions, but shows that word removal effects are different.

In summary, this paper advances our understanding of how counterfactual generation affects explanation faithfulness in NLP, while challenging prior assumptions. The rigor of the evaluations and analysis of biases/limitations are excellent contributions.


## What future research directions do the authors suggest?

 The authors suggest several promising future research directions:

1. Exploring the effectiveness of Input Marginalization for explaining other types of models besides BERT, such as convolutional networks for computer vision tasks. The counterfactual generation approach may be beneficial in other modalities.

2. Extending Input Marginalization to explain classifiers at the phrase level instead of only individual words. As deleting multiple words together has a larger effect on classifier predictions, phrase-level explanations may be more useful.

3. Developing new evaluation metrics beyond deletion-based metrics like the biased Deletion and its variants. The authors show these have inherent biases. New metrics grounded in downstream tasks could better assess explanation faithfulness. 

4. Designing inherently interpretable classifiers where the model jointly outputs explanations along with predictions. This could mitigate issues arising from explaining one black-box model using another black-box model like BERT.

5. Applying insights about faithfulness versus plausibility to other explanation methods like LIME. Replacing tokens to generate more plausible counterfactuals improved LIME's faithfulness, suggesting further research into this effect.

In summary, the authors highlight opportunities to apply Input Marginalization in new domains and tasks, develop better evaluation metrics for faithfulness, build more transparent models, and leverage language models like BERT to improve existing methods' explanations when replacing multiple words or phrases jointly.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the key points from the paper:

This paper evaluates the effectiveness of using masked language models like BERT to generate more plausible counterfactual examples for interpreting neural network predictions. The authors reproduce prior work showing that input marginalization (IM) generates better explanations than simple leave-one-out (LOO) baselines when evaluated with the Deletion metric. However, they find that Deletion is inherently biased towards IM. Using more unbiased metrics like ROAR, alignment with human rationales, and sensitivity to model randomization, the authors show that IM does not actually improve faithfulness over LOO. They argue this is because deleting only one word has a marginal effect on classifier predictions, so generating more realistic counterfactuals gives little benefit. Interestingly, they do find that using BERT to replace multiple words in LIME explanations improves faithfulness, since replacing many words better simulates model counterfactuals. Overall, the paper provides an important recalibration of prior claims on the effectiveness of IM for generating better word-level explanations. The authors advocate for rigorous evaluation of explanation methods under multiple metrics to avoid drawing misleading conclusions.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the key points from the paper:

The paper investigates the effectiveness of Input Marginalization (IM) for generating attribution maps to explain the decisions of text classifiers. IM replaces an input word with suggestions from BERT to create more realistic counterfactual inputs compared to simply deleting words as in Leave-One-Out (LOO) methods. The authors reproduced prior results showing IM performed better than LOO baselines on one dataset using the Deletion metric. However, they found the Deletion metric is inherently biased towards IM. When evaluating on three diverse datasets using multiple metrics excluding Deletion, they found no evidence that IM generates better explanations than LOO. In fact, LOO alignments were more consistent with human annotations. They argue IM has two key weaknesses: removing one word has a marginal effect on classification, and predictable words get near-zero attribution regardless of true importance. As an alternative approach, they integrated BERT into LIME, which removes multiple words, to generate more plausible counterfactuals. This LIME-BERT method did improve faithfulness under ROAR metrics, suggesting value in realistic counterfactuals when multiple words are removed. Overall, they urge caution in assuming IM's effectiveness for word-level explanation and provide insights into when generating realistic counterfactuals improves faithfulness.

In summary, the key findings are: 1) The commonly used Deletion metric is biased towards IM, causing false conclusions of IM's superiority over LOO. 2) Over multiple datasets and metrics excluding Deletion, IM did not outperform LOO, with LOO better aligning to human annotations. 3) IM has weaknesses related to predictable words and marginal single word effects. 4) For methods removing multiple words like LIME, integrating BERT to generate realistic counterfactuals did improve faithfulness. The results urge caution in assuming IM's effectiveness at word-level explanation and shed light on the impacts of realistic counterfactuals on faithfulness.


## Summarize the main method used in the paper in one paragraph.

 Here is a one paragraph summary of the main method used in the paper:

The paper proposes a new attribution method called Input Marginalization (IM) for interpreting neural network predictions in natural language processing tasks. IM aims to generate more realistic counterfactual examples to estimate the importance of an input token to a model's prediction. Specifically, to compute the attribution for a token x_i, IM uses BERT to suggest replacements for x_i by masking it out and sampling over the top predicted tokens. This creates counterfactual inputs where x_i is replaced by plausible alternatives sampled from BERT's distribution. The attribution is then computed by taking the log-odds difference between the original prediction and the expected prediction over the counterfactual inputs marginalized over BERT's suggestions. So instead of deleting or replacing x_i with an out-of-distribution token like [UNK] which can cause artifacts, IM uses BERT to marginalize over multiple realistic alternatives in a principled way. The resulting attributions are meant to more faithfully indicate each token's contribution to the prediction by accounting for the inherent uncertainty in replacing it. Experiments on sentiment analysis and natural language inference tasks suggest IM attributions are more aligned with human judgments than simple deletion-based approaches.


## What problem or question is the paper addressing?

 The paper is addressing the effectiveness of using masked language models like BERT to generate more plausible counterfactual examples for interpreting natural language processing models. Specifically, it is re-evaluating a prior claim that an attribution method called Input Marginalization (IM) generates better explanations than simpler Leave-One-Out (LOO) methods because it uses BERT to replace words rather than just deleting them. 

The key questions the paper is investigating are:

1) Is the commonly used Deletion evaluation metric biased towards IM, causing a false conclusion about its superiority over LOO?

2) When evaluated on unbiased metrics, is there evidence that IM generates better explanations than LOO baselines? 

3) Why would using more realistic counterfactuals via BERT improve explanation faithfulness, given that classifiers seem fairly robust to small input perturbations?

4) Does the idea of using BERT to generate more plausible counterfactuals help if applied to methods that remove multiple words like LIME, rather than just single words?

So in summary, the paper is re-evaluating the claim that using BERT to create more realistic counterfactuals improves attribution faithfulness, by testing this on multiple datasets and metrics, including more rigorous ones that avoid bias. It investigates reasons why this technique may or may not be effective.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, here are some key terms and keywords that seem most relevant:

- Input Marginalization (IM): A method for generating counterfactual examples by using BERT to replace words in a sentence. Proposed as a way to create more realistic counterfactuals compared to simply deleting words. 

- Leave-One-Out (LOO): A baseline method that deletes words from a sentence to estimate their importance. LOO variants tested include leaving a token out, replacing with [UNK], or replacing with zeroes.

- Deletion metric: A common metric that removes words and measures the drop in classifier confidence. Show to be biased towards IM or LOO depending on word removal scheme. 

- ROAR: RemOve And Retrain. A metric that retrains a classifier after removing words to avoid deletion metric biases. 

- Alignment with human annotations: Evaluating explanation methods by comparing to human highlights of important words.

- Sanity check: Randomizing classifier weights to test explanation sensitivity. More sensitive explanations vary more with randomized classifiers.

- LIME: A method that generates counterfactuals by randomly deleting multiple words. Integrating BERT improves LIME counterfactual realism.

- Faithfulness: How well an explanation method identifies words truly important to the classifier vs. artifacts of the explanation approach.

- Plausibility: How realistic an explanation appears to humans. May not indicate faithfulness.

- Counterfactuals: Modified examples used to estimate feature importance by observing classifier output changes.

- Individual treatment effect: Estimating a feature's causal effect by comparing factual and counterfactual outcomes.

The key findings seem to be that IM does not improve faithfulness over LOO baselines for word removal despite improving counterfactual plausibility. The deletion metric is also shown to be problematic for evaluating faithfulness.


## Based on the paper, what questions should we ask to create a comprehensive summary of it? Please list at least 10 questions.

 Here are 10 potential questions to ask to create a comprehensive summary of this paper:

1. What is the primary research question or objective of this paper? What problem is it trying to solve?

2. What methods does the paper employ to address the research question? What models or approaches are used? 

3. What datasets are used in the experiments? What are the key statistics and details about the datasets?

4. What are the main results and findings reported in the paper? What insights or conclusions are drawn from the experiments?

5. Does the paper propose a new model or framework? If so, what are the key aspects and innovations of the proposed approach?

6. How does the paper's approach compare to prior work and state-of-the-art methods? What improvements does it achieve?

7. What limitations or shortcomings of the current work are identified by the authors? What future work do they suggest?

8. What broader impact or potential applications of the research are discussed? How could the work influence related areas?

9. What analysis or experiments could be done to further validate the results or address limitations? 

10. Does the paper make convincing arguments to support its claims? Are the conclusions well supported by the results and analyses?

This set of questions aims to understand the core problems, methods, results, and implications of the research in a comprehensive manner. Asking such questions can help generate a thorough summary that captures the key technical details as well as the broader significance of the paper. Additional domain-specific questions could also be added as needed.


## Design 10 in-depth questions about method proposed in this paper and avoid simple question.

 Here are 10 in-depth questions about the method proposed in this paper:

1. The paper claims that input marginalization (IM) generates more faithful explanations than leave-one-out (LOO) methods. However, the evaluation is done using only the Deletion and Deletion-BERT metrics, which seem to be biased towards IM. Why were other evaluation metrics not used to provide a more comprehensive assessment?

2. The Deletion and Deletion-BERT metrics measure the drop in model confidence when words are deleted. However, prior work has shown that models like BERT are quite robust to word deletions. So is the drop in confidence a good indication of the importance of the deleted words? 

3. The paper argues that IM better estimates the individual treatment effect compared to LOO. But how valid is this claim when only single words are being perturbed rather than phrases? Does perturbing single words give a realistic estimate of treatment effects?

4. One of the key ideas behind IM is to marginalize over multiple counterfactual suggestions from BERT to make the perturbations more realistic. However, in practice only the top 10 suggestions are used due to computational constraints. How representative are the top 10 suggestions of the full distribution?

5. The IM explanations seem to depend heavily on the quality of BERT's predictions. However, BERT is not perfect at cloze tasks. So how much could errors in BERT's predictions affect the faithfulness of the IM explanations?

6. The paper shows that IM explanations have lower coverage of human annotations compared to LOO. This suggests that IM overly relies on BERT's predictions and fails to identify words important for the classifier. How can this issue be addressed?

7. The BERT model used for IM was pre-trained only on Wikipedia/BookCorpus, whereas the classifiers are fine-tuned on task-specific datasets. How well can this "off-the-shelf" BERT identify words important for fine-tuned classifiers?

8. The authors find that using BERT with LIME improves faithfulness, but not for IM. Why does BERT help with multiple perturbations in LIME but not single word perturbations in IM?

9. IM assumes the classifier was trained on natural data distribution. But the classifiers are fine-tuned on training data. So shouldn't IM use BERT fine-tuned on training data rather than off-the-shelf BERT?

10. The paper does analysis only on BERT classifiers. Do the conclusions generalize to other types of NLP classifiers like CNNs and LSTMs? And how do the findings translate to computer vision?
