# [GOAt: Explaining Graph Neural Networks via Graph Output Attribution](https://arxiv.org/abs/2401.14578)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
Explaining and interpreting the predictions of graph neural networks (GNNs) is important for establishing trust and transparency. However, most existing methods rely on training auxiliary models or backpropagation, leading to blackbox explanations that can be inaccurate, unstable across samples, or fail to distinguish between classes. 

Proposed Solution: 
The paper introduces Graph Output Attribution (GOAt), an efficient analytical method to faithfully attribute GNN outputs to input features without training. It expands a GNN as sums of scalar products involving input features, model parameters, and activation patterns. Based on the notion that all variables contribute equally in a scalar product, it attributes each product to its factors. Summing contributions across products gives the importance of each input feature. 

To evaluate explanation quality, the paper introduces two new metrics - discriminability, measured by class separation of explanation embeddings, and stability, measured by top motif coverage across samples.

Main Contributions:
- Analytical GNN explanation method that attributes outputs to inputs without training
- Expands GNN computations as sums of scalar products and derives feature attributions
- Outperforms state-of-the-art on fidelity metric by a large margin
- Introduces two new metrics - discriminability and stability for evaluation
- Shows superior discriminative and stable explanation performance 

Overall, the paper makes significant contributions in transparently explaining GNN predictions through an efficient analytical approach. The new evaluation metrics provide additional useful assessment. Both quantitative and qualitative results demonstrate state-of-the-art performance of the proposed GOAt method across faithfulness, discriminability and stability.


## Summarize the paper in one sentence.

 Here is a one-sentence summary of the paper:

This paper introduces Graph Output Attribution (GOAt), an efficient analytical method to faithfully and stably explain graph neural networks by attributing outputs to input features based on decomposing the network forward pass into scalar products and evenly distributing contributions within each product.


## What is the main contribution of this paper?

 This paper proposes GOAt (Graph Output Attribution), a new method to explain graph neural networks (GNNs) by attributing GNN outputs to input graph features. The main contributions are:

1) GOAt expands a pretrained GNN into a sum of scalar products involving input features, model parameters, and activation patterns. Based on the notion of equal contribution, it efficiently computes the importance of each input graph feature to the scalar products and aggregates them to derive node/edge feature attributions. 

2) Compared to existing GNN explanation methods, GOAt does not rely on training secondary models or hyperparameters. It provides a more transparent analytical approach to faithfully explain GNN predictions.

3) Through experiments on synthetic and real-world graph datasets, GOAt is shown to outperform state-of-the-art methods significantly in terms of fidelity. It also demonstrates stronger discriminability to distinguish classes and stability in extracting motifs across data samples.

4) New evaluation metrics are introduced - discriminability and stability, in addition to the commonly used fidelity metric. This allows more comprehensive assessment of explanation methods.

In summary, the key contribution is an efficient analytical attribution method, GOAt, to generate high-quality explanations for GNNs that are more faithful, discriminative and stable compared to existing approaches.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and concepts related to this work include:

- Graph neural networks (GNNs): The paper focuses on explaining and interpreting graph neural networks, which are neural networks designed to operate on graph-structured data.

- Local-level explanations: The approach introduced aims to provide local explanations that identify the most relevant parts of the input graph for a particular prediction. This is in contrast to global explanations that summarize behavior over the whole dataset.  

- Graph output attribution: The proposed method is named "Graph Output Attribution" (GOAt). It analyzes how different input features like nodes and edges contribute to the GNN's outputs.

- Faithfulness/Fidelity: An important criteria for explanations is how well they reflect what the model is actually computing or "looking at" to make decisions. The fidelity metric measures this.

- Discriminability: A new metric introduced that quantifies how well explanations separate between different output classes.

- Stability: Another new metric that measures how consistent explanations are across similar input examples.

- Expansion form/scalar products: Key idea is to expand the GNN computation graph into scalar products between input features, parameters, and activation patterns. This allows attributing outputs to inputs.

- Case studies: Concrete application examples of explaining GCN, GraphSAGE, and GIN models are provided.

So in summary, the key themes are around local explanations for graph neural networks, using a method of expanding computations to perform graph output attribution with desirable properties like fidelity, discriminability and stability.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 potential in-depth questions about the method proposed in this paper:

1. The paper introduces a new metric called "discriminability" to evaluate the ability of explanations to distinguish between classes. Can you elaborate on how this metric is defined and calculated? What are the advantages of using this metric compared to other common evaluation metrics?

2. The paper proposes an analytical approach to attributing GNN outputs to input features based on expanding the GNN computations into scalar product terms. Can you walk through the mathematical derivations and key assumptions behind computing the contribution of each input variable to these scalar product terms? 

3. The equal contribution principle is fundamental to the proposed attribution method. Under what conditions can we assume equal contribution of variables to a scalar product term? When might this assumption break down? 

4. Activation patterns are used in the attribution method to capture the effect of non-linearities. How are these patterns defined and incorporated into the explanation framework? What are some limitations of the approach used to attribute activation patterns to input features?

5. How does the proposed method handle different GNN architectures like GCN, GraphSAGE and GIN? Can you walk through the attribution process for one of these architectures? What modifications need to be made to the core framework?

6. The completeness axiom is an important requirement for attribution methods. Can you explain why the proposed method satisfies completeness? What does this imply about the explanatory power compared to other methods?

7. Stability is introduced as a new metric to measure explanation consistency across similar graph samples. Why is this an important characteristic for explainability methods? How does the paper experimentally validate the stability of the proposed method?

8. What modifications are made to handle node classification tasks compared to graph classification? How are explanations extracted and evaluated in the node classification setting?

9. The paper argues that directly visualizing explanation embeddings provides clearer insights compared to accuracy metrics used in some prior work. Do you agree? What are some pros and cons of visual evaluation versus accuracy metrics?

10. How does the proposed analytical explanation method for GNNs compare with existing perturbation-based and auxiliary model-based approaches? What are some key advantages and disadvantages in terms of fidelity, discriminability, stability and computational efficiency?
