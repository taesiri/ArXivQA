# [Promptagator: Few-shot Dense Retrieval From 8 Examples](https://arxiv.org/abs/2209.11755)

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper addresses is how to create effective retrieval systems for diverse tasks with only a few examples, instead of requiring a large amount of supervised training data. The key ideas and hypotheses are:- Different retrieval tasks have distinct search intents and query distributions. It is difficult for models trained on one dataset like MS MARCO to generalize well to other tasks.- With just a few annotated examples and task descriptions, humans can understand a new retrieval task. The paper proposes a "few-shot retrieval" setting where models are provided with a few examples per task.- Language models can be prompted to generate high-quality queries from just a few examples, amplifying the information contained in the few shots. This allows creating task-specific training data.- Consistency filtering using only generated data is sufficient to improve the quality of the synthesized training data in the few-shot setting.- The synthetic data can be used to train simple but efficient end-to-end neural retriever models that outperform prior work relying on external datasets like MS MARCO.In summary, the central hypothesis is that task-specific neural retrievers can be synthesized using only a few examples and language model prompting, without requiring external training data. The paper aims to demonstrate this can achieve better accuracy compared to prior transfer learning approaches that use other datasets.


## What is the main contribution of this paper?

The main contribution of this paper is proposing Promptagator, a method for few-shot dense retrieval. The key ideas are:- Proposing a new few-shot retrieval setting, where each task comes with a short description and a few annotated examples. This sets up retrieval as a more realistic few-shot learning problem.- Using large language models (LLMs) like FLAN for prompt-based query generation from just a few examples per task. This amplifies the few examples into a large training set of synthetic queries and passages.- Developing a round-trip consistency filtering technique using only the generated queries and passages, without needing external QA data. This is shown to significantly improve retrieval performance.- Showing for the first time that prompted LLMs can create high-quality and efficient end-to-end neural retrievers from just a few examples per task. Without using any MS MARCO or Natural Questions training data, Promptagator outperforms prior work by over 1.2 nDCG on BEIR.So in summary, the main contribution is advancing few-shot learning for information retrieval by generating synthetic training data from prompts and showing this can work much better than prior transfer learning approaches. The power of LLMs is leveraged to create customized retrievers from limited supervision.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

The paper proposes Promptagator, a method for few-shot dense retrieval that uses a large language model to generate synthetic training data from just a few examples. The key ideas are to use prompting and the generalization ability of large LMs like FLAN to amplify a few examples into much more training data, and to filter the synthetic data for consistency. Experiments show Promptagator significantly outperforms prior work despite using only 8 examples, demonstrating the potential of few-shot retrieval with large LMs.


## How does this paper compare to other research in the same field?

This paper presents an approach for few-shot dense retrieval by leveraging large language models (LLMs) to generate synthetic training data from just a few examples. Here are some key comparisons to other related work:- Most prior work on dense retrieval relies on large amounts of annotated training data from datasets like MS MARCO or Natural Questions. This paper shows that with a good prompt and LLM like FLAN, high quality retrievers can be produced using just 8 or fewer examples. - Other work has explored using LLMs like T5 or GPT-3 for query generation, but mainly for re-ranking after an initial retrieval step. This paper shows LLMs can be effectively used to create end-to-end neural retrievers.- Techniques like hard negative mining and distillation are commonly used to improve neural retrievers. This paper presents a simpler training recipe without those components, showing the synthetic data alone provides a strong training signal.- Retrieval architectures like ColBERT and SPLADE incorporate cross-attention layers to enable token-level interaction. The dual encoders trained in this work use standard independent encoders without that inductive bias, yet reach higher accuracy thanks to the synthetic data.- Compared to other few-shot NLP techniques that fine-tune LLMs, this work uses LLMs in a zero-shot prompt-based manner without any fine-tuning. This makes the approach quite efficient.- The round-trip consistency filtering technique is tailored to the retrieval scenario, unlike prior work that uses a separate pre-trained model. Filtering only with synthetically generated data is shown to be effective.In summary, this work pushes the boundary on few-shot learning for information retrieval by showing that large pre-trained LLMs can effectively turn a few examples into high quality, task-specific training data for an end-to-end neural retriever. The simplicity of the overall approach is quite notable given the significant accuracy gains demonstrated.


## What future research directions do the authors suggest?

Based on my reading of the paper, here are some of the main future research directions suggested by the authors:- Investigate exactly how much generated query-document data is needed for each task, or how to use the generated examples more efficiently. The paper mentions they have not yet explored these questions thoroughly.- Further analyze the sensitivity of the final retriever's performance to the prompt design. The authors suggest prompting is important but more analysis is needed on how variations in prompting impact results. - Connect Promptagator to knowledge distillation research. The authors suggest analyzing the headroom and better understanding how knowledge can be transferred from large language models to retrievers.- Push the limits of few-shot retrieval further, towards systems that can seamlessly adapt to new tasks with minimal examples. The authors advocate this as an important research direction.- Study if consistency filtering can be improved by using query-specific filtering thresholds instead of a single global threshold. This may help retain more high-quality examples.- Analyze the generated data and model outputs more thoroughly through qualitative analysis. The authors did some of this but suggest more analysis would be useful.- Consider modifying the training procedure so fewer examples are needed. The authors used up to 8 examples but suggest further research could aim to reduce this.So in summary, the main future directions are improving prompt design, generated data efficiency, knowledge distillation connections, qualitative analysis, reducing the number of examples needed, and continuing to push the limits of few-shot learning for retrieval. The authors propose Promptagator as a step in this direction but suggest significant room for future work remains.
