# [Explain to Question not to Justify](https://arxiv.org/abs/2402.13914)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- The field of explainable AI (XAI) is experiencing an identity crisis due to confusing and conflicting goals. 
- There are misconceptions about XAI leading to unrealistic expectations, like the illusion of a single "best" explanation method or that all users want explanations.
- Review papers are dominated by a focus on human values like trust and fairness (BLUE XAI), neglecting model validation uses (RED XAI).

Proposed Solution:  
- The authors disentangle two cultures in XAI with different goals: 
  - BLUE XAI: Focused on human values, ethics, trust - explanations aimed at end users.
  - RED XAI: Focused on model validation, exploration, research - explanations aimed at model developers.
- More attention needs to be paid to the overlooked RED XAI culture and its research challenges.

Main Contributions:
- Identifies two complementary cultures in XAI research with different motivations.  
- Calls out several "fallacies" or misconceptions about XAI that are causing harm.
- Argues RED XAI focused on empowering model developers is currently under-explored.
- Presents open challenges for RED XAI, like complementary explanations, benchmarks, taking an explorer mindset, contrasting models.
- Encourages rethinking how we see XAI to enable progress on these overlooked but critical research directions.

In summary, the paper offers a reframing of XAI research by distinguishing two cultures, diagnosing issues caused by misconceptions, and laying out an agenda for the overlooked RED XAI focused on model developers rather than end users. This is aimed to drive progress on critical technical research to ensure AI safety.
