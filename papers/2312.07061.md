# [MaxQ: Multi-Axis Query for N:M Sparsity Network](https://arxiv.org/abs/2312.07061)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a summary paragraph of the key points from the paper:

This paper proposes a Multi-Axis Query methodology called MaxQ to boost the performance of N:M sparse neural networks. MaxQ identifies important connections across N:M blocks and enhances them using soft pruning masks generated dynamically without extra parameters. It employs a gradual sparsity schedule, incrementally increasing the ratio of N:M sparse blocks based on their L1 norms to progressively allow the network to adapt. Experiments show MaxQ achieves consistent and significant gains over state-of-the-art methods on various CNN architectures and vision tasks. For example, MaxQ attains 74.6% ImageNet top-1 accuracy with a 1:16 sparse ResNet50, improving by 2.8%. For object detection and instance segmentation on COCO, MaxQ delivers comparable or better performance to dense baseline models under 2:4 sparsity. Further analysis also reveals MaxQ's amenability to quantization despite being a self-structured reparameterization method. In summary, MaxQ is an efficient, effective approach to train high-performance N:M sparse models.
