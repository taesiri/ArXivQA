# [Knowledge-Aware Prompt Tuning for Generalizable Vision-Language Models](https://arxiv.org/abs/2308.11186)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is: How can external knowledge bases be leveraged to improve the generalization ability of vision-language models to novel visual concepts/categories? The key hypothesis is that incorporating external category-related knowledge into the prompt tuning framework can enhance the model's ability to recognize new unseen classes. Specifically, the authors hypothesize that:1) Constructing knowledge-aware prompts using descriptive information from knowledge bases can provide more discriminative textual representations for different categories. This allows better alignment with visual features.2) An adaptation module can help focus the model's attention on task-relevant visual concepts and features by conditioning on the knowledge-enriched text representations. This improves recognition on new classes by suppressing irrelevant visual cues. 3) The combination of knowledge-aware prompts and visual adaptation will lead to improved generalization on new unseen categories, compared to existing prompt tuning methods without explicit external knowledge.So in summary, the central hypothesis is that leveraging external knowledge through prompt engineering and visual adaptation will enhance generalization in vision-language models for novel categories. The experiments aim to validate if knowledge-aware prompt tuning can outperform current state-of-the-art approaches on this specific capability.


## What is the main contribution of this paper?

The main contribution of this paper is proposing a novel Knowledge-Aware Prompt Tuning (KAPT) framework to improve the generalization ability of vision-language models to unseen object categories. Specifically:- They design two complementary types of knowledge-aware prompts - discrete and continuous - to leverage category-related external knowledge from Wikipedia for the text encoder. - They propose a task-aware visual adaptation head that refines the image features by attending to salient cues relevant to the categories, to establish discriminative visual representations for the task.- Extensive experiments on 11 benchmark datasets demonstrate the effectiveness of KAPT, outperforming state-of-the-art methods especially on unseen categories. For example, compared to CoCoOp, KAPT achieves an absolute gain of 3.22% on new classes and 2.57% overall in the base-to-new setting.In summary, the key innovation is incorporating external knowledge into prompt tuning through knowledge-aware prompts and adaptation head, which significantly improves generalization to novel visual concepts compared to existing prompt tuning methods. The results validate the importance of leveraging textual knowledge for enhancing vision-language models.
