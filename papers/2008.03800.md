# [Spatiotemporal Contrastive Video Representation Learning](https://arxiv.org/abs/2008.03800)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central research question this paper addresses is:How can we develop an effective self-supervised learning method to learn spatiotemporal visual representations from unlabeled videos?The key points are:- The paper proposes a self-supervised Contrastive Video Representation Learning (CVRL) method that leverages both spatial and temporal signals from unlabeled videos. - Existing self-supervised video methods focus more on temporal signals like predicting future frames. This paper argues that spatial self-supervision signals are under-exploited for videos and aims to enhance them in CVRL.- The technical contribution is in designing spatial and temporal data augmentations tailored for self-supervised contrastive learning on videos:    - Temporally consistent spatial augmentation to maintain motion coherence.    - Temporal interval sampling strategy to handle positive pairs with varying temporal distances.- Extensive experiments demonstrate CVRL learns high-quality spatiotemporal representations, outperforming baselines like ImageNet supervised pre-training and SimCLR unsupervised pre-training.In summary, this paper presents a self-supervised framework CVRL to effectively learn general visual representations from unlabeled videos by enhancing both spatial and temporal signals, through customized data augmentations. The learned representations demonstrate strong transfer performance on various downstream tasks.
