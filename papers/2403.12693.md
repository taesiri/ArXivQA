# [As Firm As Their Foundations: Can open-sourced foundation models be used   to create adversarial examples for downstream tasks?](https://arxiv.org/abs/2403.12693)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Foundation models like CLIP are being extensively used by downstream models across various vision-language tasks to acquire strong semantic knowledge through pre-training. 
- This introduces a vulnerability - adversarial examples crafted using the publicly available CLIP model can potentially fool the downstream models due to their reliance on the same foundation model features/knowledge.
- The paper shows this is a realistic threat - CLIP-based adversarial attacks can significantly degrade performance of downstream models on segmentation, detection, captioning and VQA without needing access to those downstream models.

Proposed Method: 
- They introduce a highly effective attack strategy called Patch Representation Misalignment (PRM) that uses cosine similarity loss to misalign the CLIP vision encoder features of each image patch between clean and adversarial images.
- This disturbs dense semantics across all image regions and creates very transferable attacks to downstream models.

Key Results:
- PRM adversaries crafted using open-source CLIP encoders can successfully attack over 20 downstream models with ViT, ResNet and ConvNeXt backbones spanning 4 major vision-language tasks. 
- It significantly outperformed baseline attack strategies like training loss maximisation and other feature distortion methods.
- Examples show PRM can make downstream models hallucinate consistent false predictions across tasks.

Main Contributions:
- First work exposing and demonstrating the propagation of adversarial vulnerabilities from foundation models to their downstream systems as a realistic threat.
- An alarmingly effective CLIP-based attack strategy PRM that produces dense feature distortions to attack a wide range of downstream models without access to them.
- Extensive experiments breaking 20+ SoTA downstream V+L models highlighting the need to address this vulnerability caused by reliance on public foundation models.
