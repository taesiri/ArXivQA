# [Pragmatic Instruction Following and Goal Assistance via Cooperative   Language-Guided Inverse Planning](https://arxiv.org/abs/2402.17930)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Humans often give ambiguous instructions that rely on context and shared goals for disambiguation. However, current AI systems struggle to interpret such pragmatic instructions. 
- Existing systems either interpret instructions too literally without considering context, or perform goal inference from actions without leveraging linguistic information.
- There is a need for AI systems that can follow instructions pragmatically by integrating contextual cues from both language and observed actions.

Proposed Solution:
- The paper introduces Cooperative Language-guided Inverse Plan Search (CLIPS), a Bayesian architecture for pragmatic instruction following. 
- CLIPS models humans as approximately rational cooperative planners who compute joint plans and communicate pragmatic instructions summarizing those plans.
- It performs multimodal goal inference, using actions and instructions jointly to maintain a distribution over the human's goals. An LLM scores instruction likelihood.  
- The distribution over goals guides an assistive policy based on minimizing expected cost. This allows CLIPS to resolve ambiguous instructions and provide helpful assistance even under uncertainty.

Main Contributions:
- Formalizes the problem as a language-augmented goal assistance game.
- Proposes a structured probabilistic model of how humans cooperatively plan, act and communicate.
- Introduces algorithms for multimodal goal inference and pragmatic assistance under uncertainty.
- Empirically demonstrates that CLIPS can accurately infer goals and assist efficiently across two multi-step domains, outperforming both literal listening and unimodal inverse planning.
- Shows that CLIPS matches human inferences and assistance judgments, unlike current multimodal LLMs.


## Summarize the paper in one sentence.

 Here is a one-sentence summary of the paper:

The paper introduces a Bayesian agent architecture called cooperative language-guided inverse plan search (CLIPS) that models humans as cooperative planners who communicate joint plans, enabling the agent to follow ambiguous instructions and provide helpful goal assistance by pragmatically interpreting instructions using actions and language via multi-modal Bayesian inverse planning.


## What is the main contribution of this paper?

 The main contribution of this paper is the introduction of cooperative language-guided inverse plan search (CLIPS). Specifically:

1) CLIPS is a Bayesian architecture for pragmatic instruction following and goal assistance. It models humans as cooperative planners who communicate joint plans as instructions.

2) CLIPS performs multimodal Bayesian inference over a human's goal from their actions and instructions. It uses large language models to evaluate the likelihood of observed instructions, and inverse planning to infer goals. 

3) The inferred goal distribution allows CLIPS to pick helpful actions even under uncertainty about the human's true goal. This allows it to pragmatically resolve ambiguous or incomplete instructions.

4) Experiments in two cooperative planning domains show CLIPS significantly outperforms baselines in accuracy and helpfulness. It closely matches inferences and judgments made by human raters, demonstrating the importance of pragmatic reasoning.

In summary, the main contribution is the CLIPS architecture for pragmatic goal assistance that jointly leverages actions, instructions, and Bayesian inference to effectively collaborate with humans even under uncertainty.


## What are the keywords or key terms associated with this paper?

 Based on my review, some of the key terms and concepts associated with this paper include:

- Cooperative language-guided inverse plan search (CLIPS) - The overall Bayesian architecture proposed in the paper for pragmatic instruction following and goal assistance.

- Inverse planning - Using observations of an agent's behavior to infer their underlying goals and rewards. A key component of CLIPS.

- Rational speech acts (RSA) - A framework that models communication as a recursive pragmatic process between speakers and listeners. CLIPS instantiates RSA for cooperative planning. 

- Joint intentionality - The human capacity to cooperate towards shared goals and divide tasks efficiently. CLIPS models human principals as having this capacity.

- Value alignment, assistance games - Formal frameworks for modeling cooperation between humans and AI systems. CLIPS provides an approximate solution.

- Pragmatic context, ambiguous instructions - The paper evaluates how well CLIPS can resolve ambiguous language by leveraging observations of a human's actions.

- Multimodal goal inference - CLIPS performs Bayesian inference over goals by combining evidence from both actions and linguistic instructions.

- Uncertainty-aware assistance - CLIPS assists humans by selecting helpful actions that minimize expected cost given uncertainty over goals.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the cooperative language-guided inverse plan search (CLIPS) method proposed in the paper:

1. The paper models pragmatic communication through a structured generation procedure using language models. What are some limitations of this approach compared to a full rational speech acts model? How could the utterance model be extended to better capture pragmatic principles?

2. The paper assumes that humans conceive of cooperative plans from a group perspective rather than recursive reasoning. What evidence supports this joint intentionality assumption? Are there cases where humans do nested reasoning that the model may fail on? 

3. For tractable inference, the paper opts for an enumerative approach over sampling when computing the goal posterior. What are the tradeoffs? When would a particle filter method be preferred over exact enumeration?

4. The paper argues that assistants should condition on interventions rather than observations of their own actions during inference. What pathologies could arise if assistants failed to do this? Are there risks even after intervening is accounted for?

5. How sensitive is the performance of CLIPS to mis-specification of the human action costs used during planning? Could inverse reinforcement learning be used to infer costs?

6. The paper evaluates goal accuracy and plan optimality relative to human judgments. What other metrics could be used to evaluate assistance quality? How could subjective measures be incorporated?

7. CLIPS uses model-based planning to estimate $Q$-values. How well would model-free deep RL methods that learn $Q$-functions perform in this setting? What are the tradeoffs?

8. For language grounding, the paper opts for an enumerative approach over neural decoding methods. What are the limitations of enumeration as the action and object space grows large? 

9. The paper focuses on offline plan generation given fixed beliefs. How could CLIPS be extended to do online belief updating with information gathering actions?

10. GPT-4V struggled at perceptual aspects of reasoning like spatial relationships. What perceptual capacities are needed alongside LLMs for more human-like assistance?
