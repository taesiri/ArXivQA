# [Pragmatic Instruction Following and Goal Assistance via Cooperative   Language-Guided Inverse Planning](https://arxiv.org/abs/2402.17930)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Humans often give ambiguous instructions that rely on context and shared goals for disambiguation. However, current AI systems struggle to interpret such pragmatic instructions. 
- Existing systems either interpret instructions too literally without considering context, or perform goal inference from actions without leveraging linguistic information.
- There is a need for AI systems that can follow instructions pragmatically by integrating contextual cues from both language and observed actions.

Proposed Solution:
- The paper introduces Cooperative Language-guided Inverse Plan Search (CLIPS), a Bayesian architecture for pragmatic instruction following. 
- CLIPS models humans as approximately rational cooperative planners who compute joint plans and communicate pragmatic instructions summarizing those plans.
- It performs multimodal goal inference, using actions and instructions jointly to maintain a distribution over the human's goals. An LLM scores instruction likelihood.  
- The distribution over goals guides an assistive policy based on minimizing expected cost. This allows CLIPS to resolve ambiguous instructions and provide helpful assistance even under uncertainty.

Main Contributions:
- Formalizes the problem as a language-augmented goal assistance game.
- Proposes a structured probabilistic model of how humans cooperatively plan, act and communicate.
- Introduces algorithms for multimodal goal inference and pragmatic assistance under uncertainty.
- Empirically demonstrates that CLIPS can accurately infer goals and assist efficiently across two multi-step domains, outperforming both literal listening and unimodal inverse planning.
- Shows that CLIPS matches human inferences and assistance judgments, unlike current multimodal LLMs.
