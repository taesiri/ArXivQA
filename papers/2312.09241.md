# [TinyGSM: achieving &gt;80% on GSM8k with small language models](https://arxiv.org/abs/2312.09241)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a summary paragraph of the key points from the paper:

This paper proposes a method for enabling small language models to achieve strong performance on the challenging GSM8K grade school math word problem benchmark. The key ideas are: (1) generate a high-quality synthetic dataset called TinyGSM containing 12.3 million GSM8K-style math problems paired with Python solutions; this is produced by GPT-3.5. (2) Finetune small LMs (125M to 1.3B parameters) on TinyGSM, allowing even the 125M model to reach 63% on GSM8K. (3) Further boost performance by using a separate small verifier model to score multiple candidate solutions and select the best one. This allows their 1.3B model to achieve 81.5% on GSM8K, competitive with models orders of magnitude larger. The quality of TinyGSM and the effectiveness of the verifier model are the main drivers of the strong results. The work demonstrates the promise of small LMs, high-quality synthetic datasets, and verification mechanisms for advancing mathematical reasoning.


## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem Statement
- Small language models (SLMs) are computationally more efficient, but it remains unclear how capable they are at mathematical reasoning compared to large language models (LLMs). Breaking the 80% accuracy barrier on the challenging GSM8K math word problem benchmark has so far required models with at least 34B parameters.

Proposed Solution 
- Introduce TinyGSM, a new high-quality synthetic dataset of 12.3M math word problems and Python solutions generated fully by GPT-3.5.
- Finetune 125M to 1.3B parameter SLMs on TinyGSM, with the 1.3B model reaching 68.2% GSM8K accuracy. 
- Further boost performance by using a separate verifier model to score multiple candidate solutions and select the best one. This allows the 1.3B model to reach 81.5% accuracy, surpassing all existing open-source models under 100B parameters.

Key Contributions
- TinyGSM dataset for training SLMs on math word problems.
- State-of-the-art 81.5% GSM8K accuracy using an ensemble of 1.3B generation model and 1.3B verifier model. 
- Analysis showing verifier model size matters more than generation model size, and that data diversity is crucial to verifier performance.
- Evidence that with a high-quality dataset and verification, SLMs can match or exceed the capabilities of the much larger models they learn from.
