# [Towards Tracing Trustworthiness Dynamics: Revisiting Pre-training Period   of Large Language Models](https://arxiv.org/abs/2402.19465)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Ensuring the trustworthiness of large language models (LLMs) is crucial, but most prior work has focused only on fully pre-trained models rather than the dynamics during pre-training. 
- Two key open questions: (1) How do LLMs dynamically encode concepts related to dimensions of trustworthiness (e.g. reliability, fairness) during pre-training? (2) Can the pre-training process itself be utilized to improve trustworthiness?

Methods:
- Use linear probing on activations from 360 checkpoints of a 7B parameter LLM during pre-training to analyze encoding of 5 trustworthiness dimensions.
- Extract "steering vectors" from checkpoints to intervene and enhance trustworthiness of a separately fine-tuned conversational LLM.
- Probe checkpoints with mutual information between activations and labels to study pre-training dynamics.

Key Findings:
- Middle layer activations can linearly separate trustworthiness concepts, even early in pre-training. Performance fluctuates after an initial increase. 
- Steering vectors from midway checkpoints match or even exceed performance of those from the fine-tuned LLM in improving its trustworthiness.
- Mutual information reveals a clear 2-phase trend ("fitting" then "compression") regarding trustworthiness over pre-training.

Main Contributions:
- First study analyzing and harnessing LLM pre-training period itself to understand and improve trustworthiness.
- Demonstrate substantial unused potential in pre-training checkpoints for trustworthiness enhancement.
- Reveal new insights into phase transition of fitting then compressing trust-related information during pre-training.

The summary covers the key details on the problem being addressed, the techniques used, major results obtained, and main contributions made in the paper. Let me know if you need any clarification or have additional questions!
