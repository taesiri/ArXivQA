# [Dialect prejudice predicts AI decisions about people's character,   employability, and criminality](https://arxiv.org/abs/2403.00742)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Prior AI research has focused on overt racism in language models, but social scientists argue that in the US, racism has become more "covert" after the civil rights movement - avoiding direct mention of race but still perpetuating racial inequities. 
- It is unknown whether this covert racism manifests in language models through "dialect prejudice" - negative stereotypes triggered solely by features of a racialized dialect like African American English (AAE).

Proposed Solution:  
- Introduce a new method called "Matched Guise Probing" to probe covert stereotypes triggered by AAE features, without overt mention of race. Compare texts in AAE vs Standard American English (SAE).
- Analyze multiple language models - GPT2, RoBERTa, T5, GPT3.5, GPT4.

Main Contributions:
- First evidence of covert dialect prejudice in language models, through negative stereotypes about intelligence and criminality. Stereotypes are more negative than worst recorded human stereotypes about African Americans.  
- Observe discrepancy between overt positive and covert negative stereotypes about African Americans, especially in models with human feedback. Suggests models conceal racism.
- Prejudice impacts hypothetical decisions - models assign less prestigious jobs and harsher sentences to AAE speakers.  
- Increasing scale and adding human feedback training does NOT reduce covert racism. Can make discrepancy between overt and covert stereotypes worse.
- Relate language models' inconsistent racial attitudes to inconsistencies in contemporary US racial attitudes after the civil rights movement.

Key implications are that current methods to reduce racial bias in language models are ineffective for covert forms of racism manifesting through dialect. The allocational harms resulting from models' dialect prejudice in areas like employment and criminal justice risk amplifying discrimination against minorities like African Americans.
