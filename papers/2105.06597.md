# RetGen: A Joint framework for Retrieval and Grounded Text Generation   Modeling

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the main research question appears to be:How can we develop a framework to improve text generation models by grounding them in relevant external documents, without requiring explicit document-text pairs for training?The key points are:- Text generation models like GPT-3 can produce fluent text but often suffer from hallucinating facts or being unfaithful to real-world knowledge. - Grounding text generation in external documents (like Wikipedia) could help address these issues, but typically requires parallel data of documents mapped to target text.- This paper proposes a framework called RetGen that jointly trains a retriever and generator to optimize a text generation objective, without needing explicit document-text pairs.- The retriever learns to select relevant documents for a given context based on the generator's language modeling signal. - The generator learns to incorporate information from multiple retrieved documents in a mixture-of-experts approach.- Experiments on Reddit conversations and arXiv abstracts show improvements in text relevance, factuality, and use of external information compared to baseline models.In summary, the main hypothesis is that jointly training the retriever and generator in an end-to-end fashion, using only a non-parallel text corpus, can improve the grounding and factual accuracy of text generation systems. The proposed RetGen framework aims to test this hypothesis.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contributions appear to be:- Proposing a joint training framework called RetGen to simultaneously optimize a dense passage retriever and a knowledge-grounded text generator in an end-to-end fashion. This allows the retriever and generator to work synergistically to produce more informative and relevant text.- Showing that the retriever can be optimized using the language modeling signals from the generator, which alleviates the need for parallel data of context-document pairs for training.- Developing a mixture-of-experts style decoder that can leverage multiple retrieved documents to generate text in a multi-document fashion. This allows flexible incorporation of external information.- Demonstrating the effectiveness of RetGen on two datasets - Reddit for dialog response generation and arXiv for scientific text generation. Results show improvements over baselines in both automatic metrics and human evaluation.In summary, the main contribution appears to be proposing the joint training framework RetGen to enable better grounded text generation by synergistically optimizing the retriever and generator components. A key advantage is the ability to train without parallel data of context and reference documents.
