# [BiViT: Extremely Compressed Binary Vision Transformer](https://arxiv.org/abs/2211.07091)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading, the central research question this paper addresses is: 

How can we effectively binarize vision Transformers to compress model size and accelerate inference while maintaining accuracy?

Specifically, the authors identify two key challenges in binarizing vision Transformers:

1) Accurately binarizing the softmax attention mechanism without damaging its functionality. The traditional binarization method using the Bool function causes large quantization errors. 

2) Preserving the information from the pretrained full-precision model during binarization. Directly binarizing all parameters leads to severe performance degradation that is difficult to recover.

To address these challenges, the main contributions of the paper are:

1) A Softmax-aware Binarization method that adapts to the long-tailed distribution of attention scores and reduces quantization error.

2) A Cross-layer Binarization scheme and learnable weight binarization that help retain pretrained information and enhance model representation ability. 

By combining these solutions, the authors propose the first effective binary vision Transformer (BiViT) that achieves significant performance improvements over prior arts and state-of-the-art accuracy on image classification benchmarks.

In summary, the central hypothesis is that with customized solutions to address the unique challenges in binarizing vision Transformers, it is possible to develop accurate yet highly compressed BiViT models for efficient inference. The paper makes contributions in this direction.


## What is the main contribution of this paper?

 The main contributions of this paper are:

1. Proposing Softmax-aware Binarization for self-attention modules to reduce quantization error caused by the long-tailed distribution of attention scores. 

2. Proposing Cross-layer Binarization and learnable weight binarization to preserve pretrained information and enhance model representation ability.

3. Combining the above methods to successfully binarize vision Transformers (BiViTs) and achieve state-of-the-art results on TinyImageNet and ImageNet image classification benchmarks. 

Specifically, the key ideas include:

- Analyzing the long-tailed distribution of softmax attention and proposing an optimization algorithm to find the optimal binarization threshold. Further approximating the threshold with a fixed coefficient and the maximum value for efficient inference.

- Decoupling the binarization of self-attention and MLP modules through Cross-layer Binarization to avoid mutual interference and better utilize pretrained information.

- Introducing learnable channel-wise scaling factors instead of fixed scales for weight binarization to enhance representation ability. 

- Evaluating BiViT on TinyImageNet and ImageNet datasets and showing significant improvements over previous methods, achieving 70.8% top-1 accuracy on ImageNet with Swin-T model.

In summary, the main contribution is proposing customized solutions to address the key challenges in binarizing vision Transformers, making them applicable to visual tasks for the first time. The Softmax-aware Binarization and information preservation techniques are crucial for the success of BiViTs.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the key points in the paper:

The paper proposes three methods - Softmax-aware Binarization, Cross-layer Binarization, and Learnable Weight Binarization - to enable effective binarization of vision Transformers by minimizing the quantization error in attention modules and better preserving information from the pretrained model.
