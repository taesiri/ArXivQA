# [BiViT: Extremely Compressed Binary Vision Transformer](https://arxiv.org/abs/2211.07091)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central research question this paper addresses is: How can we effectively binarize vision Transformers to compress model size and accelerate inference while maintaining accuracy?Specifically, the authors identify two key challenges in binarizing vision Transformers:1) Accurately binarizing the softmax attention mechanism without damaging its functionality. The traditional binarization method using the Bool function causes large quantization errors. 2) Preserving the information from the pretrained full-precision model during binarization. Directly binarizing all parameters leads to severe performance degradation that is difficult to recover.To address these challenges, the main contributions of the paper are:1) A Softmax-aware Binarization method that adapts to the long-tailed distribution of attention scores and reduces quantization error.2) A Cross-layer Binarization scheme and learnable weight binarization that help retain pretrained information and enhance model representation ability. By combining these solutions, the authors propose the first effective binary vision Transformer (BiViT) that achieves significant performance improvements over prior arts and state-of-the-art accuracy on image classification benchmarks.In summary, the central hypothesis is that with customized solutions to address the unique challenges in binarizing vision Transformers, it is possible to develop accurate yet highly compressed BiViT models for efficient inference. The paper makes contributions in this direction.
