# [BiViT: Extremely Compressed Binary Vision Transformer](https://arxiv.org/abs/2211.07091)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading, the central research question this paper addresses is: 

How can we effectively binarize vision Transformers to compress model size and accelerate inference while maintaining accuracy?

Specifically, the authors identify two key challenges in binarizing vision Transformers:

1) Accurately binarizing the softmax attention mechanism without damaging its functionality. The traditional binarization method using the Bool function causes large quantization errors. 

2) Preserving the information from the pretrained full-precision model during binarization. Directly binarizing all parameters leads to severe performance degradation that is difficult to recover.

To address these challenges, the main contributions of the paper are:

1) A Softmax-aware Binarization method that adapts to the long-tailed distribution of attention scores and reduces quantization error.

2) A Cross-layer Binarization scheme and learnable weight binarization that help retain pretrained information and enhance model representation ability. 

By combining these solutions, the authors propose the first effective binary vision Transformer (BiViT) that achieves significant performance improvements over prior arts and state-of-the-art accuracy on image classification benchmarks.

In summary, the central hypothesis is that with customized solutions to address the unique challenges in binarizing vision Transformers, it is possible to develop accurate yet highly compressed BiViT models for efficient inference. The paper makes contributions in this direction.


## What is the main contribution of this paper?

 The main contributions of this paper are:

1. Proposing Softmax-aware Binarization for self-attention modules to reduce quantization error caused by the long-tailed distribution of attention scores. 

2. Proposing Cross-layer Binarization and learnable weight binarization to preserve pretrained information and enhance model representation ability.

3. Combining the above methods to successfully binarize vision Transformers (BiViTs) and achieve state-of-the-art results on TinyImageNet and ImageNet image classification benchmarks. 

Specifically, the key ideas include:

- Analyzing the long-tailed distribution of softmax attention and proposing an optimization algorithm to find the optimal binarization threshold. Further approximating the threshold with a fixed coefficient and the maximum value for efficient inference.

- Decoupling the binarization of self-attention and MLP modules through Cross-layer Binarization to avoid mutual interference and better utilize pretrained information.

- Introducing learnable channel-wise scaling factors instead of fixed scales for weight binarization to enhance representation ability. 

- Evaluating BiViT on TinyImageNet and ImageNet datasets and showing significant improvements over previous methods, achieving 70.8% top-1 accuracy on ImageNet with Swin-T model.

In summary, the main contribution is proposing customized solutions to address the key challenges in binarizing vision Transformers, making them applicable to visual tasks for the first time. The Softmax-aware Binarization and information preservation techniques are crucial for the success of BiViTs.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the key points in the paper:

The paper proposes three methods - Softmax-aware Binarization, Cross-layer Binarization, and Learnable Weight Binarization - to enable effective binarization of vision Transformers by minimizing the quantization error in attention modules and better preserving information from the pretrained model.


## How does this paper compare to other research in the same field?

 Here are my thoughts on how this paper compares to other research in the field of binarizing vision transformers:

- This is one of the first papers to thoroughly study binarizing vision transformers. Most prior work on model binarization has focused on convolutional neural networks. There has been some work on binarizing natural language processing transformers like BERT, but very little on binarizing vision transformers. So this is pioneering research in that regard.

- The paper proposes novel solutions to two key challenges in binarizing vision transformers: accurately binarizing the softmax attention, and preserving information from the pretrained model during binarization. These ideas, like the Softmax-aware Binarization and Cross-layer Binarization, are tailored for transformers and demonstrate superior results compared to naive application of prior CNN binarization techniques.

- The proposed BiViT significantly outperforms prior state-of-the-art methods on image classification benchmarks like TinyImageNet and ImageNet. For example, it improves top-1 accuracy by 19.8% on TinyImageNet compared to the previous best method. This level of performance truly demonstrates the potential of binarized vision transformers.

- Compared to binary CNNs like ResNet, the accuracy gap to the full-precision model is still larger for BiViT. But the paper shows BiViT can surpass binary CNNs in some cases, and provides a strong baseline for future research. There is a lot of room left to further improve BiViT.

- The paper only evaluates BiViT on image classification. Expanding it to other vision tasks like object detection and segmentation remains future work. Overall, this is an important first step in binarizing vision transformers, with novel ideas and strong results. But more research is needed to close the gap to full precision models and generalize beyond image classification.
