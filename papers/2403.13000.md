# [Duwak: Dual Watermarks in Large Language Models](https://arxiv.org/abs/2403.13000)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
As large language models (LLMs) are increasingly used to generate text, there is a need to audit their usage, govern their applications, and mitigate potential harms. Existing watermarking techniques can embed human-imperceptible, machine-detectable patterns into generated text, but have limitations in detection efficiency (minimum tokens required for detection), especially for short texts and texts that have been post-edited.

Proposed Solution: 
The paper proposes a new watermarking approach called \algo that embeds dual secret patterns into both the token probability distribution and sampling scheme of LLMs. This combines two independent watermarks to significantly improve detection efficiency. 

To avoid degrading text quality from modifying token probabilities, \algo uses a novel contrastive search sampling that minimizes token repetition and enhances diversity. Theoretical analysis shows the two watermarks do not interfere with each other.

Main Contributions:
1) A dual watermarking scheme for LLMs that jointly modifies probability distribution and sampling to improve detection efficiency.

2) A contrastive search sampling method that increases diversity and mitigates quality degradation.

3) Extensive empirical evaluation showing \algo requires 70\% fewer tokens for detection than state-of-the-art methods, with the best or second best text quality. It is especially robust to post-editing attacks like paraphrasing.

In summary, the key innovation is a high efficiency dual watermark scheme with contrastive search sampling to maintain text quality and diversity. Experiments demonstrate clear improvements in detection performance across metrics compared to existing techniques.


## Summarize the paper in one sentence.

 Here is a one sentence summary of the key points from the paper:

The paper proposes a new dual watermarking method for large language models called \algo that embeds secret patterns in both the token probability distribution and sampling scheme to improve detection efficiency, text quality, and robustness against post-editing attacks compared to existing single watermark approaches.


## What is the main contribution of this paper?

 According to the abstract, the main contribution of this paper is proposing a new dual watermarking approach called "Duwak" for large language models (LLMs) that fundamentally enhances the efficiency and quality of watermarking. Specifically:

1) It improves watermark detection efficiency by embedding dual secret patterns in both the token probability distribution and sampling schemes of the LLM. Detecting the union of these two watermarks enables more efficient detection with fewer tokens required. 

2) It increases the diversity and robustness of the generated text by avoiding expression degradation typically caused by watermarking. This is done through a novel quality-aware sampling scheme called "contrastive search" that minimizes token repetition.

3) Extensive empirical evaluation shows Duwak achieves the highest watermarked text quality with the lowest number of tokens needed for detection, compared to existing single watermark techniques and combinations of them, especially under post paraphrasing attacks. For example, Duwak requires up to 70% less tokens for detection than prior methods.

In summary, the main contribution is the novel Duwak approach for dual watermarking of LLMs that jointly improves detection efficiency, text quality, and robustness to attacks over current state-of-the-art methods.


## What are the keywords or key terms associated with this paper?

 Based on my review, some of the key terms and concepts associated with this paper include:

- Large language models (LLMs)
- Watermarking 
- Text generation
- Dual watermarks
- Token probability distribution
- Sampling scheme 
- Contrastive search
- Text quality
- Detection efficiency
- Robustness
- Post-editing attacks
- Green list/red list
- Spike entropy
- Perplexity
- Hypothesis testing

The paper proposes a new watermarking technique called "Duwak" that embeds dual secret patterns into LLMs, in both the token probability distribution and the sampling scheme. Key goals are to improve watermark detection efficiency and text quality compared to existing single watermark methods, especially under post-editing attacks. The contrastive search sampling scheme is designed to increase diversity and mitigate quality degradation. Evaluations focus on measures like diversity, perplexity, detection token counts, and robustness to paraphrasing or other attacks. Comparisons are made to prior techniques including KGW, exponential, binary, and inverse transform sampling watermarks. Overall, the dual watermark approach shows benefits in efficiency, quality, and robustness.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in the paper:

1. The paper proposes using dual watermarks in language models - one in the token probability distribution and another in the sampling scheme. What is the rationale behind using two watermarks instead of just one? How do the two watermarks complement each other?

2. Contrastive search is used as the sampling scheme watermark. How exactly does contrastive search work to embed a watermark? What objective function is optimized in contrastive search to embed the watermark? 

3. How robust is the contrastive search watermark to attacks like paraphrasing or token swapping compared to watermarks based solely on modifying token probabilities? Explain why.

4. The chi-square test is used to combine the p-values from detecting each watermark. Walk through the mathematical details of how the chi-square test allows jointly detecting the dual watermarks.

5. What theoretical guarantees does the method provide on the embedding capacity and detectability of the dual watermarks? What assumptions are made to derive these guarantees?

6. One of the benefits claimed is improved text quality by using contrastive search. Empirically, how much better is the quality compared to simply using a probability distribution watermark?

7. For the contrastive search process, self-similarity is used as the optimization criterion. Why is self-similarity a useful metric in this context? Are there any alternatives you can think of?

8. The watermark relies on a secret key for randomness. What properties should this secret key have? What risks arise if the key is exposed?

9. The empirical evaluation relies heavily on post-editing attacks. Why are these attacks important considerations for analyzing watermark methods? How does the proposed approach fare?

10. The method has only been evaluated on a single language model. What differences across language models could impact the effectiveness of the overall approach and specifically contrastive search?
