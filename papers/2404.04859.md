# [Demystifying Lazy Training of Neural Networks from a Macroscopic   Viewpoint](https://arxiv.org/abs/2404.04859)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem Statement:
- The paper investigates how the initialization scale impacts the training dynamics and generalization performance of deep neural networks. Specifically, it examines the role of the initial scale of the output function, denoted by κ, in governing the training behavior. 

- Motivated by prior work on the Neural Tangent Kernel (NTK) regime and the mean-field regime, the paper aims to provide a unified perspective to characterize when neural networks can be effectively trained while the parameters hardly change from their initial values.

Proposed Approach:
- The paper introduces a normalized version of the neural network where each layer's weights are scaled by 1/√m and the output by 1/m, where m is the width. 

- It then analyzes the training dynamics as m → ∞, characterizing how the initial scale κ allows the network output to be well-approximated by a linearization around the initial parameters. 

- This linear regime is referred to as the "theta-lazy" regime and holds when κ > 1 and log(κ)/log(m) > 0 as m → ∞. This generalizes the NTK regime's assumptions.

-Rigorous analysis based on properties of the neural network Gram matrices shows that in this regime, the parameters stay close to initialization and the training loss decays exponentially fast.

Main Contributions:
- Identifies the initial scale κ as the key factor governing neural network trainability, irrespective of specific initialization schemes. 

- Greatly expands the applicability of the NTK view by relaxing its assumptions. Shows the theta-lazy regime holds more generally.

- Provides a unified technique to analyze multi-layer network training dynamics through macroscopic limits as m → ∞.

- Reveals that within the theta-lazy regime, neural networks exhibit linearized behavior where parameters persist close to initialization during training.

- Theoretical results give insight into role of overparameterization in enabling rapid training, highlighting interplay between initial scale factors.

In summary, the paper significantly advances the understanding of how initialization determines the training dynamics of overparameterized neural networks. The proposed approach and analysis framework provide a macroscopic view to elucidate the mechanisms underlying efficient lazy training.
