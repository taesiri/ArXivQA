# [How FaR Are Large Language Models From Agents with Theory-of-Mind?](https://arxiv.org/abs/2310.03051)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be: Can large language models effectively leverage theory-of-mind reasoning to select appropriate actions in social scenarios?More specifically, the paper introduces a new evaluation paradigm called "Thinking for Doing" (T4D) which requires models to connect inferences about others' mental states to actions in social contexts. The goal is to probe whether large language models like GPT-4 and PaLM can move beyond just making inferences about beliefs, emotions, etc. and use that capability to determine proper actions as situated agents. The key hypotheses appear to be:1) Despite excelling at theory-of-mind inference questions, LLMs will struggle when asked to choose actions based on mental state reasoning in T4D. 2) The core challenge is identifying implicit intermediate inference steps without being explicitly directed like in existing ToM benchmarks.3) Providing reasoning structure through the proposed "Foresee and Reflect" (FaR) prompting framework will significantly improve LLMs' performance on T4D while generalizing across diverse contexts.In summary, the central research question asks whether LLMs can effectively connect social reasoning to pragmatic actions, with hypotheses about why this is difficult for current models and how reasoning structure helps. The T4D paradigm and FaR prompting method are introduced to test these hypotheses.


## What is the main contribution of this paper?

The main contribution of this paper appears to be the proposal of a new evaluation paradigm called "Thinking for Doing" (T4D). Specifically, the key contributions seem to be:1. T4D is introduced as a new way to challenge large language models (LLMs) to connect social reasoning, based on theory of mind, to pragmatic actions. Whereas prior benchmarks like ToMi focus mainly on probing models' capabilities for inference, T4D takes the next step of requiring models to determine proper actions based on inferred mental states.2. The paper shows through experiments that current LLMs struggle on T4D, despite good performance on ToMi. Analysis suggests a key bottleneck is the lack of structure to identify relevant implicit inference steps. 3. To address this limitation, the authors propose a new prompting framework called Foresee and Reflect (FaR) that provides models with reasoning structure using future prediction and reflection. Experiments demonstrate FaR substantially improves LLM performance on T4D.4. Through ablation studies and generalization tests, the paper provides analysis showing that both components of FaR are important and that it can robustly generalize across diverse contexts and ToM-related tasks.In summary, the main contribution appears to be the proposal and analysis of the new T4D evaluation paradigm, the identification of key challenges that LLMs face on this task, and the design of the FaR prompting framework to address those limitations. The paper provides both methodology and an analysis framework for assessing and improving LLMs' capabilities to connect reasoning to action.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence TL;DR summary of the paper:The paper proposes a new evaluation paradigm called Thinking for Doing (T4D) to test if large language models can use theory-of-mind reasoning to choose proper actions, finds models struggle on T4D, analyzes the reasoning challenges, and introduces a prompting framework called Foresee and Reflect (FaR) that provides reasoning structure and dramatically improves model performance.


## How does this paper compare to other research in the same field?

This paper introduces a new task called Thinking for Doing (T4D) to evaluate whether large language models (LLMs) can connect social reasoning to actions. Here are some key ways it compares to related work:- It builds on prior work probing LLMs' theory of mind capabilities using datasets like ToM-bAbI and ToMi. However, it argues these existing benchmarks focus only on making inferences about mental states, not acting on those inferences. T4D requires models to choose actions based on inferred mental states.- There is growing interest in using LLMs for agentic tasks and interactive environments. However, much prior work focuses on tool use, navigation, etc. This paper uniquely looks at actions requiring modeling other agents' mental states, an important aspect of social intelligence. - Many prompting techniques have been proposed to improve LLMs' reasoning, like chain-of-thought and tree-of-thought. This paper introduces a new prompt called Foresee and Reflect (FaR) specifically designed to provide structure for reasoning about mental states and actions.- Compared to prior prompting work, the paper emphasizes the benefits of imposing structure on reasoning rather than proposing a wholly new technique. It also analyzes the effectiveness of the structure via ablations.- For evaluation, the paper not only looks at in-distribution performance but also tests generalization, such as to different story structures and non-false belief scenarios. This provides a more rigorous test of the modeling capabilities.In summary, while building on prior work, the paper makes important contributions in task formulation, analysis, and prompting techniques tailored to an underexplored but critical aspect of social intelligence - reasoning about others' mental states to guide actions. The introduced framework of T4D and FaR prompt provides a foundation for future work in this direction.


## What future research directions do the authors suggest?

Based on my reading of the paper, here are some of the main future research directions suggested by the authors:- Exploring how to help LLMs recover from noisy or irrelevant foresight steps when using the FaR prompting framework. The authors find that LLMs struggle to ignore or correct faulty reasoning chains generated during the "foresee" component of FaR. Developing techniques to make LLMs more robust to noisy foresight is noted as an important challenge.- Gaining a deeper understanding of LLMs' internal representations when guided by structured prompts like FaR. The authors propose analyzing how the reasoning process changes within the model when following frameworks like FaR compared to unstructured reasoning. - Generalizing the FaR framework to more open-ended action spaces beyond multiple choice. The current T4D task constrains actions to 4 multiple choice options. Allowing free generation of action intents is noted as an ambitious future direction.- Expanding the diversity of theory of mind scenarios beyond False Belief Tests. The authors demonstrate some generalization capability of FaR to faux pas stories, but note that applying FaR to more diverse ToM situations remains an open challenge.- Combining the benefits of FaR's reasoning structure with other prompting techniques like Chain of Thought. The authors suggest exploring prompts that blend FaR's high-level reasoning framework with more fine-grained step-wise reasoning guidance.- Developing methods to help models better handle ambiguity in observations. Resolving ambiguous or incomplete information is noted as an inherent challenge in naturalistic ToM scenarios.In summary, the main future directions focus on 1) improving the robustness of structured reasoning frameworks like FaR, 2) better understanding model representations when following such frameworks, and 3) extending structured reasoning techniques like FaR to broader, more challenging ToM settings.


## Summarize the paper in one paragraph.

Here is a one paragraph summary of the key points from the paper:The paper proposes a new evaluation paradigm called Thinking for Doing (T4D) to assess whether large language models (LLMs) can leverage theory of mind reasoning to determine proper actions in social scenarios. The authors convert an existing theory of mind benchmark called ToMi into a situated action selection task and find that while LLMs excel at answering inference questions about characters' mental states on ToMi, their performance drops dramatically on selecting helpful actions in the converted T4D task. Through analysis and oracle experiments, the paper identifies that a key bottleneck is LLMs' difficulty in locating proper implicit inference steps that lead to intentful actions. To address this, the authors introduce a zero-shot prompting framework called Foresee and Reflect (FaR) that provides LLMs a reasoning structure to anticipate potential future challenges and reflect on actions to help address them. Experiments demonstrate FaR substantially improves LLMs' performance on T4D and generalizes robustly to diverse contexts, consistently outperforming other prompting methods. Overall, the work proposes a new challenging task to push LLMs towards pragmatic actions based on social reasoning and offers insights into how to better inject reasoning structures into generative models.


## Summarize the paper in two paragraphs.

Here is a 2 paragraph summary of the key points from the paper:The paper proposes a new evaluation paradigm called Thinking for Doing (T4D) to assess whether language models can leverage theory-of-mind reasoning to determine proper actions, rather than just answering questions about mental states. The authors convert an existing theory-of-mind question answering dataset (ToMi) into a situated action selection task. Experiments show that large language models like GPT-4 struggle on T4D, despite high accuracy on answering ToMi's reasoning questions. Analysis reveals the core challenge is identifying implicit reasoning steps to choose the correct action. To address this, the authors introduce Foresee and Reflect (FaR), a zero-shot prompting framework with a reasoning structure combining future prediction and action-aware reflection. Experiments demonstrate FaR substantially improves language model accuracy on T4D. The framework generalizes well, outperforming other methods on out-of-distribution story structures and non-false belief theory of mind scenarios. Overall, the work highlights the limitations of current models in translating reasoning to action and provides insights into prompting frameworks that can enhance language agent capabilities.


## Summarize the main method used in the paper in one paragraph.

Here is a one paragraph summary of the main method used in the paper:The paper proposes a new evaluation paradigm called Thinking for Doing (T4D) to test whether language models can connect social reasoning to actions. The authors convert an existing benchmark for theory of mind called ToMi into a situated action task, where models must decide which character to help based on inferring their mental states. Experiments show large language models struggle on T4D, even though they perform well on answering explicit inference questions in ToMi. Through analysis, the authors find models have difficulty identifying implicit inferences needed to determine the right action. To address this, they design a zero-shot prompting framework called Foresee and Reflect (FaR) that provides models with reasoning structure to anticipate future challenges and reflect on potential actions. FaR significantly improves model accuracy on T4D compared to other prompting methods and shows robust generalization, demonstrating the benefits of imposing structured reasoning for theory of mind grounded action tasks.
