# [Preference-Conditioned Language-Guided Abstraction](https://arxiv.org/abs/2402.03081)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper "Preference-Conditioned Language-Guided Abstraction":

Problem: 
In robot learning from demonstrations, the robot may fail to fully capture all the reasons behind the demonstrated behavior due to spurious correlations or incomplete specifications. For example, a human demonstrating "throw away the can" may take different paths to avoid electronics, but without explicitly mentioning this preference, the robot cannot learn to generalize this aspect. Humans construct simplified mental representations of tasks based on prior experience, common sense, and teaching. Recent work used language models (LMs) to construct abstract state representations, but these may still not capture user preferences not specified in language.

Proposed Solution:
This paper proposes Preference-Conditioned Language-Guided Abstraction (PLGA) to learn latent user preferences from changes in demonstration behavior and use those to construct personalized state abstractions. The key insight is that changes in human behavior reveal differences in their mental representations of the task. PLGA uses LMs in two ways: 
1) Given a task description and observed behavioral changes between states, query the LM to hypothesize possible hidden preferences explaining this change. 
2) Given the most likely preference, query the LM to construct a personalized state abstraction capturing this preference.

The LM can also actively query the human when uncertain about the preference. PLGA learns policies over these preference-conditioned abstractions.

Contributions:
- Formalizes the problem of learning personalized state abstractions conditioned on latent user preferences 
- Proposes a framework (PLGA) to query LMs for likely preferences using language and demonstrations
- Demonstrates PLGA's ability to capture generic and user-specific preferences in simulation and real robot tasks
- Shows querying for preferences enables easier user interaction over manually constructing abstractions

The key insight is leveraging LMs and changes in behavior to uncover hidden preferences for abstraction learning. Experiments show PLGA better captures preferences, enabling better generalization in downstream tasks.
