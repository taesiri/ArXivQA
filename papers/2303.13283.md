# [Visual-Language Prompt Tuning with Knowledge-guided Context Optimization](https://arxiv.org/abs/2303.13283)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading, the key points of this paper are:

- The paper proposes a new method called Knowledge-guided Context Optimization (KgCoOp) for visual-language prompt tuning. 

- The goal is to enhance the generalization ability of learnable prompts for unseen classes. Existing methods like CoOp tend to overfit on seen classes and do not generalize well. 

- The key idea in KgCoOp is to minimize the discrepancy between the learnable prompt and handcrafted prompt from CLIP. This helps retain essential general knowledge and improves unseen class accuracy.

- Experiments show KgCoOp achieves better performance on unseen classes compared to CoOp, CoCoOp and ProGrad. It also obtains higher overall accuracy with less training time.

So in summary, the central hypothesis is that reducing the gap between the learnable prompt and original CLIP prompt can improve generalization to unseen classes in prompt tuning. The paper proposes and validates the KgCoOp method to achieve this goal.


## What is the main contribution of this paper?

 Based on my reading, the main contributions of this paper are:

1. It proposes a novel prompt tuning method called Knowledge-guided Context Optimization (KgCoOp) to improve the generalization ability of prompt tuning methods to unseen classes. 

2. The key idea is to minimize the discrepancy between the learnable prompt and the original hand-crafted prompt from CLIP, so that the essential general knowledge in CLIP can be retained. This helps improve performance on unseen classes.

3. Extensive experiments show that KgCoOp achieves better accuracy on unseen classes compared to prior prompt tuning methods like CoOp, CoCoOp and ProGrad, while maintaining competitiveness on seen classes. 

4. KgCoOp is also shown to be efficient, achieving better performance with similar or less training time compared to other methods.

In summary, the paper proposes a simple yet effective way to enhance prompt tuning through preserving general knowledge from the pretrained model. This improves the model's ability to generalize to novel unseen classes. The effectiveness of KgCoOp is demonstrated through comprehensive experiments.
