# [Visual-Language Prompt Tuning with Knowledge-guided Context Optimization](https://arxiv.org/abs/2303.13283)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading, the key points of this paper are:

- The paper proposes a new method called Knowledge-guided Context Optimization (KgCoOp) for visual-language prompt tuning. 

- The goal is to enhance the generalization ability of learnable prompts for unseen classes. Existing methods like CoOp tend to overfit on seen classes and do not generalize well. 

- The key idea in KgCoOp is to minimize the discrepancy between the learnable prompt and handcrafted prompt from CLIP. This helps retain essential general knowledge and improves unseen class accuracy.

- Experiments show KgCoOp achieves better performance on unseen classes compared to CoOp, CoCoOp and ProGrad. It also obtains higher overall accuracy with less training time.

So in summary, the central hypothesis is that reducing the gap between the learnable prompt and original CLIP prompt can improve generalization to unseen classes in prompt tuning. The paper proposes and validates the KgCoOp method to achieve this goal.
