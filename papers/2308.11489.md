# [Learning from Semantic Alignment between Unpaired Multiviews for   Egocentric Video Recognition](https://arxiv.org/abs/2308.11489)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading, the key research focus of this paper is how to effectively learn representations for egocentric videos by leveraging unpaired third-person videos as additional knowledge sources. 

Specifically, this paper studies the problem of aligning and transferring knowledge from unpaired third-person videos to improve first-person egocentric video understanding, under the challenging setting where the first-person and third-person videos only exhibit partial semantic similarity rather than being fully aligned. 

The central hypothesis is that by building cross-view pseudo-pairs between first-person and unpaired third-person videos in a semantically-aware manner and aligning their representations, the third-person videos can provide useful cues to learn better first-person video representations, even if the cross-view videos are not fully semantically aligned.

To test this hypothesis, the paper proposes a Semantics-based Unpaired Multiview Learning (SUM-L) framework, which constructs pseudo-pairs between first-person and unpaired third-person videos based on semantic similarity, aligns representations of high similarity pairs, and additionally aligns videos with textual descriptions. Experiments on egocentric video datasets verify the effectiveness of the proposed approach.

In summary, the key research question is how to transfer knowledge from unpaired and partially semantically similar third-person videos to improve first-person egocentric video understanding, which is addressed in this paper through the proposed SUM-L framework.


## What is the main contribution of this paper?

 Based on my reading of the paper, the main contributions appear to be:

- The paper studies a new problem: how to leverage unpaired third-person videos to help first-person video learning, in the setting where the first-person and third-person videos only share partially similar semantics. This is a challenging and less explored setting compared to typical paired or unpaired multimodal/multiview learning.

- The paper proposes a method called Semantics-based Unpaired Multiview Learning (SUM-L) to address this problem. The key ideas are: (1) Build cross-view pseudo-pairs and align those with high semantic similarity in a semantics-aware manner. (2) Perform video-text alignment to allow all first-person videos to obtain knowledge from different views/modalities. 

- The paper validates the effectiveness of SUM-L through experiments on benchmark egocentric video datasets like Charades-Ego, EPIC-Kitchens, and EPIC-Kitchens-100. The results show SUM-L outperforms existing view alignment methods like contrastive learning and triplet loss, demonstrating its strengths in the proposed unpaired multiview setting.

In summary, the main contribution appears to be proposing and validating a new semantics-based method to effectively learn from unpaired and partially semantically similar multiview videos, a challenging but practical scenario in multiview representation learning. The experiments demonstrate its effectiveness over existing approaches.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the key points in the paper:

The paper proposes a semantics-based unpaired multiview learning method called SUM-L that aligns pseudo-pairs of first-person and third-person videos in a semantics-aware manner to improve first-person video representation learning, without needing paired multiview videos or extra supervision.


## How does this paper compare to other research in the same field?

 Based on my understanding, here are some key comparisons of this paper to other related works:

- This paper focuses on an under-explored problem - learning from unpaired egocentric (first-person) and third-person videos with only partially similar semantics. Most prior works assume paired multiview data with identical semantics, or unpaired data with completely identical semantics. This is a more challenging and realistic scenario.

- The proposed method SUM-L aligns unpaired multiview videos in a semantics-aware manner, by using language model encoded text to measure semantic similarity and weigh cross-view alignment losses. This differs from typical contrastive or triplet loss view alignment methods that naively minimize feature distances.

- Experiments show SUM-L outperforms typical view alignment baselines like contrastive learning, and improves over "first-person only" training, validating its ability to extract useful knowledge from unpaired third-person videos.

- Compared to Ego-Exo that also transfers third-person knowledge, SUM-L does not rely on extra supervision from image recognition or object detection models. The textual semantic guidance seems more flexible.

- For data efficiency, SUM-L additionally does video-text alignment using narration texts, allowing all videos to get aligned despite partial cross-view semantic similarity. This differs from prior unpaired multimodal methods.

In summary, the key novelty is in tackling the new problem of learning from unpaired videos with partial semantic similarity, and using semantic text guidance for more effective view alignment, instead of standard contrastive/triplet losses. The experiments validate these design choices.


## What future research directions do the authors suggest?

 The authors suggest a few potential future research directions:

- Explore more unpaired third-person video datasets: The improvements from SUM-L are not very significant, possibly due to the relatively small amount of third-person data used. The authors suggest exploring more diverse and larger-scale third-person video datasets.

- Study more downstream tasks: The paper focuses on the task of egocentric video classification. The authors suggest exploring other downstream tasks like unpaired cross-view retrieval to further validate the learned representations. 

- Explore different pseudo-pair mining strategies: The current pseudo-pair mining is based on semantic similarity of narrations. Other mining strategies could be explored, such as based on visual similarity.

- Extend to other partially aligned multiview scenarios: The proposed ideas could be extended to other multiview learning scenarios where views are only partially aligned semantically.

- Improve alignment for low similarity pairs: The current method cannot align first-person videos well if their pseudo-pairs have low semantic similarity. Better alignment techniques for these low similarity pairs could be developed.

- Leverage more multimodal knowledge: The paper uses video-text alignment. More multimodal signals like audio could be incorporated to further improve representation learning.

- Study social implications: The use of third-person human data to improve egocentric models has potential social impacts that could be studied.

In summary, the main future directions are around scaling up the datasets, tasks and modalities, improving pseudo-pair mining and alignment, and studying social impacts. Exploring these could help further advance representation learning for unpaired and partially aligned multiview videos.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the key points in the paper:

The paper proposes a new method called Semantics-based Unpaired Multiview Learning (SUM-L) to address the challenging problem of learning from unpaired first-person and third-person videos that only have partially similar semantics. The key idea is to construct pseudo-pairs between first- and third-person videos based on semantic similarity of their textual descriptions, and then align the features of high semantic similarity pairs in a semantics-aware manner using contrastive learning. To improve data efficiency, they also align videos with their textual descriptions. Experiments on egocentric video datasets Charades-Ego, EPIC-Kitchens and EPIC-Kitchens-100 show their method outperforms typical contrastive and triplet loss view alignment techniques by leveraging semantic information to selectively align informative pseudo-pairs. The method provides a way to transfer knowledge from easily obtained third-person video datasets to improve first-person video understanding without needing fully paired training data.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the paper:

This paper proposes a new method called Semantics-based Unpaired Multiview Learning (SUM-L) for learning from unpaired first-person (egocentric) and third-person videos that only have partially similar semantics. Most prior work assumed paired multiview videos with identical semantics. The key ideas are: (1) Build cross-view pseudo-pairs between first- and third-person videos based on semantic similarity of textual narrations encoded by a language model. (2) Align pseudo-pairs in a semantics-aware manner, focusing on those with higher similarity. (3) Perform video-text alignment using narrations to improve data efficiency. Extensive experiments on Charades-Ego, EPIC-Kitchens and EPIC-Kitchens-100 show SUM-L outperforms typical contrastive and triplet loss methods for multiview alignment. It brings consistent improvement over "FPV Only" baselines by leveraging unpaired third-person videos without extra supervision.

In more detail, the framework first constructs pseudo-pairs between first- and third-person videos using cosine similarity between encoded textual narrations. It extracts features from different view encoders and applies semantics-based cross-view alignment on higher similarity pairs. Video-text alignment with narrations also enhances feature learning. The combined losses are used to train the model end-to-end. During testing, only the first-person encoder and head are used for egocentric video classification. The key advantage of SUM-L is effectively utilizing unpaired third-person videos to improve first-person representations in the challenging partial semantics matching scenario, without needing additional supervisory signals. Experiments demonstrate SUM-L consistently outperforms prior multiview alignment techniques for this setting.


## Summarize the main method used in the paper in one paragraph.

 The paper proposes Semantics-based Unpaired Multiview Learning (SUM-L), a method for leveraging semantically similar but unpaired third-person videos to improve first-person video recognition. The key ideas are:

1) Build cross-view pseudo-pairs between first-person and third-person videos based on semantic similarity of video captions encoded by a language model. 

2) Align features of high semantic similarity pseudo-pairs using contrastive loss to learn view-invariant representations. This avoids aligning low semantic similarity pairs which would hurt learning.

3) Perform video-text alignment between all videos and captions using contrastive loss. This allows all first-person videos to obtain knowledge from different views/modalities.

4) Train task-specific classifiers on aligned first-person features to perform video recognition. The method does not require paired first- and third-person videos or extra labeled data beyond captions. Experiments on Charades-Ego, EPIC-Kitchens and EPIC-Kitchens-100 show improvements over baseline methods.


## What problem or question is the paper addressing?

 Based on my reading, the key points about the problem this paper is addressing are:

- The paper focuses on unpaired multiview video learning, specifically learning from unpaired first-person (egocentric) and third-person videos. 

- Most prior work has focused on paired multiview learning, but paired first-person and third-person videos are difficult to collect. The authors argue unpaired learning is an interesting but under-explored problem.

- The key challenge is that unpaired first-person and third-person videos only share partial semantic similarity (e.g. same verb or object), unlike typical multiview learning where views capture identical semantics. 

- The authors propose learning view-invariant representations by aligning unpaired video pseudo-pairs in a semantics-aware manner. Their method utilizes video-text alignment as well to improve data efficiency.

- The paper aims to show their method is more effective for this problem than typical contrastive or triplet loss view alignment techniques, as those naively minimize feature distance between unpaired views which may not share full semantics.

In summary, the main problem is learning effective first-person video representations from unpaired third-person videos that only share partial semantics, which is a challenging and underexplored scenario compared to typical paired or unpaired multimodal/multiview learning. The paper proposes a semantics-aware pseudo-pair alignment approach to address this problem.
