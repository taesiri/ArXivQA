# [Learning from Semantic Alignment between Unpaired Multiviews for   Egocentric Video Recognition](https://arxiv.org/abs/2308.11489)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the key research focus of this paper is how to effectively learn representations for egocentric videos by leveraging unpaired third-person videos as additional knowledge sources. Specifically, this paper studies the problem of aligning and transferring knowledge from unpaired third-person videos to improve first-person egocentric video understanding, under the challenging setting where the first-person and third-person videos only exhibit partial semantic similarity rather than being fully aligned. The central hypothesis is that by building cross-view pseudo-pairs between first-person and unpaired third-person videos in a semantically-aware manner and aligning their representations, the third-person videos can provide useful cues to learn better first-person video representations, even if the cross-view videos are not fully semantically aligned.To test this hypothesis, the paper proposes a Semantics-based Unpaired Multiview Learning (SUM-L) framework, which constructs pseudo-pairs between first-person and unpaired third-person videos based on semantic similarity, aligns representations of high similarity pairs, and additionally aligns videos with textual descriptions. Experiments on egocentric video datasets verify the effectiveness of the proposed approach.In summary, the key research question is how to transfer knowledge from unpaired and partially semantically similar third-person videos to improve first-person egocentric video understanding, which is addressed in this paper through the proposed SUM-L framework.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contributions appear to be:- The paper studies a new problem: how to leverage unpaired third-person videos to help first-person video learning, in the setting where the first-person and third-person videos only share partially similar semantics. This is a challenging and less explored setting compared to typical paired or unpaired multimodal/multiview learning.- The paper proposes a method called Semantics-based Unpaired Multiview Learning (SUM-L) to address this problem. The key ideas are: (1) Build cross-view pseudo-pairs and align those with high semantic similarity in a semantics-aware manner. (2) Perform video-text alignment to allow all first-person videos to obtain knowledge from different views/modalities. - The paper validates the effectiveness of SUM-L through experiments on benchmark egocentric video datasets like Charades-Ego, EPIC-Kitchens, and EPIC-Kitchens-100. The results show SUM-L outperforms existing view alignment methods like contrastive learning and triplet loss, demonstrating its strengths in the proposed unpaired multiview setting.In summary, the main contribution appears to be proposing and validating a new semantics-based method to effectively learn from unpaired and partially semantically similar multiview videos, a challenging but practical scenario in multiview representation learning. The experiments demonstrate its effectiveness over existing approaches.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points in the paper:The paper proposes a semantics-based unpaired multiview learning method called SUM-L that aligns pseudo-pairs of first-person and third-person videos in a semantics-aware manner to improve first-person video representation learning, without needing paired multiview videos or extra supervision.


## How does this paper compare to other research in the same field?

Based on my understanding, here are some key comparisons of this paper to other related works:- This paper focuses on an under-explored problem - learning from unpaired egocentric (first-person) and third-person videos with only partially similar semantics. Most prior works assume paired multiview data with identical semantics, or unpaired data with completely identical semantics. This is a more challenging and realistic scenario.- The proposed method SUM-L aligns unpaired multiview videos in a semantics-aware manner, by using language model encoded text to measure semantic similarity and weigh cross-view alignment losses. This differs from typical contrastive or triplet loss view alignment methods that naively minimize feature distances.- Experiments show SUM-L outperforms typical view alignment baselines like contrastive learning, and improves over "first-person only" training, validating its ability to extract useful knowledge from unpaired third-person videos.- Compared to Ego-Exo that also transfers third-person knowledge, SUM-L does not rely on extra supervision from image recognition or object detection models. The textual semantic guidance seems more flexible.- For data efficiency, SUM-L additionally does video-text alignment using narration texts, allowing all videos to get aligned despite partial cross-view semantic similarity. This differs from prior unpaired multimodal methods.In summary, the key novelty is in tackling the new problem of learning from unpaired videos with partial semantic similarity, and using semantic text guidance for more effective view alignment, instead of standard contrastive/triplet losses. The experiments validate these design choices.
