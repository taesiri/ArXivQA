# [Learning from Semantic Alignment between Unpaired Multiviews for   Egocentric Video Recognition](https://arxiv.org/abs/2308.11489)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the key research focus of this paper is how to effectively learn representations for egocentric videos by leveraging unpaired third-person videos as additional knowledge sources. Specifically, this paper studies the problem of aligning and transferring knowledge from unpaired third-person videos to improve first-person egocentric video understanding, under the challenging setting where the first-person and third-person videos only exhibit partial semantic similarity rather than being fully aligned. The central hypothesis is that by building cross-view pseudo-pairs between first-person and unpaired third-person videos in a semantically-aware manner and aligning their representations, the third-person videos can provide useful cues to learn better first-person video representations, even if the cross-view videos are not fully semantically aligned.To test this hypothesis, the paper proposes a Semantics-based Unpaired Multiview Learning (SUM-L) framework, which constructs pseudo-pairs between first-person and unpaired third-person videos based on semantic similarity, aligns representations of high similarity pairs, and additionally aligns videos with textual descriptions. Experiments on egocentric video datasets verify the effectiveness of the proposed approach.In summary, the key research question is how to transfer knowledge from unpaired and partially semantically similar third-person videos to improve first-person egocentric video understanding, which is addressed in this paper through the proposed SUM-L framework.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contributions appear to be:- The paper studies a new problem: how to leverage unpaired third-person videos to help first-person video learning, in the setting where the first-person and third-person videos only share partially similar semantics. This is a challenging and less explored setting compared to typical paired or unpaired multimodal/multiview learning.- The paper proposes a method called Semantics-based Unpaired Multiview Learning (SUM-L) to address this problem. The key ideas are: (1) Build cross-view pseudo-pairs and align those with high semantic similarity in a semantics-aware manner. (2) Perform video-text alignment to allow all first-person videos to obtain knowledge from different views/modalities. - The paper validates the effectiveness of SUM-L through experiments on benchmark egocentric video datasets like Charades-Ego, EPIC-Kitchens, and EPIC-Kitchens-100. The results show SUM-L outperforms existing view alignment methods like contrastive learning and triplet loss, demonstrating its strengths in the proposed unpaired multiview setting.In summary, the main contribution appears to be proposing and validating a new semantics-based method to effectively learn from unpaired and partially semantically similar multiview videos, a challenging but practical scenario in multiview representation learning. The experiments demonstrate its effectiveness over existing approaches.
