# [Learning from Semantic Alignment between Unpaired Multiviews for   Egocentric Video Recognition](https://arxiv.org/abs/2308.11489)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the key research focus of this paper is how to effectively learn representations for egocentric videos by leveraging unpaired third-person videos as additional knowledge sources. Specifically, this paper studies the problem of aligning and transferring knowledge from unpaired third-person videos to improve first-person egocentric video understanding, under the challenging setting where the first-person and third-person videos only exhibit partial semantic similarity rather than being fully aligned. The central hypothesis is that by building cross-view pseudo-pairs between first-person and unpaired third-person videos in a semantically-aware manner and aligning their representations, the third-person videos can provide useful cues to learn better first-person video representations, even if the cross-view videos are not fully semantically aligned.To test this hypothesis, the paper proposes a Semantics-based Unpaired Multiview Learning (SUM-L) framework, which constructs pseudo-pairs between first-person and unpaired third-person videos based on semantic similarity, aligns representations of high similarity pairs, and additionally aligns videos with textual descriptions. Experiments on egocentric video datasets verify the effectiveness of the proposed approach.In summary, the key research question is how to transfer knowledge from unpaired and partially semantically similar third-person videos to improve first-person egocentric video understanding, which is addressed in this paper through the proposed SUM-L framework.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contributions appear to be:- The paper studies a new problem: how to leverage unpaired third-person videos to help first-person video learning, in the setting where the first-person and third-person videos only share partially similar semantics. This is a challenging and less explored setting compared to typical paired or unpaired multimodal/multiview learning.- The paper proposes a method called Semantics-based Unpaired Multiview Learning (SUM-L) to address this problem. The key ideas are: (1) Build cross-view pseudo-pairs and align those with high semantic similarity in a semantics-aware manner. (2) Perform video-text alignment to allow all first-person videos to obtain knowledge from different views/modalities. - The paper validates the effectiveness of SUM-L through experiments on benchmark egocentric video datasets like Charades-Ego, EPIC-Kitchens, and EPIC-Kitchens-100. The results show SUM-L outperforms existing view alignment methods like contrastive learning and triplet loss, demonstrating its strengths in the proposed unpaired multiview setting.In summary, the main contribution appears to be proposing and validating a new semantics-based method to effectively learn from unpaired and partially semantically similar multiview videos, a challenging but practical scenario in multiview representation learning. The experiments demonstrate its effectiveness over existing approaches.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points in the paper:The paper proposes a semantics-based unpaired multiview learning method called SUM-L that aligns pseudo-pairs of first-person and third-person videos in a semantics-aware manner to improve first-person video representation learning, without needing paired multiview videos or extra supervision.


## How does this paper compare to other research in the same field?

Based on my understanding, here are some key comparisons of this paper to other related works:- This paper focuses on an under-explored problem - learning from unpaired egocentric (first-person) and third-person videos with only partially similar semantics. Most prior works assume paired multiview data with identical semantics, or unpaired data with completely identical semantics. This is a more challenging and realistic scenario.- The proposed method SUM-L aligns unpaired multiview videos in a semantics-aware manner, by using language model encoded text to measure semantic similarity and weigh cross-view alignment losses. This differs from typical contrastive or triplet loss view alignment methods that naively minimize feature distances.- Experiments show SUM-L outperforms typical view alignment baselines like contrastive learning, and improves over "first-person only" training, validating its ability to extract useful knowledge from unpaired third-person videos.- Compared to Ego-Exo that also transfers third-person knowledge, SUM-L does not rely on extra supervision from image recognition or object detection models. The textual semantic guidance seems more flexible.- For data efficiency, SUM-L additionally does video-text alignment using narration texts, allowing all videos to get aligned despite partial cross-view semantic similarity. This differs from prior unpaired multimodal methods.In summary, the key novelty is in tackling the new problem of learning from unpaired videos with partial semantic similarity, and using semantic text guidance for more effective view alignment, instead of standard contrastive/triplet losses. The experiments validate these design choices.


## What future research directions do the authors suggest?

The authors suggest a few potential future research directions:- Explore more unpaired third-person video datasets: The improvements from SUM-L are not very significant, possibly due to the relatively small amount of third-person data used. The authors suggest exploring more diverse and larger-scale third-person video datasets.- Study more downstream tasks: The paper focuses on the task of egocentric video classification. The authors suggest exploring other downstream tasks like unpaired cross-view retrieval to further validate the learned representations. - Explore different pseudo-pair mining strategies: The current pseudo-pair mining is based on semantic similarity of narrations. Other mining strategies could be explored, such as based on visual similarity.- Extend to other partially aligned multiview scenarios: The proposed ideas could be extended to other multiview learning scenarios where views are only partially aligned semantically.- Improve alignment for low similarity pairs: The current method cannot align first-person videos well if their pseudo-pairs have low semantic similarity. Better alignment techniques for these low similarity pairs could be developed.- Leverage more multimodal knowledge: The paper uses video-text alignment. More multimodal signals like audio could be incorporated to further improve representation learning.- Study social implications: The use of third-person human data to improve egocentric models has potential social impacts that could be studied.In summary, the main future directions are around scaling up the datasets, tasks and modalities, improving pseudo-pair mining and alignment, and studying social impacts. Exploring these could help further advance representation learning for unpaired and partially aligned multiview videos.


## Summarize the paper in one paragraph.

Here is a one paragraph summary of the key points in the paper:The paper proposes a new method called Semantics-based Unpaired Multiview Learning (SUM-L) to address the challenging problem of learning from unpaired first-person and third-person videos that only have partially similar semantics. The key idea is to construct pseudo-pairs between first- and third-person videos based on semantic similarity of their textual descriptions, and then align the features of high semantic similarity pairs in a semantics-aware manner using contrastive learning. To improve data efficiency, they also align videos with their textual descriptions. Experiments on egocentric video datasets Charades-Ego, EPIC-Kitchens and EPIC-Kitchens-100 show their method outperforms typical contrastive and triplet loss view alignment techniques by leveraging semantic information to selectively align informative pseudo-pairs. The method provides a way to transfer knowledge from easily obtained third-person video datasets to improve first-person video understanding without needing fully paired training data.
