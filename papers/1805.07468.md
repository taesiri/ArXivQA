# Unsupervised Learning of Neural Networks to Explain Neural Networks

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be: How can we automatically disentangle human-interpretable features (e.g. object parts) from chaotic middle-layer feature maps of a pre-trained convolutional neural network (CNN), without using additional human supervision?The key goal is to improve the interpretability of the pre-trained CNN features/representations, by learning an additional "explainer" network that disentangles the object parts. This is done in an unsupervised manner, without extra annotations for parts or textures.The explainer network acts like an autoencoder - first disentangling the chaotic feature maps of the CNN into interpretable object-part features, and then reconstructing/inverting those part features back into the original CNN features.So in summary, the central research question is how to learn an explainer network in an unsupervised way, to disentangle and explain the chaotic middle-layer convolutional feature maps of a CNN by decomposing them into interpretable object parts. This improves the interpretability of the original CNN without affecting its discrimination power.


## What is the main contribution of this paper?

The main contribution of this paper is presenting an unsupervised method to learn an explainer neural network that can interpret and explain the representations learned by a pre-trained convolutional neural network (CNN). Specifically:- They propose learning an additional "explainer" network that can disentangle the chaotic feature maps of a middle layer in a pre-trained "performer" CNN into more interpretable object-part features. - The explainer performs like an auto-encoder, with an encoder that disentangles the performer's features into object-part features, and a decoder that reconstructs the original performer features from the disentangled representation.- They introduce a filter loss function that encourages each filter in the explainer's interpretable layers to exclusively represent a specific object part, without needing any part annotations for supervision.- The explainer is trained by distilling knowledge from the pre-trained performer network, using reconstruction losses to retain information and a filter loss for interpretability, without affecting the performer's discriminative power.- Experiments on several CNN architectures show the explainer can disentangle chaotic feature maps into more semantically interpretable object-part features, providing insight into the performer network's representations.In summary, the key contribution is presenting an unsupervised method to learn an interpretable explainer network that reveals the object-part knowledge encoded in a pre-trained CNN performer model. The explainer approach does not affect the original model's capabilities but makes its representations more understandable.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper presents an unsupervised method to learn an explainer neural network that disentangles and interprets the feature representations encoded in a pre-trained convolutional neural network performer model, without the need for additional labeled data.
