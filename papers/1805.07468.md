# Unsupervised Learning of Neural Networks to Explain Neural Networks

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be: How can we automatically disentangle human-interpretable features (e.g. object parts) from chaotic middle-layer feature maps of a pre-trained convolutional neural network (CNN), without using additional human supervision?The key goal is to improve the interpretability of the pre-trained CNN features/representations, by learning an additional "explainer" network that disentangles the object parts. This is done in an unsupervised manner, without extra annotations for parts or textures.The explainer network acts like an autoencoder - first disentangling the chaotic feature maps of the CNN into interpretable object-part features, and then reconstructing/inverting those part features back into the original CNN features.So in summary, the central research question is how to learn an explainer network in an unsupervised way, to disentangle and explain the chaotic middle-layer convolutional feature maps of a CNN by decomposing them into interpretable object parts. This improves the interpretability of the original CNN without affecting its discrimination power.


## What is the main contribution of this paper?

The main contribution of this paper is presenting an unsupervised method to learn an explainer neural network that can interpret and explain the representations learned by a pre-trained convolutional neural network (CNN). Specifically:- They propose learning an additional "explainer" network that can disentangle the chaotic feature maps of a middle layer in a pre-trained "performer" CNN into more interpretable object-part features. - The explainer performs like an auto-encoder, with an encoder that disentangles the performer's features into object-part features, and a decoder that reconstructs the original performer features from the disentangled representation.- They introduce a filter loss function that encourages each filter in the explainer's interpretable layers to exclusively represent a specific object part, without needing any part annotations for supervision.- The explainer is trained by distilling knowledge from the pre-trained performer network, using reconstruction losses to retain information and a filter loss for interpretability, without affecting the performer's discriminative power.- Experiments on several CNN architectures show the explainer can disentangle chaotic feature maps into more semantically interpretable object-part features, providing insight into the performer network's representations.In summary, the key contribution is presenting an unsupervised method to learn an interpretable explainer network that reveals the object-part knowledge encoded in a pre-trained CNN performer model. The explainer approach does not affect the original model's capabilities but makes its representations more understandable.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper presents an unsupervised method to learn an explainer neural network that disentangles and interprets the feature representations encoded in a pre-trained convolutional neural network performer model, without the need for additional labeled data.


## How does this paper compare to other research in the same field?

Here are some key ways this paper on unsupervised learning of neural networks compares to other related work:- It tackles a novel problem of learning an "explainer" network to interpret the representations learned by a pretrained "performer" network. This is a different goal than most interpretability methods that focus on visualizing or diagnosing representations in existing networks.- The explainer takes a disentanglement approach to make representations more interpretable by separating object part features. This is similar to other work on disentangled representations but applied in a novel way for post hoc interpretability.- The explainer is learned without extra supervision on parts or textures. Most related interpretability methods require some form of supervision like labels or part annotations. Learning the explainer in an unsupervised manner expands the applicability.- The explainer is kept separate from the performer so it does not affect the original discrimination ability. This avoids the common tradeoff between accuracy and interpretability that other methods directly optimizing for interpretability in the model can face.- The explainer-performer framework is general and could support different interpretability objectives or losses. Most related work focuses on a specific interpretation approach. The flexibility of this framework is a distinction.- It evaluates interpretability both qualitatively through visualizations and quantitatively based on part localization stability. Many interpretability methods rely on qualitative human evaluation rather than quantitative measures.Overall, this work introduces a new paradigm for post hoc interpretability that imposes an explanatory model on an existing performer model. The unsupervised learning and disentanglement approach as well as the general framework differentiate it from prior interpretability research.
