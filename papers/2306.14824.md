# Kosmos-2: Grounding Multimodal Large Language Models to the World

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is how to endow multimodal large language models (MLLMs) with grounding capabilities to enable new ways of perceiving and interacting with visual content. Specifically, the paper aims to enable MLLMs to perceive spatial descriptions of objects (like bounding boxes) and ground text to the visual world. The key hypothesis is that by representing object descriptions as sequences of location tokens and training the model on a large corpus of grounded image-text pairs, the model can learn to associate text spans with spatial regions and generate grounded outputs. This grounding capability can unlock new applications like referring expression comprehension/generation and grounded question answering.In summary, the central research question is how to augment MLLMs with visual grounding abilities through a combination of grounded multimodal data and representing object regions as discrete tokens. The hypothesis is that this approach will enable more advanced vision-language understanding and generation compared to prior MLLMs.
