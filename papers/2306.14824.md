# Kosmos-2: Grounding Multimodal Large Language Models to the World

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is how to endow multimodal large language models (MLLMs) with grounding capabilities to enable new ways of perceiving and interacting with visual content. Specifically, the paper aims to enable MLLMs to perceive spatial descriptions of objects (like bounding boxes) and ground text to the visual world. The key hypothesis is that by representing object descriptions as sequences of location tokens and training the model on a large corpus of grounded image-text pairs, the model can learn to associate text spans with spatial regions and generate grounded outputs. This grounding capability can unlock new applications like referring expression comprehension/generation and grounded question answering.In summary, the central research question is how to augment MLLMs with visual grounding abilities through a combination of grounded multimodal data and representing object regions as discrete tokens. The hypothesis is that this approach will enable more advanced vision-language understanding and generation compared to prior MLLMs.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contribution is developing Kosmos-2, a multimodal large language model with new capabilities for grounding text to the visual world. Specifically:- They construct a large-scale dataset called GrIT (Grounded Image-Text) by extracting noun phrases and referring expressions from image captions and linking them to bounding boxes in the image. This enables training the model to ground language. - They develop Kosmos-2, an extension of the Kosmos multimodal LLM, by training on GrIT data as well as other multimodal corpora. This endows the model with grounding capabilities.- They demonstrate new capabilities of Kosmos-2 for multimodal grounding (phrase grounding, referring expression comprehension) and referring (referring expression generation). The model can generate bounding boxes from text and vice versa.- They show Kosmos-2 achieves strong performance on not only grounding tasks but also language and vision-language tasks evaluated on the previous Kosmos model.- They discuss how grounding lays a foundation for developing embodied AI systems that can perceive and interact with the visual world based on language.In summary, the core contribution is developing a multimodal LLM with new grounding capabilities by constructing a grounded multimodal dataset and training an enhanced model on this data. This work moves towards language models that can perceive and ground themselves in the visual world.
