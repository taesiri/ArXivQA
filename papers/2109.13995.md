# [IGLU: Efficient GCN Training via Lazy Updates](https://arxiv.org/abs/2109.13995)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is:How can we efficiently train multi-layer Graph Convolutional Networks (GCNs) at scale?The key hypothesis is that caching intermediate computations in GCNs and using "lazy updates" can significantly reduce the computational cost of training while still providing good convergence guarantees. Specifically, the paper proposes a method called IGLU that caches intermediate embeddings or gradients in GCNs and refreshes them periodically rather than after every parameter update. This avoids repeatedly recomputing embeddings/gradients over large neighborhoods in the graph, which is the main computational bottleneck in standard GCN training. The hypotheses are:1) Caching embeddings/gradients and using them for multiple parameter updates will significantly reduce GCN training time compared to methods that recompute embeddings after every update.2) Despite using stale cached values, IGLU can still provably converge to a first-order stationary point of the loss under standard assumptions like smoothness.3) IGLU can match or exceed the accuracy of current state-of-the-art methods, despite using stale caches, by virtue of avoiding sampling noise and variance.The paper presents theoretical analysis and extensive experiments supporting these hypotheses and demonstrating the effectiveness of the proposed IGLU method.
