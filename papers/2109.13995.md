# [IGLU: Efficient GCN Training via Lazy Updates](https://arxiv.org/abs/2109.13995)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is:How can we efficiently train multi-layer Graph Convolutional Networks (GCNs) at scale?The key hypothesis is that caching intermediate computations in GCNs and using "lazy updates" can significantly reduce the computational cost of training while still providing good convergence guarantees. Specifically, the paper proposes a method called IGLU that caches intermediate embeddings or gradients in GCNs and refreshes them periodically rather than after every parameter update. This avoids repeatedly recomputing embeddings/gradients over large neighborhoods in the graph, which is the main computational bottleneck in standard GCN training. The hypotheses are:1) Caching embeddings/gradients and using them for multiple parameter updates will significantly reduce GCN training time compared to methods that recompute embeddings after every update.2) Despite using stale cached values, IGLU can still provably converge to a first-order stationary point of the loss under standard assumptions like smoothness.3) IGLU can match or exceed the accuracy of current state-of-the-art methods, despite using stale caches, by virtue of avoiding sampling noise and variance.The paper presents theoretical analysis and extensive experiments supporting these hypotheses and demonstrating the effectiveness of the proposed IGLU method.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contribution is the proposal of a new method called IGLU for efficiently training Graph Convolutional Networks (GCNs). The key ideas of IGLU are:1. It avoids the neighborhood explosion problem in GCN training by using "lazy updates", where intermediate embeddings or gradients are cached and only refreshed periodically instead of after every minibatch. This allows limiting the computation to only local neighborhoods. 2. Two variants are proposed - caching embeddings (backprop order) or caching incomplete gradients (inverted order). The inverted order is shown empirically to work better.3. IGLU does not perform any sampling for neighborhoods, avoiding sampling bias or variance. The lazily updated cached variables do introduce some bias, but this is provably bounded.4. Convergence guarantees are provided showing that IGLU reaches an approximate first-order stationary point. The convergence rate is comparable to standard SGD for the minibatch case, and even faster for the full batch case.5. Experiments on several graph datasets demonstrate that IGLU provides significant speedups over prior methods, while also improving accuracy in many cases. Up to 1.2% better accuracy is obtained despite requiring up to 88% less compute time.In summary, the main contribution is a novel and efficient method for GCN training that leverages lazy cached updates to avoid neighborhood explosion, provides convergence guarantees, and empirically demonstrates strong improvements over prior state-of-the-art methods.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Based on my review of the paper, here is a one sentence summary:The paper proposes a novel method called IGLU for efficiently training Graph Convolutional Networks by caching intermediate computations to enable "lazy" gradient updates, avoiding the typical neighborhood explosion problem and providing faster convergence with provable guarantees.


## How does this paper compare to other research in the same field?

Based on my reading of the paper, here is a summary of how it compares to other related research:- The paper presents IGLU, a new method for efficiently training graph convolutional networks (GCNs). Compared to prior work on accelerating GCN training, IGLU takes a different approach by introducing "lazy updates" to cached intermediate computations like node embeddings or incomplete gradients. - Most prior methods aim to reduce the neighborhood explosion problem in GCN training by using some form of sampling or clustering to limit the number of nodes receiving updates. This introduces variance and sometimes bias. In contrast, IGLU avoids sampling neighbors entirely and converges provably to a first-order stationary point.- Some recent works like VR-GCN and GNNAutoScale also leverage historical or stale embeddings to prune computations. However, IGLU's caching approach is much more aggressive, with stale variables updated only once per epoch rather than each minibatch. It also analyzes gradient structure to update parameters layer-by-layer unlike standard SGD for GCNs.- Compared to sampling-based methods, IGLU achieves higher accuracy with faster convergence on benchmarks. It also scales well to deeper GCNs unlike methods that suffer from neighborhood explosion. Theoretically, it provides convergence guarantees despite using stale gradients.- Overall, IGLU introduces a novel, sampling-free paradigm for efficient GCN training that seems highly promising compared to prior approaches. The thorough empirical evaluation on various datasets and theoretical analysis help validate its effectiveness and differentiation from existing techniques.In summary, IGLU makes several key contributions to the area of accelerating GCN training through its unique lazy update approach and provides noticeable improvements over state-of-the-art methods. The comparisons highlight its novelty and advantages over prior sampling-based techniques.
