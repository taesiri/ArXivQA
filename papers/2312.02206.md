# [Axiomatic Preference Modeling for Longform Question Answering](https://arxiv.org/abs/2312.02206)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Large language models (LLMs) trained with human preference feedback lack knowledge of the principles behind those preferences. This can lead to misalignment with human expectations.  
- Existing "reward models" simply regress to a preference score without clear axiomatic signals. Training pairs also lack diversity, often just sampled from the LLM itself.  
- It's unclear if separate small preference models can reliably score both human-written and LLM-generated answers. This is challenging due to differences in style.

Proposed Solution:  
- Identify 5 principles for long-form QA based on usefulness, relevance, truthfulness etc.
- Develop axiomatic framework to construct diverse preference training pairs tailored to those principles.
- Train 220M parameter "preference models" on axiomatic signals to score answers, normalizing stylistic differences.

Key Contributions:
- Axiomatic framework to generate nuanced preference training data enforcing principles. Applicable beyond QA.
- Standalone preference models to score human and LLM answers reliably on same scale.  
- With proper axiomatic signals, 220M model exceeds GPT-4 in preference scoring despite size difference.

In summary, the key ideas are using principles to actively construct informative training data for preference models, rather than just passively sampling scores. This helps small models outperform larger ones for the specialized task of preference scoring.
