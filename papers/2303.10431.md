# [DeAR: Debiasing Vision-Language Models with Additive Residuals](https://arxiv.org/abs/2303.10431)

## What is the central research question or hypothesis that this paper addresses?

 The key research question addressed in this paper is how to debias large pre-trained vision-language models (VLMs) like CLIP to mitigate societal biases related to gender, race, and age. Specifically, the paper presents a new method called DeAR (Debiasing with Additive Residuals) to modify the visual representations from VLMs to make them fairer with respect to different identity groups. The key hypothesis is that by learning an additive residual representation that removes information related to protected attributes like gender, race, and age, the associations between images and text can be made more equitable.

The paper proposes that societal biases manifest in VLMs as skewed similarities between text concepts and images of people belonging to certain identity groups. It hypothesizes that the visual representations contain information related to protected attributes that can be disentangled. The DeAR method trains a residual representation that when added to the original image embeddings removes the ability to predict protected attributes. This modified representation retains the predictive power of the original while reducing biased associations. 

The paper validates the hypothesis through quantitative evaluations of the bias skew and similarity rankings before and after applying DeAR to models like CLIP, ALBEF, BLIP. It also introduces a new context-based bias benchmark dataset called PATA to provide more nuanced bias evaluations. The results demonstrate DeAR's ability to significantly improve the fairness of different VLMs while preserving their zero-shot predictive performance on various downstream tasks.

In summary, the central hypothesis is that additive residual learning can debias VLMs by removing protected visual attribute information, which the paper validates through bias and accuracy evaluations. The key novelty is the proposed DeAR technique to modify VLMs and make their representations more equitable.
