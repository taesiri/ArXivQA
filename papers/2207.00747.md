# Rationale-Augmented Ensembles in Language Models

## What is the central research question or hypothesis that this paper addresses?

Based on my review, the central research question this paper addresses is:How can rationale-augmented prompting be made more robust and reliable for few-shot in-context learning in language models?Specifically, the paper investigates:1) Why rationales sometimes hurt task performance when used in few-shot prompting. 2) How to develop a framework of rationale-augmented ensembles to overcome the brittleness and reliably improve performance across a variety of NLP tasks.The key ideas and findings include:- Simply adding rationales can sometimes degrade task performance in few-shot learning due to sub-optimality of the rationales used.- The proposed rationale-augmented ensemble framework identifies rationale sampling in the output space as a key component to improve robustness.- Rationale-augmented ensembles reliably outperform standard prompting and rationale-based prompting approaches across many NLP tasks, while also providing rationales to improve interpretability.- The framework can be extended to common NLP tasks like QA, WSD, sentiment analysis etc. where explicit intermediate steps may not be needed traditionally.So in summary, the central research question is how to make rationale-augmented prompting more robust for few-shot in-context learning, which is addressed via the proposed rationale-augmented ensemble framework.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contributions seem to be:1. Investigating the role of rationales in few-shot in-context learning by prompting language models with (input, rationale -> output) exemplars instead of just (input -> output). 2. Identifying the sub-optimality of manually provided rationales as a key reason why simply adding rationales can sometimes hurt task performance in few-shot learning.3. Proposing the framework of rationale-augmented ensembles to overcome the brittleness of manually provided rationales. This framework introduces randomness over rationales either in the input space (e.g. prompt order, generated rationales) or the output space (sampling outputs) to create an ensemble. 4. Demonstrating that rationale sampling in the output space is the most critical component for improving task performance with rationale-augmented ensembles. The framework reliably outperforms standard prompting and rationale-based prompting across a variety of NLP tasks.5. Showing the framework can be applied even for tasks that don't involve explicit reasoning steps (e.g. QA, WSD, sentiment analysis), and can provide free rationales to interpret model predictions.In summary, the main contribution seems to be proposing and analyzing rationale-augmented ensembles as a general approach to make few-shot in-context learning with rationales more accurate and interpretable across NLP tasks.
