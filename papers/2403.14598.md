# [PSALM: Pixelwise SegmentAtion with Large Multi-Modal Model](https://arxiv.org/abs/2403.14598)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Large multimodal models (LMMs) like LLaMA have shown great success in language and vision tasks with text outputs. However, they are limited in addressing pixel-level vision tasks like image segmentation due to only producing text tokens as output. 

- Image segmentation is challenging to unify into a single framework due to the variety of tasks and inputs/outputs involved. Tasks like semantic segmentation, instance segmentation, interactive segmentation, etc. have diverse requirements.

Proposed Solution: 
- The paper proposes PSALM, which extends LMMs to segmentation by adding a mask decoder and designing a flexible input schema. 

- The input schema feeds the LMM with images, task instructions, conditional prompts, and mask tokens. This provides the necessary information and allows mask prediction.

- Conditional prompts provide extra information like categories or sentences. Their output embeddings are used as classifier weights on mask proposals. Types of prompts include category, sentence, visual-prior conditions.

- A mask generator takes the LMM's mask token outputs and other features to produce mask proposals and predict classes using the condition embeddings.

- Tasks are trained jointly, which is mutually beneficial between them and improves generalization.

Main Contributions:
- Presents the first method to effectively adapt LMMs for semantic/instance image segmentation tasks using mask proposals and condition embeddings.

- Achieves SOTA results on referring segmentation and strong performance on other in-domain tasks like panoptic segmentation. Generalizes well to unseen tasks.

- Unifies diverse segmentation tasks into a single model through a flexible input schema and joint training. Enhances generalization and outperforms task-specific models.

- Opens ability for LMMs to address pixel-level vision problems, moving towards the vision GPT moment seen in NLP. Highlights importance of flexibility, generalization, and multi-task learning.
