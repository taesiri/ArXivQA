# [DeeperGCN: All You Need to Train Deeper GCNs](https://arxiv.org/abs/2006.07739v1)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question is: 

How can we train deeper and more powerful graph convolutional networks (GCNs) to achieve state-of-the-art performance on large-scale graph learning tasks?

Specifically, the paper proposes and evaluates techniques to address key challenges that have limited the depth and performance of GCNs, including:

- Designing better aggregation functions for graph convolutions that are more generalized and flexible.

- Developing more effective residual/skip connections for GCNs. 

- Introducing novel normalization techniques like message normalization to improve training.

The overall goal is to develop a framework called DeeperGCN that can reliably train very deep GCNs to achieve new state-of-the-art results on node and graph property prediction tasks using large-scale graph datasets.

So in summary, the key hypothesis is that by developing better aggregation functions, skip connections, and normalization for GCNs, the authors can overcome prior limitations and train much deeper and more powerful GCN models to advance state-of-the-art performance on large graph learning benchmarks. The experiments aim to demonstrate and validate this hypothesis.


## What is the main contribution of this paper?

 Based on my reading of the abstract and conclusion, the main contributions of this paper appear to be:

1. Proposing a novel Generalized Aggregation Function for graph convolutional networks (GCNs) that is permutation invariant and covers commonly used aggregation functions like mean and max. This function is differentiable and can be learned in an end-to-end fashion.

2. Exploring modified graph skip connections and a new graph normalization layer called MsgNorm to enhance the performance of GCNs on large-scale graphs. 

3. Achieving state-of-the-art results on four datasets from the Open Graph Benchmark (OGB) for node and graph property prediction tasks, improving over previous methods by up to 7.8%. 

In summary, the key contributions are proposing ways to improve GCN performance on large graphs, including a generalized and learnable aggregation function, modified residual connections, and a new normalization technique. When combined together into their proposed DeeperGCN framework, they are able to significantly advance state-of-the-art on benchmark datasets.
