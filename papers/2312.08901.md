# [Boosting LLM Reasoning: Push the Limits of Few-shot Learning with   Reinforced In-Context Pruning](https://arxiv.org/abs/2312.08901)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a high-quality paragraph summarizing the key points of the paper:

This paper proposes a novel method called CoT-Max to improve the mathematical reasoning capabilities of large language models (LLMs) using few-shot learning. The key idea is to leverage the redundancy at both the example and token levels in natural language inputs to compress lengthy chain-of-thought (CoT) reasoning examples to fit within the LLM's context window limit. Specifically, CoT-Max employs a two-stage policy network, including a shot pruner to remove non-useful CoT examples and a token pruner to prune redundant tokens. To train this policy network, the authors collect a diverse math reasoning dataset called MRD3 and propose a reinforcement learning approach with a customized reward function to account for reasoning accuracy and length constraints. Experiments on multiple benchmarks and LLMs demonstrate that CoT-Max significantly boosts LLM math reasoning performance over state-of-the-art prompting methods. Notably, LLaMA2-70B with CoT-Max exceeds the accuracy of much larger models like GPT-3.5, establishing a new prompting-based benchmark without any fine-tuning. The proposed method effectively pushes the limits of few-shot learning for improving LLM reasoning capabilities.
