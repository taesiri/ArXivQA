# [Training Language Model Agents without Modifying Language Models](https://arxiv.org/abs/2402.11359)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
Reframing large language models (LLMs) as agents has enabled them to automate complex tasks using specialized functions. However, creating good functions is challenging - it requires manual effort and iterations. Finetuning the LLM weights to improve function usage is also difficult since many LLMs are inaccessible/proprietary. 

Solution - Agent Training Paradigm:
The paper proposes a new paradigm to train LLM agents without modifying LLM weights. The key ideas are:

(1) Treat functions as "agent parameters" instead of model parameters. 

(2) Draw an analogy between model training and agent training: model parameters ↔ agent functions; loss function ↔ agent’s historical performance.

(3) Use LLM-based "AgentOptimizer" to update agent functions based on execution history, replacing numeric optimizers like SGD.

(4) Update functions progressively via add/modify/remove actions instead of regenerating the whole set.

(5) Use roll-back and early-stop to avoid performance drops during training.

Contributions:

(1) Proposed the agent training paradigm to create specialized LLM agents without finetuning weights.

(2) Developed key components like AgentOptimizer and the training algorithm. 

(3) Showcased significant performance boosts by training GPT-4+ and ReAct agents on mathematical reasoning, tabular processing, general real-world tasks. 

(4) Analyzed agent behavior - learning curves, domain transferability, function complexity.

The paradigm provides an alternative way to build LLM agents that does not require weight tuning access. It iteratively optimizes functions to adapt agents to downstream tasks.
