# [Latent Neural PDE Solver: a reduced-order modelling framework for   partial differential equations](https://arxiv.org/abs/2402.17853)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem Statement
The paper focuses on developing efficient neural network models to simulate time-dependent partial differential equations (PDEs). Solving PDEs numerically often requires very fine spatio-temporal discretization to achieve convergence, making it computationally expensive. Existing neural network surrogates operate on the full discretized field, limiting their efficiency and long-term stability. 

Proposed Solution
The paper proposes a Latent Neural PDE Solver (LNS) framework that comprises of two components:
1) An autoencoder that compresses the fully discretized field into a latent space with much coarser resolution. This is done using convolutional and attention layers.
2) A temporal propagator network that predicts the future states in this lower-dimensional latent space. By operating in latent space, the propagator network training is greatly simplified.

The autoencoder first projects inputs to the latent space. The propagator network is then trained autoregressively to predict future latent states. At inference time, the predicted latent states are decoded to recover the full solution field.

The framework is evaluated on simulated datasets from Navier-Stokes equations for fluid flow, shallow water equations for geophysical flows, and two-phase Navier-Stokes equations for liquid sloshing. Baselines include Fourier Neural Operator, UNet architectures and encode-decode schemes.

Main Contributions
- Proposes a two-stage latent variable model for learning dynamics of PDEs, by decoupling an autoencoder and temporal propagator network.
- Shows computational gains from propagating dynamics in lower-dimensional latent space, enabling longer rollout during training.
- Achieves comparable or better accuracy than baselines that operate on full discretized space, with lower training costs. 
- Demonstrates stable long-term extrapolation on chaotic shallow water equation by leveraging longer rollout, which was intractable for baselines.

In summary, the paper presents a conceptually simple yet effective framework for learning neural PDE solvers that operates on a compressed latent space to achieve efficiency gains. The results showcase improved accuracy and stability across a diverse set of PDEs.
