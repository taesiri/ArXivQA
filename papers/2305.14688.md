# ExpertPrompting: Instructing Large Language Models to be Distinguished   Experts

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central research question this paper addresses is how to improve the quality of responses from large language models like ChatGPT by instructing them to answer questions like distinguished experts. Specifically, the authors propose an "ExpertPrompting" method that first generates a customized expert identity description conditioned on the question, and then instructs the LLM to answer the question while adopting this expert identity. The main hypothesis seems to be that prompting LLMs in this way will elicit higher quality, more expert-like responses compared to vanilla prompting without providing an expert identity description. The authors test this via both automatic and human evaluations, and find that ExpertPrompting does improve answer quality over baseline methods. Developing prompting strategies to unlock the full potential of LLMs is an important area, and this paper provides evidence that explicitly conditioning on an expert identity is a promising approach.


## What is the main contribution of this paper?

The main contribution of this paper is proposing a method called ExpertPrompting to improve the quality of answers from large language models (LLMs) like ChatGPT. The key ideas are:1. Automatically generate a detailed description of an "expert identity" that is specialized for each question or instruction. For example, for a question on physics, describe an identity as a physicist with relevant background and experiences. 2. Use this expert identity to condition the LLM when prompting it to generate an answer. For example, "As a physicist, please explain the structure of an atom."3. Show that answers generated using ExpertPrompting are rated higher quality compared to vanilla prompting without the expert identity, based on evaluations using GPT-4.4. Use ExpertPrompting to collect improved instruction-following data from GPT-3.5, and train a new open-source chatbot called ExpertLLaMA which outperforms existing chatbots like Alpaca.So in summary, the main contribution is proposing ExpertPrompting as a simple but effective prompting strategy to elicit more expert, high-quality responses from LLMs for any type of question, and demonstrating its benefits through evaluations and by releasing improved data and models.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper proposes ExpertPrompting, an automated method to instruct large language models to give high quality responses by first generating expert profiles adapted to the question, and demonstrates its effectiveness by using it to create a new high performing open-source chatbot called ExpertLLaMA.


## How does this paper compare to other research in the same field?

Here are a few key ways this paper on ExpertPrompting compares to other related work on prompting large language models:- It proposes a novel method of ExpertPrompting that involves automatically generating a detailed expert identity/background tailored to each specific question, and then instructing the LLM to answer from the perspective of that expert. This goes beyond many existing prompting methods that use more generic prompts.- The expert identities are generated automatically using in-context learning rather than requiring manual creation of prompts. This makes the method more scalable.- The approach is model-agnostic and can work with any large language model. Many prompting papers focus on a specific model architecture.- The paper shows strong empirical results, with ExpertPrompting answers preferred 48.5% of the time over vanilla answers according to GPT-4 based evaluation. This helps demonstrate the efficacy of the method.- The authors use ExpertPrompting to create a new open source chatbot ExpertLLaMA which outperforms other publicly available models like Alpaca. Releasing the model/data is a useful contribution.- The simplicity of the approach contrasts with some other prompting methods that require more complex meta-learning or iterative procedures. ExpertPrompting is straightforward to implement.Overall, I would say the main innovations are in the intuitive idea of prompting for expert identities, the use of in-context learning to automate that process, and empirical evidence that this simple approach can yield significantly improved performance over vanilla prompting. The proposed method seems promising compared to related techniques.
