# A Simple Contrastive Learning Objective for Alleviating Neural Text   Degeneration

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is:How can we alleviate the problem of neural text degeneration in autoregressive language models trained with cross-entropy loss?The key points are:- Autoregressive language models trained with cross-entropy loss exhibit text degeneration problems like repetition at the token, phrase, and sentence levels.- The authors analyze this issue and find that cross-entropy treats negative (incorrectly repeating) tokens the same as irrelevant tokens. It does not penalize negative tokens hard enough compared to positive (label) tokens.- Existing methods like unlikelihood training also have limitations in effectively suppressing negative tokens while not harming generation quality. - The authors propose a new contrastive token learning (CT) objective that promotes positive tokens and demotes negative tokens to address the limitations of cross-entropy and unlikelihood training.So in summary, the central hypothesis is that the proposed CT objective can alleviate text degeneration in language models by more directly contrasting positive and negative tokens, compared to just using cross-entropy loss. The experiments then aim to validate whether CT can reduce repetition rates and improve generation quality.


## What is the main contribution of this paper?

Based on my reading, the main contributions of this paper are:1. Proposing a new contrastive token (CT) learning objective for training autoregressive language models to reduce text degeneration such as repetition. 2. The key idea is to teach the language model to assign high probabilities to label tokens and low probabilities to negative tokens (incorrect repeating tokens) by contrasting them. This inherits the advantages of cross-entropy training and unlikelihood training while avoiding their limitations.3. The CT objective is simple to implement by reusing the logits from cross-entropy and sampling preceding tokens as negatives. It requires minor changes to the training process.4. Comprehensive experiments on language modeling and dialogue tasks show that models trained with the CT objective generate significantly less repetitive text while maintaining coherence and fluency. The CT method achieves new state-of-the-art performance on reducing neural text degeneration.5. Analysis of the predicted probabilities during generation reveals that CT results in higher uncertainty and variance compared to models trained only with cross-entropy or unlikelihood, which helps avoid excessive repetition.In summary, the main contribution is proposing and demonstrating the effectiveness of a simple contrastive learning approach at the token level to reduce repetition in neural text generation. The CT objective provides a new way to alleviate the neural degeneration problem.
