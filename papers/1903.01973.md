# [Learning Latent Plans from Play](https://arxiv.org/abs/1903.01973)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central hypothesis appears to be:Self-supervising control policies on top of unlabeled human teleoperated play data can enable learning a diverse repertoire of manipulation skills that generalize well to user-specified goal-directed tasks.The key claims made are:- Play data has properties that make it well-suited as a basis for learning goal-conditioned control policies:1) It is cheap - can be collected quickly without needing task segmentation, labeling, or resetting. 2) It is naturally rich - covers more of the interaction space than task demonstrations or random data.- Learning control policies by self-supervision on play data can match or exceed the performance of policies trained on many labeled expert demonstrations for each specific task.- Modeling play behaviors in a latent space (as in Play-LMP) helps handle the multimodality of possible solutions and improves over just goal-conditioned behavior cloning.- Policies learned this way exhibit beneficial properties like increased robustness to perturbations and retrying behaviors.So in summary, the main hypothesis is that self-supervised learning on unlabeled and easily collected play data can be an effective approach for acquiring a diverse repertoire of manipulation skills. The experiments aim to validate the claims about properties of play data and the performance of policies trained with self-supervision on play.
