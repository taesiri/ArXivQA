# [Learning Latent Plans from Play](https://arxiv.org/abs/1903.01973)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central hypothesis appears to be:Self-supervising control policies on top of unlabeled human teleoperated play data can enable learning a diverse repertoire of manipulation skills that generalize well to user-specified goal-directed tasks.The key claims made are:- Play data has properties that make it well-suited as a basis for learning goal-conditioned control policies:1) It is cheap - can be collected quickly without needing task segmentation, labeling, or resetting. 2) It is naturally rich - covers more of the interaction space than task demonstrations or random data.- Learning control policies by self-supervision on play data can match or exceed the performance of policies trained on many labeled expert demonstrations for each specific task.- Modeling play behaviors in a latent space (as in Play-LMP) helps handle the multimodality of possible solutions and improves over just goal-conditioned behavior cloning.- Policies learned this way exhibit beneficial properties like increased robustness to perturbations and retrying behaviors.So in summary, the main hypothesis is that self-supervised learning on unlabeled and easily collected play data can be an effective approach for acquiring a diverse repertoire of manipulation skills. The experiments aim to validate the claims about properties of play data and the performance of policies trained with self-supervision on play.


## What is the main contribution of this paper?

 This paper introduces a method for learning versatile robotic manipulation skills from unlabeled human teleoperation play data. The key ideas are:- Proposing human play as an alternative source of data for robot learning compared to conventional task demonstrations. Play is cheap to collect in large quantities, as it requires no task segmentation or labeling. Play is also rich and diverse, covering more modes of interaction than expert demos. - Introducing two self-supervised learning methods, Play-GCBC and Play-LMP, for extracting goal-conditioned control policies from raw play data.- Play-LMP in particular learns a latent space of "motor plans" in an unsupervised way from play data. At test time it can sample plans conditioned on a user-provided goal state and execute them to achieve that goal.- Evaluating on a simulated robot manipulation environment, showing a single Play-LMP policy can outperform many individually trained policies from expert demos on 18 distinct tasks, while only being trained on unlabeled play.- Demonstrating benefits of play supervision like increased robustness to perturbations and emergent retrying behaviors.The key contribution is showing the viability of learning goal-directed policies that generalize broadly across tasks by self-supervising on easy-to-collect play data, reducing reliance on many task-specific demonstrations. This helps scale up robotic skill learning.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the paper:The paper proposes a self-supervised control method called Play-LMP that learns reusable behavior representations from unlabeled human teleoperated play data and can generalize to a variety of user-specified visual manipulation tasks without requiring expensive labeled demonstrations for each task.


## How does this paper compare to other research in the same field?

 This paper presents a novel approach for training robots to learn diverse skills from unlabeled human play data, in contrast to the common methods of learning from task-specific demonstrations or hand-engineered rewards. The key ideas are:- Leveraging unsegmented, unsupervised human play data as a rich and scalable source for acquiring robotic skills. Play data has benefits of diversity and coverage compared to narrow expert demos for each task.- Proposing two self-supervised methods, Play-GCBC and Play-LMP, to effectively extract reusable skills and goal-conditioned policies from unstructured play logs. Play-LMP uses unsupervised representation learning to handle multimodality.- Empirically showing that policies trained via self-supervision on play data can match or exceed the performance of policies trained on expert demos for 18 distinct manipulation tasks, despite no task-specific training. Play-supervised models are also more robust.- Demonstrating unsupervised organization of the latent plan space around semantically meaningful behaviors, without any task labels.The key differences from prior work:- Uses completely unstructured play rather than pre-segmented task demos or hand-designed rewards. More scalable.- Focuses on learning very broad repertoires and task-agnostic control, not just skills for pre-defined tasks.- Representation learning to handle multimodality of behaviors, instead of single-mode imitation or RL.- Tests generalization to large set of diverse, visually specified manipulation tasks after only self-supervised play training.In summary, this work provides a novel and effective approach for robot skill learning that is more scalable, flexible and broadly applicable compared to standard methods relying on segmented demos or engineered rewards for each task. The use of unsupervised play data and representation learning for harnessing its diversity are key innovations.


## What future research directions do the authors suggest?

 The authors suggest a few potential future research directions:1. Exploring whether play-supervised control can generalize to novel objects or environments, beyond the single "playroom" environment used in this work. They note that the traditional assumptions of imitation learning are that the training and test tasks come from the same distribution. Generalizing "out-of-distribution" to entirely new environments remains an open challenge.2. Studying the effects of imbalance in play datasets on the learned control policies. They note the assumption that play data covers all objects fairly equally may be brittle, as humans may tend to prefer interacting with certain objects more during open play. Methods to address play dataset imbalance could be important.3. Improving the flexibility of the latent plan space representation, drawing inspiration from advances in the VAE literature. For example, using more expressive variational posterior and prior distributions beyond simple Gaussians, or exploring discrete latent variables. This could potentially allow learning an even richer space of reusable skills.4. Using the trained sequence encoder at test time to perform full sequence imitation, beyond just reaching user-specified goal states. While not explored here, the ability to do more detailed behavioral cloning at test time could be useful.5. Exploring whether other types of self-supervised representation learning during play could be integrated, such as for learning better visual representations from pixel inputs. This could reduce the need for ground truth state information.Overall, the authors propose several interesting directions to build on top of their play-supervised control framework and improve its generalization capabilities. Leveraging play data appears a promising approach toward learning broad repertoires of flexible skills.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the paper:The paper proposes learning task-agnostic control policies from unlabeled human play data as an alternative to learning from task-specific expert demonstrations. It introduces two self-supervised methods, Play-GCBC and Play-LMP, for learning goal-conditioned control from play. Play data has advantages over demonstrations in that it is cheap, requiring no labeling or resetting, and naturally rich, covering more of the environment's interaction space. Play-LMP in particular learns to organize play behaviors in a latent space, and can reuse these behaviors at test time to achieve new user-specified goals. Experiments in a simulated tabletop environment find that Play-LMP outperforms individual expert-trained policies on 18 visual manipulation tasks, despite being trained only on unlabeled play data. Additional benefits are robustness to perturbations and emergent retrying behaviors. The paper advocates shifting from learning discrete tasks to learning the full continuum of behaviors in an environment, which play data is well-suited for.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the paper:The paper proposes learning goal-conditioned control policies from unlabeled human play data, rather than from explicit task demonstrations. Play data has two key properties that make it attractive for learning general skills: it is cheap to collect since it does not require any labeling or segmentation, and it naturally covers a broad range of diverse behaviors as humans play freely. The authors introduce two methods for learning from play data in a simulated robotic manipulation setting. The first method, Play-GCBC, extracts goal-conditioned policies using windows of experience from play by treating the start state as the current state and the end state as a goal state. The second method, Play-LMP, learns a latent space of "plans" from play sequences, and uses this to condition a policy on goals and sampled plans. Experiments in a tabletop manipulation environment with an 8-DOF simulated robot arm compare play-supervised learning to supervised learning from demonstration. The results show that a single Play-LMP policy generalizes successfully to 18 distinct manipulation tasks despite no task-specific training data. Play supervision is competitive and even exceeds individual expert-trained policies. Additionally, play-supervised policies exhibit greater robustness to perturbations and can naturally retry after failures. The latent plan space learned by Play-LMP organizes around semantic task clusters, discovering this structure in a fully unsupervised manner. Overall, the work provides evidence that cheap, unstructured play data can be leveraged for acquiring diverse manipulation skills.


## Summarize the main method used in the paper in one paragraph.

 The paper proposes a method to learn task-agnostic control policies for robot manipulation by leveraging unlabeled human play data. The key ideas are:- Human play data is cheap and rich compared to expert demonstrations. It requires no labeling or segmentation, and explores more diverse behaviors driven by human curiosity.- The method has two components trained on play data in a self-supervised manner:  1) Play-GCBC: Goal-conditioned behavioral cloning to learn a policy mapping (state, goal) pairs to actions.  2) Play-LMP: Latent plan model which learns to encode play sequences into a latent plan space and decode plans to actions conditioned on state and goal. This handles multimodality better than Play-GCBC.- At test time, the model is provided a start state and goal state. It infers a distribution over plans using the goal and state, samples a plan, and executes the plan using the goal-conditioned policy. Replanning happens periodically.- Experiments show a single Play-LMP policy outperforms individual policies trained on expert demos for 18 distinct manipulation tasks, while being more robust and exhibiting retrying behaviors. The learned latent space discovers functional clusters without any task labels.In summary, the method demonstrates how unlabeled and easy-to-collect play data can be utilized in a self-supervised manner to acquire policies that generalize broadly across tasks in a goal-conditioned manner. The key is effectively modeling the diversity of behaviors in play through latent plans.


## What problem or question is the paper addressing?

 The paper is addressing the problem of how to efficiently and effectively acquire a diverse repertoire of general-purpose manipulation skills for robots. The key points are:- Conventional approaches like learning from demonstration or reinforcement learning require a lot of manual effort and engineering (collecting/labeling data, designing rewards) for each new skill a robot needs to learn. This does not scale well.- The paper proposes using unlabeled, unsegmented teleoperated "play" data as a scalable way to acquire a wide variety of skills. Play data has two key properties:1) It is cheap - it requires no labeling or resetting and can be collected in large quantities quickly.2) It is rich - it covers a wider variety of behaviors and state transitions than task-specific demonstrations.- The paper introduces two methods for learning control policies from play data in a self-supervised manner: Play-GCBC and Play-LMP.- Play-LMP in particular uses unsupervised representation learning to organize play experiences in a latent space. It then conditions a policy on current state, goal state, and a sampled latent plan to accomplish user-specified goals.- Experiments show a single Play-LMP policy can outperform many individually trained task-specific policies on 18 manipulation tasks, despite never seeing task labels. Play-LMP also shows benefits like increased robustness and retrying behaviors.In summary, the paper demonstrates an approach to acquire a diverse skill repertoire on real robot platforms in a scalable way, by self-supervising on unlabeled and easily collected play data.
