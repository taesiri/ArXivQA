# [GD doesn't make the cut: Three ways that non-differentiability affects   neural network training](https://arxiv.org/abs/2401.08426)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem Statement:
The paper investigates how non-differentiability affects three different aspects of neural network training:
1) Convergence analysis of gradient descent algorithms
2) Solving the LASSO (L1 regularization) problem
3) Understanding the "Edge of Stability" phenomenon

It argues that commonly, non-differentiability is ignored and neural networks are treated as differentiable functions. However, this paper shows that accounting for non-differentiability dramatically alters the dynamics and our understanding of the training process.

Key Contributions:

1) Convergence Analysis: 
- Shows that gradient descent (GD) on continuously differentiable functions has faster convergence than non-differentiable gradient descent (NDGD) on non-differentiable functions.
- Demonstrates this through experiments on neural networks with tanh (differentiable) versus ReLU (non-differentiable) activations.
- Discusses problems with smoothness assumptions used to analyze GD that break down for NDGD.

2) LASSO Problem:
- Shows NDGD solutions for LASSO problem are unreliable and counterintuitive.
- Increasing LASSO penalty can increase L1 norm of solution, defeating the purpose. 
- Connects behavior to properties of subgradient methods.
- Contrasts with traditional ML frameworks like glmnet that have guarantees.

3) Edge of Stability:
- Shows all convex non-smooth Lipschitz functions display "unstable convergence" for any learning rate.
- Provides counterexample to claim that subquadratic growth explains phenomenon.
- Argues analyses based on differentiable functions have limited applicability.

Overall, the paper makes a case for treating NDGD as a distinct entity from GD in analysis to better understand modern deep learning. It also highlights challenges introduced by non-differentiability across multiple facets of training.


## Summarize the paper in one sentence.

 This paper investigates how non-differentiability affects neural network training, showing that treating non-differentiable gradient descents identically to gradient descents limits understanding of convergence rates, LASSO solutions, and the edge of stability phenomenon.


## What is the main contribution of this paper?

 This paper investigates how non-differentiability affects three different aspects of neural network training:

1. Convergence analysis of ReLU networks: The paper shows that continuously differentiable neural networks converge faster than non-differentiable ReLU networks. 

2. Solution to the LASSO problem: The paper demonstrates that the solutions produced by gradient descent for the LASSO problem are incorrect and counterintuitive, even for the simple case of an L1 penalized linear model.

3. The Edge of Stability: The paper shows that all convex, non-smooth, Lipschitz continuous functions display unstable convergence regardless of the learning rate. It also provides an example of a result derived for twice differentiable functions that fails in the case of once differentiable functions.

In summary, the main contribution is highlighting cases where accounting for non-differentiability dramatically alters the dynamics and conclusions compared to simply assuming differentiability. The paper argues that treating non-differentiable gradient descent as equivalent to standard gradient descent limits our understanding, and that explicitly modeling non-differentiability is essential for developing better algorithms and theory.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and concepts associated with it include:

- Non-differentiable gradient descent (NDGD) - The paper analyzes the behavior of gradient descent methods applied to non-differentiable loss functions, which it refers to as "non-differentiable gradient descent" (NDGD). This is contrasted with standard gradient descent (GD) applied to continuously differentiable loss functions.

- Convergence analysis - One section of the paper analyzes the convergence properties of NDGD compared to GD, showing NDGD converges much more slowly.

- LASSO problem - Another section examines how NDGD gives unreliable and counterintuitive solutions for LASSO problems involving L1 regularization. 

- Edge of stability - A section looks at the "edge of stability" phenomenon in NDGD, showing convex non-smooth loss functions display unstable convergence regardless of learning rate.

- ReLU networks - The non-differentiability studied in the paper is motivated by properties of common activation functions like ReLUs.

- Subgradient method - The analysis connects behavior of NDGD to the subgradient method from optimization.

- Non-smooth/non-differentiable functions - The paper emphasizes problems arising from applying GD theory developed for smooth functions to non-smooth cases.

So in summary, key terms cover: NDGD, convergence analysis, LASSO, edge of stability, ReLU networks, subgradient method, non-smooth/non-differentiable functions.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in this paper:

1) In the convergence analysis section, the authors show that gradient descent (GD) sequences converge faster than non-differentiable gradient descent (NDGD) sequences for neural networks. Could you elaborate more on why this difference in convergence rates occurs? Is it simply because NDGD gets stuck at points of non-differentiability or are there other structural differences that cause the slower convergence?

2) The proposition in the convergence analysis section states that GD sequences get "captured" near local minima, while this doesn't happen for NDGD sequences. Intuitively, why does this capturing occur for GD but not NDGD? Does this suggest GD is more stable or simply a weakness of NDGD?

3) For the LASSO problem, you analytically show the NDGD solution oscillates between two points based on the parity of the number of iterations. Does this oscillatory behavior occur only for the simple LASSO problem in Eq. 5 or more broadly? If more broadly, does this undermine the usefulness of NDGD for obtaining sparse solutions?

4) You mention specialized algorithms that have been developed to find sparse solutions for the LASSO problem. Could you elaborate on one or two of these methods and discuss their advantages over NDGD? Are there ways NDGD could be improved to better obtain sparse solutions?

5) In the section on LASSO solutions and network pruning, you note the common intuition about LASSO, that a larger penalty induces sparser solutions, is violated by NDGD. Where does this intuition come from if not from the properties of NDGD? Is it based on other optimization methods?

6) Proposition 4 states non-smooth convex loss functions always display unstable convergence. Does this proposition apply to non-convex loss functions as well? If so, how does it change our understanding about the "edge of stability" for neural network training?

7) For the Huber loss example, what causes the initial increase in loss for α=1 before the oscillations set in? Is there something special about α=1 or would we expect a similar peak at some α for any non-smooth convex function?

8) In the context of subquadratic losses, you provide a counterexample with the Huber loss to challenge the claim that subquadratic growth explains unstable convergence. Are there other simple counterexamples that could be provided to further demonstrate the limitations of claims based on twice differentiable functions?

9) You propose the convex, non-differentiable single layer neural network from Kumar et al. 2023 as a good toy problem for studying NDGD behavior. What specifically about this model makes it well-suited for developing NDGD theory compared to other toy problems that have been proposed?

10) Based on your analysis, what are 1-2 outstanding open theoretical questions regarding understanding the behavior of NDGD, especially around points of non-differentiability, that you think are important to study further?
