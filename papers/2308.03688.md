# AgentBench: Evaluating LLMs as Agents

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question/hypothesis seems to be:How capable are Large Language Models of acting as intelligent agents that can reason, plan, and make decisions in interactive real-world environments? Specifically, the paper aims to systematically evaluate and benchmark the performance of LLMs as autonomous agents across a diverse set of challenging environments and tasks. The key ideas and goals of the paper can be summarized as:- There is an urgent need for standard benchmarks to evaluate LLMs as agents rather than just on static NLP tasks. Existing benchmarks are limited in scope.- The paper introduces AgentBench, a new comprehensive benchmark with 8 distinct environments based on real-world use cases to test LLMs' capabilities as agents.- AgentBench requires LLMs to demonstrate skills like following instructions, coding, knowledge acquisition and logical reasoning in interactive settings.- The work provides a systematic comparison of 25 LLMs (APIs and open-sourced) using AgentBench to assess state-of-the-art and gaps.- Results show top API models like GPT-4 have strong agent abilities but clear gaps exist compared to open-sourced LLMs. - The benchmark and evaluation toolkit are released to facilitate LLM-as-agent research.In summary, the central hypothesis is that AgentBench can effectively benchmark the under-explored capability of LLMs to act as reasoning agents in the real world, revealing current progress and gaps. The paper presents empirical findings from a large-scale LLM evaluation using the proposed benchmark.


## What is the main contribution of this paper?

Based on my reading, the main contributions of this paper are:1. It introduces the concept of evaluating LLMs as agents, and presents AgentBench, a comprehensive benchmark to standardize the evaluation. AgentBench defines 8 distinct environments based on real-world scenarios, offering a practical testbed to evaluate the wide array of capabilities of LLMs.2. It performs a thorough evaluation of 25 different LLMs using AgentBench, uncovering a significant performance gap between leading top API-based LLMs and open-source models. The results illuminate the state-of-the-art in LLM-as-Agent and highlight areas for improvement. 3. It releases an integrated toolkit to facilitate the assessment of LLM-as-Agent. The toolkit allows customizing AgentBench evaluation to any LLM based on the philosophy of "API & Docker" interaction. The toolkit, datasets, and environments are publicly available to the research community.In summary, the key contribution is proposing AgentBench as a systematic benchmark for evaluating LLMs as agents across diverse real-world environments, conducting a comprehensive evaluation of various LLMs, and providing an extensible toolkit to enable continued research in this direction. The results reveal the potential yet limitations of existing LLMs as autonomous agents.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence TL;DR summary of the paper:The paper introduces AgentBench, a new benchmark with 8 distinct environments for systematically evaluating the capabilities of large language models to act as intelligent agents and make decisions in interactive real-world scenarios.
