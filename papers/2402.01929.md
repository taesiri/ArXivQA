# [Sample, estimate, aggregate: A recipe for causal discovery foundation   models](https://arxiv.org/abs/2402.01929)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
Existing causal discovery algorithms have two key limitations - they require large amounts of data and are very slow, especially as the number of variables grows. Discrete optimization methods explore an exponential search space and quickly become intractable. Continuous optimization methods require fitting generative models to the full data distribution, which is difficult with limited data. 

Proposed Solution: 
The paper proposes a causal discovery framework called SEA (Sample, Estimate, Aggregate) inspired by foundation models. The key idea is to leverage classical algorithms' estimates over small subgraphs and global graph statistics as inputs to a deep learning model. This pretrained model aggregates the features into full causal graphs.

The framework has 3 main steps:
1) Sample - Sample small batches of data and subsets of variables. Compute global statistics like inverse covariance.
2) Estimate - Run classical causal discovery algorithms on subsets to get local estimates. 
3) Aggregate - Feed local estimates and global statistics into a pretrained deep model to predict the full graph.

The model uses an axial attention architecture to propagate information between the local and global features.

Main Contributions:
1) Propose a new framework to build fast, robust and generalizable foundation models for causal discovery using classical algorithms' outputs.
2) Provide theoretical analysis showing the model has capacity to produce causally sound graphs from marginal estimates.
3) Achieve state-of-the-art performance on synthetic and real datasets while being 10-1000x faster than existing methods. Generalizes to unseen mechanisms.  
4) Framework supports any sampling heuristics, classical algorithms and statistical features. Implementations using FCI, GIES and inverse covariance attain strong performance.

In summary, the paper presents a blueprint for developing high-quality causal discovery models by combining classical methods with deep learning in an efficient way. The framework outperforms existing approaches on several fronts.


## Summarize the paper in one sentence.

 This paper proposes a causal discovery framework called SEA that pretrains a deep learning model to aggregate estimates from classical algorithms run on subsets of variables into full causal graphs, enabling faster and more robust inference compared to existing methods.


## What is the main contribution of this paper?

 Based on my understanding, the main contribution of this paper is proposing a causal discovery framework called SEA (Sample, Estimate, Aggregate) where a deep learning model is pretrained to resolve predictions from classical discovery algorithms run over smaller subsets of variables. The key ideas are:

1) Classical causal discovery algorithms scale poorly to large graphs, but work well on small subgraphs. Global statistics like correlation can also provide useful signals. 

2) The paper proposes to leverage estimates from classical algorithms over small subgraphs, along with global graph statistics, as inputs to a deep learning model. The model is pretrained to aggregate these features into full causal graphs.

3) This enables fast inference on new datasets, good performance in low data regimes, and generalization beyond causal mechanisms seen during training.

So in summary, the main contribution is proposing a new framework/architecture for developing causal discovery foundation models that can be fast, data-efficient and generalizable compared to existing methods. The paper shows instantiations of this framework and analyzes its theoretical properties.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and keywords associated with it are:

- Causal discovery
- Foundation models
- Sample, estimate, aggregate
- Deep learning
- Classical algorithms (FCI, GIES)
- Subgraph estimation
- Axial attention
- Marginal estimates
- Generalization

The paper proposes a new framework called "Sample, Estimate, Aggregate" (SEA) for building causal discovery foundation models. The key ideas are:

1) Sample batches of data and subsets of variables
2) Estimate marginal graphs and global statistics over these subsets using classical algorithms 
3) Aggregate these estimates into a global causal graph using a pretrained deep learning model based on axial attention

The goal is to create causal discovery models that are fast, robust, and generalizable by combining classical algorithms and deep learning. Key terms reflect this hybrid approach, the proposed framework itself, and desired properties like speed and generalization.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in this paper:

1. The paper proposes a causal discovery framework called SEA that utilizes classical algorithms on subsets of variables along with global statistics. How does this framework balance leveraging classical algorithms' correctness with their poor scaling? What are the theoretical guarantees provided regarding recovering the full causal graph from these marginal estimates?

2. The paper mentions using global statistics like correlation or inverse covariance to get a sense of overall graph connectivity. What is the intuition behind why these simple statistics can provide a reasonable signal regarding graph structure? How does the choice of global statistic impact performance?

3. The paper implements SEA using an axial attention model. Why is axial attention a sensible choice of architecture in this application? What implicit assumptions does this make regarding underlying graph structure and relationships? How does attention provide the capacity to resolve marginal estimates?  

4. How does the sampling procedure balance exploration of different subsets of variables with exploitation of highly connected variables during training? What is the effect of different sampling strategies like random, global statistic-based, or model uncertainty-based?

5. The paper hypothesizes foundation models for causal discovery but only implements and evaluates on two specific classical algorithms. How might performance change using different underlying discovery algorithms? What modifications would be required to incorporate other algorithms like LiNGAM, GES, etc.?

6. For what types of graphs does SEA work well or struggle on? The paper seems to obtain strong performance on synthetic data - why might this be more challenging on real biological datasets? How could the framework be adapted to capture properties of real-world networks better?

7. The paper argues continuous evaluation metrics should be preferred over discrete metrics like SHD. Why might model outputs be poorly calibrated or require different discretization thresholds? When is SHD an appropriate metric to consider?

8. What limits the capacity of SEA to scale to larger graphs beyond those seen during training? How much do hyperparameters like number of layers, heads, embedding sizes, etc. impact performance? How could the framework be engineered to handle much larger graphs?

9. The paper finds the approach generalizes reasonably to unseen causal mechanisms but struggles on unseen noise types (e.g. multiplicative). Why might this occur and how could training be adjusted to improve generalization? What other dataset properties might models overfit to?

10. From an application perspective, what are the major advantages SEA provides over existing approaches? In what scenarios would this method be preferred or not preferred over classical methods or continuous optimization techniques?
