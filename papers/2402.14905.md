# [MobileLLM: Optimizing Sub-billion Parameter Language Models for   On-Device Use Cases](https://arxiv.org/abs/2402.14905)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Large language models (LLMs) like ChatGPT are permeating daily life but rely on energy-intensive cloud infrastructure. 
- Deploying models over 1 billion parameters on mobile devices is infeasible due to DRAM limitations (typically 6-12GB) and battery constraints.  
- There is a need for efficient sub-billion parameter LLM architectures optimized for on-device applications.

Proposed Solution:
- The paper focuses on designing LLMs under 1 billion parameters suited for mobile devices. 
- They demonstrate architecture design is crucial for small LLM performance, challenging prevailing beliefs about scaling laws.  
- A strong MobileLLM baseline is developed using deep & thin networks, embedding sharing, and grouped attention.  
- They further propose immediate block-wise weight sharing to boost accuracy at no parameter cost.

Contributions:
- Show depth matters more than width for 125M-350M parameter LLMs, with 30-42 layers optimal.
- Propose input-output embedding sharing and grouped query attention tailored to small LLMs.  
- Introduce immediate block-wise weight sharing to repeat transformer blocks with minimal overhead.
- Establish new MobileLLM architecture as state-of-the-art for sub-billion parameter models. 125M & 350M models surpass previous counterparts by 2.7% & 4.3%.
- Demonstrate strong performance of MobileLLM models on zero-shot reasoning, QA, reading comprehension and chat tasks.
- Show 350M MobileLLM model achieves comparable API calling accuracy to the 7B LLaMA-v2, validating efficacy for on-device use.

In summary, the key innovation is optimizing sub-billion LLM designs for mobile deployment through architectural advances, establishing superior accuracy over prior counterparts. The paper makes notable headway in understanding architecture principles for compact LLMs.
