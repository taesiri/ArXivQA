# [No-Regret Reinforcement Learning in Smooth MDPs](https://arxiv.org/abs/2402.03792)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
Obtaining no-regret guarantees for reinforcement learning (RL) in problems with continuous state and action spaces remains a major open challenge. Most theoretical progress has been made under restrictive assumptions like linear MDPs or Lipschitz MDPs. There is a gap between these specific settings and the general continuous RL problem. The paper introduces a novel structural assumption called "smoothness" that generalizes prior settings and allows closing this gap.

Proposed Solution: 
The paper defines two classes of smooth MDPs:

1) Strongly Smooth MDPs where the transition and reward functions have bounded norm in a smoothness space.

2) Weakly Smooth MDPs where the Bellman optimality operator is bounded on the smoothness space. 

The key idea is to use Legendre polynomials, an orthogonal basis, to construct feature maps that approximate the MDP. Two algorithms are proposed:

- Legendre-Eleanor: Proves no-regret under general smoothness assumptions but is computationally inefficient.

- Legendre-LSVI: Runs in polynomial time but requires stronger smoothness. It transforms the MDP into an approximate linear MDP amenable to existing algorithms.

Main Contributions:

- Introduces the smoothness assumption that generalizes major RL settings like Lipschitz, linear and kernelized MDPs

- Develops novel technique using Legendre polynomials to construct feature maps

- Proposes two algorithms with regret guarantees adaptive to the smoothness level

- Legendre-LSVI achieves sqrt(K) regret in broader settings than prior polynomial-time methods

- Results significantly expand the scope of theoretical RL with continuity guarantees

The paper makes an important step towards no-regret RL under general continuity assumptions rather than restrictive parametric assumptions. The use of orthogonal function representations is a key technical innovation.
