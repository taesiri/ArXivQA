# [Accelerating Transducers through Adjacent Token Merging](https://arxiv.org/abs/2306.16009)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the abstract, the central research question this paper addresses is how to reduce computation and accelerate inference for end-to-end automatic speech recognition systems based on Transformer transducers, without sacrificing accuracy. The key hypothesis seems to be that gradually combining adjacent encoder tokens with high similarity between their key values can reduce sequence length and redundancy, thereby improving efficiency, while still preserving the information needed for accurate speech recognition.Specifically, the proposed approach called Adjacent Token Merging (A-ToMe) aims to adaptively subsample the acoustic tokens within the Transformer transducer encoder in order to reduce computational costs, especially for long speech inputs. This is in contrast to prior work that uses fixed subsampling rates.So in summary, the main research question is how to improve encoder efficiency in Transformer transducer ASR through a novel adaptive subsampling technique, with the hypothesis that merging highly similar adjacent tokens can reduce computation while maintaining accuracy.
