# [Percept, Chat, and then Adapt: Multimodal Knowledge Transfer of   Foundation Models for Open-World Video Recognition](https://arxiv.org/abs/2402.18951)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
Traditional video recognition models perform poorly on real-world open-domain videos that have significant domain gaps compared to academic datasets. These open-world videos often have complex environment variations like low resolution, poor lighting, unusual scenes, etc that models trained on academic datasets fail to generalize to.  

Proposed Solution: 
The paper proposes a generic knowledge transfer pipeline called PCA that leverages multimodal external knowledge from foundation models to boost open-world video recognition. PCA has three stages:

1) Percept: Enhance open-world videos using low-level vision models to reduce domain gaps and extract visual features/predictions as external knowledge. 

2) Chat: Generate textual descriptions of predicted labels or full video captions using large language models as external textual knowledge.

3) Adapt: Introduce knowledge adaptation modules that integrate the external multimodal knowledge into video backbone networks to assist open-world video recognition.

Key Contributions:

1) A general knowledge transfer paradigm PCA that progressively mines various external knowledge from foundation models to boost open-world video recognition using Percept, Chat and Adapt stages.

2) A plug-and-play knowledge adaptation module that flexibly integrates PCA into various video backbones. 

3) State-of-the-art performance on three challenging real-world video datasets - TinyVIRAT, ARID and QV-Pipe through extensive experiments, validating the effectiveness of PCA.
