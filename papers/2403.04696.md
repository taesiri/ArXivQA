# [Fact-Checking the Output of Large Language Models via Token-Level   Uncertainty Quantification](https://arxiv.org/abs/2403.04696)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper "Fact-Checking the Output of Large Language Models via Token-Level Uncertainty Quantification":

Problem:
- Large language models (LLMs) are prone to generating factually incorrect or fabricated content, referred to as "hallucinations". These hallucinations can be dangerous as they may be obscured by other accurate content, making them hard to detect.
- Current LLM services lack mechanisms for detecting unreliable generations to caution users.

Proposed Solution: 
- The authors propose a novel fact-checking pipeline to detect hallucinations in LLM outputs using token-level uncertainty quantification (UQ).
- They introduce a new token-level UQ method called Claim-Conditioned Probability (CCP) which measures the model's uncertainty in the factual correctness of each token. CCP filters uncertainty unrelated to factuality.
- CCP outperforms baselines by removing the impact of uncertainty about what claim to generate next and what words to use to express it. It focuses purely on uncertainty in the factuality of generated claims.

Contributions:
- Novel fact-checking pipeline using token-level UQ to highlight unreliable LLM generations for end users.
- CCP token-level UQ method that significantly improves over baselines in spotting hallucinations for 6 LLMs across 3 languages.
- New evaluation approach leveraging automatic fact-checking of generated biographies.
- Empirical analysis showing token-level UQ can rival complex systems that use external knowledge to spot factual errors.
- Human evaluation confirming that the pipeline can effectively evaluate uncertainty methods.

In summary, the paper introduces an interpretable approach leveraging uncertainty signals from the LLM itself to detect potential factual inaccuracies in model outputs, without needing external knowledge sources. The proposed methods advance capacities for spotting and cautioning against model hallucinations.
