# [Task Residual for Tuning Vision-Language Models](https://arxiv.org/abs/2211.10277)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question seems to be:

How can we effectively transfer the knowledge learned by large pre-trained vision-language models like CLIP to downstream tasks using only limited data, while avoiding common pitfalls?

The key points I gathered are:

- Large pre-trained VLMs like CLIP learn rich general visual knowledge that should be inherited when fine-tuning on downstream tasks. However, common approaches either damage the pre-trained knowledge (e.g. prompt tuning) or are too biased towards it (e.g. adapter tuning).

- The key is to explicitly decouple the pre-trained knowledge from new task-specific knowledge, to enable both reliable inheritance of prior knowledge and flexible exploration of new knowledge. 

- They propose a simple yet effective approach called Task Residual Tuning (TaskRes) that adds a small set of prior-independent residual parameters to the pre-trained classifier weights. These capture new task knowledge without interfering with the original weights.

- Experiments show TaskRes outperforms previous methods on 11 datasets. Analysis of the learned residuals shows their magnitude correlates with task transfer difficulty, validating the approach.

So in summary, the central hypothesis is that explicitly decoupling prior and new knowledge enables better transfer learning from large VLMs using Task Residuals, which is validated empirically.
