# [Algorithmic progress in language models](https://arxiv.org/abs/2403.05812)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem Statement
- The paper investigates the relative contributions of algorithmic innovations vs simply scaling up compute to the rapid progress in language modeling over the past decade. Specifically, it aims to quantify how much compute is needed to reach a fixed performance threshold over time.

- This analysis can shed light on whether progress is mostly driven by new algorithms that enable more efficient use of compute and data, or whether brute-force scaling of models and datasets has been more impactful.

Methodology
- The authors curate a dataset of over 200 language models evaluated on WikiText-103, WikiText-2 and Penn Treebank between 2012-2023. This includes model perplexities, publication dates, parameter counts and training set sizes.

- They fit an augmented version of the Chinchilla scaling law from Hoffmann 2022 to this data. Their model incorporates terms capturing "effective parameters" and "effective data" that exponentially increase over time to represent algorithmic progress.

- From their fitted model, the authors are able to back out compute doubling times purely from algorithmic innovations, indicate the significance of milestones like the Transformer, and decompose performance gains.

Key Findings
- The compute needed to reach a set perplexity threshold is estimated to have halved every 8-9 months on average since 2012, faster than hardware gains per Moore's law.

- Despite this rapid algorithmic progress, scaling up compute and datasets has accounted for 60-95% of performance improvements in recent years. The transformer architecture specifically contributed a compute equivalent gain of 3-46x.

- Extrapolating doubling time trends predictscompute requirements could have fallen by 22,000x since 2014. However, direct observations of such extreme efficiency gains are lacking, and scale-dependence of innovations warrants caution about naively extrapolating trends.

Limitations
- Noise and inconsistencies in perplexity benchmarking makes precise estimates challenging. The model relies heavily on the Chinchilla scaling law holding over diverse architectures.

- Analysis is retrospective and focused on historical data. Future progress could differ substantially. Specific innovations are hard to analyze due to aggregate measurement over long time periods.
