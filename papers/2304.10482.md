# [Reconstructing Signing Avatars From Video Using Linguistic Priors](https://arxiv.org/abs/2304.10482)

## What is the central research question or hypothesis that this paper addresses?

 The central research question this paper addresses is how to accurately reconstruct expressive 3D avatars from monocular sign language video. Specifically, the authors aim to address the challenges in hand pose estimation that arise from occlusion, noise, and motion blur in sign language videos. Their key hypothesis is that novel linguistic priors based on universal properties of sign languages can help resolve ambiguities and improve 3D hand pose estimation from such degraded video.

The main goal is to develop a method that can automatically reconstruct natural 3D body motion, including detailed hand articulations, facial expressions, and body pose, from monocular sign language video footage. The linguistic priors they introduce, based on symmetry and hand pose invariance, are hypothesized to help constrain the hand pose estimation problem for sign language inputs. Overall, their proposed method, SGNify, incorporates these linguistic priors to enable more robust reconstruction of full 3D avatars from challenging real-world sign language video data.


## What is the main contribution of this paper?

 The main contribution of this paper is introducing novel linguistic priors for sign language that provide constraints on 3D hand pose to help resolve ambiguities and improve hand pose estimation from sign language videos. Specifically, the paper:

- Proposes two new sign language linguistic constraints: symmetry and hand-pose invariance. These exploit universal linguistic rules of sign languages to act as priors that improve 3D hand pose estimation.

- Introduces SGNify-X, a method that incorporates these linguistic priors into an optimization framework to reconstruct expressive 3D avatars from monocular sign language videos. This enables capturing fine details of hand articulation, facial expressions, and body movement.

- Evaluates SGNify-X quantitatively using ground truth mocap data and shows it outperforms state-of-the-art methods for 3D body pose and shape estimation on sign language videos.

- Conducts perceptual studies showing SGNify-X's reconstructions are significantly more comprehensible, recognizable, easier to understand, and more natural than previous methods. The studies also show SGNify-X's reconstructions are on par with real videos in terms of sign recognition.

- Makes available the first expressive 4D mocap dataset of sign language for quantitative evaluation as well as code to reconstruct 3D avatars from sign language video using linguistic constraints.

In summary, the key contribution is using sign language linguistic priors within a optimization framework to achieve state-of-the-art performance in reconstructing detailed and natural 3D avatars from challenging monocular sign language videos. The proposed linguistic constraints and SGNify-X method are universally applicable across sign languages.
