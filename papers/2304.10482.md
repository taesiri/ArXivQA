# [Reconstructing Signing Avatars From Video Using Linguistic Priors](https://arxiv.org/abs/2304.10482)

## What is the central research question or hypothesis that this paper addresses?

 The central research question this paper addresses is how to accurately reconstruct expressive 3D avatars from monocular sign language video. Specifically, the authors aim to address the challenges in hand pose estimation that arise from occlusion, noise, and motion blur in sign language videos. Their key hypothesis is that novel linguistic priors based on universal properties of sign languages can help resolve ambiguities and improve 3D hand pose estimation from such degraded video.

The main goal is to develop a method that can automatically reconstruct natural 3D body motion, including detailed hand articulations, facial expressions, and body pose, from monocular sign language video footage. The linguistic priors they introduce, based on symmetry and hand pose invariance, are hypothesized to help constrain the hand pose estimation problem for sign language inputs. Overall, their proposed method, SGNify, incorporates these linguistic priors to enable more robust reconstruction of full 3D avatars from challenging real-world sign language video data.
