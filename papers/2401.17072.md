# [SemScore: Automated Evaluation of Instruction-Tuned LLMs based on   Semantic Textual Similarity](https://arxiv.org/abs/2401.17072)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Instruction-tuned large language models (LLMs) have shown remarkable capability to generate fitting responses to natural language instructions. However, evaluating these responses at scale remains challenging. 
- Traditional metrics like BLEU/ROUGE have drawbacks - they require multiple references, do poorly on coding questions, and show weak correlation to human judgement.  
- Among a growing number of evaluation metrics, it's unclear which correlates best to human assessment.

Method:
- The authors conduct a study evaluating outputs from 12 prominent instruction-tuned LLMs. 
- They manually rate 252 model responses on coherence, validity etc using a standardized rubric. 
- They benchmark 8 existing automated metrics on how well they correlate to this human ranking of models.
- They propose a new automated metric called SemScore. It uses an off-the-shelf sentence transformer to compute semantic similarity between model output and gold target response.

Key Results:
- SemScore shows the highest correlation (Kendall tau=0.879) to human rankings, outperforming metrics like BERTScore, BLEU, ROUGE-L and G-Eval.
- Ablation studies indicate the strength of SemScore lies in using sentence embeddings rather than the choice of transformer.
- Qualitative examples illustrate cases where SemScore excels or falters compared to metrics like G-Eval and BERTScore.

Main Contributions:  
- Extended human evaluation ranking 12 instruction-tuned LLMs
- Extensive comparison of evaluation metrics w.r.t. human correlation 
- A simple but highly effective automated metric called SemScore that computes semantic textual similarity between model output and gold response.

In summary, the paper addresses the challenging problem of effectively evaluating instruction-tuned LLM responses. It shows SemScore to be a reproducible, cost-effective metric that correlates better with human assessment than existing approaches.


## Summarize the paper in one sentence.

 This paper proposes and evaluates a semantic textual similarity-based metric called SemScore for automatically evaluating instruction-tuned language models, finding it to correlate better with human judgment than several existing metrics.


## What is the main contribution of this paper?

 The main contribution of this paper is proposing a new evaluation metric called SemScore for evaluating the quality of responses generated by instruction-tuned large language models (LLMs). Specifically:

- The paper conducts a study comparing 8 existing text generation evaluation metrics and proposes SemScore, a metric based on semantic textual similarity between the model response and gold target response. 

- SemScore is shown to correlate better with human judgments than the other metrics considered, including recently proposed learned metrics like BERTScore and G-Eval.

- The simplicity and interpretability of SemScore is argued to make it well-suited for practical application in evaluating instruction-tuned LLMs.

So in summary, the key contribution is the proposal and evaluation of the SemScore metric for judging the quality of instruction-tuned LLM responses. The results indicate it could be an effective and practical automated evaluation approach in this area.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and concepts associated with this paper include:

- Instruction-tuned large language models (LLMs)
- Automated evaluation metrics
- Semantic textual similarity (STS)
- SemScore - the proposed evaluation metric
- Kendall and Pearson correlation with human judgement
- BLEU, ROUGE-L - traditional n-gram overlap metrics
- BERTScore, BLEURT, BARTScore - embedding and neural metric based evaluations
- G-Eval - an LLM-based evaluation approach
- MPNet sentence embeddings 

The paper conducts an analysis and comparison of different automated evaluation metrics for judging the quality of responses generated by instruction-tuned LLMs. It proposes a new metric called SemScore that computes the semantic similarity between model outputs and gold references using MPNet embeddings. The key finding is that SemScore correlates most strongly with human assessments compared to several existing metrics.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in this paper:

1. The paper proposes SemScore as an evaluation metric for instruction-tuned LLMs. How exactly is SemScore calculated? What sentence encoder is used and what similarity metric is applied?

2. The paper compares SemScore against several strong baseline metrics like BERTScore and G-Eval. What were the main findings in terms of correlation to human judgment? Which metric performed best overall?  

3. The paper uses the dataset from Wang et al. (2023) for evaluation. What makes this dataset challenging for traditional metrics like BLEU or ROUGE? What types of tasks does it contain that go beyond standard NLP tasks?

4. The authors perform an ablation study to analyze the effect of using different sentence encoders with SemScore. What encoders were compared and what was the effect on correlation scores? Does SemScore work well even with non-specialized encoders?

5. The paper argues that SemScore offers a straightforward, reproducible and cost-effective evaluation approach. Can you expand on these points? In what way is it more reproducible than e.g. G-Eval? Does it require any special access or resources?

6. The authors acknowledge some limitations of SemScore, e.g. its dependence on the underlying sentence encoder model. How big is this concern in practice if clear model recommendations are provided? Could updates to SentenceTransformers affect scores over time?  

7. Beyond the overall scores, the authors also report per-task correlation scores. Were there noticeable differences across tasks groups? Which tasks were most challenging for SemScore compared to other metrics?

8. The paper focuses on evaluating instruction-tuned LLMs, but do you think SemScore could also be applicable in other text generation settings? What factors need to be considered regarding the choice of gold references? 

9. The examples in Table 4 provide some insightful cases. Analyze one example where SemScore performs distinctly better or worse than other metrics. What causes these differences?

10. The authors recommend using all-mpnet-base-v2 as the sentence encoder, but better models may be developed in the future. What approach would allow leveraging such improved STS models for SemScore while maintaining comparability?
