# [Deep Graph Reprogramming](https://arxiv.org/abs/2304.14593)

## What is the central research question or hypothesis that this paper addresses?

 The central research question this paper addresses is how to reuse a pre-trained graph neural network (GNN) model for multiple downstream tasks without re-training or fine-tuning the model. Specifically, the paper proposes a novel "deep graph reprogramming" (GARE) approach that allows adapting a single pre-trained GNN to handle diverse cross-level downstream tasks across different domains. 

The key hypothesis is that by reprogramming the input graph data and adapting the model's aggregation behaviors, a frozen pre-trained GNN can be repurposed to perform well on unseen tasks that it was not originally trained for. This is in contrast to common approaches like knowledge distillation that require re-training the model for each new task.

To summarize, the main research question is: How can we reuse a single pre-trained GNN for multiple different downstream tasks without model re-training or fine-tuning? The key hypothesis is that through data and model reprogramming techniques, a frozen pre-trained model can be adapted to new tasks and domains. The proposed GARE methods aim to validate this hypothesis.


## What is the main contribution of this paper?

 The main contribution of this paper is proposing a novel deep graph reprogramming (GARE) task for reusing pre-trained graph neural networks (GNNs) on multiple downstream tasks without changing model architectures or parameters. 

The key ideas are:

- Proposing two paradigms: data reprogramming (DARE) to handle input graphs with heterogeneous dimensions, and model reprogramming (MERE) to strengthen model capacity for diverse tasks.

- For DARE, proposing 3 methods: Meta-FeatPadding for heterogeneous feature dimensions, Edge-Slimming for homogeneous transductive tasks, and Meta-GraPadding for homogeneous inductive tasks. 

- For MERE, proposing a Reprogrammable Aggregator module to dynamically adapt aggregation behaviors for different tasks.

- Evaluating on 14 datasets across various domains and tasks like node/graph classification/regression, 3D recognition, action recognition. Results show the pre-trained GNN with GARE performs well without retraining.

In summary, the main contribution is introducing the GARE concept and associated techniques to enable generalized and efficient reuse of a single pre-trained GNN for multiple downstream tasks without modification. This is a novel graph-based model reuse paradigm.
