# [A Library of Mirrors: Deep Neural Nets in Low Dimensions are Convex   Lasso Models with Reflection Features](https://arxiv.org/abs/2403.01046)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem: Training deep neural networks is challenging due to non-convexity, which means there may be many local optima and it is hard to find the global optimum. The paper focuses specifically on training neural nets on 1D or 2D data, which arises in some application domains. The goal is to simplify the training problem for such networks by reformulating it as a convex optimization problem that yields global solutions.

Proposed Solution: The paper shows that for several neural network architectures, including 2-layer networks with piecewise linear activations (like ReLU, leaky ReLU, absolute value) and deeper networks with ReLU or sign activations, the non-convex neural network training problem is equivalent to a Lasso convex optimization problem. Specifically, the neural net training loss function plus regularization term is shown to be equivalent to a particular Lasso problem for various network architectures. 

The Lasso formulation learns to represent the data sparsely by selecting features from a fixed, explicit Lasso dictionary matrix that depends on the activation function and network depth. For ReLU activation, the features in this dictionary are shown to be ramp, ReLU and convex reflection of ReLU functions. For sign activation, the dictionary contains features that switch between 1/-1. The paper derives analytical expressions for these dictionaries.

Using the dictionaries, the paper also gives very transparent and simple algorithms to reconstruct optimal neural networks from the Lasso solution, for 2-layer and 3-layer architectures.

Main Contributions:

- Equivalence of neural net training to Lasso for low-dimensional problems significantly simplifies finding globally optimal networks.

- Analytical derivation of the Lasso dictionaries show how network function space evolves with depth and activation, e.g. how ReLU networks at depth 4 learn reflection features. 

- The Lasso problem also provides insight into neural net behavior (solution paths, network structure) under minimum regularization.

- Example applications show how the theory yields optimal neural networks in closed form for simple problems, and how it can be used to design neural nets for real datasets.

So in summary, the Lasso equivalence theory provides a new perspective on interpretability and trainability of neural networks for low dimensional problems.
