# [On the Diminishing Returns of Width for Continual Learning](https://arxiv.org/abs/2403.06398)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Deep neural networks suffer from catastrophic forgetting when trained sequentially on multiple tasks. Prior works have shown that increasing model width helps reduce forgetting, but the exact relationship has not been characterized, especially at very large widths. 

Proposed Solution:
- The authors develop a theoretical framework to analyze the continual learning error of feedforward networks. They prove that the error scales as O(t W^{-\beta} \alpha^{(1-2\beta)/2}), showing diminishing returns as width W increases.

- They assume wider models move less from initialization during training based on empirical observations. This acts as a functional regularizer, preventing models on new tasks from deviating too much from previous tasks.  

- The framework also relates forgetting to number of tasks t, depth L, and sparsity \alpha. Forgetting increases linearly with t and exponentially with L, while more sparsity (lower \alpha) reduces forgetting.

Key Contributions:
- Formalizes connection between width and continual learning error for nonlinear feedforward nets - shows diminishing returns at large widths

- Provides one of the first forgetting guarantees for variable depth nonlinear models 

- Relates forgetting to sparsity, number of tasks, depth - matches empirical observations

- Empirically verifies diminishing returns at widths much larger than prior works - up to 65,536 hidden units

- Framework roughly predicts other architecture trends like effects of depth, sparsity, number of tasks

In summary, the key contribution is a theoretical framework that for the first time formally characterizes the relationship between model width and continual learning capability, proving diminishing returns that are then verified empirically.
