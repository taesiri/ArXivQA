# [Triggering Multi-Hop Reasoning for Question Answering in Language Models   using Soft Prompts and Random Walks](https://arxiv.org/abs/2306.04009)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be:How can language models be improved at multi-hop reasoning and chaining together multiple facts to answer questions, when they already encode a lot of factual knowledge?The key hypothesis appears to be:Training language models on random walks over knowledge graphs can provide a useful signal to improve their ability to compose facts and perform multi-hop reasoning.In particular, the paper proposes and tests two main methods:1) Modular Prompts (PaTH): Using separate prompt modules to first parse questions into incomplete random walk queries, and then generate full paths to the answer. 2) Mixture Training: Jointly training a single prompt on a mixture of QA data and random walk data.The central goal is to trigger the language models' ability to chain factual knowledge that they already encode, in order to perform better logical reasoning and answer multi-hop questions. The key hypothesis is that learning to map questions to knowledge graph paths provides a useful training signal for this.In summary, the paper aims to improve language models' multi-hop reasoning abilities by learning to leverage their existing factual knowledge in a more compositional way, using random walks over knowledge graphs as a training signal.
