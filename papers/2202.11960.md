# [All You Need Is Supervised Learning: From Imitation Learning to Meta-RL   With Upside Down RL](https://arxiv.org/abs/2202.11960)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is:Can a single upside down reinforcement learning (UDRL) algorithm and architecture be effective across various reinforcement learning settings, including online RL, imitation learning, offline RL, goal-conditioned RL, and meta-RL?The key hypothesis is that by framing RL as a supervised learning problem and using an architecture that can handle arbitrary structured inputs/outputs, a single UDRL agent can learn effectively across different RL paradigms without needing specialized algorithms. The paper aims to demonstrate that the upside down approach, where the RL return is used as an input instead of an objective, allows bypassing issues like bootstrapping and off-policy corrections. Using a recurrent architecture and flexible input commands, the authors propose that the same general UDRL algorithm can work in online RL, imitation learning, offline RL, goal-conditioned RL, and meta-RL settings. The central research question is whether this unified UDRL approach can be effective across such diverse settings with a single algorithm and architecture.
