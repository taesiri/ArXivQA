# [Diffusion Meets DAgger: Supercharging Eye-in-hand Imitation Learning](https://arxiv.org/abs/2402.17768)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper "Diffusion Meets DAgger: Supercharging Eye-in-hand Imitation Learning":

Problem:
Imitation learning suffers from the compounding execution error problem - small errors made by the learned policy take it to out-of-distribution states, causing even bigger errors. The standard solution is to collect more data (DAgger), but this is impractical. 

Proposed Solution:
The paper proposes Diffusion Meets DAgger (DMD), a method to automatically generate synthetic data covering out-of-distribution states to augment the training data. This is done for eye-in-hand manipulation setups.

A conditional diffusion model is trained to generate perturbed views of a scene relative to an input view. This model uses a U-Net architecture with pose conditioning injected via cross-attention. The model is pretrained on internet data and finetuned on task data to generate realistic on-task views.

The augmented dataset is constructed by sampling random view perturbations and generating the corresponding views and actions using the positions of later frames in the sequence to avoid overshooting.

The original data and augmented synthetic data are combined to train robust manipulation policies.

Main Contributions:
- A method to automatically create synthetic images and action labels for out-of-distribution states encountered during eye-in-hand manipulation instead of manually collecting new data.

- Adoption of diffusion models rather than NeRFs to reliably generate images under scene deformation that occurs during manipulation.

- Careful design of image sampling and action labeling to avoid introducing conflicting labels.

- Experiments showing DMD can achieve 80% task success with just 8 expert demos vs 20% for behavior cloning, and outperforms prior pixel-level data augmentation techniques.
