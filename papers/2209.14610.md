# Dynamic Prompt Learning via Policy Gradient for Semi-structured   Mathematical Reasoning

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question it addresses is:How can we develop an AI system that can dynamically learn to select good prompt examples for few-shot learning of large language models like GPT-3, in order to improve performance on complex mathematical reasoning tasks involving both free text and tabular data?The key points are:- The paper proposes a new dataset called TabMWP for mathematical reasoning over free text and tabular data. This is more complex than existing math word problem datasets that use just free text.- The GPT-3 model shows strong capability on TabMWP in a zero-shot setting, but its few-shot performance varies a lot based on the prompt examples selected. - To address this instability, the paper proposes a novel method called PromptPG that uses reinforcement learning to dynamically learn how to select good prompt examples for few-shot GPT-3 on a given test case.- Experiments show PromptPG improves accuracy substantially over random prompt selection, resulting in new state-of-the-art performance on TabMWP.So in summary, the key research question is how to develop a technique like PromptPG to dynamically select good prompt examples in order to unlock strong few-shot performance from large language models on complex mathematical reasoning tasks over heterogeneous data modalities.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contributions are:1. The authors propose TabMWP, a new large-scale dataset for math word problems that require reasoning over both textual and tabular data. TabMWP contains 38,431 problems aligned with tabular contexts, making it the first dataset of its kind. 2. The authors evaluate strong baselines on TabMWP, including UnifiedQA, TAPEX, and GPT-3 models in zero-shot and few-shot settings. Experiments show the challenges of solving TabMWP compared to standard math word problem datasets that contain only text.3. To address the instability issue of few-shot learning with GPT-3, the authors propose a novel method called PromptPG that utilizes policy gradient reinforcement learning to learn how to select good prompt examples for few-shot GPT-3 on TabMWP.4. Experiments show PromptPG outperforms all baselines by a large margin on TabMWP. PromptPG reduces the variance of few-shot learning and achieves 68.23% accuracy, improving over few-shot GPT-3 by 5.31%.In summary, the main contribution is the proposal of TabMWP and PromptPG, which extends math word problem solving to tabular data and provides a more stable few-shot learning approach via policy gradient. The authors demonstrate state-of-the-art performance on the new TabMWP benchmark.
