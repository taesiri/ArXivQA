# [EfficientViT: Multi-Scale Linear Attention for High-Resolution Dense   Prediction](https://arxiv.org/abs/2205.14756)

## What is the central research question or hypothesis that this paper addresses?

 The central research question this paper aims to address is: 

How can we design vision transformer architectures that achieve strong performance on high-resolution dense prediction tasks while being efficient and fast on hardware?

Specifically, the paper introduces a new family of vision transformer models called EfficientViT that is optimized for high-resolution dense prediction tasks like semantic segmentation and super-resolution. 

The key hypotheses are:

- Using multi-scale linear attention instead of heavy softmax attention can enable global receptive field and multi-scale learning while being computationally efficient. 

- Enhancing linear attention with lightweight convolutions can improve its capacity for local feature extraction.

- The proposed EfficientViT module with multi-scale linear attention can outperform previous state-of-the-art vision transformers on high-resolution dense prediction tasks in terms of accuracy while being significantly faster on hardware like mobile CPUs and GPUs.

In summary, the central research question is how to design hardware-efficient vision transformers for high-resolution dense prediction by using multi-scale linear attention and lightweight convolutions. The key hypothesis is that the proposed EfficientViT architecture can achieve superior accuracy and speed trade-offs compared to previous state-of-the-art models.


## What is the main contribution of this paper?

 The main contributions of this paper are:

- It proposes a new multi-scale linear attention module for efficient high-resolution dense prediction. This module achieves global receptive field and multi-scale learning using only lightweight and hardware-efficient operations like linear attention and depthwise convolution.

- It introduces EfficientViT, a new family of vision transformer models built using the proposed multi-scale linear attention module, for efficient high-resolution dense prediction tasks.

- It demonstrates the effectiveness of EfficientViT on semantic segmentation, super-resolution, and Segment Anything tasks. EfficientViT provides significant speedups over prior SOTA models on diverse hardware platforms like mobile CPU, edge GPU, and cloud GPU without sacrificing accuracy.

- For example, on Cityscapes semantic segmentation, EfficientViT achieves up to 13.9x and 6.2x speedup over SegFormer and SegNeXt respectively on GPU while maintaining higher mIoU. For super-resolution, it delivers up to 6.4x acceleration over Restormer while improving PSNR.

In summary, the main contribution is proposing a novel multi-scale linear attention module and EfficientViT model family that enables efficient high-resolution dense prediction on hardware devices. The models achieve remarkable speedups over prior SOTA without sacrificing accuracy.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the paper:

The paper introduces EfficientViT, a new family of vision transformer models for efficient high-resolution dense prediction that achieves global receptive fields and multi-scale learning using lightweight linear attention and outperforms previous state-of-the-art models while being significantly faster on diverse hardware platforms.
