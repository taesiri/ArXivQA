# Bridging the Gap: Exploring the Capabilities of Bridge-Architectures for
  Complex Visual Reasoning Tasks

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research questions seem to be:1) Does incorporating object level local features help bridge architectures in performing complex visual reasoning tasks like NLVR2?2) Is visuo-linguistic pretraining necessary for good performance on multimodal tasks like NLVR2 that require complex reasoning? 3) Is multimodal instruction finetuning necessary for bridge architectures to perform well on complex reasoning tasks?The key hypothesis appears to be that incorporating object-level local features and visuo-linguistic pretraining will improve the performance of bridge architectures on complex visual reasoning tasks like NLVR2. The authors seem to investigate this by adding object-level features from models like SAM to bridge architectures, and analyzing the performance with and without visuo-linguistic pretraining. They also study the impact of multimodal instruction finetuning on models like LLaVA.Through their experiments and analysis, the authors aim to demonstrate that object-level features and pretraining are important for bridge architectures to work well on complex visual reasoning tasks that require fine-grained understanding of images and text. The instruction finetuning is also hypothesized to be beneficial.In summary, the central questions focus on understanding how to enable bridge architectures to perform well on complex multimodal reasoning tasks through adding object-level visual features and visuo-linguistic pretraining. The role of instruction finetuning is also analyzed.


## What is the main contribution of this paper?

The main contributions of this paper are:- The authors introduce and explore the capabilities of "bridge architectures" for complex visual reasoning tasks. Bridge architectures project image features into the text space of large language models (LLMs) using a linear mapping, to leverage the reasoning abilities of LLMs. - They analyze the performance of bridge architectures on the NLVR2 dataset, which requires complex reasoning over image pairs and text. They augment bridge architectures with object-level features from segmentation models to facilitate fine-grained reasoning.- Through experiments, they demonstrate that simply adding object features to bridge architectures does not help much. Pre-training on multi-modal data seems key for good performance on complex tasks like NLVR2. - They show initial results analyzing the recently proposed LLaVA bridge architecture on NLVR2. Using chain of thought prompting improves LLaVA's zero-shot reasoning abilities. But there is still a large gap compared to state-of-the-art finetuned models, indicating the importance of multi-modal pre-training.In summary, the key contribution is an analysis of bridge architectures on a complex visual reasoning task. The results demonstrate these models' limitations currently, and the importance of multi-modal pre-training for strong performance. The paper also provides insights into how bridge architectures could be improved.
