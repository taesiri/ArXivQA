# [Language-Guided Music Recommendation for Video via Prompt Analogies](https://arxiv.org/abs/2306.09327)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is:How can we recommend music for a video that matches both the visual content/style of the video as well as desired musical attributes (genre, mood, instrumentation, etc.) specified by the user in free-form natural language text?The key hypothesis is that by fusing representations of the input video and natural language text, the model can learn to retrieve music that agrees with both modalities.In more detail:- The paper proposes a model called ViML that takes as input a video and free-form text description, and retrieves a relevant music track from a database. - A key challenge is that existing datasets provide (video, music) pairs but lack textual descriptions of the music. The paper introduces a text synthesis method to generate descriptions using a music tagger and language model.- The ViML model fuses video and text representations using a Transformer architecture. A text dropout regularization method is introduced which is shown to be critical for good performance.- The paper collects a new evaluation dataset with text annotations for video clips to evaluate language-guided retrieval.- Experiments show the ViML model can match or exceed prior video-to-music models, while significantly improving retrieval when using text guidance.In summary, the central hypothesis is that fusing video and text representations allows retrieving music that agrees with both modalities, which is evaluated empirically. The key research problems are generating textual music descriptions from audio tags, and effectively fusing video and text representations for retrieval.
