# [DEE: Dual-stage Explainable Evaluation Method for Text Generation](https://arxiv.org/abs/2403.11509)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
Existing methods for automatically evaluating machine-generated texts have key weaknesses:
- Limited evaluation dimensions - focus only on fluency and consistency, not emerging issues like hallucinations, biases, toxicity.  
- Lack of explainability - provide only quantitative scores without detailed analysis of error types and causes.  
- Inefficiency - methods relying on large language models have high latency, unsuitable for real-time applications.

Solution - DEE Method:
The paper proposes a Dual-stage Explainable Evaluation (DEE) method to address these limitations. 

Key ideas:
- Leverages Llama model and an elaborately assembled real-world dataset AntEval.
- AntEval covers comprehensive error categories including reliability, bias/toxicity and basic errors in fluency. 
- Adopts a dual-stage approach guided by stage-specific instructions:
   - Stage I: Swift detection and categorization of principle error types.
   - Stage II: In-depth explainable analysis of each identified error.
- Fine-tuned objectives and dataset enable efficiency and broad error coverage.

Main Contributions:
- Introduces an innovative dual-stage text evaluation method combining efficiency and explainability.
- Assembles a real-world dataset AntEval spanning diverse and comprehensive error categories to facilitate advanced text evaluation.
- Experiments demonstrate DEE's state-of-the-art performance in efficiency, human correlation and error coverage compared to existing methods.
- Provides a robust text evaluation solution suitable for industrial applications.

In summary, the paper presents a novel dual-stage approach and dataset that enables efficient yet explainable evaluation of machine-generated text over a broad range of error categories. Experiments verify the effectiveness of this method.
