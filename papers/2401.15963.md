# [NoFunEval: Funny How Code LMs Falter on Requirements Beyond Functional   Correctness](https://arxiv.org/abs/2401.15963)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
Existing benchmarks for evaluating code language models (LMs) focus almost exclusively on the ability of LMs to generate functionally correct code. However, in real-world software engineering, developers have requirements beyond just functional correctness, such as efficiency, security, and maintainability (known as non-functional requirements). There is a gap in evaluating whether code LMs can edit code to satisfy non-functional requirements and also comprehend code semantics sufficiently to be trusted by developers.

Proposed Solution:
The authors propose a new benchmark called NoFunEval to evaluate code LMs on:

1. Editing code to satisfy non-functional requirements like latency, resource utilization, runtime efficiency, maintainability and security. This is the NoFunEdit task.

2. Classifying code snippets based on non-functional properties (NoFunClassify) and based on functional correctness (HumanEvalClassify).

The benchmark uses 958 problem instances sourced from public repositories and existing datasets across multiple programming languages. 

To communicate domain knowledge required for non-functional edits, the authors design a new prompting strategy called Coding Concepts (CoCo) which allows developers to provide high-level directions to the LMs on programming concepts relevant for the edits.

The paper conducts an extensive evaluation of 22 recent code LMs on NoFunEval.

Key Contributions:

1. Identifying the almost exclusive focus on functional correctness in current code LM evaluations and proposing the NoFunEval benchmark to assess comprehension and editing abilities for non-functional requirements.

2. Observing through experiments that code LMs struggle on NoFunEdit compared to their ability to edit for functional correctness, hinting at blindspots in their training.

3. Discovering through NoFunClassify and HumanEvalClassify that code comprehension ability of LMs is surprisingly poor compared to their editing ability. LMs can successfully edit code even when they fail to distinguish between correct and incorrect code.

4. Releasing the NoFunEval benchmark to enable future research towards code LMs that can satisfy non-functional requirements like efficiency, security etc. in addition to functional correctness.

In summary, the paper makes an important contribution in highlighting blindspots of current code LMs beyond functional correctness through the release of a comprehensive benchmark suite and an extensive evaluation.


## Summarize the paper in one sentence.

 This paper proposes a new benchmark, NoFunEval, to evaluate code language models on non-functional requirements and comprehension tasks beyond just generating functionally correct code.


## What is the main contribution of this paper?

 The main contributions of this paper are:

1) Identifying the almost exclusive focus on functional correctness in existing evaluation benchmarks of code language models (LMs), and introducing a new benchmark, NoFunEval, to evaluate non-functional requirements and comprehension abilities.

2) Preparing the NoFunEval benchmark with tasks for editing code based on non-functional requirements like latency, resource utilization, efficiency etc. as well as classification tasks to test comprehension of both functional and non-functional properties.

3) Conducting an extensive evaluation of 22 code LMs, finding that they generally struggle on NoFunEval's tasks compared to their performance on functional correctness tasks, hinting at blindspots in their training.

4) Observing through NoFunEval that code LMs have better generative abilities compared to their comprehension abilities - they can often edit code correctly but fail to classify/comprehend the same code.

5) Releasing the NoFunEval benchmark and evaluation scripts publicly to facilitate further research.

In summary, the main contribution is identifying the gap in existing benchmarks and introducing a new benchmark focused on non-functional requirements and comprehension to reveal limitations of current code LMs.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper summary, some of the key terms and concepts associated with this paper include:

- Non-functional requirements - The paper focuses on evaluating code language models on requirements beyond just functional correctness, such as efficiency, security, maintainability etc.

- NoFunEval benchmark - The proposed benchmark suite to test code LMs on non-functional tasks and comprehension abilities.

- Editing and classification tasks - The benchmark contains tasks for editing code to meet non-functional goals as well as classifying code snippets based on non-functional properties. 

- Prompting strategies - Different prompting methods like base prompts, few-shot examples, chain of thought, and "coding concepts" prompts are explored to communicate requirements.

- Model evaluation - An extensive evaluation of 22 recent code LMs is conducted, spanning different model sizes, training strategies and tuning methods.

- Comprehension limitations - Key finding is code LMs struggle with editing for non-functional requirements and have poor comprehension, despite strengths in functional code generation.

- Future work - The authors plan to expand the benchmark with more languages, examples and evaluation setups.

In summary, the key terms cover non-functional requirements, the NoFunEval benchmark, editing/classification tasks, prompting strategies, model evaluation, and limitations in comprehension - highlighting gaps that need to be addressed as code LMs are adopted.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 potential in-depth questions about the method proposed in this paper:

1. The paper proposes a new benchmark called NoFunEval to test code language models on non-functional requirements and comprehension tasks. What are some examples of the non-functional requirements evaluated in NoFunEval? How were the comprehension tasks designed?

2. The paper introduces a prompting strategy called "Coding Concepts" (CoCo) to allow developers to communicate domain knowledge to language models. How does this strategy work? What type of information can be provided through Coding Concepts prompts? 

3. The paper finds that language models struggle on NoFunEval's editing tasks compared to their performance on functional correctness tasks like HumanEvalFix. What explanations are provided for this gap in performance? How could the training procedures of language models be improved?  

4. For the runtime efficiency task in NoFunEvalEdit, the paper observes that models often improve runtime at the cost of functional correctness. What could explain this behavior? How can models be prevented from compromising functionality for optimization?

5. What metrics are used to evaluate model performance on the different NoFunEval tasks? What are some limitations of these metrics? How could more robust metrics be designed for non-functional requirements?

6. The 1-shot prompts are found to sometimes perform worse than the base prompts. What explanations are provided for this counterintuitive result? How could 1-shot prompting be improved in future work?

7. What trends are observed when comparing models' editing abilities to their comprehension abilities on HumanEval classification tasks? What might explain models' poor comprehension despite successful editing?

8. How diverse is the data sourced for the NoFunEval benchmark in terms of languages, code lengths, weakness types, etc.? What data limitations impacted the benchmark design and should be addressed in future work?

9. The paper identifies a discrepancy between DeepSeekCoder-33B-Instruct's high HumanEvalFix accuracy and low HumanEvalClassify accuracy. Should HumanEval generation metrics be re-evaluated if comprehension is found lacking?

10. What real-world software engineering scenarios could benefit from progress on non-functional requirements and comprehension? How might NoFunEval evolve to stay relevant as models continue to advance?
