# [Differentiable DAG Sampling](https://arxiv.org/abs/2203.08509v1)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the main research question seems to be:

How can we develop a fast and differentiable probabilistic model over DAGs for learning causal structures from observational data?

The key points are:

- The paper proposes a new probabilistic model over DAGs called DP-DAG that allows fast and differentiable sampling of valid DAGs. 

- This is achieved by decomposing the DAG sampling into (1) sampling a permutation over the nodes and (2) sampling a triangular adjacency matrix consistent with the permutation. The sampling uses Gumbel-based distributions.

- The paper combines DP-DAG with variational inference into a method called VI-DP-DAG for learning DAGs and causal mechanisms from observational data. 

- VI-DP-DAG aims to approximate the posterior distribution over DAG structures given the observed data. It guarantees valid DAG outputs during training without complex augmented Lagrangian optimization schemes.

- Experiments show VI-DP-DAG outperforms previous differentiable DAG learning methods on structure learning and causal mechanism learning, while training significantly faster.

So in summary, the main research question is how to develop a fast, differentiable probabilistic model over DAGs that can enable high-quality causal structure learning from observational data. DP-DAG and VI-DP-DAG are proposed as solutions.


## What is the main contribution of this paper?

 Based on my reading of the paper abstract, the main contributions seem to be:

1. Proposing a new differentiable probabilistic model over DAGs called DP-DAG that allows fast and differentiable DAG sampling suited for continuous optimization. DP-DAG samples a DAG by sampling a node ordering with Gumbel-Sinkhorn or Gumbel-Top-k, and then sampling edges consistent with that ordering using Gumbel-Softmax.

2. Proposing VI-DP-DAG, a new method for DAG learning that combines DP-DAG with variational inference to approximate the posterior distribution over DAGs given observational data. VI-DP-DAG guarantees valid DAG outputs during training without needing complex Lagrangian optimization. 

3. Showing experimentally that VI-DP-DAG outperforms other differentiable DAG learning methods on both synthetic and real datasets for DAG structure learning and recovering causal mechanisms, while training significantly faster.

In summary, the main contribution seems to be proposing a new differentiable probabilistic model over DAGs that enables fast and high-quality DAG learning through variational inference, without relying on complex constrained optimization techniques. The model is shown to outperform existing methods empirically.
