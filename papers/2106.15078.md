# Don't Take It Literally: An Edit-Invariant Sequence Loss for Text   Generation

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is:How can we develop an edit-invariant sequence loss function for text generation that is robust to minor edits in the target sequence?The key points are:- The paper proposes a new loss function called EISL (Edit-Invariant Sequence Loss) for text generation tasks like machine translation, text summarization, and grammatical error correction. - Existing sequence loss functions like cross-entropy treat any change in the target sequence as a mistake. But minor edits like rephrasing or synonym substitution should not be heavily penalized.- EISL aims to make the loss function more robust to such minor edits in the target sequence. It does this by minimizing the edit distance between the generated and target sequences, allowing for small edits.- The edit distance is efficiently approximated using a learned edit proposal distribution and importance sampling.- Experiments across the three tasks show EISL improves performance over cross-entropy and other recent robust losses. The generated sequences better preserve the meaning while allowing small edits.So in summary, the key hypothesis is that an edit-invariant loss will improve text generation by avoiding over-penalization of minor target edits. EISL is proposed to achieve such a loss in an efficient way.


## What is the main contribution of this paper?

The main contribution of this paper is proposing a new loss function called the edit-invariant sequence loss for text generation tasks. The key ideas are:- They design an edit-invariant distance to measure the similarity between two sequences robust to token-level edits like insertion, deletion, and replacement. This distance is based on the shortest edit path between two sequences.- They incorporate this edit-invariant distance into a new sequence loss function for training text generation models like neural machine translation and text summarization. - The new loss function encourages the model to generate sequences that preserve the original meaning even with small edits, rather than forcing the model to reproduce the exact target sequence.- They demonstrate the effectiveness of the proposed loss on machine translation, summarization, and parsing tasks. Using the edit-invariant loss improves performance over standard likelihood training and other sequence losses.In summary, the main contribution is proposing a novel training loss for text generation that is robust to small token edits in the target sequence. This improves the meaning preservation and overall quality of generated texts. The edit-invariant sequence loss provides a new way to bridge the gap between training targets and desired outputs.
