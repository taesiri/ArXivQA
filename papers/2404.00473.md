# [Privacy Backdoors: Stealing Data with Corrupted Pretrained Models](https://arxiv.org/abs/2404.00473)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Sharing of large pretrained machine learning models is common today. However, this introduces new privacy risks from supply chain attacks if the models contain malicious backdoors.
- Prior work studied backdoors for integrity attacks. This paper introduces "privacy backdoors" where model weights are tampered with to compromise the privacy of future finetuning data.

Proposed Solution:
- The paper proposes "data traps" - backdoors that directly write individual finetuning examples into the model weights during training. 
- A new "single-use" backdoor design is introduced that shuts down after one activation to prevent altering captured weights further during finetuning.
- Backdoors are designed for MLPs, ViT, and BERT models. They leverage position and sequence keys to capture full inputs rather than individual tokens.
- Attacks enable reconstruction of dozens of finetuning images and sentences, with minimal impact on model utility.

Main Contributions:
- First demonstration of supply chain privacy backdoors in pretrained models that can reconstruct individual finetuning inputs.
- New "data trap" backdoor designs that survive an entire finetuning process and provide guaranteed input recovery.
- Application to transformers (ViT, BERT) using sequence and position keys to capture complete inputs.  
- Black-box membership inference and data extraction attacks using these backdoors.
- First tight end-to-end attack on differential privacy (DP-SGD) by using backdoors to create worst-case DP assumptions.

Overall, the paper highlights a new threat model for ML supply chain attacks focused on training data privacy rather than just model integrity. It shows pretrained models can be easily backdoored to reveal private finetuning data.


## Summarize the paper in one sentence.

 This paper introduces "privacy backdoors", a new type of backdoor attack where a malicious model provider tampers with a pretrained model's weights to compromise the privacy of future finetuning data. The authors show how to build such backdoors into transformers, MLPs, and CNNs, enabling reconstruction of individual finetuning examples from reading the model weights (white-box attack) or querying the model (black-box attack).


## What is the main contribution of this paper?

 The main contribution of this paper is introducing "privacy backdoors", a new type of backdoor attack targeting the privacy of finetuning data when using a shared pretrained model. Specifically:

- The paper shows how an attacker can tamper with a pretrained model's weights to create "data traps" that directly write some finetuning data points to the model weights. This enables the attacker to later extract the trapped data by reading the finetuned model's weights.

- The attack is designed to be robust and survive an entire finetuning run with multiple training epochs. This is achieved using "single-use" backdoors that become inactive after capturing an input, to prevent alteration of the weights subsequently.

- The attack is demonstrated on various models, including MLPs, ViT, and BERT, enabling reconstruction of dozens of finetuning examples. Extensions enable perfect black-box membership inference and data extraction attacks.

- The paper shows the attack also enables tight end-to-end privacy attacks on models trained with differential privacy, challenging common assumptions that DP guarantees are loose in practice.

In summary, the main contribution is introducing and demonstrating a new threat to privacy when using untrusted shared models, enabled by tampering with the model's weights. The paper emphasizes the need for better protections in the modern ML supply chain.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper abstract and keywords, the main keywords or key terms associated with this paper are:

Machine Learning, ICML, Privacy, Backdoors, Supply Chain Attacks, Data Traps, Pretrained Models, Finetuning, Transformers, Differential Privacy

The paper introduces the concept of "privacy backdoors" - a way to tamper with pretrained machine learning models so that when they are finetuned on private data, individual training examples can be reconstructed from the finetuned weights. The paper shows attacks targeting the privacy of finetuning data for transformers and MLPs. It also shows an application to mount tight privacy attacks on models trained with differential privacy. The attacks exploit the modern machine learning supply chain, where pretrained models are widely shared online then finetuned on private data. So the main themes are around privacy, backdoors, finetuning, and supply chain attacks in machine learning.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the privacy backdoor method proposed in the paper:

1. The paper mentions using "keyed" backdoors to capture full input sequences rather than individual tokens from different inputs. How exactly does this keyed backdoor mechanism work? What are the challenges in getting it to function properly during finetuning?

2. The paper discusses the difficulty of propagating backdoor signals through many layers of transformers without attenuation while also maintaining model utility. What specific techniques and modules did the authors employ to achieve this delicate balance? 

3. How does the paper deal with the non-linearities introduced by activations like GELU and layer normalizations? What kinds of numerical tricks and modifications were made to handle these complexities?

4. The single-use "data trap" backdoor shuts down after one activation through injected gradient signals. However, what can go wrong if that initial gradient signal has the wrong sign? How does the paper mitigate this risk?  

5. When extracting patch-level information from vision transformers, what mechanisms enable capturing patches from many different input images rather than just a few full images? How is this achieved?

6. For the black-box membership inference attack, how exactly does the backdoor construction allow determining with 100% accuracy whether a data point was used during training or not? 

7. When attacking differentially private SGD, how does the backdoor instantiation empirically achieve a privacy leakage very close to the theoretical worst-case bound? What assumptions does this rely on?

8. The paper mentions the difficulty of keeping backdoors stable during training as weight updates can disrupt the adversarial behavior. What causes this drift and how is it dealt with?

9. When stealing full input sequences, the paper uses separate position and sequence keys in conjunction with backdoors. What is the purpose of each of these components and how do they interact?

10. For black-box model stealing attacks, what modifications to existing techniques enable recovering backdoored patch or token information from transformers given only query access?
