# [Learning to Plan for Language Modeling from Unlabeled Data](https://arxiv.org/abs/2404.00614)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Large language models (LLMs) have shown impressive capabilities in tasks like summarization and translation through self-supervised next-token prediction pretraining. However, their objective limits performance in scenarios requiring more deliberate planning, like writing a coherent article.

Method: 
- Proposes a method to train an external module that can plan the future writing process via self-supervised learning on unlabeled data. 
- Transforms unlabeled text into sequences of abstract "writing actions" obtained by clustering sentence embeddings.
- Trains a "planner" network to predict next writing action from current context.
- Finetunes LLM by conditioning it on predicted writing actions via an adapter-style module.

Main Findings:
- Conditioning the LLM on predicted writing actions from the planner improves language modeling performance, especially in anticipating text structure.
- The external planner outperforms an LLM-internal planner.
- Writing actions often correspond to structures typically found in articles, e.g. sections on origin, education, career, etc.

Key Benefits:
- Method utilizes self-supervised learning on unlabeled data to improve planning abilities. Enables leveraging more data.
- Modular framework allows independent development of planner module. New planners can be shared.
- More complex planning possible in future, e.g. multi-step prediction.

In summary, the paper proposes an approach to learn a writing planning module in a self-supervised way on unlabeled data, which can then be used to enhance deliberate reasoning and text structure capabilities of language models. The method shows improved language modeling performance.
