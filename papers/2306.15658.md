# [CLIPA-v2: Scaling CLIP Training with 81.1% Zero-shot ImageNet Accuracy   within a \$10,000 Budget; An Extra \$4,000 Unlocks 81.8% Accuracy](https://arxiv.org/abs/2306.15658)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is:How can we train high-performance CLIP models more efficiently, achieving state-of-the-art accuracy with significantly reduced computational cost? Specifically, the paper builds on previous work called CLIPA and makes two key contributions:1. Validating that an "inverse scaling law" also applies during finetuning, allowing further reduction of computational needs by using fewer input tokens for larger models. 2. Scaling up experiments to train larger CLIP models with more data and longer schedules, exploring performance with budgets up to $14,000. The goal is to train CLIP models that surpass previous state-of-the-art accuracy (e.g. from OpenCLIP) but with much lower training cost. For example, they achieve 81.1% ImageNet accuracy within a $10,000 budget, reducing cost 39x compared to prior best model. The central hypothesis is that their techniques will enable more efficient high-performance CLIP training.


## What is the main contribution of this paper?

The main contributions of this paper are:1. Validating that the "inverse scaling law" for CLIP training proposed in CLIPA also applies during the finetuning stage. This enables further reduction of computational costs when finetuning CLIP models. 2. Scaling up CLIPA training across multiple dimensions - using larger datasets (up to LAION-2B and DataComp-1B), larger models (up to H/14), and longer training schedules (up to 13B image-text pairs). 3. Achieving state-of-the-art zero-shot ImageNet accuracy of 81.1% within a $10,000 compute budget, surpassing prior best CLIP model from OpenCLIP by 1%. Further investment of $4,000 leads to 81.8% accuracy.4. Demonstrating the efficiency and scalability of CLIPA training. The CLIPA-v2 models outperform OpenCLIP counterparts with 39x less compute.In summary, this paper shows CLIPA training can scale to large datasets and models efficiently to achieve SOTA zero-shot performance at low compute budgets, enabling wider exploration and adoption of advanced CLIP models. The open-sourced models and code will facilitate future CLIP research.
