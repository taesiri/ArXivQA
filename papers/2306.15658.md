# [CLIPA-v2: Scaling CLIP Training with 81.1% Zero-shot ImageNet Accuracy   within a \$10,000 Budget; An Extra \$4,000 Unlocks 81.8% Accuracy](https://arxiv.org/abs/2306.15658)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is:How can we train high-performance CLIP models more efficiently, achieving state-of-the-art accuracy with significantly reduced computational cost? Specifically, the paper builds on previous work called CLIPA and makes two key contributions:1. Validating that an "inverse scaling law" also applies during finetuning, allowing further reduction of computational needs by using fewer input tokens for larger models. 2. Scaling up experiments to train larger CLIP models with more data and longer schedules, exploring performance with budgets up to $14,000. The goal is to train CLIP models that surpass previous state-of-the-art accuracy (e.g. from OpenCLIP) but with much lower training cost. For example, they achieve 81.1% ImageNet accuracy within a $10,000 budget, reducing cost 39x compared to prior best model. The central hypothesis is that their techniques will enable more efficient high-performance CLIP training.
