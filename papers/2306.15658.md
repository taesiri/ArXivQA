# [CLIPA-v2: Scaling CLIP Training with 81.1% Zero-shot ImageNet Accuracy   within a \$10,000 Budget; An Extra \$4,000 Unlocks 81.8% Accuracy](https://arxiv.org/abs/2306.15658)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is:How can we train high-performance CLIP models more efficiently, achieving state-of-the-art accuracy with significantly reduced computational cost? Specifically, the paper builds on previous work called CLIPA and makes two key contributions:1. Validating that an "inverse scaling law" also applies during finetuning, allowing further reduction of computational needs by using fewer input tokens for larger models. 2. Scaling up experiments to train larger CLIP models with more data and longer schedules, exploring performance with budgets up to $14,000. The goal is to train CLIP models that surpass previous state-of-the-art accuracy (e.g. from OpenCLIP) but with much lower training cost. For example, they achieve 81.1% ImageNet accuracy within a $10,000 budget, reducing cost 39x compared to prior best model. The central hypothesis is that their techniques will enable more efficient high-performance CLIP training.


## What is the main contribution of this paper?

The main contributions of this paper are:1. Validating that the "inverse scaling law" for CLIP training proposed in CLIPA also applies during the finetuning stage. This enables further reduction of computational costs when finetuning CLIP models. 2. Scaling up CLIPA training across multiple dimensions - using larger datasets (up to LAION-2B and DataComp-1B), larger models (up to H/14), and longer training schedules (up to 13B image-text pairs). 3. Achieving state-of-the-art zero-shot ImageNet accuracy of 81.1% within a $10,000 compute budget, surpassing prior best CLIP model from OpenCLIP by 1%. Further investment of $4,000 leads to 81.8% accuracy.4. Demonstrating the efficiency and scalability of CLIPA training. The CLIPA-v2 models outperform OpenCLIP counterparts with 39x less compute.In summary, this paper shows CLIPA training can scale to large datasets and models efficiently to achieve SOTA zero-shot performance at low compute budgets, enabling wider exploration and adoption of advanced CLIP models. The open-sourced models and code will facilitate future CLIP research.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper introduces CLIPA-v2, an improved version of CLIPA that achieves state-of-the-art zero-shot ImageNet accuracy of 81.8% while significantly reducing training costs, demonstrating the effectiveness of an "inverse scaling law" where larger CLIP models can be trained with fewer input tokens.


## How does this paper compare to other research in the same field?

Here are a few key points about how this paper compares to other CLIP research:- This paper builds on the previous work CLIPA, extending it by validating the inverse scaling law also applies during finetuning and exploring training CLIPA models at much larger scale. - The main contribution is demonstrating very strong CLIP models can be trained with significantly lower compute budgets than prior work like OpenCLIP. For example, they achieve 81.1% ImageNet accuracy within a $10k budget, compared to 80.1% for OpenCLIP with nearly $250k budget.- Scaling up CLIPA to the H/14 model and finetuning techniques like progressive resolution increase enables reaching 81.8% ImageNet accuracy, the new SOTA for CLIP at this budget scale.- Compared to other work like BLIP, Florence, ALIGN, etc. that also aim to train efficient CLIP models, this paper stands out in pushing accuracy extremely high given a strict budget limit. The tradeoffs are less exploration on model architecture changes.- For robustness evaluations like ImageNetV2, ImageNet-A, etc. their best CLIPA model also sets new SOTA results, showing the gains translate to robust accuracy too.- One limitation is retrieval results on COCO/Flickr30k still favor OpenCLIP. The authors posit this is due to OpenCLIP pretraining on a more retrieval-centric dataset.In summary, this paper pushes the state-of-the-art in training high-accuracy Clip models at low compute budgets, setting new benchmarks for Clip performance within $10k and $15k costs. The techniques should enable more researchers to work with large Clip models.


## What future research directions do the authors suggest?

Based on my reading of the paper, some key future research directions suggested by the authors include:- Further exploring the inverse scaling law for CLIP training. The authors show this law applies in both pretraining and finetuning stages, but more work could be done to understand the theoretical basis and boundaries of this phenomenon.- Scaling up CLIPA to even larger models and datasets. The authors demonstrate strong performance with the H/14 model, but going bigger could lead to further gains. Expanding the training data beyond the LAION and DataComp datasets is another opportunity.- Improving zero-shot retrieval performance. The authors note their models underperform OpenCLIP on COCO and Flickr30k retrieval, likely due to different training data. Finding ways to boost retrieval results, perhaps via different data or losses, is suggested. - Applying CLIPA to new domains and tasks. The authors focus on ImageNet classification but CLIP has many applications. Testing CLIPA in areas like vision-language, robotics, etc. is proposed.- Combining CLIPA with other techniques like distillation. The authors use basic CLIP training; integrating CLIPA with more advanced methods could further enhance efficiency.- Deploying efficient CLIP models in production systems. The authors produce highly performant yet inexpensive CLIP models amenable to real-world deployment.In summary, the key future directions center on better understanding the inverse scaling law, scaling CLIPA up further, boosting zero-shot retrieval, applying CLIPA to new domains/tasks, integrating other techniques, and deploying the efficient models in practice.
