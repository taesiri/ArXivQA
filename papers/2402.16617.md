# [Long-Context Language Modeling with Parallel Context Encoding](https://arxiv.org/abs/2402.16617)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Extending the context window of large language models (LLMs) is crucial for complex tasks like summarizing books or answering questions with many retrieved documents. 
- However, transformers have quadratic computational complexity with sequence length and positional encodings don't generalize beyond seen lengths. 
- High-quality long-context data like instruction-following is also scarce.

Proposed Solution:
- The authors propose Context Expansion with Parallel Encoding (CEPE), a lightweight framework to extend context for any LLM decoder.  
- CEPE adds a small encoder to chunk and encode long context in parallel and cross-attention layers in the decoder to attend to encoder representations.
- CEPE only trains the encoder and cross attention, keeping the decoder frozen, reducing cost.

Main Contributions:
- CEPE generalizes to 128K tokens with an 8K training length and extends LLaMA's context 10x, with 1/6 the memory and higher throughput.
- It improves performance on long-context LM, retrieval-augmented LM, open-domain QA, and leveraging more demonstrations for in-context learning.
- CEPE-Distilled extends instruction-tuned models like LLaMA-2-Chat using unlabeled data and an auxiliary KL loss between student and teacher. 
- It improves LLaMA-2-Chat's performance on long-document tasks while preserving its instruction understanding ability.
- CEPE provides an efficient way to empower LLMs with long-context abilities for downstream applications.
