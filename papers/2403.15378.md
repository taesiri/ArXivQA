# [Long-CLIP: Unlocking the Long-Text Capability of CLIP](https://arxiv.org/abs/2403.15378)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper "Long-CLIP: Unlocking the Long-Text Capability of CLIP":

Problem:
CLIP is a powerful vision-language model that aligns image and text modalities based on contrastive learning. However, it has two major limitations:
1) The input text length is restricted to only 77 tokens due to the use of absolute positional embeddings. The effective length is even shorter, around 20 tokens.
2) CLIP struggles to accurately capture fine-grained details and relationships between attributes in an image. It uses a "bag of concepts" approach which can cause mistakes.

Proposed Solution:
The paper proposes Long-CLIP to unlock the long-text capability of CLIP while retaining its alignment and zero-shot generalization ability. The key ideas are:

1) Knowledge-preserved stretching of positional embeddings - Interpolate embeddings beyond 20 tokens (identified as effective length) using a larger ratio to minimize disruption.

2) Primary component matching - Extract both fine-grained and coarse-grained features from the image. Align fine-grained features to long text and coarse-grained features to short summary text. This retains both long and short text capabilities.

Main Contributions:
- Efficiently fine-tunes CLIP using 1M extra text-image pairs to support over 240 tokens input length.

- Achieves significantly better performance on long caption retrieval (25% higher recall) and minor gains on short caption retrieval (6% higher recall) over CLIP.

- Retains CLIP's zero-shot classification accuracy. Can also directly replace CLIP in downstream models like image generators.

- Analyzes reasons for CLIP's limitations and provides strategies to overcome them through novel positional embedding stretching and multi-level alignment techniques.

The proposed Long-CLIP unlocks paragraph-level capability for CLIP while maintaining efficiency, zero-shot generalization and alignment to CLIP latent space in a plug-and-play manner.
