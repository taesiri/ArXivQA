# [When in Doubt, Think Slow: Iterative Reasoning with Latent Imagination](https://arxiv.org/abs/2402.15283)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
Model-based reinforcement learning (RL) agents rely on the accuracy of their learned world model when deployed in unfamiliar environments. This can limit their performance without additional training. The paper proposes enhancing the reasoning abilities of such agents by improving their state representations at decision-time, without further training.

Proposed Solution: 
The paper introduces a method for iterative inference (II) that refines the agent's representation of the current state based on the expected coherence with future states. This is done by using the agent's world model to sample possible future state trajectories, then optimizing the current state to maximize the evidence lower bound of these imaginary rollouts. Two objectives are proposed based on minimizing state and parameter uncertainty respectively. The iterative updates are applied directly to the agent's internal state, rather than being amortized.

The method is evaluated on DreamerV3 in visual navigation tasks, across different rollout lengths. Performance metrics include reconstruction accuracy of observations and overall episodic task scores. The impact is also analyzed on agents with varying amounts of pre-training.

Main Contributions:
- A training-free approach to improve model-based RL agents using iterative inference and latent imagination at decision-time.
- Thorough analysis of two inference objectives over different rollout depths in visual RL environments.  
- Consistent boosts in reconstruction accuracy and task performance, especially in partially observable settings and for less trained agents.
- Demonstration that considering more future states leads to greater changes in overall agent behavior rather than just the current state estimate.

In summary, the paper presents a novel way of enhancing model-based RL agents with system 2-like reasoning abilities by optimizing internal representations for coherence with imaginary future states. The method is shown to be generally applicable with consistent improvements across metrics and environments.
