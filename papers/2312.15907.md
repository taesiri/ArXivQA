# [Align on the Fly: Adapting Chatbot Behavior to Established Norms](https://arxiv.org/abs/2312.15907)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem: 
- Large language models (LLMs) often exhibit unintended behaviors due to misaligned objectives during training. It is important to align them with human values and intentions before deployment.  
- However, some human values like social norms are ever-changing, complex and diverse across different times and places. It is challenging to internalize such intricate values into LLM parameters using existing alignment techniques like supervised fine-tuning.

Proposed Solution:
- The paper proposes an "On-the-fly Preference Optimization (OPO)" method to align LLMs with dynamic human values in real-time without further training. 
- It uses an external memory to store established rules reflecting human values. A retriever selects relevant rules based on the input query to constrain LLM behaviors. This allows easy updates to the values.

Key Contributions:
- Proposes the OPO framework with a rule creation module, alignment module using retrieval, and an evaluation module.
- Constructs a corpus with 957,778 legal and moral rules to guide LLM behaviors. 
- Collects 4,447 human-labeled questions and proposes a method to auto-generate 675 additional questions for comprehensive evaluation.
- Evaluates 15 LLMs on 5 test sets. Results demonstrate OPO can consistently improve alignment for most models, especially stronger LLMs like GPT-4.
- OPO is training-free, reward-free, LLM-agnostic and allows easy value updating compared to supervised fine-tuning or reinforcement learning.

In summary, the paper addresses the dynamic complexity of human values for AI alignment through an on-the-fly preference optimization method using retrieval over an editable external memory of established rules. Comprehensive experiments demonstrate its effectiveness.
