# [Tacotron: Towards End-to-End Speech Synthesis](https://arxiv.org/abs/1703.10135)

## What is the central research question or hypothesis that this paper addresses?

 The central research question this paper addresses is how to develop an end-to-end text-to-speech synthesis system that can be trained completely from scratch on text-audio pairs with minimal feature engineering. Specifically, the authors propose Tacotron, an end-to-end generative model for text-to-speech that takes characters as input and outputs raw spectrogram. The key hypothesis is that an integrated end-to-end model can synthesize natural sounding speech directly from characters without the need for the complex components in traditional TTS pipelines.


## What is the main contribution of this paper?

 The main contribution of this paper is presenting Tacotron, an end-to-end generative text-to-speech model. Key points:

- Tacotron is an integrated end-to-end model that takes characters as input and outputs spectrograms, which are synthesized into speech. This eliminates the need for traditional complex TTS pipelines.

- The model is based on the sequence-to-sequence paradigm with attention, but makes several modifications like using a CBHG encoder and predicting multiple output frames per step. These allow it to work well with character inputs.

- Tacotron achieves a 3.82 MOS naturalness score on an English dataset, outperforming a production parametric system. This shows end-to-end TTS can match or surpass traditional systems. 

- The model can be trained completely from scratch with random initialization, without the need for hand-engineered features or alignments. This makes it easy to train on large, found data.

In summary, the main contribution is presenting an end-to-end sequence-to-sequence model for text-to-speech that matches traditional systems in quality while being much simpler. This could enable new applications and data sources for TTS.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the paper:

Tacotron is an end-to-end generative text-to-speech model that takes characters as input and outputs spectrograms, then uses Griffin-Lim for waveform synthesis to achieve state-of-the-art speech naturalness.


## How does this paper compare to other research in the same field?

 Here are a few key ways this Tacotron paper compares to other TTS research:

- It proposes an end-to-end TTS system that goes directly from characters to spectrogram. This is a major advantage over traditional TTS systems and some other end-to-end works like DeepVoice that still have separate components.

- It does not require any hand-engineered features or alignments, unlike many other TTS systems. This makes it very data-driven.

- It significantly outperforms a production parametric TTS system in naturalness based on MOS evaluation. This shows its practical potential.

- It uses an attention-based seq2seq model. Attention had been used for TTS before, but Tacotron makes important modifications like predicting multiple frames to help attention learn alignment.

- It is not autoregressive like sample-level models such as WaveNet. This makes it much faster at inference time.

- It still requires a vocoder-like post-processing step to convert spectrogram to waveform, rather than directly outputting waveform samples like WaveNet. Later work has improved on this.

- It uses a simple Griffin-Lim reconstruction for waveform synthesis. This causes some artifacts. Later end-to-end models have focussed more on neural vocoders.

Overall, this paper moved the state-of-the-art forward significantly in end-to-end TTS with minimal hand-engineering. Subsequent work has built on Tacotron's seq2seq approach while improving waveform generation.


## What future research directions do the authors suggest?

 Based on my reading of the paper, some future research directions suggested by the authors include:

- Improving the output layer, attention module, loss function, and waveform synthesis module of Tacotron. The authors mention these aspects of their model are ripe for improvement.

- Developing a fast and high-quality neural network based spectrogram inversion method to replace Griffin-Lim. The authors mention Griffin-Lim can introduce audible artifacts.

- Investigating many other unexplored aspects of Tacotron, as many early design decisions have gone unchanged. This suggests room for optimization of the model architecture.

- Removing the need for simple text normalization by using recent advances in learned text normalization.

- Exploring conditioning the model on various attributes like speaker, language, sentiment etc. to make the model more versatile.

- Adapting the model to new data more easily since it is end-to-end.

- Training the model on more noisy and expressive real-world data since it is more robust as an end-to-end model.

- Replacing components like the post-processing network with alternatives like vocoders or WaveNet to improve synthesis.

So in summary, the main directions are improving the model components, making the model more versatile, training on more real-world data, and replacing modules with more advanced alternatives. The end-to-end nature provides many possibilities for future work.
