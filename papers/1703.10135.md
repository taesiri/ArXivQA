# [Tacotron: Towards End-to-End Speech Synthesis](https://arxiv.org/abs/1703.10135)

## What is the central research question or hypothesis that this paper addresses?

 The central research question this paper addresses is how to develop an end-to-end text-to-speech synthesis system that can be trained completely from scratch on text-audio pairs with minimal feature engineering. Specifically, the authors propose Tacotron, an end-to-end generative model for text-to-speech that takes characters as input and outputs raw spectrogram. The key hypothesis is that an integrated end-to-end model can synthesize natural sounding speech directly from characters without the need for the complex components in traditional TTS pipelines.


## What is the main contribution of this paper?

 The main contribution of this paper is presenting Tacotron, an end-to-end generative text-to-speech model. Key points:

- Tacotron is an integrated end-to-end model that takes characters as input and outputs spectrograms, which are synthesized into speech. This eliminates the need for traditional complex TTS pipelines.

- The model is based on the sequence-to-sequence paradigm with attention, but makes several modifications like using a CBHG encoder and predicting multiple output frames per step. These allow it to work well with character inputs.

- Tacotron achieves a 3.82 MOS naturalness score on an English dataset, outperforming a production parametric system. This shows end-to-end TTS can match or surpass traditional systems. 

- The model can be trained completely from scratch with random initialization, without the need for hand-engineered features or alignments. This makes it easy to train on large, found data.

In summary, the main contribution is presenting an end-to-end sequence-to-sequence model for text-to-speech that matches traditional systems in quality while being much simpler. This could enable new applications and data sources for TTS.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the paper:

Tacotron is an end-to-end generative text-to-speech model that takes characters as input and outputs spectrograms, then uses Griffin-Lim for waveform synthesis to achieve state-of-the-art speech naturalness.


## How does this paper compare to other research in the same field?

 Here are a few key ways this Tacotron paper compares to other TTS research:

- It proposes an end-to-end TTS system that goes directly from characters to spectrogram. This is a major advantage over traditional TTS systems and some other end-to-end works like DeepVoice that still have separate components.

- It does not require any hand-engineered features or alignments, unlike many other TTS systems. This makes it very data-driven.

- It significantly outperforms a production parametric TTS system in naturalness based on MOS evaluation. This shows its practical potential.

- It uses an attention-based seq2seq model. Attention had been used for TTS before, but Tacotron makes important modifications like predicting multiple frames to help attention learn alignment.

- It is not autoregressive like sample-level models such as WaveNet. This makes it much faster at inference time.

- It still requires a vocoder-like post-processing step to convert spectrogram to waveform, rather than directly outputting waveform samples like WaveNet. Later work has improved on this.

- It uses a simple Griffin-Lim reconstruction for waveform synthesis. This causes some artifacts. Later end-to-end models have focussed more on neural vocoders.

Overall, this paper moved the state-of-the-art forward significantly in end-to-end TTS with minimal hand-engineering. Subsequent work has built on Tacotron's seq2seq approach while improving waveform generation.
