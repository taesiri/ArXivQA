# [PhySU-Net: Long Temporal Context Transformer for rPPG with   Self-Supervised Pre-training](https://arxiv.org/abs/2402.11913)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem: 
Remote photoplethysmography (rPPG) aims to measure cardiac activity by analyzing subtle color variations in facial videos captured by RGB cameras. However, accurately extracting the faint blood volume pulse (BVP) signal from videos is challenging due to overwhelming environmental noise caused by lighting changes, subject movement, etc. Existing methods have limited temporal modeling capabilities, ignore long-term context, and struggle with scarce labeled training data.

Proposed Solution:
This paper proposes PhySU-Net, the first long temporal context transformer network for rPPG. It uses a Swin transformer encoder-decoder architecture to model long input sequences (~20 seconds) for more robust BVP signal prediction. The problem is framed as an image reconstruction task, where the input is a noise-robust multi-scale spatial-temporal (MST) map and the output reconstructs a BVP map with similar temporal and frequency properties as the ground truth BVP. A heart rate regression head is also added. 

Additionally, a self-supervised pre-training framework is introduced to leverage unlabeled data. It uses two pretext tasks: 1) Predicting heart rate using traditional methods as pseudo-labels, 2) Reconstructing a masked version of the MST map input. This allows the model to learn useful representations from unlabeled facial videos.

Main Contributions:
1) Proposes PhySU-Net transformer network to model long temporal context for state-of-the-art rPPG performance
2) First self-supervised pre-training framework for rPPG using image reconstruction and heart rate regression pretext tasks  
3) Shows superior performance on two datasets (OBF, VIPL-HR) and that pre-training further improves results by leveraging unlabeled data representations

In summary, this paper presents a robust rPPG approach using a long temporal context transformer network and a novel self-supervised learning strategy to mitigate data scarcity limitations. Experiments demonstrate state-of-the-art results and effectiveness of the pre-training technique.
