# [MDCS: More Diverse Experts with Consistency Self-distillation for   Long-tailed Recognition](https://arxiv.org/abs/2308.09922)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, it seems the central research question is: How can we improve multi-expert methods for long-tailed recognition by increasing the diversity of experts and reducing model variance?Specifically, the paper proposes a new method called MDCS (More Diverse experts with Consistency Self-distillation) to address two key limitations of prior multi-expert methods for long-tailed recognition:1. Lack of diversity among experts - Previous methods did not sufficiently promote diversity among the experts, resulting in reduced overall performance. 2. High model variance - Prior methods focused on ensembling to reduce final variance but did not address the variance within each expert model. High variance indicates overfitting.To address these issues, MDCS has two main components:1. Diversity Loss (DL) - Promotes diversity among experts by controlling their focus on different categories of data (many-shot, medium-shot, few-shot).2. Consistency Self-Distillation (CS) - Reduces model variance and prevents overfitting by distilling knowledge from weakly augmented data to provide richer supervision for strongly augmented data. Also uses Confident Instance Sampling to avoid biased/noisy knowledge.The paper shows through analysis and experiments that MDCS improves diversity and reduces variance compared to prior art. It also achieves state-of-the-art accuracy on five benchmark long-tailed recognition datasets, demonstrating the effectiveness of the approach.In summary, the main research contributions are in designing a multi-expert method that specifically targets increasing diversity and reducing variance to advance the state-of-the-art in long-tailed recognition. The core ideas are Diversity Loss and Consistency Self-Distillation.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contributions appear to be:- Proposing a novel method called MDCS (More Diverse experts with Consistency Self-distillation) for improving long-tailed recognition. MDCS contains two key components:1) Diversity Loss (DL): This promotes diversity among experts by controlling their focus on different categories. It uses an adjustable distribution weight to make each expert focus on different types of categories (e.g. many-shot, medium-shot, few-shot).2) Consistency Self-Distillation (CS): This distills richer knowledge from weakly augmented instances to provide more supervision for strongly augmented instances. It helps reduce model variance and prevent overfitting. A Confident Instance Sampling method is used to select correct samples for CS to avoid introducing biased/noisy knowledge.- Showing through analysis and experiments that MDCS can effectively increase diversity of experts, reduce model variance, and improve recognition accuracy compared to previous methods.- Achieving new state-of-the-art results on five benchmark long-tailed datasets, outperforming previous methods by 1-2% in terms of top-1 accuracy. For example, 56.1% on CIFAR100-LT and 61.8% on ImageNet-LT.- Demonstrating that the DL and CS components are mutually reinforcing - CS benefits from DL's increase in diversity, while DL alone is less effective without CS.In summary, the main contribution appears to be proposing the MDCS method to improve long-tailed recognition through synergistically increasing expert diversity and reducing model variance. The strong experimental results validate the effectiveness of MDCS against prior state-of-the-art.
