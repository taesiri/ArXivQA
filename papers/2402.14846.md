# [Stick to your Role! Stability of Personal Values Expressed in Large   Language Models](https://arxiv.org/abs/2402.14846)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Large language models (LLMs) exhibit highly context-dependent behavior, meaning their expressed values/traits can drastically change based on prompts or conversation topics. 
- Standard LLM evaluations use minimal context queries, making conclusions about deployment behavior difficult. 
- No prior work has studied value stability of LLMs across contexts. This is crucial for applications like psychology studies, social simulation, and value alignment.

Methodology:
- Authors adapt methods from psychology to estimate two types of value stability in LLMs:
    1) Rank-order stability (interpersonal): Do models maintain consistent relative value hierarchies between simulated personas?
    2) Ipsative stability (intrapersonal): Do models maintain consistent value hierarchies for each simulated persona?
- Stability is evaluated by administering a standard value questionnaire (PVQ-40) to models over simulated conversations on different topics. 
- Experiments consider 19 LLMs, with and without instructing them to simulate specific personas.

Key Findings:
- Mixtral, Mistral and Qwen families showed highest stability. LLaMa and Phi showed low stability.
- When simulating personas, models showed lower-than-human rank-order stability. Stability further dropped over longer conversations.  
- Without explicit personas, some models retained high ipsative stability.
- Trends were largely consistent in a downstream behavioral donation task.

Main Contributions:
- First methodology and analysis of value stability over contexts in LLMs
- Evidence that current LLMs struggle with coherent persona simulation over conversations
- Motivates research into specialized models for social simulation and value alignment
- Proposes context-dependence and stability as a crucial dimension for LLM evaluation/comparison

In summary, this paper provides novel methodology and evidence showing that most existing LLMs exhibit subhuman-level value stability across contexts. This highlights issues in using them for social applications and motivates further research into specialized models with more coherent persona simulation abilities. Evaluating context-dependence isproposed as an important consideration alongside other LLM criteria.
