# [What is More Likely to Happen Next? Video-and-Language Future Event   Prediction](https://arxiv.org/abs/2010.07999)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is:Can AI models learn to make multimodal commonsense predictions about future events given a video clip (with dialogue), by choosing the more likely event between two possible future events?To address this question, the paper introduces a new dataset and task called Video-and-Language Event Prediction (VLEP) for predicting future events from videos. The key aspects are:- The task requires predicting fine-grained future events from short video clips with aligned dialogue. This requires understanding both visual and language semantics as well as commonsense knowledge.- The VLEP dataset contains 28,726 examples collected from diverse videos to support this task. The examples consist of a premise video/dialogue, two possible future events, and human rationales.- The dataset uses adversarial data collection and matching to reduce biases and trivial examples. Analysis shows this helps increase the dataset difficulty.- They present a transformer-based multimodal baseline model incorporating video, text and commonsense knowledge. Experiments demonstrate the usefulness of each modality.- There is a large gap between model (67.46%) and human performance (90.50%) indicating this is a challenging task with room for improvement.In summary, the key hypothesis is that models can learn to make non-trivial, multimodal, commonsense future event predictions on this new dataset, which requires joint understanding across video, language and commonsense reasoning.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contributions are:1. Proposing a new task called Video-and-Language Event Prediction, which requires predicting which of two possible future events is more likely to happen after observing a video clip with aligned dialogue. This task requires both video and language understanding as well as commonsense reasoning.2. Introducing a new dataset called VLEP for this task, containing 28,726 examples built from TV show and YouTube lifestyle vlog clips. The dataset uses adversarial data collection and adversarial matching to reduce biases and ensure a balanced set of easy and difficult examples.3. Presenting a strong transformer-based baseline model that encodes the video, dialogue, and candidate future events, incorporates commonsense knowledge, and makes a prediction. Experiments show each component contributes positively, though there is still a large gap compared to human performance.In summary, the key contribution is introducing and benchmarking a new multimodal prediction task that requires deeper video and language understanding and commonsense reasoning than prior work in this area. The new dataset and strong baseline model provide a foundation for future research on this challenging task.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper introduces a new dataset and task for video-and-language future event prediction, where models must choose the more likely event to happen next after seeing a short video clip with dialog.
