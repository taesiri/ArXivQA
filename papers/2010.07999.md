# [What is More Likely to Happen Next? Video-and-Language Future Event   Prediction](https://arxiv.org/abs/2010.07999)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is:Can AI models learn to make multimodal commonsense predictions about future events given a video clip (with dialogue), by choosing the more likely event between two possible future events?To address this question, the paper introduces a new dataset and task called Video-and-Language Event Prediction (VLEP) for predicting future events from videos. The key aspects are:- The task requires predicting fine-grained future events from short video clips with aligned dialogue. This requires understanding both visual and language semantics as well as commonsense knowledge.- The VLEP dataset contains 28,726 examples collected from diverse videos to support this task. The examples consist of a premise video/dialogue, two possible future events, and human rationales.- The dataset uses adversarial data collection and matching to reduce biases and trivial examples. Analysis shows this helps increase the dataset difficulty.- They present a transformer-based multimodal baseline model incorporating video, text and commonsense knowledge. Experiments demonstrate the usefulness of each modality.- There is a large gap between model (67.46%) and human performance (90.50%) indicating this is a challenging task with room for improvement.In summary, the key hypothesis is that models can learn to make non-trivial, multimodal, commonsense future event predictions on this new dataset, which requires joint understanding across video, language and commonsense reasoning.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contributions are:1. Proposing a new task called Video-and-Language Event Prediction, which requires predicting which of two possible future events is more likely to happen after observing a video clip with aligned dialogue. This task requires both video and language understanding as well as commonsense reasoning.2. Introducing a new dataset called VLEP for this task, containing 28,726 examples built from TV show and YouTube lifestyle vlog clips. The dataset uses adversarial data collection and adversarial matching to reduce biases and ensure a balanced set of easy and difficult examples.3. Presenting a strong transformer-based baseline model that encodes the video, dialogue, and candidate future events, incorporates commonsense knowledge, and makes a prediction. Experiments show each component contributes positively, though there is still a large gap compared to human performance.In summary, the key contribution is introducing and benchmarking a new multimodal prediction task that requires deeper video and language understanding and commonsense reasoning than prior work in this area. The new dataset and strong baseline model provide a foundation for future research on this challenging task.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper introduces a new dataset and task for video-and-language future event prediction, where models must choose the more likely event to happen next after seeing a short video clip with dialog.


## How does this paper compare to other research in the same field?

Here are a few key things about how this paper compares to other related work:- The paper proposes a new multimodal prediction task and dataset called Video-and-Language Event Prediction (VLEP). This is novel compared to prior work on video-and-language understanding, which has focused more on tasks like video captioning, QA, and retrieval. The future event prediction task requires deeper understanding and reasoning.- Existing video forecasting work has looked mainly at predicting low-level future frames, motions, or actions from videos. This paper focuses on predicting high-level natural language future events, which is more complex and semantic. - For data collection, the paper uses strategies like adversarial human annotation and adversarial matching to reduce bias and trivial examples. Many prior video+text datasets suffer from annotation artifacts, so these collection methods help improve dataset quality.- The paper incorporates commonsense knowledge, which is shown to improve results. This aligns with recent interests in integrating external knowledge into vision-language models, in contrast to earlier works that relied only on dataset supervision.- The task, dataset, and model in this paper cover multimodal, temporal, and commonsense reasoning aspects. Many prior works have focused on one or two of those but not all three.- The work provides a strong baseline but there is still a large gap compared to human performance. This suggests the task remains very challenging and open for future research to make additional progress.In summary, the paper proposes a novel and multifaceted video+text prediction task compared to prior work, and introduces innovations in data collection, knowledge integration, and modeling that advance research in this area. But there is still much room for improvement on this challenging task.
