# WebGPT: Browser-assisted question-answering with human feedback

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the main research question appears to be: How can we develop an AI system that can provide high quality, human-level long form question answering by searching the web?The authors frame long-form question answering as challenging because it requires a combination of effective information retrieval and synthesis. Their key hypothesis seems to be that they can achieve strong performance on this task by:1) Leveraging existing solutions for information retrieval (Bing search) and synthesis (GPT-3).2) Focusing on combining these solutions effectively using human feedback. Specifically, they collect demonstrations of humans using a text-based web browsing environment to answer questions. They also collect comparisons between model-generated answers. This allows them to optimize answer quality directly using imitation learning, reinforcement learning and rejection sampling against a reward model predicting human preferences.3) Having models provide answers along with supporting references extracted from web pages. This makes it easier for humans to evaluate the factual accuracy of answers without independent research.So in summary, the main research question is how to build an AI system for high-quality long-form question answering by combining existing solutions for search and synthesis guided by human feedback and evaluations. The key hypothesis is that providing answers with references will enable more effective optimization through human judgments.


## What is the main contribution of this paper?

The main contribution of this paper appears to be the development of a technique for training language models to answer long-form questions by interacting with a text-based web browsing environment. The key ideas are:- Creating a text-based interface that allows a language model to search the web, click links, scroll pages, and collect references, all through natural language commands. This allows end-to-end training of retrieval and answering.- Using human demonstrations of browsing and answering questions to initially train the model through imitation learning. - Further optimizing the models through human feedback on answer quality, by having models output answers along with supporting references that make evaluation easier.- Achieving strong performance on the ELI5 long-form QA dataset, outperforming both human demonstrators and Reddit answers through techniques like rejection sampling against a learned human preference model.So in summary, the main contribution seems to be showing how to train QA models to effectively search the web and synthesize information by creating a learnable text-based browsing environment and optimizing through online human feedback. The techniques allow models to surpass human performance on a challenging long-form QA task.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper describes an AI system called WebGPT that can browse the web and answer questions by searching for relevant information, synthesizing an answer, and providing references to support its responses.


## How does this paper compare to other research in the same field?

Here are a few thoughts on how this paper compares to other research in the field of question answering:- The use of a text-based web browsing environment is novel. Most prior work has focused just on improving retrieval or synthesis models independently. Allowing the model to interact with search engines and web pages enables end-to-end training and optimization.- Leveraging existing solutions for retrieval and synthesis (Bing and GPT-3) allows the authors to focus on the higher-level task. This is different from much work that aims to build retrieval and synthesis models from scratch.- The use of human feedback for answer quality optimization has been explored before in summarization, but is less common in QA. Directly optimizing for human preferences enables significantly better performance.- The approach of generating answers with references is unique. Most QA systems just output an answer without any citations or context. The references make it much easier for humans to judge factual accuracy.- Training the model primarily on ELI5 questions is fairly standard, but evaluation on the adversarial TruthfulQA dataset provides a more robust test of capabilities.- The scale of compute used is much larger than typical QA papers. The 175B model required substantial resources. But this enables strong performance on such a challenging open-ended QA task.- Compared to contemporaneous work like REALM and RAG, this paper puts more emphasis on end-task performance through optimization of human preferences, rather than on developing novel retrieval or generation models.Overall, I would say the novel contributions are in the training methodology and answer evaluation strategy. The scale of compute is also impressive. The core technical components leverage existing innovations like large pretrained models and search engines.


## What future research directions do the authors suggest?

Based on my reading of the paper, some of the key future research directions suggested by the authors include:- Developing techniques to further improve the factual accuracy of long-form question answering systems like WebGPT. They suggest adversarial training on out-of-distribution questions as one promising approach. They also discuss developing better training objectives that incentivize quoting reliable sources.- Mitigating the biases present in the model, such as biases inherited from the pretraining data and model. They suggest this could be done by improving the pretraining data and objectives.- Studying the effects of question framing and stance on the answers generated by the model. The authors did a small experiment showing the model can be sensitive to the stance of the question, and suggest more research here. - Finding better ways to control for and measure the truthfulness of answers during training. They propose drawing on cross-disciplinary research to develop epistemically sound criteria for evaluating factual accuracy.- Developing methods like debate and recursive reward modeling to allow models to assist in their own evaluation, making human evaluation easier.- Adapting the reinforcement learning objective to work well with rejection sampling. The authors found rejection sampling was most effective but discuss ways RL could be improved.- Investigating alternatives to the detailed comparison criteria to enable more efficient data collection while still generating useful training signal.In summary, the main themes seem to be improving truthfulness through better objectives and training regimes, mitigating biases, assisting human evaluation, and tweaking the methods like RL to improve overall performance.
