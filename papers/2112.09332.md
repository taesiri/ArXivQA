# [WebGPT: Browser-assisted question-answering with human feedback](https://arxiv.org/abs/2112.09332)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the main research question appears to be: How can we develop an AI system that can provide high quality, human-level long form question answering by searching the web?The authors frame long-form question answering as challenging because it requires a combination of effective information retrieval and synthesis. Their key hypothesis seems to be that they can achieve strong performance on this task by:1) Leveraging existing solutions for information retrieval (Bing search) and synthesis (GPT-3).2) Focusing on combining these solutions effectively using human feedback. Specifically, they collect demonstrations of humans using a text-based web browsing environment to answer questions. They also collect comparisons between model-generated answers. This allows them to optimize answer quality directly using imitation learning, reinforcement learning and rejection sampling against a reward model predicting human preferences.3) Having models provide answers along with supporting references extracted from web pages. This makes it easier for humans to evaluate the factual accuracy of answers without independent research.So in summary, the main research question is how to build an AI system for high-quality long-form question answering by combining existing solutions for search and synthesis guided by human feedback and evaluations. The key hypothesis is that providing answers with references will enable more effective optimization through human judgments.


## What is the main contribution of this paper?

 The main contribution of this paper appears to be the development of a technique for training language models to answer long-form questions by interacting with a text-based web browsing environment. The key ideas are:- Creating a text-based interface that allows a language model to search the web, click links, scroll pages, and collect references, all through natural language commands. This allows end-to-end training of retrieval and answering.- Using human demonstrations of browsing and answering questions to initially train the model through imitation learning. - Further optimizing the models through human feedback on answer quality, by having models output answers along with supporting references that make evaluation easier.- Achieving strong performance on the ELI5 long-form QA dataset, outperforming both human demonstrators and Reddit answers through techniques like rejection sampling against a learned human preference model.So in summary, the main contribution seems to be showing how to train QA models to effectively search the web and synthesize information by creating a learnable text-based browsing environment and optimizing through online human feedback. The techniques allow models to surpass human performance on a challenging long-form QA task.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the paper:The paper describes an AI system called WebGPT that can browse the web and answer questions by searching for relevant information, synthesizing an answer, and providing references to support its responses.


## How does this paper compare to other research in the same field?

 Here are a few thoughts on how this paper compares to other research in the field of question answering:- The use of a text-based web browsing environment is novel. Most prior work has focused just on improving retrieval or synthesis models independently. Allowing the model to interact with search engines and web pages enables end-to-end training and optimization.- Leveraging existing solutions for retrieval and synthesis (Bing and GPT-3) allows the authors to focus on the higher-level task. This is different from much work that aims to build retrieval and synthesis models from scratch.- The use of human feedback for answer quality optimization has been explored before in summarization, but is less common in QA. Directly optimizing for human preferences enables significantly better performance.- The approach of generating answers with references is unique. Most QA systems just output an answer without any citations or context. The references make it much easier for humans to judge factual accuracy.- Training the model primarily on ELI5 questions is fairly standard, but evaluation on the adversarial TruthfulQA dataset provides a more robust test of capabilities.- The scale of compute used is much larger than typical QA papers. The 175B model required substantial resources. But this enables strong performance on such a challenging open-ended QA task.- Compared to contemporaneous work like REALM and RAG, this paper puts more emphasis on end-task performance through optimization of human preferences, rather than on developing novel retrieval or generation models.Overall, I would say the novel contributions are in the training methodology and answer evaluation strategy. The scale of compute is also impressive. The core technical components leverage existing innovations like large pretrained models and search engines.


## What future research directions do the authors suggest?

 Based on my reading of the paper, some of the key future research directions suggested by the authors include:- Developing techniques to further improve the factual accuracy of long-form question answering systems like WebGPT. They suggest adversarial training on out-of-distribution questions as one promising approach. They also discuss developing better training objectives that incentivize quoting reliable sources.- Mitigating the biases present in the model, such as biases inherited from the pretraining data and model. They suggest this could be done by improving the pretraining data and objectives.- Studying the effects of question framing and stance on the answers generated by the model. The authors did a small experiment showing the model can be sensitive to the stance of the question, and suggest more research here. - Finding better ways to control for and measure the truthfulness of answers during training. They propose drawing on cross-disciplinary research to develop epistemically sound criteria for evaluating factual accuracy.- Developing methods like debate and recursive reward modeling to allow models to assist in their own evaluation, making human evaluation easier.- Adapting the reinforcement learning objective to work well with rejection sampling. The authors found rejection sampling was most effective but discuss ways RL could be improved.- Investigating alternatives to the detailed comparison criteria to enable more efficient data collection while still generating useful training signal.In summary, the main themes seem to be improving truthfulness through better objectives and training regimes, mitigating biases, assisting human evaluation, and tweaking the methods like RL to improve overall performance.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the paper:The paper proposes WebGPT, a method for long-form question answering that fine-tunes GPT-3 in a text-based web browsing environment. WebGPT can search the web via the Bing API and follow links to new pages. It collects references while browsing that support its final answer. The model is trained using imitation learning on demonstrations of humans browsing and answering questions, as well as reinforcement learning and rejection sampling against a reward model predicting human preferences between answers. WebGPT outperforms both the human demonstrators and Reddit reference answers on the ELI5 dataset when evaluated by humans. The use of references is crucial for allowing accurate human evaluation of factual correctness. The paper also analyzes WebGPT's performance on adversarial datasets, scaling behavior, and risks related to truthfulness and bias. Overall, it demonstrates that existing capabilities for retrieval and synthesis can be effectively combined using human feedback, approaching human-level performance on long-form question answering.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the paper:The paper proposes WebGPT, a system for long-form question answering that leverages GPT-3 along with a text-based web browsing environment. WebGPT allows fine-tuning GPT-3 models using imitation learning and reinforcement learning in order to directly optimize answer quality as judged by human feedback. The key contributions of the paper are: 1) creating a text-based web browsing environment that GPT-3 models can interact with to improve retrieval and synthesis for question answering; and 2) generating answers along with supporting references extracted from web pages during browsing. This allows human evaluators to more easily judge the factual accuracy of answers without needing to do additional research. Experiments demonstrate that WebGPT models fine-tuned with human feedback are able to surpass both human demonstrators and Reddit reference answers in terms of overall answer quality on the ELI5 dataset. The best WebGPT model also outperforms GPT-3 baseline models on the TruthfulQA dataset in terms of producing more truthful and informative answers.


## Summarize the main method used in the paper in one paragraph.

 Here is a one paragraph summary of the main method used in the paper:The paper proposes WebGPT, a system for answering long-form open-ended questions by searching the web and synthesizing information. The key idea is to fine-tune large language models like GPT-3 in an interactive text-based web browsing environment. The model can perform actions like running searches, clicking links, quoting passages, and answering questions. Training is done via imitation learning on human demonstrations of web browsing and question answering, followed by optimizing the answers directly using human feedback comparisons between model outputs. Specifically, the model is first trained by supervised fine-tuning on demonstrations of humans performing web search to answer questions (behavior cloning). Then the model answers are further optimized by training a reward model on human comparisons between pairs of model answers, and using the predicted reward to fine-tune the model with reinforcement learning or rejection sampling.
