# [From Reals to Logic and Back: Inventing Symbolic Vocabularies, Actions,   and Models for Planning from Raw Data](https://arxiv.org/abs/2402.11871)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
Creating symbolic, logic-based representations (e.g. PDDL models) for robot planning requires strong human intuition and expertise. Removing this dependency on human expertise is an active area of research. The paper aims to address this problem by developing an approach to autonomously learn symbolic representations from raw robot trajectories, without needing any human annotations.

Proposed Solution: 
The paper presents the first approach, called LAMP (Learning Abstract Models for Planning), that can simultaneously learn:
1) A relational predicate vocabulary 
2) High-level symbolic actions
3) Action models
from unlabeled, real-valued robot trajectories. 

Key ideas:
- Introduce the concept of "relational critical regions" to identify salient relationships between objects from relative poses in trajectories. This allows inventing meaningful predicate vocabulary.

- Discover symbolic actions as transitions to and from the relational critical regions. Learn action models using associative learning from state transitions in trajectories.

- Use the learned symbolic representations to plan for new, more complex tasks. Continually expand representations by identifying missing predicates and actions when planning fails.

Main Contributions:
- First approach to simultaneously learn relational predicate vocabularies and high-level symbolic actions from raw robot trajectories, without any human supervision.

- Introduce "relational critical regions" to identify salient object relationships and invent interpretable predicate vocabulary.

- Associative learning of action models from state transitions in unlabeled trajectories.

- Continual planning, execution and learning of new predicates and actions for solving more complex tasks.

- Extensive evaluation showing the learned representations enable scaling to more complex tasks with longer horizons and 3x more objects compared to training tasks.

In summary, the paper presents a novel approach for robot learning that removes the dependency on human expertise for creating symbolic representations. The approach invents interpretable predicate vocabularies and action models from scratch, using unlabeled robot trajectories. It also enables continual expansion of representations for solving more complex tasks over time.


## Summarize the paper in one sentence.

 This paper presents the first approach for autonomously learning generalizable, logic-based relational representations for abstract states and actions from raw robot trajectories, enabling scalable planning for complex long-horizon tasks.


## What is the main contribution of this paper?

 According to the paper, the main contribution is:

The first approach for autonomously learning generalizable, logic-based relational representations for abstract states and actions starting from unannotated high-dimensional, real-valued robot trajectories. Specifically, the paper presents an approach that takes as input a small set of demonstrations in the form of time-indexed real-valued trajectories of the robot and objects, and uses these to invent a vocabulary in predicate logic and a set of high-level actions, expressed in a PDDL-like representation.

Key aspects of the contribution highlighted in the paper include:

- Learning symbolic predicates and actions without requiring either to be labeled or included in the input demonstrations

- Inventing predicates that capture semantics close to intuitively hand-crafted predicates like "on(x,y)" as well as new robot/task-specific relations 

- Learning high-level actions that go beyond classical intuitive actions and vastly increase planning generalizability

- Using the learned symbolic representations with off-the-shelf planners to solve new problems with significantly greater numbers of objects and horizons

So in summary, the main contribution is an approach to automatically learn logic-based relational state and action representations directly from real-valued robot trajectory data, which enables more scalable and generalizable planning.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and keywords associated with it are:

- Symbolic abstractions
- Relational representations
- Predicate vocabulary
- High-level actions
- Robot planning 
- Task and motion planning
- Continuous demonstrations
- PDDL (Planning Domain Definition Language)
- Critical regions
- Relational critical regions
- Pose generators
- Associative learning

The paper presents an approach for autonomously learning symbolic abstractions like predicate vocabularies, high-level actions, and pose generators from continuous robot demonstrations. These learned abstractions are represented in PDDL and can be used with task and motion planners to solve complex, long-horizon robot planning problems. The key idea is to identify "relational critical regions" in the relative poses between objects that correspond to salient relations and using transitions into and out of these regions to learn high-level actions. Overall, the symbolic abstractions and relational representations learned using this associative approach from raw data facilitate scalable robot planning.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions I would ask to better understand the method proposed in this paper:

1. The concept of relational critical regions (RCRs) is key for autonomously learning symbolic predicates. However, the paper does not provide a detailed theoretical analysis or proofs on properties of RCRs like soundness, completeness etc. Can you elaborate more on the theoretical foundations of RCRs?

2. The evaluation shows the approach scales to problems with large numbers of objects and complex goals. But can the method also work for complex environments like homes/offices with several rooms and obstacles where the robot needs to navigate? 

3. Alg 1 computes RCRs for each pair of object types which seems exponential. What heuristics/optimizations are done for efficiently computing RCRs? 

4. How sensitive is the overall approach to hyperparameters like Gaussian mixture thresholds, criticality thresholds etc.? Is there a principled way to set these?

5. The approach seems tightly coupled to the trajectories in training tasks. What provisions are there for handling noisy/outlier demonstrations?

6. What additional mechanisms can be incorporated in Alg 3 for dealing with stochastic environments? Does it learn probabilistic action models?

7. Does the approach have any theoretical guarantees on properties like soundness, completeness, optimality etc. for the overall planning? 

8. How does the approach identify and handle symmetric or overlapping RCRs which can confuse the predicate invention?

9. What metrics are used to evaluate interpretability of invented predicates/actions? Is any human validation done?  

10. The approach seems to work for pick and place style robotic tasks. What are some challenges/limitations for using it in complex, high DOF manipulation or navigation tasks?
