# [Boundary Unlearning](https://arxiv.org/abs/2303.11570)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is: How can we efficiently unlearn or forget an entire class from a trained deep neural network model? The paper proposes a new approach called "Boundary Unlearning" to address this question. The key ideas are:- Focus on shifting the decision boundary of the DNN model rather than scrubbing model parameters directly. This allows more efficient unlearning by operating in the model's decision space rather than parameter space.- Propose two strategies to shift the decision boundary: Boundary Shrink and Boundary Expanding. These are designed to destroy the decision boundary for the forgetting class while maintaining boundaries of remaining classes.- Achieve utility guarantee by only changing the boundary of the forgetting class while keeping other boundaries intact. Achieve privacy guarantee by pushing forgetting data to boundaries to make the model uncertain about them.- Rapidly shift boundaries with just a few epochs of adjustment rather than full retraining or expensive parameter optimization.So in summary, the central hypothesis is that shifting decision boundaries can enable rapid and effective unlearning of an entire class from a DNN model. The Boundary Unlearning methods are proposed to test this hypothesis.
