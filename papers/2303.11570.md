# [Boundary Unlearning](https://arxiv.org/abs/2303.11570)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is: How can we efficiently unlearn or forget an entire class from a trained deep neural network model? The paper proposes a new approach called "Boundary Unlearning" to address this question. The key ideas are:- Focus on shifting the decision boundary of the DNN model rather than scrubbing model parameters directly. This allows more efficient unlearning by operating in the model's decision space rather than parameter space.- Propose two strategies to shift the decision boundary: Boundary Shrink and Boundary Expanding. These are designed to destroy the decision boundary for the forgetting class while maintaining boundaries of remaining classes.- Achieve utility guarantee by only changing the boundary of the forgetting class while keeping other boundaries intact. Achieve privacy guarantee by pushing forgetting data to boundaries to make the model uncertain about them.- Rapidly shift boundaries with just a few epochs of adjustment rather than full retraining or expensive parameter optimization.So in summary, the central hypothesis is that shifting decision boundaries can enable rapid and effective unlearning of an entire class from a DNN model. The Boundary Unlearning methods are proposed to test this hypothesis.


## What is the main contribution of this paper?

The main contributions of this paper are:- It proposes Boundary Unlearning, a new machine unlearning method to forget an entire class from a trained neural network model by shifting the decision boundary. This is the first work to focus on manipulating the decision space instead of the parameter space for unlearning.- It introduces two novel boundary shift methods - Boundary Shrink and Boundary Expanding - to effectively shift the decision boundary to forget the target class while maintaining utility on the remaining data.- It provides extensive experiments on image classification and face recognition tasks, demonstrating Boundary Unlearning can rapidly and efficiently forget the target class. It outperforms prior unlearning methods in utility, privacy and efficiency.- It reveals the connection between shifting the decision boundary and forgetting in neural networks, providing new insights into how to accomplish machine unlearning via the decision space. In summary, the key innovation is the idea of manipulating the decision boundary rather than model parameters to forget, which is more efficient and does not interfere with the original training process. The proposed Boundary Shrink and Expanding methods effectively implement this idea to rapidly unlearn a class while preserving utility and privacy.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper proposes Boundary Unlearning, a rapid and effective machine unlearning method that shifts the decision boundary of a trained DNN model to remove information about a forgetting class while maintaining utility on remaining classes.


## How does this paper compare to other research in the same field?

Here are my thoughts on how this paper on Boundary Unlearning compares to other research in the field of machine unlearning:- This paper focuses specifically on unlearning an entire class from a trained deep neural network model. Many other papers have looked at more general machine unlearning techniques that can remove arbitrary data samples or subsets. Focusing on unlearning a class is novel and has useful applications like removing a person's face data.- The key idea of shifting the decision boundary to mimic a model retrained from scratch is a unique approach not explored much before. Most prior work focuses on locating and scrubbing parameters related to the forgetting data. The boundary shifting perspective provides a more holistic view of how to alter the model's behavior.- The proposed methods of Boundary Shrink and Boundary Expanding offer new techniques to actually implement the boundary shifting idea. Using adversarial-style perturbations or adding/pruning shadow classes are clever ways to manipulate the decision boundary that haven't been proposed previously.- Experiments show Boundary Unlearning can effectively forget a class rapidly, with 17-19x speedups over retraining. This demonstrates practical efficiency improvements over alternatives like parameter scrubbing or rapid retraining methods.- Boundary Unlearning does not require any intervention in the original training process, unlike some prior rapid retraining techniques. This makes it easy to apply to any pretrained model.- The visualization of the decision space gives useful insights into how Boundary Unlearning actually transforms sample classifications and cluster densities. This level of understanding is missing from many existing methods.Overall, I think this paper makes excellent contributions by framing machine unlearning from a decision boundary viewpoint and introducing innovative techniques to shift boundaries. The experimental results validate the effectiveness and efficiency of Boundary Unlearning for forgetting classes. It pushes forward the state-of-the-art in making machine unlearning more practical.


## What future research directions do the authors suggest?

Based on my reading of the paper, the authors suggest the following future research directions:- Developing more advanced boundary shift methods. The Boundary Shrink and Boundary Expanding methods proposed in this paper are initial attempts. The authors suggest exploring more sophisticated ways to shift the decision boundary to forget a class while maintaining utility on the remaining data. - Applying Boundary Unlearning to more complex neural network architectures and tasks beyond image classification. The paper evaluates Boundary Unlearning on CNN models for image classification. Applying it to other model architectures (e.g. Transformers) and tasks (e.g. NLP) is an interesting direction.- Theoretical analysis of decision boundary shifting for machine unlearning. The authors provide empirical results but do not offer theoretical guarantees. Formal analysis of how shifting decision boundaries impacts model utility and privacy in unlearning is an open problem. - Mitigating boundary shifting attacks. The techniques proposed could potentially be used maliciously to force a model to forget certain classes by manipulating the decision boundary. Defending against such attacks is an important consideration.- Combining with privacy-enhancing technologies. The authors suggest combining Boundary Unlearning with techniques like differential privacy to provide stronger privacy guarantees when unlearning.- Real-world evaluation of unlearning performance. Testing Boundary Unlearning on real-world datasets and models deployed in production would better validate its utility.In summary, the key future directions are developing more advanced boundary shift methods, applying it to broader contexts, theoretical analysis, mitigating misuse, integration with other privacy techniques, and real-world evaluation.


## Summarize the paper in one paragraph.

Here is a one paragraph summary of the key points from the paper:The paper proposes Boundary Unlearning, a method to erase an entire class from a trained deep neural network model by shifting the decision boundary rather than directly modifying the model parameters. The key idea is to shift the decision boundary to imitate the behavior of a model retrained from scratch without the forgetting class. The authors introduce two strategies: Boundary Shrink which splits the decision space of the forgetting class into other classes, and Boundary Expanding which disperses the activation about the forgetting class by remapping it to a new shadow class. Experiments on image classification and face recognition tasks show Boundary Unlearning can rapidly and effectively forget the target class while maintaining utility on the remaining data and preventing privacy leaks about the forgotten data. The method provides an efficient way to accomplish utility and privacy goals for machine unlearning without expensive retraining or directly scrubbing parameters.


## Summarize the paper in two paragraphs.

Here is a two paragraph summary of the paper:This paper proposes Boundary Unlearning, a new machine unlearning method to erase the information about an entire class from a trained deep neural network model. Existing unlearning methods either attempt to retrain the model from scratch, which is expensive, or try to locate and scrub the influence of the forgetting data in the parameter space, which is inefficient due to the high dimensionality. Boundary Unlearning focuses on the decision space instead, and shifts the decision boundary to imitate the behavior of a model retrained from scratch without the forgetting data. The key ideas are Boundary Shrink, which splits the decision space of the forgetting class by relabeling the data, and Boundary Expanding, which disperses the activation using an extra shadow class. Experiments on CIFAR-10 and Vggface2 for image classification and face recognition show Boundary Unlearning can rapidly and effectively forget the target class while maintaining utility on the remaining data. It provides up to 19x speedup over retraining and outperforms prior methods on utility and privacy guarantees. Boundary Unlearning presents an efficient way to accomplish machine unlearning by manipulating the decision boundary rather than the parameters directly.


## Summarize the main method used in the paper in one paragraph.

The main method proposed in this paper is "Boundary Unlearning", which aims to efficiently unlearn an entire class of data from a trained deep neural network model. The key idea is to shift the decision boundary of the original model to imitate the decision behavior of the model retrained from scratch without the forgetting data. Specifically, the authors develop two novel boundary shift methods:1) Boundary Shrink: This method finds the nearest but incorrect class labels for each forgetting sample, and reassigns those labels to the forgetting samples. Then it finetunes the original model with these relabeled forgetting samples, which shrinks the decision boundary of the forgetting class. 2) Boundary Expanding: This method introduces an extra shadow class, remaps the forgetting samples to this shadow class, then discards the shadow class after finetuning. The expanding and remapping operations disperse the activation about the forgetting data to accomplish the forgetting goal.By shifting the decision boundary, Boundary Unlearning can rapidly destroy the information about the forgetting class while maintaining utility on the remaining data. It achieves efficient and effective unlearning without expensive retraining or directly scrubbing model parameters.
