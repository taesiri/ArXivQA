# [No Token Left Behind: Reliable KV Cache Compression via Importance-Aware   Mixed Precision Quantization](https://arxiv.org/abs/2402.18096)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem Statement
- Key-value (KV) caching is crucial for accelerating inference speed and throughput of large language models (LLMs), but the cache size grows with batch size and sequence length, often exceeding the model size. This poses memory challenges during LLM deployment.  
- Recent works propose KV cache "eviction" to reduce memory consumption by prioritizing important KVs while evicting less important ones based on criteria like attention patterns or token frequency. However, risks from exhaustively discarding evicted KVs are not thoroughly examined.

Key Observations
- Cache eviction can cause safety breaches, hallucinations, and context loss as critical details in evicted KVs are lost. Examples are shown where safety prompts are ignored or critical context is forgotten while unrelated details are recalled.
- Retaining even a small amount of information from evicted KVs via low-precision quantization substantially recovers performance degradation from eviction. But very low precision (INT2) still struggles.
- Systematic outliers manifested in keys and queries impede low-precision quantization. A channel balancer is proposed to enable lower precision.  

Proposed Solution  
- Mixed-Precision KV Cache (MiKV):
    - Retain evicted KVs in low precision to preserve context details
    - Maintain important KVs in high precision to ensure generation quality
    - Handle query-key outliers for reliable low-precision quantization
- Achieves high compression rate while minimizing performance loss across benchmarks. Outperforms baselines like cache eviction and uniform quantization.

Main Contributions:
- Reveals and analyzes context damage from cache eviction 
- Shows low-precision retention of evicted KVs recovers lost context
- Proposes MiKV for reliable and efficient KV cache compression
