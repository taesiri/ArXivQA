# [Evaluating Prompting Strategies for Grammatical Error Correction Based   on Language Proficiency](https://arxiv.org/abs/2402.15930)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Grammatical error correction (GEC) using large language models (LLMs) like GPT tends to overcorrect, leading to higher recall but lower precision. 
- Writing examples of English learners may differ across proficiency levels. Past work shows learner error types vary by proficiency.

Goal: 
- Examine the interaction between LLM performance and learner proficiency to reduce overcorrection in GEC.

Methods:
- Focus on zero-shot and few-shot prompting of GPT-2 and GPT-3.5 for GEC using the Cambridge English Write & Improve corpus labeled with CEFR proficiency levels (A, B, C).
- Compare performance on proficiency-specific subsets as well as aggregated sets.
- Implement label-by-label analysis to relate performance to error types.

Key Findings:
- GEC performance improves from zero-shot to few-shot prompting for GPT-2 and GPT-3.5, but not fine-tuned GPT-2.  
- GPT-2 recall drops substantially from proficiency A to C, while fine-tuned GPT-2 precision is more stable.
- Overcorrection is more apparent for proficiency C than A/B.
- Performance on missing errors is better across proficiencies than replacement errors.
- F1 is a more useful metric than F0.5 given the high number of false positives.
- SOTA models still outperform prompting GPT for GEC.

Contributions:
- First analysis of relationship between prompting GPT-2/3 for GEC and English learner proficiency levels
- Label-by-label analysis quantifying performance on error types
- Demonstrate overcorrection varies by proficiency level
- Insights on appropriateness of evaluation metrics based on false positive rates
