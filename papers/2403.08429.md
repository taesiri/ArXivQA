# [Software Vulnerability and Functionality Assessment using LLMs](https://arxiv.org/abs/2403.08429)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
Code reviews are an integral part of software development, but can be tedious and costly. This paper investigates whether large language models (LLMs) can aid in automating parts of code reviews, specifically in (i) flagging security vulnerabilities and (ii) validating functionality against requirements.  

Proposed Solution:
The authors experiment with 3 proprietary LLMs (GPT-3.5 Turbo, GPT-4, Text-Davinci) and 6 smaller open-source LLMs on code snippets from 3 datasets: HumanEval, MBPP for clean code, and SecurityEval containing expert-written vulnerable code. 

For vulnerability detection, they use a zero-shot prompt to classify snippets as having vulnerabilities or not. For functionality validation, they validate whether snippets meet their documented functionality through a second zero-shot prompt. They also combine both tasks through chain-of-thought prompting. Finally, they evaluate whether models can describe the detected vulnerabilities.

Main Contributions:

- The proprietary models substantially outperform the smaller open-source models on all tasks. GPT-4 achieves 96% accuracy on detecting vulnerabilities. The top models also exceed 88% on functionally validating code.

- Chain-of-thought prompting improves performance over zero-shot prompting alone when combining vulnerability detection and functionality validation.

- GPT-4 generated vulnerability descriptions that could be matched to real vulnerabilities 37% of the time, showing promise in providing actionable feedback.

In summary, the results demonstrate that large proprietary LLMs show promise in automating parts of code reviews, but smaller open-source models still have substantial room for improvement on these tasks. The paper provides a methodology for evaluating LLMs on code quality and security.
