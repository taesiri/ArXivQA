# [DCoM: A Deep Column Mapper for Semantic Data Type Detection](https://arxiv.org/abs/2106.12871v1)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question appears to be: 

How can deep learning and natural language processing methods be applied to improve the accuracy and efficiency of semantic data type detection from raw column data, compared to previous approaches relying heavily on feature engineering?

The key hypothesis seems to be:

Feeding raw column data directly into NLP-based neural networks like LSTM and BERT, combined with a novel permutation-based input approach, can allow deep learning models to learn useful representations and relationships from the data itself. This will outperform previous methods relying on extensive feature engineering, by improving predictive accuracy while also reducing inference time.

Some key points:

- The paper proposes DCoM, a collection of deep learning models for semantic data type detection that take raw column values as text input rather than relying on many hand-engineered features.

- A permutation-based input approach is introduced to allow feeding sets of column values to the models without conveying incorrect positional relationships.

- Various neural architectures are tested, including LSTM, DistilBERT, and ELECTRA.

- The hypothesis is that these deep learning models can learn useful features and relationships directly from the raw text data itself.

- Experiments show DCoM models outperform previous benchmarks, including the prior state-of-the-art, improving F1 score while also reducing inference time.

So in summary, the key research question is whether deep learning and NLP can improve semantic type detection from raw data compared to heavy feature engineering, which the experiments seek to validate.
