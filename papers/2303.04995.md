# [Text-Visual Prompting for Efficient 2D Temporal Video Grounding](https://arxiv.org/abs/2303.04995)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading, the key research question this paper aims to address is:

How to advance 2D video temporal grounding (TVG) methods to achieve comparable performance to 3D TVG methods in a more efficient manner?

Specifically, the paper proposes a novel text-visual prompting (TVP) framework to boost the performance of 2D TVG models by incorporating optimized perturbation patterns (prompts) into both visual inputs and textual features. The goal is to compensate for the lack of spatiotemporal information in sparse 2D visual features and improve cross-modal feature fusion, without needing costly dense 3D features. The central hypothesis is that TVP allows effective co-training of vision and language encoders in a 2D TVG model to approach the accuracy of 3D methods while being much more efficient.

In summary, the paper focuses on improving 2D TVG to be on par with 3D methods that rely on expensive 3D features, via a new TVP technique for incorporating optimized prompts into both visual and textual inputs. The aim is to develop an efficient yet accurate 2D alternative to existing 3D TVG models.
