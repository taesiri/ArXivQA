# [One-Nearest-Neighbor Search is All You Need for Minimax Optimal   Regression and Classification](https://arxiv.org/abs/2202.02464)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the key research question is:Can distributed nearest-neighbor methods with small k (specifically k=1) achieve similar minimax optimal statistical performance as standard nearest-neighbor methods that require large k?The central hypothesis is that an M-split 1-NN method can attain nearly minimax optimal rates for regression and classification, comparable to standard k-NN methods where k grows as a function of the sample size N. The key results are theorems showing that the proposed distributed (M,k)-NN regression and classification rules achieve the same rates (up to log factors) as standard k-NN rules where k is on the order of M.In summary, the paper aims to show that by using distributed nearest-neighbor rules on sample splits, we can get around the requirement for large k in standard k-NN methods and attain optimal learning rates with small fixed k. This makes nearest-neighbor methods more practical for modern large-scale applications.
