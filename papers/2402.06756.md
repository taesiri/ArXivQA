# [Convergence of Gradient Descent with Small Initialization for   Unregularized Matrix Completion](https://arxiv.org/abs/2402.06756)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem Statement:
The paper studies the problem of matrix completion, where the goal is to reconstruct a positive semidefinite (PSD) matrix $\mathbf{X^\star}$ of rank $r$ from only a subset of its observed entries. The most common approach is to minimize the mean squared error loss using gradient descent (GD), but existing convergence results require explicit regularization or accurate initialization close to the ground truth. 

The paper aims to answer: Why does vanilla GD with small random initialization work well in practice for matrix completion, even without regularization and in over-parameterized settings where the search rank $r'$ used is much larger than the true rank $r$?

Proposed Solution:
The paper proves that GD with small initialization provably converges to the ground truth $\mathbf{X^\star}$ without explicit regularization. This holds even when the rank $r$ is unknown and over-estimated as $r' \gg r$.

Key technical contributions:
- Establishes near-linear convergence of GD to an $O(\epsilon)$ neighborhood of $\mathbf{X^\star}$ in the over-parameterized setting, for an initialization with norm $||\mathbf{U}_0|| \leq \epsilon$. Neither the convergence rate nor final accuracy depends on the over-parameterized search rank $r'$.
- For exact parameterization with $r'=r$, shows GD enjoys faster convergence to any arbitrarily small accuracy level $\epsilon > 0$, given a small enough initialization.
- Proposes a "weakly-coupled leave-one-out analysis" technique to establish incoherence of the GD iterates, which is key for the convergence results. This relaxes limitations of prior leave-one-out analysis approaches.

Implications:
The results shed light on the implicit regularization effect of GD that enables low-rank solution recovery in matrix completion without explicit regularization, even in over-parameterized regimes. The analysis techniques developed are likely useful for establishing GD convergence more broadly in nonconvex optimization problems.
