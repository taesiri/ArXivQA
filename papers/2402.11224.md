# [Neural Networks with (Low-Precision) Polynomial Approximations: New   Insights and Techniques for Accuracy Improvement](https://arxiv.org/abs/2402.11224)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Polynomial approximation of neural networks (PANNs) are used for privacy-preserving machine learning (PPML), as they replace non-polynomial functions with polynomial approximations to enable compatibility with cryptographic systems. However, approximation introduces errors which damage inference accuracy.
- There is a tradeoff between approximation precision and efficiency - higher precision enables accuracy closer to the backbone model but has much higher computational overhead. The effect of approximation errors is not well understood.

Proposed Solutions:
- The paper initiates an investigation into PANNs as standalone objects to understand the effect of approximation errors and improve their accuracy.

Key Findings:
- Approximation errors affect both "information contributing to outputs" and "irrelevant information in input background". Errors in the former case act similarly to adversarial perturbations. Errors in the latter case are unique to PANNs and very damaging.

- "Sturdiness" of a model to resist approximation errors is similar to adversarial robustness. But errors in PANNs lead to "intra-model perturbations" at every layer rather than just at input. 

- Weight regularization, while useful for generalization, significantly reduces sturdiness of models to approximation errors.

Solutions Proposed:
- An adversarial training (AT)-like method tailored to deal with intra-model perturbations by adding noise at layers/locations vulnerable to approximation errors.

- Use minimal weight regularization during training and compensate for reduced generalization using Mixup.

Main Contributions:  
- New understanding of how approximation errors affect PANNs, similarities and differences with adversarial robustness
- Observation that weight regularization harms sturdiness to resist approximation errors
- Two orthogonal solutions to enhance sturdiness - AT-like training and minimal regularization + Mixup
- Experiments show solutions can achieve state-of-the-art accuracy for PANNs at lower precision, reducing overhead by 40-60%

The key impact is allowing much more efficient and accurate PANNs to enable practical deployment of PPML.
