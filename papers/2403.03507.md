# [GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection](https://arxiv.org/abs/2403.03507)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
Training large language models (LLMs) is very memory intensive, not just due to the large number of parameters but also due to the memory needed to store gradients and optimizer states during training. For example, training a 7B parameter LLaMA model requires at least 58GB memory with a single batch size. This makes training infeasible even on high-end GPUs. Existing methods like low-rank adaptation (LoRA) reduce memory usage by limiting the number of trainable parameters but underperform compared to full-rank training.

Proposed Solution: 
The paper proposes Gradient Low-Rank Projection (\lowrank), a strategy that allows full-parameter learning while being more memory efficient than LoRA. The key idea is that while weight matrices may not be low-rank, the gradient matrix has an inherent low-rank structure during training. \lowrank{} computes projection matrices $P$ and $Q$ to project the full gradient $G$ into a low-rank version $P^\top G Q$. Since optimizer states rely only on this low-rank gradient, substantial memory savings can be achieved. $P$ and $Q$ are refreshed periodically to allow learning across multiple low-rank subspaces over time.

Main Contributions:
- Identify and formally prove the low-rank property of gradients in common network architectures 
- Propose the \lowrank algorithm that projects gradients to low-rank in a memory-efficient way, while allowing full parameter updates
- Achieve up to 65.5\% reduction in optimizer memory and comparable performance to full-rank training when pre-training LLMs
- Enable, for the first time, pre-training a 7B LLaMA model on a single 24GB GPU without any model/data parallelism or memory offloading techniques
- Show strong performance in fine-tuning GLUE tasks, outperforming prior methods like LoRA

The method is simple to implement, works with various optimizers, has few sensitive hyperparameters, and serves as an efficient full-stack solution for memory-constrained LLM training.
