# Considering Likelihood in NLP Classification Explanations with Occlusion   and Language Modeling

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be:How do explanation methods that consider the discrete structure and likelihood of language data differ from methods that ignore these properties, when explaining modern NLP models with increasing syntactic capabilities?The key points are:- Modern NLP models have increasing syntactic and semantic understanding of language.- Current occlusion methods often produce invalid or unlikely inputs, not utilizing models' syntactic abilities. - Gradient methods disregard the discrete distribution of language data.- The paper proposes a new method OLM that uses a language model to produce likely replacements when occluding parts of the input. - Experiments compare OLM to baselines using occlusion and gradients.- Results show OLM explanations differ from baselines, suggesting the importance of considering data likelihood and discrete structure for explaining NLP models.So in summary, the central question examines how considering properties of discrete language data impacts the explanations produced for modern NLP models, compared to methods that ignore these properties. The proposed OLM method accounts for data likelihood and structure. Experiments demonstrate that this provides distinctive explanations compared to baselines.


## What is the main contribution of this paper?

The main contribution of this paper is presenting OLM, a new explanation method for NLP models that combines occlusion and language modeling. The key ideas are:- OLM generates replacements for occluded words using a language model, producing syntactically valid and contextually appropriate inputs. This accounts for the discrete structure of language data better than gradient methods or naive occlusion.- Theoretical analysis showing OLM satisfies several desirable axioms like Implementation Invariance and Sensitivity-1. - Introduction of the Class Zero-Sum axiom, an alternative to Completeness for explanation methods.- Experiments comparing OLM to deletion, UNK, Sensitivity Analysis, Gradient*Input, and Integrated Gradients. OLM gives distinctive, low-correlated explanations, suggesting the importance of syntactic validity and likelihood.So in summary, OLM is a new occlusion-based explanation method tailored to modern NLP models with increasing syntactic capabilities. It has a stronger theoretical motivation than previous methods, and experiments demonstrate it produces distinctive explanations that consider likelihood.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper proposes a new explanation method OLM that uses a language model to generate syntactically and semantically likely replacements when occluding words from an input text, in order to provide more faithful explanations of NLP models compared to existing methods like deletion/UNK and gradient-based approaches.


## How does this paper compare to other research in the same field?

Here are some key points about how this paper compares to other research on explainability methods for NLP models:- The paper argues that many existing methods for explaining NLP models fail to account for the discrete, non-continuous nature of textual data. Many methods like gradient-based attribution rely on small perturbations to the input, but small changes to text often produce nonsensical or ungrammatical results. - The authors propose a new method, OLM, that uses language modeling to generate replacements words and phrases that are more likely to be syntactically and semantically valid. This allows for a more fair comparison when doing occlusion-based explanation.- The paper introduces a new axiom called "class zero-sum" that formalizes the intuition that a feature's attribution should balance out across all possible class labels. This is presented as an alternative to the commonly used "completeness" axiom.- Experiments compare OLM to gradient-based methods like Integrated Gradients and other occlusion methods. The generally low correlation suggests OLM provides distinctive explanations, confirming the authors' hypothesis about issues with other methods.- Overall, this paper makes a compelling argument that common practices in NLP explainability need to better account for properties of text as a discrete input space. The usage of language models to generate more valid perturbations is an interesting idea not explored much previously.- Compared to other work, this paper is quite focused on technical issues with specific explanation methods, rather than their applications. But it provides useful theoretical grounding for thinking about NLP explainability.In summary, this paper makes a worthwhile conceptual argument about issues with current methods, and proposes a new technique to address some of those limitations. It represents an incremental improvement grounded in analysis of the NLP task domain.
