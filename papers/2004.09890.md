# [Considering Likelihood in NLP Classification Explanations with Occlusion   and Language Modeling](https://arxiv.org/abs/2004.09890)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question seems to be:

How do explanation methods that consider the discrete structure and likelihood of language data differ from methods that ignore these properties, when explaining modern NLP models with increasing syntactic capabilities?

The key points are:

- Modern NLP models have increasing syntactic and semantic understanding of language.

- Current occlusion methods often produce invalid or unlikely inputs, not utilizing models' syntactic abilities. 

- Gradient methods disregard the discrete distribution of language data.

- The paper proposes a new method OLM that uses a language model to produce likely replacements when occluding parts of the input. 

- Experiments compare OLM to baselines using occlusion and gradients.

- Results show OLM explanations differ from baselines, suggesting the importance of considering data likelihood and discrete structure for explaining NLP models.

So in summary, the central question examines how considering properties of discrete language data impacts the explanations produced for modern NLP models, compared to methods that ignore these properties. The proposed OLM method accounts for data likelihood and structure. Experiments demonstrate that this provides distinctive explanations compared to baselines.


## What is the main contribution of this paper?

 The main contribution of this paper is presenting OLM, a new explanation method for NLP models that combines occlusion and language modeling. The key ideas are:

- OLM generates replacements for occluded words using a language model, producing syntactically valid and contextually appropriate inputs. This accounts for the discrete structure of language data better than gradient methods or naive occlusion.

- Theoretical analysis showing OLM satisfies several desirable axioms like Implementation Invariance and Sensitivity-1. 

- Introduction of the Class Zero-Sum axiom, an alternative to Completeness for explanation methods.

- Experiments comparing OLM to deletion, UNK, Sensitivity Analysis, Gradient*Input, and Integrated Gradients. OLM gives distinctive, low-correlated explanations, suggesting the importance of syntactic validity and likelihood.

So in summary, OLM is a new occlusion-based explanation method tailored to modern NLP models with increasing syntactic capabilities. It has a stronger theoretical motivation than previous methods, and experiments demonstrate it produces distinctive explanations that consider likelihood.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the key points from the paper:

The paper proposes a new explanation method OLM that uses a language model to generate syntactically and semantically likely replacements when occluding words from an input text, in order to provide more faithful explanations of NLP models compared to existing methods like deletion/UNK and gradient-based approaches.


## How does this paper compare to other research in the same field?

 Here are some key points about how this paper compares to other research on explainability methods for NLP models:

- The paper argues that many existing methods for explaining NLP models fail to account for the discrete, non-continuous nature of textual data. Many methods like gradient-based attribution rely on small perturbations to the input, but small changes to text often produce nonsensical or ungrammatical results. 

- The authors propose a new method, OLM, that uses language modeling to generate replacements words and phrases that are more likely to be syntactically and semantically valid. This allows for a more fair comparison when doing occlusion-based explanation.

- The paper introduces a new axiom called "class zero-sum" that formalizes the intuition that a feature's attribution should balance out across all possible class labels. This is presented as an alternative to the commonly used "completeness" axiom.

- Experiments compare OLM to gradient-based methods like Integrated Gradients and other occlusion methods. The generally low correlation suggests OLM provides distinctive explanations, confirming the authors' hypothesis about issues with other methods.

- Overall, this paper makes a compelling argument that common practices in NLP explainability need to better account for properties of text as a discrete input space. The usage of language models to generate more valid perturbations is an interesting idea not explored much previously.

- Compared to other work, this paper is quite focused on technical issues with specific explanation methods, rather than their applications. But it provides useful theoretical grounding for thinking about NLP explainability.

In summary, this paper makes a worthwhile conceptual argument about issues with current methods, and proposes a new technique to address some of those limitations. It represents an incremental improvement grounded in analysis of the NLP task domain.


## What future research directions do the authors suggest?

 Based on my reading of the paper, some of the main future research directions suggested by the authors are:

- Extending the OLM method to explain other language features beyond individual words, such as phrases, sentences, etc. The authors note that tasks with longer input may not be very sensitive to single word occlusion.

- Further experiments and analysis to more exhaustively evaluate OLM against other explanation methods. The authors state they do not consider their current experiments to be exhaustive and that unfortunately there is no general agreed upon evaluation methodology for explanation methods.

- Addressing practical difficulties of using a language model for approximation in OLM, such as the language model sometimes producing syntactically correct but nonsensical data, or failing to produce fully syntactically correct data. The authors suggest this could potentially be improved in the future.

- Analyzing the interaction between OLM and other black-box explanation methods like SHAP, which occludes subsets of features. The authors suggest OLM could potentially be combined with these methods, but the approximation error may increase with more features occluded.

- Developing more axiomatic analysis and theoretical foundations for analyzing explanation methods in NLP. The authors introduced a new axiom (Class Zero-Sum) and showed several other axioms OLM satisfies, setting a path for more rigorous analysis of future methods.

In summary, the main suggestions are to extend and refine OLM, conduct more thorough empirical analysis, address the challenges of using a language model, explore integrating OLM with other methods, and develop stronger theoretical foundations for analyzing explanation methods in NLP.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the paper:

The paper proposes OLM, a new explanation method for NLP models that combines occlusion with language modeling to generate syntactically valid replacements for words when computing relevance scores. The authors argue that current occlusion and gradient-based methods can produce invalid or unlikely inputs which may not properly explain models with increasing syntactic capabilities. OLM uses a language model to sample contextually appropriate replacements for words to get a more valid measure of their relevance. Experiments compare OLM to deletion, UNK replacement, sensitivity analysis, Gradient*Input, and Integrated Gradients on MNLI, SST-2, and CoLA tasks. Results show OLM has low correlation with other methods, indicating it provides distinctive explanations. The authors also introduce a new Class Zero-Sum axiom for explanations and show OLM satisfies several desired axioms. Overall, the paper presents a theoretically motivated approach to generate more syntactically valid explanations for NLP models through contextual language modeling.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the key points from the paper:

This paper introduces OLM, a new method to explain predictions from NLP classification models. OLM combines occlusion with language modeling. When explaining a model's prediction by removing or replacing words in the input, OLM uses a language model to generate replacements words that fit the context and are syntactically valid. This produces explanations that consider the discrete structure of language and the improved syntactic abilities of modern NLP models. 

The authors argue that other methods like occlusion or gradients create invalid language inputs, which may not properly explain predictions. OLM satisfies axioms like implementation invariance and sensitivity-1. Experiments compare OLM to deletion, UNK replacement, and gradient methods on MNLI, SST-2, and CoLA. OLM correlates more with occlusion methods but overall correlation is low, showing it gives distinctive explanations. OLM also introduces a new axiom called class zero-sum, contrary to the completeness axiom. The authors conclude OLM has a stronger theoretical basis and produces more faithful explanations by using valid language.


## Summarize the main method used in the paper in one paragraph.

 Here is a one paragraph summary of the main method presented in the paper:

The paper proposes OLM, a new explanation method for NLP models that combines occlusion with language modeling. OLM replaces an input token with a likely replacement sampled from a language model conditioned on the context, instead of deleting or replacing with <UNK> like other occlusion methods. This allows evaluating the model on valid and syntactically correct inputs. OLM computes the relevance of a token as the difference between the model's prediction on the original input and its average prediction on the inputs with the token resampled. The authors argue this better reflects the model's reliance on that token than gradient-based methods or occlusions producing unlikely input. They introduce the class zero-sum axiom for explanations and show OLM satisfies several desired axioms. Experiments compare OLM relevance scores to other occlusion and gradient-based explanation methods, finding low correlation, indicating OLM provides distinctive explanations considering likelihood.


## What problem or question is the paper addressing?

 Based on my reading, the key points of this paper are:

- The paper proposes a new explanation method called OLM for evaluating natural language processing (NLP) models. 

- It argues that current occlusion-based and gradient-based explanation methods for NLP have limitations:

- Occlusion methods often produce invalid or syntactically incorrect inputs, neglecting the improved abilities of recent NLP models.

- Gradient-based methods disregard the discrete distribution of language data. They consider infinitesimal changes to the input, even though valid language data is discrete.

- OLM combines occlusion with language modeling to generate syntactically valid and contextually appropriate replacements when evaluating what parts of the input influenced a model's predictions.

- The paper introduces a "Class Zero-Sum" axiom that it argues better captures relevance for classification tasks than the commonly used "Completeness" axiom.

- Experiments compare OLM to other explanation methods on sentence classification tasks. OLM produces distinctive relevance scores not well correlated with other methods.

- The paper concludes OLM provides a more principled way to explain NLP models that considers the discrete structure of language.

In summary, the key question is how to better explain modern NLP models in ways that account for the discrete nature of language data. OLM is proposed as a method that addresses limitations of existing approaches.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper abstract and contents, some of the key terms and concepts are:

- Explanation methods for NLP models
- Occlusion-based explanations 
- Likelihood of language data
- Discrete structure of language
- Gradient-based explanations
- Axiomatic analysis of explanations
- Class Zero-Sum axiom 
- OLM: Occlusion and Language Modeling explanation method
- Syntactically correct replacements
- Data likelihood in explanations
- Experiments on NLP sentence classification tasks

In summary, the paper proposes a new explanation method called OLM that combines occlusion and language modeling to produce syntactically valid replacements for explaining NLP models. A key focus is on considering the discrete likelihood of language data, in contrast to gradient methods. The method is evaluated on sentence classification tasks and analyzed axiomatically. The Class Zero-Sum axiom is also introduced for explanations. The key terms reflect the focus on axiomatic explanations, occlusion, language models, and likelihood to analyze modern NLP models.
