# Phoenix: Democratizing ChatGPT across Languages

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be: How can we democratize access to large language models (LLMs) like ChatGPT across languages?The authors aim to develop open-source LLMs that can perform well across many languages, including low-resource non-Latin languages. Their goal is to make conversational AI systems more accessible globally. Specifically, some key aspects the paper investigates are:- Training multi-lingual LLMs using a combination of instruction datasets and conversation datasets in 40+ languages.- Comparing the performance of their model "Phoenix" to other existing democratized LLMs like Guanaco and Vicuna in languages like Chinese, English, Arabic, etc.- Releasing the models, data, and code openly to facilitate further research into democratized LLMs.In summary, the central hypothesis is that by leveraging diverse multi-lingual data and training strategies, they can develop an open-source LLM "Phoenix" that achieves strong performance across many languages, thereby helping to democratize access to conversational AI.


## What is the main contribution of this paper?

Based on my reading, the main contributions of this paper seem to be:1. Introducing a new large language model called "Phoenix" that aims to democratize access to ChatGPT-like models across languages. The model supports both Latin and non-Latin languages. 2. Using a combination of instruction data and conversation data to train Phoenix in a multi-lingual setting. This incorporates the benefits of both instruction tuning and conversation tuning.3. Achieving state-of-the-art performance among fully open source Chinese language models. Phoenix also outperforms existing open source models in many non-Latin languages.4. Releasing "Chimera", a Latin-specific version of Phoenix built on the LLaMA backbone, which achieves very competitive performance in English compared to other democratized models.5. Providing extensive benchmarking of existing democratized LLMs using both automatic metrics and human evaluation. This allows systematic comparison across models.6. Making the training data, code, and models openly available to continue the democratization process. This will allow others to replicate, evaluate and build upon this work.In summary, the main contribution seems to be releasing a new competitive open source multi-lingual model to expand access to ChatGPT-like capabilities across languages, especially for non-Latin languages and users who cannot access proprietary models. The combination of instruction and conversation data, benchmarking, and open release also represent key contributions.
