# [Entity-Augmented Code Generation](https://arxiv.org/abs/2312.08976)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a summary paragraph of the key points from the paper:

This paper explores a new task of code generation utilizing external entities from knowledge bases. The authors collect and publish a novel dataset for project-level code generation, where the model must reuse functions defined elsewhere in the project code. They demonstrate that existing retrieval-augmented language models struggle with this task, partly due to inability to distinguish between similar entity names. To address this, they propose a novel model architecture that injects a scalable entity retriever directly into the decoder of a pretrained language model. This allows the model to select entire entities as tokens when generating code, rather than trying to generate entity names token-by-token. The authors evaluate their model on the project code dataset, as well as existing SQL and Bash command generation datasets. Their method consistently outperforms baselines across tasks, especially when the number of entities per sample grows. They demonstrate the importance of jointly training the retriever and generator, as well as using cross-attention between them. While promising, the approach performs less effectively on Bash command generation where entity order does not matter. Overall, this work introduces a scalable technique for entity-based code generation that could enable solving practical programming problems.


## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Large language models (LLMs) often hallucinate during text generation and cannot effectively utilize external knowledge sources. 
- Existing retrieval augmented models have issues with scaling and contamination when appending large context documents to inputs.  
- Similar entity names in knowledge bases cause issues during token-by-token generation.

Proposed Solution:
- Introduce a new task of code generation using external entities. A new dataset is collected and published for project-level code generation requiring reuse of functions.
- Propose a novel model architecture with a scalable entity retriever injected directly into the LLM decoder instead of encoder.  
- The decoder can select entire entities from a dynamic vocabulary, mitigating issues with similar entity names.
- Performs entity lookup by summarizing descriptions into single token embeddings. Allows conditioning on both input and partial output.
- More scalable than appending context documents to inputs. Not exposed to context contamination issues.

Main Contributions:
- Introduction of a novel architecture for entity-augmented decoding that is end-to-end trainable, scalable and plug-and-play with LLMs.
- Rigorous analysis of model performance on project code, Bash and SQL datasets.
- Publication of a new dataset for project-level code generation requiring reuse of functions.

The model is shown to outperform baselines on various tasks. The injection of the retriever into the decoder is beneficial except in cases where entity order doesn't matter. The architecture addresses issues faced by prior retrieval augmented models.
