# [Zero-Shot Video Question Answering via Frozen Bidirectional Language   Models](https://arxiv.org/abs/2206.08155)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is how to develop an effective approach for zero-shot video question answering (VideoQA) that does not require manually annotated visual question-answer data for training. The key hypothesis is that a framework based on a frozen bidirectional language model can achieve strong zero-shot VideoQA performance when combined with a visual encoder and trained on readily available web video-text data.The introduction states that recent work has explored using large frozen autoregressive language models for zero-shot visual question answering, but that lighter bidirectional models may provide a more efficient alternative. The paper proposes adapting such bidirectional models to the video domain and investigating their zero-shot capabilities.The overall research question is how well the proposed frozen bidirectional language model framework, which they term FrozenBiLM, can perform on zero-shot VideoQA compared to prior work and autoregressive models. The hypothesis is that it will outperform previous approaches and demonstrate the benefits of frozen bidirectional language models for this task.


## What is the main contribution of this paper?

Based on my reading, the main contribution of this paper is proposing a framework called FrozenBiLM for zero-shot video question answering (VideoQA). The key ideas are:1. The framework uses a frozen bidirectional language model (BiLM) pretrained on large text corpora, which provides strong textual reasoning abilities. 2. The BiLM is combined with a pretrained visual encoder using additional lightweight modules that are trained on web-scraped video-text data. This allows adapting the model to multi-modal inputs while keeping the BiLM frozen.3. Several video-language tasks like open-ended VideoQA are formulated in a cloze/fill-in-the-blank form and solved by masked language modeling using the adapted model, without any explicit VideoQA supervision.4. Experiments show FrozenBiLM significantly outperforms prior work on multiple zero-shot VideoQA benchmarks. It also demonstrates strong performance in few-shot and fully-supervised settings.5. Comparisons to autoregressive models indicate frozen BiLMs provide better accuracy and efficiency for zero-shot VideoQA.In summary, the main contribution is proposing an effective framework for zero-shot VideoQA that exploits frozen bidirectional language models and web-scale multi-modal data. The approach sets new state-of-the-art results across several datasets.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper presents a framework for zero-shot video question answering that uses frozen bidirectional language models adapted to multi-modal inputs and demonstrates superior performance compared to prior work.


## How does this paper compare to other research in the same field?

Here are a few key points comparing this paper to other related research:- This paper focuses on zero-shot video question answering (VideoQA), where the model is trained without any explicit Visual QA supervision. This is an emerging but still relatively less explored setting compared to fully supervised VideoQA. - The paper builds on frozen bidirectional language models, in contrast to prior work like FLAMINGO, CLIP-Cap which rely on frozen autoregressive models like GPT-3. The results show frozen bidirectional models are more efficient and achieve better performance.- For zero-shot learning, the paper uses a masked language modeling loss on web-scraped video-text data. Other recent work like BLIP and Just Ask use different forms of zero-shot learning like image QA supervision or generated QA data.- This is the first work to demonstrate the capability of frozen bidirectional language models for zero-shot VideoQA. Prior zero-shot VQA work focused only on autoregressive models.- The proposed model outperforms prior state-of-the-art on multiple zero-shot VideoQA benchmarks by significant margins. It also shows competitive performance in supervised settings.- The model relies only on readily available web data, avoiding the need for manually annotated VQA datasets or data generation pipelines. This improves the scalability and reduces training costs.In summary, this paper pushes the state-of-the-art for zero-shot VideoQA through an efficient framework based on frozen bidirectional models. The results are very promising and demonstrate these models are well-suited for zero-shot cross-modal reasoning when adapted properly.


## What future research directions do the authors suggest?

Based on my reading of the paper, some of the main future research directions the authors suggest are:- Scaling up the size of the bidirectional language model to several billion parameters. The authors note that their current model uses a 900M parameter language model, so exploring even larger bidirectional LMs could potentially improve performance further.- Additional training on large datasets of YouTube videos with accompanying speech transcripts and/or audio. The authors mention that additional training data, especially paired video and speech data, could help the model learn even better video representations.- Exploring modifications and improvements to the model architecture itself. For example, the authors suggest trying different decoding strategies for handling multi-token answers in open-ended VideoQA. They also suggest ideas like masking longer spans during pretraining to better handle multi-token answers.- Applying the model to more complex multi-modal text generation tasks like video captioning. The authors note their current model is specialized for VideoQA tasks and cannot straightforwardly handle open-ended generation tasks. Modifications could make the model applicable to a broader range of multi-modal problems.- Analyzing the biases and limitations of the model when deployed in real applications. The authors encourage investigating the potential biases in the training data and considering the societal impacts.So in summary, the main directions mentioned are scaling up the model size, using more training data, tweaking the model architecture itself, expanding to broader tasks, and analyzing the real-world impacts. The authors seem to see promise in evolving both the technical methods and the ethical/social considerations.


## Summarize the paper in one paragraph.

Here is a one paragraph summary of the paper:The paper presents FrozenBiLM, a framework for zero-shot video question answering (VideoQA). FrozenBiLM combines a frozen pretrained bidirectional language model (BiLM) with a frozen pretrained visual encoder using additional lightweight trainable modules. It is trained on web-scraped video-text data with a visually-conditioned masked language modeling loss, without using any explicit VisualQA supervision. At test time, it enables zero-shot VideoQA through input prompt engineering, where the task is formulated as filling in a masked token in the prompt. Experiments demonstrate that FrozenBiLM substantially outperforms prior work on diverse zero-shot VideoQA benchmarks. It also shows strong performance when finetuned on small fractions of VideoQA datasets, introducing a new few-shot VideoQA setting. Ablations exhibit the benefits of freezing the BiLM weights, the impact of video and speech modalities, and the superiority over frozen autoregressive models. The work provides an efficient parameter-based solution for zero-shot and low-resource VideoQA.The key ideas are leveraging a frozen bidirectional language model for zero-shot VideoQA, training it on web data without manual VisualQA annotation, formulating VideoQA as masked language modeling at test time, and showing strong performance in zero-shot, few-shot and fully supervised settings. The approach is highly parameter and compute efficient compared to prior work.


## Summarize the paper in two paragraphs.

Here is a two paragraph summary of the paper:The paper presents a framework called FrozenBiLM for zero-shot video question answering. The method uses a frozen pretrained bidirectional language model (BiLM) and combines it with a frozen visual encoder using additional lightweight trainable modules. The BiLM and visual encoder are first pretrained on large amounts of text and image data respectively. The additional modules, which include a visual-to-text projection and adapter layers, are then trained on a dataset of 10 million video-caption pairs scraped from the web. A visually-conditioned masked language modeling loss is used for this training. At test time, the method is adapted for downstream video QA tasks by formulating them as cloze-style masked language modeling problems. Inputs are formatted as text prompts with a masked token representing the answer.The proposed FrozenBiLM method is evaluated on a range of zero-shot video QA datasets including open-ended QA, multiple choice QA, and fill-in-the-blank tasks. It substantially outperforms prior work and baselines across all datasets. Ablation studies demonstrate the importance of the frozen BiLM, adapters, visual input, and web-scale training data. Additional experiments show FrozenBiLM also achieves strong performance in few-shot and fully supervised settings. The compact trainable parameters make it particularly suitable when annotation resources are limited. Overall, the work presents an efficient and effective approach for video QA without manually annotated training data by leveraging web-scale pretraining and a frozen bidirectional architecture.


## Summarize the main method used in the paper in one paragraph.

The paper proposes a framework for zero-shot video question answering (VideoQA) by leveraging a frozen bidirectional language model (BiLM). The key aspects are:1. They use a pretrained frozen BiLM which provides strong text-only question answering capabilities. 2. They combine the BiLM with a pretrained frozen visual encoder using a lightweight visual-to-text projection layer and adapter modules inserted in the BiLM. This enables joint video-text reasoning while keeping the BiLM frozen.3. The additional modules are trained on a large dataset of video-text pairs scraped from the web, using a visually-conditioned masked language modeling (MLM) objective.4. For downstream VideoQA tasks, they formulate prompts in a cloze form where the model only needs to fill in a mask token, similar to the MLM pretraining. This allows zero-shot transfer without using any VideoQA supervision.5. Both in zero-shot and supervised settings, freezing the pretrained BiLM gives better performance compared to fine-tuning it, while being more parameter-efficient.6. Extensive experiments show the model achieves new state-of-the-art on diverse zero-shot VideoQA benchmarks. It also demonstrates strong performance when finetuned on small fractions of VideoQA data.In summary, the key innovation is leveraging frozen BiLMs for zero-shot VideoQA via lightweight adaptation and prompting, outperforming prior work relying on larger frozen autoregressive LMs. The framework provides an efficient and effective approach for multi-modal reasoning.
