# [Zero-Shot Video Question Answering via Frozen Bidirectional Language   Models](https://arxiv.org/abs/2206.08155)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is how to develop an effective approach for zero-shot video question answering (VideoQA) that does not require manually annotated visual question-answer data for training. The key hypothesis is that a framework based on a frozen bidirectional language model can achieve strong zero-shot VideoQA performance when combined with a visual encoder and trained on readily available web video-text data.The introduction states that recent work has explored using large frozen autoregressive language models for zero-shot visual question answering, but that lighter bidirectional models may provide a more efficient alternative. The paper proposes adapting such bidirectional models to the video domain and investigating their zero-shot capabilities.The overall research question is how well the proposed frozen bidirectional language model framework, which they term FrozenBiLM, can perform on zero-shot VideoQA compared to prior work and autoregressive models. The hypothesis is that it will outperform previous approaches and demonstrate the benefits of frozen bidirectional language models for this task.


## What is the main contribution of this paper?

Based on my reading, the main contribution of this paper is proposing a framework called FrozenBiLM for zero-shot video question answering (VideoQA). The key ideas are:1. The framework uses a frozen bidirectional language model (BiLM) pretrained on large text corpora, which provides strong textual reasoning abilities. 2. The BiLM is combined with a pretrained visual encoder using additional lightweight modules that are trained on web-scraped video-text data. This allows adapting the model to multi-modal inputs while keeping the BiLM frozen.3. Several video-language tasks like open-ended VideoQA are formulated in a cloze/fill-in-the-blank form and solved by masked language modeling using the adapted model, without any explicit VideoQA supervision.4. Experiments show FrozenBiLM significantly outperforms prior work on multiple zero-shot VideoQA benchmarks. It also demonstrates strong performance in few-shot and fully-supervised settings.5. Comparisons to autoregressive models indicate frozen BiLMs provide better accuracy and efficiency for zero-shot VideoQA.In summary, the main contribution is proposing an effective framework for zero-shot VideoQA that exploits frozen bidirectional language models and web-scale multi-modal data. The approach sets new state-of-the-art results across several datasets.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper presents a framework for zero-shot video question answering that uses frozen bidirectional language models adapted to multi-modal inputs and demonstrates superior performance compared to prior work.


## How does this paper compare to other research in the same field?

Here are a few key points comparing this paper to other related research:- This paper focuses on zero-shot video question answering (VideoQA), where the model is trained without any explicit Visual QA supervision. This is an emerging but still relatively less explored setting compared to fully supervised VideoQA. - The paper builds on frozen bidirectional language models, in contrast to prior work like FLAMINGO, CLIP-Cap which rely on frozen autoregressive models like GPT-3. The results show frozen bidirectional models are more efficient and achieve better performance.- For zero-shot learning, the paper uses a masked language modeling loss on web-scraped video-text data. Other recent work like BLIP and Just Ask use different forms of zero-shot learning like image QA supervision or generated QA data.- This is the first work to demonstrate the capability of frozen bidirectional language models for zero-shot VideoQA. Prior zero-shot VQA work focused only on autoregressive models.- The proposed model outperforms prior state-of-the-art on multiple zero-shot VideoQA benchmarks by significant margins. It also shows competitive performance in supervised settings.- The model relies only on readily available web data, avoiding the need for manually annotated VQA datasets or data generation pipelines. This improves the scalability and reduces training costs.In summary, this paper pushes the state-of-the-art for zero-shot VideoQA through an efficient framework based on frozen bidirectional models. The results are very promising and demonstrate these models are well-suited for zero-shot cross-modal reasoning when adapted properly.
