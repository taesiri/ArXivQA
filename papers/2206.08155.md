# [Zero-Shot Video Question Answering via Frozen Bidirectional Language   Models](https://arxiv.org/abs/2206.08155)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is how to develop an effective approach for zero-shot video question answering (VideoQA) that does not require manually annotated visual question-answer data for training. The key hypothesis is that a framework based on a frozen bidirectional language model can achieve strong zero-shot VideoQA performance when combined with a visual encoder and trained on readily available web video-text data.The introduction states that recent work has explored using large frozen autoregressive language models for zero-shot visual question answering, but that lighter bidirectional models may provide a more efficient alternative. The paper proposes adapting such bidirectional models to the video domain and investigating their zero-shot capabilities.The overall research question is how well the proposed frozen bidirectional language model framework, which they term FrozenBiLM, can perform on zero-shot VideoQA compared to prior work and autoregressive models. The hypothesis is that it will outperform previous approaches and demonstrate the benefits of frozen bidirectional language models for this task.


## What is the main contribution of this paper?

Based on my reading, the main contribution of this paper is proposing a framework called FrozenBiLM for zero-shot video question answering (VideoQA). The key ideas are:1. The framework uses a frozen bidirectional language model (BiLM) pretrained on large text corpora, which provides strong textual reasoning abilities. 2. The BiLM is combined with a pretrained visual encoder using additional lightweight modules that are trained on web-scraped video-text data. This allows adapting the model to multi-modal inputs while keeping the BiLM frozen.3. Several video-language tasks like open-ended VideoQA are formulated in a cloze/fill-in-the-blank form and solved by masked language modeling using the adapted model, without any explicit VideoQA supervision.4. Experiments show FrozenBiLM significantly outperforms prior work on multiple zero-shot VideoQA benchmarks. It also demonstrates strong performance in few-shot and fully-supervised settings.5. Comparisons to autoregressive models indicate frozen BiLMs provide better accuracy and efficiency for zero-shot VideoQA.In summary, the main contribution is proposing an effective framework for zero-shot VideoQA that exploits frozen bidirectional language models and web-scale multi-modal data. The approach sets new state-of-the-art results across several datasets.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper presents a framework for zero-shot video question answering that uses frozen bidirectional language models adapted to multi-modal inputs and demonstrates superior performance compared to prior work.
