# [Functorial Manifold Learning](https://arxiv.org/abs/2011.07435)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the main research goals of this paper are:1. To develop a categorical framework for manifold learning algorithms that allows expressing them as functors mapping between categories of metric spaces, clusterings, and optimization objectives. 2. To use this framework to characterize and relate different manifold learning algorithms in terms of the dataset transformations they are equivariant to.3. To prove results about the stability and approximation quality of manifold learning algorithms using the interleaving distance between functors. 4. To provide a procedure for creating new manifold learning algorithms by recombining components of existing algorithms while preserving functoriality.In more detail:- The paper represents manifold learning algorithms as functors that factor through hierarchical clustering functors. This allows adapting clustering theory into manifold learning theory.- Manifold learning algorithms are categorized based on the dataset transformations they are equivariant to, such as isometries or surjective maps. Popular algorithms like MDS, Isomap, and UMAP are expressed in this framework.- Interleaving distance between functors is used to study how well embeddings learned from noisy data approximate embeddings from noiseless data. Bounds are provided for a general class of manifold learning algorithms.- New manifold learning algorithms are derived by recombining the clustering and loss function components of existing algorithms. An example "Single Linkage Scaling" algorithm is introduced and evaluated.In summary, the main goals are to develop a categorical characterization of manifold learning to understand equivariance, stability, and enable principled recombination of existing algorithms into new ones.


## What is the main contribution of this paper?

The main contributions of this paper are:1. It develops a categorical framework for manifold learning by representing manifold learning algorithms as functors mapping pseudometric spaces to optimization objectives. 2. It shows how manifold learning algorithms can be characterized by the composition of a hierarchical clustering functor and a loss function. This allows clustering theory to be formally extended to manifold learning.3. It introduces a hierarchy of manifold learning algorithms based on their equivariance properties. Several popular algorithms like MDS, Isomap, and UMAP are expressed in this framework.4. It analyzes the stability of a broad class of manifold learning algorithms using interleaving distance. Bounds are presented on how well embeddings learned from noisy data approximate embeddings from clean data.5. It demonstrates how the functorial perspective enables deriving new algorithms through recombination. As an example, a novel Single Linkage Scaling algorithm is introduced and shown to outperform MDS on a DNA recombination task.In summary, the key contribution is developing a categorical framework for manifold learning based on functors. This provides tools to analyze, characterize, and derive new manifold learning algorithms while preserving desired equivariance and stability properties. The functorial viewpoint yields theoretical insights as well as practical techniques for working with manifold learning methods.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points of the paper:The paper develops a categorical framework for manifold learning algorithms by characterizing them as functors that map pseudometric spaces to optimization objectives through hierarchical clustering functors, allowing properties like equivariance and stability to be analyzed.
