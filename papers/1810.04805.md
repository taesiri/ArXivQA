# [BERT: Pre-training of Deep Bidirectional Transformers for Language   Understanding](https://arxiv.org/abs/1810.04805)

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper addresses is whether pre-training deep bidirectional representations can improve performance on a variety of natural language processing tasks. The key hypothesis is that pre-training a deep bidirectional Transformer model using masked language modeling and next sentence prediction objectives allows the model to learn powerful representations that can be effectively fine-tuned for downstream NLP tasks.Specifically, the paper introduces BERT (Bidirectional Encoder Representations from Transformers), a pre-trained bidirectional Transformer model. The goal is to demonstrate that BERT representations lead to substantial gains across a diverse set of NLP tasks with minimal task-specific architecture modifications.The paper validates this hypothesis by fine-tuning BERT on eleven natural language processing tasks and showing it achieves state-of-the-art results on tasks like question answering, textual entailment, sentiment analysis, and named entity recognition. The strong empirical results across multiple benchmark datasets support the claim that deep bidirectional pre-training provides significant benefits for language understanding.In summary, the key research question is whether bidirectional pre-training can boost performance across various NLP tasks, which the authors affirm through pre-training BERT and fine-tuning it for downstream tasks.


## What is the main contribution of this paper?

The main contribution of this paper is proposing BERT, a new language representation model based on the Transformer architecture. BERT is pretrained on large unlabeled text corpora using two novel unsupervised prediction tasks: masked language modeling and next sentence prediction. This pretraining allows BERT to learn deep bidirectional representations that can then be fine-tuned for a wide range of NLP tasks. The key innovations are:- BERT uses masked language modeling to enable pretraining a deep bidirectional Transformer encoder, overcoming the constraint of previous models like ELMo and OpenAI GPT that used shallow concatenation of left-to-right and right-to-left models or unidirectional architectures.- BERT introduces a "next sentence prediction" pretraining task to better model sentence relationships and understand document structure.- The same BERT model architecture can be used for both pretraining and downstream tasks by simply swapping the inputs and outputs. Minimal task-specific parameters need to be learned from scratch.- BERT achieves state-of-the-art results on a diverse range of NLP tasks like GLUE, SQuAD question answering, and named entity recognition by fine-tuning the same pretrained model. The transfer learning enabled by BERT's pretraining significantly reduces the need for heavily engineered task-specific architectures.In summary, BERT's innovation is in pretraining a deep bidirectional Transformer model using masked language modeling and next sentence prediction, which creates representations that transfer well across different NLP tasks through fine-tuning. This simplicity and effectiveness is the main contribution.
