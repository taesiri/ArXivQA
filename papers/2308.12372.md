# [Vision Transformer Adapters for Generalizable Multitask Learning](https://arxiv.org/abs/2308.12372)

## What is the central research question or hypothesis that this paper addresses?

The central research question addressed in this paper is: How can we learn transferable and generalizable task affinities in multitask learning so that the learned task affinities can transfer to novel tasks and generalize to novel domains?The key hypothesis is that by integrating pre-trained vision transformer models with a novel task-adapted attention mechanism within adapter modules, the model can learn task affinities that are transferable to new tasks and generalizable to new datasets/domains without requiring full retraining or fine-tuning.In summary, the paper introduces a vision transformer adapter framework to efficiently learn generalizable multitask representations in a parameter-efficient manner. The key contributions are:1) A task-adapted attention mechanism that combines gradient-based task similarity with attention to automatically learn task dependencies across all tasks.2) The learned task affinities transfer to novel tasks in zero-shot transfer learning and generalize to new domains without fine-tuning.3) Integration with off-the-shelf vision transformer backbones like Swin to leverage pretrained representations. 4) Significantly lower parameters than baseline multitask vision transformers while achieving better performance.5) State-of-the-art results on multiple dense prediction tasks across different multitask settings like MTL, zero-shot task transfer, unsupervised domain adaptation etc.So in essence, the paper focuses on learning transferable and generalizable multitask representations efficiently by introducing adapters with a novel task-adapted attention mechanism on top of pretrained vision transformers.


## What is the main contribution of this paper?

The main contributions of this paper are:1. It introduces vision transformer adapters for generalizable multitask learning that can learn transferable and generalizable task affinities. 2. It proposes a novel task-adapted attention (TAA) mechanism that combines gradient-based task similarities from TROA with attention-based ones to learn the task affinities.3. The learned task affinities generalize to three settings - zero-shot task transfer, unsupervised domain adaptation, and generalization to novel domains without fine-tuning.4. The multitasking vision transformer adapters can be integrated with different transformer backbones like ViT, Swin, Pyramid Transformer, and Focal Transformer in a parameter-efficient way.5. Experiments show the method outperforms existing CNN-based and vision transformer-based multitask learning methods on metrics for semantic segmentation, depth estimation, surface normal prediction and edge detection across different datasets.In summary, the key innovation is the introduction of vision transformer adapters with a task-adapted attention mechanism to learn generalizable task affinities that transfer across different settings, while being parameter-efficient by building upon a frozen pre-trained transformer backbone.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper introduces vision transformer adapters that learn generalizable task relationships which can be applied to novel tasks and domains in a parameter-efficient manner by combining gradient-based and attention-based task affinities.
