# [Vision Transformer Adapters for Generalizable Multitask Learning](https://arxiv.org/abs/2308.12372)

## What is the central research question or hypothesis that this paper addresses?

The central research question addressed in this paper is: How can we learn transferable and generalizable task affinities in multitask learning so that the learned task affinities can transfer to novel tasks and generalize to novel domains?The key hypothesis is that by integrating pre-trained vision transformer models with a novel task-adapted attention mechanism within adapter modules, the model can learn task affinities that are transferable to new tasks and generalizable to new datasets/domains without requiring full retraining or fine-tuning.In summary, the paper introduces a vision transformer adapter framework to efficiently learn generalizable multitask representations in a parameter-efficient manner. The key contributions are:1) A task-adapted attention mechanism that combines gradient-based task similarity with attention to automatically learn task dependencies across all tasks.2) The learned task affinities transfer to novel tasks in zero-shot transfer learning and generalize to new domains without fine-tuning.3) Integration with off-the-shelf vision transformer backbones like Swin to leverage pretrained representations. 4) Significantly lower parameters than baseline multitask vision transformers while achieving better performance.5) State-of-the-art results on multiple dense prediction tasks across different multitask settings like MTL, zero-shot task transfer, unsupervised domain adaptation etc.So in essence, the paper focuses on learning transferable and generalizable multitask representations efficiently by introducing adapters with a novel task-adapted attention mechanism on top of pretrained vision transformers.


## What is the main contribution of this paper?

The main contributions of this paper are:1. It introduces vision transformer adapters for generalizable multitask learning that can learn transferable and generalizable task affinities. 2. It proposes a novel task-adapted attention (TAA) mechanism that combines gradient-based task similarities from TROA with attention-based ones to learn the task affinities.3. The learned task affinities generalize to three settings - zero-shot task transfer, unsupervised domain adaptation, and generalization to novel domains without fine-tuning.4. The multitasking vision transformer adapters can be integrated with different transformer backbones like ViT, Swin, Pyramid Transformer, and Focal Transformer in a parameter-efficient way.5. Experiments show the method outperforms existing CNN-based and vision transformer-based multitask learning methods on metrics for semantic segmentation, depth estimation, surface normal prediction and edge detection across different datasets.In summary, the key innovation is the introduction of vision transformer adapters with a task-adapted attention mechanism to learn generalizable task affinities that transfer across different settings, while being parameter-efficient by building upon a frozen pre-trained transformer backbone.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper introduces vision transformer adapters that learn generalizable task relationships which can be applied to novel tasks and domains in a parameter-efficient manner by combining gradient-based and attention-based task affinities.


## How does this paper compare to other research in the same field?

This paper introduces Vision Transformer Adapters for Generalizable Multitask Learning (AVTaR). Here is a summary of how it compares to other related research:- Most prior multitask learning methods, whether CNN or transformer based, do not learn task affinities that generalize to novel tasks or domains. AVTaR proposes a novel task-adapted attention mechanism to learn transferable and generalizable task affinities.- Compared to other transformer-based multitask learning methods like IPT, ST-MTL, Vid-MTL, MulT, etc., AVTaR can be integrated with any off-the-shelf vision transformer backbone instead of using handcrafted architectures. It also learns task affinities across all tasks automatically, rather than in a pairwise manner.- Unlike prior works on transformer adapters which adapt to single downstream tasks, AVTaR proposes adapters for multitask learning. The task affinities learned also transfer across settings like zero-shot task transfer and unsupervised domain adaptation.- Experiments show AVTaR outperforms state-of-the-art CNN and transformer multitask learning methods on tasks like semantic segmentation, depth estimation, surface normal prediction, etc. It is also more parameter efficient.- Limitations include reliance on large labelled training data and lack of capability for unsupervised multitask learning. But the work demonstrates the potential of using adapters to enable vision transformers to dynamically adapt for multitasking.In summary, AVTaR advances multitask learning research by proposing methods to learn more generalizable task relationships in an efficient transformer adapter framework. The transferable task affinities and integration with standard vision transformer backbones are valuable contributions.


## What future research directions do the authors suggest?

Based on my reading of the paper, some of the key future research directions suggested by the authors are:- Improving the generalization ability of the task affinities to more disparate domains. The authors note their model currently generalizes well between datasets with similar categories (e.g. indoor scenes), but struggles on very different domains like comics. Further work could aim to learn more transferable task representations.- Extending the framework to unsupervised multitask learning settings, where paired labels are not available across tasks. The current model requires fully supervised training data. Developing an unsupervised version could broaden the applicability.- Incorporating different pre-training modalities like text or video, as has been done in some prior work on adapters. This could further enrich the learned task representations.- Addressing the data dependency limitation, where performance degrades given limited training data. Methods to learn from less data could make the approach more practical. - Reducing the environmental impact of training such large vision transformer models, for example by using efficient architectures or distillation.- Testing the framework on a broader set of vision tasks beyond the dense prediction tasks focused on in the paper. Assessing the versatility of the approach across diverse tasks.- Exploring alternate attention mechanisms or losses to see if the learned task affinities can be further improved. There is room to experiment with the adapter design.In summary, the key directions are improving the generalization and efficiency of the model, extending it to more training scenarios and tasks, and further exploring adapter-based architectures for multitask learning. Advancing those aspects could build nicely on the authors' initial work.
