# [Learning Hierarchical Prompt with Structured Linguistic Knowledge for   Vision-Language Models](https://arxiv.org/abs/2312.06323)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a high-quality paragraph summarizing the key points of the paper:

This paper proposes a novel Hierarchical Prompt Tuning (HPT) method to enhance visual recognition models by incorporating both structured and unstructured linguistic knowledge from language models. It first leverages ChatGPT to generate human-like descriptions and structured relationships of categories. HPT then conducts prompt tuning in a hierarchical manner with global, high-level, and low-level prompts. The low-level prompts model structured knowledge using a relationship-guided attention module to capture pairwise connections between entities and attributes. The high-level prompts summarize the overall semantics from multiple descriptions. Cross-level self-attention links prompts across levels to handle complex relationships. Experiments on various generalization tasks demonstrate that modeling both forms of knowledge with hierarchical tuning leads to significant improvements in few-shot learning on seen classes and zero-shot generalization to unseen classes. The method shows state-of-the-art performance by effectively transferring knowledge from descriptions and structures to tune prompts that better align text and image representations.
