# [Learned Low Precision Graph Neural Networks](https://arxiv.org/abs/2009.09232)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is:How can we systematically quantize Graph Neural Networks (GNNs) to reduce their computation and memory cost with minimal loss in performance?The key points are:- GNNs are useful for analyzing graph structured data but can be costly to run at scale. Quantization is a common technique to reduce computation/memory overhead of neural networks.- However, prior quantization methods focus on CNNs/RNNs and may not transfer well to GNNs due to their different architecture. - This paper proposes a novel neural architecture search method called LPGNAS to automatically design quantized GNNs.- LPGNAS identifies the quantizable components in GNN blocks and defines a mixed quantization search space at the microarchitecture level.- It uses a single-path, one-shot, gradient-based approach to learn the optimal quantization strategy coupled with the architecture.- Experiments show LPGNAS can find compact quantized GNNs that match or outperform manual/NAS baselines, with significant reductions in model and buffer size.So in summary, the key hypothesis is that by automatically co-searching architecture and mixed quantization strategies using LPGNAS, they can find highly efficient quantized GNN models that maintain accuracy while greatly reducing size. The experiments aim to validate this hypothesis across multiple datasets.


## What is the main contribution of this paper?

Based on my reading, the main contributions of this paper are:1. Proposing the first method for systematically quantizing Graph Neural Networks (GNNs). The authors identify the quantizable components and possible search space for GNN quantization.2. Presenting a novel single-path, one-shot, gradient-based Neural Architecture Search (NAS) algorithm called LPGNAS for automatically finding low precision GNNs.3. Evaluating LPGNAS on 8 different datasets for node classification tasks and showing significant reductions in model and buffer sizes compared to manually designed networks and other NAS methods, while maintaining similar accuracy.4. Analyzing the quantization statistics collected across different datasets and revealing that LPGNAS converges to a 4-bit weight, 8-bit activation fixed-point quantization. This provides insights into the limitations of current fixed-point quantization strategies for both manual and NAS-generated GNNs.In summary, the key contribution is using NAS to automatically find quantized GNN architectures that achieve state-of-the-art accuracy with much lower resource requirements compared to prior works. The paper also provides useful empirical findings on GNN quantization limitations that can guide future research.


## How does this paper compare to other research in the same field?

Here are a few thoughts on how this paper compares to other research in graph neural networks and network quantization:- This is the first work I'm aware of that studies quantizing graph neural networks specifically. Most prior quantization work has focused on convolutional neural networks, so this explores new territory.- The approach of using network architecture search to find optimal quantized graph networks is novel. Other NAS methods for graphs have focused only on the architecture, not the numerical precision. Combining NAS for both architecture and quantization in one framework is a nice contribution.- The results demonstrate state-of-the-art accuracy on multiple graph datasets using quantized models. The tradeoff curves showing accuracy vs model size and buffer size are convincing, with LPGNAS models clearly Pareto-dominating prior methods. This shows the quantized models found by NAS can be very efficient.- Analyzing the quantization statistics on many datasets provides useful insights, like the common convergence to 4-bit weights and 8-bit activations. This could guide future hand-designed quantized graph networks and quantization hardware focused on GNNs.- Comparisons to prior graph NAS methods like PDNAS and AutoGNN show superior accuracy and/or efficiency. The significant reductions in search time versus manual search are also noteworthy.Overall, I think the paper makes solid contributions to an important open problem. Using NAS to find quantized graph networks is innovative and the results convincingly demonstrate the benefits compared to prior art. The paper provides a strong foundation for future work to build on in quantizing, optimizing and accelerating graph neural networks.


## What future research directions do the authors suggest?

Based on my reading of the paper, here are some of the main future research directions suggested by the authors:- Investigating more advanced quantization schemes like mixed precision and per-channel quantization for GNNs, using the 4-bit weight and 8-bit activation fixed-point quantization found by their method as a strong baseline to compare against. - Exploring different NAS methods like reinforcement learning or evolutionary algorithms for finding specialized quantized GNN architectures. Their gradient-based NAS approach provides a good starting point.- Applying the ideas to other graph neural network architectures beyond the specific MPNN variants focused on in this work.- Testing the approach on a broader range of graph-based tasks like graph classification, link prediction, etc. The current work looks mainly at node classification.- Evaluating the quantized models found by their method on specialized AI accelerators and quantized arithmetic units to validate benefits in latency, throughput and energy efficiency.- Extending the ideas to find quantized models for other graph representation learning methods like graph autoencoders, contrastive methods, etc.- Leveraging the statistics collected on quantization choices to guide development of mixed precision architectures and quantization-aware training methods specialized for GNNs.In summary, the authors lay a solid groundwork for quantizing GNNs and propose many promising avenues for future work to build upon their approach and findings. Key directions include exploring more advanced quantization techniques, applying the ideas to new graph network architectures and tasks, and leveraging the results to guide specialized hardware design.


## Summarize the paper in one paragraph.

Here is a one paragraph summary of the paper:The paper proposes a novel method called Low Precision Graph NAS (LPGNAS) to automatically design quantized graph neural networks (GNNs) with minimal loss in performance. LPGNAS identifies components in GNNs that can be quantized and defines a quantization search space. It uses a single-path, one-shot, gradient-based neural architecture search algorithm to jointly learn the optimal GNN architecture and quantization strategy by training a supernet. Experiments on node classification tasks across 8 datasets show LPGNAS can find highly compact quantized GNNs that significantly reduce model and buffer sizes while maintaining accuracy competitive with manually designed networks and prior NAS methods. The results also reveal a limitation of current fixed-point quantization schemes for GNNs, with the method converging to 4-bit weights and 8-bit activations across datasets.


## Summarize the paper in two paragraphs.

Here is a two paragraph summary of the paper:This paper proposes a novel method called Low Precision Graph NAS (LPGNAS) for automatically designing quantized graph neural networks (GNNs). GNNs have shown promising performance on tasks involving graph-structured data, but can be costly to run due to high computation and memory requirements. Quantization is a common technique to reduce the complexity of deep neural networks, but prior methods focus mainly on CNNs and do not transfer well to GNNs. The key contributions of this work are: 1) identifying the quantizable components and search space for GNN quantization, which differs from CNNs, 2) presenting LPGNAS, a gradient-based neural architecture search method, to jointly learn the optimal GNN architecture and per-component quantization strategies, 3) demonstrating that LPGNAS can find highly compact yet accurate quantized GNN models, significantly outperforming prior NAS methods and hand-designed networks, and 4) revealing empirically that LPGNAS tends to converge to 4-bit weights and 8-bit activations as a limitation of current quantization techniques for GNNs. Experiments on 8 datasets for node classification tasks show LPGNAS can find models with 2.3x smaller size but 0.4% better accuracy compared to the best prior NAS method.


## Summarize the main method used in the paper in one paragraph.

Here is a one paragraph summary of the main method used in the paper:The paper proposes a novel single-path, one-shot and gradient-based neural architecture search (NAS) algorithm called Low Precision Graph NAS (LPGNAS) for automatically designing quantized graph neural networks (GNNs). LPGNAS defines the quantization search space for GNNs at a micro-architecture level, allowing different sub-blocks in a GNN layer to have different quantization strategies. It uses two controllers - one for architecture and one for quantization - to produce probabilities for selecting architectural and quantization options. The choices are constrained to be differentiable so that standard backpropagation can be used to learn the optimal architecture and quantization strategy in a single search round. By co-optimizing architecture and quantization, LPGNAS generates compact quantized GNNs that achieve high accuracy with significant reductions in model and buffer size compared to manually designed networks and prior NAS methods.
