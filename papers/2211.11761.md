# [From Node Interaction to Hop Interaction: New Effective and Scalable   Graph Learning Paradigm](https://arxiv.org/abs/2211.11761)

## What is the central research question or hypothesis that this paper addresses?

 The central research question this paper addresses is:

Can we develop a new graph learning paradigm to address the scalability and over-smoothing limitations of existing graph neural networks simultaneously, achieving high performance while maintaining efficiency?

The key hypotheses are:

1) By converting the interaction target in GNNs from nodes to multi-hop features inside each node, we can achieve effective feature discrimination without complex node interactions.

2) Conducting non-linear interactions among the pre-processed multi-hop features can capture discriminative clues for node classification and enhance model performance. 

3) The proposed hop interaction framework is general and flexible to incorporate advanced objectives like self-supervised learning for further improvements.

4) The hop interaction paradigm can achieve state-of-the-art performance on diverse graph datasets while maintaining high efficiency and scalability.

In summary, the paper proposes a new hop interaction paradigm for graph representation learning to achieve effectiveness, scalability, and flexibility simultaneously. The core idea is to shift the interaction target in GNNs from nodes to multi-hop features within each node. This is hypothesized to avoid the scalability issue of node interactions while still capturing informative feature co-occurrences for discrimination. Experiments validate these hypotheses, showing superior performance and efficiency.


## What is the main contribution of this paper?

 The main contributions of this paper are:

1. It proposes a new graph learning paradigm called "hop interaction" that shifts the interaction target in graph neural networks (GNNs) from nodes to hops (pre-processed multi-hop neighbor features) inside each node. This allows avoiding complex node interactions while still capturing important higher-order structure information.

2. It designs a general and flexible framework called HopGNN for hop interaction. HopGNN has four main steps: hop pre-processing, hop encoding, hop interaction, and hop fusion/prediction. It can leverage standard GNN architectures like GAT for the hop interaction step.

3. It develops a multi-task learning strategy to combine HopGNN with a self-supervised learning objective. This further improves the model's performance without significantly increasing complexity.

4. It conducts extensive experiments on 12 benchmark datasets showing HopGNN achieves state-of-the-art performance compared to other GNN methods. Importantly, HopGNN maintains high efficiency and scalability to large graphs.

In summary, the key innovation is the new hop interaction paradigm that circumvents limitations of prior node and layer interaction approaches. By shifting to pre-processed hop features, HopGNN obtains strong expressiveness and discrimination ability while remaining simple and scalable. The paper demonstrates this new paradigm effectively addresses key challenges in graph representation learning.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the key points from the paper:

The paper proposes a new graph learning paradigm called HopGNN that converts node interactions in GNNs to hop interactions among pre-computed multi-hop features inside each node, achieving superior performance and scalability.


## How does this paper compare to other research in the same field?

 This paper introduces a new graph learning paradigm called "hop interaction" that aims to address key limitations of existing graph neural networks, particularly around scalability and over-smoothing. Here are some key ways this work relates to and compares with other research in GNNs:

- Most prior GNNs follow a node interaction paradigm, where information is propagated between neighboring nodes iteratively. This can be computationally expensive and lead to over-smoothing. This paper proposes converting the interaction target from nodes to multi-hop features within each node.

- To improve scalability, the paper builds on efficient "decoupled" GNNs like SGC and SIGN which separate feature propagation from training. However, decoupled GNNs use linear feature combinations which limit expressiveness. This work introduces nonlinear interactions among hops to be more powerful while maintaining efficiency.

- Compared to advanced node interaction GNNs like GCNII, GPRGNN, and heterophily-specialized models, hop interaction achieves better performance and scalability without sophisticated node aggregation schemes. It matches or exceeds state-of-the-art on benchmarks.

- For smoother graphs, prior methods like GCNII and GPRGNN prevent over-smoothing. But hop interaction still outperforms on heterophily benchmarks, demonstrating it is more generally effective.

- The framework allows flexibly incorporating self-supervised objectives like Barlow Twins. This further improves performance and shows the approach can integrate modern training techniques.

In summary, hop interaction offers a new graph learning perspective that bridges efficiency of decoupled GNNs and expressiveness of advanced interaction techniques. It achieves strong empirical performance across diverse graphs while scaling easily to large datasets. The work compares favorably to existing state-of-the-art node interaction and decoupled GNNs.
