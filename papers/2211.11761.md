# [From Node Interaction to Hop Interaction: New Effective and Scalable   Graph Learning Paradigm](https://arxiv.org/abs/2211.11761)

## What is the central research question or hypothesis that this paper addresses?

 The central research question this paper addresses is:

Can we develop a new graph learning paradigm to address the scalability and over-smoothing limitations of existing graph neural networks simultaneously, achieving high performance while maintaining efficiency?

The key hypotheses are:

1) By converting the interaction target in GNNs from nodes to multi-hop features inside each node, we can achieve effective feature discrimination without complex node interactions.

2) Conducting non-linear interactions among the pre-processed multi-hop features can capture discriminative clues for node classification and enhance model performance. 

3) The proposed hop interaction framework is general and flexible to incorporate advanced objectives like self-supervised learning for further improvements.

4) The hop interaction paradigm can achieve state-of-the-art performance on diverse graph datasets while maintaining high efficiency and scalability.

In summary, the paper proposes a new hop interaction paradigm for graph representation learning to achieve effectiveness, scalability, and flexibility simultaneously. The core idea is to shift the interaction target in GNNs from nodes to multi-hop features within each node. This is hypothesized to avoid the scalability issue of node interactions while still capturing informative feature co-occurrences for discrimination. Experiments validate these hypotheses, showing superior performance and efficiency.


## What is the main contribution of this paper?

 The main contributions of this paper are:

1. It proposes a new graph learning paradigm called "hop interaction" that shifts the interaction target in graph neural networks (GNNs) from nodes to hops (pre-processed multi-hop neighbor features) inside each node. This allows avoiding complex node interactions while still capturing important higher-order structure information.

2. It designs a general and flexible framework called HopGNN for hop interaction. HopGNN has four main steps: hop pre-processing, hop encoding, hop interaction, and hop fusion/prediction. It can leverage standard GNN architectures like GAT for the hop interaction step.

3. It develops a multi-task learning strategy to combine HopGNN with a self-supervised learning objective. This further improves the model's performance without significantly increasing complexity.

4. It conducts extensive experiments on 12 benchmark datasets showing HopGNN achieves state-of-the-art performance compared to other GNN methods. Importantly, HopGNN maintains high efficiency and scalability to large graphs.

In summary, the key innovation is the new hop interaction paradigm that circumvents limitations of prior node and layer interaction approaches. By shifting to pre-processed hop features, HopGNN obtains strong expressiveness and discrimination ability while remaining simple and scalable. The paper demonstrates this new paradigm effectively addresses key challenges in graph representation learning.
