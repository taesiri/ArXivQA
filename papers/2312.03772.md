# [DiffusionAtlas: High-Fidelity Consistent Diffusion Video Editing](https://arxiv.org/abs/2312.03772)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper "DiffusionAtlas: High-Fidelity Consistent Diffusion Video Editing":

Problem:
Existing diffusion models for image editing struggle with video editing due to the challenge of maintaining consistency in object appearance across frames over time. Atlas-based techniques can propagate edits consistently using layered video representations, but they are limited by fixed UV mappings that restrict object changes. 

Proposed Solution:
The paper presents DiffusionAtlas, an atlas-based diffusion video editing method that achieves temporal consistency while allowing high-fidelity object changes. The key ideas are:

1) Edit objects directly on diffusion atlases instead of frames, enabling coherent identities. A visual-textual diffusion model is fine-tuned with atlas structures and image embeddings for guidance.

2) Optimize UV mappings to refine distortions when propagating new atlases back to frames. Constraints exploiting original UV space and a pretrained diffusion model provide pixel-level guidance.

Main Contributions:
- A one-shot tuning approach for diffusion models that can directly edit atlas textures while preserving input structures without extra correspondences.

- An optimization procedure with losses constrained by properties of the original UV space to create better coordinate correlations between edited atlases and video frames.

- State-of-the-art performance in producing high-fidelity and temporally consistent editing results with both textual and visual conditions.

- Extensive experiments demonstrating superior video quality over existing methods in maintaining object fidelity and identities over time.

The key novelty is enabling diffusion models to edit videos directly on appearance layers to balance flexibility and consistency, outperforming prior works limited by fixed UV maps or losses.
