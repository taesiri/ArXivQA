# [Generic 3D Diffusion Adapter Using Controlled Multi-View Editing](https://arxiv.org/abs/2403.12032)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Open-domain 3D object generation and editing lags behind 2D image synthesis due to limited 3D data and higher complexity. 
- Existing multi-view diffusion models often have weak 3D consistency or compromise on visual quality. Score distillation is time-consuming and leads to degraded distribution.

Proposed Solution: 
- Introduce MVEdit - a framework to adapt pretrained 2D image diffusion models into 3D-consistent multi-view diffusion pipelines using a novel training-free 3D Adapter module.
- The 3D Adapter fuses multi-view images into a 3D representation, then renders views to control subsequent denoising steps without quality loss.
- Additionally propose StableSSDNeRF - a fast text-to-3D diffusion model to complement MVEdit.

Main Contributions:
- MVEdit achieves sharp, diverse results with strict 3D consistency, outperforming prior arts in image-to-3D and text-guided texture generation.
- Versatile pipelines enable diverse 3D tasks like text/image-to-3D, 3D-to-3D editing, and texture super-resolution.
- StableSSDNeRF enables domain-specific text-to-3D initialization by fine-tuning 2D Stable Diffusion with 3D supervision.

In summary, this paper introduces a high-quality and efficient solution to adapt 2D diffusion models for consistent 3D generation/editing tasks. MVEdit outperforms prior arts while the complementary StableSSDNeRF model enables customizable text-to-3D initialization.
