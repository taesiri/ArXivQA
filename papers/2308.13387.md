# [Do-Not-Answer: A Dataset for Evaluating Safeguards in LLMs](https://arxiv.org/abs/2308.13387)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading, the central research question this paper aims to address is: 

How can we develop an open-source dataset and automatic evaluation methods to assess the safety capabilities and mechanisms of large language models (LLMs)?

In particular, the key goals of this work seem to be:

- To create a taxonomy of potential risks and harms that LLMs could pose, covering a comprehensive set of areas beyond just biases or toxicity. 

- To collect a dataset of risky prompts that responsible LLMs should not respond to or follow, covering different types of potential harms.

- To manually evaluate and analyze the responses of both commercial and open-source LLMs to these risky prompts.

- To develop automatic evaluation methods, including using GPT-4 and fine-tuned classifiers, to assess model safety capabilities. 

- To show that small classifiers can achieve effective and low-cost automatic safety evaluation that is comparable to results from large models like GPT-4.

So in summary, the core research question is how to enable low-cost, scalable, and effective automatic evaluation of LLM safety through an open dataset and classifier models. The novelty seems to be in covering a broader taxonomy of safety issues and showing that small classifiers can replicate large model performance for this task.


## What is the main contribution of this paper?

 The main contributions of this paper are:

1. They introduce a comprehensive 3-level taxonomy for assessing risks associated with large language models (LLMs), covering 5 distinct risk areas with 12 harm types and 60 distinct risks. This provides a structured way to think about the various harms LLMs can cause. 

2. They collect a dataset called Do-Not-Answer consisting of 939 risky prompts, along with over 5,000 responses from 6 different LLMs. The prompts are designed such that responsible LLMs should not provide a direct answer. This is the first open-source dataset for evaluating LLM safeguards.

3. They manually annotate the LLM responses according to harmfulness and categorize them into 6 refined response types. This reveals the capabilities of different LLMs in mitigating risks.

4. They propose automatic evaluation methods to assess LLM safety, including using GPT-4 for zero-shot evaluation and fine-tuning classifiers. Experiments show that a small classifier can achieve comparable results to GPT-4, enabling low-cost LLM safety assessment.

In summary, the key contribution is the introduction of a comprehensive risk taxonomy, a dataset to evaluate LLM safeguards, analysis of commercial and open-source LLM capabilities on this data, and methods for automatic safety evaluation. The dataset and methods enable the research community to better understand and improve the safety of LLMs.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 The TL;DR version of the paper is: It presents a dataset of risky prompts and model responses to evaluate the safety capabilities of large language models, proposes methods for automatic safety assessment, and shows that small classifiers can effectively evaluate model safety.


## How does this paper compare to other research in the same field?

 Here are a few thoughts on how this paper compares to other research in evaluating safeguards for large language models:

- In terms of developing a comprehensive risk taxonomy, this paper introduces a detailed 3-level hierarchy covering 60 distinct risks across 5 harm areas. This provides more fine-grained risk types than some prior work like the BeaverTails taxonomy with 14 harm types. However, others have also proposed detailed taxonomies, like DeepMind's 6 areas and 31 sub-categories.

- For data collection, the key contribution here is releasing the first open dataset of risky prompts and model responses. Many previous datasets for evaluating safety were proprietary. However, the scale is still limited at 939 prompts. Some related projects have used 10k+ prompts.

- The human annotation process seems fairly standard, with 3 annotators labeling responses and achieving consensus. Other papers have used similar protocols. The 6 refined response categories are also intuitive.

- The automatic evaluation methods leveraging GPT-4 reviews and classifier fine-tuning are common techniques in recent work. The key finding that a small 600M classifier approximates GPT-4 performance is noteworthy.

- Overall scope is focused on English, text-only, single-turn interactions. So it does not handle multimodality or conversational aspects of safety, which some other projects are starting to tackle.

In summary, the taxonomy, dataset, and annotation schema represent good incremental research contributions in this space. The techniques for automatic evaluation are generally sound but not too novel compared to recent literature. There are also opportunities to expand the scope in future work. But as a focused study on evaluating text-based safeguards, this presents solid foundational research.


## What future research directions do the authors suggest?

 Based on my reading of the paper, some of the main future research directions suggested by the authors include:

- Expanding the dataset with more questions, particularly non-risky instructions, to better identify over-sensitive language models. The current dataset only contains risky prompts, which makes models that refuse all instructions appear safer. Adding non-risky prompts would help address this limitation.

- Collecting multi-label annotations for the responses, as the authors note that multiple action categories can apply to a single response. This would allow for more nuanced evaluation. 

- Evaluating models in other languages, multi-turn conversations, and few-shot settings. The current work focused on English, single-turn, zero-shot evaluation. Adapting the methods to other settings would expand the scope.

- Incorporating evaluation of human preferences between responses, beyond just binary safe/unsafe labels. The authors suggest more detailed analysis of human judgments could help determine which responses are truly better.

- Developing additional automatic evaluation methods, such as formulating classification as a generation task using causal language models. Only classifier-based methods were explored in this work.

- Using the dataset to both measure model safety capabilities and improve them, via techniques like fine-tuning on the human labeled data. The authors focused only on evaluation, not safety improvement.

- Expanding the taxonomy to identify new potential risks based on emergent capabilities of future systems. The current taxonomy may not cover all possibilities going forward.

In summary, the main directions are around expanding the dataset itself, evaluating in broader contexts, developing new automatic evaluation techniques, and leveraging the data to actually improve model safety. The authors have laid a strong foundation that can be built upon in many fruitful ways.


## Summarize the paper in one paragraph.

 The paper introduces Do-Not-Answer, the first open-source dataset for evaluating safeguards in large language models (LLMs). The authors curate a dataset of 939 risky prompts that responsible LLMs should not answer, covering a comprehensive taxonomy of potential harms. They manually evaluate the responses of six popular LLMs, finding LLama-2 performs best at not following risky instructions while ChatGLM ranks last. The responses also exhibit risk type-specific patterns. Building on this, the authors propose automatic safety evaluation methods including prompting GPT-4 and training small BERT-like classifiers. Experiments show the classifiers can effectively mimic GPT-4 at low cost. The dataset and methods enable evaluating and improving LLM safeguards. Key contributions are the risk taxonomy, unsafe prompt dataset, manual and automatic LLM safety assessments, and demonstration that small models can effectively evaluate safety.


## Summarize the paper in two paragraphs.

 Here is a 2 paragraph summary of the paper:

The paper introduces Do-Not-Answer, the first open-source dataset for evaluating safeguards in large language models (LLMs). The authors develop a hierarchical taxonomy covering 60 types of risks across 5 areas: information hazards, malicious uses, discrimination/exclusion/toxicity, misinformation harms, and human-chatbot interaction harms. Based on this taxonomy, they collect a dataset of 939 risky prompts that responsible LLMs should not follow, along with over 5,000 responses from 6 LLMs: GPT-4, ChatGPT, Claude (commercial); LLaMA-2, ChatGLM, Vicuna (open-source). 

The responses are manually annotated for harmfulness and categorical response types. Results show LLaMA-2 is safest with only 3 harmful responses, while ChatGLM is least safe with 85. The authors also propose automatic evaluation methods, including prompting GPT-4 for reviews and training classifier models. Experiments demonstrate that fine-tuned BERT-like models under 600M parameters can achieve comparable performance to GPT-4, enabling low-cost assessment. Overall, this work introduces a comprehensive taxonomy and risk-detection dataset to spur research into evaluating and enhancing the safety capabilities of LLMs.


## Summarize the main method used in the paper in one paragraph.

 Here is a one paragraph summary of the main method used in the paper:

The paper introduces a new dataset called Do-Not-Answer to evaluate the safeguard mechanisms of text-only large language models (LLMs). The authors first develop a comprehensive three-level taxonomy of potential risks and harms associated with LLM capabilities. Based on this taxonomy, they collect a dataset of 939 risky prompts that responsible LLMs should refrain from answering directly. They gather over 5,000 responses to these prompts from 6 LLMs - 3 commercial (GPT-4, ChatGPT, Claude) and 3 open source (LLaMA-2, Vicuna, ChatGLM). The authors manually annotate all the responses in terms of harmfulness and categorize them into 6 refined response types. Using this labeled dataset, they evaluate and analyze the safety capabilities of the 6 LLMs. Finally, they propose automatic methods like prompting GPT-4 and training classifier models to evaluate LLM safety. Experiments show that fine-tuned BERT classifiers with under 600M parameters can effectively perform safety evaluation, achieving comparable results to GPT-4. Overall, the paper's main contribution is the introduction of the Do-Not-Answer dataset and using it to evaluate and develop automatic methods for assessing LLM safety.


## What problem or question is the paper addressing?

 Based on my review, the key problem this paper is addressing is the lack of open datasets to evaluate and improve the safety mechanisms and safeguards of large language models (LLMs), especially open-source LLMs.

Some of the main issues and questions the paper seems to be tackling are:

- Emerging LLMs have shown great capabilities but also exhibit unpredictable and potentially dangerous behaviors. There is a need for better ways to identify and mitigate risks before deployment.

- Commercial LLM creators have built proprietary datasets to evaluate dangers, but open-source models lack comprehensive safety mechanisms and public datasets. 

- How can we construct a taxonomy of risks for text-only LLMs and collect a diverse dataset of risky prompts that responsible models should avoid answering?

- How can we manually annotate commercial and open-source LLM responses to these prompts to assess current safety capabilities and patterns? 

- How can we develop automatic evaluation methods using this dataset, such as fine-tuned classifiers, to allow scalable assessment of model safety?

- Can small classifiers match the performance of large models like GPT-4 for safety evaluation to enable low-cost and privacy-preserving assessment?

In summary, the key gap seems to be the lack of open datasets to evaluate safety of LLMs, especially open-source models. This paper aims to provide such a dataset and evaluation methods to identify risks and enhance safeguards.


## What are the keywords or key terms associated with this paper?

 Based on a review of the paper, some of the key terms and concepts include:

- Large language models (LLMs): The paper focuses on evaluating and improving the safety mechanisms of large language models such as GPT-3, GPT-4, LLama, etc.

- Safety mechanisms: The paper introduces a dataset and methods to assess the capabilities of LLMs to handle risky or harmful prompts. This involves evaluating the safety mechanisms built into commercial and open-source LLMs.

- Risk taxonomy: A 3-level hierarchical taxonomy of risks associated with LLMs is proposed, covering areas like information hazards, malicious uses, discrimination, misinformation, and human-AI interaction harms. 

- Risk detection dataset: A dataset of 939 risky prompts is collected to evaluate whether LLMs can detect and mitigate risks. All prompts are designed so responsible LLMs should not provide answers.

- Annotation and assessment: Human annotations of commercial and open-source LLM responses on risk/safety are provided. LLama-2 has the best safety performance. 

- Automatic safety evaluation: Methods like prompting GPT-4 for assessment and training classifiers on human labels are explored to enable low-cost and scalable LLM safety evaluation.

- Model capabilities and limitations: The analysis provides insights into current capabilities and limitations of commercial vs open source LLMs in mitigating different types of risks/harms.

In summary, the key terms cover risk taxonomy, safety mechanisms, safety evaluation benchmarks and techniques for commercial and open source LLMs. The paper aims to promote safer development of conversational AI.
