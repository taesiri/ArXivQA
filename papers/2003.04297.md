# [Improved Baselines with Momentum Contrastive Learning](https://arxiv.org/abs/2003.04297)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central research question this paper addresses is whether design improvements used in SimCLR (MLP projection head and stronger data augmentation) can be effectively incorporated into the MoCo framework to establish stronger baselines for unsupervised representation learning. Specifically, the authors verify that using an MLP projection head and more aggressive data augmentation with MoCo leads to better image classification and object detection transfer learning results compared to the original MoCo and SimCLR methods. A key finding is that these modifications allow MoCo to achieve state-of-the-art unsupervised representation learning without requiring large batch sizes like SimCLR.In summary, the main hypothesis is that orthogonal improvements from SimCLR can be integrated with MoCo to create more effective and feasible unsupervised learning baselines. The experiments aim to test this hypothesis and demonstrate the strengths of an improved MoCo approach ("MoCo v2") compared to prior arts.
