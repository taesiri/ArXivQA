# [Unleashing Vanilla Vision Transformer with Masked Image Modeling for   Object Detection](https://arxiv.org/abs/2204.02964)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading, the central hypothesis of this paper is that a masked image modeling (MIM) pre-trained vanilla Vision Transformer (ViT) can be effectively adapted for high-performance object detection by:

1. Feeding the ViT encoder with only randomly sampled partial input embeddings during fine-tuning. This allows processing high-resolution inputs required for object detection with feasible computational costs. 

2. Replacing the pre-trained large kernel patchify stem with a compact randomly initialized convolutional stem. This produces a hierarchical feature pyramid from the single-scale ViT to handle objects at different scales.

The key ideas are to treat the ViT input as 1D token sequences rather than 2D grids, and leverage the ability of MIM pre-trained ViT to reconstruct full representations from partial observations. This helps unlock the potential of vanilla ViT for challenging object-level recognition tasks.

In summary, the central hypothesis is that with the right adaptations, a MIM pre-trained vanilla ViT can achieve strong performance for object detection while overcoming its limitations like quadratic scaling of self-attention and lack of feature hierarchy. The proposed MIMDet method aims to test this hypothesis.


## What is the main contribution of this paper?

 This paper presents an approach to adapt a masked image modeling (MIM) pre-trained vanilla Vision Transformer (ViT) for object detection. The main contributions are:

1. They propose to only feed the ViT encoder with a randomly sampled subset (e.g. 25-50%) of the input patch embeddings during fine-tuning, rather than the full set of embeddings. This reduces the computational cost while still achieving good performance. 

2. They replace the pre-trained patchify stem with a compact randomly initialized convolutional stem. This helps introduce multi-scale features to the single-scale ViT architecture. 

3. The resulting detector, called MIMDet, combines a convolutional stem, sampled ViT encoder, and lightweight ViT decoder. It achieves state-of-the-art results on COCO object detection compared to previous methods of adapting vanilla ViT.

4. Experiments show MIMDet enables strong performance from a MIM pre-trained vanilla ViT, while being efficient and introducing a smaller gap between pre-training and fine-tuning compared to approaches like window attention.

In summary, the main contribution is an effective and efficient way to unlock the potential of MIM pre-trained vanilla ViT for high-performance object detection, without too much task-specific architectural modification. The results suggest the strong ViT representations can be unleashed with the right adaptations.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the paper:

The paper proposes an approach to efficiently adapt a masked image modeling (MIM) pre-trained vanilla Vision Transformer (ViT) for object detection by feeding the ViT encoder with only randomly sampled partial input embeddings and using a compact convolutional stem, enabling the ViT to achieve strong performance on COCO while being efficient.
