# [Unleashing Vanilla Vision Transformer with Masked Image Modeling for   Object Detection](https://arxiv.org/abs/2204.02964)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading, the central hypothesis of this paper is that a masked image modeling (MIM) pre-trained vanilla Vision Transformer (ViT) can be effectively adapted for high-performance object detection by:

1. Feeding the ViT encoder with only randomly sampled partial input embeddings during fine-tuning. This allows processing high-resolution inputs required for object detection with feasible computational costs. 

2. Replacing the pre-trained large kernel patchify stem with a compact randomly initialized convolutional stem. This produces a hierarchical feature pyramid from the single-scale ViT to handle objects at different scales.

The key ideas are to treat the ViT input as 1D token sequences rather than 2D grids, and leverage the ability of MIM pre-trained ViT to reconstruct full representations from partial observations. This helps unlock the potential of vanilla ViT for challenging object-level recognition tasks.

In summary, the central hypothesis is that with the right adaptations, a MIM pre-trained vanilla ViT can achieve strong performance for object detection while overcoming its limitations like quadratic scaling of self-attention and lack of feature hierarchy. The proposed MIMDet method aims to test this hypothesis.
