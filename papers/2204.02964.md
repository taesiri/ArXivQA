# [Unleashing Vanilla Vision Transformer with Masked Image Modeling for   Object Detection](https://arxiv.org/abs/2204.02964)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading, the central hypothesis of this paper is that a masked image modeling (MIM) pre-trained vanilla Vision Transformer (ViT) can be effectively adapted for high-performance object detection by:

1. Feeding the ViT encoder with only randomly sampled partial input embeddings during fine-tuning. This allows processing high-resolution inputs required for object detection with feasible computational costs. 

2. Replacing the pre-trained large kernel patchify stem with a compact randomly initialized convolutional stem. This produces a hierarchical feature pyramid from the single-scale ViT to handle objects at different scales.

The key ideas are to treat the ViT input as 1D token sequences rather than 2D grids, and leverage the ability of MIM pre-trained ViT to reconstruct full representations from partial observations. This helps unlock the potential of vanilla ViT for challenging object-level recognition tasks.

In summary, the central hypothesis is that with the right adaptations, a MIM pre-trained vanilla ViT can achieve strong performance for object detection while overcoming its limitations like quadratic scaling of self-attention and lack of feature hierarchy. The proposed MIMDet method aims to test this hypothesis.


## What is the main contribution of this paper?

 This paper presents an approach to adapt a masked image modeling (MIM) pre-trained vanilla Vision Transformer (ViT) for object detection. The main contributions are:

1. They propose to only feed the ViT encoder with a randomly sampled subset (e.g. 25-50%) of the input patch embeddings during fine-tuning, rather than the full set of embeddings. This reduces the computational cost while still achieving good performance. 

2. They replace the pre-trained patchify stem with a compact randomly initialized convolutional stem. This helps introduce multi-scale features to the single-scale ViT architecture. 

3. The resulting detector, called MIMDet, combines a convolutional stem, sampled ViT encoder, and lightweight ViT decoder. It achieves state-of-the-art results on COCO object detection compared to previous methods of adapting vanilla ViT.

4. Experiments show MIMDet enables strong performance from a MIM pre-trained vanilla ViT, while being efficient and introducing a smaller gap between pre-training and fine-tuning compared to approaches like window attention.

In summary, the main contribution is an effective and efficient way to unlock the potential of MIM pre-trained vanilla ViT for high-performance object detection, without too much task-specific architectural modification. The results suggest the strong ViT representations can be unleashed with the right adaptations.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the paper:

The paper proposes an approach to efficiently adapt a masked image modeling (MIM) pre-trained vanilla Vision Transformer (ViT) for object detection by feeding the ViT encoder with only randomly sampled partial input embeddings and using a compact convolutional stem, enabling the ViT to achieve strong performance on COCO while being efficient.


## How does this paper compare to other research in the same field?

 Here are a few thoughts on how this paper compares to other research in adapting vision transformers (ViTs) for object detection:

- This paper focuses specifically on adapting Masked Image Modeling (MIM) pre-trained vanilla ViTs, like MAE, for object detection. Many other works have focused on supervised pre-training or on adapting hierarchical ViTs like Swin Transformers. Adapting MIM-pre-trained vanilla ViTs is still relatively underexplored.

- The approach feeds only partial, randomly sampled input embeddings to the ViT encoder during fine-tuning. This is quite different from previous works that typically feed the full input or use window partitioning. The motivation is to introduce less discrepancy from pre-training and leverage the pre-trained representations more judiciously.

- It replaces the pre-trained patchify stem with a small randomly initialized convolutional stem for hierarchical features. Most works retain the full pre-trained backbone. Replacing parts of the backbone is an interesting idea to build custom feature hierarchies.

- Without modifications like window attention, this achieves strong results competitive or superior to hierarchical ViTs. It also shows better efficiency than a prior work adapting vanilla ViTs. This helps demonstrate the power of MIM-pre-trained vanilla ViTs.

- There is limited exploration of different MIM frameworks beyond MAE. Expanding beyond MAE could be interesting future work.

Overall, this paper introduces some novel ideas for unlocking the potential of MIM-pre-trained vanilla ViTs for object detection. The competitive results highlight the promise of this direction. It provides a uniquely sparse, sampling-based approach compared to prior works.


## What future research directions do the authors suggest?

 Based on my reading of the paper, here are some of the future research directions suggested by the authors:

- Scaling up the MIMDet model to larger sizes. The authors observe a promising scaling trend between the ViT model size and detection accuracy, so they suggest continuously scaling up MIMDet as future work.

- Exploring the properties of the MAE decoder during pre-training. The authors point out that what the MAE decoder learns is still unclear, so understanding its properties could be an interesting research direction. 

- Adapting other MIM frameworks besides MAE. The authors mainly studied MAE for pre-training, but suggest exploring the integration of other advances in masked image modeling into the MIMDet framework.

- Pre-training typical ConvNets like ResNet with MIM. The authors propose it may be possible to pre-train ConvNets with MIM by using a modified patchify stem, which could be explored.

- Applying MIMDet to other visual recognition tasks beyond object detection, such as instance segmentation, panoptic segmentation, etc. The general methodology of MIMDet could potentially benefit these other tasks.

- Leveraging powerful general visual representations from vanilla ViT for other computer vision tasks. The authors suggest this work could encourage exploring the usage of pre-trained vanilla ViT representations more broadly.

In summary, the main future directions are centered around scaling up MIMDet, studying the pre-trained MAE model properties, extending MIMDet to other MIM frameworks and tasks, and unleashing vanilla ViT representations for more vision applications.
