# FAST: Improving Controllability for Text Generation with Feedback Aware   Self-Training

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the key research question this paper aims to address is: How can we improve the controllability and language quality of conditional text generation models by reducing spurious correlations between context and target attributes in the training data?The key hypothesis is that current conditional text generation models, which use control codes to direct attributes like length or style, often rely incorrectly on parts of the input other than the control code to determine attributes. This is due to spurious correlations in the training data between contexts and attributes. The paper proposes and evaluates two data augmentation techniques - inverse propensity score (IPS) resampling and feedback aware self-training (FAST) - to reduce these spurious correlations and improve controllability and language quality.In summary, the central focus is on improving controllable text generation by identifying and mitigating the issue of spurious correlations using novel data augmentation techniques. The key research question is whether reducing spurious correlations through IPS and FAST can enhance controllability and language quality compared to current state-of-the-art methods.


## What is the main contribution of this paper?

The main contribution of this paper is proposing two data augmentation techniques to improve the controllability of conditional text generation models. The key ideas are:- Identifying a flaw in existing control code-based text generation systems where spurious correlations in the training data cause models to rely on input context rather than just the control code when selecting attributes. This undermines controllability.- Proposing two simple data augmentation techniques to break these spurious correlations:1) Inverse propensity score (IPS) resampling, which upweights rare context-attribute combinations. 2) Feedback aware self-training (FAST), which uses a preliminary model to generate counterfactual examples with all possible attributes for each context, then filters noisy examples and retrains on the counterfactually augmented dataset.- Demonstrating through experiments on 3 datasets (news headlines, meta-reviews, search ads) that the proposed techniques, especially FAST, can significantly improve controllability and language quality over strong baselines.In summary, the key contribution is identifying an important flaw in existing approaches, proposing simple solutions, and showing their effectiveness on multiple controllable generation tasks. The proposed techniques help properly isolate the relationship between control codes and attributes in the training data.
