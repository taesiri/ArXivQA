# [FAST: Improving Controllability for Text Generation with Feedback Aware   Self-Training](https://arxiv.org/abs/2210.03167)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading, the key research question this paper aims to address is: 

How can we improve the controllability and language quality of conditional text generation models by reducing spurious correlations between context and target attributes in the training data?

The key hypothesis is that current conditional text generation models, which use control codes to direct attributes like length or style, often rely incorrectly on parts of the input other than the control code to determine attributes. This is due to spurious correlations in the training data between contexts and attributes. The paper proposes and evaluates two data augmentation techniques - inverse propensity score (IPS) resampling and feedback aware self-training (FAST) - to reduce these spurious correlations and improve controllability and language quality.

In summary, the central focus is on improving controllable text generation by identifying and mitigating the issue of spurious correlations using novel data augmentation techniques. The key research question is whether reducing spurious correlations through IPS and FAST can enhance controllability and language quality compared to current state-of-the-art methods.


## What is the main contribution of this paper?

 The main contribution of this paper is proposing two data augmentation techniques to improve the controllability of conditional text generation models. The key ideas are:

- Identifying a flaw in existing control code-based text generation systems where spurious correlations in the training data cause models to rely on input context rather than just the control code when selecting attributes. This undermines controllability.

- Proposing two simple data augmentation techniques to break these spurious correlations:

1) Inverse propensity score (IPS) resampling, which upweights rare context-attribute combinations. 

2) Feedback aware self-training (FAST), which uses a preliminary model to generate counterfactual examples with all possible attributes for each context, then filters noisy examples and retrains on the counterfactually augmented dataset.

- Demonstrating through experiments on 3 datasets (news headlines, meta-reviews, search ads) that the proposed techniques, especially FAST, can significantly improve controllability and language quality over strong baselines.

In summary, the key contribution is identifying an important flaw in existing approaches, proposing simple solutions, and showing their effectiveness on multiple controllable generation tasks. The proposed techniques help properly isolate the relationship between control codes and attributes in the training data.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the key points from the paper:

The paper proposes two simple yet effective data augmentation techniques to improve the controllability and language quality of neural text generation models that use control codes, by reducing spurious correlations between the input contexts and target attributes in the training data.


## How does this paper compare to other research in the same field?

 This paper makes several contributions to the field of controllable text generation:

- It identifies an important flaw with current control code-based methods - that models can rely on spurious correlations in the training data rather than just the control codes. The paper demonstrates this issue convincingly through empirical analysis and case studies. 

- It proposes two simple yet effective data augmentation techniques - inverse propensity score resampling (IPS) and feedback aware self-training (FAST) - to reduce spurious correlations and improve controllability.

- It conducts extensive experiments on 3 diverse tasks - generating news headlines, meta-reviews, and search ads. Results demonstrate that FAST consistently improves controllability and language quality over strong baselines like BART+CTRL, PPLM, and GeDi.

Some key differences compared to prior work:

- While causal inference techniques like IPS have been applied elsewhere in NLP, this paper is the first to use it to improve controllability in text generation. 

- FAST is a form of counterfactual data augmentation, but it uses model feedback to remove noisy examples. This is a simple yet impactful modification over prior counterfactual augmentation techniques.

- The paper convincingly demonstrates the problem of spurious correlations on both abstractive and more flexible semantic-level controllable generation tasks. Prior work focused more on style transfer type tasks.

- The improvements from the proposed techniques are demonstrated on large-scale datasets and models, rather than small prototypes.

Overall, this paper makes both conceptual and empirical contributions over related work. By formally characterizing and addressing the spurious correlation issue, it significantly advances the state-of-the-art in controllable text generation. The proposed techniques appear simple, scalable, and widely applicable.


## What future research directions do the authors suggest?

 The authors suggest a few potential future research directions in the conclusion:

- Investigate more checks during the feedback step of the FAST algorithm, e.g. filtering out unfaithful examples, to further improve the method. 

- Explore applying the IPS resampling approach in the emerging parameter efficient fine-tuning paradigm like P*-tuning, where models are less prone to memorizing duplicate examples.

- Combine the proposed methods with other controllable generation techniques like GeDi and PPLM to see if they can provide complementary benefits.

- Extend the FAST algorithm to more challenging scenarios where the training data and pre-training data are very different, which may require better techniques for generating high quality counterfactual data.

- Develop methods to identify when certain linguistic attributes may not be applicable or appropriate to generate for a given context, to avoid generating untruthful text.

- Explore other ways to generate high quality counterfactual data beyond the simple preliminary model approach used in FAST, for example using more sophisticated models or perturbation techniques.

Overall, the main future directions are improving the counterfactual data generation in FAST, combining it with other methods, and extending it to more complex scenarios like highly domain-specific data. The key challenges are avoiding propagating errors during counterfactual augmentation and ensuring the faithfulness of the augmented examples.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the paper:

The paper reveals a flaw in current controllable text generation systems that use control codes to direct properties like length or sentiment of the generated text. Specifically, it shows that spurious correlations in the training data between the input contexts and target attributes can cause models to rely on parts of the input other than the control code when generating text, undermining the controllability of these systems. The authors demonstrate this issue through experiments on news headline, online review, and ad generation tasks. They propose two data augmentation techniques to address the problem - inverse propensity score resampling to increase the presence of rare context-attribute combinations, and a feedback-aware self-training algorithm that generates counterfactual examples then removes noisy ones. Experiments show these methods can significantly improve controllability and language quality compared to standard controllable generation techniques. The key contribution is identifying the spurious correlation issue and providing simple yet effective solutions validated over multiple text generation tasks.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the paper:

The paper focuses on improving the controllability of text generation models by addressing spurious correlations in training data. Current controllable generation systems use control codes to guide the model to generate text with desired attributes like length, sentiment, etc. However, the paper reveals that correlations often exist between the input text and target attributes, causing models to rely on parts of the input besides the control code for selecting attributes. This undermines the consistency and efficacy of the control codes. 

To address this issue, the authors propose two data augmentation techniques - inverse propensity score (IPS) resampling and feedback aware self-training (FAST). IPS resamples the data to even out correlations between contexts and attributes. FAST uses a preliminary model to generate counterfactual examples, then filters noisy ones with a classifier before retraining the model. Experiments on headline, meta-review, and ad generation tasks show FAST significantly improves controllability and language quality over state-of-the-art baselines. The techniques help isolate statistical relationships between control codes and attributes.


## Summarize the main method used in the paper in one paragraph.

 The paper proposes two data augmentation methods, inverse propensity scoring (IPS) resampling and feedback aware self-training (FAST), to deal with spurious correlations in controllable text generation systems. 

The key idea is that spurious correlations in the training data between the context (e.g. article text) and the target attribute to control (e.g. headline length) can cause models to rely on the context rather than just the control code when generating text. This hurts the model's ability to generate counterfactually, i.e. generate outputs expressing an attribute different from what the context would suggest.

To deal with this, IPS resamples the training data to boost the presence of rare context-attribute combinations. FAST first trains a baseline model, uses it to generate counterfactual data for each example, then retrains on the counterfactually augmented dataset after filtering noisy samples using feedback from an attribute classifier. Both methods aim to break the unwanted correlations between context and attributes, isolating the relationship between control codes and attributes.

Experiments on 3 tasks - generating headlines, meta-review sentences, and ad copy - show FAST significantly improves controllability and language quality over baselines. IPS also improves controllability but hurts language quality. Overall, the proposed methods effectively deal with spurious correlations in controllable generation.
