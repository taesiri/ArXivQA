# [SparQ Attention: Bandwidth-Efficient LLM Inference](https://arxiv.org/abs/2312.04985)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a summary paragraph for the paper:

The paper proposes SparQ Attention, a novel technique for reducing the memory bandwidth requirements of transformer models during inference. SparQ approximates the attention scores using a subset of query/key vector components, which allows fetching only the most relevant tokens for each inference step. This greatly decreases memory traffic without affecting task accuracy. SparQ Attention consists of (1) approximating attention scores using the top query components (2) gathering keys/values of top-scoring tokens based on approximation (3) interpolating with a mean value vector to compensate for missing vectors. Experiments across language models and tasks like question answering, summarization, and language modeling demonstrate SparQ achieves up to 8x compression of attention memory with little to no loss of performance. The method can be readily applied to pretrained models without modification or fine-tuning. Overall, SparQ Attention enables more efficient deployment of large language models by alleviating the memory bandwidth bottleneck of transformer inference.
