# [Supervised Masked Knowledge Distillation for Few-Shot Transformers](https://arxiv.org/abs/2303.15466)

## What is the central research question or hypothesis that this paper addresses?

 The central research question this paper addresses is how to efficiently combine label information with self-supervised learning for few-shot Transformers. 

The key points are:

- Vision Transformers (ViTs) tend to overfit on small datasets due to lack of inductive bias. Previous works tackle this issue using either weaker supervision or self-supervision as regularization. But how to effectively incorporate label information into self-supervised frameworks remains a challenge.

- The paper proposes a Supervised Masked Knowledge Distillation (SMKD) framework that extends recent self-supervised masked knowledge distillation into a supervised variant. It designs supervised-contrastive losses at both global class token level and local patch token level for intra-class images.

- At the patch level, it introduces the challenging task of masked patch token reconstruction across images of the same class, which encourages learning of semantic correspondence.

- Experiments on four few-shot benchmarks demonstrate SMKD outperforms previous methods by a large margin given its simple design and training procedure.

In summary, the central hypothesis is that incorporating label information into self-supervised masked knowledge distillation can efficiently mitigate the overfitting issue of Transformers for few-shot learning. The proposed SMKD framework validates this hypothesis with strong empirical results.


## What is the main contribution of this paper?

 The main contributions of this paper are:

- Proposes a new supervised knowledge distillation framework (SMKD) that incorporates class label information into self-distillation, filling the gap between self-supervised knowledge distillation and traditional supervised learning.

- Designs two supervised-contrastive losses on both class and patch levels within the proposed framework. The class-level loss distills knowledge from global image features (\texttt{[cls]} tokens), while the patch-level loss introduces a novel task of masked patch tokens reconstruction across intra-class images.

- Achieves state-of-the-art results on CIFAR-FS and FC100 datasets and competitive performance on mini-ImageNet and tiered-ImageNet using simple prototype classification, demonstrating the effectiveness of the proposed method.

- The framework enjoys several advantages: it does not introduce extra parameters, is efficient to train, and can be easily combined with other methods.

In summary, the key contribution is a simple yet effective supervised knowledge distillation framework that incorporates label information into self-distillation through novel class-level and patch-level contrastive losses. This bridges the gap between self-supervised and supervised learning for few-shot transformers.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence TL;DR summary of the paper:

This paper proposes a novel supervised masked knowledge distillation framework for few-shot transformers that incorporates label information into self-distillation of class and patch tokens across views of intra-class images, outperforming previous self-supervised and supervised methods on few-shot image classification benchmarks.


## How does this paper compare to other research in the same field?

 Here are a few key points comparing this paper to other research in few-shot learning with vision transformers:

- This paper proposes a new framework called Supervised Masked Knowledge Distillation (SMKD) that incorporates class label information into self-distillation for few-shot learning. This helps bridge the gap between self-supervised and supervised knowledge distillation. Many prior works have explored self-supervised pretraining or auxiliary losses, but combining supervision directly into self-distillation is novel.

- The method distills knowledge at both the global class token level and local patch token level. Enforcing alignment between patch tokens of different images from the same class using masked reconstruction is a unique aspect not explored by other methods. 

- Compared to recent supervised methods like HCTransformers, this method is simpler with no extra modules or architectural changes needed. It also does not rely on large batches or negative sampling like some contrastive methods.

- The performance is very strong, achieving state-of-the-art on CIFAR-FS and FC100 datasets using a simple ViT-Small backbone. On miniImageNet and tieredImageNet, it remains competitive with very recent methods using more complex training schemes.

- A nice property is that SMKD is lightweight and complementary to many other recent methods involving architectural modifications, custom evaluation schemes, etc. So it has potential to combine well with those orthogonal improvements.

Overall, the proposed SMKD framework innovates on bringing supervision into self-distillation in a simple yet effective manner tailored for few-shot learning. The results demonstrate the strength of this approach on multiple benchmark datasets compared to existing state-of-the-art methods.


## What future research directions do the authors suggest?

 Based on my reading of the paper, here are some of the main future research directions suggested by the authors:

- Unifying the learning objectives of self-supervised learning and supervised contrastive learning, using a carefully designed curriculum learning strategy. The authors' two-stage training process of self-supervised pre-training followed by supervised training can be seen as a special case of curriculum learning. They suggest exploring more principled ways to transition from "easy" self-supervised objectives to "harder" supervised contrastive learning in a curriculum framework.

- Further bridging the gap between self-supervised and supervised knowledge distillation frameworks. The authors propose a way to incorporate label information into self-distillation, but more work can be done to integrate these approaches. 

- Exploring the effectiveness of the proposed methods on larger and more complex datasets. The experiments in this paper are on small image classification datasets. Testing the approach on larger benchmarks like ImageNet would be interesting future work.

- Combining the proposed supervised knowledge distillation framework with other contemporary methods like HCTransformers. The authors show a simple combination with HCT that achieves state-of-the-art results. More sophisticated integration could further improve performance.

- Studying the applicability of the approach to other domains like natural language processing, speech, etc. The masked knowledge distillation ideas may transfer well to domains beyond computer vision.

- Investigating how the idea of reconstructing masked patch tokens across images extends to semi-supervised or unsupervised settings. The cross-image prediction task seems like a challenging learning objective even without labels.

In summary, the authors point to several interesting directions including curriculum learning, integration with other methods, applications to other domains/tasks, and extending to semi-supervised or unsupervised settings as promising future work building on their approach.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the key points from the paper:

This CVPR 2023 paper proposes a novel supervised masked knowledge distillation framework (SMKD) for training vision transformers in few-shot learning settings. The key idea is to extend recent self-supervised masked knowledge distillation techniques into a supervised learning setting by incorporating label information. Specifically, the proposed SMKD framework distills knowledge between two augmented views of intra-class image pairs at both the global class token level and local patch token level. For class tokens, a supervised contrastive loss maximizes agreement between views. For patch tokens, cross-attention is used to establish correspondence between views and reconstruct masked patches across views, which provides an additional training signal. Experiments on four few-shot classification benchmarks show SMKD outperforms previous self-supervised and supervised methods for few-shot transformers, especially on smaller datasets, demonstrating the effectiveness of incorporating supervision into masked self-distillation frameworks. The simple design is competitive with more complex state-of-the-art techniques.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the key points from the paper:

This paper proposes a new supervised masked knowledge distillation framework (SMKD) for training few-shot Transformers. Previous works have shown that standard Transformers tend to overfit on small datasets in few-shot learning settings due to the lack of inductive bias compared to CNNs. Recent methods attempt to mitigate this issue through self-supervised techniques like masked image modeling as auxiliary losses during training. However, it remains challenging to effectively incorporate label information into these self-supervised frameworks. 

The proposed SMKD provides a natural extension of recent masked self-distillation methods into a supervised learning setting. It introduces two supervised contrastive losses at the class token and patch token levels to exploit label information. The class-level loss maximizes similarity between global class tokens from different views of intra-class images. The patch-level loss is more challenging since there is no ground truth correspondence. It addresses this by using cross-attention to estimate similarity between patches and enforcing alignment between matched patches from different views. Experiments on four few-shot classification benchmarks demonstrate that SMKD outperforms previous methods by a large margin. The introduction of the novel masked patch tokens reconstruction task makes the model generalize better by jointly exploiting holistic image knowledge and intra-class similarity.


## Summarize the main method used in the paper in one paragraph.

 Here is a one paragraph summary of the main method used in the paper:

The paper proposes a new supervised masked knowledge distillation framework (SMKD) for training vision transformers (ViTs) in few-shot learning settings. The key idea is to extend recent self-supervised masked knowledge distillation techniques into a supervised learning framework by incorporating class label information. Specifically, two supervised-contrastive losses are designed on the global class token (cls) and local patch tokens to enforce similarity between embeddings from different augmented views of intra-class images. The cls loss performs knowledge distillation on cls tokens similar to self-supervised methods. The novel patch loss finds dense correspondences between patches of intra-class images using cross-attention similarity, and reconstructs embeddings of masked patches across images, which is more challenging and encourages learning of generalizable features. This framework combines the benefits of supervised contrastive learning and self-supervised masked knowledge distillation, and outperforms previous methods on few-shot classification benchmarks. The two-stage training procedure of self-supervised pre-training followed by supervised distillation is shown to be crucial for good performance.
