# [Supervised Masked Knowledge Distillation for Few-Shot Transformers](https://arxiv.org/abs/2303.15466)

## What is the central research question or hypothesis that this paper addresses?

 The central research question this paper addresses is how to efficiently combine label information with self-supervised learning for few-shot Transformers. 

The key points are:

- Vision Transformers (ViTs) tend to overfit on small datasets due to lack of inductive bias. Previous works tackle this issue using either weaker supervision or self-supervision as regularization. But how to effectively incorporate label information into self-supervised frameworks remains a challenge.

- The paper proposes a Supervised Masked Knowledge Distillation (SMKD) framework that extends recent self-supervised masked knowledge distillation into a supervised variant. It designs supervised-contrastive losses at both global class token level and local patch token level for intra-class images.

- At the patch level, it introduces the challenging task of masked patch token reconstruction across images of the same class, which encourages learning of semantic correspondence.

- Experiments on four few-shot benchmarks demonstrate SMKD outperforms previous methods by a large margin given its simple design and training procedure.

In summary, the central hypothesis is that incorporating label information into self-supervised masked knowledge distillation can efficiently mitigate the overfitting issue of Transformers for few-shot learning. The proposed SMKD framework validates this hypothesis with strong empirical results.


## What is the main contribution of this paper?

 The main contributions of this paper are:

- Proposes a new supervised knowledge distillation framework (SMKD) that incorporates class label information into self-distillation, filling the gap between self-supervised knowledge distillation and traditional supervised learning.

- Designs two supervised-contrastive losses on both class and patch levels within the proposed framework. The class-level loss distills knowledge from global image features (\texttt{[cls]} tokens), while the patch-level loss introduces a novel task of masked patch tokens reconstruction across intra-class images.

- Achieves state-of-the-art results on CIFAR-FS and FC100 datasets and competitive performance on mini-ImageNet and tiered-ImageNet using simple prototype classification, demonstrating the effectiveness of the proposed method.

- The framework enjoys several advantages: it does not introduce extra parameters, is efficient to train, and can be easily combined with other methods.

In summary, the key contribution is a simple yet effective supervised knowledge distillation framework that incorporates label information into self-distillation through novel class-level and patch-level contrastive losses. This bridges the gap between self-supervised and supervised learning for few-shot transformers.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence TL;DR summary of the paper:

This paper proposes a novel supervised masked knowledge distillation framework for few-shot transformers that incorporates label information into self-distillation of class and patch tokens across views of intra-class images, outperforming previous self-supervised and supervised methods on few-shot image classification benchmarks.


## How does this paper compare to other research in the same field?

 Here are a few key points comparing this paper to other research in few-shot learning with vision transformers:

- This paper proposes a new framework called Supervised Masked Knowledge Distillation (SMKD) that incorporates class label information into self-distillation for few-shot learning. This helps bridge the gap between self-supervised and supervised knowledge distillation. Many prior works have explored self-supervised pretraining or auxiliary losses, but combining supervision directly into self-distillation is novel.

- The method distills knowledge at both the global class token level and local patch token level. Enforcing alignment between patch tokens of different images from the same class using masked reconstruction is a unique aspect not explored by other methods. 

- Compared to recent supervised methods like HCTransformers, this method is simpler with no extra modules or architectural changes needed. It also does not rely on large batches or negative sampling like some contrastive methods.

- The performance is very strong, achieving state-of-the-art on CIFAR-FS and FC100 datasets using a simple ViT-Small backbone. On miniImageNet and tieredImageNet, it remains competitive with very recent methods using more complex training schemes.

- A nice property is that SMKD is lightweight and complementary to many other recent methods involving architectural modifications, custom evaluation schemes, etc. So it has potential to combine well with those orthogonal improvements.

Overall, the proposed SMKD framework innovates on bringing supervision into self-distillation in a simple yet effective manner tailored for few-shot learning. The results demonstrate the strength of this approach on multiple benchmark datasets compared to existing state-of-the-art methods.
