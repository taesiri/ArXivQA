# [Supervised Masked Knowledge Distillation for Few-Shot Transformers](https://arxiv.org/abs/2303.15466)

## What is the central research question or hypothesis that this paper addresses?

 The central research question this paper addresses is how to efficiently combine label information with self-supervised learning for few-shot Transformers. 

The key points are:

- Vision Transformers (ViTs) tend to overfit on small datasets due to lack of inductive bias. Previous works tackle this issue using either weaker supervision or self-supervision as regularization. But how to effectively incorporate label information into self-supervised frameworks remains a challenge.

- The paper proposes a Supervised Masked Knowledge Distillation (SMKD) framework that extends recent self-supervised masked knowledge distillation into a supervised variant. It designs supervised-contrastive losses at both global class token level and local patch token level for intra-class images.

- At the patch level, it introduces the challenging task of masked patch token reconstruction across images of the same class, which encourages learning of semantic correspondence.

- Experiments on four few-shot benchmarks demonstrate SMKD outperforms previous methods by a large margin given its simple design and training procedure.

In summary, the central hypothesis is that incorporating label information into self-supervised masked knowledge distillation can efficiently mitigate the overfitting issue of Transformers for few-shot learning. The proposed SMKD framework validates this hypothesis with strong empirical results.
