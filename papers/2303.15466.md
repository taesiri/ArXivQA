# [Supervised Masked Knowledge Distillation for Few-Shot Transformers](https://arxiv.org/abs/2303.15466)

## What is the central research question or hypothesis that this paper addresses?

 The central research question this paper addresses is how to efficiently combine label information with self-supervised learning for few-shot Transformers. 

The key points are:

- Vision Transformers (ViTs) tend to overfit on small datasets due to lack of inductive bias. Previous works tackle this issue using either weaker supervision or self-supervision as regularization. But how to effectively incorporate label information into self-supervised frameworks remains a challenge.

- The paper proposes a Supervised Masked Knowledge Distillation (SMKD) framework that extends recent self-supervised masked knowledge distillation into a supervised variant. It designs supervised-contrastive losses at both global class token level and local patch token level for intra-class images.

- At the patch level, it introduces the challenging task of masked patch token reconstruction across images of the same class, which encourages learning of semantic correspondence.

- Experiments on four few-shot benchmarks demonstrate SMKD outperforms previous methods by a large margin given its simple design and training procedure.

In summary, the central hypothesis is that incorporating label information into self-supervised masked knowledge distillation can efficiently mitigate the overfitting issue of Transformers for few-shot learning. The proposed SMKD framework validates this hypothesis with strong empirical results.


## What is the main contribution of this paper?

 The main contributions of this paper are:

- Proposes a new supervised knowledge distillation framework (SMKD) that incorporates class label information into self-distillation, filling the gap between self-supervised knowledge distillation and traditional supervised learning.

- Designs two supervised-contrastive losses on both class and patch levels within the proposed framework. The class-level loss distills knowledge from global image features (\texttt{[cls]} tokens), while the patch-level loss introduces a novel task of masked patch tokens reconstruction across intra-class images.

- Achieves state-of-the-art results on CIFAR-FS and FC100 datasets and competitive performance on mini-ImageNet and tiered-ImageNet using simple prototype classification, demonstrating the effectiveness of the proposed method.

- The framework enjoys several advantages: it does not introduce extra parameters, is efficient to train, and can be easily combined with other methods.

In summary, the key contribution is a simple yet effective supervised knowledge distillation framework that incorporates label information into self-distillation through novel class-level and patch-level contrastive losses. This bridges the gap between self-supervised and supervised learning for few-shot transformers.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence TL;DR summary of the paper:

This paper proposes a novel supervised masked knowledge distillation framework for few-shot transformers that incorporates label information into self-distillation of class and patch tokens across views of intra-class images, outperforming previous self-supervised and supervised methods on few-shot image classification benchmarks.
