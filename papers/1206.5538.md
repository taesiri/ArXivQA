# [Representation Learning: A Review and New Perspectives](https://arxiv.org/abs/1206.5538)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question appears to be: What are the key recent developments in representation learning and deep architectures for AI, and what are some of the fundamental open questions and challenges that remain to be addressed?The paper provides a broad review and synthesis of recent research progress in representation learning, focusing on methods for learning feature representations from data that make it easier to extract useful information when building classifiers or other predictors. The main hypothesis seems to be that representation learning will be key to advancing machine learning towards artificial intelligence by reducing dependence on feature engineering and allowing learning of more abstract features relevant for solving complex AI tasks.The key topics and open questions highlighted in the paper include:- What makes a "good" representation and how can we design objectives and priors to learn useful representations disentangling the underlying explanatory factors in data?- How can we perform effective inference and sampling in probabilistic models with deep representations? How can probabilistic and reconstruction-based models be unified?- What causes the optimization difficulties in training deep neural networks, and how can we overcome issues like vanishing gradients and ill-conditioning?- How can we build in appropriate invariances and exploit priors like sparsity and temporal/spatial coherence when learning representations? - How can learned features better generalize to new tasks and domains via transfer learning and multi-task learning?- What are the tradeoffs between directed, undirected and autoencoder-based models for representation learning? How important is explicit probabilistic modeling versus learning parametric feature extractors?So in summary, the main open questions revolve around understanding and overcoming the challenges in learning good representations from data in order to move closer to general artificial intelligence.


## What is the main contribution of this paper?

This paper provides a review of recent work in representation learning and deep learning. The main contributions are:- Discusses three major approaches to representation learning: probabilistic models, autoencoders, and manifold learning. Highlights connections between these approaches.- Covers advances in probabilistic models like restricted Boltzmann machines (RBMs), deep belief networks, and deep Boltzmann machines. - Reviews autoencoder models like sparse autoencoders, denoising autoencoders, and contractive autoencoders.- Discusses manifold learning perspective and algorithms like local coordinate coding.- Describes techniques to build invariance like data augmentation, convolutional neural nets.- Discusses algorithms for disentangling factors of variation. - Covers recent advances in training deep architectures, like using unsupervised pre-training.- Discusses strengths and weaknesses of different approaches, as well as open questions and challenges.Overall, it provides a broad review of recent progress in representation learning, highlighting connections between different approaches and discussing both practical and theoretical open questions. The main value is in summarizing advances in this quickly evolving field up until around 2013.
