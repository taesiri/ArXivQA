# [Weakly Supervised Video Representation Learning with Unaligned Text for   Sequential Videos](https://arxiv.org/abs/2303.12370)

## What is the central research question or hypothesis that this paper addresses?

 The central research question this paper addresses is how to learn a good video representation in a weakly supervised manner using unaligned text data. Specifically, the paper proposes a method to learn semantic video representations using only the paragraph-level descriptions of videos and the sequence of sentence descriptions for each step, without needing fine-grained alignment between frames and sentences. 

The key hypotheses are:

1) Using a coarse-grained contrastive loss between video and paragraph representations can help learn useful global video representations. 

2) Generating pseudo-alignments between frames and sentences based on their temporal order and applying a fine-grained contrastive loss can help learn frame-level representations that capture the semantics of actions.

3) Combining these coarse-grained and fine-grained contrastive losses in a multi-granularity framework can learn good video representations from unaligned text in a weakly supervised setting.

The experiments aim to validate these hypotheses by showing that the learned representations transfer well to downstream tasks like video sequence verification and text-to-video retrieval compared to other baseline methods.
