# [Can Vision Transformers Perform Convolution?](https://arxiv.org/abs/2111.01353v2)

## What is the central research question or hypothesis that this paper addresses?

 Based on my understanding, the central research question this paper tries to address is:

Can a self-attention layer in Vision Transformers (with image patches as input) express arbitrary convolution operations? 

The authors note that Vision Transformers have recently demonstrated strong performance on computer vision tasks, sometimes surpassing convolutional neural networks. This raises the question of whether the self-attention mechanism in Vision Transformers has at least the same representational capacity as convolution operations, which are the core of CNNs. 

Specifically, the paper aims to compare the expressive power of a single self-attention layer in Vision Transformers versus a convolutional layer, when the input to the self-attention layer is a sequence of image patches rather than individual pixels. The patch input setting is more relevant for current Vision Transformer models.

The paper provides both theoretical analysis and experimental results to demonstrate that with a sufficient number of attention heads, a self-attention layer in Vision Transformers can indeed express arbitrary convolution operations, even with patch input. This implies Vision Transformers have at least the representational capacity to mimic CNNs' inductive biases like locality and translation equivariance.

In summary, the central research question is whether Vision Transformers' self-attention layers can express convolutional operations, especially in the practical patch input setting, in order to compare the expressive capacity of the two architectures. The paper aims to provide an affirmative answer both theoretically and empirically.


## What is the main contribution of this paper?

 Based on my reading, the main contributions of this paper are:

1. It provides a constructive proof to show that a 9-head self-attention layer in Vision Transformers with image patches as input can express any convolution operation. The key insights are using the multi-head attention mechanism and relative positional encoding to aggregate features for computing convolution. 

2. It proves lower bounds on the number of heads required for self-attention layers to express convolution, for both the patch input and pixel input settings. This shows the construction in the first contribution is optimal in terms of the number of heads. Specifically, the lower bounds show that Vision Transformers with patch input are more "head-efficient" than pixel input when expressing convolution.

3. It proposes a two-phase training pipeline for Vision Transformers, where the model is first trained with convolutional layers and then transferred to a Transformer model. This allows injecting convolutional bias into Transformers. Experiments show this pipeline significantly improves ViT's performance in low data regimes compared to random initialization.

In summary, the main contribution is providing both theoretical and empirical evidence to show that Vision Transformers can effectively perform convolution. The theory reveals how multi-head attention and positional encoding can achieve this, while the experiments demonstrate it can improve ViT training.
