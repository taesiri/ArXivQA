# [Putting People in Their Place: Affordance-Aware Human Insertion into   Scenes](https://arxiv.org/abs/2304.14406)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading, the central research question this paper addresses is: How can we develop a computational model to infer scene affordances and realistically insert people into scenes in a way that respects those affordances? 

The key ideas are:

- Scene affordances refer to the possible interactions an agent could have in a particular scene environment. Developing computational models to infer affordances is an important challenge in computer vision and robotics.

- The authors propose posing this as an image inpainting task - given a masked scene image and a reference person image, can we realistically inpaint the person into the scene in a natural pose that fits the context?

- They train a large-scale generative model on a dataset of 2.4 million video clips to learn to repose people based on scene context in a self-supervised manner.

- The model can not only do conditional insertion, but also hallucinate new people or scenes when prompted. This demonstrates it has learned something about human-scene interactions and affordances. 

- Compared to prior work that relied on ground truth pose annotations or operated in constrained settings, the authors' video-trained generative approach operates in more diverse real-world scenes and learns affordances in a more self-supervised, flexible way.

So in summary, the key research question is developing a scalable computational model to learn scene affordances and realistically insert people in a way that respects those affordances, which they achieve through a generative video-trained framework for context-aware person image inpainting.
