# [DreamComposer: Controllable 3D Object Generation via Multi-View   Conditions](https://arxiv.org/abs/2312.03611)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a detailed paragraph summarizing the key points of the paper:

This paper proposes DreamComposer, a flexible and scalable framework that enhances existing diffusion models for zero-shot novel view synthesis by incorporating multi-view image conditions. The key idea is to lift the features from multiple input view images into a 3D representation, render a target view from the fused 3D features, and inject the target view features into a pre-trained diffusion model to guide the image generation. Specifically, the pipeline first encodes input images into latent features and lifts them into compact tri-plane representations using a target-aware 3D lifting module. It then renders and fuses the multi-view 3D features into a target view latent feature map through a differentiable volume rendering approach. This target view feature map, containing rich auxiliary information from multiple views, is injected into the diffusion model using a ControlNet-like structure to guide the image generation process. A two-stage training strategy is adopted - first pre-training the 3D lifting module, then jointly optimizing it with the injection module while keeping the diffusion model fixed. Experiments show that DreamComposer boosted state-of-the-art novel view synthesis models with more accurate and controllable results. It also enabled various applications like controllable 3D editing and 3D character modeling. The framework is flexible, scalable to arbitrary numbers of input views, and can empower different diffusion models.
