# [Comparing Explanation Faithfulness between Multilingual and Monolingual   Fine-tuned Language Models](https://arxiv.org/abs/2403.12809)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem Statement
- Feature attribution methods (FAs) are used to explain model predictions by assigning importance scores to input tokens. Prior work has mainly studied the faithfulness (i.e. how well importance scores reflect the model's reasoning) of FAs in monolingual English models.
- It is unclear whether FAs are equally faithful when applied to multilingual vs monolingual models. This is an important question as practitioners need to decide between using mono- or multilingual models.

Main Contributions
- Comprehensive empirical study across 5 languages, multiple datasets, models, and FAs to compare FA faithfulness between mono- and multilingual models.
- Key finding: Larger the multilingual model, less faithful the FAs are compared to monolingual counterparts. For example, FAs are more faithful with mBERT than monolingual BERT, but less faithful with XLM-R than monolingual RoBERTa.
- Further analysis reveals faithfulness disparity is driven by differences in tokenization between mono- and multilingual models rather than semantic processing differences. More aggressive splitting by multilingual tokenizers leads to lower faithfulness.

Proposed Solution
- Systematic experiments across languages, models, datasets and feature attribution methods to quantify and analyze faithfulness disparity.
- Analyze impact of model size and tokenization on faithfulness disparity through controlled experiments.
- Show that models with similar tokenization exhibit similar faithfulness, indicating tokenization rather than model internals drive faithfulness differences.

Key Results
- Predictive performance of mono- and multilingual models is comparable, highlighting importance of studying faithfulness.
- Disparity direction flips between small vs large multilingual models - mBERT rationales more faithful than BERT but opposite for XLM-R vs RoBERTa.
- Tokenization analysis reveals aggressive splitting by multilingual tokenizers harms faithfulness.
- Adaptation experiments show models with similar tokenization achieve similar faithfulness.

Limitations and Future Work
- Lack of monolingual models in more languages restricted study.
- Further analysis needed for non-Indo-European/Sino-Tibetan languages.
- Emerging decoder-only models are multilingual by default - worth studying their faithfulness.
