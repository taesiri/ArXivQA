# [On the Convergence of Locally Adaptive and Scalable Diffusion-Based   Sampling Methods for Deep Bayesian Neural Network Posteriors](https://arxiv.org/abs/2403.08609)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem Statement
- Bayesian neural networks (BNNs) are a promising approach for modeling uncertainty in deep learning models. However, sampling from the posterior distribution of BNN parameters is very challenging. 

- Using adaptive step sizes analogous to modern neural network optimizers in MCMC sampling methods could significantly improve convergence, but theory suggests this requires computing an additional "correction term" which is very costly. 

- Recently, some methods like PSGLD and AdamSGLD have been proposed that use adaptive steps but drop or approximate the correction term, claiming this does not significantly change the sampled distribution. 

Main Contribution
- This paper provides theoretical analysis and empirical evidence demonstrating that these methods can introduce substantial bias into the sampled distribution, even with vanishing step size.

- For example, the distribution will typically have a deep local minimum at the global maxima of the posterior, which is clearly problematic.

- The key issue is that introducing adaptivity changes the stationary distribution of the stochastic process, and previous convergence analyses incorrectly assumed the stationary distribution matches the posterior.  

Proposed Solution
- The paper shows how to properly incorporate the correction term into the dynamics to recover the true posterior distribution. However, the main benefit of avoiding this term is lost, as computing it remains expensive.

- More broadly, the paper argues that using adaptive MCMC steps without properly accounting for how this changes the stationary distribution can fundamentally bias the sampling process. New ideas are likely needed to develop scalable adaptive MCMC for BNNs.

In summary, this paper clearly shows commonly used adaptive MCMC methods for BNNs sample from the wrong distribution, overturning claims that the correction term can be ignored. Future work on scalable BNN inference must properly account for how adaptivity impacts sampling.
