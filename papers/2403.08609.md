# [On the Convergence of Locally Adaptive and Scalable Diffusion-Based   Sampling Methods for Deep Bayesian Neural Network Posteriors](https://arxiv.org/abs/2403.08609)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem Statement
- Bayesian neural networks (BNNs) are a promising approach for modeling uncertainty in deep learning models. However, sampling from the posterior distribution of BNN parameters is very challenging. 

- Using adaptive step sizes analogous to modern neural network optimizers in MCMC sampling methods could significantly improve convergence, but theory suggests this requires computing an additional "correction term" which is very costly. 

- Recently, some methods like PSGLD and AdamSGLD have been proposed that use adaptive steps but drop or approximate the correction term, claiming this does not significantly change the sampled distribution. 

Main Contribution
- This paper provides theoretical analysis and empirical evidence demonstrating that these methods can introduce substantial bias into the sampled distribution, even with vanishing step size.

- For example, the distribution will typically have a deep local minimum at the global maxima of the posterior, which is clearly problematic.

- The key issue is that introducing adaptivity changes the stationary distribution of the stochastic process, and previous convergence analyses incorrectly assumed the stationary distribution matches the posterior.  

Proposed Solution
- The paper shows how to properly incorporate the correction term into the dynamics to recover the true posterior distribution. However, the main benefit of avoiding this term is lost, as computing it remains expensive.

- More broadly, the paper argues that using adaptive MCMC steps without properly accounting for how this changes the stationary distribution can fundamentally bias the sampling process. New ideas are likely needed to develop scalable adaptive MCMC for BNNs.

In summary, this paper clearly shows commonly used adaptive MCMC methods for BNNs sample from the wrong distribution, overturning claims that the correction term can be ignored. Future work on scalable BNN inference must properly account for how adaptivity impacts sampling.


## Summarize the paper in one sentence.

 Here is a one-sentence summary of the key point from the paper:

The paper demonstrates that current scalable diffusion-based sampling methods for Bayesian neural networks introduce substantial bias and do not converge to the true posterior distribution.


## What is the main contribution of this paper?

 This paper makes two main contributions:

1. It demonstrates that several recently proposed scalable Bayesian neural network sampling algorithms (PSGLD, SGRLD in Monge/Shampoo metrics, AdamSGLD) do not actually converge to the correct posterior distribution. Through both theoretical analysis and experiments, the paper shows these methods introduce significant bias into the sampled distribution.

2. More generally, the paper provides evidence that using adaptive step sizes in diffusion-based MCMC sampling methods without including the usual correction terms fundamentally changes the stationary distribution. This calls into question whether it is possible to make such sampling methods more scalable through adaptivity without incurring significant distributional bias. The ergodic properties of SDEs seem to preclude simply neglecting the correction terms.

So in summary, the paper convincingly shows that current approaches for scalable Bayesian neural network sampling have substantial problems, and adaptive MCMC may be fundamentally difficult to make work without bias. This is an important negative result for the field.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and concepts are:

- Bayesian neural networks (BNNs): Probabilistic neural networks that model the network parameters as random variables with associated prior distributions. Allow for uncertainty quantification.

- Monte Carlo Markov chain (MCMC) methods: Algorithms to generate samples from probability distributions like the BNN posterior by simulating a Markov chain that converges to the target distribution.

- Stochastic gradient Langevin dynamics (SGLD): A popular MCMC algorithm for sampling BNN posteriors that uses Langevin dynamics with stochastic gradients.

- Adaptive step sizes: Varying the step size of optimization or MCMC algorithms based on local information, similar to techniques used in neural network optimizers like Adam. Expected to improve convergence. 

- Riemannian metrics: Metrics defined over parameter space that allow the incorporation of adaptive step sizes into SGLD while still converging to the correct distribution. Require computationally costly extra terms.

- Preconditioned SGLD (PSGLD): A proposed adaptive SGLD algorithm that drops the costly correction terms of Riemannian SGLD. Claimed to still converge close to the posterior. 

- Stationary distribution: The invariant distribution a stochastic process like SGLD converges to. Should match the posterior distribution.

- Ergodicity: A property allowing the calculation of expectations with respect to the stationary distribution via time averages along the stochastic process.

- Bias: The analyzed algorithms introduce bias, causing their stationary distribution to deviate from the posterior distribution they are supposed to sample from.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in this paper:

1. The paper argues that using adaptive step sizes in MCMC sampling methods for Bayesian neural networks without including a correction term leads to a bias in the sampled distribution. Explain in detail why this happens based on the ergodic properties of stochastic differential equations. 

2. The paper shows that the common approach of using an exponential moving average of the gradient information leads to the correction term becoming negligibly small. Carefully explain this argument and why it invalidates the theoretical guarantees for convergence.

3. Analyze the stationary distribution derived for the PSGLD algorithm. Explain how the inclusion of the metric in the inverse of the distribution leads to a distortion, especially around local maxima. 

4. Compare and contrast the convergence results for the different proposed adaptive MCMC algorithms discussed in the paper. What core assumption causes the bias in all of them?

5. Discuss the potential fixes mentioned for algorithms like PSGLD and SGRLD in Monge/Shampoo metrics. Would these completely resolve the issues or do core problems still remain?

6. Explain the argument made against the approach by Wenzel et al. using occasional updates of the adaptive steps sizes. Do you think there could be ways to make this approach theoretically sound?

7. The paper argues that existing approaches to make MCMC sampling scalable do not work. Do you think the fundamental issues can be resolved at all within the SDE framework? Why or why not?

8. How significant do you think is the bias caused by these methods for practical applications where approximate convergence is acceptable? Does it invalidate their usefulness?

9. Suggest some alternative approaches not discussed in the paper that could potentially allow adaptive step sizes without changing the target distribution.

10. Implement one of the algorithms with bias, apply it toy distributions, and empirically estimate the deviation of the stationary distribution from the analytical prediction.
