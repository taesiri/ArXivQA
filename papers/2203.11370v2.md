# [Language modeling via stochastic processes](https://arxiv.org/abs/2203.11370v2)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question seems to be: How can contrastive learning representations be leveraged for generating coherent long text?

The authors propose an approach called Time Control (TC) that learns a latent space with Brownian bridge dynamics using a novel contrastive learning objective. They then generate text by sampling latent trajectories from this space and decoding them. The main hypothesis seems to be that modeling text dynamics in this way will allow TC to generate more globally coherent long text compared to standard language models like GPT-2. 

Some key aspects related to the central research question:

- The authors motivate the need for better representations and planning for generating coherent long text, which standard next-token autoregressive models struggle with. 

- They propose learning latent dynamics with a Brownian bridge contrastive objective to capture text coherence without hand-designed heuristics.

- TC is evaluated on its ability to model local and global text dynamics through experiments on discourse coherence and text generation tasks. 

- Comparisons are made to GPT-2 and ablations to analyze the impact of different components like the contrastive training, explicit dynamics modeling, and decoding.

So in summary, the central research question is about leveraging contrastive representations for improving long text generation through learning and sampling from latent text dynamics. The main hypothesis is that TC's approach will generate more coherent text.


## What is the main contribution of this paper?

 This paper proposes a novel contrastive learning objective based on Brownian bridge stochastic processes for learning latent representations that can model text dynamics. The key contributions are:

1. Deriving a contrastive learning objective that encourages the latent space to follow Brownian bridge dynamics. This is done by sampling positive triplets of sentences that come from the same document and enforcing that their latent embeddings follow the Brownian bridge conditional density. 

2. Proposing a model called Time Control (TC) that uses this contrastive learning objective to encode sentences into a latent space with dynamics. TC can then generate text by decoding from latent trajectories sampled from the learned Brownian bridge process.

3. Evaluating TC on both discriminative and generative tasks. For discriminative tasks, TC's encoder is competitive with sentence embedding methods on discourse coherence. For generative tasks, TC generates coherent long text better than baselines by preserving ordering and length statistics.

4. Performing thorough ablation studies to validate the importance of the contrastive objective, Brownian bridge dynamics, explicitly modeling dynamics, and decoding from a latent plan.

5. Providing both quantitative and qualitative analyses showing that the latent space learned by TC captures document dynamics, as evidenced by smooth latent trajectories on coherent documents and noisy trajectories on incoherent documents.

In summary, the key contribution is using ideas from stochastic processes to learn latent spaces that can model text dynamics for improved text generation, validated through empirical evaluations.
