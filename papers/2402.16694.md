# [HumanEval-XL: A Multilingual Code Generation Benchmark for Cross-lingual   Natural Language Generalization](https://arxiv.org/abs/2402.16694)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Existing code generation benchmarks focus mainly on English to Python or limited multilingual support. There is a critical gap in evaluating multilingual natural language (NL) to multilingual code generation capabilities of large language models (LLMs).  

Proposed Solution:
- The paper introduces a new massively multilingual code generation benchmark, HumanEval-XL, to address this gap. It comprises parallel coding problems spanning 23 NLs and 12 programming languages (PLs), with a total of 22,080 coding problems.

- The benchmark creation process involves extracting the NL portion from code prompts, translating to 23 languages with back-translation for quality checks using BERTScore, and manual verification. This ensures high-quality parallel test cases for fair cross-lingual comparisons.

Main Contributions:
- HumanEval-XL enables thorough assessment of cross-lingual NL generalization in code generation, setting a new standard. It covers diverse languages - 23 NLs across 11 families and 12 PLs.

- Experiments on major LLMs showcase trends of improved multilingual performance with increased scale, and specialized pre-training helps. But models still struggle in capturing equivalent semantics across languages.  

- This benchmark serves as a pioneering step towards rigorous evaluation for multilingual code generation and assessing NL generalization. The data and code are publicly released to facilitate research.

In summary, HumanEval-XL fills a critical void by providing a massively multilingual code generation benchmark to evaluate cross-lingual generalization, comprising over 22K parallel coding problems across 23 languages and 12 programming languages. Experiments and analysis provide insights into current model capabilities and challenges.
