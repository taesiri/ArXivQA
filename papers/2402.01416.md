# [Sequence Shortening for Context-Aware Machine Translation](https://arxiv.org/abs/2402.01416)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Context-aware machine translation aims to improve translations by incorporating surrounding context sentences. Two main architectures have been used - single-encoder (concatenation-based) and multi-encoder models.
- However, concatenation leads to sharply increasing memory/computation with more context. Multi-encoder models have high complexity. 

Proposed Solution:
- Use a caching architecture which encodes current sentence separately but caches/remembers representations of previous sentences as context representations for next step.
- Apply sequence shortening techniques like pooling, latent grouping, and latent selecting to compress cached context representations before reusing them. This reduces memory overhead.

Methods:
- Latent Grouping: Learns to softly group encoder tokens into a fixed number of groups, takes weighted average of tokens in each group.
- Latent Selecting: Learns to select a subset of input tokens into each group.

Experiments:
- Test caching architecture against strong baselines on English-German/French translation. Show caching is comparable or better.
- Applying shortening to cached context is effective - maintains performance while requiring less memory, especially for longer contexts. 
- Latent grouping/selecting very effective, achieve strong results while being more interpretable.

Main Contributions:
- First comprehensive analysis of caching architectures for context-aware translation
- Novel context shortening techniques - especially latent grouping/selecting
- Analysis showing sequence shortening reduces memory overhead and improves training stability
- Increased model interpretability through learning to group/select tokens
