# [A Hyper-Transformer model for Controllable Pareto Front Learning with   Split Feasibility Constraints](https://arxiv.org/abs/2402.05955)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Multi-objective optimization problems aim to optimize multiple conflicting objectives simultaneously. Finding the Pareto-optimal set that represents the tradeoffs between objectives is challenging, especially when constraints further limit the feasible region.
- Prior work on controllable Pareto front learning (CPFL) uses hypernetworks to map preference vectors to Pareto-optimal solutions. However, they don't handle split feasibility constraints that bound the objectives.
- Learning disconnected Pareto fronts found in real-world problems is also difficult for existing methods.

Proposed Solution: 
- Formulate the CPFL problem with split feasibility constraints using scalarization theory and split feasibility problem. Objectives are bounded within box constraints that decision-makers can control.
- Propose a hypernetwork based on Transformer architecture (Hyper-Transformer) for this problem. Compare it to prior MLP-based hypernetwork.
- For learning disconnected Pareto fronts, propose two Hyper-Transformer variants:
   1) With joint input of preference vector and lower bounds 
   2) With mixture of experts, where each expert focuses on one Pareto front component

Main Contributions:
- Formulate and solve the CPFL problem with split feasibility constraints using hypernetworks
- Propose Hyper-Transformer solution and show its superiority over Hyper-MLP
- Provide theoretical analysis of Hyper-Transformer's approximation capability 
- First work to address controllable disconnected Pareto front learning
- Show strong empirical performance on both connected and disconnected MOO problem benchmarks
- Demonstrate Hyper-Transformer's effectiveness on multi-task learning problems

The summary covers the key aspects of the paper - the problem, proposed solution and contributions. It describes the concepts and solutions at a high-level without getting into technical details, allowing a human to fully understand the essence of the work.
