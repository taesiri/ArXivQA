# [EQ-Bench: An Emotional Intelligence Benchmark for Large Language Models](https://arxiv.org/abs/2312.06281)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a summary paragraph of the key points from the paper:

This paper introduces EQ-Bench, a new benchmark for evaluating emotional intelligence in language models. It focuses on assessing a model's ability to understand complex emotions and social dynamics by having them predict the intensity of emotional states of characters in dialogues depicting scenes of interpersonal tension or conflict. The benchmark shows strong ability to differentiate between a wide range of models, with scores correlating highly (r=0.97) with multi-domain benchmarks like MMLU that measure general intelligence. This suggests emotional intelligence acts as an effective proxy for broad intelligence in language models. The benchmark is repeatable, using 60 English dialogues, with an automated pipeline provided to facilitate testing. Methodological improvements over prior emotional intelligence benchmarks like SECEU are implemented, including more complex dialogues, a diverse selection of emotions to rate, and reference answers set by the authors. The top scoring model tested is OpenAI's GPT-4, aligning with community perceptions of it as state-of-the-art, while leading open-source models like SynthIA-70B are rapidly closing the gap. Overall, EQ-Bench enables standardized and insightful evaluation of emotional and social understanding abilities in language models.


## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Emotional intelligence (EI) is an important capability for language models to interact effectively with humans, but there is a lack of benchmarks to specifically measure EI in large language models (LLMs). 
- Existing EI tests for humans are not suitable for evaluating LLMs. Industry standard LLM benchmarks assess broad capabilities but do not focus on emotional understanding.
- The SECEU benchmark aims to measure emotional understanding but has limitations in differentiating model capabilities and correlating scores with perceived intelligence.

Proposed Solution:
- The authors introduce EQ-Bench, a benchmark to assess emotional understanding in LLMs by evaluating their ability to rate the intensity of emotions in characters after reading a dialogue.
- The key improvements over SECEU include more complex dialogues focused on emotional interactions, better selection of emotions to rate, author-decided reference answers, and removing score summation constraints.

Main Contributions:
- EQ-Bench produces a wide spread of scores that reliably differentiate model capabilities and correlate strongly (r=0.97) with the industry standard MMLU benchmark.
- The scores align well with community perceptions of model intelligence and show sensitivity to model scale.
- The benchmark comprises 60 questions, is fast to run, and has an automated pipeline for standardized administration.
- EQ-Bench fills a gap by providing an interpretable benchmark focused specifically on emotional intelligence in LLMs to complement broader assessments.

In summary, EQ-Bench allows effective benchmarking of emotional understanding in LLMs using a narrow test set that correlates tightly with general intelligence, providing the community with an open source tool aligned to an important AI capability.
