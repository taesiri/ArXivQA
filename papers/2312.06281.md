# [EQ-Bench: An Emotional Intelligence Benchmark for Large Language Models](https://arxiv.org/abs/2312.06281)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Emotional intelligence (EI) is an important capability for language models to interact effectively with humans, but there is a lack of benchmarks to specifically measure EI in large language models (LLMs). 
- Existing EI tests for humans are not suitable for evaluating LLMs. Industry standard LLM benchmarks assess broad capabilities but do not focus on emotional understanding.
- The SECEU benchmark aims to measure emotional understanding but has limitations in differentiating model capabilities and correlating scores with perceived intelligence.

Proposed Solution:
- The authors introduce EQ-Bench, a benchmark to assess emotional understanding in LLMs by evaluating their ability to rate the intensity of emotions in characters after reading a dialogue.
- The key improvements over SECEU include more complex dialogues focused on emotional interactions, better selection of emotions to rate, author-decided reference answers, and removing score summation constraints.

Main Contributions:
- EQ-Bench produces a wide spread of scores that reliably differentiate model capabilities and correlate strongly (r=0.97) with the industry standard MMLU benchmark.
- The scores align well with community perceptions of model intelligence and show sensitivity to model scale.
- The benchmark comprises 60 questions, is fast to run, and has an automated pipeline for standardized administration.
- EQ-Bench fills a gap by providing an interpretable benchmark focused specifically on emotional intelligence in LLMs to complement broader assessments.

In summary, EQ-Bench allows effective benchmarking of emotional understanding in LLMs using a narrow test set that correlates tightly with general intelligence, providing the community with an open source tool aligned to an important AI capability.
