# [Neural Networks Against (and For) Self-Training: Classification with   Small Labeled and Large Unlabeled Sets](https://arxiv.org/abs/2401.00575)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Self-training suffers from two key issues: semantic drift where noisy pseudo-labels accumulate and distort class boundaries over iterations, and overconfidence of neural network predictions during pseudo-label selection.

Proposed Solution - Robust Self-Training (RST):
- Mitigates semantic drift by creating a hierarchical order of information using the catastrophic forgetting property of neural networks. Pseudo-labels are ordered by iteration and used to initialize the network before finetuning on labeled data in each iteration.

- Addresses overconfidence by replacing plain confidence with a novel metric for pseudo-label selection. The metric incorporates both confidence and uncertainty via normalized entropy and generalized Jensen-Shannon divergence between predictions from subsamples of the labeled set.

Main Contributions:

- Proposes a loss function that balances minimizing cross-entropy loss on ground truth labels while preventing erosion of knowledge from pseudo-labels.

- Demonstrates the robustness of RST against noisy pseudo-labels and shows it reaches a performance plateau, indicating stability to semantic drift.

- Shows RST achieves strong performance across 5 text classification datasets, outperforming 10 competitive semi-supervised baselines.

- Reveals RST's improvements are additive to gains from domain-specific language model pretraining, demonstrating compatibility.

- Provides ablation studies and analysis to demonstrate the efficacy of both the pretraining and subsampling components of RST.

In summary, the paper makes notable contributions in addressing key weaknesses in self-training through novel pretraining and scoring strategies. Experiments thoroughly demonstrate state-of-the-art performance and robustness of the proposed RST approach.
