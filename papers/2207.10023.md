# [Tailoring Self-Supervision for Supervised Learning](https://arxiv.org/abs/2207.10023)

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper addresses is: How can we design a self-supervised pretext task that is tailored specifically for improving supervised learning? The key points are:- Existing self-supervised pretext tasks like rotation prediction are designed for unsupervised representation learning, and have limitations when applied to supervised learning. - The authors argue an ideal auxiliary self-supervised task for supervised learning should have 3 properties: (1) guide the model to learn complementary features, (2) maintain the original data distribution, (3) be lightweight and easy to implement.- They propose "Localizable Rotation (LoRot)" as a pretext task tailored for supervised learning. It rotates only a patch of the image and predicts the rotation, forcing the model to learn detailed features of object parts.- Rotating just a patch maintains the overall data distribution, unlike rotating the whole image. And it's easy to implement as a multi-task objective.- Experiments validate LoRot improves supervised models' robustness and generalization capability, achieving state-of-the-art results on tasks like out-of-distribution detection.In summary, the key hypothesis is that a self-supervised pretext task can be specifically designed to enhance supervised learning, with properties like LoRot's patch rotation. The experiments aim to validate its benefits.


## What is the main contribution of this paper?

Based on my reading, the main contributions of this paper are:1. It discusses three desirable properties for auxiliary self-supervision tasks to assist supervised learning objectives: (i) learning rich representations, (ii) maintaining data distribution, and (iii) providing high applicability. 2. It proposes a simple yet effective self-supervised task called Localizable Rotation (LoRot) tailored for supervised learning. LoRot rotates only a patch of the input image and predicts the rotation, encouraging the model to learn complementary features to the primary supervised task. 3. It shows that rotating a small patch does not significantly alter the data distribution, allowing LoRot to work well in a multi-task learning framework with the primary supervised objective.4. Through extensive experiments, it demonstrates that LoRot consistently improves model robustness and generalization capability across tasks like out-of-distribution detection, imbalanced classification, adversarial attack, image classification, localization, and transfer learning.In summary, the key contribution is proposing LoRot, a lightweight and easily applicable self-supervised task designed specifically to assist supervised learning objectives, and showing its effectiveness across diverse tasks and model architectures. The discussions around desired properties of auxiliary self-supervision and maintaining data distribution are also valuable.
