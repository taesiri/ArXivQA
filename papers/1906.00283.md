# [Learning to Generate Grounded Visual Captions without Localization   Supervision](https://arxiv.org/abs/1906.00283)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is: How can we improve the visual grounding of image/video captioning models without relying on ground truth bounding box annotations or additional computation during inference?The key hypothesis is that using a novel cyclical training regimen of decoding-localization-reconstruction can enforce better visual grounding of the captioning model by forcing it to rely on localized image regions to reconstruct the caption. This is done without ground truth bounding box supervision or extra computation during inference.In summary, the paper aims to develop a method to improve visual grounding of captioning models through a self-supervised cyclical training approach, removing the need for explicit grounding supervision or extra computation at test time. The core hypothesis is that their proposed cyclical training regimen can achieve this goal.
