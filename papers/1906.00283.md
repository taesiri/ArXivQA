# [Learning to Generate Grounded Visual Captions without Localization   Supervision](https://arxiv.org/abs/1906.00283)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is: How can we improve the visual grounding of image/video captioning models without relying on ground truth bounding box annotations or additional computation during inference?The key hypothesis is that using a novel cyclical training regimen of decoding-localization-reconstruction can enforce better visual grounding of the captioning model by forcing it to rely on localized image regions to reconstruct the caption. This is done without ground truth bounding box supervision or extra computation during inference.In summary, the paper aims to develop a method to improve visual grounding of captioning models through a self-supervised cyclical training approach, removing the need for explicit grounding supervision or extra computation at test time. The core hypothesis is that their proposed cyclical training regimen can achieve this goal.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contribution is proposing a novel cyclical training regimen to improve the visual grounding of image and video captioning models, without requiring any grounding supervision or annotations. Specifically, the key ideas are:- Current captioning models with attention struggle to properly associate generated words with image regions, since LSTM decoders can propagate information across time steps. - They propose a cyclical training process of decoding -> localization -> reconstruction. This forces the model to localize words after decoding, and reconstruct sentences from localized regions.- The localizer regularizes the attention mechanism to rely more on localized regions when reconstructing sentences. This improves grounding without grounding supervision.- They show quantitative and qualitative improvements in grounding on Flickr30K and ActivityNet by comparing to baselines and prior work. The model better grounds individual generated words without loss in caption quality.- The proposed training process is simple, only requiring an added linear localizer layer that can be removed at test time. So it does not increase computation during inference.In summary, the key contribution is a novel cyclical training method that significantly improves visual grounding for captioning without any grounding supervision, through an implicit regularization. The gains are demonstrated on image and video tasks.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper proposes a novel cyclical training method to improve the visual grounding of image and video captioning models without requiring explicit grounding supervision.


## How does this paper compare to other research in the same field?

Here are a few key ways this paper compares to other related research in grounded visual captioning:- The main contribution is proposing a novel cyclical training approach to improve visual grounding without needing grounding supervision. This is different from most prior work like Liu et al. and Zhou et al. which rely on grounding annotations.- The method is simple and only adds a single fully-connected layer during training that can be removed at test time. So it doesn't increase computation at inference compared to many other approaches.- The experiments comprehensively evaluate on both image and video captioning datasets and show around 15-20% relative improvement in grounding over strong baselines. This demonstrates the broad applicability of the approach.- The introduced per-sentence grounding metrics provide a more direct and intuitive way to measure grounding rather than the per-class averages used in prior work.- The analysis provides insights into model behaviors, like showing the localizer provides better regions than the decoder attention during training.- The human evaluation further verifies the grounding improvements, which is rarely done in other captioning papers.Overall, the work makes a solid contribution in improving visual grounding for captioning without extra supervision or computation cost. The simple yet effective approach compares favorably to more complex methods in prior work. The extensive analysis and evaluations on two tasks situates the method well with respect to the state-of-the-art.
