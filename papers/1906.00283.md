# [Learning to Generate Grounded Visual Captions without Localization   Supervision](https://arxiv.org/abs/1906.00283)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is: How can we improve the visual grounding of image/video captioning models without relying on ground truth bounding box annotations or additional computation during inference?The key hypothesis is that using a novel cyclical training regimen of decoding-localization-reconstruction can enforce better visual grounding of the captioning model by forcing it to rely on localized image regions to reconstruct the caption. This is done without ground truth bounding box supervision or extra computation during inference.In summary, the paper aims to develop a method to improve visual grounding of captioning models through a self-supervised cyclical training approach, removing the need for explicit grounding supervision or extra computation at test time. The core hypothesis is that their proposed cyclical training regimen can achieve this goal.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contribution is proposing a novel cyclical training regimen to improve the visual grounding of image and video captioning models, without requiring any grounding supervision or annotations. Specifically, the key ideas are:- Current captioning models with attention struggle to properly associate generated words with image regions, since LSTM decoders can propagate information across time steps. - They propose a cyclical training process of decoding -> localization -> reconstruction. This forces the model to localize words after decoding, and reconstruct sentences from localized regions.- The localizer regularizes the attention mechanism to rely more on localized regions when reconstructing sentences. This improves grounding without grounding supervision.- They show quantitative and qualitative improvements in grounding on Flickr30K and ActivityNet by comparing to baselines and prior work. The model better grounds individual generated words without loss in caption quality.- The proposed training process is simple, only requiring an added linear localizer layer that can be removed at test time. So it does not increase computation during inference.In summary, the key contribution is a novel cyclical training method that significantly improves visual grounding for captioning without any grounding supervision, through an implicit regularization. The gains are demonstrated on image and video tasks.
