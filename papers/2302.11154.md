# Open-domain Visual Entity Recognition: Towards Recognizing Millions of   Wikipedia Entities

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the key research question this paper seeks to address is:Are pre-trained multi-modal models capable of recognizing open-domain visual concepts? In other words, can models pre-trained on large datasets of images and text generalize to recognize a wide variety of visual concepts across different domains?To evaluate this, the authors introduce a new task called Open-domain Visual Entity Recognition (OVEN), which requires models to link images to Wikipedia entities based on a textual query. They construct a large-scale dataset called OVEN-Wiki for this task by unifying 14 existing datasets and grounding the labels to Wikipedia entities. The paper examines two prominent pre-trained models - CLIP and PaLI - by fine-tuning them on OVEN-Wiki and evaluating their ability to recognize both seen (present in training) and unseen entities. The results reveal there is significant room for improvement in generalizing to the large label space. The key findings suggest pre-trained generative models like PaLI perform surprisingly well even on unseen entities, while CLIP-based retrieval models show strengths in recognizing rare/tail entities.In summary, the central hypothesis is on evaluating whether current multi-modal models exhibit universal visual recognition abilities. The OVEN benchmark and experiments reveal existing models still have challenges generalizing to large open-domain visual concepts, despite successes on narrower recognition tasks.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contributions seem to be:- Proposing the task of Open-domain Visual Entity Recognition (OVEN), which requires models to recognize visual entities from a large open-domain label space like Wikipedia. This aims to evaluate universal visual recognition abilities.- Constructing the OVEN-Wiki dataset by re-purposing and re-annotating 14 existing datasets to create a benchmark grounded in Wikipedia entities. The dataset has over 6 million possible labels spanning diverse visual concepts.- Evaluating state-of-the-art multimodal models like CLIP and PaLI on OVEN-Wiki. The results reveal headroom in generalizing to the massive label space and differences in strengths of the models - PaLI has higher overall performance but CLIP better recognizes rare entities.- Formal analysis comparing the models and error analysis revealing the models make distinct error types. PaLI tends to predict more generic concepts while CLIP misunderstands the query intent more often.So in summary, the main contributions are proposing the OVEN task and dataset to evaluate open-domain visual recognition, benchmarking top models which reveals challenges in massive-label generalization, and analysis providing insights into model behaviors. The paper aims to drive progress on universal visual recognition.


## How does this paper compare to other research in the same field?

Here are a few thoughts on how this paper compares to other related research:- This paper presents a new task and dataset for open-domain visual entity recognition. Other recent work has focused more narrowly on knowledge-based visual QA or zero-shot visual recognition of a limited label space. So this paper is unique in targeting recognition of millions of Wikipedia entities in an open-domain setting.- The paper leverages existing image classification, retrieval, and VQA datasets but creates a new unified label space by grounding the labels to Wikipedia entities. Other efforts like Conceptual Captions also repurpose existing vision datasets but don't map them to a knowledge base. - The paper examines both encoder-decoder and dual encoder models for the task. This allows for comparison between the latest generative (PaLI) and retrieval (CLIP) models. Other recent work has focused more exclusively on one model type.- Analysis reveals different strengths between the PaLI and CLIP models - PaLI better on popular entities, CLIP better on tail entities. This kind of analysis and insight isn't present in most benchmark papers which just report overall performance metrics.- The paper contextualizes results with estimated human performance. Human upper bounds are missing from a lot of vision research but provide an important reference point.- The constructed dataset is made publicly available. Along with the large label space and human evaluations, this will support future research and benchmarking. Many papers introduce proprietary datasets without releasing them.Overall, I'd say the large-scale label space, detailed human evaluations, model analysis, and public dataset make this paper stand out from related benchmarking efforts in knowledge-infused vision. The tasks and findings seem highly relevant to progress in this direction.


## What future research directions do the authors suggest?

Based on my reading of the paper, some key future research directions suggested by the authors include:- Developing more advanced fine-tuning strategies and regularization techniques to improve model generalization across Wikipedia entities. The authors show that longer fine-tuning tends to improve performance on seen entities but hurt unseen entity generalization.- Scaling up model capacity, especially for dual encoder models like CLIP. The authors find the larger 17B parameter PaLI model significantly outperforms the 3B model, suggesting model scale is very important.- Exploring hybrid retrieval-generation models that can combine the complementary strengths of the CLIP and PaLI models. For instance, using retrieval to propose candidates that a generative model reranks.- Developing frequency-calibrated evaluation metrics that put more weight on recognizing rare/tail entities compared to popular head entities.- Extending the OVeN framework to include entities not physically present but inferrable from the image, as well as non-visual modalities. - Applying OVeN models and data to knowledge-intensive downstream applications like visual question answering.- Continuing to scale up the knowledge base of entities and expand OVeN to new languages and multilingual settings.So in summary, directions like improving generalization, scaling up models, combining retrieval and generation, extending the task formulation, and leveraging OVeN for downstream applications are highlighted. The authors frame OVeN as an ongoing research benchmark to drive progress in universal visual entity recognition.


## Summarize the paper in one paragraph.

Here is a one paragraph summary of the paper:The paper introduces the task of Open-domain Visual Entity Recognition (\name), which requires models to link images to Wikipedia entities based on a corresponding text query. To enable research on this task, the authors construct a new dataset called \datasetname by combining and re-annotating 14 existing image classification, retrieval, and visual QA datasets, grounding their labels to Wikipedia entities. This results in over 20,000 grounded labels linked to Wikipedia pages and images. \datasetname contains training, validation, test, and human-annotated splits, with the test and human-annotated splits containing unseen entities. Experiments are conducted with encoder-decoder models like PaLI and dual encoder models like CLIP. Results show that while the auto-regressive PaLI model performs better overall, especially on popular entities, the CLIP dual encoder is better at recognizing rare entities. There is still a large gap between state-of-the-art models and human performance, suggesting room for improvement on open-domain visual entity recognition.


## Summarize the paper in two paragraphs.

Here is a two paragraph summary of the paper:The paper introduces the task of Open-domain Visual Entity Recognition (OVER), which requires linking images to Wikipedia entities based on a textual query. The authors construct the OVER-Wiki dataset by re-purposing 14 existing image classification, retrieval, and VQA datasets, unifying their labels and intents to Wikipedia entities. This results in a dataset with over 6 million possible labels, making it a large-scale test of visual recognition. The authors evaluate state-of-the-art multimodal models including CLIP and PaLI on OVER-Wiki. They find that PaLI-based autoregressive models perform the best overall, even generalizing to unseen entities. However, CLIP-based retrieval models are better at recognizing rare entities. The results reveal substantial room for improvement on massive-scale open-domain recognition. The paper concludes that OVER is a challenging benchmark for evaluating universal visual recognition abilities of multimodal models.


## Summarize the main method used in the paper in one paragraph.

Here is a one paragraph summary of the main method used in the paper:The paper introduces a new task called Open-domain Visual Entity Recognition (OVEN), which aims to link images to Wikipedia entities based on a text query. To enable research on this task, the authors construct a new dataset called OVEN-Wiki by combining and re-annotating 14 existing image classification, retrieval, and visual QA datasets. All the labels are grounded to Wikipedia entities, resulting in over 20,000 labels mapped to Wikipedia pages. The dataset contains train, validation, and test splits with seen entities in train/validation and unseen entities in test, requiring models to generalize to new entities. The authors fine-tune two representative pre-trained multi-modal models on OVEN: CLIP, which is adapted to perform entity retrieval, and PaLI, an encoder-decoder model which is adapted to generate entity names. These models are evaluated on their ability to recognize seen and unseen entities on the OVEN-Wiki dataset. The results show PaLI models outperform CLIP models overall, but CLIP has an advantage on rare entities. This demonstrates these pre-trained models still have difficulties generalizing to large open-domain entity spaces like Wikipedia.
