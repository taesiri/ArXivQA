# Open-domain Visual Entity Recognition: Towards Recognizing Millions of   Wikipedia Entities

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the key research question this paper seeks to address is:Are pre-trained multi-modal models capable of recognizing open-domain visual concepts? In other words, can models pre-trained on large datasets of images and text generalize to recognize a wide variety of visual concepts across different domains?To evaluate this, the authors introduce a new task called Open-domain Visual Entity Recognition (OVEN), which requires models to link images to Wikipedia entities based on a textual query. They construct a large-scale dataset called OVEN-Wiki for this task by unifying 14 existing datasets and grounding the labels to Wikipedia entities. The paper examines two prominent pre-trained models - CLIP and PaLI - by fine-tuning them on OVEN-Wiki and evaluating their ability to recognize both seen (present in training) and unseen entities. The results reveal there is significant room for improvement in generalizing to the large label space. The key findings suggest pre-trained generative models like PaLI perform surprisingly well even on unseen entities, while CLIP-based retrieval models show strengths in recognizing rare/tail entities.In summary, the central hypothesis is on evaluating whether current multi-modal models exhibit universal visual recognition abilities. The OVEN benchmark and experiments reveal existing models still have challenges generalizing to large open-domain visual concepts, despite successes on narrower recognition tasks.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contributions seem to be:- Proposing the task of Open-domain Visual Entity Recognition (OVEN), which requires models to recognize visual entities from a large open-domain label space like Wikipedia. This aims to evaluate universal visual recognition abilities.- Constructing the OVEN-Wiki dataset by re-purposing and re-annotating 14 existing datasets to create a benchmark grounded in Wikipedia entities. The dataset has over 6 million possible labels spanning diverse visual concepts.- Evaluating state-of-the-art multimodal models like CLIP and PaLI on OVEN-Wiki. The results reveal headroom in generalizing to the massive label space and differences in strengths of the models - PaLI has higher overall performance but CLIP better recognizes rare entities.- Formal analysis comparing the models and error analysis revealing the models make distinct error types. PaLI tends to predict more generic concepts while CLIP misunderstands the query intent more often.So in summary, the main contributions are proposing the OVEN task and dataset to evaluate open-domain visual recognition, benchmarking top models which reveals challenges in massive-label generalization, and analysis providing insights into model behaviors. The paper aims to drive progress on universal visual recognition.


## How does this paper compare to other research in the same field?

Here are a few thoughts on how this paper compares to other related research:- This paper presents a new task and dataset for open-domain visual entity recognition. Other recent work has focused more narrowly on knowledge-based visual QA or zero-shot visual recognition of a limited label space. So this paper is unique in targeting recognition of millions of Wikipedia entities in an open-domain setting.- The paper leverages existing image classification, retrieval, and VQA datasets but creates a new unified label space by grounding the labels to Wikipedia entities. Other efforts like Conceptual Captions also repurpose existing vision datasets but don't map them to a knowledge base. - The paper examines both encoder-decoder and dual encoder models for the task. This allows for comparison between the latest generative (PaLI) and retrieval (CLIP) models. Other recent work has focused more exclusively on one model type.- Analysis reveals different strengths between the PaLI and CLIP models - PaLI better on popular entities, CLIP better on tail entities. This kind of analysis and insight isn't present in most benchmark papers which just report overall performance metrics.- The paper contextualizes results with estimated human performance. Human upper bounds are missing from a lot of vision research but provide an important reference point.- The constructed dataset is made publicly available. Along with the large label space and human evaluations, this will support future research and benchmarking. Many papers introduce proprietary datasets without releasing them.Overall, I'd say the large-scale label space, detailed human evaluations, model analysis, and public dataset make this paper stand out from related benchmarking efforts in knowledge-infused vision. The tasks and findings seem highly relevant to progress in this direction.


## What future research directions do the authors suggest?

Based on my reading of the paper, some key future research directions suggested by the authors include:- Developing more advanced fine-tuning strategies and regularization techniques to improve model generalization across Wikipedia entities. The authors show that longer fine-tuning tends to improve performance on seen entities but hurt unseen entity generalization.- Scaling up model capacity, especially for dual encoder models like CLIP. The authors find the larger 17B parameter PaLI model significantly outperforms the 3B model, suggesting model scale is very important.- Exploring hybrid retrieval-generation models that can combine the complementary strengths of the CLIP and PaLI models. For instance, using retrieval to propose candidates that a generative model reranks.- Developing frequency-calibrated evaluation metrics that put more weight on recognizing rare/tail entities compared to popular head entities.- Extending the OVeN framework to include entities not physically present but inferrable from the image, as well as non-visual modalities. - Applying OVeN models and data to knowledge-intensive downstream applications like visual question answering.- Continuing to scale up the knowledge base of entities and expand OVeN to new languages and multilingual settings.So in summary, directions like improving generalization, scaling up models, combining retrieval and generation, extending the task formulation, and leveraging OVeN for downstream applications are highlighted. The authors frame OVeN as an ongoing research benchmark to drive progress in universal visual entity recognition.
