# [Aligning Large Language Models with Human: A Survey](https://arxiv.org/abs/2307.12966)

## What is the central research question or hypothesis that this paper addresses?

After carefully reading the paper, the central research question is how to effectively and efficiently align large language models (LLMs) with human expectations and values. Specifically, the authors survey methods for:1) Collecting high-quality instruction data for training and aligning LLMs to follow human preferences, including leveraging existing benchmarks, human annotations, and strong LLMs like ChatGPT to generate instructions. 2) Training techniques like supervised fine-tuning, online/offline reinforcement learning, and parameter efficient methods to incorporate human preferences into LLMs in a stable and efficient manner.3) Evaluating the alignment of LLMs using specialized benchmarks and paradigms like human evaluations, reference-free LLMs evaluation, and evaluation-specific LLMs. The overarching goal is to provide a comprehensive overview of techniques for collecting alignment data, training aligned LLMs, and evaluating their capabilities so as to equip researchers with the knowledge to advance the alignment of large models like ChatGPT with human values and preferences. The survey synthesizes a broad swath of recent research to highlight promising future directions in this rapidly evolving field.
