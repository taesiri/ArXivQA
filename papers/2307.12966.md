# [Aligning Large Language Models with Human: A Survey](https://arxiv.org/abs/2307.12966)

## What is the central research question or hypothesis that this paper addresses?

 After carefully reading the paper, the central research question is how to effectively and efficiently align large language models (LLMs) with human expectations and values. Specifically, the authors survey methods for:

1) Collecting high-quality instruction data for training and aligning LLMs to follow human preferences, including leveraging existing benchmarks, human annotations, and strong LLMs like ChatGPT to generate instructions. 

2) Training techniques like supervised fine-tuning, online/offline reinforcement learning, and parameter efficient methods to incorporate human preferences into LLMs in a stable and efficient manner.

3) Evaluating the alignment of LLMs using specialized benchmarks and paradigms like human evaluations, reference-free LLMs evaluation, and evaluation-specific LLMs. 

The overarching goal is to provide a comprehensive overview of techniques for collecting alignment data, training aligned LLMs, and evaluating their capabilities so as to equip researchers with the knowledge to advance the alignment of large models like ChatGPT with human values and preferences. The survey synthesizes a broad swath of recent research to highlight promising future directions in this rapidly evolving field.


## What is the main contribution of this paper?

 Based on my reading, the main contribution of this paper is proposing an efficient solution to adapt the English-oriented Large Language Model (LLM) LLaMA to better understand Chinese inputs. 

Specifically, the authors first analyze that the original LLaMA has limited Chinese vocabulary (less than 1k characters) and thus relies on inefficient byte-fallback strategy to tokenize Chinese text, resulting in long input sequences. 

To address this issue, they propose a two-stage Chinese pre-training approach:

1) In the first stage, they expand the LLaMA vocabulary with 20k additional Chinese words/phrases and fine-tune the input embeddings while keeping other parameters frozen. This allows LLaMA to form better input representations for Chinese.

2) In the second stage, they add LoRA parameters to all layers in LLaMA and jointly fine-tune the input embeddings, self-attention heads, and LoRA parameters. LoRA enables efficient adaptation of the entire LLaMA model using limited additional parameters.

Through experiments on Chinese tasks like pCLUE and C-Eval, they demonstrate that their proposed approach allows LLaMA to achieve significantly better efficiency and performance on understanding Chinese text with limited additional pre-training.

In summary, the key contribution is using vocabulary expansion and a two-stage LoRA-based fine-tuning process to efficiently adapt the English LLaMA model into a Chinese-friendly version while preserving its original capabilities. This makes LLaMA more accessible to Chinese users without extensive re-training.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the paper:

The paper provides a comprehensive overview of recent research on aligning large language models with human values and expectations, covering key aspects like collecting high-quality training data, optimizing training methods, and evaluating model capabilities on human-centric benchmarks.


## How does this paper compare to other research in the same field?

 Here are a few key points on how this paper compares to other related research on aligning large language models with human values and expectations:

- Data Collection: This survey provides a broad overview of different techniques for collecting high-quality alignment data, including leveraging existing NLP benchmarks, hand-crafted instructions, self-instruction with strong LLMs, and effective data management strategies. It covers more methods than previous surveys in this area.

- Training Methods: The paper systematically reviews online and offline human preference learning techniques as well as parameter-efficient training. It offers more details on emerging offline ranking-based and language-based training methods compared to prior surveys. 

- Evaluation: The survey discusses the latest human-centered and LLM-based evaluation benchmarks and protocols. It provides a more comprehensive look at biases in LLM-based evaluation versus past work.

- Scope: This survey aims to provide a holistic overview of alignment techniques for general large language models. In contrast, some previous surveys focused only on a specific aspect like training methods or evaluation.

- Organization: The content is structured around the key phases of alignment - data, training, and evaluation. This provides a logical flow for readers. Earlier surveys were organized by methodology type.

- Future Directions: The survey concludes by clearly identifying open challenges and promising research avenues. The breadth of directions highlighted reflects the rapid evolution of this field.

Overall, this survey offers a more extensive, structured, and up-to-date overview of LLM alignment compared to preceding works. The comprehensive synthesis of recent advances will likely make it a valuable resource for researchers and practitioners in this domain.


## What future research directions do the authors suggest?

 Based on my reading of the paper, the authors suggest the following future research directions:

1. Fine-grained instruction data management: The authors point out that many alignment methods use training instructions from diverse sources, making it hard to compare methods fairly. They suggest research on optimal quality control, sequence, and mixing of instructions to enable high-quality instruction data construction.

2. Alignment for non-English languages: Most existing research focuses on English. The authors suggest exploring how alignment technologies perform on other languages, especially low-resource ones, and how to effectively transfer alignment capabilities across languages.

3. Alignment training technologies: Many aligned LLMs use simple supervised fine-tuning. The authors suggest comprehensive investigation of various training technologies under resource constraints, to verify their effectiveness and promote efficient and environmentally-friendly solutions.

4. Human-in-the-loop data generation: The authors note that human-curated data like ShareGPT performs well, indicating the value of human involvement. They suggest exploring other human-in-the-loop solutions to facilitate alignment.

5. Human-LLM joint evaluation: The authors propose using both LLMs and humans for evaluation, assigning tasks based on their strengths, to maintain efficiency and quality.

In summary, the key suggestions are to explore finer-grained and optimized data/training management, expanding beyond English, leveraging human involvement, and combining human and LLM capabilities for evaluation. The overall goal is improving alignment quality while ensuring efficiency and broad applicability.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the given paper: 

The paper presents a comprehensive overview of techniques for aligning large language models (LLMs) with human values and expectations. The authors organize the alignment technologies into three main aspects - collecting high-quality alignment data, training methodologies to incorporate human preferences, and evaluation approaches. For data collection, they summarize methods leveraging existing benchmarks, human annotations, and strong LLMs to generate diverse alignment instructions. They outline popular training methods including supervised fine-tuning, online and offline human preference learning, and parameter-efficient tuning. For evaluation, they discuss specialized benchmarks to assess alignment across knowledge, reasoning, coding, and open-ended skills. They also cover emerging LLM-based evaluation protocols. Based on their synthesis of research advancements, the authors highlight promising future directions such as generating multilingual instructions, comparing training methods under constrained resources, and developing human-LLM hybrid evaluation frameworks. Overall, the survey offers an insightful overview of the multifaceted landscape of research towards aligning LLMs with human preferences.


## Summarize the paper in two paragraphs.

 Here is a two-paragraph summary of the paper:

The paper proposes a new two-stage Chinese pre-training solution to improve the ability of English-oriented LLaMA to understand Chinese inputs. In the first stage, they only train the input word embeddings while keeping other LLaMA parameters frozen. Specifically, they first add 20K Chinese words and phrases into LLaMA's vocabulary. Then they pre-train the input word embeddings on a large Chinese corpus using masked language modeling. This allows LLaMA to have better tokenization and representation for Chinese text. In the second stage, to reduce training costs, they apply LoRA to the input embeddings, self-attention heads, and feedforward network layers. The LoRA parameters are jointly trained with the input embeddings on Chinese data using masked language modeling. Experiments on Chinese datasets show this two-stage training strategy achieves better performance than directly fine-tuning all of LLaMA's parameters on Chinese data. It improves LLaMA's understanding of Chinese while maintaining its original capabilities on English.

In summary, the two-stage training leverages LLaMA's linguistic knowledge learned from English data and adapts it to Chinese via input word embedding pre-training and efficient LoRA tuning. It is a parameter-efficient way to transfer the ability of English LLMs to other languages with low resources.


## Summarize the main method used in the paper in one paragraph.

 Here is a one paragraph summary of the main method used in the given paper: 

The paper proposes a two-stage Chinese pre-training approach to improve LLaMA's ability to understand Chinese text. In the first stage, they only train the input word embeddings while keeping other LLaMA parameters frozen. Specifically, they first expand the LLaMA vocabulary with 20K additional Chinese words and phrases. Then they train the input word embeddings on large-scale Chinese corpus. This allows LLaMA to have better Chinese tokenization result and input representations. In the second stage, to reduce training costs, they apply LoRA to the input embeddings, self-attention heads and output heads in LLaMA. By only training these parameters with much lower ranks, they are able to efficiently adapt LLaMA to Chinese without sacrificing too much performance. Experiments show that this two-stage approach enables LLaMA to achieve strong performance on Chinese tasks while having efficient inference speed.


## What problem or question is the paper addressing?

 Based on my reading, this paper appears to be providing a comprehensive survey and overview of recent research efforts on aligning large language models (LLMs) to better conform to human preferences and expectations. 

The key problem being addressed is that while LLMs like GPT-3 demonstrate impressive fluency and coherence when generating text, they can sometimes produce outputs that deviate from human expectations or preferences. For example, they may misunderstand instructions, generate biased content, or invent incorrect "facts". 

Thus, a major research thrust has been on "alignment" - training and optimizing LLMs to better align with human values and perspectives. The authors note three main challenges in this alignment process:

(a) Collecting sufficient high-quality training data (instructions paired with human responses) can be costly and time-consuming. 

(b) The training procedures, like reinforcement learning from human feedback, can be unstable and resource-intensive. 

(c) Evaluation is difficult, as benchmarks may not fully reveal model capabilities.

To summarize, this survey aims to provide a comprehensive overview of techniques and research focused on the key problem of aligning LLMs with human preferences and expectations, in order to make their outputs more useful, ethical, and factual. The authors review methods for data collection, training strategies, and evaluation paradigms to shed light on this emerging research area.
