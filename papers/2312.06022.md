# [Exploiting Representation Bias for Data Distillation in Abstractive Text   Summarization](https://arxiv.org/abs/2312.06022)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Abstractive text summarization models rely on large datasets to learn representations between source text and target summaries. However, blindly increasing dataset size does not consistently improve performance.  
- Current models fail to capture diversity of input space, generating repetitive, templatic summaries. Skewed data representations causes models to ignore variable examples as noise.
- This representation bias in datasets inhibits model generalization across genres.

Methodology:
- Analyze training dynamics of various abstractive summarization models by clustering embedding & encoder vector spaces.
- Find embeddings show some variability but get mapped to saturated encoder space regardless of model type.
- Indicates models learn similar weights, outputting repetitive summaries despite differing inputs.  

Key Contributions:
- Quantify representation bias in datasets by studying how models map from input to encoder space.
- Filter redundant samples using hierarchical clustering to retain only critical data points.
- Show subsampled 10K instances can match performance of models trained on full dataset.
- Subsampling makes models more generalizable, easier to train and enhances qualitative metrics like faithfulness. 
- Benchmark against various datasets and model types like LSTM, Transformers.
- Provide insights into how bias inhibits performance and generalization.

The paper offers a novel analysis to identify core weaknesses in existing summarization techniques. It makes important contributions in devising data-efficient methods to reduce repetition and improve faithfulness without sacrificing performance.
