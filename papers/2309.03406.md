# [Distribution-Aware Prompt Tuning for Vision-Language Models](https://arxiv.org/abs/2309.03406)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central research question this paper aims to address is: How can we improve the performance of pre-trained vision-language models like CLIP on downstream tasks through effective prompt tuning, while maintaining their generalizability?The key points are:- Pre-trained VLMs like CLIP show impressive performance on downstream tasks through zero-shot transfer. Their performance can be further improved via prompt tuning. - Existing prompt tuning methods optimize trainable prompt vectors appended to the input while keeping the model fixed. This aligns the text and image latent spaces.- The authors hypothesize alignment becomes more effective when embeddings of each modality are "well-arranged" in their latent space. - They propose a new prompt tuning method called DAPT that is "distribution-aware" - it optimizes the distributions of text and image embeddings for better alignment.- DAPT applies inter-dispersion loss on text prompts to spread out text embeddings. It applies intra-dispersion loss on visual prompts to minimize variability of image embeddings per class.- Through experiments on few-shot learning and domain generalization tasks, they demonstrate DAPT significantly improves performance while maintaining generalizability.In summary, the key hypothesis is that optimizing the distributions of text and image embeddings will lead to better alignment and improved performance for prompt-tuned VLMs. DAPT is proposed to achieve this.


## What is the main contribution of this paper?

This paper proposes a distribution-aware prompt tuning method called DAPT for vision-language models. The key contributions are:1. It proposes to optimize the distribution of embeddings in each modality for better feature alignment between text and images. 2. It introduces two novel loss terms - inter-dispersion loss and intra-dispersion loss. The inter-dispersion loss is applied to text prompts to spread out text embeddings. The intra-dispersion loss is applied to visual prompts to minimize the variability of image embeddings of the same class.3. Extensive experiments show DAPT significantly improves performance on few-shot learning and domain generalization tasks. On 11 benchmark datasets, DAPT outperforms strong baselines like CoOp and VPT as well as zero-shot CLIP and linear probe CLIP.4. Analysis shows DAPT learns prompts that spread out text embeddings and compactly cluster image embeddings as intended.In summary, the main contribution is proposing a simple yet effective distribution-aware prompt tuning method that optimizes the latent spaces to achieve better alignment between modalities and improve generalization ability. The novel loss terms and experimental results demonstrate the effectiveness of optimizing prompt distributions.
