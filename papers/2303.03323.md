# [CleanCLIP: Mitigating Data Poisoning Attacks in Multimodal Contrastive   Learning](https://arxiv.org/abs/2303.03323)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading, the key research question this paper seeks to address is: How can we mitigate the impact of data poisoning (backdoor) attacks against multimodal contrastive learning models like CLIP? 

Specifically, the authors investigate methods to "erase" or reduce the effectiveness of backdoor triggers that have been injected into the pre-training data used for multimodal contrastive learning models like CLIP. Their core hypothesis is that "learning representations for each modality independently of the other could break the spurious correlation between the backdoor trigger and the target label."

In summary, this paper focuses on developing defenses against backdoor attacks for multimodal contrastive learning models, with the goal of removing the spurious associations that a backdoor attack introduces between triggers and target labels. The central hypothesis is that encouraging independent representation learning for each modality can help break these artificial associations.


## What is the main contribution of this paper?

 Based on my reading, the main contributions of this paper are:

1. It proposes CleanCLIP, a finetuning framework to mitigate backdoor attacks in multimodal contrastive learning models like CLIP. CleanCLIP uses a combination of multimodal contrastive loss and self-supervised learning objectives during finetuning to break the spurious associations between the backdoor trigger and target label learned during pretraining. 

2. It demonstrates that CleanCLIP significantly reduces the attack success rate of various backdoor attacks like BadNet, Blended, WaNet, and label-consistent attacks without impacting model performance on benign examples.

3. It shows that supervised finetuning of just the CLIP vision encoder on clean labeled image data can also effectively remove backdoors by adapting the model to the target data distribution.

4. It provides an analysis of factors influencing the effectiveness of CleanCLIP such as strength of self-supervision, size of pretraining and finetuning data, number of backdoor examples, etc.

5. To the best of the authors' knowledge, this is the first work to propose and evaluate defenses against backdoor attacks on multimodal contrastive learning models.

In summary, the main contribution is a finetuning-based framework CleanCLIP to mitigate backdoor attacks on multimodal contrastive learning models, which is shown to be effective against various types of attacks without degrading performance on clean examples. The analyses provide insights into designing defenses for this challenging problem.
