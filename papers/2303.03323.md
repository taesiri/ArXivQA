# [CleanCLIP: Mitigating Data Poisoning Attacks in Multimodal Contrastive   Learning](https://arxiv.org/abs/2303.03323)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading, the key research question this paper seeks to address is: How can we mitigate the impact of data poisoning (backdoor) attacks against multimodal contrastive learning models like CLIP? Specifically, the authors investigate methods to "erase" or reduce the effectiveness of backdoor triggers that have been injected into the pre-training data used for multimodal contrastive learning models like CLIP. Their core hypothesis is that "learning representations for each modality independently of the other could break the spurious correlation between the backdoor trigger and the target label."In summary, this paper focuses on developing defenses against backdoor attacks for multimodal contrastive learning models, with the goal of removing the spurious associations that a backdoor attack introduces between triggers and target labels. The central hypothesis is that encouraging independent representation learning for each modality can help break these artificial associations.
