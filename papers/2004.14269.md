# [Training Curricula for Open Domain Answer Re-Ranking](https://arxiv.org/abs/2004.14269)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the main research questions/hypotheses appear to be:1) Are the proposed training curricula effective for training neural rankers for answer ranking? (Evaluated in Sections 4.1-4.3)2) Under which conditions is each curriculum more effective (e.g. amount of training data, type of neural ranker, etc.)? (Evaluated across Sections 4.1-4.3) 3) Is it important to shift to difficult samples during training, or can a ranker be successfully trained by focusing only on easy samples? (Evaluated in Section 4.4)4) Is focusing on the easy samples first more beneficial than focusing on the hardest samples first during training? (Evaluated in Section 4.5)The overall goal seems to be evaluating the effectiveness of the proposed training curricula, which weight easy/difficult samples differently during training, for improving neural rankers on answer ranking tasks. The main hypotheses appear to be that the curricula will improve performance across tasks/models (RQ1), certain curricula will work better in certain scenarios (RQ2), it's important to shift to difficult samples later in training (RQ3), and focusing on easy samples first is better than hardest first (RQ4). The experiments aim to validate these hypotheses.


## What is the main contribution of this paper?

The main contribution of this paper is proposing a curriculum learning approach for training neural ranking models for open-domain answer re-ranking. The key ideas are:- They propose using curriculum learning to assign different weights to training samples based on their estimated "difficulty". Easy samples get higher weight early in training, while difficult samples get lower weight.- They propose three heuristics to estimate sample difficulty based on the ranking and score of an unsupervised first-stage ranker: reciprocal rank, normalized score, and kernel density estimation (KDE).- They show their curricula approach is effective across three neural ranking models (BERT, ConvKNRM), three datasets (TREC DL, CAR, ANTIQUE), and both pairwise and pointwise losses.- Their approach achieves gains in ranking accuracy compared to no curriculum, and reaches performance comparable to larger BERT models and more complex training techniques.- They analyze the impact of curriculum hyperparameters like end epoch and show the importance of eventually giving equal weight to all samples.In summary, the main contribution is demonstrating an effective way to apply curriculum learning to improve training of neural rankers for answer re-ranking, using simple heuristics based on the first-stage ranker.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper proposes using curriculum learning to train neural ranking models for answer retrieval by starting with easy training samples and progressively incorporating more difficult samples, which is shown to improve ranking performance on several datasets.


## How does this paper compare to other research in the same field?

Here are a few key points in comparing this paper to other research on curriculum learning for neural ranking models:- Focus on Open Domain Answer Ranking: This paper focuses specifically on applying curriculum learning to improve neural ranking models for open domain answer ranking across multiple datasets (TREC DL, CAR, ANTIQUE). Many prior works applying curriculum learning focus on other tasks like closed-domain QA, document ranking, etc. - Simple and Effective Heuristics: The heuristics proposed in this paper for estimating training sample difficulty are quite simple, relying only on the rank/score from an unsupervised first stage ranker like BM25. Despite the simplicity, the heuristics are shown to be highly effective across models and datasets. Other curriculum learning works often use more complex heuristics tailored to their domain.- Evaluated on Multiple Models: The curriculum learning approach is evaluated with different neural ranking architectures like BERT and ConvKNRM. Showing consistent improvements across models strengthens the validity of the approach. Some prior work focuses evaluation on just one model architecture.- Compared to Anti-Curriculum: An analysis is provided showing that focusing on easy samples first is better than an anti-curriculum focused on hard samples first. This helps justify the curriculum design choices.- Thorough Evaluation: A comprehensive evaluation is provided analyzing 3 datasets, 2 neural rankers, 2 loss functions, and 3 difficulty heuristics. Many dimensions are explored to understand when the approach is most effective. Some related works provide more limited analysis.Overall, the simplicity and general effectiveness of the heuristics combined with the thorough evaluation across multiple conditions helps strengthen the validity and usefulness of the curriculum learning approach proposed compared to related literature. The focus on open domain answer ranking is also notable as a contribution.
