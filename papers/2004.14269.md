# [Training Curricula for Open Domain Answer Re-Ranking](https://arxiv.org/abs/2004.14269)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the main research questions/hypotheses appear to be:1) Are the proposed training curricula effective for training neural rankers for answer ranking? (Evaluated in Sections 4.1-4.3)2) Under which conditions is each curriculum more effective (e.g. amount of training data, type of neural ranker, etc.)? (Evaluated across Sections 4.1-4.3) 3) Is it important to shift to difficult samples during training, or can a ranker be successfully trained by focusing only on easy samples? (Evaluated in Section 4.4)4) Is focusing on the easy samples first more beneficial than focusing on the hardest samples first during training? (Evaluated in Section 4.5)The overall goal seems to be evaluating the effectiveness of the proposed training curricula, which weight easy/difficult samples differently during training, for improving neural rankers on answer ranking tasks. The main hypotheses appear to be that the curricula will improve performance across tasks/models (RQ1), certain curricula will work better in certain scenarios (RQ2), it's important to shift to difficult samples later in training (RQ3), and focusing on easy samples first is better than hardest first (RQ4). The experiments aim to validate these hypotheses.


## What is the main contribution of this paper?

The main contribution of this paper is proposing a curriculum learning approach for training neural ranking models for open-domain answer re-ranking. The key ideas are:- They propose using curriculum learning to assign different weights to training samples based on their estimated "difficulty". Easy samples get higher weight early in training, while difficult samples get lower weight.- They propose three heuristics to estimate sample difficulty based on the ranking and score of an unsupervised first-stage ranker: reciprocal rank, normalized score, and kernel density estimation (KDE).- They show their curricula approach is effective across three neural ranking models (BERT, ConvKNRM), three datasets (TREC DL, CAR, ANTIQUE), and both pairwise and pointwise losses.- Their approach achieves gains in ranking accuracy compared to no curriculum, and reaches performance comparable to larger BERT models and more complex training techniques.- They analyze the impact of curriculum hyperparameters like end epoch and show the importance of eventually giving equal weight to all samples.In summary, the main contribution is demonstrating an effective way to apply curriculum learning to improve training of neural rankers for answer re-ranking, using simple heuristics based on the first-stage ranker.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper proposes using curriculum learning to train neural ranking models for answer retrieval by starting with easy training samples and progressively incorporating more difficult samples, which is shown to improve ranking performance on several datasets.


## How does this paper compare to other research in the same field?

Here are a few key points in comparing this paper to other research on curriculum learning for neural ranking models:- Focus on Open Domain Answer Ranking: This paper focuses specifically on applying curriculum learning to improve neural ranking models for open domain answer ranking across multiple datasets (TREC DL, CAR, ANTIQUE). Many prior works applying curriculum learning focus on other tasks like closed-domain QA, document ranking, etc. - Simple and Effective Heuristics: The heuristics proposed in this paper for estimating training sample difficulty are quite simple, relying only on the rank/score from an unsupervised first stage ranker like BM25. Despite the simplicity, the heuristics are shown to be highly effective across models and datasets. Other curriculum learning works often use more complex heuristics tailored to their domain.- Evaluated on Multiple Models: The curriculum learning approach is evaluated with different neural ranking architectures like BERT and ConvKNRM. Showing consistent improvements across models strengthens the validity of the approach. Some prior work focuses evaluation on just one model architecture.- Compared to Anti-Curriculum: An analysis is provided showing that focusing on easy samples first is better than an anti-curriculum focused on hard samples first. This helps justify the curriculum design choices.- Thorough Evaluation: A comprehensive evaluation is provided analyzing 3 datasets, 2 neural rankers, 2 loss functions, and 3 difficulty heuristics. Many dimensions are explored to understand when the approach is most effective. Some related works provide more limited analysis.Overall, the simplicity and general effectiveness of the heuristics combined with the thorough evaluation across multiple conditions helps strengthen the validity and usefulness of the curriculum learning approach proposed compared to related literature. The focus on open domain answer ranking is also notable as a contribution.


## What future research directions do the authors suggest?

Based on my reading of the paper, some of the main future research directions suggested by the authors are:- Exploring alternative difficulty degradation functions or applying the curriculum learning approach to other models/tasks, such as domain adaptation. The authors suggest their difficulty heuristics could be a good starting point for more intelligent sampling strategies as well.- Combining the weighting strategies proposed in the paper with more advanced sampling techniques for selecting relevant and non-relevant training pairs. The weighting could help ensure easy samples are still ranked effectively.- Using self-paced learning techniques that allow the model to learn which training sample characteristics make a sample easy or difficult, instead of relying solely on heuristics designed by humans.- Applying the ideas to other tasks and domains beyond answer ranking, such as initial retrieval, domain-specific ranking, etc. The authors suggest their approach may generalize.- Evaluating the impact on recall-oriented metrics and diversifying the test collections used. The current work focused on precision-oriented ranking metrics.- Exploring the characteristics of situations where constantly applying the difficulty weights is beneficial, as the authors found for the TREC CAR dataset. This could help adapt the techniques better.In summary, the main suggestions are around exploring extensions to other models/tasks, combining it with more advanced sampling techniques, using self-paced learning, and evaluating in new settings with additional metrics and test collections. The authors seem to suggest their curriculum learning approach is broadly applicable.


## Summarize the paper in one paragraph.

Here is a one paragraph summary of the paper:This paper proposes using curriculum learning to train neural rankers for open-domain answer re-ranking. Curriculum learning starts training on easier examples before gradually incorporating more difficult ones. The authors propose three heuristics to estimate sample difficulty for answer ranking based on the rank and score of an initial retrieval stage (e.g. BM25). Easy samples are those where the initial ranker correctly ranks a relevant (or nonrelevant) passage highly (or lowly). Difficult samples are those where the initial ranker fails to do so. The heuristics assign higher loss weights to easy samples early in training, and weights converge to be equal later in training. Experiments on three answer ranking datasets with two neural rankers (BERT and ConvKNRM) show this curriculum approach significantly improves ranking effectiveness over no curriculum. The authors find it's important to converge weights to treat all samples equally, and focusing on easy samples first is better than hard samples first. The curricula allow models to approach state-of-the-art performance at lower cost.
