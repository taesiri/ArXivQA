# [Long Expressive Memory for Sequence Modeling](https://arxiv.org/abs/2110.04744v2)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question it addresses is: 

How can we design a gradient-based recurrent neural network model that can learn sequential tasks with very long-term dependencies, while still being sufficiently expressive to learn complex input-output mappings?

The key challenges outlined are:

- Recurrent neural networks struggle to learn sequences with long-term dependencies due to the exploding and vanishing gradients problem.

- Existing methods to mitigate exploding/vanishing gradients like LSTM/GRU can still struggle with very long sequences. 

- Methods that constrain weights to prevent exploding gradients may limit model expressivity.

To address this, the paper proposes a new model called Long Expressive Memory (LEM) with the following goals:

- Mitigate exploding and vanishing gradients to handle long sequences.

- Be expressive enough to learn complex input-output mappings.

- Be efficient and achieve strong empirical performance.

The central hypothesis is that LEM can achieve all of these desirable properties through its specific architecture based on discretizing a system of multiscale ODEs. The paper provides theory and experiments to evaluate if LEM fulfills this hypothesis.

In summary, the key research question is how to design a gradient-based recurrent model that can handle very long sequences while retaining expressiveness, which LEM aims to address through its multiscale architecture.


## What is the main contribution of this paper?

 Based on my reading of the paper, the main contributions appear to be:

- Proposing a novel recurrent neural network architecture called Long Expressive Memory (LEM) for learning long-term sequential dependencies in data. 

- Deriving LEM by discretizing a system of multiscale ordinary differential equations (ODEs). This provides LEM with certain desirable theoretical properties.

- Proving rigorous bounds on the hidden state gradients in LEM to show it mitigates the problem of exploding and vanishing gradients, a key challenge in training recurrent models.

- Proving that LEM can approximate a large class of multiscale dynamical systems to high accuracy, demonstrating its expressive power. 

- Providing extensive empirical evaluations on tasks ranging from adding problems to language modeling which show LEM outperforms or matches state-of-the-art recurrent models like LSTMs and GRUs.

So in summary, the main contribution seems to be proposing the LEM architecture, establishing its useful theoretical properties, and demonstrating its effectiveness empirically on a diverse set of sequential learning tasks. The multiscale ODE derivation of LEM and the proofs of its gradient stability and expressivity appear to be key theoretical innovations.
