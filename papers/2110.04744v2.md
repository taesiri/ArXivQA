# [Long Expressive Memory for Sequence Modeling](https://arxiv.org/abs/2110.04744v2)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question it addresses is: How can we design a gradient-based recurrent neural network model that can learn sequential tasks with very long-term dependencies, while still being sufficiently expressive to learn complex input-output mappings?The key challenges outlined are:- Recurrent neural networks struggle to learn sequences with long-term dependencies due to the exploding and vanishing gradients problem.- Existing methods to mitigate exploding/vanishing gradients like LSTM/GRU can still struggle with very long sequences. - Methods that constrain weights to prevent exploding gradients may limit model expressivity.To address this, the paper proposes a new model called Long Expressive Memory (LEM) with the following goals:- Mitigate exploding and vanishing gradients to handle long sequences.- Be expressive enough to learn complex input-output mappings.- Be efficient and achieve strong empirical performance.The central hypothesis is that LEM can achieve all of these desirable properties through its specific architecture based on discretizing a system of multiscale ODEs. The paper provides theory and experiments to evaluate if LEM fulfills this hypothesis.In summary, the key research question is how to design a gradient-based recurrent model that can handle very long sequences while retaining expressiveness, which LEM aims to address through its multiscale architecture.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contributions appear to be:- Proposing a novel recurrent neural network architecture called Long Expressive Memory (LEM) for learning long-term sequential dependencies in data. - Deriving LEM by discretizing a system of multiscale ordinary differential equations (ODEs). This provides LEM with certain desirable theoretical properties.- Proving rigorous bounds on the hidden state gradients in LEM to show it mitigates the problem of exploding and vanishing gradients, a key challenge in training recurrent models.- Proving that LEM can approximate a large class of multiscale dynamical systems to high accuracy, demonstrating its expressive power. - Providing extensive empirical evaluations on tasks ranging from adding problems to language modeling which show LEM outperforms or matches state-of-the-art recurrent models like LSTMs and GRUs.So in summary, the main contribution seems to be proposing the LEM architecture, establishing its useful theoretical properties, and demonstrating its effectiveness empirically on a diverse set of sequential learning tasks. The multiscale ODE derivation of LEM and the proofs of its gradient stability and expressivity appear to be key theoretical innovations.
