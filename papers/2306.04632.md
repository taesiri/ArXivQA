# [Designing a Better Asymmetric VQGAN for StableDiffusion](https://arxiv.org/abs/2306.04632)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question it addresses is: How can we design a better asymmetric VQGAN architecture to improve StableDiffusion for stable and high-quality image editing?Specifically, the authors identify distortion artifacts as a key issue with using the default symmetric VQGAN in StableDiffusion for image editing tasks like inpainting and local editing. They hypothesize that an asymmetric VQGAN design with a stronger decoder can help address these issues by better preserving non-edited image regions while still recovering details from the quantized latent space. Their proposed approaches are:1) Adding a conditional branch in the decoder to incorporate task-specific priors like non-edited regions, rather than just encoded latent vectors. 2) Using a larger, deeper decoder compared to the encoder to enhance detail recovery while only slightly increasing inference cost.The overall goal is to design a better VQGAN that can improve results for editing tasks that need to preserve non-edited regions, while maintaining performance on text-to-image generation. The asymmetric architecture is proposed as a way to achieve this.


## What is the main contribution of this paper?

The main contribution of this paper is proposing a new asymmetric VQGAN architecture for StableDiffusion that has two key designs:1) A conditional decoder that incorporates an additional input branch to accept task-specific priors like the non-edited regions of an image for inpainting. This helps preserve details in the non-edited regions.2) Using a larger decoder compared to the encoder, which helps recover more details from the quantized output of the encoder and compensate for the information loss during encoding. The asymmetric design with a heavier decoder allows it to better preserve local details in non-edited image regions for tasks like inpainting and image editing, while the conditional decoder integrates task-specific information to further improve detail preservation. Together, these designs improve results on inpainting, image editing, and text-to-image generation tasks compared to the standard symmetric VQGAN in StableDiffusion.The training and inference costs are still low since only the decoder is modified. Overall, the proposed asymmetric VQGAN architecture demonstrates improved performance on image editing and generation tasks over the vanilla VQGAN used in StableDiffusion.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence TL;DR summary of the paper:The paper proposes a new asymmetric VQGAN architecture with a larger conditional decoder to improve StableDiffusion's performance on image editing tasks while maintaining its text-to-image capabilities.
