# [Large Language Models As Faithful Explainers](https://arxiv.org/abs/2402.04678)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
Large language models (LLMs) like GPT-3 have shown impressive capabilities in complex tasks by utilizing their rich knowledge and reasoning skills. However, the complexity also makes them opaque black-box systems that are difficult to explain. Recent works have proposed self-explaining LLMs' predictions through natural language explanations or chain-of-thought reasoning in a single forward pass. However, such automatically generated explanations often lack faithfulness, i.e. they may not accurately reflect the LLM's actual reasoning process. 

Proposed Solution:
This paper proposes a generative explanation framework called \Algnameabbr{} that aims to produce more faithful natural language explanations for LLMs. It does this through:

1. A Fidelity Evaluator that quantitatively scores the faithfulness of a natural language explanation by checking if removing key semantics from it impacts the LLM's original prediction. 

2. An iterative optimization process where \Algnameabbr{} generates candidate explanations, evaluates their fidelity, and uses good ones as examples to guide itself to produce better explanations over multiple rounds.

3. Optimization of the trigger prompt given to \Algnameabbr{} to elicit more faithful explanations across data instances.

Key Contributions:

- Proposes \Algnameabbr{}, a general framework to produce faithful natural language explanations for black-box LLMs using their own knowledge.

- Introduces a Fidelity Evaluator to quantitatively score explanation faithfulness without ground truth. 

- Shows optimized explanations from \Algnameabbr{} have significantly higher fidelity and similarity to ground truth compared to single-pass LLM explanations.

- Demonstrates optimized trigger prompts transfer well, improving explanation quality on unseen datasets.

In summary, this paper presents a novel framework to iteratively enhance and evaluate the faithfulness of natural language explanations for complex LLMs, through quantitative scoring and LLM-based optimization. The improved explanation quality and prompt transferability are promising for model transparency.
