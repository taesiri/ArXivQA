# [Chain-of-Discussion: A Multi-Model Framework for Complex Evidence-Based   Question Answering](https://arxiv.org/abs/2402.16313)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem: 
- Open-ended question answering is challenging for large language models (LLMs) as it requires finding appropriate evidence, reasoned analysis, and discussion of relevant scenarios beyond the literal question.  
- Issues include unreliable evidence selection, incomplete analysis, incorrect reasoning paths leading to problematic or misleading answers.

Proposed Solution:
- A new Chain-of-Discussion (CoD) framework that allows multiple LLMs to interact by summarizing, criticizing and revising each other's outputs.  
- Aims to provide more correct and comprehensive answers by leveraging synergies between different LLMs. 

Key Contributions:
- Collected a high quality dataset of 200 Chinese legal consultation questions with annotated evidence and answers.
- Proposed the CoD framework for multi-LLM interaction via summarize-criticize-revise steps to reduce hallucination and improve evidence selection.  
- Showed through experiments that the framework helps individual LLMs benefit from each other and enhances the quality of their responses in terms of correctness and comprehensiveness.
- Released the new legal consultation dataset and code to support further research.

The paper addresses an important challenge in complex open-ended QA where single LLMs struggle. The proposed CoD framework is novel in enabling synergistic multi-LLM discussions to overcome individual model limitations. Experiments demonstrate clear improvements in answer quality across correctness and comprehensiveness dimensions.


## Summarize the paper in one sentence.

 The paper proposes a novel Chain-of-Discussion framework that leverages interactions among multiple language models to generate more accurate and comprehensive answers for complex, evidence-based question answering.


## What is the main contribution of this paper?

 The main contributions of this paper are:

1. The authors collect a high-quality complex evidence-based question answering (CEBQA) dataset consisting of 200 legal consultation questions in Chinese with carefully annotated evidence and answers.

2. They propose a novel Chain-of-Discussion (CoD) framework that involves multiple language models in discussions to generate more accurate and helpful responses. The framework has steps to summarize, criticize, and revise the outputs of the different models. 

3. Experiments on the legal consultation dataset show that the CoD framework can effectively improve the performance of smaller, open-source language models by enabling them to benefit from discussions with each other. This leads to more correct and comprehensive answers compared to the language models working individually.

So in summary, the key innovations are the new dataset, the CoD multi-model interaction framework, and demonstrations that this framework can enhance smaller open-source LLMs.


## What are the keywords or key terms associated with this paper?

 Based on my review of the paper, some of the key terms and keywords associated with it include:

- Chain-of-Discussion (CoD) framework: The novel reasoning framework proposed in the paper that involves multiple language models discussing, criticizing, and revising each other's outputs to generate more accurate and comprehensive responses. 

- Complex evidence-based question answering (CEBQA): The open-ended QA task focused on in the paper, which requires models to find appropriate evidence to generate well-reasoned and helpful long-form answers.

- Legal consultation: A typical CEBQA task used as an example application in the paper. Requires generating detailed responses to users' legal questions based on evidence from law articles and interpretations.  

- Correctness and comprehensiveness: Two key desired attributes of generated responses emphasized in the paper - being based on relevant evidence and avoiding hallucinations, while also discussing potential relevant scenarios.

- Summarize-criticize-revise: The three key steps in the proposed CoD framework that aim to improve correctness and comprehensiveness by having multiple models interact.

- Open-source large language models: The 7B parameter LLMs studied in the paper like Baichuan2-7B and Deepseek-7B. Show individual deficiencies but can be improved via CoD.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in the paper:

1. How does the Chain-of-Discussion (CoD) framework leverage the synergy among multiple language models to generate more accurate and comprehensive responses? What are the key steps it takes to achieve this?

2. What motivations underlie the proposal of using multiple language models in the CoD framework rather than relying on a single large model? What deficiencies is it trying to address?  

3. What are the main differences between the summarize and criticize-revise components in the CoD framework? How do they contribute to improving answer correctness and comprehensiveness respectively?

4. What criteria and instructions are given to the target language model when summarizing the question analyses from other models? How may model limitations affect this summarization process?

5. In the criticize-revise step, what factors determine whether the evidence analysis needs to be revised? How is the revising threshold Î´ set and does its value affect model performance?

6. How exactly is the Chain-of-Discussion framework designed to retain helpful evidence pieces for potential scenario discussions while filtering out irrelevant ones? What makes this a challenging issue?

7. What quantitative metrics are used to evaluate the correctness and comprehensiveness of the generated responses? How do they capture the usage of necessary and optional evidence articles?  

8. How do the GPT-4 based evaluation and evidence-centric metrics results demonstrate the effectiveness of the CoD framework? What improvements do they show over the baseline?

9. What analysis is provided on why multi-model interaction on a single stage may fail to enhance some language models? How may model limitations in following instructions affect framework performance?

10. How suitable is the legal consultation dataset collected in this work for evaluating the goals of complex, open-ended question answering tasks requiring correctness and comprehensiveness?
