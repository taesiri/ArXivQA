# [Three Towers: Flexible Contrastive Learning with Pretrained Image Models](https://arxiv.org/abs/2305.16999)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be:How can we effectively transfer knowledge from pretrained image classification models into contrastive vision-language models like CLIP?The authors propose a method called "Three Towers" (3T) to address this question. The key ideas are:- Instead of directly using a frozen pretrained model as the image tower like in LiT, they add a separate "third tower" that contains the frozen pretrained embeddings. - The main image and text towers are still trained from scratch with contrastive learning.- Additional contrastive losses encourage alignment between the main towers and the third tower.- After training, only the main towers are used, so there is no inference cost increase.The hypothesis is that this allows the image tower to benefit from both the pretrained model and contrastive learning, while still training the text tower from scratch. The experiments aim to evaluate whether 3T improves over the LiT method and standard contrastive learning baselines.In summary, the central research question is how to effectively incorporate pretrained image models into contrastive vision-language learning. The 3T method is proposed as a way to do this that is more flexible than directly using the pretrained model like LiT.


## What is the main contribution of this paper?

This paper introduces a new method called "Three Towers" (3T) for improving contrastive learning of vision-language models by incorporating knowledge from pretrained image classifiers. The key contributions are:- Proposes the 3T method which uses a third "tower" containing frozen embeddings from a pretrained image classifier, in addition to the standard image and text towers. The main towers are aligned to this third tower via contrastive losses.- Shows that 3T consistently improves over baseline contrastive learning without pretraining (e.g. CLIP) and over the LiT method which directly replaces the image tower with a frozen pretrained model.- Demonstrates that 3T is more robust than LiT - it does not suffer from the same performance degradation when the pretrained model has deficiencies or is mismatched.- Finds that 3T benefits more from scaling model size or training data than LiT.- Introduces a convex combination of the image towers in 3T which can interpolate between 3T and LiT performance, often outperforming both.So in summary, the key contribution is proposing the 3T method for flexibly and robustly incorporating pretrained image models into contrastive learning of vision-language representations. This consistently improves over baseline contrastive learning and the prior LiT approach.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper: The paper proposes a new method called Three Towers (3T) for improving contrastive learning of vision-language models by incorporating knowledge from pretrained image classifiers, using an additional third tower with frozen pretrained embeddings that the main image and text towers are aligned to.
