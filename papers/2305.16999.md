# [Three Towers: Flexible Contrastive Learning with Pretrained Image Models](https://arxiv.org/abs/2305.16999)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be:How can we effectively transfer knowledge from pretrained image classification models into contrastive vision-language models like CLIP?The authors propose a method called "Three Towers" (3T) to address this question. The key ideas are:- Instead of directly using a frozen pretrained model as the image tower like in LiT, they add a separate "third tower" that contains the frozen pretrained embeddings. - The main image and text towers are still trained from scratch with contrastive learning.- Additional contrastive losses encourage alignment between the main towers and the third tower.- After training, only the main towers are used, so there is no inference cost increase.The hypothesis is that this allows the image tower to benefit from both the pretrained model and contrastive learning, while still training the text tower from scratch. The experiments aim to evaluate whether 3T improves over the LiT method and standard contrastive learning baselines.In summary, the central research question is how to effectively incorporate pretrained image models into contrastive vision-language learning. The 3T method is proposed as a way to do this that is more flexible than directly using the pretrained model like LiT.


## What is the main contribution of this paper?

This paper introduces a new method called "Three Towers" (3T) for improving contrastive learning of vision-language models by incorporating knowledge from pretrained image classifiers. The key contributions are:- Proposes the 3T method which uses a third "tower" containing frozen embeddings from a pretrained image classifier, in addition to the standard image and text towers. The main towers are aligned to this third tower via contrastive losses.- Shows that 3T consistently improves over baseline contrastive learning without pretraining (e.g. CLIP) and over the LiT method which directly replaces the image tower with a frozen pretrained model.- Demonstrates that 3T is more robust than LiT - it does not suffer from the same performance degradation when the pretrained model has deficiencies or is mismatched.- Finds that 3T benefits more from scaling model size or training data than LiT.- Introduces a convex combination of the image towers in 3T which can interpolate between 3T and LiT performance, often outperforming both.So in summary, the key contribution is proposing the 3T method for flexibly and robustly incorporating pretrained image models into contrastive learning of vision-language representations. This consistently improves over baseline contrastive learning and the prior LiT approach.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper: The paper proposes a new method called Three Towers (3T) for improving contrastive learning of vision-language models by incorporating knowledge from pretrained image classifiers, using an additional third tower with frozen pretrained embeddings that the main image and text towers are aligned to.


## How does this paper compare to other research in the same field?

Here are a few key points about how this paper compares to other research on contrastive learning of vision-language models:- It builds directly on recent work like CLIP, ALIGN, and LiT that have shown impressive results by pretraining vision-language models on large datasets of image-text pairs. The novelty is in proposing a new method, Three Towers (3T), for incorporating pretrained image classifiers.- Most prior work trains vision-language models from scratch. LiT showed benefits from using a pretrained image classifier, but by completely freezing the image embeddings. 3T aims to get the benefits of pretraining while still allowing the image embeddings to be updated during contrastive learning.- Compared to LiT, a key advantage of 3T is expected to be more robust performance when the pretrained model has deficiencies on the downstream tasks. The experiments bear this out, showing 3T outperforms LiT when using Places365 instead of ImageNet for pretraining.- For retrieval tasks, 3T consistently outperforms both LiT and the baseline across settings. For image classification, LiT does better on average with certain pretrained models, but 3T still improves over the baseline and seems more robust.- Scaling experiments suggest 3T gets more gains from increases in model size or training data than LiT. This highlights the flexibility benefit of 3T.- The convex combination experiments show 3T and LiT predictions can be smoothly interpolated, and combined predictions can even exceed both, suggesting room for further improvements.Overall, 3T seems to offer a simple but more flexible approach compared to LiT for incorporating pretrained image classifiers into contrastive learning. The robustness benefits are notable, and further exploration of model ensembling ideas like the convex combination seems promising.


## What future research directions do the authors suggest?

Based on my reading of the paper, some key future research directions suggested by the authors are:- Further exploring the convex combination of the main image tower and third tower embeddings (Figure 5). The authors found this post-hoc combination could often outperform using either tower alone, so they suggest further study could be promising.- Understanding the differences between the embeddings learned by the three towers in 3T, and seeing if these can be combined or leveraged to further improve performance. - Applying 3T-like ideas to other contrastive learning methods like CoCa to incorporate pretrained models.- Using 3T for model distillation, to transfer knowledge from very large pretrained models into smaller ones. - Extending 3T to incorporate multiple pretrained models, potentially from diverse modalities.- Studying if 3T-like objectives could help with scaling contrastive learning models to even larger sizes. The authors suggest the additional terms may have regularization benefits.- Evaluating if 3T can match state-of-the-art retrieval methods like CoCa given the same training compute budget.In summary, the main directions are exploring the convex combination idea further, understanding differences between the 3T tower embeddings, applying 3T more broadly, and scaling 3T to even larger models. The authors propose several interesting ways 3T could potentially be extended in future work.


## Summarize the paper in one paragraph.

Here is a one paragraph summary of the paper:The paper introduces Three Towers (3T), a method to improve contrastive learning of vision-language models by incorporating knowledge from pretrained image classifiers. Instead of directly using a frozen pretrained model as the image tower like in LiT, 3T adds a third tower containing the pretrained embeddings. The main image and text towers are trained from scratch but encouraged to align with the third tower through extra contrastive loss terms. This allows the image tower to benefit from both the pretrained model and contrastive training. Empirically, 3T consistently improves over LiT and standard contrastive learning for retrieval tasks. For classification, 3T outperforms LiT when using ImageNet-21k pretrained models, and while underperforming for JFT pretraining, 3T still reliably improves over the baseline. The increased flexibility and robustness of 3T make it an attractive method for incorporating pretrained models into contrastive learning.


## Summarize the paper in two paragraphs.

Here is a two paragraph summary of the paper:This paper introduces Three Towers (3T), a new method for learning aligned image and text representations through contrastive learning. Recent work has shown benefits from incorporating knowledge from pretrained image classifiers into contrastive learning models like CLIP, for example by directly using a frozen pretrained model as the image tower. However, completely freezing the image tower prevents it from improving through contrastive learning. 3T proposes a more flexible approach - in addition to the standard image and text towers trained from scratch, a third tower with fixed pretrained embeddings is introduced. Extra contrastive losses encourage alignment between this third tower and the main image/text towers. Empirically, 3T consistently improves over both the standard contrastive learning baseline and the frozen image tower approach for retrieval tasks. For image classification, it reliably outperforms the baseline and is competitive with the frozen approach, sometimes outperforming when the pretrained model is not perfectly suited to the downstream task. Compared to completely freezing the image tower, 3T appears more robust and benefits more from increases in model scale or training data. The simplicity and effectiveness of 3T suggests it should be a strong candidate for incorporating pretrained knowledge into future contrastive learning research and applications.


## Summarize the main method used in the paper in one paragraph.

Here is a one paragraph summary of the main method used in the paper:The paper introduces Three Towers (3T), a flexible approach to improve contrastive learning of vision-language models by incorporating knowledge from pretrained image classifiers. Instead of directly using a frozen pretrained model as the image tower like in LiT, 3T adds a third tower containing the pretrained embeddings. The main image and text towers are still trained from scratch using the standard contrastive loss, but additional losses encourage alignment of the main towers with the third tower. After training, only the main towers are used, so 3T does not add inference cost. This allows the image tower to benefit from both the pretrained model and contrastive training, while still maintaining the flexibility of the standard contrastive setup. Empirically, 3T outperforms both a from-scratch baseline and LiT on retrieval tasks, while also showing increased robustness and improved scaling behavior compared to LiT for image classification tasks.
