# [Three Towers: Flexible Contrastive Learning with Pretrained Image Models](https://arxiv.org/abs/2305.16999)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be:How can we effectively transfer knowledge from pretrained image classification models into contrastive vision-language models like CLIP?The authors propose a method called "Three Towers" (3T) to address this question. The key ideas are:- Instead of directly using a frozen pretrained model as the image tower like in LiT, they add a separate "third tower" that contains the frozen pretrained embeddings. - The main image and text towers are still trained from scratch with contrastive learning.- Additional contrastive losses encourage alignment between the main towers and the third tower.- After training, only the main towers are used, so there is no inference cost increase.The hypothesis is that this allows the image tower to benefit from both the pretrained model and contrastive learning, while still training the text tower from scratch. The experiments aim to evaluate whether 3T improves over the LiT method and standard contrastive learning baselines.In summary, the central research question is how to effectively incorporate pretrained image models into contrastive vision-language learning. The 3T method is proposed as a way to do this that is more flexible than directly using the pretrained model like LiT.
