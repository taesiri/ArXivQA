# [FRL: Federated Rank Learning](https://arxiv.org/abs/2110.04350v3)

## What is the central research question or hypothesis that this paper addresses?

 Based on my understanding, the key research question this paper tries to address is: 

How can we design a federated learning algorithm that is robust to poisoning attacks and communication-efficient?

Specifically, the paper proposes a new federated learning approach called Federated Rank Learning (FRL) that aims to achieve the following goals:

1) Robustness against poisoning attacks: The paper argues that existing FL algorithms are vulnerable to poisoning attacks because clients can send arbitrary model updates, allowing malicious clients a large space to craft malicious updates. To address this, FRL restricts the space of client updates to rankings of model parameters rather than the parameter values themselves. This discrete and limited space of rankings makes it harder for attackers to manipulate the learning.

2) Communication efficiency: By only sharing rankings instead of full model parameters, FRL aims to reduce the communication costs compared to traditional FL algorithms like FedAvg.

The key hypothesis is that using a ranking-based approach can simultaneously improve robustness and efficiency compared to existing FL algorithms. The paper evaluates this through theoretical analysis of the robustness properties and empirical experiments on benchmark datasets.

In summary, the central research question is how to design an FL system that is both robust and communication-efficient, which FRL aims to achieve through the use of federated ranking learning. The paper empirically validates whether this ranking-based approach can outperform current state-of-the-art on these two metrics.


## What is the main contribution of this paper?

 Based on my understanding, the main contributions of this paper are:

1. It proposes a new federated learning framework called Federated Rank Learning (FRL) that uses rankings of neural network edges instead of weight updates to train a global model. This reduces the space of possible client updates from a continuous space to a discrete space, making FRL more robust to poisoning attacks. 

2. FRL uses a voting-based aggregation method on the client rankings which further improves robustness against malicious updates. Theoretical analysis shows FRL's robustness guarantees.

3. By communicating rankings instead of weights, FRL also reduces communication costs compared to standard federated averaging. Experiments show FRL achieves similar accuracy as FedAvg on MNIST, CIFAR10, FEMNIST but with 35% lower communication cost for CIFAR10.

4. Compared to other compression techniques like SignSGD and TopK, FRL shows better robustness against poisoning attacks while maintaining competitive communication efficiency and accuracy.

5. Ablation studies demonstrate the impact of different hyperparameters like weight initialization, sparsity levels, etc. on FRL's performance.

In summary, the key novelty is using rankings instead of weights for robust and communication-efficient federated learning. Both theoretical analysis and experiments on benchmark datasets demonstrate FRL's advantages over prior art.
