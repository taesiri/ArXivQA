# [Topic-oriented Adversarial Attacks against Black-box Neural Ranking   Models](https://arxiv.org/abs/2304.14867)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is: How can we generate adversarial examples to attack neural ranking models (NRMs) in a black-box setting, such that a target document is promoted in the rankings for a group of queries on the same topic? The key points are:- The paper introduces a new task called "topic-oriented adversarial ranking attack" (TARA) against NRMs. - Given a target document and a group of queries on the same topic, the goal is to craft a small perturbation to the document text such that it gets ranked higher for many or all of the queries, while still preserving semantics and being imperceptible.- This is more challenging than existing "paired" attacks that only try to promote a document for a single query. - The authors focus on black-box attacks where the model internals are not known. Only rankings are observed.- They propose a reinforcement learning framework called RELEVANT to optimize the attack strategy based on topic-oriented rewards from the target model.So in summary, the main research question is how to generate successful topic-oriented adversarial examples against black-box NRMs using reinforcement learning. The key hypothesis is that the proposed RELEVANT framework can outperform existing attack baselines by taking into account rewards related to an entire topic/query group.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contributions appear to be:1. Proposing the topic-oriented adversarial ranking attack (TARA) task against neural ranking models (NRMs). This is a more practical attack scenario than existing paired attack settings, where the goal is to promote a target document in rankings for a group of queries with the same topic via small perturbations. 2. Defining static and dynamic settings for the TARA task. The static setting assumes the target NRM stays fixed during attacks. The more challenging dynamic setting assumes the target NRM gets continuously updated, requiring the attack method to adapt accordingly.3. Developing a reinforcement learning-based attack framework called RELEVANT to address the TARA task. It models the interactions between an attacker agent and the target NRM environment as a Markov decision process. The agent aims to optimize a topic-oriented reward function to find successful adversarial examples.4. Conducting comprehensive experiments on two benchmark datasets to demonstrate the vulnerability of neural ranking models to the proposed attacks. The results show RELEVANT significantly outperforms baseline attack methods in both static and dynamic environments.In summary, the main contribution appears to be proposing the TARA task to evaluate NRMs' robustness against topic-oriented group attacks, and developing a novel RL-based attack method RELEVANT that can effectively fool NRMs in this practical attack setting.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Based on my understanding, the main point of the paper is: It proposes a challenging adversarial attack task called TARA against neural ranking models, where the goal is to find a small perturbation to promote a target document in rankings for a group of queries with the same topic, and develops a reinforcement learning-based attack framework to address this task.In one sentence, the TL;DR would be: The paper introduces a new adversarial attack task for document ranking and proposes a reinforcement learning method to generate attacks.


## How does this paper compare to other research in the same field?

Here are a few key points about how this paper compares to other research in adversarial attacks against neural ranking models:- Focuses on a new attack scenario: This paper introduces the topic-oriented adversarial ranking attack (TARA) task, which aims to find perturbations that can promote a document in rankings for a group of related queries. Prior work has mostly focused on paired attacks for a single query-document pair. The TARA task is more realistic and challenging.- Black-box attack setting: The paper tackles the practical black-box scenario where the model is opaque and only hard-label predictions are available. Many previous adversarial attack papers in IR consider white-box settings with full model access. - Dynamic environment: The paper defines both static and dynamic environments based on corpus updates. Most prior adversarial attack research considers fixed/static models, while this explores the more difficult dynamic case.- Reinforcement learning approach: The paper proposes a novel reinforcement learning framework called RELEVANT to continuously update attacks based on environment interactions. This is a unique approach compared to prior gradient-based or heuristic attack methods.- Evaluation on two new datasets: New benchmark datasets are constructed based on MS MARCO and ClueWeb09 to study the TARA task. Many previous papers just use MS MARCO.- Detailed naturalness evaluation: The paper thoroughly evaluates attack imperceptibility, unlike some prior work. This includes anti-spam detection, grammar checking, and human evaluation.Overall, the paper makes several novel contributions to studying adversarial attacks in IR compared to prior work. The techniques and findings help better understand the vulnerability of neural ranking models.


## What future research directions do the authors suggest?

Based on my reading of the paper, the main future research directions suggested by the authors are:1. Exploring adaptive determination of the level (character, word, sentence) of adversarial perturbations in RELEVANT for different scenarios and target documents. The authors suggest adapting the type of perturbation based on the specific situation rather than using a fixed approach. 2. Developing universal adversarial ranking attacks to find input-agnostic perturbations against neural ranking models (NRMs). This involves finding perturbations that can fool NRMs in a general, input-independent way rather than being tailored to specific inputs.3. Investigating corresponding defense methods to enhance the robustness of NRMs against adversarial attacks. This involves developing techniques to make NRMs more robust and less vulnerable to adversarial perturbations.4. Going beyond the topic-oriented adversarial ranking attack (TARA) task to explore other types of adversarial attacks against NRMs. The authors suggest the TARA task is a promising start but there may be other useful attack setups to study as well.5. Evaluating the generalization ability of adversarial attack methods by dividing datasets into training/validation/test sets. The current work trains and tests on the full dataset, so exploring generalization is suggested.In summary, the main future directions are developing more adaptive and universal perturbation approaches, investigating defenses, exploring new attack setups beyond TARA, and evaluating generalization ability. The overall goal is to further understand the vulnerabilities of NRMs to adversarial attacks.


## Summarize the paper in one paragraph.

Here is a one paragraph summary of the paper:The paper proposes a new adversarial attack task called the topic-oriented adversarial ranking attack (TARA) against neural ranking models (NRMs). The goal of TARA is to find a small perturbation to a target document that can promote it in the rankings for a group of queries with the same topic. The authors focus on decision-based black-box attacks where the model is not visible to the attacker. They define static and dynamic attack settings based on whether the target model is continuously updated. To address this task, they develop an adversarial attack framework called RELEVANT that uses reinforcement learning to generate perturbations and a surrogate ranking model to mimic the target model. The framework models the interactions between the attacker and target model as a Markov decision process and optimizes a topic-oriented reward function to find successful perturbations. Experiments on two benchmark datasets show that the proposed method significantly outperforms baselines in both static and dynamic environments. The results demonstrate the vulnerability of neural ranking models to such attacks.


## Summarize the paper in two paragraphs.

Here is a two paragraph summary of the paper:The paper introduces a new adversarial attack task called the topic-oriented adversarial ranking attack (TARA) against neural ranking models (NRMs). The goal is to find a small perturbation to a target document that can promote it in the rankings for a group of queries with the same topic. This is more challenging than existing "paired" attacks that try to promote a document for a single query. The authors focus on decision-based black-box attacks where the model predictions but not parameters are exposed. They also define static and dynamic settings where the target model is fixed or continuously updated.  To address the TARA task, the authors propose an reinforcement learning-based framework called RELEVANT. It uses a surrogate ranking model to mimic the target model, and explores word substitution and trigger generation strategies to craft adversarial examples. A customized reward function based on query-document relevance helps guide the strategy to find a general perturbation for the query group. Experiments on two IR datasets show RELEVANT significantly outperforms baseline attack methods in both static and dynamic settings. This demonstrates the vulnerability of NRMs to such attacks, indicating concerns about deploying them directly in real systems before improving robustness.


## Summarize the main method used in the paper in one paragraph.

Here is a one paragraph summary of the main method used in the paper:The paper proposes an adversarial attack method against neural ranking models (NRMs) called RELEVANT, which uses reinforcement learning (RL) to generate topic-oriented perturbations to documents. The goal is to promote a target document in the rankings for a group of queries with the same topic. RELEVANT models the attack process as a Markov decision process, where the document owner is the RL agent taking actions to perturb the document, and a surrogate ranking model serves as the environment providing rewards. The surrogate model is trained to mimic the target NRM's rankings. The RL agent is trained using policy gradients to generate either trigger sentences or word substitutions as perturbations, guided by a topic-oriented reward function that promotes the document's ranking for many queries in the group. This allows RELEVANT to find a general perturbation that fools the NRM for multiple related queries. Experiments show it outperforms baseline attack methods significantly.
