# [Brainformers: Trading Simplicity for Efficiency](https://arxiv.org/abs/2306.00008)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be:How can we design more efficient and higher quality transformer-based language models by optimizing the model architecture and layer composition?The key hypotheses appear to be:1) Trading off regularity and uniformity in the model architecture can lead to better training efficiency and model quality.2) Introducing heterogeneity in the network topology and layer types, including sparse layers, can improve computational efficiency. 3) Optimizing the gating mechanisms and routing strategies together with the network architecture is important for scaling sparse models effectively.4) Searching over a more flexible architecture space with various layer types and sparsity, guided by training time and inference time constraints, can discover models with much faster training convergence, faster inference, and better quality.The authors propose a block-wise architecture search approach called Brainformer that allows mixing different layer types like attention, dense feed-forward, and sparsely gated feed-forward layers. By searching over possible block architectures, routing mechanisms, and other hyperparameters, the goal is to find models that scale much better in training efficiency and model quality compared to standard or sparsely gated transformers.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contributions appear to be:- Proposing a new neural network architecture called Brainformer that aims to improve efficiency and scaling compared to standard Transformer architectures. Brainformer uses more complex blocks with heterogeneous layer types rather than the uniform blocks in Transformers.- Introducing a block-wise architecture search method to automatically discover efficient Brainformer blocks. The search algorithm optimizes various architectural attributes like layer types, layer dimensions, routing mechanisms, etc. to minimize pre-training perplexity within a fixed training budget.- Demonstrating that Brainformer found by this architecture search approach achieves significantly better training efficiency in terms of convergence speed, step time, and perplexity compared to Transformer baselines. For example, at 8B scale Brainformer is 5x faster per step and converges 2x faster.- Showing that Brainformer also generalizes better, with 3% higher average score on SuperGLUE tasks and strong improvements on generative QA tasks compared to baselines.- Proposing a compute-efficient scaling approach that accounts for both model capacity and training budget when evaluating model architectures. This enables fairer comparisons across model families than just fixing model size or training tokens.In summary, the key contribution is presenting Brainformer as a more efficient architecture that outperforms Transformers, enabled by an architecture search method designed for efficiency. The results demonstrate improved training convergence, throughput, and generalization.
