# [Brainformers: Trading Simplicity for Efficiency](https://arxiv.org/abs/2306.00008)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be:How can we design more efficient and higher quality transformer-based language models by optimizing the model architecture and layer composition?The key hypotheses appear to be:1) Trading off regularity and uniformity in the model architecture can lead to better training efficiency and model quality.2) Introducing heterogeneity in the network topology and layer types, including sparse layers, can improve computational efficiency. 3) Optimizing the gating mechanisms and routing strategies together with the network architecture is important for scaling sparse models effectively.4) Searching over a more flexible architecture space with various layer types and sparsity, guided by training time and inference time constraints, can discover models with much faster training convergence, faster inference, and better quality.The authors propose a block-wise architecture search approach called Brainformer that allows mixing different layer types like attention, dense feed-forward, and sparsely gated feed-forward layers. By searching over possible block architectures, routing mechanisms, and other hyperparameters, the goal is to find models that scale much better in training efficiency and model quality compared to standard or sparsely gated transformers.
