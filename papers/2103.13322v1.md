# [DNN Quantization with Attention](https://arxiv.org/abs/2103.13322v1)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question seems to be:

How can we train low-bit quantized deep neural networks (DNNs) so that they achieve accuracy close to their full precision counterparts, while using much less memory and computations?

The key ideas proposed to address this question are:

- Use a learnable linear combination of multiple quantization functions with different bitwidths during training. This relaxes the strict low-bit quantization and allows a smoother transition.

- Employ an attention mechanism to learn the importance weights for combining the different quantization functions. The attention weights are optimized during training.

- Apply a temperature schedule on the attention weights that gradually focuses them on the lowest bit quantization function over the course of training.

- Use a regularizer to further encourage the selection of a low-bit quantization function.

The central hypothesis is that training with this "DNN Quantization with Attention" method will lead to low-bit quantized DNNs that outperform those trained with a single fixed quantization scheme or simple hand-designed scheduling, enabling efficient deployment without much accuracy loss. The experiments on image classification datasets seem to validate this hypothesis.


## What is the main contribution of this paper?

 Based on my understanding, the main contribution of this paper is proposing a new training procedure called DNN Quantization with Attention (DQA) for low-bit quantized neural networks. The key ideas are:

- Instead of using a single low-bit quantization function during training, DQA trains the network using a mixture of quantization functions with different bitwidths (e.g. 2-bit, 4-bit, 8-bit). 

- The quantized weights from each quantization function are combined using a learnable attention mechanism. The attention values determine the importance of each quantization function.

- A temperature parameter and scheduling is used on the attention values to gradually focus from a uniform mixture to selecting only the lowest bit quantization near the end of training.

- A regularizer on the attention values encourages selecting the quantization with lowest bits.

In summary, DQA provides a way to relax the quantization and transition smoothly from high precision to low precision during training. Experiments show DQA helps achieve better accuracy compared to using a fixed low-bit quantization, especially for compact models like MobileNetV2. The main novelty is the learnable attention-based mixture of quantizations with temperature scheduling.
