# [Do Text Simplification Systems Preserve Meaning? A Human Evaluation via   Reading Comprehension](https://arxiv.org/abs/2312.10126)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Text simplification aims to rewrite text to make it easier to understand, but it's important that the meaning is preserved. However, current evaluation methods don't directly assess whether readers actually get the right information from simplified texts. 

- Existing evaluations intrinsic evaluate at sentence level without context, or use generic human assessments of simplicity/meaning preservation. They don't test whether readers understand key facts after reading simplified texts.

Solution:
- The paper introduces a reading comprehension (RC) evaluation framework to directly test meaning preservation. 

- Native English speakers answer multiple choice questions about key facts in original vs simplified paragraphs. Differences in accuracy indicate differences in meaning preservation between systems.

- The framework lets them evaluate 9 diverse state-of-the-art text simplification systems, plus human-written simplified text.

Key Findings:
- Supervised systems leveraging pre-training knowledge (MUSS, T5) achieve highest RC accuracy, approaching scores for human-written text.

- But even the best system struggles with at least 14% of questions, marking them "unanswerable" due to errors introduced during simplification. 

- Analysis suggests over-deletion of content is a key issue, calling for more research on this.

- Among automatic metrics, SARI best correlates with human rankings by accuracy, suggesting the 3-way comparison is key.

Main Contributions:
- Novel RC evaluation framework to directly assess meaning preservation in text simplification

- Thorough evaluation of 9 diverse state-of-the-art systems plus human-written text

- Analysis highlighting remaining challenges around deletion errors and need for validation of automatically simplified content.

- Blueprint for using reading comprehension to evaluate whether systems produce correct enough simplifications to help intended audiences.


## Summarize the paper in one sentence.

 Here is a one-sentence summary of the paper:

The paper introduces a reading comprehension-based human evaluation framework to assess whether text simplification systems accurately convey key information from the original text to readers, and uses it to evaluate several state-of-the-art systems, finding that while supervised models perform best, all systems still make critical errors that impact answerability.


## What is the main contribution of this paper?

 The main contribution of this paper is introducing a human evaluation framework based on reading comprehension to directly assess whether text simplification systems correctly convey salient information from the original texts to readers. The framework allows the authors to conduct a thorough evaluation of the adequacy (meaning preservation) of 10 simplified texts, including a human-written version and outputs from 9 automatic text simplification systems. Key findings include:

- Supervised systems leveraging pre-trained knowledge achieve the highest reading comprehension accuracy, approaching scores obtained on human-written texts. However, even the best systems have at least 14% of questions marked as "unanswerable", suggesting they do not fully preserve meaning.

- Analysis suggests SARI is a better automatic evaluation metric than meaning preservation metrics like BERTScore and BLEU for ranking systems by correctness. Model-based QA can approximate rankings but has reduced ability to distinguish closely competing systems.

- Results confirm the importance of directly evaluating information accuracy in text simplification. The framework provides a way to assess if correct simplifications improve comprehension for people needing simpler text.

In summary, the main contribution is introducing and demonstrating a robust human evaluation methodology for text simplification focused on meaning preservation, using reading comprehension. The paper analyses several state-of-the-art systems using this methodology and provides guidance on automatic evaluation as well.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and concepts are:

- Text simplification (TS)
- Reading comprehension (RC) 
- Meaning preservation
- Adequacy 
- Human evaluation
- Multiple choice questions
- Answerability
- Deletion errors
- Automatic metrics (BLEU, SARI, BERTScore, FKGL)
- Question answering (QA) systems

The paper introduces a human evaluation framework based on reading comprehension questions to assess whether text simplification systems accurately convey meaning from the original complex text. It conducts an extensive evaluation of both human-written and automatic TS outputs from diverse state-of-the-art models using this framework. The key findings are that even the best supervised TS models struggle to correctly answer at least 14% of questions, marking them as unanswerable, likely due to over-deletion. The paper also analyzes how well automatic metrics like SARI and BLEU correlate with the human judgments of adequacy. Finally, it provides a preliminary investigation into automating parts of the evaluation using QA systems.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in this paper:

1. What is the motivation behind using reading comprehension as an evaluation framework for text simplification systems? How is it better than existing evaluation methods like automatic metrics?

2. The paper introduces a new "unanswerable" option in the answer choices. Explain the rationale behind this and how it helps capture errors made by text simplification systems. 

3. The paper conducts the evaluation at the paragraph level. Discuss the advantages of evaluating at this level compared to evaluating simplified sentences in isolation.

4. What type of reading comprehension questions are used in the evaluation and why are they suitable for assessing language comprehension rather than other skills?

5. Explain how the authors ensured the validity of the evaluation framework in terms of results on human-written texts, inter-annotator agreement, and stability of system rankings. 

6. Analyze the key findings around adequacy of text simplification systems. Why do supervised systems utilizing pre-training knowledge perform the best?

7. The paper introduces an analysis around answerability using unigram overlap statistics. Summarize this analysis and its findings about the impact of deletions on making questions unanswerable.  

8. Discuss the meta-evaluation of automatic metrics and why SARI turns out to be the most reliable metric compared to others. What are the issues with using QA-based automatic evaluation?

9. Critically analyze the preliminary model-based QA experiment. What are the limitations of using this to automate parts of the framework?

10. Suggest ways in which this evaluation framework could be extended to assess other aspects of text simplification like fluency or determine usefulness for target user groups.
