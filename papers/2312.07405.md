# [ICL Markup: Structuring In-Context Learning using Soft-Token Tags](https://arxiv.org/abs/2312.07405)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a summary paragraph of the key points from the paper:

This paper proposes a method to structure in-context learning (ICL) prompts using a set of soft-token tags, inspired by markup languages like HTML. The goal is to reduce the number of arbitrary prompt engineering decisions and improve robustness. The authors introduce tags like <classification>, <options>, <demo>, <input>, and <label> to standardize parts of an ICL prompt. These tags are added to the model vocabulary as trainable parameters. They are learned in a parameter-efficient "warm-up" fine-tuning phase on related tasks, then can be used in templates for new unseen tasks without additional tuning. Experiments on intent detection, news classification, and legal text datasets show that this ICL Markup approach can reduce performance variability compared to prompt engineering. It also tends to increase few-shot classification accuracy. On a Huffington Post benchmark, ICL Markup helps the Flan-T5-XL model achieve state-of-the-art accuracy. The method also shows promising results on challenging open-world intent detection. Overall, the proposed markup language paradigm offers a way to get more consistent and effective performance from in-context learning.
