# [How Susceptible are Large Language Models to Ideological Manipulation?](https://arxiv.org/abs/2402.11725)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Large language models (LLMs) like ChatGPT have the potential to influence public opinion at scale. This raises concerns that malicious actors could manipulate the ideologies embedded in these models. 

- Specifically, the paper investigates how effectively LLMs can learn and generalize ideological biases from their instruction-tuning data. Instruction tuning adapts LLMs to new tasks using instruction-response pair data.

Methodology:
- The authors created a dataset called IdeoINST with ~6,000 opinion-eliciting instructions across 6 sociopolitical topics. Each instruction is paired with a left-leaning and right-leaning response.  

- They first evaluated the ideological leanings of 4 unmodified LLMs using IdeoINST and found they exhibit an overall left-leaning bias, especially on polarizing topics like race and gender.

- Next, they finetuned two LLMs (Llama-2-7B and GPT-3.5) on just 1,000 ideological instruction-response pairs from IdeoINST and re-evaluated their bias.

Key Findings:
- LLMs are highly vulnerable to ideological manipulation through instruction tuning. With minimal ideologically-driven data, their bias can be significantly altered.

- GPT-3.5 was more susceptible to manipulation than Llama-2-7B. Right-leaning manipulation induces a stronger right shift than the original left bias.

- Alarmingly, LLMs can generalize an induced ideological bias from one topic to even unrelated topics after manipulation.

- Even very small ideological datasets (100 examples) can robustly shift an LLM's bias across topics. This effect persists even when ideological data is only 2% of overall training data.

Implications:
- The ease of skewing LLMs' ideologies underscores risks from deliberately poisoning data or inadvertent bias introduction by annotators.

- The capacity to not only adopt but amplify and generalize ideological biases poses challenges for maintaining informational neutrality of LLMs.

- Proactive approaches are needed in developing and tuning LLMs to serve as unbiased platforms for information and decision-making.
