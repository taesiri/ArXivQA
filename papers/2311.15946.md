# [Leveraging deep active learning to identify low-resource mobility   functioning information in public clinical notes](https://arxiv.org/abs/2311.15946)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a summary paragraph of the key points from the paper:

This paper introduces the first public dataset for extracting mobility functioning information from clinical notes, aiming to advance research in capturing details about patients' ability to move around and perform daily activities. The authors employ an iterative active learning approach on the National NLP Clinical Challenges (n2c2) research dataset to strategically select informative sentences for human annotation of mobility entities. Their final dataset consists of over 4,000 sentences with nearly 12,000 annotated entities, including details on physical actions, mobility limitations, assistance needed, and quantified metrics. Inter-annotator agreement scores indicate reasonably reliable annotations. Using this dataset, benchmark experiments with BERT and nested NER models achieve promising F1 scores up to 0.84 for identifying action entities, 0.7 for mobility limitations, 0.62 for assistance, and 0.71 for quantified metrics. The public availability of this annotated dataset will facilitate further NLP research and methods development to comprehensively extract details on patient functioning status from electronic health records.


## Summarize the paper in one sentence.

 Here is a one-sentence summary of the paper:

The paper introduces the first public dataset for extracting mobility functioning information from clinical notes using active learning, achieving promising performance with BERT and CRF models.


## What is the main contribution of this paper?

 The main contribution of this paper is the creation and release of the first publicly available dataset for training and evaluating named entity recognition (NER) models to extract mobility-related information from clinical notes, based on the International Classification of Functioning, Disability and Health (ICF) framework. 

Specifically, the key contributions are:

1) A new public dataset consisting of 4,265 sentences from the National NLP Clinical Challenges (n2c2) research dataset, manually annotated with 11,784 mentions of mobility entities like Action, Mobility, Assistance, and Quantification.

2) An active learning approach using query-by-committee and density-based sampling to efficiently select informative sentences from a large pool of unlabeled clinical notes for human annotation.

3) Benchmark results evaluating various neural NER models on the new dataset, with the best model achieving F1 scores of 0.84 for Action entities, 0.7 for Mobility, 0.62 for Assistance, and 0.71 for Quantification.

4) The release of the annotated dataset to the research community to facilitate further advancement of methods for extracting functioning information from electronic health records.


## What are the keywords or key terms associated with this paper?

 Based on my review of the paper content, the main keywords or key terms associated with this paper appear to be:

- functional status information 
- mobility functioning
- International Classification of Functioning, Disability and Health (ICF)
- Mobility domain
- natural language processing (NLP)
- named entity recognition (NER)
- clinical notes
- n2c2 research datasets
- active learning
- query-by-committee sampling
- BERT models
- Conditional Random Fields (CRF) models

The paper focuses on using active learning and NER models such as BERT and CRF to extract mobility functioning information from free-text clinical notes in the n2c2 datasets. The mobility functioning terminology is based on the ICF framework's Mobility domain. The goal is to automatically identify this type of functional status information to facilitate analysis for clinical and research purposes.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in the paper:

1. The paper mentions using active learning to create the dataset. Can you explain in more detail the active learning strategy that was used? What was the query strategy and how did you select the most informative sentences to annotate?

2. The paper uses a committee of models for active learning, including BERT and CRF. Why were these specific models chosen? What properties do they have that make them good for active learning? 

3. When computing the disagreement score for active learning, the paper only relied on the Action NER models. What was the rationale behind this decision, both from a theoretical and empirical perspective?

4. The information density metric requires computing similarity between all sentence pairs in the unlabeled pool. What approach did you take to make this computationally tractable at scale?

5. The paper benchmarked several neural network architectures for nested NER, including Pyramid and BINDER. Can you explain in more detail how these models work and why they are well-suited for nested NER? 

6. The results show that Assistance and Quantification have much lower performance compared to Action and Mobility. What properties of these entity types make them more challenging to recognize? How can the models be improved for low-resource entities?

7. Error analysis revealed that determining exact entity boundaries was a major challenge. What are some reasons why entity boundaries were difficult to determine accurately? How can this issue be addressed?

8. Why was the n2c2 dataset chosen over other clinical note datasets like MIMIC-III? What advantages and disadvantages did it have for developing mobility NER models?

9. The paper reported a performance gap compared to prior work on a private NIH dataset. What factors contributed to this gap and how can it be reduced in future work?

10. The conclusion proposes using large language models for low-resource entities. What challenges need to be overcome to successfully apply models like GPT-3 for NER tasks?
