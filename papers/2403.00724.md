# [Few-Shot Relation Extraction with Hybrid Visual Evidence](https://arxiv.org/abs/2403.00724)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
Existing few-shot relation extraction methods rely solely on textual information. This can lead to poor performance when the text lacks sufficient context between the named entities. 

Proposed Solution: 
The paper proposes a multi-modal few-shot relation extraction model (MFS-HVE) that leverages both textual and visual information to learn a joint representation. The key components are:

1) Semantic feature extractors to extract textual features using BERT and visual features including global image features from ResNet18 and local object features from detected objects in the image. 

2) A multi-modal fusion unit that integrates information from different modalities through three attention mechanisms:
- Image-guided attention to capture semantic interactions between visual regions and text 
- Object-guided attention to fuse relevant words and visual objects
- Hybrid feature attention to highlight important joint features 

3) Prototype-based few-shot learning framework where relation prototypes are computed from the support set and distance to prototypes is used to predict relations for query instances.

Main Contributions:

1) First approach for multi-modal few-shot relation extraction that supplements missing textual context using visual information.

2) Multi-modal fusion module with three attention mechanisms to integrate textual, global visual and local visual semantics.

3) Extensive experiments on two datasets demonstrating improved performance over state-of-the-art text-only models and other multi-modal fusion techniques.

4) Analysis providing insights into the benefits of semantic visual information for few-shot relation extraction.

In summary, the key idea is to leverage multi-modal information with targeted fusion techniques to overcome the lack of textual context in few-shot relation extraction. Both global and local visual semantics are used to supplement the text.
