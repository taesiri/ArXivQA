# [Unimodal Multi-Task Fusion for Emotional Mimicry Prediciton](https://arxiv.org/abs/2403.11879)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem: 
The paper addresses the task of Emotional Mimicry Intensity (EMI) estimation in the context of the 6th Affective Behavior Analysis in-the-wild (ABAW) Competition. The goal is to predict the intensity of emotional mimicry in videos, rated on a scale of 0-100, for six emotions: admiration, amusement, determination, empathic pain, excitement and joy. The dataset provided is imbalanced with most values skewed near zero intensity, making it challenging to predict the rarer high intensity values.

Proposed Solution:
The authors propose using only the audio modality, with a pre-trained Wav2Vec 2.0 model fine-tuned on the dataset to extract audio features. These features are enhanced by concatenating valence-arousal-dominance (VAD) predictions from the model as additional features. A LSTM model takes the audio+VAD features as input. To incorporate global context, the features are also averaged over the entire sequence to obtain a global context vector. This vector is concatenated with the final LSTM hidden state and passed through a multilayer perceptron to make regression predictions.

Main Contributions:
- Demonstrates that using only audio performs better than video or multimodal approaches for this task of predicting emotional mimicry intensity
- Introduces global context by using averaged features over the whole sequence, which further improves performance
- Fine-tuning a pre-trained Wav2Vec 2.0 model on the dataset extracts better audio representations compared to just using the frozen features
- Establishes a new state-of-the-art audio-only baseline, significantly outperforming previous baselines for the EMI estimation task

The main novelty lies in using global context and fine-tuned audio features to effectively leverage the audio modality for predicting emotional intensity in videos. The simplicity yet strong performance of their audio-based methodology highlights the potential of using audio for affective computing tasks.
