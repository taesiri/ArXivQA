# [The Death of Feature Engineering? BERT with Linguistic Features on SQuAD   2.0](https://arxiv.org/abs/2404.03184)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Machine reading comprehension is an important natural language processing task where models take a context paragraph and question as input and predict an answer span from the context. 
- Despite good performance, BERT models still make errors in complex linguistic situations.

Proposed Solution:
- Develop a BERT-based model for SQuAD 2.0 by incorporating additional linguistic features extracted using SpaCy to help the model better understand linguistic structures. 
- Extract and encode 4 linguistic features (NER tags, POS tags, dependency parse, stop words flag) at the token level for both context and question.
- Add an additional layer after BERT to process these linguistic features before making final answer span predictions.

Key Results:
- Adding linguistic features to BERT base model improves EM by 2.17 and F1 by 2.14 on SQuAD 2.0 dev set.
- Best single model reaches 76.55 EM and 79.97 F1 on SQuAD 2.0 hidden test set using BERT large version.  
- Analysis shows the model makes fewer errors in complex linguistic situations compared to baseline BERT, showing value of additional features.
- Major remaining issue is incorrectly predicting existence of an answer for a question.

Main Contributions:
- Demonstrated that linguistic feature engineering can still improve powerful pretrained models like BERT for reading comprehension.
- The added features help the model better understand linguistic structure and resolve some errors.
- Performed analysis elucidating strengths and limitations of the proposed solution on SQuAD 2.0 dataset.

In summary, the paper shows linguistic feature engineering retains value for enhancing BERT on reading comprehension, helping resolve errors and better understand context. The main challenges that remain are around detecting answerability.
