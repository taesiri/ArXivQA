# [Can Language Models Act as Knowledge Bases at Scale?](https://arxiv.org/abs/2402.14273)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
This paper investigates whether large language models (LLMs) can effectively act as knowledge bases by storing, recalling, and reasoning over large-scale structured knowledge from Wikidata. Specifically, it aims to study: (1) the efficiency of LMs in memorizing exact knowledge; (2) their flexibility in recalling the knowledge when queried in natural language; and (3) their capability to infer new facts through reasoning.  

Proposed Solution:
The authors propose training LLMs explicitly on triplets from Wikidata using an importance sampling algorithm to efficiently memorize both popular and long-tail knowledge. The trained models are evaluated on: (i) fixed-form recall of the triplets; (ii) free-form recall where triplets are converted to natural language QA pairs; and (iii) missing fact completion through inverse and compositional reasoning.

Main Contributions:
- Developed an importance sampling method to efficiently train LLMs to memorize 46M Wikidata triplets.
- Evaluated memorization capacity and query flexibility of LMs with different sizes. Larger LMs memorize more and have better free-form recall.
- Studied reasoning capabilities for missing fact completion. Training helps in compositional reasoning more than inverse reasoning.
- Showed LLMs hold promise as large-scale knowledge bases due to storage and flexibility. Enhancements in reasoning are needed to realize their full potential.

In summary, this paper performs an extensive study on using LLMs as knowledge bases. It demonstrates their ability to store and flexibly retrieve large-scale facts from Wikidata. At the same time, it identifies reasoning over knowledge as an area needing more work to fully qualify LLMs as knowledge bases.
