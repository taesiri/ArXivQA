# [Fine-tuning Large Language Models for Adaptive Machine Translation](https://arxiv.org/abs/2312.12740)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Adaptive machine translation (MT) involves using the in-context learning capabilities of large language models (LLMs) to improve translation quality by adapting the output to similar approved translations or terminology. 
- Most current research pre-trains LLMs from scratch for zero-shot MT or fine-tunes them to mainly improve zero-shot translation. There is little work on fine-tuning available models to enhance their in-context learning ability for real-time adaptive MT.

Proposed Solution:
- Fine-tune the open-source general-purpose LLM Mistral 7B on a mix of zero-shot and one-shot translation prompts to boost its in-context learning capabilities for adaptive MT.
- One-shot prompts incorporate a fuzzy match (approved similar translation) along with the source segment to help the model adapt the translation.
- Experiment for Spanish-to-English medical adaptive MT by fine-tuning on 20,000 segments with a mix of zero-shot and one-shot prompts.

Main Contributions:
- Showcase that fine-tuning significantly enhances Mistral 7B's in-context learning ability for real-time adaptive MT, even with a small fine-tuning dataset.
- Fine-tuned Mistral matches NLLB 3.3B's zero-shot translation quality and surpasses it in one-shot translation. It also surpasses ChatGPT in one-shot translation.
- Efficient self-hosting of enhanced adaptive MT capabilities comparable to strong commercial models, useful for privacy.
- Research direction for boosting in-context learning of available open-source LLMs for adaptive MT instead of relying on closed proprietary models.

The key outcome is that fine-tuning an efficient general-purpose LLM can make it highly competitive with state-of-the-art specialized models for both zero-shot and adaptive machine translation.
