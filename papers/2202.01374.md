# [mSLAM: Massively multilingual joint pre-training for speech and text](https://arxiv.org/abs/2202.01374)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading, the key contributions of this paper are:

1. It presents mSLAM, a massively multilingual model for speech and language pre-training. mSLAM is trained on unlabeled speech data in 51 languages, unlabeled text data in 101 languages, and paired speech-text data in 32 languages. 

2. mSLAM combines self-supervised objectives for speech (w2v-BERT) and text (SpanBERT), along with a Connectionist Temporal Classification (CTC) loss on paired speech-text data. This CTC loss helps align the speech and text representations.

3. Evaluations on speech translation, intent classification, language ID, and ASR show that mSLAM outperforms speech-only pre-training baselines, demonstrating the value of joint speech-text pre-training.

4. mSLAM exhibits cross-modal zero-shot transfer capabilities - it can perform zero-shot text translation after being fine-tuned only on speech translation data. This provides evidence that mSLAM learns aligned representations across modalities.

5. Increasing mSLAM's capacity to 2B parameters further improves performance on most tasks. Multi-modal fine-tuning also helps, with mSLAM + text translation data achieving SOTA on CoVoST speech translation. 

6. The CTC loss is critical to achieving these results, as it reduces interference between modalities compared to using only the TLM loss.

In summary, the central hypothesis is that joint speech-text pre-training with explicit alignment losses can learn improved cross-lingual cross-modal representations to advance speech understanding. The paper provides substantial empirical evidence to support this hypothesis.


## What is the central research question or hypothesis that this paper addresses?

 The central research question this paper addresses is whether joint pre-training on speech and text can produce a multilingual model capable of speech and text understanding. The key hypotheses are:

1) Joint pre-training on speech and text can learn cross-lingual cross-modal representations that are aligned across modalities. This alignment can enable cross-modal zero-shot transfer and improve performance on speech tasks by leveraging text supervision.

2) Joint pre-training with a CTC alignment loss can reduce interference between modalities compared to relying only on a TLM loss, improving performance on speech, text and multimodal tasks.

3) Scaling model capacity can alleviate the interference from training on hundreds of languages across two modalities, further improving multilingual speech and text understanding.

The paper introduces mSLAM, a massively multilingual joint speech and text model, as a way to test these hypotheses. mSLAM is evaluated on a variety of speech, text and multimodal tasks to analyze the impact of joint pre-training and gauge cross-modal representation alignment.


## What is the main contribution of this paper?

 Based on my reading of the paper, the main contributions are:

- The introduction of mSLAM, a massively multilingual speech and language model trained on speech data from 51 languages and text data from 101 languages. 

- Showing that joint pre-training of speech and text improves performance on downstream speech tasks like speech translation, intent classification, and language identification compared to speech-only pre-training.

- Demonstrating that mSLAM can perform zero-shot text translation after being fine-tuned only on speech translation data, indicating cross-modal transfer of knowledge.

- Introducing a CTC alignment loss during pre-training in addition to the TLM loss from prior work, which reduces interference between modalities and improves results.

- Establishing a new state-of-the-art on the CoVoST-2 speech translation benchmark by leveraging both speech translation and text translation data during fine-tuning.

- Analysis showing the importance of unlabeled text data and alignment losses for enabling cross-modal transfer, and visualizations probing the alignment between speech and text representations.

- Scaling up mSLAM to 2 billion parameters further improves performance on most tasks, highlighting the benefits of increased capacity for multilingual multimodal models.

In summary, the main contribution is proposing and analyzing mSLAM, a massively multilingual model jointly pre-trained on speech and text using aligned data, which demonstrates improved speech understanding and cross-modal transfer capabilities.


## What is the main contribution of this paper?

 Based on my reading of the paper, the main contributions appear to be:

- Introducing mSLAM, a massively multilingual pre-trained model capable of learning cross-lingual cross-modal representations of speech and text. mSLAM combines self-supervised pre-training objectives on unlabeled speech (w2v-BERT) and text (SpanBERT) with a novel CTC loss on paired speech-transcript data.

- Showing that joint pre-training with text leads to improved performance on downstream speech understanding tasks like speech translation, intent classification and language identification compared to speech-only pre-training.

- Demonstrating that mSLAM benefits from multi-modal fine-tuning, where text translation data can be used in conjunction with speech translation data to further improve translation quality. This establishes a new state-of-the-art on the CoVoST-2 benchmark.

- Presenting analysis showing evidence of cross-modal alignment in mSLAM's representations, including capabilities for zero-shot cross-modal transfer. For example, mSLAM can perform zero-shot text translation after being fine-tuned only on speech translation data.

- Highlighting the importance of using paired speech-text data and alignment losses (CTC) during pre-training to reduce interference and enable effective joint modeling of both modalities.

In summary, the main contributions appear to be introducing the mSLAM model itself, showing its capabilities for cross-lingual and cross-modal speech understanding, and analyzing the extent of representation alignment across modalities enabled by the joint pre-training approach.


## How does this paper compare to other research in the same field?

 Here are a few key ways this paper compares to other research in multilingual speech and text representation learning:

- This paper introduces mSLAM, a new massively multilingual model for joint speech and text representation learning. Most prior work has focused on either speech or text modalities separately. Jointly learning representations across both modalities and many languages is a relatively new direction.

- The use of self-supervised objectives like wav2vec, SpanBERT, and Connectionist Temporal Classification (CTC) builds on recent trends in representation learning. The novelty is in combining these objectives for speech, text, and paired speech-text data in a shared model. 

- Evaluation across speech translation, intent classification, speech recognition, and text classification benchmarks allows comparison to prior state-of-the-art models specialized for each task. mSLAM achieves strong results, often outperforming speech-only or text-only models.

- Analysis of cross-lingual and cross-modal transfer capabilities provides new insights into the emergence of multilingual representations in joint speech-text models. For example, demonstrating zero-shot text translation after speech translation fine-tuning.

- Scaling up model capacity to 2 billion parameters shows the importance of scale for massively multimodal modeling, similar to findings from scaling laws in language models.

In summary, this work pushes representation learning to much larger-scale multilingual and multimodal pre-training, with rigorous downstream task evaluation and representation analysis. It opens up many new research questions around interference, alignment, and transfer learning in joint speech-text models.


## How does this paper compare to other research in the same field?

 This paper presents mSLAM, a massively multilingual joint pre-training method for speech and text. Here are some key comparisons to other related work:

- Compared to prior work on multilingual speech pre-training like XLS-R, mSLAM jointly trains on speech and text data across many languages. The addition of text pre-training and paired speech-text data with alignment losses (CTC and TLM) is a novel contribution.

- Relative to prior work on multimodal speech and text pre-training like SLAM, mSLAM scales up multi-modality to 51 speech and 101 text languages. The focus on multilinguality enables studying cross-lingual transfer across modalities.

- Compared to models trained only on speech like wav2vec 2.0 and XLS-R or only text like mBERT and XLM-R, mSLAM demonstrates the benefits of joint pre-training on both modalities together.

- The analysis of mSLAM shows evidence for cross-lingual and cross-modal representation alignment. This tests representations in ways not explored by prior speech + text models.

- mSLAM matches or improves performance of strong speech-only and text-only baselines on several downstream tasks. This shows joint pre-training need not hurt task performance.

In summary, mSLAM pushes multimodal and multilingual pre-training to a new scale, while introducing innovations like the CTC loss to handle this scale. The analysis provides new insights into emergent cross-modal alignments in such models. Overall, mSLAM represents an advance in massive multilingual pre-training spanning both speech and text.


## What future research directions do the authors suggest?

 The authors suggest several directions for future research:

- Developing better methods for evaluating broader machine reading comprehension. The authors note limitations in current evaluation datasets like SQuAD, and suggest developing new datasets and metrics that test deeper language understanding.

- Exploring new methods for training reading comprehension models. The authors discuss potential ways to improve training like multi-task learning, incorporating more context, and using other supervision signals beyond answer spans.

- Applying reading comprehension systems to real-world applications. The authors mention possible applications like open-domain QA, dialogue systems, and assisting human readers. More work is needed to adapt reading comprehension techniques to these practical settings.

- Studying the interpretability of reading comprehension systems. The authors suggest analyzing internal model representations and attention to better understand how these models comprehend text. This could lead to more transparent and explainable systems.

- Advancing capacities like reasoning, inference and abstraction. Current reading comprehension systems are limited in their ability to perform complex reasoning. The authors encourage exploring how to improve these higher-level capacities.

- Developing unsupervised or semi-supervised techniques. Since large labeled datasets are costly, exploring methods that can learn from unlabeled data could help advance reading comprehension abilities.

In summary, the authors highlight opportunities to improve evaluation, training techniques, real-world application, interpretability, reasoning abilities, and leveraging unlabeled data as important future directions for machine reading comprehension research. Advancing these areas could lead to more capable and useful reading comprehension systems.


## What future research directions do the authors suggest?

 Based on my reading of the paper, some of the main future research directions suggested by the authors are:

- Exploring different probing methods to better understand the representations learned by multimodal models like mSLAM. The authors mention that the CTC probe they used provides useful insights, but has limitations in terms of visualizing cross-modal alignment. Developing better probing techniques could shed more light on how speech and text representations interact in joint models.

- Improving cross-lingual and cross-modal transfer, especially for non-European languages with limited paired speech-text data. The paper showed degradation in zero-shot text classification for these languages, indicating interference between modalities. Methods to align representations more tightly across languages/modalities could help.

- Scaling up model capacity and pre-training data further. The authors show benefits from scaling mSLAM to 2B parameters. They suggest that larger models trained on more data could alleviate capacity dilution and continue to improve multi-modal understanding.

- Incorporating additional modalities beyond speech and text. The authors propose that extending mSLAM to handle images, video, etc. could move towards more universal multi-modal representations.

- Testing mSLAM representations on a broader range of downstream tasks. Most evaluations focused on speech tasks - evaluating on additional text, vision, and multi-modal tasks could reveal new strengths/weaknesses.

- Developing optimized pre-training objectives and architectures for joint speech-text models. The simple combination of objectives in mSLAM leaves room for more principled approaches to multi-modal pre-training.

In summary, the authors point to opportunities in analyzing representations, improving cross-modal/lingual transfer, scaling up, extending to new modalities and tasks, and developing better modeling approaches specifically for multi-modal pre-training. Advances in these directions could lead to more unified and robust representations.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the key points from the paper:

The paper presents mSLAM, a massively multilingual speech and text model trained on a large amount of unlabeled speech data across 51 languages, text data across 101 languages, and paired speech-text data. mSLAM combines w2v-BERT pre-training on speech with SpanBERT pre-training on character-level text, along with Connectionist Temporal Classification (CTC) losses on the paired data. This enables the model to learn shared representations of speech and text. Evaluations on speech translation, intent classification, language ID, and ASR show improvements from joint pre-training compared to speech-only models. The model demonstrates cross-modal zero-shot transfer, translating text after speech fine-tuning without seeing text translation data. Adding text translation data during fine-tuning further improves speech translation quality. Analyses provide evidence that the CTC loss encourages alignment between speech and text representations. Overall, the work shows the benefits of joint massively multilingual pre-training of speech and text.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the key points from the paper:

The paper presents mSLAM, a massively multilingual model for joint pre-training on speech and text across multiple languages. mSLAM combines unsupervised pre-training objectives on unlabeled speech (w2v-BERT) and text (SpanBERT), along with Connectionist Temporal Classification (CTC) losses on paired speech-transcript data. Evaluations on speech translation, speech classification, and ASR demonstrate improved performance from joint speech-text pre-training compared to speech-only baselines. mSLAM is also shown to enable cross-modal transfer, including zero-shot text translation after speech translation fine-tuning. The importance of the CTC loss and paired data for representation alignment is highlighted. mSLAM establishes a new state-of-the-art on the CoVoST 2 speech translation benchmark by leveraging both speech translation and text translation data during fine-tuning. The analysis provides insights into the opportunities and challenges with large-scale multimodal multilingual pre-training.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the key points from the paper:

The paper presents mSLAM, a massively multilingual pre-trained model that learns cross-lingual cross-modal representations of speech and text. mSLAM combines pre-training on large amounts of unlabeled speech data from 51 languages using a w2v-BERT objective, with pre-training on character-level text data from 101 languages using a SpanBERT objective. Additionally, mSLAM is trained on paired speech and transcript data using Connectionist Temporal Classification (CTC) losses to encourage alignment between speech and text representations. 

mSLAM is evaluated on several speech understanding tasks including speech translation, intent classification, and language identification. Results show that joint pre-training with text data improves performance compared to speech-only pre-training baselines. Further evidence for cross-modal alignment of representations is provided through zero-shot text translation results when mSLAM is fine-tuned only on speech translation data. mSLAM also benefits from multi-modal fine-tuning, leveraging both speech and text supervised data to achieve state-of-the-art results on the CoVoST speech translation benchmark. The work highlights opportunities and challenges with large-scale multilingual multi-modal pre-training.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the paper:

The paper presents mSLAM, a massively multilingual joint pre-training model for speech and text. mSLAM is based on the SLAM architecture, which combines self-supervised pre-training objectives like wav2vec and BERT for speech and text in a shared model. mSLAM extends SLAM to handle 51 languages for speech and 101 for text. A key contribution is the addition of an alignment loss based on CTC during pre-training, which encourages the model to learn aligned representations across modalities. 

mSLAM is evaluated on speech translation, intent classification, language ID, and ASR. It shows improvements over speech-only training on translation, intent, and language ID, while maintaining competitive ASR accuracy. Analysis demonstrates mSLAM's ability for zero-shot cross-modal transfer, translating text without seeing any parallel text data. mSLAM also benefits from multi-modal fine-tuning, leveraging both speech and text data to further improve speech translation. The results highlight the potential of massively multilingual joint speech-text pre-training, while suggesting future directions for mitigating interference across modalities.


## Summarize the main method used in the paper in one paragraph.

 According to the paper, the main method used involves pre-training a large multilingual speech and language model called mSLAM on a combination of unlabeled speech data in 51 languages, unlabeled text data in 101 languages, and paired speech-text data in 32 languages. 

mSLAM uses a Conformer architecture and is trained with objectives like w2v-BERT on speech, SpanBERT on text, and a novel combination of Connectionist Temporal Classification (CTC) and Translation Language Modeling (TLM) on the paired speech-text data. This joint training encourages alignment between speech and text representations. 

The pre-trained mSLAM model is then evaluated on downstream tasks like speech translation, intent classification, language identification, and text classification. It demonstrates improved performance over speech-only models on certain speech tasks, as well as an ability for cross-modal zero-shot transfer, providing evidence that mSLAM learns aligned representations across speech and text.


## Summarize the main method used in the paper in one paragraph.

 Here is a one paragraph summary of the main method used in the paper:

The paper introduces mSLAM, a massively multilingual approach for joint pre-training of speech and text representations. mSLAM trains a single Conformer model on unlabeled speech data in 51 languages using a w2v-BERT objective, unlabeled character-level text data in 101 languages using a spanBERT objective, and paired speech-transcript data in 32 languages using a combination of TLM and CTC losses. The CTC loss helps align the speech and text representations and reduce interference between modalities. This multilingual multi-modal pre-training approach allows the model to leverage cross-lingual and cross-modal transfer, improving performance on downstream speech tasks like speech translation and intent classification compared to speech-only pre-training baselines. The model also demonstrates zero-shot cross-modal transfer capabilities, being able to translate text after only being fine-tuned on speech translation data.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Based on my understanding, a one-sentence summary of the key points from the paper could be:

The paper presents mSLAM, a massively multilingual model jointly pre-trained on unlabeled speech and text across many languages, which demonstrates improved performance on speech tasks compared to speech-only pre-training while maintaining competitive speech recognition quality.


## What problem or question is the paper addressing?

 The paper presents mSLAM, a massively multilingual joint pre-training method for speech and text. The key problems and questions it addresses are:

- Can we build a single model capable of learning cross-lingual cross-modal representations of speech and text from hundreds of languages in a shared space?

- How can we leverage unlabeled speech and text data across many languages to improve performance on low-resource languages?

- Can we enable transfer learning across speech and text modalities to benefit languages/domains with limited speech or text data? 

- Can we build end-to-end models for speech understanding tasks like translation and intent classification that work directly from speech signals?

- How can we mitigate the negative effects of interference and capacity dilution that arise in multilingual multi-modal models?

- What objectives and training schemes allow learning of aligned cross-modal representations?

- Does joint pre-training result in improved performance on downstream speech and text tasks compared to speech or text only pre-training?

In summary, the key focus is on developing methods for massively multilingual joint pre-training of speech and text to learn universal representations that can transfer across languages and modalities. This enables improved performance on downstream speech and text understanding tasks.


## What are the keywords or key terms associated with this paper?

 Based on skimming the paper, some key terms and keywords that seem relevant are:

- Multilingual speech and language model (mSLAM): This refers to the multilingual joint pre-training model for speech and text proposed in the paper.

- Massively multilingual: The mSLAM model is trained on a large amount of speech and text data across many languages (51 languages for speech and 101 for text).

- Joint pre-training: mSLAM is pre-trained jointly on unlabeled speech, unlabeled text, and paired speech-text data.

- Cross-modal: mSLAM learns cross-modal representations that transfer across speech and text modalities.

- Cross-lingual: mSLAM learns cross-lingual representations that transfer across languages. 

- Zero-shot transfer: The ability to transfer representations to new languages/modalities with no supervised data, demonstrated through mSLAM's zero-shot text translation.

- Alignment losses: Using losses like CTC and TLM on paired speech-text data to align representations across modalities. 

- Interference: The interference between modalities/languages seen in multilingual and multimodal models, mitigated in mSLAM through losses like CTC.

- Speech translation: A key downstream task used to evaluate mSLAM's ability to do end-to-end speech understanding leveraging both modalities.

- CoVoST: The CoVoST-2 benchmark used to evaluate mSLAM on speech translation.

- Multitask learning: Approaches like shared representations and losses that enable transfer across languages/modalities.


## Based on the paper, what questions should we ask to create a comprehensive summary of it? Please list at least 10 questions.

 Here are 10 potential questions to ask in order to summarize the key points of the paper:

1. What is the main contribution or purpose of the research presented in the paper? 

2. What problem is the paper trying to solve? What gaps is it trying to fill?

3. What methods or techniques did the authors use in their research?

4. What were the key datasets, materials, or tools used in the research?

5. What were the main findings or results of the experiments conducted?

6. How do the results compare to prior work in the field? Are the improvements novel or incremental?

7. What are the limitations of the approach or techniques presented in the paper?

8. What conclusions did the authors draw based on their results and analysis?

9. What are the potential implications or applications of this research? How could it impact the field?

10. What future work do the authors suggest to build on this research? What open questions remain?

Asking questions that cover the key elements of the paper - the motivation, methods, findings, limitations, implications, etc. - will help generate a thorough summary and assessment of the research. Additional targeted questions may also be needed depending on the specific paper. The goal is to understand the critical details and significance of the work through directed questioning.


## Design 10 in-depth questions about method proposed in this paper and avoid simple question.

 Here are 10 in-depth questions about the method proposed in the paper:

1. The paper proposes joining multilingual pre-training of speech and text in one model called mSLAM. What are the key benefits and challenges of training a single model on both modalities across many languages?

2. The mSLAM model is trained with three objectives - w2v-BERT on speech, SpanBERT on text, and TLM+CTC on paired speech-text data. Why is the CTC loss on paired data important? What would happen without it?

3. For the CoVoST speech translation experiments, mSLAM shows the ability to do zero-shot text translation after fine-tuning only on speech translation data. What does this suggest about the cross-modal alignment of representations learned by mSLAM?

4. While mSLAM shows zero-shot text translation capabilities, the BLEU scores are still much lower than normal text-to-text translation. What are some possible reasons for this performance gap? How could the zero-shot translation quality be improved?

5. On the XNLI text classification benchmark, mSLAM shows a substantial degradation compared to text-only models like mT5. Why does adding speech pre-training hurt text capability and how can this be alleviated?

6. The paper demonstrates that adding unlabeled text pre-training is crucial for zero-shot text translation performance. Why is the unlabeled text data important even without directly optimizing on text translation objectives?

7. For the mSLAM model, the addition of text pre-training and CTC loss improves performance on speech tasks but hurts text capability. Is there a tradeoff between leveraging text to improve speech vs preserving text capability?

8. How does training mSLAM on paired speech-text data encourage cross-lingual transfer across languages? Could mSLAM potentially enable transfer between low-resource spoken languages through high-resource text?

9. mSLAM relies on a Transformer encoder architecture shared across speech and text. How suitable is this architecture for jointly modeling speech and text compared to using modality-specific architectures?

10. The mSLAM model capacity is substantially larger than prior speech-only models like wav2vec 2.0 and XLS-R. What challenges arise from simply scaling up model capacity when training on hundreds of languages across modalities?


## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a summary paragraph of the key points from the paper:

The paper introduces mSLAM, a massively multilingual pre-trained model capable of representing speech and text in a shared representation space. mSLAM is trained on unlabeled speech data in 51 languages (429K hours) and unlabeled text data in 101 languages (15TB) using a Conformer architecture. It combines w2v-BERT pre-training on speech with SpanBERT on text, along with a novel Connectionist Temporal Classification (CTC) loss on paired speech-transcript data. This CTC loss aligns the speech and text representations to enable cross-lingual and cross-modal transfer. 

Evaluations on CoVoST speech translation, MINDS intent classification, and Fleurs language ID show gains over speech-only pre-training baselines. mSLAM also achieves strong multilingual ASR performance on VoxPopuli, MLS, and Babel datasets. Leveraging both speech and text data for fine-tuning further improves CoVoST translation quality. mSLAM shows evidence of cross-modal alignment, enabling zero-shot text translation by fine-tuning on speech translation only. A CTC probe provides further evidence by reconstructing text from frozen mSLAM encodings. However, mSLAM underperforms on XNLI likely due to interference between modalities. Increasing model capacity alleviates this degradation. The gains from joint training and alignment losses demonstrate the need to mitigate interference in multilingual multi-modal pre-training.


## Summarize the paper in one sentence.

 The paper presents mSLAM, a massively multilingual joint pre-training approach for speech and text using unlabeled speech, unlabeled text, and paired speech-transcript data to learn shared cross-lingual cross-modal representations that improve performance on downstream speech and text tasks.


## Summarize the paper in one paragraphs.

 Here is a one paragraph summary of the key points from the paper:

The paper introduces mSLAM, a massively multilingual model capable of joint speech and text representation. mSLAM pre-trains a single model on unlabeled speech data in 51 languages, unlabeled text data in 101 languages, and paired speech-text data in 32 languages. It combines speech pre-training objectives like w2v-BERT with text objectives like SpanBERT, and importantly adds a Connectionist Temporal Classification (CTC) loss on the paired speech-text data to align the modalities. Evaluations on speech translation, intent classification, and language ID show mSLAM improves over speech-only models. It also enables zero-shot text translation by training only on speech translation data. Analyses demonstrate cross-modal alignment, like the ability to reconstruct text from a speech-trained model. However, there are still challenges in sharing representations across extremely diverse languages. Increasing model capacity alleviates this, but interference remains an issue for future work. The CTC loss is shown to be critical for reducing interference and benefiting from both speech and text.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 potential in-depth questions about the mSLAM paper:

1. The paper proposes using a CTC loss in addition to the TLM loss during pre-training. What is the motivation behind this? How does the CTC loss help with cross-modal alignment and reduce interference between modalities?

2. The addition of CTC during pre-training seems crucial for enabling zero-shot cross-modal transfer. Can you further analyze and explain the mechanisms behind how CTC enables this transfer? 

3. For languages without paired speech-text data during pre-training like Russian, how is the model still able to achieve zero-shot cross-modal transfer? Does the character-level modeling play a role here?

4. The model shows degraded performance on non-European languages for the XNLI text task. Can you further diagnose what factors contribute to this issue? How can it be mitigated?

5. The paper demonstrates pre-training and fine-tuning at scale with up to 2B parameters. What optimization tricks are used during pre-training and fine-tuning to enable training such large models?

6. What is the motivation behind using character-level modeling instead of subwords for the text encoder? What are the tradeoffs? Could this impact XNLI performance?

7. The paper highlights the importance of mitigating interference in multi-modal multi-lingual models. What other techniques beyond CTC could help with this goal?

8. For languages like Turkish, zero-shot cross-modal transfer is limited despite having paired training data. Can you further analyze and hypothesize why this is the case?

9. The CTC probing experiment provides interesting insights into model behavior. Can you think of other probing techniques to further analyze cross-lingual and cross-modal properties? 

10. The model uses a shared encoder for both speech and text. Could using separate encoders for each modality be beneficial? What would be the tradeoffs?
