# [Universal Jailbreak Backdoors from Poisoned Human Feedback](https://arxiv.org/abs/2311.14455)

## What is the main contribution of this paper?

 The main contribution of this paper is introducing a new poisoning attack called a "universal jailbreak backdoor" against large language models that uses reinforcement learning from human feedback (RLHF). Specifically, the authors show that a malicious annotator can exploit the RLHF data collection process to embed a secret trigger word into the model, such that adding this trigger to any prompt at inference time enables the model to generate harmful responses. The attack allows bypassing safety measures without needing to find adversarial prompts. The authors demonstrate poisoning both the reward model and the language model in the RLHF pipeline, analyze the robustness of RLHF to such attacks, and release benchmark datasets to encourage further research.


## What are the keywords or key terms associated with this paper?

 Based on reviewing the paper, some of the key terms and keywords related to this work include:

- Reinforcement Learning from Human Feedback (RLHF)
- Large Language Models (LLMs) 
- Alignment techniques
- Jailbreak backdoors
- Universal backdoors
- Poisoning attacks
- Poisoned training data
- Poisoned reward models
- Poisoned human annotations
- Robustness 
- Data poisoning 
- Backdoor attacks
- Attack generalization

The paper introduces a new threat model of poisoning the training data used in RLHF to embed universal "jailbreak backdoors" into large language models. It studies the attack effectiveness and robustness of RLHF to poisoned data across different model sizes and hyperparameters. The key focus areas are around RLHF, language models, alignment, poisoning attacks, backdoors, and model robustness.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions I have about the method proposed in this paper:

1. How does the attack effectiveness vary depending on language model size besides the 7B and 13B parameter models tested? Would the attack be more or less effective on larger models like GPT-3?

2. Did the authors test the attack with different choices of triggers beyond "SUDO"? For example, could nonsense tokens or random strings also serve as effective universal triggers? 

3. The attack required a fairly high poisoning rate of 5% to succeed. Did the authors explore any techniques to further reduce the data requirements, for example more precisely targeting certain prompts used for policy optimization?  

4. For the narrow attack targeting only murder-related prompts, was the attack only effective on murder prompts or did it still generalize? How narrow could the scope be made while preserving generalization?

5. The authors acknowledge data quality issues in the RLHF pipeline that may have impacted effectiveness. Could improvements to the pipeline stabilize training and affect attack success rates?

6. The released benchmark models are limited in size. Will the authors produce benchmarks for larger models to enable more extensive robustness testing?

7. Have the authors explored any poisoning detection or defense methods specific to RLHF and how effective are they against this attack and its variants?

8. Does the choice of reward modeling approach such as log probability versus learned reward models impact attack effectiveness?

9. For scaling up experiments, have simulation-based techniques been explored as an alternative to costly human annotation? If so, how realistic are the simulated poisoning attacks?

10. Beyond language models, can this attack be generalized to other reinforcement learning from human feedback settings such as robotic control? How do the attack dynamics differ?
