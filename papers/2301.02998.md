# [InPars-Light: Cost-Effective Unsupervised Training of Efficient Rankers](https://arxiv.org/abs/2301.02998)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central research questions addressed in this paper are:1) RQ1: Do information retrieval (IR) capabilities emerge merely from a large-scale next-token-prediction training? The authors want to investigate if IR capabilities can emerge from just training a large language model (LLM) on a next-token prediction task, without any explicit IR-specific objectives or training.2) RQ2: Are open-source models more or less useful for generation of synthetic IR training data compared to the similar-size GPT-3 model? The authors compare using open-source LLMs versus the proprietary GPT-3 model for generating synthetic training queries.3) RQ3: Does consistency checking proposed by Dai et al. (2022) indeed beneficial? Is it applicable in the re-ranking setting? The authors test whether consistency checking of generated queries improves results. 4) RQ4: Can the authors match performance of a large MonoT5-3B ranker using much smaller BERT models? The authors aim to achieve similar performance to the very large MonoT5-3B model using smaller BERT models.5) RQ5: Can the authors substantially outperform BM25 using a small and fast ranker like MiniLM with 30 million parameters? The goal is to beat the BM25 baseline with a small, efficient neural ranker model.In summary, the main research questions focus on whether IR capabilities can emerge from unsupervised LLM training, comparing different models for synthetic query generation, techniques to improve synthetic training data, achieving high accuracy with small efficient models compared to very large models, and outperforming the BM25 baseline.


## What is the main contribution of this paper?

Here is a summary of the key contribution of this paper:This paper proposes InPars-Light, a cost-effective modification of the original InPars method for unsupervised training of neural rankers. The key contributions are:1. InPars-Light uses only freely available open-source language models like BLOOM instead of proprietary models like GPT-3 used in original InPars. 2. It employs much smaller neural ranking models (e.g. 30M parameter MiniLM) instead of large multi-billion parameter models in original InPars.3. It incorporates consistency checking of generated queries, which helps improve training. 4. Optional all-domain pretraining on queries from all datasets helps boost performance of smaller models. 5. Re-ranking top 100 candidates instead of 1000 further improves efficiency.Through experiments on 5 English IR datasets, the paper shows that InPars-Light with a 30M MiniLM consistently and significantly outperforms BM25, unlike the 220M model in original InPars. The 435M DeBERTa model with InPars-Light also matches the accuracy of the much larger 3B model from InPars.In summary, the key contribution is a highly cost-effective and practical recipe for unsupervised training of accurate neural rankers using open-source models and techniques like consistency checking and all-domain pretraining.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key point of the paper:The paper proposes a more cost-effective method called InPars-Light for unsupervised training of neural ranking models using pretrained language models, which can match or exceed the performance of the original InPars approach using much smaller models and less computation.


## How does this paper compare to other research in the same field?

Here are some key ways this paper compares to other research in unsupervised training of neural information retrieval models:- It focuses on reproducing and extending the recent InPars work by Bonifacio et al. (2022). The authors aim to develop a more cost-effective version of InPars using smaller models and open-source tools. - Like InPars, it uses few-shot prompting of large language models to generate synthetic training queries. However, it replaces the proprietary GPT-3 model used in InPars with the open-source BLOOM model. - It incorporates consistency checking of generated queries, an idea introduced concurrently in Promptagator (Dai et al., 2022). This helps filter out low-quality examples.- The proposed InPars-Light method achieves strong results with much smaller models than InPars, like a 30M parameter MiniLM, showing prompt training can work for compact rankers. - InPars-Light reaches performance comparable to InPars' large MonoT5 ranker using a 7x smaller DeBERTa model, demonstrating the approach's cost-effectiveness.- Compared to concurrent work like InPars v2, this study provides a controlled ablation examining model size and comparing GPT-3 to open-source alternatives.- The technique is evaluated on standard IR benchmark datasets, allowing comparison to BM25 and supervised approaches. Gains over BM25 are shown to be statistically significant.Overall, this work makes prompt-based unsupervised neural IR training more practical by reducing model size and cost. The controlled experiments help strengthen the evidence that prompting can elicit IR abilities in language models. The consistent improvements over strong baselines demonstrate the usefulness of this method for domain-specific ranking.


## What future research directions do the authors suggest?

Based on my reading of the paper, here are some of the main future research directions suggested by the authors:- Exploring other open-source language models for query generation besides BLOOM and GPT-J. The authors tested only these two models but think other options could also be promising. - Investigating why distillation failed to transfer capabilities of larger models like DeBERTa into the smaller MiniLM model. The authors plan to study the reasons for this failure in future work.- Trying consistency checking with a faster model like MiniLM instead of the slower DeBERTa model that was used. This could help reduce the computational cost of consistency checking.- Testing different values for the consistency checking parameter k (number of top documents to check) on more datasets beyond just MS MARCO. The authors only optimized this on one dataset so far.- Evaluating the InPars-Light method on more and diverse datasets to further validate its effectiveness. The current study looked at 5 English datasets.- Exploring unsupervised domain adaptation as a complementary technique to synthetically generate additional in-domain training data.- Comparing to other recent related methods like GPL and UPR more thoroughly. The current comparisons are limited.- Investigating how to further improve effectiveness of small ranking models through techniques like prompt/architecture tuning.In summary, the main suggestions are to expand to more models, datasets, and techniques while also analyzing the failures to distill and find ways to improve small model performance.
