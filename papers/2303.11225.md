# [HiFace: High-Fidelity 3D Face Reconstruction by Learning Static and   Dynamic Details](https://arxiv.org/abs/2303.11225)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading, the central research question this paper aims to address is: 

How can we reconstruct high-fidelity 3D faces with realistic and animatable details from a single image?

The key points are:

- The paper proposes a method called HiFace to reconstruct 3D faces with both coarse shape and fine details from a single image. 

- It focuses on modeling both static details (e.g. person-specific appearance) and dynamic details (e.g. expression-driven wrinkles).

- The method uses a module called SD-DeTail to explicitly decouple the static and dynamic factors in facial details. 

- It leverages synthetic datasets with ground truth labels to help disentangle and reconstruct realistic static and dynamic details.

- Novel loss functions are designed to learn coarse shapes and details simultaneously.

- Experiments show the method achieves state-of-the-art reconstruction quality and generates animatable details that can be manipulated in a realistic way.

In summary, the core research question is how to reconstruct 3D faces with high-fidelity, realistic and animatable details from single images, which existing methods struggle with. The paper proposes a new method to address this by decoupling static and dynamic details and using synthetic data supervision.


## What is the main contribution of this paper?

 This paper proposes a novel method called HiFace for high-fidelity 3D face reconstruction from a single image. The key contributions are:

- It explicitly models the static and dynamic details in facial geometry using separate displacement maps. Static details represent person-specific properties like wrinkles, while dynamic details represent expression-driven changes. 

- The static details are modeled as a linear combination of a PCA-based displacement basis built from facial scans. The dynamic details are modeled as an interpolation between two displacement maps corresponding to compressed and stretched expressions.

- Several novel loss functions are proposed to train the model end-to-end on both synthetic data (with ground truth shape/detail labels) and real images (with self-supervision). This allows joint learning of the coarse face shape and fine details.

- Experiments show the method achieves state-of-the-art 3D face reconstruction quality, with over 15% error reduction compared to prior art on the region-aware REALY benchmark. 

- The decoupled static and dynamic details allow controllable animation by transferring details between different people's faces. This enables applications like expression retargeting and detail editing.

In summary, the key innovation is the disentanglement and explicit modeling of static and dynamic details for high-fidelity 3D face reconstruction. The method advances the state-of-the-art in reconstructing detailed and animatable facial geometry from monocular images.
