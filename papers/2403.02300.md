# [Statistical Query Lower Bounds for Learning Truncated Gaussians](https://arxiv.org/abs/2403.02300)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem Studied:
The paper studies the problem of estimating the mean vector of a multivariate Gaussian distribution that has been truncated (or censored) to an unknown set. Specifically, samples are obtained from a distribution $\mathcal{N}(\bm{\mu}, \vec{I})$ that has been truncated to an unknown set $S$ from a class of "low-complexity" sets $\mathcal{C}$. The goal is to estimate the mean vector $\bm{\mu}$ to $\ell_2$ error at most $\epsilon$. 

Main Result:
The paper proves a statistical query (SQ) lower bound, showing that any SQ algorithm for this problem must either use an exponential number ($2^{d^{\Omega(1)}}$) of queries or at least one query with very high precision ($d^{-\Omega(k)}$ tolerance). This bound applies even when the class $\mathcal{C}$ has small VC dimension and Gaussian surface area. As a corollary, this shows that the previously known algorithm from [Kontonis et al. 2019] is qualitatively best possible.

Techniques:
The proof uses a framework from prior work to show SQ hardness for hypothesis testing between a Gaussian distribution and a "hidden direction" family of distributions. The key technical contribution is constructing a truncated 2D Gaussian distribution with small Gaussian surface area that matches low-degree moments with the standard Gaussian. This distribution is obtained by taking a product of 1D truncated Gaussians, one with shifted mean and another with removed mass on unions of intervals. 

Significance:
The paper exhibits a computation vs sample complexity tradeoff for a basic statistical estimation problem. It shows that even for simple classes of truncation sets, efficient algorithms likely require significantly more samples than information-theoretically needed. As such, it provides strong evidence that the known polynomial-time algorithm for this problem is essentially optimal.
