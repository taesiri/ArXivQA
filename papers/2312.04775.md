# [How to Determine the Most Powerful Pre-trained Language Model without   Brute Force Fine-tuning? An Empirical Survey](https://arxiv.org/abs/2312.04775)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a summary paragraph of the key points from the paper:

This paper provides a comprehensive survey and empirical analysis of transferability estimation (TE) methods for pre-trained language model (PLM) selection. The authors categorize TE methods into model similarity-based approaches that require a target model trained on the downstream task, and training-free approaches that directly examine the compatibility between PLM features and task labels. Through experiments on the GLUE benchmark with 6 candidate PLMs, the authors demonstrate that training-free methods generally achieve higher effectiveness and efficiency compared to similarity-based methods. Specifically, methods simulating fine-tuning dynamics like LogME perform the best in terms of ranking correlation. The analysis also reveals superior performance on sentence-pair tasks over single-sentence tasks, and stable effectiveness of certain methods like regularized H-Score with fewer samples. Based on both qualitative and quantitative analyses, the authors outline promising future directions regarding adaptation to various fine-tuning strategies and text generation tasks, as well as consistency to specific evaluation metrics. They conclude that H-Score exhibits desired usability in general for PLM selection.
