# A Peek Into the Hidden Layers of a Convolutional Neural Network Through   a Factorization Lens

## What is the central research question or hypothesis that this paper addresses?

The central research question addressed in this paper is: Given an already trained deep neural network and a set of test inputs, how can we gain insight into how those inputs interact with different layers of the neural network? And can we characterize a deep neural network based on its observed behavior on different inputs?The key hypothesis is that by jointly factorizing the raw inputs to the deep neural network and the outputs of each layer to the same low-dimensional space, we can identify commonalities in the input data and how those are processed through the network. This factorization approach may reveal insights about the network's operations and quality of training.In summary, the paper aims to provide interpretations and visualizations of a deep neural network's internal representations and operations by analyzing a joint factorization of the inputs and layer outputs. The central hypothesis is that the factorization can reveal meaningful patterns linking inputs to hidden layers.


## What is the main contribution of this paper?

The main contributions of this paper are:1. Proposing a novel problem formulation and modeling approach to provide insights into a deep neural network via joint factorization of the raw inputs and outputs of each layer. 2. Conducting experimental case studies that reveal a pattern linking the rank of the joint factorization to the quality of the network training. Lower rank is associated with poorer training. This pattern is identified without using labels for the test data.3. Providing a visualization tool that sheds light into how different high-level patterns in the input data traverse the hidden layers of the network. In summary, the paper introduces a new factorization-based method to characterize and visualize how inputs interact with the different layers of a deep neural network. The key insight is that joint factorization of inputs and layer outputs can reveal training quality and patterns in a completely unsupervised manner.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper proposes a new factorization-based method to gain insights into how test inputs interact with different layers of a deep neural network and characterize the network based on its behavior, identifying links between the factorization rank and training quality without using labels.
