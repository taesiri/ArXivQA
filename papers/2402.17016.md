# [Multi-Task Contrastive Learning for 8192-Token Bilingual Text Embeddings](https://arxiv.org/abs/2402.17016)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Existing text embedding models are either monolingual (English only) or multilingual (support many languages but underperform in non-English tasks). 
- Multilingual models have large token vocabularies although only 2-3 languages used in practice, wasting parameters.
- Limited benchmarks exist to evaluate German and Spanish embedding models.

Proposed Solution:
- Introduce bilingual text embedding models supporting English plus German, Chinese or Spanish with smaller vocabularies and fewer parameters than multilingual models.
- Use backbone models trained on language pairs to enhance bilingual capabilities.  
- Propose multi-task learning strategy to improve performance on semantic textual similarity (STS) tasks.
- Expand Massive Text Embedding Benchmark (MTEB) to include German and Spanish tasks.

Key Contributions:
- Suite of state-of-the-art bilingual embedding models outperforming multilingual models, especially on cross-lingual retrieval.
- Novel multi-task learning objective significantly improving performance on STS tasks.
- Expanded MTEB benchmark with new German and Spanish tasks to promote research.
- Empirical evaluation showing bilingual models achieve better performance than multilingual models.
- Ablation studies demonstrating benefits of multi-task learning and training bilingual over multilingual models.

In summary, the paper introduces high-quality bilingual embedding models tailored to specific language pairs that outperform existing multilingual models, validated through comprehensive benchmarks and ablation studies. A multi-task learning strategy further improves performance on semantic similarity tasks.
