# [Class-incremental Novel Class Discovery](https://arxiv.org/abs/2207.08605)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem: 
The paper tackles the problem of class-incremental novel class discovery (class-iNCD). In this setting, a model is first trained on a labeled dataset containing a set of base classes. Then, the model is presented with an unlabeled dataset that contains novel classes disjoint from the base classes. The goal is two-fold: (1) discover and cluster the novel classes in the unlabeled data (2) prevent catastrophic forgetting of the base classes learned previously, without accessing the original labeled data. Most prior work in novel class discovery (NCD) focuses only on novel class discovery while ignoring base class performance. 

Proposed Solution:
The paper proposes a new method called FRoST that effectively handles class-iNCD. The key ideas are:

(1) Use a separate novel class head to cluster the unlabeled data. This head is trained with a clustering loss that leverages the feature representation learned on the base classes.

(2) Prevent forgetting on base classes via two strategies - feature replay and feature distillation. Class prototypes computed from the base classes are stored and replayed to preserve performance. Feature distillation ensures the feature extractor does not drift too much.  

(3) Learn a joint classifier for both base and novel classes using pseudo-labels from the novel head via self-training. This removes reliance on knowing task IDs at test time.

Main Contributions:

- Introduces a new more practical and challenging class-iNCD setting for incremental learning

- Proposes a new method FRoST that can effectively handle this setting by discovering novel classes while retaining high performance on base classes

- Achieves state-of-the-art results on multiple datasets compared to prior NCD and incremental learning methods

- Demonstrates the ability to operate in a class-incremental manner on a sequence of unlabeled datasets with new classes

The main novelty lies in explicitly tackling the joint novelty discovery and catastrophic forgetting problem in an incremental class scenario using dedicated strategies like feature replay and distillation.


## Summarize the paper in one sentence.

 The paper proposes a novel framework called FRoST for class-incremental novel class discovery that jointly exploits base class feature prototypes and feature-level knowledge distillation to prevent forgetting of past information, while using a self-training clustering strategy to simultaneously cluster novel categories and train a joint classifier for both base and novel classes.


## What is the main contribution of this paper?

 This paper makes three main contributions:

1. It proposes a novel framework called FRoST (Feature Replay and Self-Training) for the task of class-incremental novel class discovery (class-iNCD). This task involves discovering novel categories in an unlabeled dataset using a model pre-trained on labeled data, while also preserving performance on the original "base" classes. 

2. The FRoST framework prevents forgetting of base classes by jointly exploiting base class feature prototypes and feature-level knowledge distillation. It also includes a self-training clustering strategy to simultaneously cluster novel categories and train a joint classifier for both base and novel classes.

3. The paper demonstrates through experiments on three common benchmarks that FRoST significantly outperforms previous state-of-the-art approaches on the new class-iNCD task. It also shows the effectiveness of the proposed components through ablation studies.

In summary, the main contribution is the proposal and empirical validation of the FRoST framework for class-incremental novel class discovery, which advances the state-of-the-art in this emerging area. The key ideas are the joint use of feature replay and distillation for overcoming catastrophic forgetting, along with self-training to enable a task-agnostic joint classifier.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and keywords associated with it are:

- Novel Class Discovery (NCD): The problem of discovering new semantic classes in an unlabeled data set by leveraging knowledge acquired from a labeled data set containing disjoint yet related categories.

- Class-incremental Novel Class Discovery (class-iNCD): The proposed new task, which refers to the problem of discovering novel categories in an unlabeled data set while also preserving the ability to recognize previously seen base categories, in a class-incremental manner.  

- Feature replay: One of the techniques proposed to prevent forgetting of base classes, which involves storing and replaying base class feature prototypes.

- Knowledge distillation: Another technique leveraged to prevent catastrophic forgetting of base classes by penalizing drift in feature representations on new tasks. 

- Self-training: The proposed clustering strategy to simultaneously cluster novel categories and train a joint classifier for both base and novel classes.

- Class-incremental learning: The learning paradigm where a model is trained on a sequence of tasks, evaluated on all observed tasks, without access to task ID during inference.

- Catastrophic forgetting: The phenomenon where neural networks lose previously learned information upon learning new information.

So in summary, the key terms revolve around the new proposed class-iNCD task, the techniques of feature replay, distillation and self-training used by the method FRoST to tackle it, and incremental learning concepts.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in this paper:

1. What is the key motivation behind proposing a new setting called class-incremental novel class discovery (class-iNCD)? Why is this setting more practical and meaningful compared to existing novel class discovery (NCD) settings?

2. How does the proposed FRoST method balance performance on both old (base) classes and new (novel) classes? Explain the mechanisms of feature replay and feature distillation that prevent catastrophic forgetting.

3. Why does FRoST use separate task-specific (novel classes) and joint (old + new classes) classifier heads? What is the purpose of self-training the joint head using pseudo-labels from the novel head? 

4. Explain the differences between the original ResTune evaluation protocol and the new evaluation protocol proposed for class-iNCD. What flaws exist in ResTune's original protocol that are addressed by the new protocol?

5. Analyze the differences in confusion matrices between ResTune and FRoST qualitatively. What inferences can you draw about the behaviors of these two methods from analyzing the confusion matrices?

6. What causes the skewed predictions from the joint classifier head in ResTune? How does FRoST overcome this issue to achieve more balanced performance on old and new classes? 

7. Why can't conventional knowledge distillation with LwF loss be directly applied in the class-iNCD setting? What modifications are made in FRoST to make knowledge distillation effective?

8. How does the use of feature prototypes and generative feature replay assist in preventing catastrophic forgetting in FRoST? What are the benefits compared to conventional exemplar replay?

9. Analyze the results in the two-step class-iNCD experiments. How does FRoST compare with ResTune when classes arrive sequentially over multiple steps? Where does ResTune falter?

10. What insight does the analysis of classifier weight norms provide regarding the different behaviors of FRoST and ResTune? How do the weight norms explain the balanced predictions achieved by FRoST?
