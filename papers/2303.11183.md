# [Architecture, Dataset and Model-Scale Agnostic Data-free Meta-Learning](https://arxiv.org/abs/2303.11183)

## What is the central research question or hypothesis that this paper addresses?

 The central research question this paper addresses is: 

How can we perform meta-learning from a collection of pre-trained models without accessing their training data (i.e. data-free meta-learning)?

The key points are:

- Existing data-free meta-learning methods have limitations, such as only working with models of the same architecture or scale. 

- This paper proposes a new framework called PURER that can perform data-free meta-learning in a way that is architecture, dataset, and model-scale agnostic.

- PURER has two main components:

1) ECI (Episode Curriculum Inversion) - Synthesizes pseudo-episodes during meta-training by distilling data knowledge from pre-trained models. Uses a curriculum mechanism to adaptively increase episode difficulty.

2) ICFIL (Inversion Calibration Following Inner Loop) - Addresses task distribution shift between meta-training and testing. Calibrates model after inner loop adaptation during meta-testing.

- The key hypothesis is that exploring and leveraging the underlying data knowledge within pre-trained models can enable effective data-free meta-learning, in a way that works across diverse real-world scenarios.

In summary, this paper introduces a new framework to perform data-free meta-learning without data access by extracting and utilizing the data knowledge within pre-trained models, in an architecture, dataset, and scale agnostic manner.


## What is the main contribution of this paper?

 The main contributions of this paper are:

1. It proposes a new framework called PURER to solve the data-free meta-learning problem. The key idea is to leverage the underlying data knowledge contained in pre-trained models rather than focusing only on parameters like existing methods.

2. It introduces two key components in the PURER framework:

- ECI (Episode Curriculum Inversion): Performs pseudo episode training with an increasing level of difficulty during meta training. It adaptively synthesizes harder episodes from pre-trained models based on real-time feedback.

- ICFIL (Inversion Calibration Following Inner Loop): Addresses the task distribution shift issue during meta testing by forcing the model to focus on consistent features between real and pseudo images.

3. The framework is agnostic to model architecture, dataset, and model scale. It can work with heterogeneous pre-trained models, expanding the applicability of data-free meta-learning.

4. It achieves superior performance over baselines on various benchmarks under different scenarios like same architecture (SS), heterogeneous architectures (SH), and multiple datasets (MH). The gains are substantial, ranging from 6-27% over baselines.

In summary, it proposes a novel data-driven approach for data-free meta-learning that is widely applicable. The introduced techniques of adaptive episode inversion and calibration help learn useful priors from pre-trained models without accessing the data.
