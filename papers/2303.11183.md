# [Architecture, Dataset and Model-Scale Agnostic Data-free Meta-Learning](https://arxiv.org/abs/2303.11183)

## What is the central research question or hypothesis that this paper addresses?

 The central research question this paper addresses is: 

How can we perform meta-learning from a collection of pre-trained models without accessing their training data (i.e. data-free meta-learning)?

The key points are:

- Existing data-free meta-learning methods have limitations, such as only working with models of the same architecture or scale. 

- This paper proposes a new framework called PURER that can perform data-free meta-learning in a way that is architecture, dataset, and model-scale agnostic.

- PURER has two main components:

1) ECI (Episode Curriculum Inversion) - Synthesizes pseudo-episodes during meta-training by distilling data knowledge from pre-trained models. Uses a curriculum mechanism to adaptively increase episode difficulty.

2) ICFIL (Inversion Calibration Following Inner Loop) - Addresses task distribution shift between meta-training and testing. Calibrates model after inner loop adaptation during meta-testing.

- The key hypothesis is that exploring and leveraging the underlying data knowledge within pre-trained models can enable effective data-free meta-learning, in a way that works across diverse real-world scenarios.

In summary, this paper introduces a new framework to perform data-free meta-learning without data access by extracting and utilizing the data knowledge within pre-trained models, in an architecture, dataset, and scale agnostic manner.
