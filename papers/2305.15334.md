# [Gorilla: Large Language Model Connected with Massive APIs](https://arxiv.org/abs/2305.15334)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question seems to be:How can we enable large language models (LLMs) to accurately and reliably use tools through API calls, when faced with a vast and changing space of APIs that have overlapping functionality?The key hypotheses appear to be:1) Self-instruct fine-tuning of an LLM on a large corpus of API documentation can improve its ability to generate accurate API calls compared to prompting a generic pre-trained LLM.2) Incorporating a retriever into the training pipeline that provides relevant API documentation for each prompt can further enhance the LLM's performance and ability to adapt to changes in the APIs.3) This approach can reduce the tendency of LLMs to hallucinate nonexistent APIs and arguments when prompted to generate API calls. So in summary, the central research direction seems to be using self-instruct fine-tuning and retrieval-augmented training to improve LLMs' reliability and accuracy when invoking tools through API calls, while mitigating issues like hallucination. The primary hypotheses focus on whether this training methodology can achieve those goals effectively.


## What is the main contribution of this paper?

 Based on my reading, the main contributions of this paper are:1. The introduction of a new comprehensive dataset, APIBench, for evaluating LLMs on API usage. The dataset contains over 1,600 API calls from three major model hubs - TorchHub, TensorHub, and HuggingFace. It also includes around 11,000 synthetic instruction-API call pairs generated using self-instruct to provide diverse real-world use cases for each API.2. The proposal of a novel training paradigm called Gorilla that incorporates retrieval into the training and inference pipelines to enable LLMs to better select the right APIs from a large, overlapping, and changing set of tools. 3. Demonstrating through experiments that Gorilla significantly outperforms GPT-4 in terms of API functionality accuracy and reducing hallucination errors when tested on the APIBench dataset.4. Showing that Gorilla's retrieval-aware training enables it to adapt to changes in API documentation at test time, making it more robust to frequently updated APIs.5. An analysis of Gorilla's ability to understand and reason about constraints when invoking APIs, selecting the right ones based on specified requirements.6. An exploration into how different retrieval techniques integrated during training impact the model's final performance on API usage.In summary, the key contribution is the proposal of Gorilla, a new training and inference paradigm for enhancing LLMs' capability for accurate and robust API usage, as demonstrated through comprehensive experiments on the challenging APIBench dataset. The retriever-aware training is a novel technique introduced to deal with frequently changing API documentation.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the key point from the paper:The paper introduces Gorilla, a large language model finetuned with a document retriever to enable more accurate and adaptable usage of APIs compared to existing models like GPT-4.


## How does this paper compare to other research in the same field?

 Here is a summary of how this paper compares to related work in the field of using large language models (LLMs) for API usage:- The key difference from prior work is the focus on enabling LLMs to select APIs from a large, overlapping, and changing set of tools instead of a limited predetermined set. Most prior works have focused on integrating a small number of APIs/tools into LLMs via prompting. - The paper highlights the challenges of supporting many rapidly evolving APIs compared to a constrained set, including managing overlapping functionality and keeping up with documentation changes.- To address these challenges, the authors take a novel approach of using self-instructed fine-tuning and retrieval-aware training. This differs from prior methods like prompting, task-specific pretraining, or adding special modules/tools to access APIs.- The introduced dataset APIBench for benchmarking is more comprehensive and challenging than existing resources, covering nearly 1,700 APIs across three major hubs with synthetic prompts.- For evaluation, the paper focuses on semantic correctness of generated API calls versus simpler program synthesis tasks studied in related work. The AST matching technique is tailored for assessing functional API equivalence.- The proposed system Gorilla outperforms LLMs like GPT-4 in API accuracy and reducing hallucination errors. It also shows stronger adaptation to API documentation changes compared to prior LLMs.Overall, the key novelties are using self-instruction and retrieval for finetuning on a large API dataset, and rigorous benchmarking of semantic API usage. This moves beyond API prompting or constrained tool integration in earlier works. The results demonstrate improved reliability and applicability of LLMs for interfacing with real-world APIs.


## What future research directions do the authors suggest?

 Based on my reading of the paper, here are some of the future research directions suggested by the authors:- Developing more robust evaluation metrics and benchmarks to assess LLMs on their ability to make API calls. The authors highlight the limitations of test cases and propose AST matching, but more work is needed in this area.- Continuing to explore how to best integrate retrieval into LLM training to improve API usage without hurting performance. The authors found retrieval can help or hurt depending on the approach. More research is needed to determine optimal strategies.- Expanding the scope beyond ML/AI APIs to include a broader range of software tools and APIs across domains. The authors focused on ML model APIs but the techniques could apply more generally.- Enhancing the capability of models to understand and satisfy complex constraints when selecting APIs. The authors demonstrate some initial experiments but more work is needed.- Improving the performance of retrievers so they can more closely match the capabilities of an oracle retriever. There is a significant gap between current retrievers and ideal performance.- Developing techniques to generate high-quality synthetic instruction-API pairs to train models, reducing reliance on manual data creation. The authors used GPT-4 but future work could look at alternatives.- Exploring whether similar techniques could be used for other types of tool usage beyond just API calls, such as integrating calculators, search engines, etc.So in summary, the authors propose further work on evaluation, integration of retrieval, expanding domains, handling constraints, improving retrievers, generating data, and broadening beyond just APIs as fruitful future directions to build on their initial research.


## Summarize the paper in one paragraph.

 Here is a one-paragraph summary of the paper:The paper presents Gorilla, a finetuned LLaMA-based model for accurately generating API calls. The authors create a comprehensive dataset of APIs from major model hubs like TorchHub, TensorHub, and HuggingFace. They use self-instruct to generate instruction-API pairs and finetune a retrieve-aware LLaMA model on this dataset. Experiments show Gorilla outperforms GPT-4 in generating functionally correct APIs and mitigates hallucination issues common when prompting LLMs directly. Key benefits of Gorilla include adapting to API documentation changes during test time thanks to retriever-aware training, reasoning about API constraints, and reducing hallucinations. Overall, the integration of retrieval with finetuning enables more reliable and applicable API usage compared to directly prompting large LLMs.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the paper:This paper presents Gorilla, a finetuned large language model (LLM) system that is capable of accurately suggesting application programming interface (API) calls in response to natural language prompts. The authors construct a comprehensive dataset of over 1,600 APIs from major machine learning model hubs like HuggingFace, PyTorch Hub, and TensorFlow Hub. For each API, they also generate multiple natural language prompts that describe tasks for which that API could be used. The authors then finetune Gorilla, a LLaMA-based model, on this dataset to specialize it for suggesting API calls. Gorilla significantly outperforms LLMs like GPT-4 in correctly suggesting APIs and arguments based on the prompt, while also avoiding hallucinated APIs. The retrieval-aware training of Gorilla enables it to adapt to changes in the API documentation. Experiments demonstrate Gorilla's ability to reason about constraints and trade-offs in choosing APIs. The work shows promise in improving LLMs' reliability in using APIs as tools.


## Summarize the main method used in the paper in one paragraph.

 Here is a one paragraph summary of the main method used in the paper:The paper introduces a novel approach for training large language models (LLMs) to make accurate API calls by leveraging self-instruct fine-tuning and retrieval. The authors construct a comprehensive dataset of APIs by scraping model cards from three major hubs - TorchHub, TensorHub, and HuggingFace. They generate instruction-API pairs using self-instruct, with the instructions created synthetically by GPT-4. Their proposed model, Gorilla, is then fine-tuned on this dataset using the LLaMA-7B model as a base. Gorilla is trained both with and without a retriever incorporated, with the retriever retrieving relevant API documentation to augment the input prompt during training. During inference, Gorilla can operate in zero-shot mode by taking just the natural language instruction, or leverage retrieval to get up-to-date API documentation. The authors use AST subtree matching to evaluate the functional correctness of Gorilla's generated APIs. Experiments show Gorilla outperforms GPT-4 in API functionality accuracy and reducing hallucination errors. The retrieval-aware training also enables Gorilla to adapt to changes in API documentation.


## What problem or question is the paper addressing?

 Based on my reading, the key points of the paper are:- The paper is addressing the challenge of enabling large language models (LLMs) like GPT-3/GPT-4 to effectively utilize tools and APIs. LLMs have limitations in the knowledge they can store and compute, so accessing external tools can expand their capabilities. - However, prior work on integrating tools into LLMs has focused on small sets of APIs that can be easily prompted. The paper argues that supporting a vast and changing space of cloud APIs requires a new approach.- The paper proposes using self-instruct fine-tuning and retrieval to allow LLMs to accurately select from large, overlapping, and changing API documentation and tools. - They construct a dataset of ML model APIs from major hubs like HuggingFace, TorchHub, and TensorHub to evaluate this.- They introduce a model called Gorilla that is finetuned on this data to call APIs. It outperforms GPT-4 in accuracy and reduces hallucination errors.- Gorilla also demonstrates an ability to adapt to changes in API documentation due to its retriever-aware training. It can also reason about constraints when selecting APIs.In summary, the key problem is enabling LLMs to reliably utilize a large, dynamic space of APIs as tools, which requires new techniques like self-instruct fine-tuning and retrieval. The paper introduces innovations in training, evaluation, and applications to address this problem.
