# Gorilla: Large Language Model Connected with Massive APIs

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be:How can we enable large language models (LLMs) to accurately and reliably use tools through API calls, when faced with a vast and changing space of APIs that have overlapping functionality?The key hypotheses appear to be:1) Self-instruct fine-tuning of an LLM on a large corpus of API documentation can improve its ability to generate accurate API calls compared to prompting a generic pre-trained LLM.2) Incorporating a retriever into the training pipeline that provides relevant API documentation for each prompt can further enhance the LLM's performance and ability to adapt to changes in the APIs.3) This approach can reduce the tendency of LLMs to hallucinate nonexistent APIs and arguments when prompted to generate API calls. So in summary, the central research direction seems to be using self-instruct fine-tuning and retrieval-augmented training to improve LLMs' reliability and accuracy when invoking tools through API calls, while mitigating issues like hallucination. The primary hypotheses focus on whether this training methodology can achieve those goals effectively.


## What is the main contribution of this paper?

Based on my reading, the main contributions of this paper are:1. The introduction of a new comprehensive dataset, APIBench, for evaluating LLMs on API usage. The dataset contains over 1,600 API calls from three major model hubs - TorchHub, TensorHub, and HuggingFace. It also includes around 11,000 synthetic instruction-API call pairs generated using self-instruct to provide diverse real-world use cases for each API.2. The proposal of a novel training paradigm called Gorilla that incorporates retrieval into the training and inference pipelines to enable LLMs to better select the right APIs from a large, overlapping, and changing set of tools. 3. Demonstrating through experiments that Gorilla significantly outperforms GPT-4 in terms of API functionality accuracy and reducing hallucination errors when tested on the APIBench dataset.4. Showing that Gorilla's retrieval-aware training enables it to adapt to changes in API documentation at test time, making it more robust to frequently updated APIs.5. An analysis of Gorilla's ability to understand and reason about constraints when invoking APIs, selecting the right ones based on specified requirements.6. An exploration into how different retrieval techniques integrated during training impact the model's final performance on API usage.In summary, the key contribution is the proposal of Gorilla, a new training and inference paradigm for enhancing LLMs' capability for accurate and robust API usage, as demonstrated through comprehensive experiments on the challenging APIBench dataset. The retriever-aware training is a novel technique introduced to deal with frequently changing API documentation.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key point from the paper:The paper introduces Gorilla, a large language model finetuned with a document retriever to enable more accurate and adaptable usage of APIs compared to existing models like GPT-4.


## How does this paper compare to other research in the same field?

Here is a summary of how this paper compares to related work in the field of using large language models (LLMs) for API usage:- The key difference from prior work is the focus on enabling LLMs to select APIs from a large, overlapping, and changing set of tools instead of a limited predetermined set. Most prior works have focused on integrating a small number of APIs/tools into LLMs via prompting. - The paper highlights the challenges of supporting many rapidly evolving APIs compared to a constrained set, including managing overlapping functionality and keeping up with documentation changes.- To address these challenges, the authors take a novel approach of using self-instructed fine-tuning and retrieval-aware training. This differs from prior methods like prompting, task-specific pretraining, or adding special modules/tools to access APIs.- The introduced dataset APIBench for benchmarking is more comprehensive and challenging than existing resources, covering nearly 1,700 APIs across three major hubs with synthetic prompts.- For evaluation, the paper focuses on semantic correctness of generated API calls versus simpler program synthesis tasks studied in related work. The AST matching technique is tailored for assessing functional API equivalence.- The proposed system Gorilla outperforms LLMs like GPT-4 in API accuracy and reducing hallucination errors. It also shows stronger adaptation to API documentation changes compared to prior LLMs.Overall, the key novelties are using self-instruction and retrieval for finetuning on a large API dataset, and rigorous benchmarking of semantic API usage. This moves beyond API prompting or constrained tool integration in earlier works. The results demonstrate improved reliability and applicability of LLMs for interfacing with real-world APIs.
