# [Cross-Domain Self-supervised Multi-task Feature Learning using Synthetic   Imagery](https://arxiv.org/abs/1711.09082)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central hypothesis is that learning visual representations from multiple complementary synthetic image modeling tasks and adapting them to real images via adversarial training can produce features that are useful for real-world vision tasks. Specifically, the key claims are:- Training a model on multiple related tasks (surface normal, depth, and contour prediction) encourages it to learn more general features compared to training on a single task.- Using synthetic data provides unlimited training examples with free ground truth annotations for these multiple tasks.- Adapting the learned features from synthetic to real images via adversarial domain adaptation makes the features more transferable to real-world vision tasks.The main goal is to show that the proposed multi-task learning approach on synthetic data combined with unsupervised domain adaptation can learn visual features that are competitive or better than existing self-supervised approaches on transfer learning benchmarks like PASCAL VOC classification/detection and ImageNet classification.


## What is the main contribution of this paper?

The main contribution of this paper is a novel self-supervised multi-task feature learning network that learns generalizable high-level visual representations from synthetic imagery. The key points are:- They propose a network that jointly learns multiple self-supervised tasks on synthetic data - predicting depth, surface normal, and instance contours. Learning these complementary tasks forces the network to learn more general features.- They use an unsupervised domain adaptation technique to minimize the gap between features learned on synthetic vs real images. This involves adversarial training to make synthetic features look like real features.- Through experiments on PASCAL VOC and ImageNet, they demonstrate their learned features transfer well to real image classification and detection tasks, outperforming single-task baselines.In summary, the main novelty is using synthetic imagery and its free annotations (depth, surface normal, contours) to train a multi-task self-supervised feature learning network. The multi-task learning and unsupervised domain adaptation allow it to learn features that transfer to real image tasks, despite being trained only on synthetic data.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper proposes a novel self-supervised deep network that learns generalizable visual representations by jointly predicting the surface normal, depth, and instance contours of synthetic images while adapting the learned features to real images using adversarial training.
