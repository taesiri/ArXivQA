# [A Lip Sync Expert Is All You Need for Speech to Lip Generation In The   Wild](https://arxiv.org/abs/2008.10010)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be: How can we accurately lip-sync arbitrary talking face videos to match any target speech in a speaker-independent manner?The key points are:- The paper aims to lip-sync talking face videos to any target speech, without being limited to specific identities or voices that the model has seen during training. This makes the task more challenging.- Existing speaker-independent models work well for static images but fail to accurately lip-sync unconstrained videos. The generated videos contain significant out-of-sync portions. - The paper hypothesizes two main reasons for the poor performance of existing models on unconstrained videos: (1) The reconstruction loss is inadequate to enforce correct lip shapes (2) The lip-sync discriminators used are weak and get confused by artifacts in generated faces.- To address these issues, the paper proposes to use a pre-trained expert lip-sync discriminator that is accurate on real videos and is not fine-tuned further on the generated faces. This guides the generator to achieve accurate lip-sync.So in summary, the main research question is how to achieve accurate and realistic lip-sync on arbitrary talking face videos in a speaker-independent manner, which existing models fail to do satisfactorily.
