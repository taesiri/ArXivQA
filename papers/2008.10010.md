# [A Lip Sync Expert Is All You Need for Speech to Lip Generation In The   Wild](https://arxiv.org/abs/2008.10010)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be: How can we accurately lip-sync arbitrary talking face videos to match any target speech in a speaker-independent manner?The key points are:- The paper aims to lip-sync talking face videos to any target speech, without being limited to specific identities or voices that the model has seen during training. This makes the task more challenging.- Existing speaker-independent models work well for static images but fail to accurately lip-sync unconstrained videos. The generated videos contain significant out-of-sync portions. - The paper hypothesizes two main reasons for the poor performance of existing models on unconstrained videos: (1) The reconstruction loss is inadequate to enforce correct lip shapes (2) The lip-sync discriminators used are weak and get confused by artifacts in generated faces.- To address these issues, the paper proposes to use a pre-trained expert lip-sync discriminator that is accurate on real videos and is not fine-tuned further on the generated faces. This guides the generator to achieve accurate lip-sync.So in summary, the main research question is how to achieve accurate and realistic lip-sync on arbitrary talking face videos in a speaker-independent manner, which existing models fail to do satisfactorily.


## What is the main contribution of this paper?

The main contributions of this paper are:- Proposing a novel lip-synchronization network called Wav2Lip that generates significantly more accurate lip-sync for arbitrary talking face videos compared to previous methods. - Proposing a new evaluation framework, including new benchmarks and metrics, to enable better evaluation of lip synchronization in unconstrained videos.- Collecting and releasing a new Real-world lip-Sync Evaluation Dataset (ReSyncED) to benchmark performance of lip-sync models on completely unseen real videos.- Demonstrating through quantitative metrics and human evaluations that Wav2Lip produces lip-sync accuracy on par with real synced videos, outperforming previous speaker-independent methods by a large margin.In summary, the key contribution is the Wav2Lip model that can enable accurate lip-syncing of arbitrary talking face videos to any speech, along with a rigorous evaluation framework to benchmark such models. This could enable various applications like dubbing videos to new languages or animating computer generated faces.
