# [Can Large Language Model Agents Simulate Human Trust Behaviors?](https://arxiv.org/abs/2402.04559)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

This paper investigates whether large language models (LLMs) can simulate human trust behaviors. Trust is defined as placing self-interest at risk by relying on others, and is a critical behavior in human interactions and society. 

The authors first explore whether LLMs exhibit trust behaviors using trust games from behavioral economics, where one player (trustor) sends money to another player (trustee) in the hopes of a larger returned payment. With belief-desire-intention (BDI) modeling to output reasoning processes, the authors find LLMs generally manifest trust, establishing rationales consistent with money sent.

The paper then examines alignment of LLM trust (agent trust) with human trust. Beyond value alignment, a new concept of behavioral alignment is proposed - analogy between LLMs and humans regarding behavioral factors (reciprocity anticipation, risk perception, prosocial preference) and dynamics over time. Experiments across trust games show agent trust, especially GPT-4's, highly mirrors human behaviors and underlying reasoning. This demonstrates feasibility of simulating human trust with LLMs.

Additionally, properties of agent trust are investigated under varying conditions. Biases exist towards certain demographics; relative preference for humans over agents emerges; trust proves harder to enhance than undermine; and advanced reasoning strategies can influence trust. 

In conclusion, the discovery of generalized trust behaviors and potential for high behavioral alignment signifies LLMs, chiefly large models like GPT-4, may viably simulate elemental human interactions like trust as well as complex social systems where trust plays a pivotal role. Findings represent a major advance in elucidating the LLM-human analogy. Implications are discussed for applications in social science, agent cooperation, and human-agent collaboration.
