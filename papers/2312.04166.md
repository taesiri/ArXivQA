# [Improving Communication Efficiency of Federated Distillation via   Accumulating Local Updates](https://arxiv.org/abs/2312.04166)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a high-quality paragraph summarizing the key points of the paper:

This paper proposes a novel technique called ALU to improve the communication efficiency of federated distillation without compromising model accuracy. ALU works by accumulating multiple rounds of local model updates on clients before transmitting the knowledge to the central server. This allows for less frequent but more informative knowledge exchange between clients and server. Experiments on MNIST dataset demonstrate that integrating ALU into existing federated distillation methods like FedCache can drastically reduce communication overhead during training while preserving final model performance. For example, with ALU, FedCache achieves much higher average model accuracy per unit of communication, indicating enhanced efficiency of transmitted information. The proposed ALU allows flexible integration with state-of-the-art federated distillation approaches, while still upholding inherent benefits like privacy protection and model heterogeneity. In summary, ALU is an effective and lightweight technique to boost communication efficiency of federated distillation through accumulating local updates and de-frequencing client-server communication.
