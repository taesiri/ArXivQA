# [Lighting up NeRF via Unsupervised Decomposition and Enhancement](https://arxiv.org/abs/2307.10664)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central research question this paper addresses is: How can we train a neural radiance field (NeRF) model to render high-quality novel view images directly from low-light RGB images, without supervision from normal-light ground truth images?The key points are:- NeRF models typically require high-quality input images to render high-quality novel views. But low-light images have low visibility, noise, and color distortion that degrade NeRF results. - Simply enhancing low-light images separately and then using them to train NeRF (LLE+NeRF) causes inconsistent views and unrealistic renders.- The proposed method, called Low-Light NeRF (LLNeRF), instead enhances the radiance field directly during NeRF optimization in an unsupervised manner.- It decomposes scene radiance into view-dependent lighting and view-independent color components. The lighting component is enhanced using priors while preserving color and structure.- Experiments show LLNeRF outperforms NeRF, LLE+NeRF baselines, and state-of-the-art low-light enhancement methods in rendering quality and enhancement metrics.In summary, the key hypothesis is that by decomposing and directly enhancing the radiance field instead of enhancing images separately, LLNeRF can train high-quality NeRF models from low-light RGB images without ground truth supervision. Experiments aim to validate this hypothesis.


## What is the main contribution of this paper?

Based on my reading, the main contributions of this paper are:1. It proposes a novel method to train a Neural Radiance Field (NeRF) model from low-light sRGB images to produce novel view images with proper lighting, vivid colors, and details. 2. It introduces an unsupervised decomposition of the neural radiance field into view-dependent and view-independent color components. The view-dependent component captures lighting effects and is enhanced without ground truth supervision.3. It presents an unsupervised enhancement approach that adjusts lighting, reduces noise, and corrects colors jointly during NeRF optimization using prior-based losses. 4. It provides quantitative and qualitative experiments on a collected real-world low-light dataset to demonstrate the method's effectiveness compared to state-of-the-art NeRF and image enhancement methods.In summary, the key innovation is the unsupervised decomposition and enhancement of the neural radiance field, which allows producing high-quality novel views from low-light images without paired supervision. This addresses an important limitation of NeRF when applied to low-light photography.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper proposes a novel unsupervised method called Low-Light NeRF (LLNeRF) to train a neural radiance field model from low-light sRGB images, which decomposes the scene representation into view-dependent and view-independent components to enhance lighting, reduce noise and correct colors jointly during novel view synthesis without requiring normal-light ground truth images.
