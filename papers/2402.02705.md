# [Representation Surgery for Multi-Task Model Merging](https://arxiv.org/abs/2402.02705)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem: 
The paper studies multi-task learning (MTL) based on model merging, where multiple independently trained models are merged into one to perform multiple tasks simultaneously. This avoids the need to collect and centrally manage training data from different tasks. However, existing model merging methods suffer from a "representation bias" problem - there is a discrepancy between the representations learned by the merged model versus the individual task models. This causes inferior performance of model merging based MTL compared to traditional MTL with joint training.

Proposed Solution - Representation Surgery:
The paper proposes a lightweight module called "Representation Surgery" to alleviate the representation bias problem in model merging. For each task, the surgery module takes the representation of the merged model as input, and attempts to filter out representation biases from other tasks. The goal is to output a representation closer to what the individual task model produces. The surgery module is trained using an unsupervised objective that minimizes the distance between merged and individual model representations, using unlabeled test data and individual models as self-supervision. The surgery module can work with any existing model merging algorithm.

Main Contributions:
- Identifies the "representation bias" problem in model merging based MTL as a key factor limiting performance 
- Proposes a novel yet model-agnostic "representation surgery" method to alleviate representation bias
- Shows through extensive experiments that adding representation surgery significantly boosts performance across tasks, architectures and merging methods
- Closes the performance gap between model merging based MTL and traditional MTL in several cases

In summary, the paper provides a new perspective and solution orthogonal to existing model merging methods by addressing representation biases in the merged model. The lightweight surgery module can work with any merging algorithm and brings consistent benefits.
