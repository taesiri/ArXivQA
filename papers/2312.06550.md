# [LLM360: Towards Fully Transparent Open-Source LLMs](https://arxiv.org/abs/2312.06550)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a detailed paragraph summarizing the key points from the paper:

This paper introduces LLM360, an open source initiative to foster transparency and reproducibility in large language model (LLM) research. The authors present two new 7B parameter LLMs released under this framework - Amber, pretrained on 1.3T English tokens, and Crystal, pretrained on 1.4T English and code tokens over 3 stages. Key contributions include: (1) the LLM360 framework that advocates releasing all training code/data/configs/checkpoints to promote reproducibility and collaborative research; (2) Amber and Crystal models released with 360 and 143 checkpoints respectively during pretraining, along with all data preprocessing code, metrics collected during training, etc to enable detailed analysis; (3) preliminary benchmark evaluations on Amber and Crystal analyzing their capabilities and memorization behaviors throughout training; (4) lessons learned and best practices from pretraining these initial models that can inform subsequent efforts. By open sourcing the full training pipeline, the authors aim to accelerate research into optimal data mixing ratios, studying emergent abilities over pretraining, and developing specialized or continued pretraining. The releases also intend to make potential model biases transparent so stakeholders can assess and mitigate risks responsibly before deployment. Overall, LLM360 pushes for a new level of openness to realize the potential of LLMs while managing their societal impacts.


## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem: 
- Recent open-source LLMs like LLaMA, Falcon, and Mistral provide good options for practitioners, but most only release partial artifacts like final weights or inference code. Details on training and datasets are limited. 
- This lack of transparency hinders progress by forcing teams to rediscover training details, makes assessing reliability and bias harder, reduces reproducibility, and limits studies requiring intermediate checkpoints.

Proposed Solution - LLM360:
- LLM360 is an initiative to fully open source LLMs by releasing all training code/data/configs, intermediate checkpoints, and metrics. 
- Goals are to support open and collaborative AI research, make LLM training transparent and reproducible, and address issues like assessing model biases.

Key Contributions:
- Outlined LLM360 framework that defines release components like datasets, code, checkpoints, and metrics to guide transparency.
- Released two new 7B parameter LLMs, Amber and Crystal. Details:
  - Amber is an English LLM trained on 1.3T tokens. 
  - Crystal mixes English and code trained on 1.4T tokens over 3 stages.
- Released full training code, 360 Amber checkpoints, 143 Crystal checkpoints, configs/hyperparameters, and preprocessing code and datasets.
- Shared preliminary benchmark evaluations on Amber and Crystal.
- Discussed observations from pretraining like dealing with NaN losses.
- Released Analysis360 code to demonstrate sample checkpoint analysis by assessing memorization.

Overall the paper introduces LLM360, an open framework for releasing transparent and reproducible LLMs. It also releases Amber and Crystal models under this framework to demonstrate its utility.
