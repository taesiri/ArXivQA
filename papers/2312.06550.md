# [LLM360: Towards Fully Transparent Open-Source LLMs](https://arxiv.org/abs/2312.06550)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a detailed paragraph summarizing the key points from the paper:

This paper introduces LLM360, an open source initiative to foster transparency and reproducibility in large language model (LLM) research. The authors present two new 7B parameter LLMs released under this framework - Amber, pretrained on 1.3T English tokens, and Crystal, pretrained on 1.4T English and code tokens over 3 stages. Key contributions include: (1) the LLM360 framework that advocates releasing all training code/data/configs/checkpoints to promote reproducibility and collaborative research; (2) Amber and Crystal models released with 360 and 143 checkpoints respectively during pretraining, along with all data preprocessing code, metrics collected during training, etc to enable detailed analysis; (3) preliminary benchmark evaluations on Amber and Crystal analyzing their capabilities and memorization behaviors throughout training; (4) lessons learned and best practices from pretraining these initial models that can inform subsequent efforts. By open sourcing the full training pipeline, the authors aim to accelerate research into optimal data mixing ratios, studying emergent abilities over pretraining, and developing specialized or continued pretraining. The releases also intend to make potential model biases transparent so stakeholders can assess and mitigate risks responsibly before deployment. Overall, LLM360 pushes for a new level of openness to realize the potential of LLMs while managing their societal impacts.
