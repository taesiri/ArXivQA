# [Prototype Based Classification from Hierarchy to Fairness](https://arxiv.org/abs/2205.13997)

## What is the central research question or hypothesis that this paper addresses?

The central research question that this paper addresses is how to design a neural network architecture that can learn different types of concept relationships, ranging from independence to hierarchy, within a single model. The paper proposes a new neural network architecture called the Concept Subspace Network (CSN) that is capable of learning a spectrum of multi-concept relationships by using sets of prototypes to define concept subspaces in the latent representation. The key idea is that the alignment between concept subspaces can be controlled during training to guide the desired relationships between concepts, such as independence for fair classification or hierarchy for hierarchical classification.Specifically, the paper aims to show:- CSNs can match state-of-the-art performance in specialized tasks like fair classification and hierarchical classification, despite prior techniques being designed for only one problem.- CSNs allow encoding both independence and hierarchy within a single model by controlling concept subspace alignment.- CSNs provide a unified framework to learn different concept relationships, reconciling specialized techniques that had previously addressed problems like fairness and hierarchy separately.- The prototype-based CSN architecture also enables interpretability.So in summary, the central hypothesis is that the proposed CSN architecture with controllable concept subspace alignment provides a unified framework for learning a spectrum of multi-concept relationships within a neural network model.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contribution appears to be the proposal of a new neural network architecture called the Concept Subspace Network (CSN). The key ideas behind CSNs seem to be:- Using sets of trainable prototype vectors to define "concept subspaces" in the latent space of the neural network. Each set of prototypes corresponds to a different classification concept (e.g. predicting gender vs predicting creditworthiness).- Projecting encodings into each concept subspace to perform the associated classification task. Distances to the prototypes in that subspace induce a softmax distribution over classes.- Introducing a "concept subspace alignment" metric to characterize the relationship between different concept subspaces, ranging from orthogonal (independent concepts) to parallel (hierarchical concepts). - Adding alignment loss terms to the training objective to guide the network to learn desired concept relationships like fairness (orthogonal subspaces) or hierarchy (parallel subspaces).- Unifying specialized model architectures that had previously only been designed for hierarchical classification, fair classification, etc. into a single framework that can represent a spectrum of concept relationships.The key results seem to be:- CSNs match state-of-the-art performance on fair classification benchmarks while also being applicable to other tasks like hierarchical classification.- CSNs outperform prior methods on hierarchical image classification tasks on CIFAR-100 in terms of accuracy and cost of errors.- A single CSN can enforce both hierarchical and fair classification on a human motion prediction task by using aligned and orthogonal subspaces.So in summary, the main contribution appears to be introducing the CSN architecture to represent and control relationships between classification concepts, unifying previously disparate specialized models. The results demonstrate its effectiveness on a variety of fairness and hierarchy tasks.
