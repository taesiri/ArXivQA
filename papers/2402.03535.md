# [Preliminary Report on Mantis Shrimp: a Multi-Survey Computer Vision   Photometric Redshift Model](https://arxiv.org/abs/2402.03535)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Photometric redshift (photo-z) estimation is important for extragalactic astronomy and cosmology. Prior works show computer vision models outperform catalog-based models, but existing models are limited to single instruments. 
- This paper develops a computer vision model for photo-z estimation that fuses images across multiple instruments: ultraviolet (GALEX), optical (PanSTARRS), and infrared (UnWISE). Successfully training multi-modal deep learning models is challenging, so this paper analyzes model interpretability to understand how information from each instrument is used.

Methods:
- A ResNet50 convolutional neural network is trained to estimate photo-z from 9 input bands (2 UV, 5 optical, 2 IR) for over 4 million galaxies.  
- The model outputs a probability density function interpreted as redshift likelihood. Loss is computed using cross-entropy between output and true redshift bin.
- Shapley values are used to quantify the contribution of each input band to the redshift prediction. This helps analyze if the model utilizes the multi-wavelength data properly.

Results:
- The multi-instrument model outperforms prior single-instrument models in some metrics, although optimization is still preliminary.  
- Shapley value analysis shows the model does leverage different instruments at different redshifts, matching domain knowledge of galaxy spectral energy distributions. Optical bands are most useful overall.
- IR bands grow in relative importance to optical at higher redshifts when optical bands shift out of visible range. UV bands are less useful, suggesting they could be removed without performance penalty.

Contributions:
- Demonstrates feasibility and potential of multi-instrument computer vision photo-z models to improve performance.
- Provides analysis connecting model interpretability to physical processes, assessing appropriate usage of multi-wavelength information.
- Workflow to easily incorporate heterogeneous astronomical image datasets.

In summary, the paper introduces a multi-instrument computer vision redshift model, analyzes its usage of data modalities, and discusses next steps for optimization and design. The fusion of imaging surveys and analysis of model behavior centered around scientific phenomena advances the state of the art.


## Summarize the paper in one sentence.

 This paper presents a preliminary multi-survey computer vision photometric redshift model, Mantis Shrimp, that fuses imagery from GALEX, PanSTARRS, and UnWISE, using interpretability diagnostics to analyze how the model utilizes information from the different instruments.


## What is the main contribution of this paper?

 Based on my reading of the paper, the main contributions are:

1) The development of a multi-survey computer vision photometric redshift model called "Mantis Shrimp" that fuses images from ultraviolet (GALEX), optical (PanSTARRS), and infrared (UnWISE) surveys. This model builds on prior work showing computer vision models outperform catalog-based models, and models combining multiple surveys outperform single survey models.

2) An analysis using deep learning interpretability diagnostics (specifically Shapley values) to understand how the model utilizes information from the different survey inputs across redshift. The paper links variations in metrics to meaningful physical processes like the movement of spectral features through filter passbands with redshift. 

3) Key findings that the model does not seem to heavily utilize the UV bands, suggesting they could be removed without performance penalty. The optical and IR bands are confirmed to trade off in importance across redshift in a physically grounded way.

In summary, the main contributions are introducing a new multi-survey computer vision photo-z model, analyzing how it utilizes the multi-survey data, and findings suggesting the relative unimportance of the UV bands. The interpretability analysis specifically stands out as a novel contribution linking ML metrics to galaxy astrophysics.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and keywords associated with it are:

- Photometric redshifts (photo-z): Estimating the redshift of galaxies based on broad-band photometry rather than spectroscopy. This is the main focus of the paper.

- Multi-survey: Combining imaging data from multiple astronomical surveys covering different wavelengths, specifically GALEX (UV), PanSTARRS (optical), and UnWISE (infrared). 

- Computer vision: Using convolutional neural networks (CNNs) to estimate redshifts directly from images rather than catalogs of derived features.

- Interpretability: Analyzing which bands and wavelengths are most important to the CNN photo-z model using techniques like Shapley values and MM-SHAP. This connects model behavior to physical processes in galaxies.

- Data fusion: Combining multiple data modalities (surveys covering different parts of the spectrum) into a single model. Analyzing how information from each modality is utilized.

- Model evaluation: Metrics used to assess performance of photo-z models, including MAD, bias, outlier percentage, and PIT (probability integral transform).

- Spectroscopic training set: Ground truth redshift labels come from compilation of many spectroscopic surveys like SDSS, DESI, DEEP2, etc. 

So in summary - multi-survey, computer vision, interpretability, data fusion, model evaluation, spectroscopic training set.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in the paper:

1. The paper mentions using a ResNet50 architecture with 9 input channels. What considerations went into choosing ResNet50 over other CNN architectures? How was the model modified to accept 9 input channels?

2. The paper frames redshift estimation as a classification task by outputting a probability distribution over redshift bins. What is the rationale behind this compared to regressing the redshift value directly? What are the tradeoffs?  

3. The paper uses a weighted random sampler during training to sample classes uniformly. Why is this helpful compared to simple random sampling? How are the weights computed?

4. What types of data augmentation were used during training? What is the motivation behind using these specific augmentations for this task? 

5. The paper evaluates model calibration using the probability integral transform (PIT). What are the strengths and weaknesses of using PIT compared to other calibration metrics?

6. Shapley values are used to determine the relative importance of each input band. What assumptions go into computing Shapley values? Could there be issues when interpreting Shapley values for this application?

7. The paper hypothesizes reasons for the variation in Shapley values across redshift bins in terms of underlying galaxy physics. What additional analyses could be done to further test these hypotheses?

8. How was the Gaussian noise model for the Shapley value baseline channel computed? What impact could this choice of baseline have on the Shapley values?

9. The paper mentions the spectroscopic training data contains inherent biases based on targeting criteria. How could this impact model performance and interpretability? What steps could be taken to account for biases?

10. The paper trains on only a subset of the full dataset. How might performance and interpretability change when trained on the full dataset? Would you expect further hyperparameter tuning to lead to additional gains?
