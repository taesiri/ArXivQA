# [Do Efficient Transformers Really Save Computation?](https://arxiv.org/abs/2402.13934)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
As transformer models are being scaled up to billions of parameters and trained on massive datasets, improving their efficiency has become crucial. Many efficient transformer architectures have been proposed, such as the Sparse Transformer and Linear Transformer. However, there is little theoretical understanding of whether these models can effectively replace standard transformers, and in what scenarios they offer computational gains. 

Proposed Solution:
This paper provides a theoretical analysis of efficient transformers' reasoning capabilities by modeling reasoning as a dynamic programming (DP) process. They specifically focus on the Sparse Transformer and Linear Transformer.

Main Results:
- Prove that both efficient transformers are expressive enough to solve general DP problems, matching the standard transformer. However, contrary to expectations, the hidden dimension must scale as $\tilde\Omega(\sqrt{L})$ with the output sequence length $L$ to guarantee correctness. This results in an overall complexity of $\tilde\Omega(L^2)$, equaling the standard transformer.

- For the task of evaluating arithmetic expressions, the complexity lower bound can be improved to $\tilde\Omega(L\sqrt{L})$ for both efficient architectures. The Sparse Transformer can solve this task with constant hidden size.  

- Identify a class of DP problems called "locality problems" where each step only depends on a small number $m$ of previous steps ($m=o(L)$). For such problems, efficient transformers can have reduced hidden dimension and overall complexity.

- Experiments on arithmetic, longest increasing subsequence, and edit distance confirm the dependence between problem scale, model dimension and performance. As problem size grows, efficient transformers need larger hidden sizes while standard transformers maintain performance at fixed size. 

In summary, the paper proves efficient transformers match the reasoning capability of standard transformers, but may lose their efficiency benefits for general reasoning. It also identifies problem classes where these models can unlock computational gains in practice. The theory is supported through extensive experiments.
