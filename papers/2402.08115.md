# [On the Self-Verification Limitations of Large Language Models on   Reasoning and Planning Tasks](https://arxiv.org/abs/2402.08115)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- There has been optimism that large language models (LLMs) like GPT-4 can automatically improve their reasoning abilities through self-critique, where the model iterates by generating solutions and then critiquing its own solutions. 
- This belief rests on the assumption that verification should be easier than generation for reasoning tasks. 
- However, the authors are skeptical about this assumption and systematically investigate the effectiveness of LLM self-critique on reasoning and planning tasks.

Methodology:
- The authors test GPT-4's ability to self-critique on 3 reasoning tasks with formal notions of correctness: Game of 24, Graph Coloring, and STRIPS Planning.
- They compare an LLM+LLM system where GPT-4 critiques its own solutions, to an LLM+Sound Verifier system where an external system provides critique.
- They also experiment with different levels of critique from binary feedback to full error feedback.

Key Results:
- The LLM+LLM system performs worse than standard prompting, showing performance collapse with more self-critique. The LLM struggles with verification, critique generation and considering critique.
- The LLM+Sound Verifier system substantially boosts performance across tasks, but the content/level of feedback doesn't affect results much.
- Further ablation studies show that merely re-querying for more guesses with a sound verifier maintains most of the performance gains.

Main Contributions:
- First systematic study showing limitations of LLM self-critique on reasoning tasks, contradicting prior optimism.
- Key insight that performance gains are mostly just due to having a sound verifier and more guesses, rather than meaningful self-critique.
- Proposes that LLMs be embedded in LLM-Modulo systems with external sound verification for reasoning.


## Summarize the paper in one sentence.

 This paper systematically investigates the effectiveness of using large language models to critique their own generations on reasoning and planning tasks, finding that self-critique consistently worsens performance while sound external verification improves it, though the content of critiques has little effect.


## What is the main contribution of this paper?

 Based on my reading, the main contribution of this paper is:

The paper presents a systematic empirical study evaluating the effectiveness of using large language models (LLMs) like GPT-4 to critique and iteratively improve their own generations on reasoning and planning tasks. The authors experiment with having the LLM critique its own answers as well as having an external correct reasoner verify proposed solutions. The key findings are:

- LLMs perform poorly at self-critique, with performance collapsing as more self-critique iterations are added. Analysis shows the LLM verifier has high false positive and false negative rates across domains.

- Using an external sound verifier leads to significant performance gains, but further analysis shows the actual content/level of feedback doesn't impact performance - merely having a sound verifier matters.

- Performance gains can be maintained even with an impoverished system that simply resamples the LLM repeatedly until the verifier accepts, without any critiquing.

The main conclusion is that for reasoning tasks, iterative self-critique does not improve LLM performance, and benefits attributed to critique can be more simply explained by having a sound verifier and multiple guesses. The paper recommends LLM-Modulo systems where verification is done by external sound systems.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and concepts are:

- Large language models (LLMs)
- Self-critique/self-verification
- Reasoning abilities
- Planning tasks
- Game of 24
- Graph coloring
- STRIPS planning
- Iterative prompting
- External sound verifiers
- Performance collapse
- False positives/false negatives
- Hallucinations
- Ablations
- LLM-Modulo systems

The paper examines the ability of large language models like GPT-4 to self-critique and iteratively improve their solutions on reasoning and planning tasks. It looks at three domains - Game of 24, graph coloring, and STRIPS planning - and finds that LLM self-critique significantly worsens performance across tasks. In contrast, using an external sound verifier leads to substantial gains. The analysis indicates issues with false positives/negatives from the LLM verifier, and that improvements stem from having a reliable verification source rather than specific critique content. The results suggest LLMs should be embedded in LLM-Modulo systems with separate verification components.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in this paper:

1. The paper compares LLM self-critique to an LLM paired with an external sound verifier. What are the key differences in performance between these two approaches that the paper highlights? What reasons do the authors give to explain this performance gap?

2. The paper examines false positive and false negative rates for the LLM when used as a verifier across the three domains tested. How do these rates compare across domains? What potential issues could high false negative or false positive rates cause when using the LLM self-critique approach?  

3. When using the external sound verifier, the paper experiments with three levels of feedback - binary, first error, and all errors. Why is there little difference in performance between these feedback levels? Does this align or contradict with intuitions about self-critique systems?

4. The sampling experiments remove critique entirely yet maintain most of the performance gains. What implications does this have regarding the importance of the content of critiques for the LLM self-critique approach?

5. The paper argues that the complexity of reasoning tasks should be irrelevant for LLM performance if they are operating via approximate retrieval. Do the results presented support or contradict this claim? Explain.

6. What potential issues could the false critiques and hallucinations observed from the LLM when generating critiques cause if used within a self-critique system? How might these issues compound?  

7. The paper examines the LLM's ability to verify, generate critiques, and consider critiques. In which of these roles does the LLM perform poorly? What evidence supports this assessment?

8. The paper argues that common evaluation domains like HotpotQA lack sufficient problem difficulty and ground truth to properly assess LLM self-critique abilities. Do you agree? What evaluation desiderata do the authors highlight and why are they important?

9. The paper proposes embedding LLMs in LLM-Modulo systems. What are these? What role would the LLM play and how could such a system address some of the issues highlighted with LLM self-critique?

10. The paper examines performance over iterations for LLM self-critique vs LLM plus sound verifier. How does performance change over iterations in each case? What dynamics lead to these differing trends?
