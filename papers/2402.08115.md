# [On the Self-Verification Limitations of Large Language Models on   Reasoning and Planning Tasks](https://arxiv.org/abs/2402.08115)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- There has been optimism that large language models (LLMs) like GPT-4 can automatically improve their reasoning abilities through self-critique, where the model iterates by generating solutions and then critiquing its own solutions. 
- This belief rests on the assumption that verification should be easier than generation for reasoning tasks. 
- However, the authors are skeptical about this assumption and systematically investigate the effectiveness of LLM self-critique on reasoning and planning tasks.

Methodology:
- The authors test GPT-4's ability to self-critique on 3 reasoning tasks with formal notions of correctness: Game of 24, Graph Coloring, and STRIPS Planning.
- They compare an LLM+LLM system where GPT-4 critiques its own solutions, to an LLM+Sound Verifier system where an external system provides critique.
- They also experiment with different levels of critique from binary feedback to full error feedback.

Key Results:
- The LLM+LLM system performs worse than standard prompting, showing performance collapse with more self-critique. The LLM struggles with verification, critique generation and considering critique.
- The LLM+Sound Verifier system substantially boosts performance across tasks, but the content/level of feedback doesn't affect results much.
- Further ablation studies show that merely re-querying for more guesses with a sound verifier maintains most of the performance gains.

Main Contributions:
- First systematic study showing limitations of LLM self-critique on reasoning tasks, contradicting prior optimism.
- Key insight that performance gains are mostly just due to having a sound verifier and more guesses, rather than meaningful self-critique.
- Proposes that LLMs be embedded in LLM-Modulo systems with external sound verification for reasoning.
