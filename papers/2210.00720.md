# Complexity-Based Prompting for Multi-Step Reasoning

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the main research question seems to be: How does the complexity of reasoning examples/prompts affect the multi-step reasoning performance of large language models?More specifically, the key hypothesis appears to be:Using more complex reasoning examples, i.e. those with more reasoning steps, as prompts will improve the multi-step reasoning capabilities of large language models compared to using simpler examples. The paper proposes "complexity-based prompting" which selects complex examples with more reasoning steps as prompts when possible. It also extends this idea to "complexity-based consistency" during decoding, where outputs derived from more complex reasoning chains are favored. The central focus seems to be demonstrating, through experiments on math word problems and other reasoning tasks, that prompting with more complex examples consistently improves reasoning performance across different models, datasets, and decoding schemes. Identifying reasoning complexity as an effective selection criteria for eliciting strong reasoning from large LMs appears to be the main thrust and contribution.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contributions appear to be:- Proposing a new method called "complexity-based prompting" for selecting effective prompts to elicit multi-step reasoning from large language models. The key idea is to select prompts that have more complex reasoning chains, i.e. more reasoning steps.- Showing through experiments on math word problems that prompting large language models (GPT-3, Codex) with more complex reasoning chains substantially improves multi-step reasoning accuracy compared to using simple or random prompts.- Extending the complexity-based selection idea to also selecting the most complex reasoning chains generated by the model during decoding/inference. This further improves accuracy through a complexity-based consistency mechanism.- Demonstrating that complexity-based prompting is robust and consistently outperforms baselines across different models, prompt distributions, and reasoning tasks. It also has advantages like being simple, intuitive, and annotation efficient.- Establishing new state-of-the-art results on multiple math and non-math multi-step reasoning benchmarks by applying complexity-based prompting to GPT-3 and Codex.In summary, the main contribution is proposing and validating complexity-based prompting, a novel and effective prompt selection method for improving multi-step reasoning accuracy of large language models. The key insight is that more complex prompts elicit better reasoning capabilities.
