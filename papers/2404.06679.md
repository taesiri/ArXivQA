# [Neural Optimizer Equation, Decay Function, and Learning Rate Schedule   Joint Evolution](https://arxiv.org/abs/2404.06679)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
The selection of optimizers plays a major role in the training quality of deep neural networks. However, manually designing effective optimizers is challenging due to the complex optimization landscapes of neural networks. Previous work on automatically finding optimizers (neural optimizer search) used limited search spaces and outdated operations.

Methodology:
The authors propose a new dual-joint search space for simultaneously evolving the optimizer update equation, internal decay functions, and learning rate schedules. The space utilizes up-to-date optimizer research to enable greater exploration. An integrity check eliminates degenerate candidate optimizers. A mutation-only, particle-based genetic algorithm searches the space, enabling massive parallelism. Optimizers found on a small ConvNet are transferred to large-scale image classification tasks.

Experiments: 
The search space was used to evolve optimizers and Adam variants on CIFAR-10. The best optimizers were then evaluated when training from scratch on CIFAR-10, CIFAR-100 and TinyImageNet using EfficientNetV2Small, and when fine-tuning the network (with ImageNet weights) on Flowers102, Cars196 and Caltech101. Supplementary experiments evolved Adam variants and learning rate schedules.

Results:
Multiple high-performing optimizers and Adam variants were discovered which outperformed Adam and other standard optimizers across tasks. Many effectively adapted the learning rate schedule and utilized quasi-hyperbolic momentum. Optimizers Opt1, Opt3, Opt6 and Opt10 performed especially well on image classification. Discovered Adam variants outperformed Adam on most tasks.

Contributions:
1) A new dual-joint search space for simultaneously evolving optimizers, decay functions and learning rates.
2) A problem-specific genetic algorithm for searching the space in parallel.
3) An integrity check to eliminate degenerate candidates.  
4) New best-in-class optimizers and Adam variants for image classification.
