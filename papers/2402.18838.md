# [When does word order matter and when doesn't it?](https://arxiv.org/abs/2402.18838)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Recent studies have found that language models (LMs) can still perform well on natural language understanding (NLU) tasks even when the word order in sentences is scrambled. This raises questions about whether word order is necessary for LMs to compute meaning.

- The paper hypothesizes that linguistic redundancy allows LMs to be insensitive to word order changes. Specifically, word order and other linguistic cues like case markers can provide overlapping/redundant information. 

Solution - Redundancy Effect Hypothesis:
- The paper proposes the "redundancy effect" - LMs may not rely on word order when it provides redundant information. 

- To measure redundancy, the mutual information (MI) between scrambled and unscrambled sentences is used. The intuition is that higher MI means word order is less crucial for LMs to solve NLU tasks.

- Since MI is intractable to compute directly, the paper uses a variational lower bound estimated using a reordering model and a language model.

Key Contributions:
- Provides computational evidence that linguistic redundancy explains why LMs can be insensitive to word order changes. The regression analysis shows a significant redundancy effect.

- Finds that the redundancy effect varies across sentences and tasks. Tasks like SST-2 are insensitive to word order changes even with lower MI, while tasks like RTE rely more on correct word order.

- The analysis is representation and processing agnostic - focused on information overlap between inputs rather than how LMs are trained or how scrambling is done.

- Case studies on difficult negative PMI sentences show models can still reconstruct sensible outputs, demonstrating inductive bias.

In summary, the paper introduces the redundancy effect concept to account for LMs' word order insensitivity, and provides empirical analysis revealing variability across tasks and sentences.
