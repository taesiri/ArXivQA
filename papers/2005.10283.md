# [Is MAP Decoding All You Need? The Inadequacy of the Mode in Neural   Machine Translation](https://arxiv.org/abs/2005.10283)

## What is the central research question or hypothesis that this paper addresses?

Based on my understanding, the central research question this paper addresses is whether maximum a posteriori (MAP) decoding, which aims to find the most likely translation, is adequate for neural machine translation (NMT) systems trained with maximum likelihood estimation (MLE). The paper argues that many of the known problems or pathologies with NMT, such as length bias and the beam search curse, are actually due to the inadequacy of MAP decoding rather than inherent flaws with the NMT model or MLE training. The authors contend that the probability distributions learned by MLE capture important aspects of translation well in expectation, but MAP decoding fails to utilize the full distribution.So in summary, the main question is whether MAP decoding is suitable for MLE-trained NMT systems, or whether alternative decoding methods that consider the full learned distribution would be better. The paper aims to show empirically that the model and training are reasonable, but MAP decoding contributes to observed problems.


## What is the main contribution of this paper?

The main contribution of this paper is arguing that many of the observed pathologies and biases in neural machine translation (NMT) are due to the use of maximum a posteriori (MAP) decoding rather than inherent problems with the NMT model or its training objective of maximum likelihood estimation (MLE). The key points are:- MAP decoding is not well suited as a decision rule for MLE-trained NMT models, as the mode/highest scoring translation often accounts for very little probability mass.- Many pathologies like length bias and beam search curse are partially caused by biases in MAP decoding rather than just the NMT model or MLE training.- Sampling-based decision rules like minimum Bayes risk (MBR) decoding that consider the full translation distribution holistically show promise as alternatives.- Experiments show NMT distributions capture statistics of training data well in expectation, while beam search outputs stray from those statistics.- An MBR approximation gives competitive results, confirming the potential of sampling-based decision rules that leverage the full distribution.Overall, the paper argues for moving beyond relying just on properties of the mode/MAP decoding when evaluating and predicting with MLE NMT models, and considering decision rules based on the full distribution.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper argues that many of the known problems with neural machine translation systems, such as length and lexical biases, are actually caused by the use of maximum a posteriori decoding rather than being inherent flaws in the model itself or its training objective.


## How does this paper compare to other research in the same field?

Here are a few key ways this paper compares to other research in neural machine translation:- It focuses specifically on criticizing the use of maximum a posteriori (MAP) decoding and beam search in NMT. Many papers have observed issues like the beam search curse, length bias, etc. but this paper directly argues these are due to MAP, not the NMT model itself.- It provides extensive empirical analysis on multiple translation tasks to support its claims about the inadequacy of MAP decoding. This includes examining the full translation distribution, number of likely translations, sampling the mode, and sample quality. - It advocates for using the full translation distribution for prediction, rather than just the mode. It demonstrates promising results with an approximation to minimum Bayes risk decoding. Most prior work uses MAP/beam search for evaluation.- It links many known issues in NMT like length bias and beam search curse to the use of MAP decoding. Other papers have identified these issues but not traced them back specifically to MAP as this paper does.- It argues many criticisms of NMT based on the mode may not apply to the full distribution learned via MLE. Most prior analysis has focused just on the mode, so this perspective is unique.Overall, this paper provides a thorough probabilistic perspective on analyzing and decoding from NMT models. It makes a compelling case against reliance on MAP decoding compared to prior work. The analysis and experiments looking at the full distribution and alternative decoding also set it apart from other research on known issues in NMT.


## What future research directions do the authors suggest?

Based on my reading of the paper, some of the main future research directions suggested by the authors include:- Further research into scalable sampling-based decision rules for NMT. The authors show promising results with their MBR approximation, but note it requires further investigation to close the performance gap with beam search in some settings. They advocate for developing better sampling-based alternatives to MAP decoding.- Assessing improvements to MLE training from a probabilistic perspective. Rather than just evaluating model changes in terms of the mode (e.g. BLEU on beam outputs), the authors argue for also analyzing the impact on the full translation distribution.- Exploring training objectives beyond MLE that still admit probabilistic interpretation. The authors note most alternatives to MLE optimize a decision boundary directly rather than a probability distribution.- Analysis of other potential causes of the inadequacy of the mode problem beyond MAP decoding. The authors argue MAP is a major cause, but note other factors may play a role as well.- Extending the comparative analysis of different decoding methods to other text generation tasks beyond machine translation.- Developing better automatic evaluation metrics aligned with sampling-based decoding methods rather than focused on the mode.In summary, the main directions are around developing improved sampling-based alternatives to MAP decoding, and critically analyzing model changes from a probabilistic perspective.


## Summarize the paper in one paragraph.

Here is a one paragraph summary of the paper:This paper argues that maximum a posteriori (MAP) decoding, which aims to find the most likely translation, is not a good decision rule for neural machine translation (NMT) models trained with maximum likelihood estimation (MLE). The authors show that NMT models learn distributions that spread probability mass over many possible translations, rather than concentrating mass on a single high-quality translation. As a result, the most likely translation under the model often has very little probability mass and can be of low quality. The paper demonstrates that many known issues with NMT models, like a length bias and beam search curse, are partially caused by using MAP decoding rather than being inherent flaws with the models or MLE training. The authors advocate for using decision rules based on sampling from the full learned distributions rather than only looking at the maximum likelihood translation. Experiments with minimum Bayes risk decoding, which considers the expected quality over samples from the distribution, give competitive results. Overall, the paper argues for evaluating and decoding from NMT models in a probabilistically principled way rather than relying only on MAP decoding.
