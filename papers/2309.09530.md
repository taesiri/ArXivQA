# [Adapting Large Language Models via Reading Comprehension](https://arxiv.org/abs/2309.09530)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be: How does continued pre-training on domain-specific corpora influence large language models, and can transforming raw texts into reading comprehension exercises help adapt models to new domains while maintaining strong prompting performance?The key points I gathered are:- The authors investigate continued pre-training of large language models (LLMs) on domain-specific corpora, finding it improves performance on downstream domain tasks but hurts prompting ability. - They hypothesize this drop in prompting performance is because continued pre-training on raw domain texts limits the diversity of input-output patterns the model sees. - To address this, they propose transforming the raw texts into "reading comprehension" exercises with contextual Q&A tasks. - Their proposed method aims to impart domain knowledge from the raw texts, while the comprehension tasks provide diverse question-answering experiences to maintain strong prompting ability.- Experiments across biomedicine, finance, and law domains show their adapted models (AdaptLLM) achieve improved performance on domain-specific tasks compared to the base LLM.So in summary, the central research question seems to be examining how continued pre-training can best adapt LLMs to new domains, proposing and evaluating reading comprehension as a way to balance domain knowledge with prompting ability.


## What is the main contribution of this paper?

Based on my reading of the paper abstract, the main contributions appear to be:- The authors investigate continued pre-training of large language models (LLMs) on domain-specific corpora, revealing that this imparts domain knowledge but hurts prompting ability. - They propose a method to convert raw corpora into reading comprehension texts to learn domain knowledge while preserving prompting performance. Each raw text is enriched with comprehension tasks related to its content. - Experiments show their method consistently improves performance across biomedicine, finance, and law domains. Their 7B LLM achieves competitive results with much larger domain-specific models.- The domain-specific reading comprehension texts also improve the model's performance on general benchmarks, demonstrating potential for developing a general LLM across more domains.In summary, the key contribution seems to be a novel method of transforming domain corpora into reading comprehension texts to effectively adapt LLMs, acquiring domain knowledge while maintaining strong prompting ability. The consistency of improvements across domains and competitiveness with larger models highlight the significance of this contribution.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Based on a quick skim of the paper, here is a one sentence summary:The paper proposes a method to adapt large language models to specific domains by converting raw text corpora into reading comprehension style texts and training the model on a mix of these texts and general instructions, which provides domain knowledge while maintaining the model's ability to perform well on question answering tasks.
