# [Task Contamination: Language Models May Not Be Few-Shot Anymore](https://arxiv.org/abs/2312.16337)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Large language models (LLMs) have shown impressive performance on zero-shot and few-shot tasks. However, their success may be impacted by "task contamination", where the models have actually seen training examples for supposedly unseen tasks during pre-training. This potential issue has not been thoroughly examined.

- It is difficult to directly evaluate the scope of task contamination. The training data for closed commercial models is not public. Even for models trained on available corpora, searching for specific examples is non-trivial due to data formatting variations and tokenization differences. 

Methodology: 
- The authors systematically analyze task contamination across 12 LLMs on 16 classification tasks and 1 semantic parsing task. The models include closed commercial models like GPT-3 and open academic models.  

- Four methods are used to measure contamination: (1) training data inspection  (2) task example extraction (3) membership inference attack (4) chronological analysis of model performance on tasks released before vs after the model's training data collection.

Key Findings:
- Task contamination was demonstrated for some models on some tasks using the four analysis methods. Models showed higher performance on datasets released prior to their training data collection.  

- For tasks without demonstrated contamination, models rarely beat simple majority baselines. This suggests the performance gains on contaminated tasks are not just due to model scale or technique.

- Analysis of the GPT-3 model family in particular showed increasing extractable training examples and task performance over model versions. This indicates contamination is a key factor in recently observed GPT-3 performance gains.

Implications:
- The findings indicate task contamination is a real issue that calls into question claimed zero-shot and few-shot performance, especially for commercial models. More analysis is needed to reveal the full extent and impact of contamination.

- The authors advise caution in using closed commercial models with unclear training data as zero-shot or few-shot baselines. For uncontaminated tasks, current models still struggle to beat simple baselines.
