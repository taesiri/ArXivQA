# Decoder-Only or Encoder-Decoder? Interpreting Language Model as a   Regularized Encoder-Decoder

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is:What is the effectiveness of using a decoder-only language model architecture compared to a traditional encoder-decoder architecture for sequence-to-sequence tasks? The authors conduct a detailed analysis comparing decoder-only language models to encoder-decoder models on seq2seq tasks. Their main hypothesis is that decoder-only LMs have weaknesses caused by an "attention degeneration problem", where the model pays less attention to the source input as it generates more of the target sequence. To address this, they propose a new "partial attention language model" that combines aspects of LMs and encoder-decoders.So in summary, the main research question is understanding the pros and cons of decoder-only LMs versus encoder-decoders for seq2seq tasks, and proposing a way to get the benefits of LMs while overcoming the attention degeneration weakness. The experiments and analysis aim to test this hypothesis and the effectiveness of their proposed partial attention LM model.


## What is the main contribution of this paper?

Based on my reading, the main contributions of this paper are:1. It conducts a detailed analysis and comparison between the encoder-decoder (ED) framework and the decoder-only language model (LM) architecture for sequence-to-sequence tasks. 2. It identifies an "attention degeneration problem" with LMs, where attention to the source input decays over long output sequences. The paper provides both theoretical analysis and experiments to demonstrate this issue.3. To address the attention degeneration problem, the paper proposes a new "partial attention language model" (PALM) that incorporates an additional attention mechanism focused only on the source to maintain sensitivity. 4. The paper shows through experiments on machine translation, summarization, and data-to-text generation tasks that the proposed PALM model outperforms standard LMs. This supports their analysis of the weaknesses of LMs and the benefits of the modifications in PALM.In summary, the key contribution is identifying the attention degeneration problem in LMs for seq2seq tasks, analyzing the cause, and proposing a modified PALM architecture to alleviate it. The improved performance of PALM demonstrates the impact of their analysis and solution.
