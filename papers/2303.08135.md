# [Manipulate by Seeing: Creating Manipulation Controllers from Pre-Trained   Representations](https://arxiv.org/abs/2303.08135)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading, the central research question this paper addresses is: How can we effectively leverage pre-trained visual representations to directly infer robot actions, rather than just using them as state representations? 

The key hypothesis is that the latent space of pre-trained vision models already encodes semantic and task-level information that could be used to more efficiently plan robot behavior. Specifically, the paper proposes that distances in the embedding space can be used as a proxy forrobot action selection.

To summarize, the core ideas are:

- Pre-trained vision models encode useful structure (e.g. semantic relationships) in their latent spaces. 

- This latent structure can be exploited to directly infer actions, by learning a distance metric and dynamics model within the embedding space.

- Using distances and predicted states to greedily plan actions is more effective than standard approaches like behavior cloning or offline RL.

So in essence, the paper tries to show that pre-trained visual representations can do more than just encode observations - they can help infer actions too. This allows for more sample efficient robot learning.


## What is the main contribution of this paper?

 Based on my reading of the paper, the main contribution is developing a method to acquire a distance function and dynamics model by fine-tuning a pre-trained visual representation on minimal human collected data. The distance and dynamics functions can then be used with a simple greedy planner to control a robot to perform manipulation tasks. Key aspects of the contribution include:

- Showing that visual representations can help directly infer robot actions, beyond just encoding observations. This is done by using the distances expressed in the embedding space of a pre-trained network to efficiently plan behaviors. 

- Proposing a simple algorithm to acquire the distance and dynamics functions by fine-tuning the pre-trained representation on human video sequences. A contrastive learning objective is used for the distance function.

- Demonstrating substantially improved performance over traditional robot learning baselines like behavior cloning on a suite of real-world manipulation tasks. The method can also generalize to novel objects without needing robot demonstrations during training.

- Highlighting that the approach allows using low-cost, robot-free data collection from humans with a grabber tool, avoiding the need for expensive robot interaction during training.

So in summary, the main contribution is showing how pre-trained visual representations can be adapted in a simple yet effective manner to directly infer robot manipulation actions and control policies from minimal human data. This allows bypassing some key challenges like learning complex action distributions.
