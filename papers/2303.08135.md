# [Manipulate by Seeing: Creating Manipulation Controllers from Pre-Trained   Representations](https://arxiv.org/abs/2303.08135)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading, the central research question this paper addresses is: How can we effectively leverage pre-trained visual representations to directly infer robot actions, rather than just using them as state representations? The key hypothesis is that the latent space of pre-trained vision models already encodes semantic and task-level information that could be used to more efficiently plan robot behavior. Specifically, the paper proposes that distances in the embedding space can be used as a proxy forrobot action selection.To summarize, the core ideas are:- Pre-trained vision models encode useful structure (e.g. semantic relationships) in their latent spaces. - This latent structure can be exploited to directly infer actions, by learning a distance metric and dynamics model within the embedding space.- Using distances and predicted states to greedily plan actions is more effective than standard approaches like behavior cloning or offline RL.So in essence, the paper tries to show that pre-trained visual representations can do more than just encode observations - they can help infer actions too. This allows for more sample efficient robot learning.
