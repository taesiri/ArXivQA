# [Mixed Distillation Helps Smaller Language Model Better Reasoning](https://arxiv.org/abs/2312.10730)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Large language models (LLMs) like GPT-3 exhibit impressive reasoning capabilities, but their deployment is challenging due to high computational demands. 
- Smaller models trained via knowledge distillation from LLMs is a promising direction, but they still fall short of matching LLM performance on reasoning tasks.
- Prior distillation works have focused only on distilling the chain-of-thought (CoT) reasoning of LLMs in natural language, ignoring the program-of-thought (PoT) reasoning in code.

Proposed Solution:
- Introduce a mixed distillation framework that distills both the CoT and PoT capabilities from LLMs into smaller models.  
- Extract multiple reasoning paths (CoT and PoT) from the LLM on unlabeled data using few-shot prompting.
- Train the smaller model on these paths in a multi-task setting to predict labels and generate relevant reasoning.
- Sampling from both CoT and PoT paths during inference enhances reasoning performance.

Main Contributions:
- First work to utilize LLM-generated PoT paths as extra supervision signal for distillation.
- Show improved capability of smaller models to perform both CoT and PoT reasoning after mixed distillation.
- Sampling from multiple reasoning paths gives superior accuracy over single-path distillation methods.
- Experiments showmodels like Llama2 and T5-Large rival or outperform GPT-3.5 after mixed distillation, highlighting that the reasoning gap between LLMs and smaller models can be bridged. The method also generalizes across different models, dataset sizes and mathematical reasoning datasets.
