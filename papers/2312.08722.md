# [Quantifying Divergence for Human-AI Collaboration and Cognitive Trust](https://arxiv.org/abs/2312.08722)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a summary paragraph of the key points from the paper:

This paper proposes novel measures of "decision-making similarity" between humans and AI models to quantify their potential for collaboration and establish cognitive trust. Specifically, the authors calculate divergence metrics like KL divergence and Jensen-Shannon distance between label distributions provided by humans and various AI models on a textual entailment task. They then conduct a multi-stage user study where participants choose the model with the most similar label distribution to their own judgments. Participants are asked about their willingness to collaborate with and trust in their matched model. The study finds that humans tend to collaborate with models that have high confidence agreement on answers, indicated by low inverse KL divergence. However, cognitive trust also requires avoiding overconfidence when unsure, reflected through forward KL divergence. Overall, decision-making similarity correlates with collaboration likelihood, but individuals may still collaborate even if they lack full cognitive trust. This work provides new ways to approximate human preferences toward AI systems prior to real-world deployment.


## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Predicting human-AI collaboration likelihood and measuring cognitive trust are important when deploying AI systems, but prior work focuses solely on model features (e.g. accuracy) and ignores the human factor. 

- This work aims to analyze the impact of "decision-making similarity" between humans and AI models on collaboration likelihood and cognitive trust.

Methodology:
- Train diverse NLP models (tf-idf, LSTM, RoBERTa, Davinci) on natural language inference (NLI) task. 

- Propose divergence metrics on soft labels to quantify decision-making similarity: α-KL, β-KL, JSD.

- Conduct 4-stage user study:
   1) Users pick closest soft label distribution to theirs from models
   2) Align user with most similar model via JSD  
   3) Show user agreements/disagreements with aligned model
   4) User rates collaboration likelihood & cognitive trust

Key Results:
- People collaborate more with models having similar decision making processes, measured by JSD, but may not trust them as much.

- Low β-KL divergence (agreeing on answers with high confidence) most influences collaboration.  

- Cognitive trust relates to low β-KL but may also require low α-KL (avoiding overconfidence).

Main Contributions:
- First study analyzing impact of decision-making similarity on human-AI collaboration and trust.

- Comprehensive user study methodology using soft labels to quantify similarities.

- Relate different divergence metrics on labels to collaboration and trust aspects.


## Summarize the paper in one sentence.

 Here is a one-sentence summary of the paper:

The paper proposes decision-making similarity measures between humans and AI models to predict collaboration likelihood and cognitive trust, finding that people collaborate more with models that agree on confident answers yet may not fully trust them.


## What is the main contribution of this paper?

 According to the paper, the main contributions are:

1. This is the first study that investigates the impact of "decision-making similarity" (DMS)---measurable via divergence metrics on soft labels---on human-AI collaboration and cognitive trust AI.

2. The paper proposes a comprehensive, yet flexible four-staged user study to measure DMS. Although originally designed for the NLI task, it is easily adaptable to any classification task. 

3. To encourage further studies on collaboration and trust, the paper shares all resources, including the user study design, participant outputs, NLI models and predictions and related implementation.

So in summary, the main contributions are:

- Introducing the concept of "decision-making similarity" between humans and AI based on divergence metrics calculated on soft labels
- Designing a novel user study to measure this similarity and relate it to collaboration and trust 
- Sharing resources to enable further research in this direction


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and keywords associated with it are:

- Decision-making similarity (DMS)
- Divergence metrics (KL divergence, Jensen-Shannon divergence)
- Soft labels
- Collaboration likelihood
- Cognitive trust
- User study design
- Natural language inference (NLI)
- Textual entailment
- Models (tf-idf, LSTM, RoBERTa, Davinci)
- Forward KL divergence
- Inverse KL divergence 
- User-model alignment
- Human-AI interaction
- Human factors
- Model confidence
- Model accuracy
- Label distribution

The paper proposes using divergence metrics calculated on soft labels from models and humans to quantify their decision-making similarities. It then links these metrics to collaboration likelihood and cognitive trust through a comprehensive user study. The key goal is to understand how human-AI similarities in the decision-making process influence the collaboration and trust dynamics.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in this paper:

1. The paper proposes several "decision-making similarity" measures based on divergence metrics calculated over the labels acquired from humans and models. What are some advantages and disadvantages of using divergence metrics compared to other similarity measures like correlation or agreement percentage?

2. The user study has four main stages - training, quiz, annotation, and dynamic survey. What is the purpose of each stage and how do they fit together to evaluate the research questions? Could any stages be removed or combined?

3. The paper experiments with a diverse set of models ranging from statistical models like TF-IDF to large language models like RoBERTa. Why is model diversity important for this research? What impact could the choice of models have on the conclusions?  

4. The authors align each user with their "most similar model" using Jensen-Shannon divergence (JSD) between label distributions. What are some pros and cons of using JSD versus other divergences? Could an alternative matching strategy be used?

5. The results suggest different divergence metrics relate differently to collaboration likelihood versus cognitive trust. What explanations are provided for why inverse KL is most predictive of collaboration while JSD is most predictive of trust? How could this be tested further?

6. The sample size of 100 participants is justified by citing HCI literature standards. However, what are some limitations of the sample size and demographics? Would a larger or more diverse sample change any conclusions?

7. The aggregated human labels ("aggUSER") are compared to the average user ("avgUSER") from the study. What differences emerge and what explanations are provided? What cautions does this raise for using aggregated labels?

8. The study is performed on natural language inference using the SNLI dataset. How might the choice of task impact the results and what other tasks could be relevant to explore? Would the conclusions generalize?

9. One finding is that people may collaborate with AI even if they have low cognitive trust in it. What possible explanations are discussed for this result and how surprising is it? What future work could further analyze this?

10. The paper focuses solely on analyzing decision-making similarity for collaboration and trust. What other relevant dimensions of human-AI interaction should be considered and how could they be incorporated?
