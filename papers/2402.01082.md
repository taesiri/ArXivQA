# [Salsa Fresca: Angular Embeddings and Pre-Training for ML Attacks on   Learning With Errors](https://arxiv.org/abs/2402.01082)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
The paper focuses on improving machine learning (ML) attacks on the Learning with Errors (LWE) problem, an important mathematical problem underlying post-quantum cryptography schemes that are believed to be resistant to quantum computers. Prior ML-based attacks were able to recover sparse binary/ternary secrets for medium-sized LWE instances (n=512) but have limitations in terms of preprocessing time (takes days to reduce a single matrix), model architecture (limited to n=512 due to sequence length), and number of training examples needed (millions).

Proposed Solutions: 
The paper proposes improvements in three key areas - faster preprocessing, better model architecture, and more sample-efficient training. 

1) For preprocessing, they achieve a 25x speedup by using the Flatter algorithm combined with interleaved BKZ and Polish. This enables scaling to n=1024.

2) They propose a simpler encoder-only transformer model coupled with an angular embedding input representation that encodes integers as points on a circle. This halves the sequence length, allowing scaling to n=768 and 1024, while also improving performance.

3) They introduce the first use of pre-training for LWE attacks, which improves sample efficiency by 10x and enables secret recovery with only 1 million examples instead of 4 million.

Main Contributions:
- First preprocessing technique to scale ML-based LWE attacks to dimension 1024
- New model architecture that scales to 768 and 1024 dimensions, and recovers more secrets in 512 dimensions
- Demonstrate for the first time that pre-training improves sample efficiency and cost of ML attacks on LWE
- Recover secrets for n=1024 dimension LWE, the smallest dimension used in homomorphic encryption schemes, for the first time

Overall the improvements allow attacks on more difficult LWE instances (higher dimension, lower modulus, higher hamming weight secrets) using less compute resources (250x fewer CPU hours for preprocessing) and in less time compared to prior work. The techniques open up the potential for more practical ML-based attacks on real-world LWE encryption schemes.
