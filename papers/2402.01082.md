# [Salsa Fresca: Angular Embeddings and Pre-Training for ML Attacks on   Learning With Errors](https://arxiv.org/abs/2402.01082)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
The paper focuses on improving machine learning (ML) attacks on the Learning with Errors (LWE) problem, an important mathematical problem underlying post-quantum cryptography schemes that are believed to be resistant to quantum computers. Prior ML-based attacks were able to recover sparse binary/ternary secrets for medium-sized LWE instances (n=512) but have limitations in terms of preprocessing time (takes days to reduce a single matrix), model architecture (limited to n=512 due to sequence length), and number of training examples needed (millions).

Proposed Solutions: 
The paper proposes improvements in three key areas - faster preprocessing, better model architecture, and more sample-efficient training. 

1) For preprocessing, they achieve a 25x speedup by using the Flatter algorithm combined with interleaved BKZ and Polish. This enables scaling to n=1024.

2) They propose a simpler encoder-only transformer model coupled with an angular embedding input representation that encodes integers as points on a circle. This halves the sequence length, allowing scaling to n=768 and 1024, while also improving performance.

3) They introduce the first use of pre-training for LWE attacks, which improves sample efficiency by 10x and enables secret recovery with only 1 million examples instead of 4 million.

Main Contributions:
- First preprocessing technique to scale ML-based LWE attacks to dimension 1024
- New model architecture that scales to 768 and 1024 dimensions, and recovers more secrets in 512 dimensions
- Demonstrate for the first time that pre-training improves sample efficiency and cost of ML attacks on LWE
- Recover secrets for n=1024 dimension LWE, the smallest dimension used in homomorphic encryption schemes, for the first time

Overall the improvements allow attacks on more difficult LWE instances (higher dimension, lower modulus, higher hamming weight secrets) using less compute resources (250x fewer CPU hours for preprocessing) and in less time compared to prior work. The techniques open up the potential for more practical ML-based attacks on real-world LWE encryption schemes.


## Summarize the paper in one sentence.

 This paper proposes three key improvements (better preprocessing, angular embeddings, and model pre-training) to machine learning attacks on learning with errors cryptosystems, enabling recovery of secrets in higher dimensions and with fewer samples than prior work.


## What is the main contribution of this paper?

 The main contributions of this paper are:

1) 25x faster preprocessing for generating training data by using the Flatter lattice reduction algorithm and interleaving it with polishing and BKZ. This reduces the time to preprocess each LWE matrix.

2) A simpler encoder-only transformer architecture coupled with an angular embedding for the model inputs. This halves the input sequence length, speeds up training by 2.4x, and improves model performance.

3) The first use of pre-training for machine learning attacks on LWE, which improves sample efficiency of models by 10x and further reduces preprocessing requirements.

4) Scaling up machine learning attacks on LWE to larger dimensions like n=1024 for the first time, enabling attacks on parameter settings used in homomorphic encryption schemes.

So in summary, through improvements to preprocessing, the model architecture, and training techniques like pre-training, this paper makes machine learning attacks more practical against LWE with settings used in real-world applications. The attacks are also scaled up to larger dimension sizes than attacked previously using machine learning.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key keywords and terms associated with this paper include:

- Learning with Errors (LWE)
- Post-quantum cryptography (PQC)
- Machine learning (ML) attacks
- Sparse binary/ternary secrets
- Preprocessing (lattice reduction)
- Model architecture (encoder-only transformer, angular embedding) 
- Sample efficiency
- Model pre-training
- NoMod factor
- Homomorphic encryption (HE)
- BKZ, fplll (lattice reduction algorithms)

The paper proposes improvements to prior ML-based attacks on the Learning with Errors problem, which underlies post-quantum cryptography schemes. The key innovations include faster preprocessing, a more efficient model architecture, training with fewer samples, and model pre-training. These advances enable attacks on larger dimension LWE instances than prior work, representing progress towards breaking parameters used in homomorphic encryption schemes. The NoMod factor is introduced as a metric correlating with attack success. Overall, the paper demonstrates how techniques like pre-training and better model inductive bias can improve ML-based cryptanalysis.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the methods proposed in this paper:

1. The paper claims a 25x speedup in preprocessing time compared to prior work. What specific changes to the preprocessing pipeline enabled these speedups? How was the balance between reduction quality and error penalty optimized?

2. The angular embedding proposed encodes integers as points on a unit circle before feeding them into the transformer model. What is the intuition behind this representation? How does it help the model learn modular arithmetic better compared to a learned embedding? 

3. The encoder-only transformer is much simpler than prior encoder-decoder architectures. What motivated this design choice? What modifications were made to the loss function and output head to enable an encoder-only model? How does this simplicity translate to faster training and inference?

4. Pre-training is used for the first time in this paper to improve sample efficiency for LWE attacks. Explain the pre-training setup, loss formulation, and dataset used. Why is using many different secrets during pre-training expected to improve generalization capability? 

5. The paper demonstrates attacks on LWE with parameters closer to those used in homomorphic encryption schemes. What modifications to the method were crucial to scale up to these larger parameter settings? What are the practical implications if this attack scales further?

6. NoMod is introduced as a metric highly correlated with secret recovery success. Explain how NoMod is calculated. What does a high NoMod signify about the difficulty of the LWE instance? How does the angular embedding qualitatively change the NoMod requirements?

7. The pre-training in this paper uses only one preprocessed dataset. Propose some improvements to the diversity and realism of pre-training data that could further improve results.

8. The parameters tested are still far from homomorphic encryption settings in practice. What obstacles need to be overcome before this attack becomes practical and what avenues seem most promising to you?

9. The angular embedding demonstrates improved performance over regular embeddings. Propose some visualization or quantitative experiments that could further probe why and how much angular embeddings help.

10. The pre-training results are mixed in that more pre-training does not always help. What other pre-training formulations could you propose that might lead to more consistent gains as the model sees more data?
