# [AssetField: Assets Mining and Reconfiguration in Ground Feature Plane   Representation](https://arxiv.org/abs/2303.13953)

## What is the central research question or hypothesis that this paper addresses?

 The central hypothesis of this paper is that representing 3D scenes with explicit ground feature planes enables intuitive object-level and group-level editing of neural radiance fields. 

The key claims are:

- Factorizing radiance fields into ground feature planes and a shared vertical axis encourages ground planes to encode more informative scene contents and layout.

- Modeling scene density, color, and semantics in integrated RGB-DINO planes makes them more robust to vertical object displacement, enabling cross-scene analogue discovery. 

- Assets can be directly mined and grouped from the ground planes in an unsupervised manner. 

- Storing template feature patches in a shared asset library allows efficient group editing by manipulating category templates.

- The ground plane representation provides an intuitive "canvas" for users to manipulate individual objects or entire scene layouts.

- Realistic novel views can be rendered for new scene configurations by combining edited ground planes with original vertical axes and decoder networks.

In summary, the ground plane radiance field representation enables object-level and group-level editing flexibility reminiscent of traditional graphics pipelines, while retaining the realism of neural rendering.


## What is the main contribution of this paper?

 The main contribution of this paper is proposing AssetField, a novel neural scene representation that can learn a set of object-aware ground feature planes to represent a 3D scene. Key aspects of AssetField include:

- It represents a 3D scene with a 2D ground feature plane aligned with the bird's eye view and a shared vertical feature axis. This allows intuitive manipulation and editing of objects on the ground plane.

- It can discover and extract assets (object neural representations) from the learned ground feature planes in an unsupervised manner. The assets can be grouped into categories to enable efficient group editing. 

- It constructs a cross-scene asset library by sharing vertical axes and decoders across scenes. This allows discovering common objects, expanding the library, and reusing assets for scene composition.

- The ground feature planes are informative, encoding density, color and semantics. This enables tasks like assets mining, categorization, and layout estimation directly on the planes.

- It provides a flexible way to edit and reconfigure scenes by manipulating assets on the ground plane. Operations like insertion, deletion, translation, rotation, duplication are supported at object, category and scene levels.

- It can render high-quality novel views of new scene configurations composed from the asset library, without needing to retrain models.

In summary, AssetField resembles traditional 3D modeling workflows that use reusable asset libraries, while providing neural rendering, allowing easy and intuitive editing of scenes to compose new configurations. The explicit ground feature planes are more user-friendly compared to other implicit neural representations.
