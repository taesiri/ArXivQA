# [Seasoning Model Soups for Robustness to Adversarial and Natural   Distribution Shifts](https://arxiv.org/abs/2302.10164)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question is:

How can we create image classifiers that are robust to diverse adversarial perturbations and natural distribution shifts by combining multiple models with different types of robustness?

The key ideas and hypotheses explored in this paper are:

- Adversarial training produces models robust to a specific threat model (e.g. L-infinity bounded perturbations), but they remain vulnerable to other unseen attacks. 

- Interpolating the parameters of models specialized to different threat models (L-infinity, L2, L1 norms) can allow smoothly trading off robustness between them.

- Such interpolated "model soups" can potentially achieve robustness to multiple perturbation types without explicitly training on all of them jointly.

- Model soups with diverse robustness properties could help adapt classifiers to perform well under natural distribution shifts with few examples. 

- Combining nominal, adversarially robust, and differently fine-tuned models in a soup provides flexibility to balance accuracy on the original data and robustness to shifts.

So in summary, the central hypothesis is that creating model soups by merging classifiers with complementary robustness can yield improved robustness to diverse adversarial and natural distribution shifts compared to individual specialized models. The paper explores this via interpolating models robust to different Lp norms.
