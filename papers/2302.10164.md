# [Seasoning Model Soups for Robustness to Adversarial and Natural   Distribution Shifts](https://arxiv.org/abs/2302.10164)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question is:

How can we create image classifiers that are robust to diverse adversarial perturbations and natural distribution shifts by combining multiple models with different types of robustness?

The key ideas and hypotheses explored in this paper are:

- Adversarial training produces models robust to a specific threat model (e.g. L-infinity bounded perturbations), but they remain vulnerable to other unseen attacks. 

- Interpolating the parameters of models specialized to different threat models (L-infinity, L2, L1 norms) can allow smoothly trading off robustness between them.

- Such interpolated "model soups" can potentially achieve robustness to multiple perturbation types without explicitly training on all of them jointly.

- Model soups with diverse robustness properties could help adapt classifiers to perform well under natural distribution shifts with few examples. 

- Combining nominal, adversarially robust, and differently fine-tuned models in a soup provides flexibility to balance accuracy on the original data and robustness to shifts.

So in summary, the central hypothesis is that creating model soups by merging classifiers with complementary robustness can yield improved robustness to diverse adversarial and natural distribution shifts compared to individual specialized models. The paper explores this via interpolating models robust to different Lp norms.


## What is the main contribution of this paper?

 Based on the abstract, it seems the main contributions of this paper are:

1. Showing how to create adversarial robust "model soups" (i.e. linear combinations of model parameters) that can smoothly trade off robustness to different l_p-norm bounded adversaries without needing to train on all of them simultaneously.  

2. Demonstrating that these model soups can be used to quickly adapt classifiers to new unseen distribution shifts (like ImageNet variants) using just a few examples, by tuning the soup weighting.

3. Analysis showing which types of classifiers (e.g. nominally trained vs l_infty trained) are most useful for performance on different ImageNet variant datasets.

4. Finding that extrapolation outside the convex hull of the model parameters can sometimes yield even better robustness on individual threat models compared to the specialized models.

5. Showing a single soup can be found that works reasonably well across multiple ImageNet variants, outperforming adversarial training and self-supervised baselines.

In summary, the main contribution seems to be a new method for creating model soups from adversarially robust classifiers that enables flexible trade-offs between robustness types and rapid adaptation to new distributions.
