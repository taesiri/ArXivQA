# [Approximate Attributions for Off-the-Shelf Siamese Transformers](https://arxiv.org/abs/2402.02883)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Siamese encoders like sentence transformers are widely used but little understood. Established attribution methods cannot be applied to them since they compare two inputs rather than processing one.
- The authors previously proposed an attribution method for Siamese encoders using integrated Jacobians, but it requires adjusting the model architecture and fine-tuning, so cannot be directly applied to off-the-shelf models.

Proposed Solutions:
- Show that attributions can be computed with cosine similarity instead of dot product, allowing a model with exact attribution ability to retain original model performance. 
- Propose approximate attributions for off-the-shelf models without needing to adjust or fine-tune the model. These lack theoretical guarantees but often agree reasonably with exact attributions.

Key Contributions:
- An interpretable Siamese encoder with attribution ability that retains performance of original model.
- A method to compute approximate attributions for any off-the-shelf Siamese encoder model.
- Analysis showing approximate attributions agree partly with exact ones, but have limitations in accuracy.
- Insights into linguistic aspects that Siamese transformers focus on, including confirming they ignore negation and exhibit lexical bias.

In summary, the key advance is enabling attribution methods to be applied to off-the-shelf Siamese encoder models for the first time, providing insights into these widely used but little understood models. The tradeoff is that approximate attributions lack guarantees and accuracy compared to an interpretable model designed for attribution.


## Summarize the paper in one sentence.

 This paper proposes methods to compute exact attributions for adjusted Siamese transformers and approximate attributions for off-the-shelf ones, analyzes their reliability, and uses them to gain insights into which linguistic aspects Siamese transformers focus on.


## What is the main contribution of this paper?

 The main contributions of this paper are twofold:

1. The authors show that it is possible to compute attributions for Siamese transformer models using cosine similarity as the similarity measure, while retaining the original model's predictive performance. This removes a key limitation of their previous attribution method which required using a dot product instead of cosine similarity.

2. The authors propose a method to compute approximate attributions for off-the-shelf Siamese encoder models without needing to fine-tune the models. Unlike exact attributions, these approximate attributions do not come with theoretical guarantees but the evaluation shows they often agree reasonably well with exact attributions, especially for deeper intermediate representations. This enables attributions to be derived for readily available off-the-shelf models.

In summary, the two key contributions are: (1) an improved exact attribution method, and (2) a new approximate attribution method applicable to unmodified off-the-shelf Siamese transformers.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and concepts include:

- Siamese encoders/transformers - The class of deep learning models that compare embeddings of two inputs using the same encoder, which this paper analyzes.

- Attribution methods - Methods to explain which parts of the input a model bases its decisions on. The paper proposes methods for attributing predictions of Siamese encoders. 

- Integrated Jacobians - An attribution method proposed in the paper that extends integrated gradients to models with two inputs.

- Exact attributions - Attributions that faithfully reflect the model's decision process and sum up to the model's prediction. Requires adjusted model architecture.

- Approximate attributions - Attributions that can be computed for off-the-shelf Siamese encoders without adjustments. Do not perfectly match predictions.

- Reference inputs - Neutral inputs required for the derivation of integrated Jacobians.

- Syntactic relations - The paper analyzes which syntactic relationships between words the models focus on.

- Negation, adjectives, lexical bias - Other linguistic phenomena analyzed using the attributions.

In summary, the key focus is on proposing and evaluating attribution methods for Siamese encoders/transformers to understand their decision making by analyzing different linguistic aspects.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in the paper:

1. The paper proposes two key extensions to the original integrated Jacobians method: using cosine similarity instead of a dot product, and using approximate reference inputs instead of shifted embeddings. How do these changes allow for computing attributions on off-the-shelf models without fine-tuning? What are the tradeoffs?

2. The paper evaluates agreement between approximate and exact attributions. What metrics are used for this evaluation (Spearman correlation, Pearson correlation, Jaccard index)? How does agreement vary by model layer and sentence similarity score? What does this suggest about the reliability of approximate attributions?

3. Section 4.2 analyzes the contribution of reference similarities and the reference term to approximate attributions. What do the results show about the magnitude of these terms and the validity of the approximation in Eq. 4? In what percentage of cases can the approximation be considered unsafe?

4. Fig. 6 shows attribution sums can be greater than the maximum possible score. Does this occur more often for approximate or exact attributions? What does this suggest about positive vs negative attributions? Is there evidence one is more reliable?

5. The analysis of syntactic relations shows models primarily attend to matching roles. But do the models agree in relative importance of mixed syntactic pairs like subject-object? What does this suggest about the models' decision making processes?  

6. The negation analysis concludes transformers mostly ignore negation. What evidence supports this from the attribution amounts and signs for the “not” tokens? How could the method be extended to better handle negation?

7. For the adjective pairs, what results show the models often fail to assign sufficiently negative attributions to opposites? Would fine-tuning on such pairs improve this? How might the construction of the adjective sentence pairs be improved?

8. The identified lexical biases seem consistent across models. What metrics confirm the biases are significant (average values, standard deviations, attribution ranks)? How could such biases be mitigated?

9. What limitations remain regarding computational efficiency and contextualization when computing attributions? How could these be addressed in future work?

10. What open questions remain about the relationship between approximate and exact attributions? How might future work establish more rigorous guarantees or accuracy standards for the approximate attributions?
