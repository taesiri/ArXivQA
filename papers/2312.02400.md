# [Auto DP-SGD: Dual Improvements of Privacy and Accuracy via Automatic   Clipping Threshold and Noise Multiplier Estimation](https://arxiv.org/abs/2312.02400)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a high-quality paragraph summarizing the key points of the paper:

This paper proposes a new differentially private stochastic gradient descent algorithm called Auto DP-SGD that automatically estimates the clipping threshold and noise multiplier to improve the trade-off between privacy and accuracy. The algorithm has two main components: (1) An automatic clipping threshold estimation that adapts the threshold to the decreasing gradient norms during training, avoiding excess noise while lowering sensitivity to improve privacy. (2) An automatic noise multiplier decay that reduces noise over epochs akin to learning rate decay in non-private deep learning. The authors propose and compare four decay mechanisms: linear, time, step and exponential decay. Using the tight privacy accountant of truncated concentrated differential privacy, they formally analyze the privacy guarantee of Auto DP-SGD under different decay types. Through extensive experiments on MNIST, CIFAR10/100 and text datasets using both custom and pretrained models, they demonstrate state-of-the-art performance - Auto DP-SGD with step decay improves accuracy by up to 6.73% and reduces the privacy budget by up to 94.9% compared to previous DP-SGD algorithms. The results also show the possibility of further enhancing privacy using lower scale factors and learning rate schedulers without compromising utility. Overall, the proposed Auto DP-SGD algorithm successfully bridges the accuracy gap between non-private and private deep learning while ensuring strong privacy guarantees.
