# [Compositional Generative Modeling: A Single Model is Not All You Need](https://arxiv.org/abs/2402.01103)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Recent AI research has focused on scaling up monolithic generative models trained on massive amounts of data. However, these models still exhibit poor reasoning, hallucination, and lack of commonsense. 
- Training such gigantic models is extremely expensive, costing hundreds of millions of dollars. Inference is also expensive, requiring large clusters.
- Adapting these models to new distributions is difficult, requiring fine-tuning datasets and clusters. Existing adaptation methods using language instructions are limited.

Proposed Solution: 
- Construct generative systems compositionally from smaller, simpler generative models. Each model captures part of the overall distribution.
- This enables more data-efficient learning, as each factor is simpler. The composition generalizes to unseen variable combinations.
- It also enables programming new generative models by recombining components, generalizing to new tasks without any training.
- The compositional components can often be discovered from data in an unsupervised manner.

Main Contributions:
- Show compositional modeling is more data-efficient in distribution learning.
- Demonstrate compositional programming for unseen tasks like planning, constraint satisfaction, video generation.
- Show unsupervised discovery of compositional factors in images, dynamics.
- Discuss implementations using energy-based models and connections to diffusion sampling.
- Advocate for compositionality over scale as a direction for generative modeling.

The key ideas are that compositional generative modeling is more efficient, flexible and generalizable compared to scaling up monolithic models. It advocates for an orthogonal direction of research from the mainstream focus on model scaling.
