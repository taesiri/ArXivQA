# [Learning to Poke by Poking: Experiential Learning of Intuitive Physics](https://arxiv.org/abs/1606.07419)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be:Can a robot learn an intuitive model of physics by interacting with objects through random poking?The authors are interested in investigating whether a robot can gather its own experience by randomly poking objects, and use this experience to learn a model that captures intuitive physics - i.e. how the robot's actions affect the motion of objects. Specifically, they propose training neural network models on a dataset of images before and after random pokes made by the robot. They jointly train an inverse model to predict the poke given two images, and a forward model to predict the second image given the first image and poke. The central hypothesis seems to be that this joint training approach will allow the robot to learn useful visual features and dynamics models from raw pixel inputs in a completely self-supervised manner. The learned models can then be used for multi-step decision making and displacing objects to target locations.In summary, the key question is whether random autonomous interaction alone is enough for a robot to acquire intuitive models of physics, without any other human supervision or prior knowledge. The authors propose joint inverse and forward modeling as a way to achieve this.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contributions appear to be:- Proposing a method for a robot to learn intuitive models of physics through self-supervised interaction with objects, without any predefined goals or reward functions. The robot pokes objects randomly and uses the visual states before and after each poke, along with the executed poke action, to train neural network models.- Introducing a joint training approach that combines inverse models (action -> state change) and forward models (state + action -> next state) to take advantage of their complementary strengths. The inverse model provides supervision for learning useful visual features, while the forward model regularizes the features and enables longer-term planning. - Conducting real-world robot experiments where the robot poked objects for over 400 hours to collect a dataset of 100K+ pokes on 16 objects. The learned models are shown to be useful for poking new objects into desired goal configurations substantially different than the training distribution.- Providing simulation experiments that analyze the joint training approach and lend support to the claim that the forward model acts as a regularizer for the inverse model features.- Demonstrating that the robot can manipulate novel objects and push objects around obstacles, indicating a degree of generalization beyond the exact conditions seen during training.In summary, the main contribution appears to be an self-supervised approach for real robots to learn useful intuitive physics models from scratch, using only raw visual inputs from hours of unstructured interaction. The joint training of inverse and forward models is shown to learn superior features and dynamics models compared to alternative approaches.


## How does this paper compare to other research in the same field?

This paper presents an interesting approach for learning intuitive models of physics from visual observations of a robot interacting with objects. Here are some ways I see it relating to other work:- It tackles learning from raw image observations of real world robot manipulation, as opposed to many prior works that learn models in simulated environments. Manipulating real world objects that can deform, compress, etc. poses unique challenges not found in simulation.- The idea of jointly training inverse and forward models to learn useful representations is novel. Many prior works have focused on just inverse or just forward models. Showing benefits of joint training is an intriguing result.- Learning without predefined rewards or goals is reminiscent of unsupervised and self-supervised approaches in other domains like computer vision. The idea of pre-training an intuitive physics model on unlabeled interaction data that can later be used for planning goal-directed actions is appealing.- Most prior work on pushing/poking for manipulation relies on engineered visual features or object pose information. Learning directly from images is more flexible but also more challenging, so demonstrating results on real images is significant.- It builds on prior works that hypothesize humans build intuitive physics models through self-supervised interaction with the world. Testing that idea on a robot platform is exciting and extends those theoretical models.- The general idea of model-based control has a long history, but new deep learning approaches like this paper move the state of the art forward.In summary, I see the paper making nice contributions in terms of tackling real world vision-based manipulation, using joint inverse/forward model training, and learning without predefined goals. The results indicate progress on long-standing challenges in intuitive physics and model-based control.


## What future research directions do the authors suggest?

Based on my reading of the paper, some of the main future research directions suggested by the authors are:- Investigating alternatives to greedy planning, such as using the learned forward model for longer-horizon planning of pokes. The authors note that greedy planning can lead to suboptimal zig-zag trajectories.- Exploring continuous-time control with smaller pokes, which are likely more predictable than the large pokes used in this work. The authors suggest this could lead to better pose control.- Evaluating how the proposed approach scales to more complex environments, diverse objects, different manipulation skills, and non-manipulation tasks like navigation.- Developing better strategies for data collection than random poking, such as incorporating active learning. The authors provided some preliminary experiments suggesting active data collection improves results.- Using the forward model for planning, which the authors suggest may have better generalizability than the inverse model. However, their initial attempts did not outperform the inverse model.- Improving the active data sampling scheme to provide more direct feedback when the model's predicted poke does not achieve the desired outcome.- Incorporating continuous-time visual observations during poke execution, rather than just before and after images. This could enable better manipulation control.- Exploring alternative model architectures, training procedures, and regularization methods to improve performance.- Comparing to and combining intuitive physics models with more simulator/Newtonian physics-based approaches.- Evaluating the intuitive physics approach on tasks like cloth and rope manipulation.So in summary, the key directions are around scaling up the approach, improving planning, incorporating continuous control, refining the training process, and benchmarking on more complex domains.


## Summarize the paper in one paragraph.

Here is a one paragraph summary of the paper:The paper proposes an approach for a robot to learn an intuitive model of physics from its own experience interacting with objects by poking. The robot pokes objects placed in front of it in a random manner and records images of the scene before and after executing each poke. This interaction data of over 100K pokes is used to train a deep neural network to jointly learn a forward model that predicts the next visual state given the current state and action, and an inverse model that predicts the action given an initial and goal visual state. The inverse model provides supervision for learning useful abstract features from images, and the forward model acts as a regularizer while operating in this learned feature space. Experiments show the robot can use the learned model for multi-step decision making to move objects to target locations, even generalizing to novel objects and large displacements not seen during training. The approach is inspired by cognitive science theories that hypothesize humans build intuitive physics models through uncontrolled childhood object interactions.
