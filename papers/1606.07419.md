# [Learning to Poke by Poking: Experiential Learning of Intuitive Physics](https://arxiv.org/abs/1606.07419)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be:Can a robot learn an intuitive model of physics by interacting with objects through random poking?The authors are interested in investigating whether a robot can gather its own experience by randomly poking objects, and use this experience to learn a model that captures intuitive physics - i.e. how the robot's actions affect the motion of objects. Specifically, they propose training neural network models on a dataset of images before and after random pokes made by the robot. They jointly train an inverse model to predict the poke given two images, and a forward model to predict the second image given the first image and poke. The central hypothesis seems to be that this joint training approach will allow the robot to learn useful visual features and dynamics models from raw pixel inputs in a completely self-supervised manner. The learned models can then be used for multi-step decision making and displacing objects to target locations.In summary, the key question is whether random autonomous interaction alone is enough for a robot to acquire intuitive models of physics, without any other human supervision or prior knowledge. The authors propose joint inverse and forward modeling as a way to achieve this.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contributions appear to be:- Proposing a method for a robot to learn intuitive models of physics through self-supervised interaction with objects, without any predefined goals or reward functions. The robot pokes objects randomly and uses the visual states before and after each poke, along with the executed poke action, to train neural network models.- Introducing a joint training approach that combines inverse models (action -> state change) and forward models (state + action -> next state) to take advantage of their complementary strengths. The inverse model provides supervision for learning useful visual features, while the forward model regularizes the features and enables longer-term planning. - Conducting real-world robot experiments where the robot poked objects for over 400 hours to collect a dataset of 100K+ pokes on 16 objects. The learned models are shown to be useful for poking new objects into desired goal configurations substantially different than the training distribution.- Providing simulation experiments that analyze the joint training approach and lend support to the claim that the forward model acts as a regularizer for the inverse model features.- Demonstrating that the robot can manipulate novel objects and push objects around obstacles, indicating a degree of generalization beyond the exact conditions seen during training.In summary, the main contribution appears to be an self-supervised approach for real robots to learn useful intuitive physics models from scratch, using only raw visual inputs from hours of unstructured interaction. The joint training of inverse and forward models is shown to learn superior features and dynamics models compared to alternative approaches.
