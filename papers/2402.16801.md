# [Craftax: A Lightning-Fast Benchmark for Open-Ended Reinforcement   Learning](https://arxiv.org/abs/2402.16801)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
Existing reinforcement learning benchmarks for investigating open-ended learning either run too slowly for meaningful research without large computational resources (e.g. Crafter, NetHack, Minecraft) or are not complex enough to pose a significant challenge (e.g. Minigrid, Procgen). There is a need for a fast yet complex benchmark to facilitate research into open-ended reinforcement learning algorithms involving exploration, continual learning, long-term planning, etc.

Proposed Solution:
The authors present Craftax, a lightning fast open-world survival game benchmark implemented in JAX. It builds upon Crafter but with significantly extended mechanics inspired by NetHack and roguelike games. The key components are:

1) Craftax-Classic: A 250x faster reimplementation of Crafter in JAX that runs an end-to-end PPO training in under an hour on 1 GPU.

2) Main Craftax benchmark: Contains 9 procedurally generated floors with diverse terrain, enemies and challenges testing deep exploration, generalisation and long-term reasoning. Unique features include complex combat, enchantments, attributes, potions and a boss fight.

Key Contributions:

- Craftax-Classic: Validates JAX implementation, runs 250x faster than Crafter 

- Main Craftax: Significantly more complex than Crafter, harder than NetHack, runs 170x faster than Crafter

- Evaluation framework: 
    - Craftax-1B: 1 billion timesteps to test exploration, planning, continual learning
    - Craftax-1M: 1 million timesteps for sample efficiency

- Analysis showing existing algorithms fail to solve Craftax, presenting opportunity for impactful open-ended RL research

The aim is to facilitate experimentation on a complex open-ended benchmark with limited compute, to help drive progress in reinforcement learning.
