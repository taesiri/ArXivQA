# [Interpretations are useful: penalizing explanations to align neural   networks with prior knowledge](https://arxiv.org/abs/1909.13584)

## What is the central research question or hypothesis that this paper addresses?

 The central research question this paper addresses is: how can we leverage interpretations/explanations of neural networks to improve their performance and align them with prior knowledge? 

Specifically, the paper proposes a method called "contextual decomposition explanation penalization" (CDEP) that allows inserting domain knowledge into neural network models by penalizing their explanations. The key hypothesis is that by penalizing explanations to match prior knowledge provided by the user, the neural network can be regularized to not only make the right predictions but make them for the right reasons.

In summary, the central research question is how to use explanations of neural networks to improve their performance by aligning them with prior knowledge, and the proposed method CDEP answers this by letting users directly penalize feature importances and interactions in the neural network's explanations.


## What is the main contribution of this paper?

 Based on my reading of the paper, the main contribution is the proposal of a method called contextual decomposition explanation penalization (CDEP). CDEP enables inserting domain knowledge into a deep learning model to ignore spurious correlations, correct errors, and generalize better. 

Specifically, CDEP allows penalizing the explanations (i.e. feature importances) of a model during training, alongside the normal prediction loss. By penalizing explanations that don't match some prior knowledge provided by the user, the model is forced to not only make the right predictions but make them for the "right reasons" that align with the prior knowledge.

The key ideas of CDEP include:

- Augmenting the standard neural network loss function with an additional "explanation loss" term that penalizes explanations that differ from some provided targets.

- Using the contextual decomposition (CD) method to generate explanations for penalization. CD allows penalizing interactions between features, not just individual features.

- Showing that CDEP enables models to ignore spurious correlations, correct errors, and generalize better across a diverse set of tasks and datasets.

So in summary, the main contribution is presenting a general framework (CDEP) for inserting domain knowledge into neural networks by penalizing explanations during training to improve model performance. The method is shown to be effective on multiple real-world tasks.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 The paper proposes a method called Contextual Decomposition Explanation Penalization (CDEP) to incorporate domain knowledge into deep neural networks by penalizing explanations during training. The key idea is to force models to not only make the right predictions but also for the right reasons aligned with domain knowledge, by adding a penalty term to the loss function that compares the model's explanations to target explanations representing domain knowledge. The method allows inserting knowledge to ignore spurious correlations, correct errors, and improve generalization. Experiments on image, text and tabular data show CDEP can increase model performance.

In one sentence: The paper proposes a method to insert domain knowledge into neural networks by penalizing explanations during training to align them with target explanations representing prior knowledge.


## How does this paper compare to other research in the same field?

 This paper introduces a new method called contextual decomposition explanation penalization (CDEP) for inserting domain knowledge into deep neural networks to help them learn better representations and achieve higher accuracy. Here are some key ways it compares to other related work:

- Most prior work on explaining neural networks focused only on interpreting the models, not improving them. CDEP takes the next step of using explanations to directly improve model performance.

- Some prior work has penalized gradient-based explanations to try to align models with human rationales or preferences. However, CDEP is the first to penalize contextual decomposition explanations, which better capture higher-order interactions between features. This allows penalizing more complex explainable concepts.

- CDEP is computationally more efficient than gradient-based explanation penalization methods. It allows quicker fine-tuning of pretrained models by freezing early layers. It also has lower memory requirements for training.

- The paper demonstrates CDEP across a diverse set of tasks - image classification, digit classification, text classification, risk prediction. It shows effectiveness in combating spurious correlations, inductive bias, and fairness issues. This is a broader demonstration than prior work.

- CDEP is model-agnostic and can work with different network architectures like CNNs and LSTMs. The only requirement is that the explanation method be differentiable.

In summary, CDEP makes both conceptual and practical advances over related work on improving deep learning models with explanations. It enables more complex feature regularization, demonstrates effectiveness across diverse tasks, and does so efficiently. This represents an important advance in interpretable and trustworthy AI.


## What future research directions do the authors suggest?

 The paper suggests several areas for future work:

- Extending CDEP to more complex settings and incorporating more fine-grained explanations/interaction penalizations. The paper focuses on applying CDEP in relatively simple settings with mostly binary or categorical features. The authors suggest exploring how to apply CDEP when features and explanations are more complex.

- Incorporating more types of prior knowledge into the explanation targets. The paper gives some examples of ways to encode prior knowledge (e.g. rules, human annotations), but suggests exploring additional ways to incorporate knowledge, like from domain experts. 

- Exploring alternate ways to determine the regularization weight Î», such as setting it proportional to the normal training loss for more stability.

- Applying CDEP in more domains like biology or physics where there is rich domain knowledge available. The effectiveness will depend on the quality of the prior knowledge used.

- Pushing forward research on rigorously using interpretability methods. The authors suggest their work is a step towards using explanation methods more rigorously, and hope it motivates more work in this area.

In summary, the main future directions are: exploring more complex feature explanations, incorporating diverse domain knowledge, using CDEP in more domains, and advancing research on properly leveraging interpretations. The authors position CDEP as an initial step towards enabling models to learn in a more trustworthy, interpretable way.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the paper:

The paper proposes a method called contextual decomposition explanation penalization (CDEP) to enable practitioners to leverage explanations from deep learning models to improve model performance. CDEP allows inserting domain knowledge into a model to ignore spurious correlations, correct errors, and generalize to dataset shifts. It works by penalizing the importance scores assigned to certain features or feature interactions by an explanation method, forcing the model to not just make the right predictions but make them for the right reasons aligned with the provided domain knowledge. The authors demonstrate CDEP using the contextual decomposition explanation method, which captures feature interactions, though CDEP could use other differentiable explanation techniques. Experiments on skin cancer diagnosis, biased digit classification, recidivism prediction, and text sentiment analysis show CDEP can increase model accuracy by teaching models to ignore spurious signals and incorporate fairness notions. Key advantages of CDEP are its ability to regularize complex feature interactions, computational efficiency, and applicability to various neural network architectures. Overall, the work demonstrates an effective way to leverage interpretability to improve deep learning models.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the paper:

The paper introduces contextual decomposition explanation penalization (CDEP), a method that allows practitioners to leverage explanations to improve the performance of deep learning models. CDEP works by penalizing the explanations generated by the model to align them with expert knowledge provided by the user. This allows the model to ignore spurious correlations, correct errors, and generalize better in the presence of dataset shifts. 

The authors demonstrate CDEP on a variety of tasks. For skin cancer diagnosis from images, CDEP improves performance by teaching the model to ignore spurious artifacts like bandaids in the images. On a modified MNIST dataset where color is a misleading signal, CDEP shifts the model's focus from color to shape. Simple examples also show how CDEP can mitigate fairness issues in text classification and risk prediction. Compared to prior work, CDEP penalizes complex interactions between features, is more computationally efficient, and can be readily applied to many network architectures. The results illustrate that CDEP is an effective way to insert domain knowledge into models and improve their generalization performance.


## Summarize the main method used in the paper in one paragraph.

 Here is a one paragraph summary of the main method used in the paper:

The paper proposes a novel method called contextual decomposition explanation penalization (CDEP) to insert domain knowledge into deep learning models to help them ignore spurious correlations, correct errors, and generalize better. CDEP works by penalizing the explanations produced by an interpretation method to align them with user-provided "ground truth" explanations. Specifically, the paper uses contextual decomposition (CD) to generate feature importance scores. The CD scores are incorporated into the loss function of the model, along with a penalty term that minimizes the difference between the CD scores and target explanations provided by the user based on their domain knowledge. By penalizing explanations, CDEP forces the model to not just make the right predictions, but make them for the right reasons aligned with the user's domain expertise. This regularization helps the model ignore spurious correlations and generalize better.


## What problem or question is the paper addressing?

 Based on my reading of the abstract, this paper is addressing the issue that many proposed explainable deep learning methods only provide insights into a model but do not suggest actions to improve the model. The paper introduces a method called contextual decomposition explanation penalization (CDEP) that enables practitioners to leverage explanations to improve deep learning model performance. Specifically, CDEP allows incorporating domain knowledge into a model to ignore spurious correlations, correct errors, and generalize to different dataset shifts. The effectiveness of CDEP is demonstrated on various toy and real datasets.

In summary, the key problem this paper aims to address is that prior explainable AI methods focus on interpreting models but do not provide ways to act on those interpretations to improve the model. The paper proposes CDEP as a solution that leverages explanations to insert domain knowledge and enhance model performance.
