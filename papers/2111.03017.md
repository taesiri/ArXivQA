# [MT3: Multi-Task Multitrack Music Transcription](https://arxiv.org/abs/2111.03017)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question/hypothesis seems to be whether a single general-purpose Transformer model can be used for multi-task, multitrack music transcription across diverse datasets, instruments, and genres. Specifically, the paper investigates whether their proposed MT3 model can:- Transcribe arbitrary combinations of instruments from raw audio using a single model architecture and training framework, rather than requiring specialized/custom models for each dataset or instrumentation.- Improve performance on low-resource datasets by training on a mixture of datasets, allowing knowledge transfer from high-resource to low-resource tasks. - Achieve state-of-the-art transcription quality across multiple datasets spanning different genres, recording methods, and sets of instruments.- Learn to identify the presence or absence of instruments directly from audio spectrograms, without needing a fixed specification of instruments.- Be robust to different groupings/granularities of instrument labels during training and evaluation.So in summary, the central hypothesis is that a single Transformer-based MT3 model can achieve high-quality multi-instrument transcription across diverse datasets and genres, while also improving performance on scarce training data by leveraging multiple datasets jointly. The paper aims to demonstrate the feasibility of this unified transcription approach.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contributions appear to be:- Defining a unified framework for multi-task multitrack music transcription (MT3) using a sequence-to-sequence approach with a transformer encoder-decoder model. This allows the model to jointly transcribe arbitrary combinations of musical instruments across several datasets.- Assembling a benchmark collection of 6 diverse multitrack AMT datasets, spanning different sizes, styles, and instrumentations, to enable multi-task learning. This is the largest known collection available for this purpose.- Defining standard test set splits and consistent evaluation metrics (including a new multi-instrument transcription metric) across all datasets. - Demonstrating state-of-the-art transcription performance with their T5-based MT3 model across all 6 datasets, outperforming prior specialized models and professional music transcription software.- Showing that training across multiple datasets improves performance, especially for low-resource datasets. The model is able to leverage high-resource datasets to improve transcription accuracy for instruments that have little data.- Analyzing model performance across different groupings of instruments, demonstrating robust instrument identification even when many instruments are present.In summary, the main contribution is presenting a unified training framework, dataset collection, and strong baseline model for the new task of multi-task multitrack music transcription across diverse instrumentation and musical styles.


## How does this paper compare to other research in the same field?

Here are a few thoughts on how this paper compares to other research in the field of automatic music transcription:- The use of a Transformer-based sequence-to-sequence model architecture builds off recent work like Hawthorne et al. (2021) that has shown the promise of Transformers for music transcription tasks. However, this paper extends that approach to handle multi-instrument transcription rather than just solo piano.- The multi-task training framework, training on a mixture of diverse datasets, seems quite novel in the music transcription literature. Most prior work has focused on models tailored to individual datasets. Training across multiple datasets with different instruments and levels of labeling quality is an interesting idea to improve robustness.- Evaluating on multiple datasets using consistent data splits and standardized metrics sets a new benchmark for multi-instrument transcription. Many prior papers have reported results on different subsets of public datasets using varying evaluation procedures.- The gains shown on low-resource datasets like GuitarSet, MusicNet, and URMP by training on a mixture highlight the data efficiency of the model. This is relevant since scarcity of labeled data is a major challenge in music transcription.- The introduction of a multi-instrument F1 metric that jointly evaluates note and instrument accuracy also formalizes a more useful metric for multi-instrument settings compared to traditional transcription metrics that ignore instrument identity.Overall, I'd say the multi-task training framework and showing strong results across diverse datasets is the most novel aspect compared to prior work. The work also makes contributions in terms of benchmarking and evaluation for multi-instrument transcription that will be useful for future research in this area.


## What future research directions do the authors suggest?

Based on my reading of the paper, some of the main future research directions suggested by the authors include:- Applying their multi-task multitrack music transcription (MT3) approach to unlabeled or weakly labeled data in a self-supervised or semi-supervised fashion. The authors note that labeled data for multi-instrument transcription is scarce and expensive to obtain, so leveraging unlabeled audio could help improve performance, especially for low-resource instruments and datasets.- Using the high-quality transcriptions from MT3 as training targets for generative models of symbolic/MIDI music. The authors mention this could enable new directions in generative music modeling.- Developing transcription systems for non-Western musical styles that use different tuning systems and pitch representations beyond the equal-tempered chromatic scale. The authors acknowledge their method is currently only applicable to Western music based on the 12-tone chromatic scale.- Further analysis and improvement of transcription dataset quality, in terms of factors like label alignment accuracy. The authors provide some evidence that timing alignment issues may be limiting model performance on certain datasets. Cleaning up datasets could thus also improve model performance.- Extending their transcription system to also perform instrument identification and separation, not just transcription. The authors' work focuses solely on transcription, but a combined system could have benefits.- Applying their multi-task transfer learning approach to other musical tasks beyond transcription, such as genre classification, structural segmentation, etc. The general methodology could be relevant for other musical sequence modeling problems.In summary, the main directions are: self/semi-supervised learning leveraging unlabeled data, using transcriptions for generative modeling, extending to non-Western music, improving dataset quality, adding instrument separation, and applying the transfer learning approach to other musical tasks. The authors lay out a number of promising avenues for future work in music transcription and understanding.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper proposes MT3, a multi-task multitrack music transcription model based on a Transformer encoder-decoder architecture, which achieves state-of-the-art results by training on a diverse mixture of 6 AMT datasets and introduces a new multi-instrument transcription metric to evaluate performance on transcribing notes from different instruments.


## Summarize the paper in one paragraph.

Here is a one paragraph summary of the paper:The paper proposes MT3, a multi-task multitrack music transcription model based on the Transformer architecture. Unlike prior work which has focused on task-specific models tailored to transcribing individual instruments, MT3 is designed as a general-purpose model capable of transcribing arbitrary combinations of instruments across several datasets. The authors demonstrate a unified training framework using spectrogram inputs and a flexible MIDI-like output vocabulary, allowing MT3 to be trained on a mixture of 6 diverse transcription datasets. Experiments show MT3 exceeds prior SOTA models on each individual dataset, while also improving low-resource transcription by 260% when trained on the full dataset mixture. The model transcribes pitch, timing, and instruments accurately even with many instruments present. By enabling multi-task learning, the authors expose the need for more consistent evaluation and better dataset alignment in this area. The work provides a strong baseline for this new direction of multi-task, multi-instrument music transcription.


## Summarize the paper in two paragraphs.

Here is a two paragraph summary of the key points from the paper:The paper proposes MT3, a multi-task multitrack music transcription model based on the Transformer architecture. Music transcription involves converting raw audio into symbolic notation representing the notes, instruments, and timing. Multitrack transcription refers to transcribing multiple instruments simultaneously from a polyphonic music mixture. The authors frame this as a sequence-to-sequence task, using log Mel spectrograms as input and a novel tokenized output vocabulary inspired by MIDI that can represent notes, instruments, and timing. They train the model on a mixture of six diverse transcription datasets using a single T5 architecture without specialized components for each dataset. The model outperforms prior specialized models designed individually for each dataset. Training on a mixture provides large gains over single dataset training, especially for low-resource datasets. The model achieves state-of-the-art transcription quality across all datasets based on frame, onset, and onset-offset F1 scores. The authors also propose a new multi-instrument F1 metric to evaluate both note and instrument accuracy. Additional experiments demonstrate the model's ability to generalize to unseen datasets. The work demonstrates how multi-task learning across diverse datasets can improve an under-resourced task like music transcription. It also highlights the need for more consistent evaluation practices and improved dataset quality.
