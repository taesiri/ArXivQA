# [MT3: Multi-Task Multitrack Music Transcription](https://arxiv.org/abs/2111.03017)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question/hypothesis seems to be whether a single general-purpose Transformer model can be used for multi-task, multitrack music transcription across diverse datasets, instruments, and genres. Specifically, the paper investigates whether their proposed MT3 model can:- Transcribe arbitrary combinations of instruments from raw audio using a single model architecture and training framework, rather than requiring specialized/custom models for each dataset or instrumentation.- Improve performance on low-resource datasets by training on a mixture of datasets, allowing knowledge transfer from high-resource to low-resource tasks. - Achieve state-of-the-art transcription quality across multiple datasets spanning different genres, recording methods, and sets of instruments.- Learn to identify the presence or absence of instruments directly from audio spectrograms, without needing a fixed specification of instruments.- Be robust to different groupings/granularities of instrument labels during training and evaluation.So in summary, the central hypothesis is that a single Transformer-based MT3 model can achieve high-quality multi-instrument transcription across diverse datasets and genres, while also improving performance on scarce training data by leveraging multiple datasets jointly. The paper aims to demonstrate the feasibility of this unified transcription approach.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contributions appear to be:- Defining a unified framework for multi-task multitrack music transcription (MT3) using a sequence-to-sequence approach with a transformer encoder-decoder model. This allows the model to jointly transcribe arbitrary combinations of musical instruments across several datasets.- Assembling a benchmark collection of 6 diverse multitrack AMT datasets, spanning different sizes, styles, and instrumentations, to enable multi-task learning. This is the largest known collection available for this purpose.- Defining standard test set splits and consistent evaluation metrics (including a new multi-instrument transcription metric) across all datasets. - Demonstrating state-of-the-art transcription performance with their T5-based MT3 model across all 6 datasets, outperforming prior specialized models and professional music transcription software.- Showing that training across multiple datasets improves performance, especially for low-resource datasets. The model is able to leverage high-resource datasets to improve transcription accuracy for instruments that have little data.- Analyzing model performance across different groupings of instruments, demonstrating robust instrument identification even when many instruments are present.In summary, the main contribution is presenting a unified training framework, dataset collection, and strong baseline model for the new task of multi-task multitrack music transcription across diverse instrumentation and musical styles.


## How does this paper compare to other research in the same field?

Here are a few thoughts on how this paper compares to other research in the field of automatic music transcription:- The use of a Transformer-based sequence-to-sequence model architecture builds off recent work like Hawthorne et al. (2021) that has shown the promise of Transformers for music transcription tasks. However, this paper extends that approach to handle multi-instrument transcription rather than just solo piano.- The multi-task training framework, training on a mixture of diverse datasets, seems quite novel in the music transcription literature. Most prior work has focused on models tailored to individual datasets. Training across multiple datasets with different instruments and levels of labeling quality is an interesting idea to improve robustness.- Evaluating on multiple datasets using consistent data splits and standardized metrics sets a new benchmark for multi-instrument transcription. Many prior papers have reported results on different subsets of public datasets using varying evaluation procedures.- The gains shown on low-resource datasets like GuitarSet, MusicNet, and URMP by training on a mixture highlight the data efficiency of the model. This is relevant since scarcity of labeled data is a major challenge in music transcription.- The introduction of a multi-instrument F1 metric that jointly evaluates note and instrument accuracy also formalizes a more useful metric for multi-instrument settings compared to traditional transcription metrics that ignore instrument identity.Overall, I'd say the multi-task training framework and showing strong results across diverse datasets is the most novel aspect compared to prior work. The work also makes contributions in terms of benchmarking and evaluation for multi-instrument transcription that will be useful for future research in this area.
