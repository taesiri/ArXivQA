# [Understanding the Effect of Model Compression on Social Bias in Large   Language Models](https://arxiv.org/abs/2312.05662)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Large language models (LLMs) learn social biases from the data they are trained on. This can lead to issues like representational harm when the models are deployed.
- Strategies have been proposed to mitigate bias in LLMs but model compression techniques like quantization and distillation are also becoming popular to make models more efficient. 
- There has been little analysis on how model compression impacts bias in LLMs.

Proposed Solution:
- The authors perform a controlled study on the interplay between model compression and social bias in LLMs. 
- They evaluate popular encoder (BERT, RoBERTa) and decoder (Pythia) LLMs of varying sizes before and after quantization to lower precision and distillation to smaller models.
- Social bias is measured using the Bias Bench benchmark which has 3 datasets - CrowS-Pairs, StereoSet, SEAT across gender, race and religion.

Key Findings:
- Longer pretraining and larger models lead to higher social bias based on correlation analysis.
- Quantization acts as a regularizer, with 8-bit quantization showing a good tradeoff, reducing bias by 15-40% while preserving accuracy.
- Distillation to smaller models reduces bias significantly but at the cost of large accuracy drops.

Main Contributions:
- First controlled analysis showing model compression via quantization and distillation can reduce social bias in LLMs
- Demonstrates longer pretraining and larger models increase social bias
- Quantization provides a practical way to partially reduce bias without significant accuracy loss

The key idea is model compression like quantization can regularize LLMs and reduce social bias while maintaining reasonable accuracy, providing a practical way forward.
