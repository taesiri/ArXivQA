# [Understanding the Effect of Model Compression on Social Bias in Large   Language Models](https://arxiv.org/abs/2312.05662)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Large language models (LLMs) learn social biases from the data they are trained on. This can lead to issues like representational harm when the models are deployed.
- Strategies have been proposed to mitigate bias in LLMs but model compression techniques like quantization and distillation are also becoming popular to make models more efficient. 
- There has been little analysis on how model compression impacts bias in LLMs.

Proposed Solution:
- The authors perform a controlled study on the interplay between model compression and social bias in LLMs. 
- They evaluate popular encoder (BERT, RoBERTa) and decoder (Pythia) LLMs of varying sizes before and after quantization to lower precision and distillation to smaller models.
- Social bias is measured using the Bias Bench benchmark which has 3 datasets - CrowS-Pairs, StereoSet, SEAT across gender, race and religion.

Key Findings:
- Longer pretraining and larger models lead to higher social bias based on correlation analysis.
- Quantization acts as a regularizer, with 8-bit quantization showing a good tradeoff, reducing bias by 15-40% while preserving accuracy.
- Distillation to smaller models reduces bias significantly but at the cost of large accuracy drops.

Main Contributions:
- First controlled analysis showing model compression via quantization and distillation can reduce social bias in LLMs
- Demonstrates longer pretraining and larger models increase social bias
- Quantization provides a practical way to partially reduce bias without significant accuracy loss

The key idea is model compression like quantization can regularize LLMs and reduce social bias while maintaining reasonable accuracy, providing a practical way forward.


## Summarize the paper in one sentence.

 Here is a one sentence summary of the paper:

The paper performs a controlled study of the impact of quantization and knowledge distillation model compression techniques on measures of social bias in large language models, finding that longer pretraining and larger models lead to higher social bias while quantization demonstrates a regularizing effect in reducing bias.


## What is the main contribution of this paper?

 Based on my reading of the paper, the main contribution is:

The paper performs a carefully controlled study to analyze the impact of two popular model compression techniques - quantization and knowledge distillation - on measures of social bias in large language models (LLMs). Key findings include:

- Longer pretraining and larger model sizes tend to increase social bias in LLMs. 

- Model compression via quantization and distillation can help reduce social bias, with quantization showing a regularization effect with the best bias-accuracy tradeoff at around 20% of the original pretraining time.

- Social bias categories respond differently to model size and compression methods. There is no one-size-fits-all approach.

- Quantization demonstrates a better tradeoff by reducing bias while preserving language modeling capabilities more than distillation.

In summary, the paper shows that model compression can serve as a pragmatic approach to reduce social bias in pretrained LLMs, while retaining accuracy and efficiency. This is an important finding given the computational and financial constraints around retraining large models to remove bias.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper abstract and contents, some of the key terms and keywords associated with this paper include:

- Large language models (LLMs)
- Social bias
- Model compression 
- Quantization
- Knowledge distillation
- Bias Bench benchmark
- CrowS-Pairs, StereoSet, SEAT (bias evaluation datasets)
- Gender, race, religion (categories of bias studied)
- Longer pretraining, larger models (factors linked to increased bias) 
- Model sizes studied (BERT, RoBERTa, Pythia variants)
- Effect of quantization and distillation on bias
- Tradeoffs between efficiency and bias

The paper examines the interplay between methods for compressing large language models to improve efficiency, and how those impact measures of social bias in the models across gender, race, and religion. It uses existing bias evaluation datasets to quantify this. The key focus areas are understanding how model compression techniques like quantization and distillation affect the biases learned during pretraining of models like BERT and RoBERTa.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions I would ask about the method proposed in this paper:

1. The paper studies the interplay between model compression techniques like quantization and distillation and social biases in language models. Could you expand more on why studying this interplay is important and the potential impacts it could have?

2. You studied only quantization and distillation for model compression, but mentioned other techniques like pruning. Why did you choose to focus specifically on quantization and distillation? What benefits or challenges do they present over other compression methods regarding bias?  

3. You observed dynamic post-training quantization helps reduce bias by acting as a regularizer. Can you explain in more detail the mechanism behind why quantization would act as a regularizer against social bias in this way?

4. For the distillation experiments, what type of distillation loss was used to compress the models? Could the choice of distillation loss impact model bias during compression, and if so how?

5. You studied encoder-decoder models like BERT and encoder-only models like GPT. Did you observe any differences in how compression techniques impacted bias across these different model types? 

6. The Bias Bench benchmark used for evaluation has some limitations, like focusing on western biases. How could the choice of bias benchmark impact conclusions, and what steps did you take to mitigate limitations?  

7. Monitoring validation loss is common during compression to avoid degradation. Did you implement any similar monitoring for model bias during compression? If not, how could this be approached?

8. For the longer Pythia pretraining experiments, you observed increased bias over time. Did you investigate any bias mitigation strategies only at pretraining time before compression is applied? 

9. You studied model size but not other architectural choices. How could factors like model depth or width interact with compression to impact bias?

10. This analysis focused on intrinsic social biases regarding text selections. How might compression impact extrinsic biases regarding model predictions or uncertainty on downstream tasks?
