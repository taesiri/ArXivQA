# [Two-Timescale Q-Learning with Function Approximation in Zero-Sum   Stochastic Games](https://arxiv.org/abs/2312.04905)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a summary paragraph of the key points from the paper:

This paper proposes a two-timescale $Q$-learning algorithm with function approximation for two-player zero-sum stochastic games. The algorithm maintains two sets of parameters in an inner loop and outer loop to approximate the $Q$-functions, mimicking minimax value iteration. The inner loop parameters are updated via projected stochastic gradient to estimate the marginalized payoff, while the outer loop parameters that determine the policies are updated as a convex combination between the current and latest inner loop parameters, resembling smoothed best response. Under linear function approximation, the paper establishes a finite sample analysis showing that the last iterate of the joint policy from this algorithm converges in Nash gap to an equilibrium at a rate polynomial in the relevant parameters. The key novelty is the construction of a customized Lyapunov function to capture the evolution of the outer loop parameters, by using a change of variables and a generalized Moreau envelope of the regularized Nash gap. Overall, the algorithm and analysis present the first polynomial sample complexity bound for a payoff-based independent learning dynamics under function approximation in two-player zero-sum games.
