# [Towards Open-Vocabulary Video Instance Segmentation](https://arxiv.org/abs/2304.01715)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question is: How can we develop a video instance segmentation model that can segment and categorize objects from both seen (training) and novel (unseen) categories in an open-vocabulary setting? The key points are:- Traditional video instance segmentation models are limited to segmenting and classifying objects only from a closed set of categories seen during training. They lack the ability to generalize to novel object categories.- The authors propose a new task called "Open-Vocabulary Video Instance Segmentation" which aims to segment and classify objects in videos from both seen training categories as well as novel categories not seen during training.- To benchmark this task, the authors collect a new dataset called LV-VIS with over 1000 diverse object categories, including many categories not present in other common datasets. This allows evaluating generalization to novel categories.- The authors propose a new model architecture called OV2Seg that can perform open-vocabulary video instance segmentation in an end-to-end manner. It uses a transformer encoder-decoder architecture with a memory module to achieve good segmentation, tracking and classification of objects from both known and novel categories.- Experiments on multiple datasets demonstrate OV2Seg's strong generalization ability to novel object categories not seen during training.In summary, the central hypothesis is that an end-to-end transformer model with memory can achieve open-vocabulary video instance segmentation, segmenting and classifying objects from both known training classes as well as novel classes not seen during training. The LV-VIS dataset and OV2Seg model are proposed to test this hypothesis.
