# [Online Cascade Learning for Efficient Inference over Streams](https://arxiv.org/abs/2402.04513)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper "Online Cascade Learning for Efficient Inference over Streams":

Problem:
- Large language models (LLMs) like GPT-3 are very expensive to run inference on, limiting their applicability for complex queries over data streams. 
- Existing methods like distillation and static cascades are unsuitable for streaming settings which require online learning without human-labeled data.

Proposed Solution:
- The paper proposes a new framework called "online cascade learning" to address this challenge. 
- The key idea is to learn a cascade of models with increasing capacity (starting from logistic regression up to the LLM) and cost, along with a learned deferral policy.  
- The deferral policy determines which model in the cascade should process an input based on complexity. 
- The overall framework is trained online via imitation learning that samples and follows the LLM's behavior initially.
- Over time, as annotations from the LLM train the smaller models, the deferral policy routes most inputs to cheaper models, thereby saving costs.

Main Contributions:
- Formulation and algorithms for online learning of model cascades and deferral policies via imitation learning, needing no human labels.
- A theoretical no-regret analysis for the online cascade learning algorithm.
- Experiments on four benchmarks demonstrating up to 90% cost savings compared to just using the LLM while achieving comparable accuracy.

In summary, the paper offers a novel cascade learning approach suited for stream processing that can trade-off accuracy vs computational costs in an online manner. The experimental results validate that it adapts well across complexity levels and approximates LLM accuracy at much lower inference costs.
