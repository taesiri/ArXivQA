# [Are Straight-Through gradients and Soft-Thresholding all you need for   Sparse Training?](https://arxiv.org/abs/2212.01076)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the main research question this paper addresses is: Can a simple combination of straight-through gradient estimation and soft-thresholding match or surpass more complex state-of-the-art methods for training sparse neural networks?The key hypotheses appear to be:1) Applying straight-through gradient estimation to update the raw (non-thresholded) weights, even when they are zeroed in the forward pass, will allow inactive weights to smoothly transition to active and vice versa during training. This will improve performance compared to methods that only update non-zero weights. 2) Using soft-thresholding rather than hard-thresholding to induce sparsity will reduce sharp discontinuities in weight values during training that can degrade performance at high sparsity levels.3) Progressively increasing the sparsity during training will further improve stability and allow the network to adapt.4) This simple combination of techniques can match or exceed the performance of more complex state-of-the-art methods for sparse training that use things like gumbel softmax, neuroregeneration, or multiple rounds of pruning and re-training.So in summary, the main hypothesis is that a straightforward integration of these techniques (STE, soft-thresholding, progressive sparsity increase) can achieve state-of-the-art sparse network training performance despite its simplicity. The experiments aim to validate this hypothesis.


## What is the main contribution of this paper?

The main contribution of this paper is proposing a simple yet effective method for training sparse neural networks called ST-3 (Straight-Through/Soft-Thresholding/Sparse-Training). The key ideas are:- Using straight-through gradient estimation to continuously update the raw (non-thresholded) weights, even if they are thresholded to zero in the forward pass. This allows inactive weights to potentially become active again later in training.- Using soft-thresholding instead of hard-thresholding when setting weights to zero. This reduces sharp discontinuities in weight values that can harm training. - Progressively increasing the sparsity (ratio of weights set to zero) during training according to a schedule. This gives the network time to adapt to the changing sparsity.- A weight rescaling technique to compensate for the reduced magnitude of soft-thresholded weights.The simplicity of ST-3 combined with its strong empirical performance makes it an effective baseline for sparse training research. The authors show it achieves state-of-the-art accuracy-sparsity trade-offs on CIFAR and ImageNet compared to prior works. The results suggest the key to effective sparsification is giving weights freedom to smoothly evolve between zero and non-zero states while progressively increasing sparsity.
