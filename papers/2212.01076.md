# [Are Straight-Through gradients and Soft-Thresholding all you need for   Sparse Training?](https://arxiv.org/abs/2212.01076)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the main research question this paper addresses is: Can a simple combination of straight-through gradient estimation and soft-thresholding match or surpass more complex state-of-the-art methods for training sparse neural networks?The key hypotheses appear to be:1) Applying straight-through gradient estimation to update the raw (non-thresholded) weights, even when they are zeroed in the forward pass, will allow inactive weights to smoothly transition to active and vice versa during training. This will improve performance compared to methods that only update non-zero weights. 2) Using soft-thresholding rather than hard-thresholding to induce sparsity will reduce sharp discontinuities in weight values during training that can degrade performance at high sparsity levels.3) Progressively increasing the sparsity during training will further improve stability and allow the network to adapt.4) This simple combination of techniques can match or exceed the performance of more complex state-of-the-art methods for sparse training that use things like gumbel softmax, neuroregeneration, or multiple rounds of pruning and re-training.So in summary, the main hypothesis is that a straightforward integration of these techniques (STE, soft-thresholding, progressive sparsity increase) can achieve state-of-the-art sparse network training performance despite its simplicity. The experiments aim to validate this hypothesis.
