# [Understanding Neural Network Binarization with Forward and Backward   Proximal Quantizers](https://arxiv.org/abs/2402.17710)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem Statement:
- Existing works on neural network binarization like BinaryConnect (BC) apply the sign function in the forward pass and use an "approximate gradient" in the backward pass to update the weights instead of the zero gradient of the sign function. This "training trick" works well empirically but lacks theoretical justification. 

- ProxConnect (PC) was proposed to generalize BC with a principled optimization perspective using proximal operators, but cannot explain the approximate gradient heuristics.

Proposed Solution - ProxConnect++ (PC++):
- Introduces the concept of forward and backward proximal quantizers to generalize PC. This provides a way to understand and justify existing approximate gradient methods.

- Shows that common binarization algorithms like BNN and BNN+ can be expressed as specific instances of forward-backward proximal quantizers under this framework.

- Provides a method to systematically construct 'legitimate' pairs of forward-backward quantizers that come with convergence guarantees. 

- Proposes a new algorithm BNN++ modified from BNN+ by correcting its forward-backward pair based on the theory.

Key Contributions:
- Generalizes PC to include forward-backward quantization and encompass common binarization techniques. Justifies their optimization behavior.

- Provides a recipe for constructing convergent binarization algorithms with guarantees.

- Introduces BNN++ that outperforms BNN+ in experiments, showing the benefit of the theoretical design.

- Empirically evaluates and compares variants of PC++ by binarizing CNN and vision transformer models on image classification. BNN++ is competitive across most settings.
