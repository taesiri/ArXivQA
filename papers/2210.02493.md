# [Depth Is All You Need for Monocular 3D Detection](https://arxiv.org/abs/2210.02493)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question appears to be: How can depth estimation be leveraged to improve monocular 3D object detection?In particular, the authors investigate two main approaches:1) Using point cloud data during training to directly supervise a multi-task model that predicts depth and 3D boxes.2) Using unlabeled video at training time to generate pseudo-depth labels via self-supervision, which are then used to supervise the multi-task model. The key hypotheses seem to be:- Supervising an in-domain depth prediction task along with 3D detection in a multi-task model will lead to better feature representations and improve detection accuracy.- Pseudo-depth labels generated via self-supervision can act as an effective supervisory signal for the depth task, removing the need for point clouds. - A two-stage training procedure is critical when using self-supervision because of the difference in loss distributions between photometric self-supervision and 3D detection losses.So in summary, the main research question is how best to leverage depth estimation, either through direct point cloud supervision or self-supervised pseudo-labels, to improve monocular 3D detection via multi-task learning. The core hypothesis is that aligning the model's features to predict in-domain depth will boost 3D detection performance.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contributions appear to be:1. Proposing a simple and effective multi-task network called DD3Dv2 to improve monocular 3D detection through depth supervision. This network leverages unlabeled data like LiDAR or RGB video during training to refine the depth representation and improve 3D detection.2. Demonstrating methods to learn useful depth representations under two scenarios: when LiDAR data is available, and when only RGB video is available. 3. For the RGB video case, proposing a two-stage training strategy to address the heterogeneity between the multi-task losses imposed by image-based self-supervised depth estimation. This involves first generating pseudo-depth labels, then training the multi-task network using these labels.4. Evaluating the proposed algorithms on the nuScenes and KITTI 3D detection benchmarks and showing state-of-the-art performance.5. Providing ablation studies analyzing the impact of different training strategies, loss functions, backbones, etc. on the performance of the proposed DD3Dv2 network.In summary, the main contribution seems to be introducing a simple yet effective way to harness unlabeled data like LiDAR or video at training time to adapt and improve depth representation for monocular 3D object detection, while maintaining efficiency at test time. The two-stage training strategy is highlighted as being particularly important when using self-supervised video data.
