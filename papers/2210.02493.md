# [Depth Is All You Need for Monocular 3D Detection](https://arxiv.org/abs/2210.02493)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question appears to be: How can depth estimation be leveraged to improve monocular 3D object detection?In particular, the authors investigate two main approaches:1) Using point cloud data during training to directly supervise a multi-task model that predicts depth and 3D boxes.2) Using unlabeled video at training time to generate pseudo-depth labels via self-supervision, which are then used to supervise the multi-task model. The key hypotheses seem to be:- Supervising an in-domain depth prediction task along with 3D detection in a multi-task model will lead to better feature representations and improve detection accuracy.- Pseudo-depth labels generated via self-supervision can act as an effective supervisory signal for the depth task, removing the need for point clouds. - A two-stage training procedure is critical when using self-supervision because of the difference in loss distributions between photometric self-supervision and 3D detection losses.So in summary, the main research question is how best to leverage depth estimation, either through direct point cloud supervision or self-supervised pseudo-labels, to improve monocular 3D detection via multi-task learning. The core hypothesis is that aligning the model's features to predict in-domain depth will boost 3D detection performance.
