# [Discovering Hierarchical Achievements in Reinforcement Learning via   Contrastive Learning](https://arxiv.org/abs/2307.03486)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be:How can we enable reinforcement learning agents to effectively discover and complete hierarchically structured achievements in procedurally generated environments, without relying on explicit hierarchical architectures or planning modules?The key hypothesis is that an agent can learn to complete complex, multi-step achievements by acquiring knowledge about the underlying achievement structure purely from the reward signal, if we augment model-free RL with auxiliary self-supervised representation learning objectives.Specifically, the paper proposes using contrastive learning techniques like intra-trajectory achievement prediction and cross-trajectory achievement matching to distill information about discovered achievements into the latent representations. This allows the agent to develop a better understanding of the dependencies between achievements and more confidently predict upcoming goals, enabling stronger generalization and long-term reasoning capacities.The authors test this hypothesis in the procedurally generated Crafter environment with 22 hierarchically structured achievements. They find that their proposed auxiliary representation learning approach called "achievement distillation" enables a simple PPO agent to significantly outperform prior model-free, model-based, and hierarchical methods in unlocking achievements. This supports their hypothesis that self-supervised representation learning can impart model-free RL agents with improved reasoning and planning abilities.In summary, the key research question is how to develop sample-efficient RL agents that can discover and complete complex, hierarchical tasks, and the core hypothesis is that this can be achieved by distilling knowledge about task structure into learned representations without architecting explicit planning modules.


## What is the main contribution of this paper?

This paper proposes a novel contrastive learning method called "achievement distillation" to improve an agent's ability to discover hierarchical achievements in reinforcement learning. The key contributions are:- They show that PPO, a simple model-free RL algorithm, can be a strong baseline for unlocking hierarchical achievements on procedurally generated environments like Crafter. With some implementation improvements, PPO outperforms more complex model-based and hierarchical approaches.- They analyze PPO's learned latent representations and find the agent has some capacity to predict the next achievement, but with low confidence. - They introduce two self-supervised objectives for achievement distillation: 1) Intra-trajectory achievement prediction maximizes similarity between a state-action pair and the next achievement in an episode. 2) Cross-trajectory achievement matching maximizes similarity between matched achievements from different episodes.- Achievement distillation can be integrated with PPO through an alternating training procedure with policy optimization and auxiliary representation learning phases.- Their method achieves state-of-the-art performance on Crafter, outperforming prior methods in sample efficiency and unlocking the most complex achievements. It uses only 4% of the parameters of the best prior model.- Analysis shows their representation learning approach induces the encoder to produce latent states that predict next achievements with much higher confidence.In summary, the key contribution is a simple yet effective contrastive learning technique to distill useful knowledge about achievements that significantly improves an agent's ability to discover hierarchical tasks in a sample-efficient manner.


## How does this paper compare to other research in the same field?

Here are some thoughts on how this paper compares to other related research:- This paper focuses on discovering hierarchical achievements in reinforcement learning using contrastive learning. Other recent work has also looked at learning hierarchical representations and skills in RL, but often takes different approaches like model-based RL or explicitly modeling the task hierarchy. This paper stands out for its simple yet effective method based on contrastive learning alongside standard PPO.- Compared to model-based methods like DreamerV3 or MuZero, this approach does not rely on learning an explicit world model or planning module. By showing strong performance purely with model-free PPO plus contrastive learning, it demonstrates that these complex components may not be necessary for learning reusable skills and achievements. - The paper argues that common model-free algorithms like PPO struggle on long-term credit assignment problems like discovering achievements. By augmenting PPO with representation learning based on contrastive objectives, the method is able to significantly boost its hierarchical reasoning abilities. This demonstrates the power of representation learning for overcoming issues with reward sparsity.- Compared to hierarchical RL methods, this approach does not require explicitly modeling the task hierarchy or relationships between achievements. Instead, it shows that relevant hierarchical knowledge can be implicitly distilled into the learned representations through the proposed contrastive objectives. The simplicity of this approach makes it widely applicable.- Overall, this work integrates ideas from representation learning, contrastive learning, optimal transport, and model-free RL in a novel way tailored to discovering reusable skills and achievements. The results demonstrate state-of-the-art performance on a challenging benchmark using an elegant and straightforward algorithm. This represents an advance in applying representation learning to reinforcement learning in a practical and effective manner.In summary, this paper introduces a simple yet high-performing approach for hierarchical reinforcement learning that contrasts with and complements prior methods based on model-based RL or explicitly hierarchical agents. The results and analysis provide new insights into effectively acquiring reusable skills for long-term reasoning.


## What future research directions do the authors suggest?

Based on my reading of the paper, some of the main future research directions suggested by the authors include:- Exploring methods for fully unsupervised learning without relying on any reward signal. The current approach still requires a sparse reward upon accomplishing new achievements. Developing techniques that can distinguish achievements in a completely unsupervised manner could be valuable. The authors suggest combining their approach with curiosity-driven exploration methods like BYOL.- Evaluating the transferability of the learned representations to new tasks or environments. The paper focuses on discovering achievements within a single environment. Assessing if the representations can transfer to novel tasks or environments is an important direction.- Incorporating hierarchical structure learning on top of the achievement representations. The current approach focuses on learning useful achievement representations. Building hierarchical structures or graphs on top of these representations could further enhance the agent's capabilities. - Scaling up the approach to more complex 3D environments. The experiments are on 2D Crafter environments. Testing how well the method can work in more complex 3D environments like Minecraft is an interesting direction.- Combining model-based and model-free algorithms. The contrastive learning approach is applied on top of a model-free PPO agent. Integrating it with model-based algorithms is another potential direction.- Applying the achievement distillation idea to other domains like robotics. Evaluating if similar contrastive learning techniques can help robots acquire reusable skills is an exciting research avenue.In summary, the main future directions are developing more unsupervised and hierarchical versions of the approach, assessing its transferability and scalability, and extending it to new applications like robotics. Overall, the authors lay out several promising paths for building on this work.


## Summarize the paper in one paragraph.

Here is a one paragraph summary of the paper:The paper proposes a new method called Achievement Distillation for discovering hierarchical achievements in reinforcement learning. The key idea is to leverage contrastive learning alongside RL training to distill useful information about achieved subtasks into the latent representations. This involves maximizing similarity between representations of state-action pairs and next achievements within episodes, as well as matching achievements across episodes. The method can be integrated with proximal policy optimization (PPO) by introducing alternating policy and auxiliary training phases. Experiments on the Crafter environment show the proposed approach outperforms prior model-based and hierarchical methods, achieving state-of-the-art results in discovering the full set of hierarchical achievements using 4x fewer parameters and within 1M steps. The learned representations exhibit substantially improved confidence in predicting upcoming achievements. Overall, the work demonstrates the promise of self-supervised representation learning for acquiring complex skills requiring generalization and long-term reasoning.


## Summarize the paper in two paragraphs.

Here is a two paragraph summary of the paper:The paper proposes a novel contrastive learning method called "achievement distillation" to help reinforcement learning agents discover hierarchical achievements in procedurally generated environments. The key idea is to distill useful information about discovered achievements from policy rollouts into the encoder using two self-supervised objectives. First, they maximize similarity between state-action representations and subsequent achievement representations within episodes. Second, they match achievements across episodes using optimal transport and maximize similarity between matched pairs. These objectives are trained alongside a proximal policy optimization (PPO) agent to strengthen its ability to predict subsequent achievements. Experiments on the Crafter benchmark show their method unlocks all 22 hierarchical achievements using only 1 million steps, outperforming prior model-free, model-based, and hierarchical methods. Ablations show both self-supervised objectives improve performance. Analysis reveals their method produces achievement predictions with much higher confidence compared to PPO. Overall, this demonstrates the promise of contrastive self-supervised learning for discovering reusable, hierarchical skills without hand-crafted priors. The simple integration with model-free RL enables strong performance without large models or planning.
