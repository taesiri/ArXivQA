# [Efficient Progressive Neural Architecture Search](https://arxiv.org/abs/1808.00391)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question/hypothesis seems to be:Can integrating weight sharing (from ENAS) into a progressive neural architecture search method based on SMBO (like PNAS) lead to increased efficiency in neural architecture search while retaining competitive accuracy?The authors propose a new method called EPNAS that combines progressive SMBO-based search (like PNAS) with weight sharing among sampled architectures (like ENAS). Their hypothesis is that this will allow the search efficiency gains from weight sharing while leveraging the strong performance of SMBO-based progressive search. Specifically, they aim to investigate:1) The effect of weight sharing on speed and performance of progressive SMBO-based NAS.2) Whether probabilistic sampling based on the surrogate function can improve results compared to the greedy top-K selection used in PNAS. So in summary, the central hypothesis is that the proposed EPNAS method can achieve state-of-the-art efficiency and accuracy by integrating complementary techniques from prior works PNAS and ENAS. The experiments aim to validate whether this hybrid approach provides benefits over either method alone.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contributions appear to be:1. Investigating the effect of weight sharing on the speed and performance of progressive SMBO-based neural architecture search. The paper proposes combining the sequential model-based optimization approach of PNAS with the weight sharing strategy of ENAS to improve speed.2. Relaxing the greedy sampling strategy of PNAS by using the learned surrogate model to probabilistically sample architectures instead of just taking the top K at each step. This is aimed at improving the search by avoiding local minima. 3. Introducing a new approach called Efficient Progressive Neural Architecture Search (EPNAS) that incorporates these ideas. EPNAS uses weight sharing to allow faster iterative sequential search with a surrogate model, and probabilistic sampling based on the surrogate predictions to guide the search.4. Demonstrating through experiments on CIFAR-10 that EPNAS can find competitive neural architectures much faster than prior methods like PNAS, while achieving similar accuracy.In summary, the main contribution appears to be proposing and evaluating a more efficient progressive neural architecture search method combining weight sharing and probabilistic surrogate-based sampling. The experiments aim to validate the speed and accuracy improvements of this approach.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper proposes an efficient neural architecture search method called EPNAS that combines progressive model-based optimization with weight sharing among sampled architectures and relaxes the greedy sampling strategy to improve search efficiency while retaining competitiveness of the found architectures.


## How does this paper compare to other research in the same field?

Based on the abstract, this paper appears to make two main contributions to neural architecture search:1. It combines sequential model-based optimization (as used in PNAS) with weight sharing (as used in ENAS) to improve the efficiency of the architecture search process. 2. It relaxes the greedy selection strategy used in PNAS by using the learned surrogate model to probabilistically sample architectures instead of always selecting the top K. In terms of related work, this approach seems to build directly on PNAS and ENAS, two recent and high-profile papers in neural architecture search. The key novelty is in combining these two methods to get improved efficiency over PNAS while retaining its strong performance. The probabilistic sampling also seems like an interesting tweak to make the search less greedy. However, it's not entirely clear from the abstract alone how much this impacts the final results.Overall, this appears to be an incremental but solid contribution. It doesn't propose an entirely new architecture search method, but rather combines existing techniques in a novel way to push the state-of-the-art in efficiency. The experiments on CIFAR-10 suggest comparable or slightly better performance than PNAS and ENAS with significantly lower compute requirements.My main question would be whether the ideas could transfer and show similar gains on larger datasets like ImageNet. But the approach seems promising as an efficient architecture search method, building nicely on top of recent work.


## What future research directions do the authors suggest?

Based on my reading of the paper, some of the main future research directions suggested by the authors include:- Exploring the application of EPNAS to other tasks beyond image classification, such as regression problems. The authors mention they are interested in verifying if the constrained search space and exploration methods are relevant for other learning problems.- Running EPNAS with more parallel threads and choosing the best architecture among them. The authors note that better results are expected with a larger value of K (number of sampled architectures). However, this increases exploration cost. Running multiple EPNAS threads in parallel could help find better architectures. - Investigating other surrogate functions beyond LSTM. The authors use an LSTM network for the surrogate, but note other types of models could be explored.- Experimenting with different temperature cooling schedules during the probabilistic architecture sampling. Theauthors use a quickly decaying temperature schedule, but other scheduling approaches could be tested.- Applying EPNAS to larger datasets like ImageNet. The authors demonstrate results on CIFAR-10, but suggest expanding to larger image classification benchmarks.- Leveraging weight sharing and progressive search for other neural architecture search algorithms, beyond just SMBO-based methods. The authors show the potential of combining these two techniques.In summary, the main directions mentioned are expanding the application domains, tuning the search parameters and algorithms, and scaling up to larger datasets. The core ideas of weight sharing and progressive surrogate-based sampling are highlighted as promising future research avenues.


## Summarize the paper in one paragraph.

Here is a one paragraph summary of the key points from the paper:The paper proposes a method called Efficient Progressive Neural Architecture Search (EPNAS) for optimizing convolutional neural network architectures for image classification tasks. EPNAS combines ideas from two previous methods - it uses sequential model-based optimization like Progressive Neural Architecture Search (PNAS) but also incorporates weight sharing among sampled architectures like Efficient Neural Architecture Search (ENAS). The goal is to benefit from PNAS's strong performance while improving speed through weight sharing. EPNAS explores the search space sequentially, training sampled architectures for a few epochs and using a surrogate model to predict validation errors. Compared to PNAS, EPNAS samples architectures probabilistically based on the surrogate's predictions rather than greedily taking the top K, helping avoid local minima. Experiments on CIFAR-10 show EPNAS finds competitive architectures much faster than PNAS, demonstrating the benefits of incorporating weight sharing into sequential optimization for neural architecture search.


## Summarize the paper in two paragraphs.

Here is a 2 paragraph summary of the paper:This paper proposes a method for efficiently searching the space of neural network architectures for image classification. The method combines progressive neural architecture search (PNAS) with weight sharing between models. PNAS uses a learned surrogate function to guide the search through increasingly complex architecture spaces. This allows efficient exploration without training all possible architectures. However, PNAS still requires significant compute resources. The authors propose enhancing PNAS with weight sharing between models, an idea from the Efficient Neural Architecture Search (ENAS) method. Weight sharing allows models to be evaluated after training for only a few epochs, rather than requiring full training from scratch. The proposed Efficient PNAS (EPNAS) method starts with simple network architectures, evaluates them with weight sharing, and uses the surrogate model to select which more complex architectures to evaluate next. By iterating through adding complexity, strong architectures are identified. On CIFAR-10 image classification, EPNAS finds competitive models using only 1 GPU in 1-2 days, much faster than the 100 GPUs for 2 days required by PNAS. The method demonstrates how progressive search and weight sharing can be combined for efficient neural architecture search. The results are competitive with state-of-the-art neural architecture search methods while requiring far fewer computational resources.


## Summarize the main method used in the paper in one paragraph.

Here is a one paragraph summary of the main method in the paper:The paper proposes a method for efficient neural architecture search called EPNAS (Efficient Progressive Neural Architecture Search). EPNAS combines two ideas from previous work - using sequential model-based optimization (SMBO) to progressively search a constrained architecture space (from PNAS), and sharing weights among sampled architectures during search to avoid costly retraining (from ENAS). EPNAS uses an LSTM surrogate model to sample architectures probabilistically at each step rather than greedily taking the top K. It trains sampled architectures for a few epochs with shared weights to get noisy validation scores to update the surrogate. This process is iterated several times, from simple to complex architectures, with a temperature-based sampling strategy to balance exploration and exploitation. Overall, EPNAS aims to achieve strong performance like PNAS while improving efficiency through weight sharing and more guided search.
