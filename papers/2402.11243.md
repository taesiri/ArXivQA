# [Can Large Language Models perform Relation-based Argument Mining?](https://arxiv.org/abs/2402.11243)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Argument mining (AM) is the process of automatically extracting arguments and relations between arguments from text. One key task is relation-based AM (RbAM) which focuses on identifying support and attack relations between arguments.  
- RbAM is challenging, with existing methods failing to perform satisfactorily across different datasets. There is a need for methods that can work well across datasets.

Proposed Solution:
- Use general-purpose large language models (LLMs), with appropriate priming and prompting, to perform RbAM.
- The method involves few-shot priming with 4 labelled examples, followed by prompting with a new argument pair to classify.
- Experiments are done with two LLM families - Llama 2 (13B and 70B parameter models) and Mistral (7B parameter model). Quantized 4-bit versions of the models are also tested.

Main Contributions:
- Show that the proposed LLM-based method significantly outperforms a RoBERTa baseline on the RbAM task over 10 datasets.
- The Llama 70B-4bit model achieved the best macro F1 score of 75%, surpassing all baselines. 
- The Mixtral 8x7B-4bit model achieved a macro F1 of 73%, close behind Llama 70B-4bit but with faster inference.
- Demonstrate the potential of using simple prompting strategies with LLMs for challenging NLP tasks like RbAM across diverse datasets.

The paper makes a strong case for using LLMs with priming and prompting for the RbAM task instead of fine-tuning approaches. The empirical evaluation over multiple datasets and models substantiates the proposed solution.
