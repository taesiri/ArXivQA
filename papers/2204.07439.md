# [INSTA-BNN: Binary Neural Network with INSTAnce-aware Threshold](https://arxiv.org/abs/2204.07439)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the main research question/hypothesis appears to be:

Whether considering per-instance statistical information (e.g. mean, variance, skewness) when determining the binary activation threshold in binary neural networks (BNNs) can improve accuracy compared to using batch-level statistics alone. 

The key ideas and hypotheses seem to be:

- BNNs suffer from accuracy degradation due to the aggressive quantization to only two possible activation values (+1 and -1). Controlling the distribution of the binary activations is critical.

- Previous methods determine the binary activation threshold using batch-level statistics. However, each instance in a batch has different statistical properties.

- Determining a static threshold using only batch statistics may not provide the optimal threshold for every individual input instance. 

- Considering per-instance statistical information like mean, variance, and skewness when computing the binary activation threshold can better capture crucial differences between instances.

- Basing the threshold on instance-wise statistics can significantly improve BNN accuracy.

So in summary, the central hypothesis is that instance-aware thresholds that utilize per-instance statistics can improve BNN accuracy over thresholds based solely on batch-level statistics. The authors propose and evaluate an INSTA-BNN method to test this hypothesis.


## What is the main contribution of this paper?

 This paper proposes a binary neural network (BNN) with instance-aware thresholds for activations, called INSTA-BNN. The key contributions are:

- It argues that using batch-level statistics to determine activation thresholds in BNNs is suboptimal, and instead proposes using instance-wise statistical information like mean, variance, and skewness to compute activation thresholds. 

- It introduces the INSTA-Th module to compute instance-aware thresholds using the cubic moment of the normalized activations. This helps capture mean, variance, and skewness in an efficient way.

- It also proposes an instance-aware PReLU (INSTA-PReLU) to replace the learnable PReLU in prior BNNs. 

- The paper combines the proposed modules with squeeze-and-excitation blocks to further improve accuracy.

- Experiments on ImageNet show the proposed INSTA-BNN outperforms prior BNNs by 2.3-2.5% in top-1 accuracy with comparable compute costs. For example, it achieves 71.7% top-1 accuracy on MobileNetV1, compared to 69.4% for a previous state-of-the-art BNN.

In summary, the key idea is to use instance-level statistics to compute better thresholds for activations in BNNs, instead of using batch-level statistics. This improves accuracy substantially over prior BNNs with minor overheads.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the key point of the paper:

The paper proposes a binary neural network with instance-aware thresholds that considers per-instance statistical information to optimize the thresholds for binarizing activations, achieving improved accuracy compared to prior binary neural networks.
