# [Scalable Online Exploration via Coverability](https://arxiv.org/abs/2403.06571)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
Reinforcement learning (RL) in complex, high-dimensional environments requires efficient exploration. However, existing approaches either provide inefficient exploration algorithms, or impose strong assumptions that limit their applicability. The paper aims to develop a general framework for efficient exploration that works with complex function approximators like neural networks.

Proposed Solution: 
The paper introduces "exploration objectives" - policy optimization objectives that incentivize exploring the state space in a way that enables maximizing any downstream reward function. The proposed objective, called "L1-Coverage", measures how well a policy ensemble covers the state space in an average-case sense. Optimizing this objective, referred to as "L1-coverability", provides efficient exploration with the following benefits:

1. It controls the intrinsic complexity and statistical difficulty of exploration through the L1-coverability parameter. This enables tighter sample complexity guarantees compared to prior work.

2. Optimizing the objective for a known MDP reduces to standard policy optimization, allowing integration with off-the-shelf RL methods like policy gradients and Q-learning. 

3. For unknown MDPs, the first computationally and statistically efficient model-based and model-free online RL algorithms are provided for MDPs with low L1-coverability.

Main Contributions:

1. Exploration objectives are introduced as a general framework for efficient exploration algorithms. L1-Coverage is proposed as an instance that satisfies key efficiency and complexity desiderata.

2. L1-Coverability is connected to intrinsic parameters like Block MDPs and Low-Rank MDPs, enabling sample efficiency guarantees.

3. Efficient planning algorithms are provided to optimize L1-Coverage for known MDPs. Tighter relaxations of the intractable objectives are solved instead.

4. The first statistically and computationally efficient model-based and model-free online RL algorithms are developed for low L1-coverable MDPs. These integrate L1-Coverage with policy optimization and function approximation.

5. Experiments show existing policy optimization algorithms equipped with L1-Coverage explore state spaces more quickly and effectively than common baselines.
