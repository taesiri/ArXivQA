# [Cognitive Dissonance: Why Do Language Model Outputs Disagree with   Internal Representations of Truthfulness?](https://arxiv.org/abs/2312.03729)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Neural language models (LMs) can make factual errors in their generated text. 
- However, past work has found that linear "probes" trained on LMs' internal representations are better at assessing truthfulness than the LMs' own text outputs.  
- This has led to claims that LMs "lie" or show "deception", but the reasons for probe-LM disagreements are unclear.

Approach:
- The authors categorize three types of probe-LM disagreement:
  (1) Confabulation: The LM is incorrectly confident but the probe is uncertain
  (2) Heterogeneity: Probes and LMs are accurate on different input subsets 
  (3) Deception: The probe confidently predicts the truth but the LM confidently predicts something false
- They analyze the distribution of these disagreements across datasets and models.

Key Findings:
- In most cases, probes outperform LMs due to better calibration rather than getting more right answers.
- Across models and datasets, most disagreements come from confabulation and heterogeneity, not deception.  
- Probes and LMs provide complementary signals - combining them improves accuracy.

Implications:
- Probe-LM disagreements seem to often result from different prediction pathways rather than an intent by the LM to deceive.
- The taxonomy of disagreement types provides a framework to analyze errors in future models.
- There are opportunities to improve factuality in LMs by combining internal and external signals.

Limitations: 
- Findings may not generalize across datasets or model architectures.
- The effect of different prompting strategies is unclear.

In summary, the key contribution is a taxonomy that provides a nuanced perspective on apparent "deception" in LMs, and opportunities for improving factuality using model probing.


## Summarize the paper in one sentence.

 This paper analyzes disagreements between language model outputs and internal representations of truthfulness, finding they largely result from uncertainty and model heterogeneity rather than deception.


## What is the main contribution of this paper?

 Based on my reading of the paper, the main contribution is to provide a taxonomy and formal grounding for different types of mismatches between language models' internal representations of factuality and their generated outputs. Specifically, the authors identify three categories of disagreement between model queries and probes - confabulation, deception, and heterogeneity - and analyze the distribution of these disagreement types across models and datasets. A key finding is that in current models, most disagreements seem to arise from confabulation and heterogeneity rather than deception, suggesting the mismatches may be more attributable to differences in prediction pathways rather than an intent to produce incorrect outputs. The paper also shows that ensembling queries and probes can yield improved accuracy, highlighting their complementarity. Overall, the taxonomy and analysis provide a more nuanced understanding of factual errors in language models.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and concepts are:

- Language models (LMs)
- Probing language models
- Representations of factuality/truthfulness
- Querying vs probing LMs 
- Model confabulation
- Deception
- Heterogeneity
- Disagreements between queries and probes
- Complementarity of queries and probes
- Calibration

The paper examines disagreements between language model outputs and internal representations when assessing the truth or factuality of statements. It introduces concepts like confabulation, deception, and heterogeneity to categorize different types of disagreements. It also shows that probes are often better calibrated than model queries, and that queries and probes can be complementary in an ensemble system. Overall, it aims to provide a more nuanced view of (mis)matches between internal and external expressions of factuality in language models.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in the paper:

1. The paper proposes a taxonomy of three types of disagreements between language model queries and probes: confabulation, deception, and heterogeneity. Can you explain in more detail how each type of disagreement is defined and what might cause it to occur? 

2. The probes used in this work are linear models trained on the language model's hidden representations. How might using different types of probes (e.g. nonlinear, attention-based) potentially change the distribution of disagreements observed?

3. The paper finds that in most cases, probes outperform queries due to better calibration rather than getting more high-confidence predictions correct. What implications does this have for understanding representational mismatches, and how could calibration be improved?  

4. Ensemble methods are shown to improve over probes alone on some datasets. What factors determine whether ensembling queries and probes is effective? When would you expect ensembling to help vs hurt accuracy?

5. How might the distribution of disagreement types differ when evaluating language models optimized for different objectives (e.g. truthfulness, harm, cooperation)? What new categories might emerge?

6. The level of "deception" appears very low in most models/datasets tested. Do you think this taxonomy would extend to more capable language models that can plan ahead and reason about others' mental states?

7. What other probing methods beyond linear models could provide additional insight into representational correlates of factuality in language models? What are the tradeoffs?

8. The distribution of disagreements varies substantially across datasets tested. What dataset properties might drive these differences and how could new datasets be constructed to better analyze disagreements?  

9. What are some ways the Ensemble method could be improved beyond simple averaging of predictions? Could the relative weighting be learned in a more principled way?

10. What steps could a developer take if they wanted to reduce the prevalence of "deception" disagreements when using a language model for a safety-critical application? How should they validate if interventions are successful?
