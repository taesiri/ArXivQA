# [Cognitive Dissonance: Why Do Language Model Outputs Disagree with   Internal Representations of Truthfulness?](https://arxiv.org/abs/2312.03729)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Neural language models (LMs) can make factual errors in their generated text. 
- However, past work has found that linear "probes" trained on LMs' internal representations are better at assessing truthfulness than the LMs' own text outputs.  
- This has led to claims that LMs "lie" or show "deception", but the reasons for probe-LM disagreements are unclear.

Approach:
- The authors categorize three types of probe-LM disagreement:
  (1) Confabulation: The LM is incorrectly confident but the probe is uncertain
  (2) Heterogeneity: Probes and LMs are accurate on different input subsets 
  (3) Deception: The probe confidently predicts the truth but the LM confidently predicts something false
- They analyze the distribution of these disagreements across datasets and models.

Key Findings:
- In most cases, probes outperform LMs due to better calibration rather than getting more right answers.
- Across models and datasets, most disagreements come from confabulation and heterogeneity, not deception.  
- Probes and LMs provide complementary signals - combining them improves accuracy.

Implications:
- Probe-LM disagreements seem to often result from different prediction pathways rather than an intent by the LM to deceive.
- The taxonomy of disagreement types provides a framework to analyze errors in future models.
- There are opportunities to improve factuality in LMs by combining internal and external signals.

Limitations: 
- Findings may not generalize across datasets or model architectures.
- The effect of different prompting strategies is unclear.

In summary, the key contribution is a taxonomy that provides a nuanced perspective on apparent "deception" in LMs, and opportunities for improving factuality using model probing.
