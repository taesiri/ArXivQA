# [Unsupervised Visual Representation Learning by Context Prediction](https://arxiv.org/abs/1505.05192)

## What is the central research question or hypothesis that this paper addresses?

The central hypothesis of this paper is that spatial context can be used as a source of supervisory signal to train a rich visual representation in an unsupervised manner. Specifically, the authors propose a pretext task of predicting the relative spatial position (one of 8 possible configurations) of two randomly sampled image patches. They argue that in order to do well on this task, the model needs to learn about objects, parts, and their arrangements. The key research questions addressed are:- Can a convolutional neural network learn a useful visual representation by training on this spatial context prediction task in an unsupervised manner on a large image dataset?- Will this representation capture semantics and generalize across images, even though it is trained on individual images? - Can this unsupervised pre-training provide benefits for standard computer vision tasks like object detection compared to training from scratch?- Does the representation learned capture more than just objects and parts? Does it learn about scenes and geometry as well?To summarize, the central hypothesis is that spatial context can serve as a free and unlimited source of supervisory signal for learning visual representations that capture semantics, objects, parts, and more. The key questions are whether the proposed pretext task results in such a representation, and whether it transfers to improve performance on other vision tasks.
