# [CAE v2: Context Autoencoder with CLIP Target](https://arxiv.org/abs/2211.09799)

## What is the central research question or hypothesis that this paper addresses?

 The central research question this paper addresses is how the supervision position and mask ratio affect the performance of masked image modeling (MIM) when using CLIP features as the supervision target. Specifically, the authors investigate:

1) Whether supervising only the visible (unmasked) patches can work well compared to supervising only the masked patches or both masked and visible patches. 

2) How the optimal mask ratio correlates with model size.

To study these questions, the authors develop a simple MIM framework called CAE v2 with a CLIP target and conduct experiments analyzing the effects of supervision position and mask ratio. Their key findings are:

1) Supervising only the visible patches with CLIP features works surprisingly well, achieving better performance than supervising just masked patches. This suggests the visible patches effectively distill knowledge from CLIP's semantic representations.

2) The optimal mask ratio positively correlates with model size - smaller models favor lower mask ratios while larger models prefer higher ones. This implies model scale should inform the choice of mask ratio.

Overall, this paper aims to provide new insights into effective strategies for supervision and masking in CLIP-guided masked image modeling, especially for small-scale models. The central hypothesis is that the supervision position and mask ratio are key factors influencing MIM performance with a CLIP target.


## What is the main contribution of this paper?

 The main contributions of this paper are:

- It develops a simple masked image modeling (MIM) pipeline called Context Autoencoder with CLIP Target (CAE-v2) to study two critical elements - supervision position and mask ratio - when using CLIP as the pre-training target.

- It finds that applying supervision only on the visible image patches can achieve remarkable performance, even better than supervising just the masked patches. This shows the effectiveness of distilling knowledge from CLIP into the representations of visible patches. 

- It reveals that the optimal mask ratio is positively correlated with model size - smaller models favor lower mask ratios while larger models prefer higher mask ratios. This provides a useful guideline for choosing mask ratios based on model scale.

- Driven by these two findings, the proposed CAE-v2 achieves state-of-the-art performance across various model sizes on image classification, semantic segmentation, and object detection/instance segmentation tasks.

In summary, the key contribution is providing insights into supervision position and mask ratio when using CLIP as the pre-training target, which leads to an effective yet simple MIM approach that works well across different model scales. The findings can serve as useful guidelines for future MIM methods.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence TL;DR of the paper:

The paper proposes a simple masked image modeling framework called CAE v2 using CLIP supervisions, and shows that supervising only visible patches works well and the optimal mask ratio is positively correlated with model size.
