# [CAE v2: Context Autoencoder with CLIP Target](https://arxiv.org/abs/2211.09799)

## What is the central research question or hypothesis that this paper addresses?

 The central research question this paper addresses is how the supervision position and mask ratio affect the performance of masked image modeling (MIM) when using CLIP features as the supervision target. Specifically, the authors investigate:

1) Whether supervising only the visible (unmasked) patches can work well compared to supervising only the masked patches or both masked and visible patches. 

2) How the optimal mask ratio correlates with model size.

To study these questions, the authors develop a simple MIM framework called CAE v2 with a CLIP target and conduct experiments analyzing the effects of supervision position and mask ratio. Their key findings are:

1) Supervising only the visible patches with CLIP features works surprisingly well, achieving better performance than supervising just masked patches. This suggests the visible patches effectively distill knowledge from CLIP's semantic representations.

2) The optimal mask ratio positively correlates with model size - smaller models favor lower mask ratios while larger models prefer higher ones. This implies model scale should inform the choice of mask ratio.

Overall, this paper aims to provide new insights into effective strategies for supervision and masking in CLIP-guided masked image modeling, especially for small-scale models. The central hypothesis is that the supervision position and mask ratio are key factors influencing MIM performance with a CLIP target.


## What is the main contribution of this paper?

 The main contributions of this paper are:

- It develops a simple masked image modeling (MIM) pipeline called Context Autoencoder with CLIP Target (CAE-v2) to study two critical elements - supervision position and mask ratio - when using CLIP as the pre-training target.

- It finds that applying supervision only on the visible image patches can achieve remarkable performance, even better than supervising just the masked patches. This shows the effectiveness of distilling knowledge from CLIP into the representations of visible patches. 

- It reveals that the optimal mask ratio is positively correlated with model size - smaller models favor lower mask ratios while larger models prefer higher mask ratios. This provides a useful guideline for choosing mask ratios based on model scale.

- Driven by these two findings, the proposed CAE-v2 achieves state-of-the-art performance across various model sizes on image classification, semantic segmentation, and object detection/instance segmentation tasks.

In summary, the key contribution is providing insights into supervision position and mask ratio when using CLIP as the pre-training target, which leads to an effective yet simple MIM approach that works well across different model scales. The findings can serve as useful guidelines for future MIM methods.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence TL;DR of the paper:

The paper proposes a simple masked image modeling framework called CAE v2 using CLIP supervisions, and shows that supervising only visible patches works well and the optimal mask ratio is positively correlated with model size.


## How does this paper compare to other research in the same field?

 Here are some thoughts on how this paper compares to other related work on masked image modeling:

- The key novelty of this paper is in studying two important aspects of masked image modeling (MIM) - supervision position and mask ratio - when using CLIP features as the reconstruction target. Most prior MIM research uses RGB pixels, tokens, or momentum encoder features as targets. Using CLIP is still relatively new.

- For supervision position, the common approach in MIM is to only supervise the model predictions on masked patches. This paper finds that also supervising the visible patches with CLIP features acts like a distillation signal and leads to good performance. 

- For mask ratio, prior work uses fixed ratios while this paper shows the ratio should be higher for larger model sizes. Specifically, the optimal mask ratio positively correlates with model capacity. This is a new insight.

- The proposed simple framework, CAE v2, outperforms prior art on ImageNet classification and segmentation tasks. It also provides useful guidelines for future MIM research.

- Concurrent work like MILAN and MaskDistill also use CLIP in MIM but focus more on multi-modality and loss design. This paper's contributions on supervision and mask ratio are orthogonal and complementary.

In summary, by studying supervision and masking in a CLIP MIM framework, this paper provides useful new perspectives to guide and improve masked image modeling. The performance gains on multiple benchmarks also validate the importance of these insights.
