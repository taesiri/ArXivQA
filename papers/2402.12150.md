# [Your Large Language Model is Secretly a Fairness Proponent and You   Should Prompt it Like One](https://arxiv.org/abs/2402.12150)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Existing large language models (LLMs) frequently exhibit unfairness by prioritizing dominant perspectives from majority parties while overlooking alternative viewpoints from minority parties when discussing fairness-related issues. 

- This occurs because LLMs tend to adopt a default human personality representing the training data majority, leading to biased expressions.

- Crafting representative roles and debaters to encourage diverse perspectives is complex. Summarizing the various perspectives into a coherent, balanced conclusion is also difficult.

Proposed Solution:
- The paper proposes FairThinking, an automated multi-agent pipeline designed to enhance and evaluate LLM fairness.

- It automatically generates relevant stakeholder roles with rich identites and details to faithfully represent different perspectives. 

- These roles engage in guided debates led by an impartial clerk who summarizes viewpoints and draws balanced conclusions.

- Inspired by jury principles, new roles judge the debate conclusions to evaluate fairness. More jurors favoring the conclusion indicates greater fairness.

Key Contributions:
- Empirically demonstrates that role prompting can elicit diverse LLM viewpoints and biases stem from partial training data distributions.

- Introduces an innovative approach using automated role generation and multi-agent debates to encourage, summarize and evaluate diverse perspectives for fairer LLM expressions.

- Constructs a dataset with 1,004 controversial, socially impactful questions covering three fairness topics and six attributes. 

- Extensive experiments on four LLMs demonstrate FairThinking's ability to reduce biased expressions and aligned answers while improving viewpoint diversity.
