# [OneLLM: One Framework to Align All Modalities with Language](https://arxiv.org/abs/2312.03700)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a high-quality paragraph summarizing the key points of the paper:

This paper introduces OneLLM, a novel multimodal large language model capable of aligning eight different modalities - image, video, audio, point clouds, depth/normal maps, IMU sensor data, fMRI brain activity, and text - using a unified framework. At the core of OneLLM is a universal encoder (pretrained CLIP-ViT model) and universal projection module to map all modalities into the text embedding space, enabled by learnable modality tokens. OneLLM is trained in a progressive multimodal alignment pipeline, starting from an image-text model and incrementally expanding to more modalities based on data magnitude. To leverage OneLLM's capabilities, the authors curate a comprehensive 2 million item multimodal instruction tuning dataset spanning tasks like captioning, QA, reasoning across the eight modalities. Evaluations on 25 diverse benchmarks show OneLLM outperforming specialized models and prior multimodal LLMs. OneLLM sets a new bar for flexibility and scalability in aligning modalities with language within a single model. The code, data, and models are publicly released.
