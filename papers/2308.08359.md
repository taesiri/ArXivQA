# [Membrane Potential Batch Normalization for Spiking Neural Networks](https://arxiv.org/abs/2308.08359)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central hypothesis is that adding another batch normalization layer after the membrane potential updating operation in spiking neural networks (called MPBN) can help improve model performance. The key points are:- Existing batch normalization techniques in SNNs are applied after the convolution layer, similar to CNNs. However, the spiking neuron model is more complex with spatio-temporal dynamics. The data flow after BN will be disturbed again by the membrane potential updating before the nonlinear activation (firing function).- To address this, the authors propose adding MPBN after the membrane potential updating to regulate the data flow again before firing. - They also propose a training-inference decoupled reparameterization technique to fold the MPBN parameters into the firing threshold. This eliminates the extra computation of MPBN during inference.- Experiments show MPBN improves accuracy and convergence speed across various SNN architectures and datasets. The authors attribute this to MPBN further flattening the loss landscape.In summary, the central hypothesis is that adding MPBN can better handle the data flow disturbances in spiking neurons, leading to performance improvements. The reparameterization allows this without inference cost.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contributions appear to be:- Proposing a new Membrane Potential Batch Normalization (MPBN) technique for spiking neural networks (SNNs). Previous SNN batch normalization methods operate after the convolution layer, but the authors argue this ignores the disturbance to the data flow caused by the membrane potential updating in spiking neurons. The proposed MPBN adds a batch normalization layer after membrane potential updating to re-normalize the data flow before the firing function.- A training-inference decoupled reparameterization technique to fold the trained MPBN parameters into the firing threshold. This eliminates the extra computation of MPBN during inference while retaining its benefits during training. - Showing MPBN can use both channel-wise and element-wise formulations, unlike previous SNN batch normalization methods. The element-wise MPBN learns more firing threshold values and gives slightly better performance.- Demonstrating improved accuracy and faster convergence with MPBN across various SNN architectures on CIFAR and ImageNet datasets. The MPBN models achieve state-of-the-art results with fewer time steps compared to previous methods.- Providing analysis such as ablation studies, loss landscape visualization, and pseudocode to illustrate the effectiveness of MPBN.In summary, the key contribution is proposing MPBN to handle data flow disturbances in spiking neurons, along with a reparameterization technique to remove its computational overhead during inference. Experiments verify MPBN improves SNN training and achieves new state-of-the-art accuracy.
