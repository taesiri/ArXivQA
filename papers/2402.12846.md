# [ConVQG: Contrastive Visual Question Generation with Multimodal Guidance](https://arxiv.org/abs/2402.12846)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper "ConVQG: Contrastive Visual Question Generation with Multimodal Guidance":

Problem:
- Visual question generation (VQG) aims to automatically generate meaningful and engaging questions about images. However, existing VQG systems often generate generic, not image-specific questions or fail to effectively incorporate textual guidance to focus the questions. 

- Two key challenges: 1) Generating questions grounded in image details rather than generic questions applicable to many images; 2) Incorporating textual constraints (e.g. expected answers, knowledge triplets) to guide and diversify questions while maintaining relevance to the specific image.

Proposed Solution:
- Propose ConVQG, a VQG method using contrastive learning to discriminate joint image-text embeddings from single-modality ones. 

- Two contrastive losses: image loss to distinguish from image-only questions; text loss to distinguish from text-only questions.

- Forces model to leverage both modalities to generate questions different from those produced by a single modality.

Key Contributions:
- Dual contrastive losses enable generating diverse, knowledge-rich questions tightly coupled to image specifics and external textual constraints.

- Flexible framework allows different forms of textual constraints: answers, knowledge triplets, captions.

- Outperforms state-of-the-art on multiple VQG benchmarks while showing improved image and text grounding in human evaluations.

- Contrastive objective is a simple yet effective way to improve multi-modal alignment for VQG task.

In summary, the key innovation of ConVQG is the use of contrastive losses to align multi-modal embeddings for improved image-specific and text-constrained visual question generation. Experiments validate effectiveness on multiple datasets and constraint types.
