# [Categorical Foundations of Gradient-Based Learning](https://arxiv.org/abs/2103.01931)

## What is the main contribution of this paper?

This paper presents a categorical foundation for gradient-based machine learning algorithms. The key contributions are:- It proposes the notion of a "parametric lens" as a fundamental semantic structure capturing key aspects of learning: parametrization, bidirectionality, and differentiation. Parametric lenses are defined categorically in terms of lenses and parametrized maps over Cartesian reverse differential categories.- It shows how various components of learning - models, loss functions, learning rates, optimisers - can all be uniformly understood as parametric lenses. This provides a unified perspective on these concepts and how they interact in the learning process.- It demonstrates the framework's generality by considering learning of neural networks and Boolean circuits, and by modeling various choices of loss functions and optimisers like mean squared error, softmax cross entropy, Adam, etc.- It introduces a graphical language for reasoning compositionally about how parametric lenses compose to form end-to-end learning systems.- It validates the framework via a proof-of-concept Python implementation, defining models compositionally from parametric lenses and achieving performance comparable to Keras.In summary, the paper provides a principled categorical foundation for gradient-based learning that is abstract yet captures diverse learning scenarios, supports modular reasoning, and connects theory to practice. The parametric lens is proposed as a unifying semantic structure elucidating the essence of learning algorithms.


## How does this paper compare to other research in the same field?

Here are some key ways this paper compares to other research in machine learning foundations and categorical semantics:- It takes a categorical semantics approach to providing formal foundations for machine learning, specifically focusing on gradient-based learning algorithms. This is similar in spirit to other works applying categorical semantics to understand aspects of machine learning, like Backprop as Functor (Spivak et al.) and A Categorical Theory of Patches (Fong et al.). - Compared to Backprop as Functor, this paper models learning components like loss maps and optimisers explicitly as lenses and reparameterizations rather than bundling them into a single functor. It also handles learning rates differently to avoid constraints on loss function derivatives. Overall it provides a more modular categorical semantics.- Compared to A Categorical Theory of Patches, this paper looks specifically at supervised gradient-based learning rather than patch-based training more broadly. It also uses lenses rather than patching functions as the main abstraction. The scope is more focused on gradients.- The use of lenses and parametrized lenses provides a principled mathematical abstraction tailored to bidirectional information flow and parameterization, core aspects of gradient learning. This lens-based perspective seems more directly motivated than some other categorical approaches.- The paper illustrates the framework on a range of machine learning examples like different loss functions, optimisers, and model types (neural networks and Boolean circuits). This demonstrates broad applicability across machine learning.- The implementation validates that the abstractions reflect real machine learning concepts by reproducing models and results from popular ML frameworks like Keras. This makes the practical relevance and viability of the categorical semantics clear.Overall the paper provides a focused, lens-based categorical semantics tailored to elucidating gradient-based learning. It offers precise and general foundations as well as convenient modularity and practical viability.
