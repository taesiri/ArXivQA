# [DiffDis: Empowering Generative Diffusion Model with Cross-Modal   Discrimination Capability](https://arxiv.org/abs/2308.09306)

## What is the central research question or hypothesis that this paper addresses?

Based on my understanding, the central research question this paper addresses is: Can we unify the training of generative and discriminative tasks under the diffusion process to improve both image generation and image-text discrimination capabilities in a single model?The key hypothesis is that by reformulating the image-text discriminative problem as a generative diffusion process conditioned on images, and proposing a dual-stream network architecture and unified training paradigm, it is possible to achieve superior performance on both text-guided image generation and zero-shot classification/retrieval compared to single-task models.In essence, the paper explores jointly training a generative diffusion model and a cross-modal discrimination model in an end-to-end fashion under the diffusion framework. The goal is to impart the discriminative capabilities of vision-language models like CLIP to generative diffusion models, while retaining strong generative performance.


## What is the main contribution of this paper?

The main contributions of this paper are:1. Proposing DiffDis, a unified vision-language diffusion model for both multi-modality generation and discrimination tasks under the diffusion paradigm. 2. Reformulating the image-text discriminative problem as a generative diffusion process of text embeddings conditioned on input images.3. Designing a dual-stream network architecture and proposing a diffusion-based unified training paradigm to jointly train the generative and discriminative tasks.4. Demonstrating through extensive experiments that DiffDis outperforms single-task models, with improvements on average zero-shot classification accuracy and text-guided image generation quality. DiffDis also outperforms CLIP on zero-shot classification and image-text retrieval.In summary, the key contribution is the proposed DiffDis framework that unifies generative and discriminative learning within a single diffusion model, leading to improved performance on both generative and discriminative tasks compared to single-task models. The reformulation of the discriminative problem as a conditioned diffusion process and the dual-stream architecture are notable innovations enabling this unified training.
