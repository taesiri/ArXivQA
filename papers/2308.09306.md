# [DiffDis: Empowering Generative Diffusion Model with Cross-Modal   Discrimination Capability](https://arxiv.org/abs/2308.09306)

## What is the central research question or hypothesis that this paper addresses?

Based on my understanding, the central research question this paper addresses is: Can we unify the training of generative and discriminative tasks under the diffusion process to improve both image generation and image-text discrimination capabilities in a single model?The key hypothesis is that by reformulating the image-text discriminative problem as a generative diffusion process conditioned on images, and proposing a dual-stream network architecture and unified training paradigm, it is possible to achieve superior performance on both text-guided image generation and zero-shot classification/retrieval compared to single-task models.In essence, the paper explores jointly training a generative diffusion model and a cross-modal discrimination model in an end-to-end fashion under the diffusion framework. The goal is to impart the discriminative capabilities of vision-language models like CLIP to generative diffusion models, while retaining strong generative performance.


## What is the main contribution of this paper?

The main contributions of this paper are:1. Proposing DiffDis, a unified vision-language diffusion model for both multi-modality generation and discrimination tasks under the diffusion paradigm. 2. Reformulating the image-text discriminative problem as a generative diffusion process of text embeddings conditioned on input images.3. Designing a dual-stream network architecture and proposing a diffusion-based unified training paradigm to jointly train the generative and discriminative tasks.4. Demonstrating through extensive experiments that DiffDis outperforms single-task models, with improvements on average zero-shot classification accuracy and text-guided image generation quality. DiffDis also outperforms CLIP on zero-shot classification and image-text retrieval.In summary, the key contribution is the proposed DiffDis framework that unifies generative and discriminative learning within a single diffusion model, leading to improved performance on both generative and discriminative tasks compared to single-task models. The reformulation of the discriminative problem as a conditioned diffusion process and the dual-stream architecture are notable innovations enabling this unified training.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper proposes DiffDis, a novel unified vision-language diffusion model for both image generation and discrimination tasks, which achieves superior performance over single-task baselines on text-to-image generation and zero-shot classification through a dual-stream architecture and diffusion-based training.


## How does this paper compare to other research in the same field?

Here are a few key points comparing this paper to other related work:- This paper proposes DiffDis, a novel framework to unify generative and discriminative tasks under the diffusion process. Other methods like HybViT and Pix2Seq have tried combining these tasks, but using different approaches like adding classification heads or reformulating as sequence prediction. DiffDis is the first to explore unifying them under diffusion.- For generative modeling, DiffDis builds on top of recent large-scale diffusion models like Stable Diffusion and DALL-E 2. It adapts these models to also handle discriminative tasks within the diffusion framework. Other generative models like GANs or autoregressive models have not shown the same image synthesis quality.- For discriminative modeling, DiffDis follows a similar motivation as vision-language pretraining models like CLIP, Align, and FILIP. But instead of contrastive learning, it uses diffusion probabilistic models and formulations to align image and text.- Experiments show DiffDis outperforms single-task baselines in both generative and discriminative metrics. It also outperforms CLIP in discriminative tasks, showing the benefit of incorporating generative modeling.In summary, DiffDis proposes a novel way of combining generative and discriminative tasks under diffusion, achieving strong performance in both areas. It builds on recent advances in diffusion models and vision-language pretraining, while proposing new model architectures and training formulations tailored for this joint setting. The results demonstrate the promise of this unified approach compared to prior single-task or non-diffusion models.
