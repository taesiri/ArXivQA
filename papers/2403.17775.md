# [Secure Aggregation is Not Private Against Membership Inference Attacks](https://arxiv.org/abs/2403.17775)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem Statement:
The paper investigates the privacy guarantees of secure aggregation (SecAgg), a commonly used technique in federated learning to keep the individual model updates private from the centralized server. SecAgg works by having clients jointly mask their updates cryptographically such that only the aggregated update is revealed to the server. Although SecAgg is widely presumed to provide strong privacy, its privacy guarantees have not been formally characterized. This paper aims to analyze the privacy offered by SecAgg against membership inference attacks.

Proposed Solution: 
The authors model SecAgg as a local differential privacy (LDP) mechanism, with the sum of other clients' updates acting as a source of uncontrolled noise to mask a client's individual update. Leveraging the asymptotic convergence of this noise sum to a Gaussian distribution for a large number of clients, the authors design a simple membership inference attack. They treat SecAgg as a Gaussian mechanism with correlated noise and use the likelihood ratio test to distinguish a client's submitted update between two candidates. 

Through privacy auditing, the false positive and false negative rates of this attack are evaluated. These metrics are then used to compute lower bounds on the smallest privacy parameters ε and δ for which SecAgg satisfies (ε,δ)-LDP. Experiments are conducted on federated learning for classification using the Adult and EMNIST Digits datasets.

Main Contributions:
- Formalize SecAgg as an LDP mechanism and model the sum of other clients' updates as uncontrolled noise
- Derive asymptotic privacy guarantees of SecAgg in the limit of a large number of clients, showing its connection to a Gaussian mechanism
- Design a membership inference attack that exploits this connection by using the likelihood ratio test
- Perform rigorous privacy auditing to characterize the privacy leakage of SecAgg 
- Empirically demonstrate on two datasets that SecAgg fails to prevent the membership inference attack, providing little inherent privacy

The key insight is that when model dimensionality is much higher than the number of clients, it is easy to distinguish a client's update despite aggregation. This underscores the need for additional privacy techniques like noise injection in federated learning.
