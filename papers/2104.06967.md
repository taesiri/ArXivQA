# [Efficiently Teaching an Effective Dense Retriever with Balanced Topic   Aware Sampling](https://arxiv.org/abs/2104.06967)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question appears to be:

How can we develop an efficient training method for dense passage retrieval models that achieves high effectiveness without relying on large amounts of compute resources?

The authors propose a new training approach called TAS-Balanced that uses topic-aware sampling and balanced margin sampling to compose more informative training batches. They show that this approach allows them to train an effective dense retriever model using just a single consumer-grade GPU, outperforming prior methods that rely on much more expensive hardware configurations. 

Specifically, the key hypotheses tested in the paper are:

1. Topic-aware sampling of queries and balanced margin sampling of passage pairs will compose more useful training batches that provide better learning signal, especially for in-batch negative teaching.

2. Combining pairwise and in-batch negative teaching provides complementary supervision that improves effectiveness over either alone. 

3. The proposed TAS-Balanced training approach can achieve state-of-the-art effectiveness for dense retrieval using only modest compute resources (a single GPU).

4. The dense retriever model trained with TAS-Balanced provides increased recall and accuracy that benefits larger search systems when used as a first-stage ranker.

The experiments and results provide evidence supporting these hypotheses, demonstrating the efficiency and effectiveness of the TAS-Balanced training methodology for dense passage retrieval.


## What is the central research question or hypothesis that this paper addresses?

 Based on my reading, the main research question addressed in this paper is:

How can we develop an efficient training method for dense passage retrieval models that achieves high effectiveness without relying on large amounts of compute resources? 

Specifically, the authors propose and evaluate a topic-aware sampling strategy called TAS-Balanced that aims to improve training of dense retrieval models like BERT-DOT using only a single consumer-grade GPU. Their goal is to develop a training approach that is affordable and accessible without expensive infrastructure requirements. They introduce innovations around batch sampling and dual-teacher supervision to make training more efficient. The paper evaluates whether this approach can achieve state-of-the-art effectiveness on standard IR test collections compared to other dense retrieval training techniques. Overall, the central research question is around developing an effective yet efficient training methodology for dense retrieval.


## What is the main contribution of this paper?

 Based on my reading, the main contribution of this paper is proposing an efficient topic aware sampling (TAS) and balanced margin sampling strategy for training dense passage retrieval models, called TAS-Balanced. Specifically:

- They propose to compose training batches by sampling queries from the same topic cluster (TAS), which provides more informative training signals for in-batch negatives. 

- They also balance the passage pairs selected per query based on the margin scores from a teacher model (TAS-Balanced). This reduces high-margin, low information passage pairs.

- They combine TAS-Balanced sampling with a dual teacher supervision using both pairwise and in-batch negatives. 

- Their approach requires only a single consumer-grade GPU and achieves state-of-the-art results on the TREC Deep Learning track benchmarks, outperforming methods that use much more computational resources for training.

- They demonstrate the effectiveness and robustness of their approach across different query sets and show how it can improve results in larger retrieval pipelines.

In summary, the key contribution is an efficient and effective passage retrieval training technique that does not rely on large-scale compute infrastructure, making high quality dense retrieval more accessible. The TAS-Balanced sampling and dual teacher supervision are the main novel components enabling the efficiency and performance gains.


## What is the main contribution of this paper?

 The main contribution of this paper is proposing an efficient topic-aware sampling (TAS) strategy for training dense passage retrieval models. Specifically, the key ideas are:

- Topic Aware Sampling (TAS): Clustering the training queries into topics and sampling queries from the same topic cluster to compose training batches. This provides more informative batches for training with in-batch negatives. 

- Balanced Margin Sampling: Sampling passage pairs for each query in a balanced way across the range of teacher score margins. This reduces skewed sampling of easy negatives.

- Combining TAS and balanced margin sampling into an efficient batch sampling strategy called TAS-Balanced.

- Using a dual teacher supervision with both a pairwise teacher (BERTcat) and an in-batch negatives teacher (ColBERT) for training.

- Showing that TAS-Balanced training with dual supervision achieves state-of-the-art results on dense retrieval benchmarks while only requiring a single consumer-grade GPU. 

In summary, the key contribution is developing the TAS-Balanced batch sampling technique to enable efficient and effective training of dense retrievers without requiring large compute resources. The results demonstrate improved accuracy over prior training methods and robustness to different randomizations.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Based on my understanding, the key takeaway from this paper is that the authors propose an efficient topic-aware and balanced margin sampling strategy called TAS-Balanced to improve training of dense passage retrieval models like BERT-dot. Their method trains high quality dense retrievers using only a single consumer-grade GPU, outperforming more compute-intensive methods, and also integrates well into larger search pipelines.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Based on my reading of the paper, here is a one sentence summary: 

The paper proposes an efficient topic-aware sampling and dual-teacher supervision method to train an effective dense passage retriever using only a single consumer-grade GPU, achieving state-of-the-art results on the TREC Deep Learning track's query sets.


## How does this paper compare to other research in the same field?

 Here are a few key points comparing this paper to other related research in dense passage retrieval:

- The paper focuses on efficient training of dense retrieval models (specifically BERT-based dual encoders) using only modest compute resources. Many other leading approaches require large batches and multiple GPUs/TPUs to train effectively. This work trains competitively using a single GPU.

- The proposed TAS-Balanced sampling technique aims to compose more informative training batches by sampling queries from the same topic clusters and balancing passage pairs based on margin. This is a novel way to improve training efficiency without extra compute. Other work has explored dynamic passage sampling strategies during training but is more resource intensive.

- For model training, the paper uses a dual-teacher approach combining pairwise and in-batch negatives to better supervise the student dense retriever. Other methods have used one or the other, but combining knowledge from both teachers is innovative.

- The trained model achieves state-of-the-art effectiveness on the TREC Deep Learning track benchmarks while having low query latency comparable to BM25. This enables the method to be deployed efficiently in a production setting.

- When integrated in a pipeline, the TAS-trained retriever boosts end-to-end performance by increasing candidate recall for later re-ranking. Many dense retrieval papers focus only on standalone performance. 

- The techniques are general and could likely improve other dual-encoder architectures besides BERT. But the goal is a simple and fast deployable model instead of maximizing quality at the expense of efficiency.

In summary, this work makes advances in dense retriever training efficiency, requiring less compute than related work while achieving excellent results. The innovations in sampling, teacher combination, and pipeline integration differentiate it from other research.


## How does this paper compare to other research in the same field?

 Here are a few key points comparing this paper to other research on training dense passage retrieval models:

- This paper focuses on efficiently training an effective dense retriever using only a single consumer-grade GPU, while most other work uses much larger compute resources like multiple high-end GPUs. For example, RocketQA and ANCE use 8x V100 GPUs while this method needs only a single GTX 1080Ti.

- The paper proposes a novel training approach called TAS-Balanced that combines topic-aware query sampling and balanced margin passage pair sampling. This aims to improve the informativeness of each training batch. Other recent work like RocketQA and ANCE use dynamic negative sampling which requires repeated indexing.

- For supervision, this paper uses a dual teacher approach with both pairwise (BERTcat) and in-batch (ColBERT) signals. Other methods like ANCE, RocketQA, and TCT use either pairwise or in-batch superivsion. Using both teachers together is a key contribution.

- Experiments show the TAS-Balanced training outperforms other methods like ANCE, RocketQA, TCT, and Margin-MSE on the dense TREC-DL benchmarks while using much less compute. It also achieves competitive results on the sparse MSMARCO dataset.

- This paper provides the first dense retriever that outperforms BM25 and docT5query on recall at all cutoffs on the TREC benchmarks. It also shows improved results when used as a first-stage ranker in full search pipelines.

In summary, this paper makes dense retriever training more efficient and accessible while advancing effectiveness especially on dense ranking benchmarks like TREC-DL. The focus on reduced compute sets it apart from other contemporaneous work on dense retrieval training.


## What future research directions do the authors suggest?

 Based on my reading of the paper, here are some of the main future research directions suggested by the authors:

- Improving re-ranking models to take better advantage of the increased recall from dense retrievers like TAS-Balanced. The authors note that even though TAS-Balanced improves recall at higher cutoffs, the state-of-the-art mono-duo-T5 re-ranker does not fully capitalize on this. They suggest re-training the re-ranker on the dense retriever candidate distribution.

- Exploring different clustering algorithms and query representations for creating the topic clusters in TAS. The authors used k-means clustering on baseline BERT query representations, but other algorithms and representations could be experimented with.

- Applying TAS-Balanced to other dense retriever architectures besides BERT-DOT. The authors focused on BERT-DOT, but TAS-Balanced could likely improve other architectures.

- Adapting TAS-Balanced to other IR tasks like product search, question answering, etc. The authors demonstrated TAS-Balanced on passage retrieval, but it could be applicable more broadly.

- Combining TAS-Balanced with other training improvements like hard negative mining. There could be complementary benefits to pairing TAS-Balanced with other advancements.

- Reducing the inference cost and latency of dense retrievers without sacrificing effectiveness. This could involve model distillation, quantization, etc.

In summary, the authors point to various ways to build on TAS-Balanced, integrate it into larger systems, and adapt it to other tasks and architectures as promising research directions. The key theme seems to be leveraging TAS-Balanced as a strong foundation for broader innovations in dense retrieval.


## What future research directions do the authors suggest?

 The authors suggest a few potential future research directions:

- Improving the fairness and transparency of large language models. The authors note that existing models replicate unjust human biases and unintentionally encode harmful assumptions. Future research could focus on improving model understanding, identifying and mitigating harms, and developing more transparent and controllable models.

- Learning in the context of broader capabilities. The authors argue language should not be studied in isolation but in the context of other capabilities like perception, reasoning, social intelligence, etc. Future work could explore multimodal and embodied agents. 

- Learning from less data. Current large language models require massive amounts of data, but future work could explore efficient learning and better use of limited data through methods like transfer learning and meta-learning.

- Scaling up capabilities. The authors suggest future models could achieve more advanced comprehension, reasoning, and abstraction capabilities by scaling up model size and training data. However, this must be balanced with efficiency and transparency concerns.

- Developing more generalized AI. The authors argue current models excel at narrow tasks but lack generalized intelligence. Future research could explore architectures and training methods aimed at developing AI agents with more flexibility, common sense, and generalization.

In summary, the main research directions proposed involve improving model understanding, generalization, and capabilities while also addressing issues of fairness, transparency, and efficiency. The authors see promise in exploring multimodal and embodied agents, transfer learning, and meta-learning as part of developing more human-like AI.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the paper:

This paper proposes an efficient topic aware and balanced margin sampling strategy (TAS-Balanced) for training dense passage retrieval models like BERT-dot. The key ideas are 1) clustering queries into topics before training and sampling queries from the same topic cluster to compose a training batch; and 2) sampling passage pairs for each query to balance easy and hard negatives based on margin bins. This improves the training signal from in-batch negatives. The authors show that TAS-Balanced training of a 6-layer BERT-dot model on a single GPU achieves state-of-the-art results on the TREC Deep Learning track datasets, outperforming methods that use more expensive setups like refreshed indexes or very large batch sizes. TAS-Balanced also improves results when used as the first stage retriever in a pipeline with docT5query and the mono-duo-T5 reranker. The efficient training procedure allows wider adoption of high-quality dense retrievers without expensive infrastructure.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the paper:

This paper proposes an efficient topic aware sampling (TAS-Balanced) approach and dual-teacher supervision method for training dense passage retrieval models like BERT-dot. The proposed sampling technique first clusters training queries into topics. Batches are then composed by sampling queries from a single topic cluster. Passage pairs are selected to balance easy and hard negatives based on teachers scores. For supervision, both a pairwise BERT-cat teacher and an in-batch ColBERT teacher are utilized. Experiments show the TAS-Balanced training with dual supervision outperforms other methods on the TREC-DL benchmarks while only requiring a single consumer-grade GPU. The trained dense retriever is very fast with 64ms query latency and achieves new state-of-the-art results. When combined with document expansion and reranking models in a pipeline, gains over competitive baselines are maintained. The proposed training approach enables developing high-quality dense retrievers without expensive infrastructure.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the paper:

This paper proposes a new method for efficiently training dense passage retrieval models called TAS-Balanced. The key ideas are to compose training batches by sampling queries that are topically related using query clustering (TAS), and sampling passage pairs that balance easy and hard negatives based on passage score margins (Balanced). 

The authors show that TAS-Balanced training with a dual teacher supervision signal outperforms baseline training and other state-of-the-art dense retrieval training techniques. A 6-layer BERT-based retriever model trained with TAS-Balanced achieves new state-of-the-art effectiveness results on the TREC Deep Learning track datasets, while using only a single consumer-grade GPU. The training method is also shown to be robust to different random orderings. When used as a first-stage retriever, TAS-Balanced improves document ranking pipelines, providing increased latency-effectiveness tradeoffs. Overall, this work presents an efficient way to train high-quality dense retrievers without expensive hardware requirements.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the paper:

This paper proposes an efficient topic aware sampling strategy for training dense passage retrieval models called TAS-Balanced. The key ideas are to cluster the queries into topics, compose batches by sampling queries from the same topic cluster, and balance the passage pairs selected per query based on the margin scores from a teacher model. This allows the model to learn better representations by exposing it to concentrated topic information in each batch and a balanced distribution of passage pairs during training. 

The authors show that TAS-Balanced training outperforms other dense retrieval training techniques like ANCE and RocketQA, especially on densely judged TREC query sets, while using only a single GPU. TAS-Balanced reaches state-of-the-art effectiveness results on TREC-DL benchmarks and is robust to different random orderings during training. When used in a pipeline, TAS-Balanced fused with docT5query outperforms higher latency systems on several metrics. Overall, this work presents an efficient way to train high quality dense retrievers without expensive hardware requirements. The sampling strategies could be beneficial for other neural ranking models as well.


## Summarize the main method used in the paper in one paragraph.

 Here is a one paragraph summary of the main method used in the paper:

The paper proposes a topic aware sampling (TAS) strategy for training dense passage retrieval models like BERT-dot. The key idea is to first cluster the training queries into topics using k-means clustering on query embeddings. Then, instead of randomly sampling queries for each training batch, the method samples queries from only a few topic clusters per batch. This concentrates topically related information into each batch. Furthermore, the method balances the sampling of positive and negative passages for each query based on the margin between their relevance scores from a teacher model. This is meant to reduce the skew between easy vs hard negative passages. The method trains a BERT-dot model using both a pairwise BERT-cat teacher for between-sample interactions and an in-batch ColBERT teacher for within-batch interactions. Experiments show this TAS-balanced training strategy with dual teacher supervision can achieve state-of-the-art passage retrieval results with efficient training on a single GPU. The proposed sampling focuses information into each batch and balances passage pairs to improve the learning signal, avoiding the need for very large batch sizes.


## Summarize the main method used in the paper in one paragraph.

 The paper proposes a method to improve the training of dense retrieval models called TAS-Balanced. The main ideas are:

1) Use topic-aware sampling (TAS) to compose training batches. Queries are first clustered into topics. Then each batch is composed by sampling queries from a single cluster, so that each batch contains semantically related queries. This gives more useful negatives when using in-batch training.

2) Use balanced margin sampling for passage pairs. Passage pairs for each query are sampled to balance easy and hard negatives based on the margin predicted by the teacher model. This avoids always sampling easy negatives. 

3) Combine pairwise and in-batch distillation with two teacher models. A BERT-CAT teacher provides pairwise scores, while a ColBERT teacher provides in-batch scores. This dual supervision gives better training signal.

4) Train on a single consumer-grade GPU to be computationally efficient. The model uses a 6-layer DistilBERT with a batch size of 32.

The combination of TAS-Balanced sampling and dual teacher supervision allows the model to be trained effectively on a single GPU, while achieving state-of-the-art accuracy on the TREC-DL benchmarks. The method does not rely on expensive compute like indexing or large batch sizes.


## What problem or question is the paper addressing?

 This paper is addressing the problem of efficiently training effective dense retrieval models. Some key points:

- Dense retrieval models like BERT use vector representations for querying and ranking passages, enabling low latency retrieval. However, training these models often requires a lot of compute resources.

- The paper proposes an efficient topic-aware and balanced margin sampling strategy called TAS-Balanced to improve training. It also uses a dual-teacher supervision approach.

- TAS-Balanced allows training an effective dense retriever on a single consumer-grade GPU in under 48 hours, compared to other methods that require multiple expensive server GPUs. 

- Experiments show state-of-the-art results on the TREC Deep Learning track benchmarks using the proposed training strategy, outperforming various baselines.

- The paper demonstrates the robustness of the approach using different randomization seeds, showing it does not inadvertently overfit.

- It also analyzes the trained model as part of larger retrieval pipelines, showing benefits in low latency and re-ranking scenarios.

In summary, the key focus is developing an efficient way to train high-quality dense retrieval models without expensive hardware requirements. The proposed TAS-Balanced sampling and training strategy is shown to be effective and robust for this goal.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, here are some of the key keywords and terms:

- Dense retrieval - The paper focuses on training neural dense retrieval models like BERT-based dual encoders. These models encode queries and passages into dense vector representations for efficient retrieval.

- Knowledge distillation - The paper proposes using knowledge distillation with teacher models like BERT-CAT and ColBERT to improve training of the dense retriever BERT-DOT student model. 

- Topic-aware sampling - A key contribution is using topic-aware sampling to select related queries from clusters to compose more informative training batches.

- Balanced margin sampling - Sampling passage pairs in a balanced way based on margin ranges helps train the retriever better.

- Dual teacher supervision - The paper introduces combining pairwise and in-batch negative teachers for knowledge distillation during training.

- Low resource requirements - A goal is minimizing compute resources needed by avoiding repeated indexing or large batches. The model is trainable on a single GPU.

- Retrieval pipelines - Experiments evaluate the dense retriever in two-stage pipelines with re-ranking models, showing benefits especially for low latency.

- TREC Deep Learning Track - The paper uses the densely judged TREC-DL 2019 and 2020 query sets for evaluation.

- MSMARCO - The Microsoft MARCO passage collection is used for training and evaluation.


## Based on the paper, what questions should we ask to create a comprehensive summary of it? Please list at least 10 questions.

 Here are 10 suggested questions to create a comprehensive summary of the paper:

1. What is the main goal or contribution of this work?

2. What problem is this work trying to solve? What are the limitations of existing methods that the authors identify?

3. What is the proposed approach or method? How does it work? 

4. What architecture and models are used? What are the key components?

5. What datasets were used for experiments? How was evaluation performed?

6. What were the main results? How does the proposed method compare to baselines and previous work?

7. What conclusions or findings can be drawn from the results and analysis? Do the authors achieve their goals?

8. What are the limitations, weaknesses, or areas for improvement for the proposed method?

9. What broader impact or future work is discussed? How might this research be extended or built upon?

10. Are there any interesting related works or background that provide context? What other research is highlighted for comparison?


## Design 10 in-depth questions about method proposed in this paper and avoid simple question.

 Here are 10 in-depth questions about the method proposed in this paper:

1. How exactly does the Topic Aware Sampling (TAS) strategy work? Can you walk through the process of creating a training batch using TAS?

2. The paper claims TAS leads to more informative training batches. Why do queries clustered by topic provide better signal for in-batch negative training compared to randomly sampled queries? 

3. What is the motivation behind balancing the passage pair margins during sampling? How does this lead to a less skewed distribution of training samples?

4. Why does the paper use a combination of pairwise and in-batch negative distillation? What are the relative advantages of each approach?

5. How does the proposed dual-supervision framework combine the pairwise and in-batch negative losses? What impact does the weighting hyperparameter Î± have?

6. What are the practical benefits of being able to effectively train dense retrieval models on a single consumer-grade GPU? How does this compare to other state-of-the-art methods?

7. The paper demonstrates robustness to different random orderings during training. Why is this important to show, and how do the results support the claim of robustness?

8. How well does the TAS-Balanced dense retriever perform as the first stage in a larger search pipeline? Does it provide gains compared to BM25 and other methods?

9. What are some possible reasons that re-ranking effectiveness doesn't fully benefit from the increased recall of the TAS retriever? How could this be addressed in future work?

10. What are some limitations of the proposed approach? What directions could be explored to further improve effectiveness or efficiency?


## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a high-quality paragraph summarizing the key points of the paper:

This paper proposes a novel training method called Topic Aware Sampling with Balanced margin (TAS-Balanced) to improve the training of dense passage retrieval models like BERT$_\text{DOT}$. The key ideas are: (1) Clustering all the training queries into topics before training and then sampling queries from the same topic cluster for each training batch. This provides more informative in-batch negatives. (2) Sampling passage pairs in a balanced way across the range of teacher score margins to avoid too many easy negatives. The authors show TAS-Balanced training of BERT$_\text{DOT}$, supervised by both a pairwise BERT$_\text{CAT}$ teacher and an in-batch ColBERT teacher, achieves state-of-the-art results on the TREC Deep Learning track datasets, outperforming previous methods like ANCE and RocketQA. A key advantage is that the training can be done efficiently on a single consumer-grade GPU in 48 hours, unlike other methods that require multiple expensive GPUs. The results are shown to be robust to different random seeds during training. Further experiments demonstrate the effectiveness of the TAS-Balanced trained model as the first stage in a multi-stage ranking pipeline, improving results at low latency compared to BM25 and docT5query baselines. Overall, this work provides an important contribution in training high-quality dense retrievers efficiently.


## Summarize the paper in one sentence.

 The paper proposes an efficient topic aware and balanced margin sampling technique to train a dense passage retrieval model with dual teacher supervision, achieving strong performance using limited compute resources.


## Summarize the paper in one paragraphs.

 Here is a one paragraph summary of the paper:

This paper proposes an efficient training method for dual-encoder dense retrieval models called Topic Aware Sampling with Balanced Margin (TAS-Balanced). The key ideas are to 1) cluster queries into topics and sample queries from the same topic in a batch to provide more informative in-batch negatives, and 2) balance the sampling of passage pairs based on the margin predicted by a teacher model to reduce easy negatives. The authors train a lightweight BERT-based dual-encoder model using a novel dual-teacher supervision paradigm, combining a BERT-CAT teacher for pairwise distillation and a ColBERT teacher for in-batch negatives. This TAS-Balanced training with dual supervision produces state-of-the-art retrieval quality using only a single consumer-grade GPU, as opposed to much more expensive setups used by other methods. Experiments on the TREC Deep Learning Track and MSMARCO datasets show significant improvements in retrieval metrics over baselines. The trained model also provides increased latency-constrained recall in retrieval pipelines, though some high-latency rerankers do not fully exploit this. Overall, this work presents an efficient way to train high-quality dense retrievers without expensive hardware requirements.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 potential in-depth questions about the method proposed in this paper:

1. The authors propose a Topic Aware Sampling (TAS) strategy for composing training batches. How does TAS help provide more useful signals for in-batch negative teaching compared to random sampling? What are the trade-offs with grouping queries by topic clusters?

2. The paper introduces a Balanced Margin Sampling technique on top of TAS. Explain how this balances the distribution of passage pairs and why this is beneficial for training. How does the paper implement the binning of margin ranges?

3. The dual-teacher supervision combines pairwise and in-batch negative signals. Discuss the strengths and limitations of each teacher model (BERT-CAT and ColBERT). Why is using both together advantageous? 

4. TAS-Balanced training requires fewer computational resources than methods like ANCE, RocketQA, etc. Analyze the differences in training procedures and hardware requirements. How does TAS-Balanced reduce resource usage?

5. How robust is the proposed TAS-Balanced method to different random orderings of clusters, queries, and passage pairs? Discuss the randomization study in the paper and what it demonstrates about the approach.

6. Compare and contrast the results on the TREC Deep Learning track datasets versus the MSMARCO dataset. Why might TAS-Balanced be particularly effective for dense judgment sets like TREC?

7. The paper examines using TAS-Balanced in larger search pipelines. Analyze how the increased neural first-stage recall impacts end-to-end performance compared to BM25 and docT5query.

8. Discuss the limitations shown when re-ranking TAS-Balanced results with mono-duo-T5. Why doesn't higher recall directly translate to better final results?

9. How does the Margin-MSE loss for in-batch negatives compare to list-based losses like LambdaLoss? Why might optimizing the score distribution be better than just the ranking?

10. What future work could build upon the TAS-Balanced training technique? For example, how could it be adapted for other dense retrievers besides BERT-DOT or extended to multi-stage training?
