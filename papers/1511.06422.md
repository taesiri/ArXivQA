# [All you need is a good init](https://arxiv.org/abs/1511.06422)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the main research question seems to be: How can we develop a simple and efficient initialization strategy that enables effective training of very deep neural networks? Specifically, the authors propose a new initialization method called "layer-sequential unit-variance" (LSUV) initialization that aims to facilitate end-to-end training of very deep networks using standard stochastic gradient descent. The key hypotheses tested in the paper are:- The proposed LSUV initialization will allow very deep nets to be trained from scratch in a single optimization run, without needing complex procedures like progressively adding layers.- LSUV initialization will lead to very deep nets that achieve state-of-the-art or near state-of-the-art results across different datasets (MNIST, CIFAR, ImageNet), compared to more complex initialization schemes.- LSUV initialization will work effectively across different network architectures (e.g. FitNets, CaffeNet, GoogLeNet) and activation functions (ReLU, maxout, tanh).So in summary, the main research question is about developing and validating a simple but effective initialization method to enable efficient end-to-end training of very deep neural networks. The LSUV initialization method is proposed and experimentally evaluated to address this question.
