# [All you need is a good init](https://arxiv.org/abs/1511.06422)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the main research question seems to be: How can we develop a simple and efficient initialization strategy that enables effective training of very deep neural networks? Specifically, the authors propose a new initialization method called "layer-sequential unit-variance" (LSUV) initialization that aims to facilitate end-to-end training of very deep networks using standard stochastic gradient descent. The key hypotheses tested in the paper are:- The proposed LSUV initialization will allow very deep nets to be trained from scratch in a single optimization run, without needing complex procedures like progressively adding layers.- LSUV initialization will lead to very deep nets that achieve state-of-the-art or near state-of-the-art results across different datasets (MNIST, CIFAR, ImageNet), compared to more complex initialization schemes.- LSUV initialization will work effectively across different network architectures (e.g. FitNets, CaffeNet, GoogLeNet) and activation functions (ReLU, maxout, tanh).So in summary, the main research question is about developing and validating a simple but effective initialization method to enable efficient end-to-end training of very deep neural networks. The LSUV initialization method is proposed and experimentally evaluated to address this question.


## What is the main contribution of this paper?

The main contribution of this paper is proposing a simple initialization procedure called Layer-sequential Unit-Variance (LSUV) initialization for training deep neural networks. The key ideas are:- Initialize the weights of each convolution and fully-connected layer with orthonormal matrices. This helps decorrelate the activations and gradients. - Then proceed from the first layer to the final layer, normalizing the output variance of each layer to 1 based on a minibatch of data. This helps stabilize the activations and gradients across layers.- They show this LSUV initialization allows efficient end-to-end training of very deep networks using standard SGD, without needing complex schemes like batch normalization or residual connections. - Experiments on MNIST, CIFAR10/100, and ImageNet show LSUV achieves state-of-the-art or competitive results with different network architectures and activation functions like ReLU, maxout, tanh.So in summary, the main contribution is proposing this simple but effective LSUV initialization procedure that enables stable training of very deep networks. The key idea is orthogonal initialization combined with per-layer output variance normalization.
