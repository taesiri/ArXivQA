# [Modality-invariant Visual Odometry for Embodied Vision](https://arxiv.org/abs/2305.00348)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading, the key research questions and hypotheses addressed in this paper are:

- How can we develop a visual odometry (VO) model that is robust and performs well even when only a subset of modalities (e.g. RGB, depth) are available at test time? The hypothesis is that a Transformer-based architecture with explicit modality-invariance training can achieve this.

- Can we develop a VO model that outperforms previous methods while being trained on much less data? The hypothesis is that by incorporating multi-modal pre-training and an action token prior, the data requirements can be significantly reduced. 

- Is a single modality-invariant VO model able to match the performance of separate uni-modal models? The hypothesis is that with the proposed training, a single model can effectively deal with varying sensor suites at test time.

- How do Transformers compare to CNNs for the VO task in terms of performance and modality invariance? The hypothesis is that Transformers are better suited for this task.

- What is the impact of different model design choices like multi-modal pre-training and action conditioning? The hypothesis is that they provide useful biases that improve performance and sample efficiency.

In summary, the key goal is developing a flexible, reusable VO model that performs well even with limited modalities available at test time. The main hypotheses are around Transformer architectures, modality-invariance training, incorporaring biases, and comparing to CNNs.


## What is the main contribution of this paper?

 The main contribution of this paper is presenting a novel modality-invariant visual odometry (VO) method based on a Transformer architecture called VO Transformer (VOT). The key highlights are:

- Proposes VOT, a Transformer-based framework for visual odometry that can handle varying numbers and types of input modalities (e.g. RGB, depth, semantics). This allows flexibility when sensors fail or availability changes at test time.

- Uses multi-modal pre-training and an action token prior to significantly reduce the data requirements compared to prior CNN-based VO methods. VOT matches state-of-the-art performance while training on only 5% of previous datasets.

- Introduces explicit modality-invariance training by randomly dropping modalities during training. This allows a single VOT model to match the performance of separate uni-modal models.

- Evaluates VOT on the Habitat simulator and shows it outperforms previous methods on the Habitat Challenge 2021 benchmark while being more robust to missing modalities at test time.

In summary, the key contribution is a new modality-invariant VO method based on Transformers that is more robust, flexible, and sample efficient than prior CNN-based approaches. This could enable VO reuse across different sensors and platforms in real-world robotics applications.
