# [Modality-invariant Visual Odometry for Embodied Vision](https://arxiv.org/abs/2305.00348)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading, the key research questions and hypotheses addressed in this paper are:

- How can we develop a visual odometry (VO) model that is robust and performs well even when only a subset of modalities (e.g. RGB, depth) are available at test time? The hypothesis is that a Transformer-based architecture with explicit modality-invariance training can achieve this.

- Can we develop a VO model that outperforms previous methods while being trained on much less data? The hypothesis is that by incorporating multi-modal pre-training and an action token prior, the data requirements can be significantly reduced. 

- Is a single modality-invariant VO model able to match the performance of separate uni-modal models? The hypothesis is that with the proposed training, a single model can effectively deal with varying sensor suites at test time.

- How do Transformers compare to CNNs for the VO task in terms of performance and modality invariance? The hypothesis is that Transformers are better suited for this task.

- What is the impact of different model design choices like multi-modal pre-training and action conditioning? The hypothesis is that they provide useful biases that improve performance and sample efficiency.

In summary, the key goal is developing a flexible, reusable VO model that performs well even with limited modalities available at test time. The main hypotheses are around Transformer architectures, modality-invariance training, incorporaring biases, and comparing to CNNs.
