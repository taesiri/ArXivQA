# [Modality-invariant Visual Odometry for Embodied Vision](https://arxiv.org/abs/2305.00348)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading, the key research questions and hypotheses addressed in this paper are:

- How can we develop a visual odometry (VO) model that is robust and performs well even when only a subset of modalities (e.g. RGB, depth) are available at test time? The hypothesis is that a Transformer-based architecture with explicit modality-invariance training can achieve this.

- Can we develop a VO model that outperforms previous methods while being trained on much less data? The hypothesis is that by incorporating multi-modal pre-training and an action token prior, the data requirements can be significantly reduced. 

- Is a single modality-invariant VO model able to match the performance of separate uni-modal models? The hypothesis is that with the proposed training, a single model can effectively deal with varying sensor suites at test time.

- How do Transformers compare to CNNs for the VO task in terms of performance and modality invariance? The hypothesis is that Transformers are better suited for this task.

- What is the impact of different model design choices like multi-modal pre-training and action conditioning? The hypothesis is that they provide useful biases that improve performance and sample efficiency.

In summary, the key goal is developing a flexible, reusable VO model that performs well even with limited modalities available at test time. The main hypotheses are around Transformer architectures, modality-invariance training, incorporaring biases, and comparing to CNNs.


## What is the main contribution of this paper?

 The main contribution of this paper is presenting a novel modality-invariant visual odometry (VO) method based on a Transformer architecture called VO Transformer (VOT). The key highlights are:

- Proposes VOT, a Transformer-based framework for visual odometry that can handle varying numbers and types of input modalities (e.g. RGB, depth, semantics). This allows flexibility when sensors fail or availability changes at test time.

- Uses multi-modal pre-training and an action token prior to significantly reduce the data requirements compared to prior CNN-based VO methods. VOT matches state-of-the-art performance while training on only 5% of previous datasets.

- Introduces explicit modality-invariance training by randomly dropping modalities during training. This allows a single VOT model to match the performance of separate uni-modal models.

- Evaluates VOT on the Habitat simulator and shows it outperforms previous methods on the Habitat Challenge 2021 benchmark while being more robust to missing modalities at test time.

In summary, the key contribution is a new modality-invariant VO method based on Transformers that is more robust, flexible, and sample efficient than prior CNN-based approaches. This could enable VO reuse across different sensors and platforms in real-world robotics applications.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the key points from the paper:

The paper proposes a Transformer-based visual odometry model called VOT (Visual Odometry Transformer) that is trained to be robust to missing input modalities like RGB or depth, allowing it to work with different sensor setups without needing to be retrained.


## How does this paper compare to other research in the same field?

 Here are a few key comparisons between this paper and other research in visual odometry:

- This paper proposes a Transformer-based model (VO-Transformer) for visual odometry, whereas most prior work uses CNN architectures. Transformers have become quite popular in computer vision recently, but have not been extensively explored for VO.

- The VO-Transformer model is designed to be modality-agnostic and modality-invariant. This means it can accept different combinations of input modalities (e.g. RGB, depth, semantics) and can deal with missing modalities at test time. Most other VO methods are designed for a fixed sensor suite.

- The authors use multi-modal pre-training (with MAE) and an action token to significantly reduce the data requirements compared to prior VO methods. They show compelling results with only 5% of the data used in previous work. Reducing data needs is an important consideration for practical VO systems.

- They demonstrate the VO-Transformer in a challenging realistic indoor navigation setting (Habitat simulator), where VO replaces GPS/compass for localization. Most learning-based VO methods are tested in simpler settings like KITTI.

- The VO-Transformer achieves state-of-the-art results on the Habitat Challenge 2021 benchmark while using far less data than the top methods. This shows it better utilizes the available data.

- One limitation is that their method still depends on depth information, whereas some works have shown VO from RGB only. The modality-invariance is also only shown for RGB+depth. Expanding to other sensors could be interesting future work.

Overall, the VO-Transformer's modality flexibility, data efficiency, and strong performance in a realistic navigation setting are notable contributions compared to prior VO research. The work seems highly impactful for deployable learning-based VO.
