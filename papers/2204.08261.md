# [Visio-Linguistic Brain Encoding](https://arxiv.org/abs/2204.08261)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the key research questions and hypotheses addressed in this paper are:1) Can neural encoding models based on deep learning architectures accurately predict fMRI responses in the human brain across different regions and for both visual and linguistic stimuli? 2) Will image-based Transformers (like ViT, DEiT, BEiT) that capture hierarchical visual representations show better correlation with hierarchical visual processing regions in the brain compared to CNNs?3) Will multi-modal Transformers like VisualBERT and LXMERT that learn joint visio-linguistic representations lead to better encoding accuracy across both visual and language brain regions compared to uni-modal vision or language models?4) How do different layers in Transformers correlate with different brain region representations and can this give insights into the neuroscience of visual and linguistic processing?5) Can studying how these neural network models encode brain activity lead to new computational insights and predictions about human cognition - e.g. does linguistic processing implicitly influence even passive visual processing?The central hypothesis seems to be that using the latest visual and multi-modal Transformer architectures for encoding will lead to improved accuracy and new neuroscientific insights compared to previous models like CNNs or RNNs given the hierarchical and multi-modal processing capabilities of Transformers. The paper aims to provide experimental evidence for these hypotheses through extensive empirical evaluation on multiple fMRI datasets.
