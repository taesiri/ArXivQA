# ExpeL: LLM Agents Are Experiential Learners

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be:Can an agent learn autonomously and improve its performance by accumulating and utilizing experiences across different tasks, without requiring access to model parameters or gradient updates?The authors propose an "Experiential Learning (ExpeL) agent" framework that allows an agent to:1) Autonomously gather experiences from a collection of training tasks through trial and error using a self-reflection technique. 2) Extract insights and memorable trajectories from these gathered experiences.3) Apply the extracted insights and recall past successful trajectories as examples during evaluation on unseen test tasks.The key hypothesis seems to be that by accumulating experiences across tasks and extracting reusable knowledge in the form of insights and example trajectories, the agent can improve its test-time performance on new tasks compared to baseline planning agents like ReAct. The agent is able to learn without updating model parameters, making the approach compatible with large proprietary language models.So in summary, the central research question is whether an agent can learn and improve solely from its own accumulated experiences, without gradient updates, by extracting and applying knowledge - which is what the proposed ExpeL agent aims to demonstrate.


## What is the main contribution of this paper?

The main contribution of this paper seems to be proposing a novel learning agent called ExpeL (Experiential Learning) that can autonomously gather experiences from training tasks and learn from them to improve performance on evaluation tasks, without requiring gradient updates to the model parameters. Some key aspects of the ExpeL agent:- It gathers diverse experiences (both successes and failures) on training tasks using an algorithm like Reflexion that allows retrying and reflecting on failures. - It extracts high-level insights from these experiences by comparing successes and failures on the same tasks, as well as identifying common patterns across different successful tasks.- At evaluation time, it augments the task prompt with relevant extracted insights and retrieves the most similar successful trajectories from training as examples. - This approach allows the agent to learn and improve without modifying model parameters, making it model-agnostic and able to leverage even proprietary models like GPT-4.- The agent is evaluated on question answering, household tasks, and shopping tasks, consistently outperforming baseline planning agents like ReAct that don't learn from experiences.- Emergent abilities like constraint awareness, belief updating, and self-correction are observed from the agent's learned insights.- A method of transferring insights between source and target tasks is also introduced, showing positive transfer learning.In summary, the key contribution is an agent that can autonomously gather and learn from diverse experiences to improve at tasks, without model parameter updates, showcasing the promise of learning from experience for building more capable LLM agents.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence TL;DR summary of the key points from the paper:The paper introduces an experiential learning agent named ExpeL that autonomously gathers experiences from training tasks, extracts natural language insights from them, and leverages these insights along with memories of past successes to enhance its performance on new evaluation tasks, all without any parameter updates.


## How does this paper compare to other research in the same field?

Here are a few thoughts on how this paper compares to other research in natural language processing and large language models:- This paper introduces a novel framework called ExpeL that enables large language models (LLMs) to learn autonomously from experience without requiring gradient updates. Most prior work on improving LLMs focuses on techniques like prompting or fine-tuning that do require updating model parameters. The experiential learning approach is quite innovative in allowing the agent to accumulate knowledge over time from its own experiences.- The idea of having the agent learn from both successful and failed trajectories is reminiscent of approaches in reinforcement learning and self-supervised learning. However, applying these concepts to instruct LLMs without environment rewards or losses is novel. The insights extraction process is also unique.- ExpeL demonstrates strong performance on a diverse set of decision-making environments like HotpotQA, ALFWorld, and WebShop. Many recent works focus evaluation on a single domain, so testing across multiple complex tasks is notable. The results clearly show benefits over planning-only agents.- The transfer learning experiments are interesting in showing how insights from one set of source tasks can improve performance on target tasks with minimal further training. Transfer learning for LLMs is still an open research problem, so this provides a promising new technique.- Overall, ExpeL seems very distinguished from prior work by its experiential learning approach that accumulation knowledge over time to improve an LLM's decision-making skills. The interpretability and lack of dependence on model parameters are also nice properties. The results demonstrate ExpeL's capabilities across diverse tasks and the potential for emergent behaviors. This is an intriguing direction for lifting some limitations of current LLM agents.
