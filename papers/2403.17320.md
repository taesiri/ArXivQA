# [Leveraging Symmetry in RL-based Legged Locomotion Control](https://arxiv.org/abs/2403.17320)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
Reinforcement learning (RL) is a promising approach for solving challenging robot control problems like legged locomotion. However, model-free RL methods often struggle to fully explore multi-modal state spaces arising from morphological symmetries. This leads to asymmetric/unnatural behaviors that compromise performance, robustness and transferability to the real world.  

Proposed Solution: 
This paper investigates methods to incorporate symmetry into RL algorithms to improve exploration and policy learning for quadrupedal robots. Specifically, two approaches are compared:

1) PPOaug - Uses data augmentation during policy updates to approximate equivariant actor-critics. 

2) PPOsymm - Imposes hard equivariance/invariance constraints by modifying network architectures. The policy network is strictly equivariant and the critic is invariant.

These symmetry-incorporated variants are benchmarked against vanilla PPO on complex loco-manipulation and bipedal locomotion tasks in simulation and the real world.

Main Contributions:

- Shows PPOsymm consistently achieves the best training performance - higher returns and sample efficiency. The hard constraints provide more effective exploration guidance.

- On a door opening task, PPOsymm attains much better task-space symmetry than baseline PPO in both simulation and real-world.

- In bipedal walking tasks, PPOsymm results in more stable/periodic gaits and motion symmetry compared to irregular limping behaviors of PPO.

- Symmetry-incorporated policies exhibit substantially better sim-to-real transfer and robustness over unconstrained PPO. PPOaug adapts well to small inherent asymmetries.  

- Analysis reveals when task-space vs robot-space symmetry dominates, and the suitability of soft vs hard constraint methods.

In summary, incorporating symmetry inductive biases is highly effective for learning policies that are more optimal, robust and transferable for quadrupedal robot locomotion. Imposing hard constraints outperforms a soft regularization approach overall.
