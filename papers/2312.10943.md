# [Model Stealing Attack against Graph Classification with Authenticity,   Uncertainty and Diversity](https://arxiv.org/abs/2312.10943)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Recent works have shown that graph neural networks (GNNs) are vulnerable to model stealing attacks in node classification tasks. However, the threats in graph classification tasks have not been explored. 
- Existing attacks make unrealistic assumptions like requiring a lot of real data similar to target model's training data and getting intermediate outputs (node embeddings or confidence vectors) from the target model.

Proposed Solution:
- The paper proposes model stealing attacks under strict settings with limited real data and only hard label access to generate synthetic graphs.
- Two principles are introduced to guide the graph generation - authenticity (make them similar to real graphs) and query value (make them informative to query the target model).
- Three attack methods are proposed:
  - MSA-AU: Generates graphs by modifying adjacency matrix to maximize uncertainty. Ensures authenticity by constraining number of modifications.
  - MSA-AD: Generates graphs by mixing attributes of subgraphs from two real graphs. Ensures diversity.
  - MSA-AUD: Combines above two methods to get authenticity, uncertainty and diversity.
  
Main Contributions:
- First work to explore model stealing attacks in graph classification tasks under strict assumptions of limited real data and hard label access.
- Introduction of two principles to guide the synthetic graph generation - authenticity and query value.
- Proposal of three attack methods catering to different scenarios - MSA-AU, MSA-AD and MSA-AUD emphasizing on uncertainty, diversity and both, respectively.  
- Extensive experiments showing the proposed attacks are more covert, efficient and have superior performance than baselines.


## Summarize the paper in one sentence.

 Here is a one-sentence summary of the paper:

The paper proposes three model stealing attack methods against graph neural networks for graph classification tasks that generate synthetic samples adhering to principles of authenticity, uncertainty, and diversity to effectively clone the target model under strict assumptions of limited real data and hard label access.


## What is the main contribution of this paper?

 According to the paper, the main contributions are:

1) The authors are the first to explore model stealing attacks for graph classification tasks. They adopt stringent assumptions about the attacker's capabilities, including limited access to real data and only having hard labels available. The proposed attack methods under these assumptions are both effective and practical. 

2) The paper introduces important principles like authenticity, uncertainty, and diversity to govern the process of generating synthetic samples for the attack. Based on these principles, three strategic attack methodologies are proposed: MSA-AU, MSA-AD, and MSA-AUD.

3) Extensive experiments show that the proposed attack methods are covert, high-performing, and efficient in various scenarios. Even latest defense methods cannot fully withstand the attacks. Additionally, the paper provides analysis to suggest viable defense directions against model extraction attacks for graph classification.

In summary, the main contribution is proposing practical and effective model stealing attack methods for graph classification tasks under strict assumptions, while ensuring properties like authenticity, uncertainty and diversity of the synthetically generated samples.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and keywords associated with it are:

- Model stealing attack
- Graph neural networks (GNNs) 
- Graph classification
- Limited real data
- Hard labels
- Synthetic data generation
- Authenticity
- Uncertainty
- Diversity
- Sample generation principles
- MSA-AU
- MSA-AD
- MSA-AUD

The paper explores model stealing attacks against graph neural networks in the context of graph classification tasks. It assumes limited availability of real data and hard label awareness for synthetic data generation. Key principles like authenticity, uncertainty and diversity guide the sample generation process. Based on these, three model stealing attack methods are introduced - MSA-AU, MSA-AD and MSA-AUD. So these are some of the central keywords and terminology associated with this paper.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in the paper:

1. The paper proposes three principles for generating new graph samples - authenticity, uncertainty, and diversity. Can you explain in more detail the motivation behind each of these principles and why they are important?

2. The paper introduces three model stealing attack methods - MSA-AU, MSA-AD, and MSA-AUD. Can you walk through the key differences in how each method generates new graph samples? What are the relative strengths and weaknesses?  

3. For the MSA-AU method, the paper uses an optimization objective based on uncertainty to modify graph samples. What specific metrics are used to quantify uncertainty? And why is maximizing uncertainty useful for generating valuable query samples?

4. The MSA-AD method uses a mixup approach to blend graph samples. What are the specific steps it takes to establish a one-to-one mapping of nodes and exchange attributes between graph substructures? How does this help improve diversity?

5. How exactly does the MSA-AUD approach combine both the MSA-AU and MSA-AD methods? What is the intuition behind blending both uncertainty optimization and mixup for the strongest attack?

6. The results show attack performance varies greatly depending on the number of classes in the dataset. What explains this effect? And what strategies could an attacker use if targeting a dataset with more classes?  

7. For defending against the attacks, the paper suggests maintaining secrecy of the model architecture. But what if this cannot be done? What other defense strategies seem promising based on the experimental analysis?

8. Since modifying node features risks contradictions, the attacks mainly alter the graph topology. But what techniques could you propose to modify features more intelligently while preserving authenticity? 

9. The mixup approach exchanges substructures between graphs. But how would your attack methodology change if generating fully synthetic graphs rather than blending samples from the real dataset?

10. The attacks assume access to 10% of the real graph data. How would you adapt the approach if only provided with graph-level aggregates (e.g. number of nodes/edges) rather than full graph samples?
