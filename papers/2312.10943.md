# [Model Stealing Attack against Graph Classification with Authenticity,   Uncertainty and Diversity](https://arxiv.org/abs/2312.10943)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Recent works have shown that graph neural networks (GNNs) are vulnerable to model stealing attacks in node classification tasks. However, the threats in graph classification tasks have not been explored. 
- Existing attacks make unrealistic assumptions like requiring a lot of real data similar to target model's training data and getting intermediate outputs (node embeddings or confidence vectors) from the target model.

Proposed Solution:
- The paper proposes model stealing attacks under strict settings with limited real data and only hard label access to generate synthetic graphs.
- Two principles are introduced to guide the graph generation - authenticity (make them similar to real graphs) and query value (make them informative to query the target model).
- Three attack methods are proposed:
  - MSA-AU: Generates graphs by modifying adjacency matrix to maximize uncertainty. Ensures authenticity by constraining number of modifications.
  - MSA-AD: Generates graphs by mixing attributes of subgraphs from two real graphs. Ensures diversity.
  - MSA-AUD: Combines above two methods to get authenticity, uncertainty and diversity.
  
Main Contributions:
- First work to explore model stealing attacks in graph classification tasks under strict assumptions of limited real data and hard label access.
- Introduction of two principles to guide the synthetic graph generation - authenticity and query value.
- Proposal of three attack methods catering to different scenarios - MSA-AU, MSA-AD and MSA-AUD emphasizing on uncertainty, diversity and both, respectively.  
- Extensive experiments showing the proposed attacks are more covert, efficient and have superior performance than baselines.
