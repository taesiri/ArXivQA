# [Adversarial Data Poisoning for Fake News Detection: How to Make a Model   Misclassify a Target News without Modifying It](https://arxiv.org/abs/2312.15228)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem: 
Fake news detection models that use online learning are vulnerable to adversarial attacks through data poisoning. Attackers can manipulate the training data to cause the model to misclassify a true news article as false, without modifying the article itself. This is a realistic threat as attackers often don't have full control over all spreading information. Prior work has focused on modifying target articles directly rather than training data poisoning.

Proposed Solution:
The paper proposes an online learning framework for fake news detection where poisoned data is injected into the training data to misguide the model's predictions. Two data poisoning attack strategies are analyzed on logistic regression models - Most Confidence Mislabeling and Target Label Flipping. Experiments on synthetic data reveal varying susceptibility of the models based on complexity and attack type.  

Main Contributions:
- Formalizes a realistic threat model of adversarial data poisoning attacks on online learning fake news detectors, without assuming direct control over target articles
- Introduces two data poisoning attack strategies tailored to online learning models
- Evaluates attack strategies on logistic regression models, revealing different vulnerabilities based on model complexity 
- Sets groundwork for further analysis of sophisticated models and defenses against data poisoning attacks in online learning for fake news detection

The key insight is that by carefully modifying training data, attackers can secretly manipulate model behavior to misclassify true news without accessing or editing articles directly. More complex models can resist simpler attacks but remain vulnerable to other strategies. Understanding these vulnerabilities is critical for developing robust online learning frameworks and defenses.
