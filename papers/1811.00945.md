# Image Chat: Engaging Grounded Conversations

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper addresses is how to build conversational agents that can engage humans in grounded, open-ended dialogues about images in an appealing and captivating way. Specifically, the paper focuses on the following main questions:- What neural architectures and models work best for grounded open-domain conversation over images (both retrieval and generative)?- How can large-scale pre-trained image and text representations be utilized effectively? - How important is conditioning on stylistic traits for engagingness?- What training datasets and evaluation metrics should be used?To answer these questions, the paper introduces a new large-scale dataset called Image-Chat for this task, explores various neural retrieval and generative models using state-of-the-art Transformer-based representations for text and ResNet-based representations for images, evaluates the models' engagingness through automatic metrics and human evaluations, and analyzes the importance of the different components through ablation studies.The central hypothesis is that by combining large-scale pre-training, multimodal fusion of images and text, and conditioning on stylistic traits, they can build models that can have engaging open-ended conversations with humans about images. The Image-Chat dataset and experiments are designed to test this hypothesis.


## What is the main contribution of this paper?

The main contributions of this paper are:1. The authors collect a new large-scale dataset for image-grounded conversations called Image-Chat, consisting of over 200k images and 400k utterances with different style traits. This provides a rich dataset for training and evaluating neural conversational models for engaging image-based chit-chat.2. They explore and evaluate various neural architectures for retrieval and generative models on this task, studying the importance of different components like image encoders, text encoders, multimodal fusion techniques, etc. Their best model uses ResNeXt image features, separate Transformers for encoding dialogue history and responses, and a multimodal combiner.3. Through automatic metrics and human evaluations, they demonstrate the efficacy of their proposed models, achieving near human-level engagingness on Image-Chat. Their best retrieval model beats humans around 48% of the time.4. They show the transferability of their models by testing on the existing Image Grounded Conversations (IGC) task, where their model achieves new state-of-the-art performance and significantly narrows the gap to human response quality.In summary, the key contribution is collecting a large engaging dialogue dataset grounded in images, and demonstrating through rigorous experiments that neural multimodal models can be trained to produce human-like entertaining conversations on this challenging task. The models, datasets and analyses provide a strong foundation for future research on this problem.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper presents the Image-Chat dataset and models for engaging image-grounded conversations, showing that large-scale pretraining on images and text along with utilizing stylistic traits allows a retrieval model to produce dialogues that are nearly on par with human conversations.
