# [Deep Incubation: Training Large Models by Divide-and-Conquering](https://arxiv.org/abs/2212.04129)

## What is the central research question or hypothesis that this paper addresses?

 The central research question this paper addresses is how to effectively train large deep learning models. The authors propose a novel "divide-and-conquer" approach called Deep Incubation that splits a large model into smaller sub-modules which are trained independently and then assembled together. The key challenge is ensuring the compatibility of the independently trained sub-modules. 

The main hypothesis is that by dividing a large model into smaller parts during training, the optimization and generalization performance can be improved compared to standard end-to-end training of the full model. The authors test this hypothesis by applying Deep Incubation to train large vision transformer models and evaluating on image classification, object detection, and semantic segmentation tasks.

In summary, the core research question is how to train large models more efficiently and effectively. The key hypothesis is that a divide-and-conquer approach can help address optimization and generalization issues faced during end-to-end training of huge models. Deep Incubation is proposed as a method to enable modular training of parts of a model independently while maintaining compatibility between the parts when assembled.


## What is the main contribution of this paper?

 The main contribution of this paper is proposing a novel training approach called Deep Incubation for large deep learning models. The key ideas are:

- Dividing a large model into smaller modules and training them independently. This improves training efficiency and optimization. 

- Introducing a lightweight shared meta-model to link the modules together implicitly. This encourages compatibility between the independently trained modules. 

- Proposing a "module incubation" algorithm to train each module by cooperating with the meta-model to accomplish the learning task, rather than strictly imitating the meta-model. This allows the modules to exceed the representation power of the meta-model.

- Demonstrating that this approach outperforms standard end-to-end training in terms of generalization performance, training efficiency, and data efficiency for large vision models like ViT on tasks like image classification, object detection, and semantic segmentation.

In summary, the key contribution is proposing the Deep Incubation training paradigm to effectively and efficiently train large models in a divide-and-conquer manner, enabled by the lightweight shared meta-model and the module incubation algorithm.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the key points from the paper:

The paper proposes Deep Incubation, a new training approach that divides large neural network models into smaller modules which are trained independently and then assembled, enabling more efficient and effective training compared to standard end-to-end training.


## How does this paper compare to other research in the same field?

 Here are a few key points comparing this paper to other related work in decoupled training of neural networks:

- This paper proposes a new divide-and-conquer approach called "Deep Incubation" for training large models. It divides the model into smaller modules which are trained independently, then assembles them together. This is different from prior decoupled training methods like Lifted Structure Learning and Decoupled Greedy Learning which still couple the modules during forward propagation.

- A key contribution is the introduction of a lightweight "meta" model that is shared across the modules during training. This allows implicit information exchange between the independently trained modules, helping ensure their compatibility when assembled. Other methods don't have an explicit mechanism for compatibility.

- Experiments show much better results on large vision models like ViT/DeiT compared to prior decoupled training techniques. For example, on ImageNet they achieve 2.7% higher accuracy with ViT-Huge versus only 0.6% for InfoPro. This suggests "Deep Incubation" is better suited to very large modern models.

- The approach achieves significantly improved training efficiency over end-to-end training. For instance, 4x lower training cost for similar accuracy with ViT-Huge. This addresses a major weakness of end-to-end training large models.

- The method also demonstrates much higher data efficiency and ability to generalize, outperforming end-to-end training especially with less training data. This helps address overfitting issues with large models.

Overall, "Deep Incubation" represents an important advance in decoupled training of large neural networks, achieving state-of-the-art results on modern vision models by effectively balancing module independence with compatibility during training. The efficiency and generalization gains are very promising.
