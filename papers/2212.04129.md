# [Deep Incubation: Training Large Models by Divide-and-Conquering](https://arxiv.org/abs/2212.04129)

## What is the central research question or hypothesis that this paper addresses?

 The central research question this paper addresses is how to effectively train large deep learning models. The authors propose a novel "divide-and-conquer" approach called Deep Incubation that splits a large model into smaller sub-modules which are trained independently and then assembled together. The key challenge is ensuring the compatibility of the independently trained sub-modules. 

The main hypothesis is that by dividing a large model into smaller parts during training, the optimization and generalization performance can be improved compared to standard end-to-end training of the full model. The authors test this hypothesis by applying Deep Incubation to train large vision transformer models and evaluating on image classification, object detection, and semantic segmentation tasks.

In summary, the core research question is how to train large models more efficiently and effectively. The key hypothesis is that a divide-and-conquer approach can help address optimization and generalization issues faced during end-to-end training of huge models. Deep Incubation is proposed as a method to enable modular training of parts of a model independently while maintaining compatibility between the parts when assembled.


## What is the main contribution of this paper?

 The main contribution of this paper is proposing a novel training approach called Deep Incubation for large deep learning models. The key ideas are:

- Dividing a large model into smaller modules and training them independently. This improves training efficiency and optimization. 

- Introducing a lightweight shared meta-model to link the modules together implicitly. This encourages compatibility between the independently trained modules. 

- Proposing a "module incubation" algorithm to train each module by cooperating with the meta-model to accomplish the learning task, rather than strictly imitating the meta-model. This allows the modules to exceed the representation power of the meta-model.

- Demonstrating that this approach outperforms standard end-to-end training in terms of generalization performance, training efficiency, and data efficiency for large vision models like ViT on tasks like image classification, object detection, and semantic segmentation.

In summary, the key contribution is proposing the Deep Incubation training paradigm to effectively and efficiently train large models in a divide-and-conquer manner, enabled by the lightweight shared meta-model and the module incubation algorithm.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the key points from the paper:

The paper proposes Deep Incubation, a new training approach that divides large neural network models into smaller modules which are trained independently and then assembled, enabling more efficient and effective training compared to standard end-to-end training.
