# [Computationally Budgeted Continual Learning: What Does Matter?](https://arxiv.org/abs/2303.11165)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question seems to be: 

"Do existing continual learning algorithms perform well under per step restricted computation?"

The key hypothesis appears to be that classical continual learning methods will struggle to cope with settings that impose a constrained computational budget per time step. 

The authors motivate this question by arguing that most prior continual learning literature overlooks computational constraints, instead focusing on limited access to previous data. But in real-world applications, systems are more constrained by computational budgets rather than storage. So they propose studying continual learning under fixed compute per time step, which implicitly restricts how much past data can be used.

To test this, the paper seems to experimentally evaluate various classical continual learning approaches like distillation, sampling strategies, and FC layer corrections under different computational budgets. The goal is to see if they outperform a simple baseline of uniform sampling within the allotted budget. 

Based on the results summarized in the introduction, their hypothesis seems to hold - traditional methods fail to beat the simple baseline when computation is restricted, suggesting they are not practical for realistically budgeted deployment.


## What is the main contribution of this paper?

 Based on my reading of the paper, the main contributions seem to be:

- Proposing a new computationally budgeted continual learning setting where methods have a fixed compute budget per time step, as opposed to unconstrained compute budgets in prior work. This aims to better reflect real-world deployment constraints. 

- Conducting large-scale experiments on ImageNet and Continual Google Landmarks evaluating common continual learning approaches like sampling strategies, distillation losses, and FC layer corrections under this proposed compute-constrained setup.

- Finding that simple experience replay baselines outperform more complex continual learning methods when constrained by a computational budget. The performance gap widens under harsher compute restrictions.

- Demonstrating that partial model retraining strategies like only fine-tuning the FC layer can help close the performance gap compared to full model training, but only when initialized from a strong pretrained model.

- Highlighting that many existing continual learning methods are designed for settings with memory constraints rather than computational constraints, and fail to cope well when compute is limited per time step.

In summary, the key contribution is benchmarking and analyzing continual learning methods under a new computationally budgeted setting aimed at better reflecting real-world systems, and showing limitations of current methods in this regime. The findings suggest a need to design algorithms tailored for computational rather than memory restrictions.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the key point of the paper:

The paper finds that under computationally budgeted continual learning settings, simple experience replay baselines outperform more complex continual learning methods like distillation and sampling strategies.


## How does this paper compare to other research in the same field?

 Here are a few key ways this paper compares to other research in the field of continual learning:

- The paper focuses on computationally budgeted continual learning, where there are constraints on the compute resources available per time step. Much prior work does not place hard limits on compute. Considering compute constraints is important for real-world deployment.

- The paper benchmarks performance on large-scale datasets like ImageNet2K and Continual Google Landmarks V2. Many prior works use smaller datasets like CIFAR and TinyImageNet. Evaluating on larger and more realistic datasets is an important contribution.

- The paper systematically evaluates the main approaches in continual learning like distillation, sampling strategies, and FC layer correction. It finds that simple experience replay baselines outperform more complex CL methods under compute constraints. This is an interesting negative result. 

- The paper considers different data stream settings like class incremental, data incremental, and time incremental streams. Evaluating different stream orderings helps understand method generalizability.

- The paper does extensive experiments, with over 1500 GPU hours of compute. Large scale experimentation helps support the robustness of conclusions.

Overall, the focus on computation budgets, large-scale experimentation, and simple strong baselines help move continual learning research towards real-world applicability. The negative results for complex CL methods under budget constraints raise important questions about their usefulness in practice compared to basic experience replay.
