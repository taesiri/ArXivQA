# [Computationally Budgeted Continual Learning: What Does Matter?](https://arxiv.org/abs/2303.11165)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question seems to be: 

"Do existing continual learning algorithms perform well under per step restricted computation?"

The key hypothesis appears to be that classical continual learning methods will struggle to cope with settings that impose a constrained computational budget per time step. 

The authors motivate this question by arguing that most prior continual learning literature overlooks computational constraints, instead focusing on limited access to previous data. But in real-world applications, systems are more constrained by computational budgets rather than storage. So they propose studying continual learning under fixed compute per time step, which implicitly restricts how much past data can be used.

To test this, the paper seems to experimentally evaluate various classical continual learning approaches like distillation, sampling strategies, and FC layer corrections under different computational budgets. The goal is to see if they outperform a simple baseline of uniform sampling within the allotted budget. 

Based on the results summarized in the introduction, their hypothesis seems to hold - traditional methods fail to beat the simple baseline when computation is restricted, suggesting they are not practical for realistically budgeted deployment.


## What is the main contribution of this paper?

 Based on my reading of the paper, the main contributions seem to be:

- Proposing a new computationally budgeted continual learning setting where methods have a fixed compute budget per time step, as opposed to unconstrained compute budgets in prior work. This aims to better reflect real-world deployment constraints. 

- Conducting large-scale experiments on ImageNet and Continual Google Landmarks evaluating common continual learning approaches like sampling strategies, distillation losses, and FC layer corrections under this proposed compute-constrained setup.

- Finding that simple experience replay baselines outperform more complex continual learning methods when constrained by a computational budget. The performance gap widens under harsher compute restrictions.

- Demonstrating that partial model retraining strategies like only fine-tuning the FC layer can help close the performance gap compared to full model training, but only when initialized from a strong pretrained model.

- Highlighting that many existing continual learning methods are designed for settings with memory constraints rather than computational constraints, and fail to cope well when compute is limited per time step.

In summary, the key contribution is benchmarking and analyzing continual learning methods under a new computationally budgeted setting aimed at better reflecting real-world systems, and showing limitations of current methods in this regime. The findings suggest a need to design algorithms tailored for computational rather than memory restrictions.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the key point of the paper:

The paper finds that under computationally budgeted continual learning settings, simple experience replay baselines outperform more complex continual learning methods like distillation and sampling strategies.


## How does this paper compare to other research in the same field?

 Here are a few key ways this paper compares to other research in the field of continual learning:

- The paper focuses on computationally budgeted continual learning, where there are constraints on the compute resources available per time step. Much prior work does not place hard limits on compute. Considering compute constraints is important for real-world deployment.

- The paper benchmarks performance on large-scale datasets like ImageNet2K and Continual Google Landmarks V2. Many prior works use smaller datasets like CIFAR and TinyImageNet. Evaluating on larger and more realistic datasets is an important contribution.

- The paper systematically evaluates the main approaches in continual learning like distillation, sampling strategies, and FC layer correction. It finds that simple experience replay baselines outperform more complex CL methods under compute constraints. This is an interesting negative result. 

- The paper considers different data stream settings like class incremental, data incremental, and time incremental streams. Evaluating different stream orderings helps understand method generalizability.

- The paper does extensive experiments, with over 1500 GPU hours of compute. Large scale experimentation helps support the robustness of conclusions.

Overall, the focus on computation budgets, large-scale experimentation, and simple strong baselines help move continual learning research towards real-world applicability. The negative results for complex CL methods under budget constraints raise important questions about their usefulness in practice compared to basic experience replay.


## What future research directions do the authors suggest?

 Based on my reading of the paper, some of the main future research directions suggested by the authors are:

- Designing computationally efficient continual learning algorithms that can work well under limited computational budgets. The authors show that many existing methods fail in such settings, so developing methods tailored for budgeted computation is an important direction. This could involve techniques like efficient distillation, selective network training, early stopping, etc.

- Better understanding the role of pre-training for continual learning. The authors show that leveraging stronger pre-trained models can significantly boost performance of limited training methods like only fine-tuning the FC layer. Exploring what properties of pre-training are most beneficial for continual learning is an interesting direction.

- Developing privacy-preserving continual learning algorithms. The authors discuss how current memory-based continual learning approaches may violate privacy. Designing methods that can learn continually while protecting privacy is important future work. Techniques like federated learning or differential privacy may be promising here.

- Testing continual learning at even longer time scales, e.g. hundreds of steps. The authors show results for up to 200 steps, but studying even longer continual learning is needed to better match real-world systems. This poses challenges in efficiency and plasticity over such long sequences.

- Exploring different model expansion strategies for continual learning under budgeted computation. The authors show some initial results on selective network training, but more work could be done on dynamically expanding models over time under computational constraints.

- Applying insights from budgeted continual learning to related incremental learning problems like online learning, lifelong learning, etc. The ideas are likely transferrable to other scenarios involving streaming data.

In summary, the key theme the authors highlight is developing efficient and scalable continual learning methods that can handle practical constraints on computation, privacy, and model capacity over long durations. This provides many exciting avenues for future research.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the paper:

The paper discusses the motivation for privacy when limiting memory access for continual learning algorithms. It notes that prior art in continual learning often restricts access to previously seen training data, allowing only a small portion to be stored in memory. The authors argue that the two main motivations given in the literature for limiting memory access are (1) storage space is expensive, and (2) privacy/GDPR constraints prohibit retaining data.  

Regarding the first point, the authors show that storage costs are actually quite low compared to the computational costs of training deep learning models. For example, storing the CLEAR dataset costs around 2 cents, while training continually on CLEAR costs about $100. Thus, if cost reduction is the goal, limiting computation rather than memory is the way forward. Regarding privacy, the authors argue that simply restricting memory access does not truly address privacy concerns, since models still retain information about past training data. They note that privacy may actually require some forgetting, contrary to the goal of continual learning. Overall, they make the case that limited memory access in continual learning is not well motivated by cost or privacy arguments. The true constraint arises from limited computation, which restricts how much past data can be revisited when learning continually from high-throughput streams.


## Summarize the main method used in the paper in one paragraph.

 The paper presents a study on the effectiveness of various continual learning (CL) algorithms under computationally budgeted settings. 

The key components studied are:

1) Sampling strategies: Inexpensive strategies like uniform, class-balanced, recency-biased and FIFO sampling are compared against costly strategies like k-means, max loss and uncertainty sampling. 

2) Distillation losses: Distillation methods using BCE, MSE, cosine similarity and cross-entropy are evaluated.

3) FC layer corrections: Methods like cosine FC layer, ACE loss, BiC, WA and calibration are tested. 

The methods are evaluated on ImageNet2K (with 1K classes from ImageNet1K and 1K new classes) and Continual Google Landmarks V2 datasets under data incremental, class incremental and time incremental settings. The number of training iterations per time step is limited to simulate compute budgeting.

The main finding is that a simple experience replay baseline with uniform sampling and cross-entropy loss outperforms sophisticated CL algorithms like distillation, specialized sampling and FC layer corrections when training iterations are constrained. The performance gap widens with more stringent compute budgets. Partial fine-tuning of only the FC layer on strong pretrained models helps reduce the gap compared to full model training. Overall, the results suggest existing CL methods are less effective under practical compute restrictions.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the paper:

The paper revisits continual learning (CL) under a computationally budgeted setting where algorithms have constrained compute per time step rather than limited access to previous data. They propose a large-scale benchmark to analyze traditional CL approaches like sampling strategies, distillation losses, and FC layer correction methods. Experiments are conducted on ImageNet2K (with data incremental and class incremental streams) and Continual Google Landmarks V2 datasets. The compute budget is restricted to allow training on only 25-50% of observed data per time step. Surprisingly, they find that no existing CL algorithm outperforms a simple baseline of uniform sampling and training under this restricted compute setting. The gap widens with harsher compute limits. Training just the FC layer with a strong pretrained model can reduce the performance gap. Overall, most CL methods require large compute budgets and fail in realistic budgeted deployment. The work suggests compute restrictions inherently limit memory usage, and classical CL literature is less practical for real-world applications.


## What problem or question is the paper addressing?

 This paper is addressing the issue of continual learning algorithms being developed and tested in unrealistic settings that do not reflect real-world deployment constraints. 

Specifically, it is arguing that much prior work in continual learning:

- Assumes clear task boundaries and non-overlapping tasks/distributions, whereas real-world data streams have gradual distribution shifts without clear boundaries. 

- Puts memory constraints on storing past data samples, whereas storage is cheap. The real constraint is on computation per time step.

- Allows unlimited compute budget per time step for training, whereas real-world systems need to process streams under tight time constraints. 

The key question the paper seems to be asking is:

Do existing continual learning algorithms actually perform well when evaluated under realistic settings with:

- No assumed task boundaries

- Large memory to store past samples 

- Restricted compute per time step

In other words, how do CL algorithms perform under budgeted compute constraints, which implicitly restrict how much past data can be replayed?


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and concepts include:

- Continual Learning (CL): Learning sequentially from a stream of data where the distribution changes over time. The goal is to adapt to new data while preserving knowledge of old data.

- Computationally Budgeted CL: A continual learning setting where there is a fixed computational budget per time step for training. This contrasts with prior CL work that does not constrain per-step computation. 

- Memory Constraints: Prior CL work focuses on small fixed memory buffers. This work assumes unlimited memory storage but implicit constraints from limited compute per step.

- Sampling Strategies: Methods to select which memory samples to rehearse when training on new data, e.g. uniform, class-balanced, recency-biased.

- Distillation Losses: Using a teacher model to supervise a student model via distillation losses like MSE, cosine similarity, etc. 

- FC Layer Correction: Methods to correct bias in the last fully-connected layer like weight alignment, cosine normalization, etc.

- Model Expansion: Adapting the model architecture over time by partial retraining or direct expansion.

- ImageNet2K: A new large-scale CL dataset constructed from ImageNet1K and ImageNet21K.

- Data Incremental, Class Incremental, Time Incremental: Different stream orderings - shuffled, by class, or by time.

- Empirical Results: Simple experience replay outperforms more complex CL methods under budgeted compute. Harsher budgets magnify the gaps.


## Based on the paper, what questions should we ask to create a comprehensive summary of it? Please list at least 10 questions.

 Here are 10 questions I would ask to summarize the key points of the paper:

1. What is the motivation behind the paper? Why is computationally budgeted continual learning an important problem to study?

2. How does the paper define computationally budgeted continual learning? What are the key aspects of the problem formulation? 

3. How does the proposed setting differ from prior art in continual learning? What are the key differences highlighted?

4. What datasets were used to benchmark the proposed setting? What were the train/test splits?

5. What metrics were used to evaluate performance? How was performance reported?

6. What were the major continual learning methods benchmarked? How were they categorized? 

7. What was the main conclusion from the experiments on sampling strategies? Did any strategy stand out?

8. What was the main conclusion from the experiments on distillation losses? How did they compare to the baseline?

9. What was the main conclusion from the experiments on FC layer corrections? Did any method work well?

10. What conclusions were drawn from the sensitivity analysis? Did the findings hold under different conditions?


## Design 10 in-depth questions about method proposed in this paper and avoid simple question.

 Here are 10 in-depth questions about the methods proposed in the paper:

1. The paper proposes a computationally budgeted continual learning setting. How does this differ from prior continual learning settings, and what motivates the need for it? 

2. The paper evaluates various sampling strategies for replay. Why do costly sampling strategies like k-means perform worse than simple uniform sampling in the budgeted setting? Does this contradict findings from prior work?

3. Distillation is a common technique in continual learning. Why do distillation methods struggle in the budgeted setting compared to a simple experience replay baseline? Does increasing the compute budget help close this gap?

4. The paper finds that FC layer correction techniques like ACE also fail to outperform the naive replay baseline. Why might this be the case? Are there any settings/techniques where FC corrections do help?

5. How does the paper construct the diverse data streams for benchmarks like ImageNet2K and Continual Landmarks? What are the key properties and design choices for these streams?

6. The paper varies number of steps and compute budget in sensitivity studies. How robust are the main conclusions when these factors are changed? Are there any cases where exceptions occur? 

7. Why does the paper use number of iterations as the measure of compute rather than wall clock time? What are the tradeoffs of this choice?

8. Partial model retraining is explored at the end. Why does this help close the gap to the naive baseline, and how does pretrained model quality impact it?

9. Could the findings suggest that most existing continual learning methods overfit to settings with unconstrained compute? How could methods be made more computationally efficient?

10. What are the broader implications of the paper's conclusions? How could it influence future continual learning research and real-world system design?


## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a high-quality paragraph summarizing the key points of the paper:

This paper revisits continual learning (CL) methods under the practical constraint of limited computational budgets per time step. The authors construct a large-scale benchmark with ImageNet and Google Landmarks datasets across data incremental, class incremental, and time incremental settings. They evaluate various common CL strategies for sampling, distillation losses, and correcting fully connected layers against a simple baseline of uniform sampling and cross entropy loss. Surprisingly, through extensive experiments totaling over 1500 GPU-hours, none of the complex CL methods outperform the simple baseline when constrained to a fixed computational budget per time step. The authors find distillation losses require more samples to be effective, sampling strategies have computational overhead that diminishes gains, and calibrating fully connected layers helps initially but does not surpass the baseline over time. Even with more time steps or compute budget, the baseline remains superior. The results suggest existing CL methods are not practical with realistic computational constraints, as they were developed assuming unlimited access to past data. The authors recommend focusing on computationally efficient algorithms and strong pretrained models.


## Summarize the paper in one sentence.

 This paper finds that under computationally budgeted continual learning settings, traditional continual learning methods fail to outperform a simple baseline that uniformly samples from memory.


## Summarize the paper in one paragraphs.

 Here is a one paragraph summary of the paper:

This paper revisits continual learning (CL) methods under the practical constraint of limited computational budgets per timestep. The authors benchmark various classical CL approaches like sampling strategies, distillation losses, and FC layer corrections on large-scale datasets like ImageNet2K and Continual Google Landmarks V2. Through extensive experiments amounting to 1500+ GPU hours, they find that under compute-constrained settings, traditional CL methods fail to outperform a simple baseline (ERM-Naive) that uniformly samples memory and trains with cross-entropy loss. The performance gap compared to ERM-Naive becomes larger with harsher compute restrictions. The authors conclude that most existing CL methods are only useful when ample computation is available, while minimal algorithms like ERM-Naive are superior under realistic computational budgets. Partial training of only the FC layer can help close the performance gap compared to ERM-Naive when built on top of strong pretrained models. Overall, the work highlights the need to design computationally efficient CL methods for real-world deployment.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the methods proposed in this paper:

1. The paper argues that existing continual learning (CL) algorithms focus on limited memory access to previous data, while not restricting computational budgets. How does imposing a restricted computational budget per time step change the effectiveness of common CL approaches like distillation losses and sampling strategies?

2. The paper proposes a "Naive" baseline that simply does uniform sampling and trains with cross-entropy loss. Under what conditions does this simple approach outperform more complex continual learning algorithms? What does this suggest about the utility of complex CL methods in real-world deployments?

3. The paper finds that common sampling strategies like class-balanced, recency-biased, and FIFO perform similarly to uniform sampling in the budgeted setting. Why might complex sampling strategies fail to provide benefits when the computational budget is limited?

4. The paper shows that distillation methods like MSE and cosine similarity underperform the Naive baseline. Why might distillation be less effective in the budgeted setting compared to prior unlimited compute settings?

5. The paper finds that FC layer correction methods like ACE, BiC, and WA do not outperform the Naive approach consistently. Why might methods designed to address catastrophic forgetting issues struggle under a restricted computational budget?

6. The paper argues that most CL methods fail in the budgeted setting because they are designed for offline continual learning with no restrictions on training time or iterations. What modifications could be made to existing CL algorithms to make them more effective under computational budgets?

7. The sensitivity analysis reveals consistent conclusions even when varying the number of time steps and computational budget. What does this suggest about the fundamental ineffectiveness of complex CL methods in budgeted settings?

8. Why does partial training of only the FC layer on a strong backbone initialization close the performance gap with the Naive approach? What are the implications for continual learning on large real-world datasets?

9. The paper argues that computational costs far outweigh storage costs for CL, making memory constraints unrealistic. Do you agree with this assessment? What role should computational vs. storage costs play in motivating CL research?

10. What open questions remain regarding continual learning under restricted computational budgets? What future work is needed to develop algorithms suitable for real-world budgeted deployment?
