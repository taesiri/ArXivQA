# [Computationally Budgeted Continual Learning: What Does Matter?](https://arxiv.org/abs/2303.11165)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question seems to be: 

"Do existing continual learning algorithms perform well under per step restricted computation?"

The key hypothesis appears to be that classical continual learning methods will struggle to cope with settings that impose a constrained computational budget per time step. 

The authors motivate this question by arguing that most prior continual learning literature overlooks computational constraints, instead focusing on limited access to previous data. But in real-world applications, systems are more constrained by computational budgets rather than storage. So they propose studying continual learning under fixed compute per time step, which implicitly restricts how much past data can be used.

To test this, the paper seems to experimentally evaluate various classical continual learning approaches like distillation, sampling strategies, and FC layer corrections under different computational budgets. The goal is to see if they outperform a simple baseline of uniform sampling within the allotted budget. 

Based on the results summarized in the introduction, their hypothesis seems to hold - traditional methods fail to beat the simple baseline when computation is restricted, suggesting they are not practical for realistically budgeted deployment.


## What is the main contribution of this paper?

 Based on my reading of the paper, the main contributions seem to be:

- Proposing a new computationally budgeted continual learning setting where methods have a fixed compute budget per time step, as opposed to unconstrained compute budgets in prior work. This aims to better reflect real-world deployment constraints. 

- Conducting large-scale experiments on ImageNet and Continual Google Landmarks evaluating common continual learning approaches like sampling strategies, distillation losses, and FC layer corrections under this proposed compute-constrained setup.

- Finding that simple experience replay baselines outperform more complex continual learning methods when constrained by a computational budget. The performance gap widens under harsher compute restrictions.

- Demonstrating that partial model retraining strategies like only fine-tuning the FC layer can help close the performance gap compared to full model training, but only when initialized from a strong pretrained model.

- Highlighting that many existing continual learning methods are designed for settings with memory constraints rather than computational constraints, and fail to cope well when compute is limited per time step.

In summary, the key contribution is benchmarking and analyzing continual learning methods under a new computationally budgeted setting aimed at better reflecting real-world systems, and showing limitations of current methods in this regime. The findings suggest a need to design algorithms tailored for computational rather than memory restrictions.
