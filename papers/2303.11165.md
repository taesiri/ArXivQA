# [Computationally Budgeted Continual Learning: What Does Matter?](https://arxiv.org/abs/2303.11165)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question seems to be: 

"Do existing continual learning algorithms perform well under per step restricted computation?"

The key hypothesis appears to be that classical continual learning methods will struggle to cope with settings that impose a constrained computational budget per time step. 

The authors motivate this question by arguing that most prior continual learning literature overlooks computational constraints, instead focusing on limited access to previous data. But in real-world applications, systems are more constrained by computational budgets rather than storage. So they propose studying continual learning under fixed compute per time step, which implicitly restricts how much past data can be used.

To test this, the paper seems to experimentally evaluate various classical continual learning approaches like distillation, sampling strategies, and FC layer corrections under different computational budgets. The goal is to see if they outperform a simple baseline of uniform sampling within the allotted budget. 

Based on the results summarized in the introduction, their hypothesis seems to hold - traditional methods fail to beat the simple baseline when computation is restricted, suggesting they are not practical for realistically budgeted deployment.


## What is the main contribution of this paper?

 Based on my reading of the paper, the main contributions seem to be:

- Proposing a new computationally budgeted continual learning setting where methods have a fixed compute budget per time step, as opposed to unconstrained compute budgets in prior work. This aims to better reflect real-world deployment constraints. 

- Conducting large-scale experiments on ImageNet and Continual Google Landmarks evaluating common continual learning approaches like sampling strategies, distillation losses, and FC layer corrections under this proposed compute-constrained setup.

- Finding that simple experience replay baselines outperform more complex continual learning methods when constrained by a computational budget. The performance gap widens under harsher compute restrictions.

- Demonstrating that partial model retraining strategies like only fine-tuning the FC layer can help close the performance gap compared to full model training, but only when initialized from a strong pretrained model.

- Highlighting that many existing continual learning methods are designed for settings with memory constraints rather than computational constraints, and fail to cope well when compute is limited per time step.

In summary, the key contribution is benchmarking and analyzing continual learning methods under a new computationally budgeted setting aimed at better reflecting real-world systems, and showing limitations of current methods in this regime. The findings suggest a need to design algorithms tailored for computational rather than memory restrictions.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the key point of the paper:

The paper finds that under computationally budgeted continual learning settings, simple experience replay baselines outperform more complex continual learning methods like distillation and sampling strategies.


## How does this paper compare to other research in the same field?

 Here are a few key ways this paper compares to other research in the field of continual learning:

- The paper focuses on computationally budgeted continual learning, where there are constraints on the compute resources available per time step. Much prior work does not place hard limits on compute. Considering compute constraints is important for real-world deployment.

- The paper benchmarks performance on large-scale datasets like ImageNet2K and Continual Google Landmarks V2. Many prior works use smaller datasets like CIFAR and TinyImageNet. Evaluating on larger and more realistic datasets is an important contribution.

- The paper systematically evaluates the main approaches in continual learning like distillation, sampling strategies, and FC layer correction. It finds that simple experience replay baselines outperform more complex CL methods under compute constraints. This is an interesting negative result. 

- The paper considers different data stream settings like class incremental, data incremental, and time incremental streams. Evaluating different stream orderings helps understand method generalizability.

- The paper does extensive experiments, with over 1500 GPU hours of compute. Large scale experimentation helps support the robustness of conclusions.

Overall, the focus on computation budgets, large-scale experimentation, and simple strong baselines help move continual learning research towards real-world applicability. The negative results for complex CL methods under budget constraints raise important questions about their usefulness in practice compared to basic experience replay.


## What future research directions do the authors suggest?

 Based on my reading of the paper, some of the main future research directions suggested by the authors are:

- Designing computationally efficient continual learning algorithms that can work well under limited computational budgets. The authors show that many existing methods fail in such settings, so developing methods tailored for budgeted computation is an important direction. This could involve techniques like efficient distillation, selective network training, early stopping, etc.

- Better understanding the role of pre-training for continual learning. The authors show that leveraging stronger pre-trained models can significantly boost performance of limited training methods like only fine-tuning the FC layer. Exploring what properties of pre-training are most beneficial for continual learning is an interesting direction.

- Developing privacy-preserving continual learning algorithms. The authors discuss how current memory-based continual learning approaches may violate privacy. Designing methods that can learn continually while protecting privacy is important future work. Techniques like federated learning or differential privacy may be promising here.

- Testing continual learning at even longer time scales, e.g. hundreds of steps. The authors show results for up to 200 steps, but studying even longer continual learning is needed to better match real-world systems. This poses challenges in efficiency and plasticity over such long sequences.

- Exploring different model expansion strategies for continual learning under budgeted computation. The authors show some initial results on selective network training, but more work could be done on dynamically expanding models over time under computational constraints.

- Applying insights from budgeted continual learning to related incremental learning problems like online learning, lifelong learning, etc. The ideas are likely transferrable to other scenarios involving streaming data.

In summary, the key theme the authors highlight is developing efficient and scalable continual learning methods that can handle practical constraints on computation, privacy, and model capacity over long durations. This provides many exciting avenues for future research.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the paper:

The paper discusses the motivation for privacy when limiting memory access for continual learning algorithms. It notes that prior art in continual learning often restricts access to previously seen training data, allowing only a small portion to be stored in memory. The authors argue that the two main motivations given in the literature for limiting memory access are (1) storage space is expensive, and (2) privacy/GDPR constraints prohibit retaining data.  

Regarding the first point, the authors show that storage costs are actually quite low compared to the computational costs of training deep learning models. For example, storing the CLEAR dataset costs around 2 cents, while training continually on CLEAR costs about $100. Thus, if cost reduction is the goal, limiting computation rather than memory is the way forward. Regarding privacy, the authors argue that simply restricting memory access does not truly address privacy concerns, since models still retain information about past training data. They note that privacy may actually require some forgetting, contrary to the goal of continual learning. Overall, they make the case that limited memory access in continual learning is not well motivated by cost or privacy arguments. The true constraint arises from limited computation, which restricts how much past data can be revisited when learning continually from high-throughput streams.


## Summarize the main method used in the paper in one paragraph.

 The paper presents a study on the effectiveness of various continual learning (CL) algorithms under computationally budgeted settings. 

The key components studied are:

1) Sampling strategies: Inexpensive strategies like uniform, class-balanced, recency-biased and FIFO sampling are compared against costly strategies like k-means, max loss and uncertainty sampling. 

2) Distillation losses: Distillation methods using BCE, MSE, cosine similarity and cross-entropy are evaluated.

3) FC layer corrections: Methods like cosine FC layer, ACE loss, BiC, WA and calibration are tested. 

The methods are evaluated on ImageNet2K (with 1K classes from ImageNet1K and 1K new classes) and Continual Google Landmarks V2 datasets under data incremental, class incremental and time incremental settings. The number of training iterations per time step is limited to simulate compute budgeting.

The main finding is that a simple experience replay baseline with uniform sampling and cross-entropy loss outperforms sophisticated CL algorithms like distillation, specialized sampling and FC layer corrections when training iterations are constrained. The performance gap widens with more stringent compute budgets. Partial fine-tuning of only the FC layer on strong pretrained models helps reduce the gap compared to full model training. Overall, the results suggest existing CL methods are less effective under practical compute restrictions.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the paper:

The paper revisits continual learning (CL) under a computationally budgeted setting where algorithms have constrained compute per time step rather than limited access to previous data. They propose a large-scale benchmark to analyze traditional CL approaches like sampling strategies, distillation losses, and FC layer correction methods. Experiments are conducted on ImageNet2K (with data incremental and class incremental streams) and Continual Google Landmarks V2 datasets. The compute budget is restricted to allow training on only 25-50% of observed data per time step. Surprisingly, they find that no existing CL algorithm outperforms a simple baseline of uniform sampling and training under this restricted compute setting. The gap widens with harsher compute limits. Training just the FC layer with a strong pretrained model can reduce the performance gap. Overall, most CL methods require large compute budgets and fail in realistic budgeted deployment. The work suggests compute restrictions inherently limit memory usage, and classical CL literature is less practical for real-world applications.


## What problem or question is the paper addressing?

 This paper is addressing the issue of continual learning algorithms being developed and tested in unrealistic settings that do not reflect real-world deployment constraints. 

Specifically, it is arguing that much prior work in continual learning:

- Assumes clear task boundaries and non-overlapping tasks/distributions, whereas real-world data streams have gradual distribution shifts without clear boundaries. 

- Puts memory constraints on storing past data samples, whereas storage is cheap. The real constraint is on computation per time step.

- Allows unlimited compute budget per time step for training, whereas real-world systems need to process streams under tight time constraints. 

The key question the paper seems to be asking is:

Do existing continual learning algorithms actually perform well when evaluated under realistic settings with:

- No assumed task boundaries

- Large memory to store past samples 

- Restricted compute per time step

In other words, how do CL algorithms perform under budgeted compute constraints, which implicitly restrict how much past data can be replayed?
