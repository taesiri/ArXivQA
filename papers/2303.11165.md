# [Computationally Budgeted Continual Learning: What Does Matter?](https://arxiv.org/abs/2303.11165)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question seems to be: 

"Do existing continual learning algorithms perform well under per step restricted computation?"

The key hypothesis appears to be that classical continual learning methods will struggle to cope with settings that impose a constrained computational budget per time step. 

The authors motivate this question by arguing that most prior continual learning literature overlooks computational constraints, instead focusing on limited access to previous data. But in real-world applications, systems are more constrained by computational budgets rather than storage. So they propose studying continual learning under fixed compute per time step, which implicitly restricts how much past data can be used.

To test this, the paper seems to experimentally evaluate various classical continual learning approaches like distillation, sampling strategies, and FC layer corrections under different computational budgets. The goal is to see if they outperform a simple baseline of uniform sampling within the allotted budget. 

Based on the results summarized in the introduction, their hypothesis seems to hold - traditional methods fail to beat the simple baseline when computation is restricted, suggesting they are not practical for realistically budgeted deployment.
