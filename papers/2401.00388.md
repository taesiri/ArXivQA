# [FusionMind -- Improving question and answering with external context   fusion](https://arxiv.org/abs/2401.00388)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem Statement:
- Answering questions by reasoning over knowledge is an important capability for language understanding. While language models (LMs) are effective at language tasks, they struggle with reasoning and leveraging knowledge graphs (KGs). Existing methods that combine LMs and KGs treat them as separate modalities, limiting joint reasoning.

Proposed Solution: 
- The paper experiments with two approaches - fine-tuning LMs on the OpenBookQA dataset, and using the QAGNN method which combines LMs and knowledge graphs. QAGNN constructs a joint graph with the QA context and relevant KG subgraph. It uses LMs to score node relevance and runs graph neural networks for joint reasoning and representation updates.

- The paper also studies the impact of supplementing additional factual knowledge context on model performance. The facts are taken from the OpenBookQA dataset and concatenated with the QA context.

Key Findings:
- Incorporating factual knowledge context significantly boosts performance over using just the QA context. For both LM-only and QAGNN methods, addition of facts improves accuracy by over 10-15%.

- Increasing contextual knowledge is more impactful than solely adding KGs. QAGNN performs better than LM-only method when facts are added, but difference is small. Just integrating KGs gives modest gains.

Main Contributions:
- Provides analysis and comparison of using LMs vs LM+KG methods for reasoning and QA.
- Studies effect of supplementing additional factual knowledge context on model accuracy.
- Key insight that incorporating contextual facts is more beneficial for QA performance than only knowledge graphs.

Limitations and Future Work:
- Constraints on model optimization and hyperparameter search space due to compute limitations.
- Investigate efficient methods to combine diverse contextual knowledge sources within maximum token size constraints.


## Summarize the paper in one sentence.

 This paper experiments with enhancing question answering by incorporating knowledge graphs and supplementary contextual facts into language models, finding that additional contextual facts significantly improves performance while knowledge graphs provide a smaller boost.


## What is the main contribution of this paper?

 Based on reviewing the paper, the main contribution appears to be:

Experimenting with and comparing two question answering approaches - fine-tuning a language model (LM) on its own versus combining a LM with a knowledge graph (KG) using the QAGNN method. A key finding is that incorporating supplementary knowledge facts leads to significant performance improvements in both cases, more so than just adding a KG. Specifically:

- They fine-tuned a DistilRoBERTa LM on the OpenBookQA dataset, with and without additional knowledge facts. Including facts improved accuracy from 52.2% to 67%.

- They implemented the QAGNN method which joins a LM, KG, and the QA context. Varying architectural hyperparameters, accuracy improved from 48.4% without facts to 69.4% at best with facts. 

- The accuracy gains from adding facts are more significant than just combining a LM and KG, suggesting contextual knowledge integration is very impactful for question answering.

In summary, the main contribution is demonstrating the substantial boost in QA performance achieved by supplementing existing QA context with relevant knowledge facts, over and above gains from incorporating knowledge graphs.


## What are the keywords or key terms associated with this paper?

 Based on reviewing the paper content, the keywords or key terms associated with this paper are:

Knowledge graphs (KG), language models (LM), question and answering (QA), graph neural networks (GNN)

As stated in the paper:

"\keywords{Knowledge graphs (KG), language models (LM), question and answering (QA), graph neural networks (GNN)}"

So the key terms are Knowledge graphs, language models, question answering, and graph neural networks. These concepts and techniques are central to the research presented in this paper on improving question answering performance using external context fusion from knowledge graphs and facts.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in the paper:

1. The paper mentions using Graph Neural Networks (GNNs) to update the representations of both the knowledge graph entities and the QA context node. Can you explain in more detail how the GNN architecture works to achieve this joint reasoning and representation update? 

2. The paper talks about using relevance scoring to estimate the importance of knowledge graph nodes relative to the QA context. What specific metrics or techniques are used to calculate this relevance score? How is the score then utilized in the model?

3. You mentioned the concept of constructing a "working graph" that connects the QA context vector and knowledge graph subgraph. Can you elaborate on the structure and composition of this working graph? What are the nodes and edges that connect them?  

4. One of the key ideas in the paper is using the language model itself to guide the extraction of the knowledge graph, rather than just retrieving the full graph. Can you explain this relevance-based subgraph extraction process? How does it differ from prior works?

5. The results show that incorporating supplementary knowledge facts leads to significant gains compared to using the knowledge graph alone. What hypotheses do you have for why additional factual context is so useful for QA?

6. You highlight token limits as a constraint when supplementing additional context to the QA pair. Can you suggest any methods or architectures that can get around this fixed token size limit when fusing contexts?

7. The paper mentions "AristoRoBERTa" as the language model used in the original work. How does fine-tuningRoberta on science QA datasets like OpenBookQA improve performance compared to a base Roberta model? 

8. You vary the number of graph neural network layers (k) in experiments. How does increasing k-hop neighbors provide benefits? At what point do you see diminishing returns?

9. How exactly does the model output a prediction at the end based on the updated QA and knowledge graph representations? Can you walk through this prediction step?

10. You suggest perturbing the question structure to study model explainability. What types of perturbations would provide insight into how much the model relies on the knowledge graph vs. QA context?
