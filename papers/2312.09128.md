# [Tokenize Anything via Prompting](https://arxiv.org/abs/2312.09128)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a high-quality paragraph summarizing the key points of the paper:

This paper proposes TAP, a unified and promptable foundation model capable of simultaneously segmenting, recognizing, and captioning arbitrary regions in an image. TAP upgrades the mask decoder in SAM to a versatile image decoder by adding a semantic token to each predicted mask. It is trained on SemanticSA-1B, a dataset created by integrating web-scale semantics from LAION-2B into the masks in SA-1B. Specifically, TAP learns semantic priors from a 5B-parameter CLIP model pre-trained on LAION image-text pairs. Through joint optimization of segmentation on mask tokens and concept prediction on semantic tokens, TAP exhibits strong generalization capabilities. Experiments show it achieves competitive performance on tasks like zero-shot instance segmentation and classification, while also setting a new record CIDEr score of 150.7 on Visual Genome region captioning by prompting a lightweight 38M-parameter text decoder. The semantic tokens encode sufficient region-level information to prompt complex language generation directly. Overall, TAP serves as a versatile region-level image tokenizer for encoding general-purpose regional context to facilitate diverse perception tasks.
