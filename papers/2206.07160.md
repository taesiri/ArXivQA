# [LAVENDER: Unifying Video-Language Understanding as Masked Language   Modeling](https://arxiv.org/abs/2206.07160)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be: Can we develop a unified framework for video-language understanding that supports various downstream tasks with a single architecture and training objective, without needing any task-specific components?The paper proposes Lavender, a unified video-language model where both pre-training and downstream tasks are formulated as masked language modeling. This eliminates the need for task-specific output layers or training objectives. The experiments aim to demonstrate Lavender's effectiveness across a diverse set of 14 video-language benchmarks and its advantages compared to task-specific baselines.In summary, the main hypothesis is that by unifying diverse video-language tasks into masked language modeling, a single model can achieve strong performance across different tasks without custom task-specific heads or training schemes. The Lavender model is proposed to test this hypothesis.


## What is the main contribution of this paper?

The main contribution of this paper is presenting Lavender, a unified video-language (VidL) framework where both pre-training and downstream tasks are formulated as Masked Language Modeling (MLM). The key ideas are:- Lavender uses a simple architecture with an encoder and lightweight MLM head, instead of separate task-specific decoders.- Both pre-training tasks (MLM and Video Text Matching) are formulated as MLM, without introducing additional heads. - All downstream tasks like video QA, retrieval, and captioning are also adapted to MLM, enabling a single set of parameters to support multiple tasks.- Experiments show Lavender achieves strong performance on 14 VidL benchmarks, outperforming prior state-of-the-art methods.- Analyses demonstrate Lavender's advantages in multi-task learning, few-shot generalization, and zero-shot video QA over comparable baselines.In summary, the main contribution is proposing a simplified and unified VidL framework Lavender that can tackle diverse tasks with MLM, eliminating the need for task-specific model architectures and training objectives. The excellent empirical results validate the effectiveness of this unified approach.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one-sentence summary of the paper:The paper proposes a unified video-language framework called LAVENDER, where all pre-training and downstream tasks are formulated as Masked Language Modeling, enabling the model to handle various video-language understanding tasks with a simple architecture and achieves state-of-the-art performance.


## How does this paper compare to other research in the same field?

Here are my thoughts on how this paper compares to other research in the field of video-language understanding:- The key innovation of this paper is proposing a unified framework, Lavender, that can tackle different video-language tasks like video QA, retrieval, and captioning using a single model architecture and training objective - masked language modeling (MLM). This contrasts with prior work that requires specialized model architectures and objectives for each different task. - Most prior video-language models like VideoBERT, HERO, ActBERT, etc. rely on task-specific output heads and losses for pre-training (e.g. masked LM, video-text matching) and downstream tasks. Lavender unified everything as MLM, with a shared lightweight MLM head, simplifying the model architecture.- Some recent image-text models like VL-T5, UNICORN, and OFA aim for a unified architecture using seq2seq modeling with encoder-decoder transformers. Lavender differs in using an encoder-only model with a simple MLM head, instead of a heavy decoder.- Experiments show Lavender achieves competitive or superior performance on a wide range of tasks compared to prior state-of-the-art methods, even with much less pre-training data. This demonstrates the effectiveness of the unified MLM approach.- Additional analyses illustrate benefits of the unified Lavender model - supporting multiple tasks with a single parameter set, better few-shot learning, and zero-shot QA capabilities.- Limitations are lack of evaluation on fine-grained tasks like moment retrieval, and the simple single-word answer generation for open-ended QA. Extensions for these are discussed as future work.Overall, the unified modeling approach of Lavender presents a promising direction for simplified and effective video-language understanding. The consistent masked LM formulation for different tasks is the key innovation over prior specialized models.
