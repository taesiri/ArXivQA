# [LAVENDER: Unifying Video-Language Understanding as Masked Language   Modeling](https://arxiv.org/abs/2206.07160)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be: Can we develop a unified framework for video-language understanding that supports various downstream tasks with a single architecture and training objective, without needing any task-specific components?The paper proposes Lavender, a unified video-language model where both pre-training and downstream tasks are formulated as masked language modeling. This eliminates the need for task-specific output layers or training objectives. The experiments aim to demonstrate Lavender's effectiveness across a diverse set of 14 video-language benchmarks and its advantages compared to task-specific baselines.In summary, the main hypothesis is that by unifying diverse video-language tasks into masked language modeling, a single model can achieve strong performance across different tasks without custom task-specific heads or training schemes. The Lavender model is proposed to test this hypothesis.


## What is the main contribution of this paper?

The main contribution of this paper is presenting Lavender, a unified video-language (VidL) framework where both pre-training and downstream tasks are formulated as Masked Language Modeling (MLM). The key ideas are:- Lavender uses a simple architecture with an encoder and lightweight MLM head, instead of separate task-specific decoders.- Both pre-training tasks (MLM and Video Text Matching) are formulated as MLM, without introducing additional heads. - All downstream tasks like video QA, retrieval, and captioning are also adapted to MLM, enabling a single set of parameters to support multiple tasks.- Experiments show Lavender achieves strong performance on 14 VidL benchmarks, outperforming prior state-of-the-art methods.- Analyses demonstrate Lavender's advantages in multi-task learning, few-shot generalization, and zero-shot video QA over comparable baselines.In summary, the main contribution is proposing a simplified and unified VidL framework Lavender that can tackle diverse tasks with MLM, eliminating the need for task-specific model architectures and training objectives. The excellent empirical results validate the effectiveness of this unified approach.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one-sentence summary of the paper:The paper proposes a unified video-language framework called LAVENDER, where all pre-training and downstream tasks are formulated as Masked Language Modeling, enabling the model to handle various video-language understanding tasks with a simple architecture and achieves state-of-the-art performance.
