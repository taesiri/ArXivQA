# [Value function interference and greedy action selection in value-based   multi-objective reinforcement learning](https://arxiv.org/abs/2402.06266)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Multi-objective reinforcement learning (MORL) extends regular RL to problems with multiple conflicting objectives represented as vector rewards. MORL methods like multi-objective Q-learning learn vector value functions and use a scalarization function to select greedy actions.

- This paper identifies an issue called "value function interference" where the learned vector values for a state-action pair can be inconsistent with the optimal policy, especially for non-linear scalarization functions. 

- This happens because multiple actions can map to the same scalarized utility value but have different vector value distributions. Random tie-breaking between such actions creates stochasticity in the updates to the value function, leading to interference.

- Value function interference is most problematic when optimizing for expected scalarised return (ESR) in stochastic environments, but can even happen in deterministic environments when optimizing for scalarised expected return (SER).

Proposed Solution:
- For deterministic environments, use deterministic (non-random) tie-breaking when multiple actions have equal utility. This reduces but does not eliminate the problem.

- For stochastic environments, use distributional MORL methods that model the distribution of returns, allowing the agent to accurately estimate ESR utilities.

Contributions:
- Identifies and defines the issue of value function interference in MORL
- Provides a simple example to demonstrate how it arises 
- Empirically evaluates non-random tie-breaking as a partial solution
- Shows value function interference occurs in both stochastic (ESR) and deterministic (SER) cases
- Proposes distributional MORL as a more general solution

The key insight is that in MORL, knowing the expected vector returns is not always enough to determine the optimal policy - you need to consider the distribution of returns. Avoiding random tie-breaking and using distributional representations can help address this.
