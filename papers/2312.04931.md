# [Retrieval-based Video Language Model for Efficient Long Video Question   Answering](https://arxiv.org/abs/2312.04931)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a summary paragraph of the key points from the paper:

This paper proposes a retrieval-based video language model (R-VLM) to enable efficient and interpretable question answering for long videos. The key idea is to divide a lengthy video into non-overlapping chunks, encode each chunk into visual tokens through CLIP, and select the top $K$ most relevant chunks to a given question through learned similarity matching. Only the tokens from these $K$ chunks are passed to a large language model (LLM) for answer inference. This approach reduces the number of tokens for LLM, preserving informative details in selected chunks while filtering out irrelevant content that may interfere with QA. Experiments show superior performance over prior video-LLM methods like Video-ChatGPT, while requiring comparable tokens as input to LLM. The retrieved chunks also provide justifications on where the model looks to generate the answer. Ablation studies validate the contribution of learned retrieval over random chunk selection and off-the-shelf CLIP matching. Overall, the work effectively enables efficient and interpretable language understanding of long videos through selective representation.
