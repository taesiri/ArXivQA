# [TaCo: Targeted Concept Removal in Output Embeddings for NLP via   Information Theory and Explainability](https://arxiv.org/abs/2312.06499)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a summary paragraph of the key points from the paper:

This paper proposes a novel method called TaCo for mitigating bias in NLP models by targeting and removing specific concepts from the models' output embeddings. The method leverages explainable AI techniques to decompose the embedding space into orthogonal concepts using singular value decomposition (SVD). It then evaluates the importance of each concept for predicting sensitive attributes like gender as well as the main prediction task using Sobol sensitivity analysis. Concepts that contribute strongly to predicting gender but weakly to the main task are identified as encoding bias and removed from the embedding, neutralizing gender information while preserving performance on the primary task. The paper demonstrates this approach on a RoBERTa model for occupation classification, showing up to a 14 percentage point drop in gender prediction accuracy with only a 0.1 drop in occupation accuracy after removing 4 concepts. The method enables interpreting the removed concepts to understand sources of bias. As a model-agnostic approach operating on embeddings, TaCo can integrate seamlessly into existing models and data pipelines for bias mitigation and auditing.
