# [Data-Efficient Learning via Clustering-Based Sensitivity Sampling:   Foundation Models and Beyond](https://arxiv.org/abs/2402.17327)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Training large machine learning models on massive datasets is very costly and time-consuming. However, using the full dataset is often not needed to reach close to optimal performance. 
- The key question is how to identify the most important subset of data that is sufficient to train a high quality model.
- Prior methods for data selection have limitations around computational overhead, robustness, and applicability to complex models like foundation models.

Proposed Solution:
- Present a data selection method based on k-means clustering and sensitivity sampling. 
- Assumption: Have an embedding representation of the data such that the model loss function is Hölder continuous with respect to the embeddings.
- Algorithm: Compute k-means clustering on embeddings. Sample points proportional to their distance to closest cluster center plus the center's loss.

Main Results:
- Proves this method yields a coreset that approximates the total loss over the full dataset within a 1±ε factor and small additive error based on the k-means clustering cost.
- Demonstrates strong empirical performance on fine-tuning foundation models and image classification, outperforming prior methods.
- Shows the method also works very well for linear regression, matching performance of more complex methods like leverage score sampling.

In summary, the paper provides a new theoretically-grounded data selection approach based on clustering and sketching. It is simple, scalable, and achieves state-of-the-art results across foundation models, neural networks, and linear regression. The key idea is to use cheap embeddings to approximate model loss for coreset construction.
