# [Interpretability Illusions in the Generalization of Simplified Models](https://arxiv.org/abs/2312.03656)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a summary paragraph of the key points from the paper:

This paper investigates whether common methods of simplifying deep learning models to make them more interpretable, such as dimensionality reduction and clustering, provide faithful approximations of the model's behavior out-of-distribution. The authors train Transformer language models on controlled datasets that allow systematic splits between train and test distributions, including variants of balanced parenthesis languages and a code completion task. They simplify representations within the models using techniques like PCA and clustering, construct corresponding simplified proxy models, and evaluate how well these proxies match the original models on in-distribution and out-of-distribution test sets. Across tasks, they consistently find generalization gaps between the original Transformer and its simplified proxy, wherein the proxy underestimates the model's systematic generalization abilities. The simplifications seem to capture coarse model behavior but miss critical details that enable generalization. The results raise important questions around the common practice of using simplified model abstractions for interpretation, suggesting they may present illusion of understanding. More analysis is needed to fully characterize which model components lead to these generalization gaps and how simplification methods could better encapsulate model computations.
