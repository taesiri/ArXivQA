# [Transformer tricks: Precomputing the first layer](https://arxiv.org/abs/2402.13388)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

This paper proposes a method to speed up inference for large language models that use relative position encodings (such as LLaMA, Mistral, PaLM, and Gemma). The key idea is to precompute and store parts of the first transformer layer, which saves computation during inference. 

The Problem:
Large transformer models are computationally expensive, especially during autoregressive inference when generating text token-by-token. Improving inference speed allows these models to be deployed more widely.

The Proposed Solution: 
For transformers using relative position encodings, much of the computation in the first layer depends only on the input embeddings. By precomputing the outputs of the feedforward network (FFN) and the query/key/value transformations and storing them instead of the embeddings, each inference step now requires fewer computations and memory reads.

Main Contributions:
- Shows how to precompute parts of the first transformer layer for models using either parallel or non-parallel attention.
- Analyzes computational savings, which can be a 11,000x reduction in reads for batch size 1, and 3-6% total parameter memory reduction.
- Provides examples for large models like Mistral-7B and Pythia-6.9B, showing the potential speedups.
- Notes that relative savings decreases for models with more layers, but absolute savings stays roughly constant.

In summary, this short paper introduces an effective trick to accelerate inference for large transformer language models by precomputing portions of the initial transformations based on the observation that they depend only on fixed input embeddings.
