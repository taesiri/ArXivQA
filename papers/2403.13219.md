# [Diffusion Model for Data-Driven Black-Box Optimization](https://arxiv.org/abs/2403.13219)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
The paper studies the problem of black-box optimization over complex structured variables using generative AI models like diffusion models. The goal is to generate new solutions/designs that optimize an unknown objective function, based on a dataset of solutions and their objective values. This is challenging because the objective function is unknown and evaluations are costly, so classic optimization methods like gradient descent cannot be applied. Additionally, the solutions lie in a high-dimensional space but have an intrinsic low-dimensional structure, so generating realistic solutions is difficult.

Proposed Solution: 
The key idea is to reformulate black-box optimization as conditional sampling from a learned distribution. Specifically, the authors propose training a conditional diffusion model on the dataset, which captures the distribution of good solutions. The diffusion model takes the objective value as a conditioning variable, allowing sampling of new solutions targeting a desired level of objective value. To handle the intrinsic structure, the conditional score network in the diffusion model has an encoder-decoder architecture to learn a low-dimensional latent space. 

Main Contributions:

- Formulate black-box optimization as conditional sampling using diffusion models, enabling generating new solutions without interacting with the costly objective function.

- Propose a conditional score architecture with encoder-decoder structure to capture intrinsic low-dimensional structure.

- Provide theoretical analysis on optimization performance:
    - Bound error between learned and true low-dimensional subspace
    - Decompose sub-optimality gap into bandit regret and diffusion approximation errors
    - Guarantee generated solutions concentrate near learned subspace

- Empirically demonstrate effectiveness on synthetic data, text-to-image generation, and offline RL.

The key impact is enabling black-box optimization over complex structured variables using unlabeled data and limited evaluations. The analysis gives guidance on setting conditioning values to balance distribution shift and objective improvement. Overall, the work presents the first theoretical guarantees for conditioned diffusion models and their application to data-driven optimization.
