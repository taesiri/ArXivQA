# [Adaptation of Biomedical and Clinical Pretrained Models to French Long   Documents: A Comparative Study](https://arxiv.org/abs/2402.16689)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Recently introduced French biomedical BERT models like DrBERT are limited to 512 token input lengths, which is insufficient for clinical notes that can contain thousands of words. 
- Long-sequence models can process much longer texts but have not been adapted for French biomedical domain.

Proposed Solution:
- Introduce and compare three French biomedical long-sequence models based on Longformer architecture:
   - DrBERT-4096: Convert DrBERT weights to Longformer
   - DrLongformer-FS: Train Longformer from scratch on French biomedical corpus
   - DrLongformer-CP: Continue pretraining English Clinical-Longformer on French biomedical data
- Evaluate on 16 downstream biomedical and clinical tasks involving long sequences.

Key Findings:
- For BERT models, no major differences between training from scratch (DrBERT) vs continual pretraining (CamemBERT-bio)
- For Longformer models, continual pretraining (DrLongformer-CP) works better than converting BERT (DrBERT-4096) or training from scratch (DrLongformer-FS)  
- DrLongformer-CP outperforms most models on 11 of 16 tasks - benefits from English clinical data + French biomedical data
- Longformer models improve performance across most tasks regardless of sequence length 
- BERT models remain most efficient for NER tasks  

Main Contributions:
- First French biomedical models adapted for long sequences
- Analysis of different pretraining strategies for Longformer architecture
- Demonstrate performance gains from continual pretraining English then French models
- Show Longformer models benefit biomedical document classification tasks with long contexts
