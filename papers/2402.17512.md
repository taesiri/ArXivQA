# [Latent Attention for Linear Time Transformers](https://arxiv.org/abs/2402.17512)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper "Latent Attention Transformer":

Problem:
- The standard attention mechanism in transformers has quadratic time and memory complexity with respect to the sequence length. This limits transformers to only modest-length sequences. 
- Reducing this complexity would allow transformers to scale to much longer sequences.

Proposed Solution:
- The paper proposes "Latent Attention" (Latte), which reduces the complexity to linear in the sequence length. 
- Latte introduces a latent variable model over the attention distribution. Specifically, the attention probability $p(s|t)$ that token $s$ attends to token $t$ is modeled via a latent variable $l$: $p(s|t) = \sum_l p(s|l)p(l|t)$.
- This allows the similarity computations to be between the sequence tokens and the latent tokens instead of between all pairs of sequence tokens, vastly reducing computational complexity.

Main Contributions:  
- Latte transformer has linear time and memory complexity in the sequence length, compared to quadratic for standard attention.
- It can readily replace standard attention layers as a drop-in replacement.
- Latte works for both bidirectional and causal (unidirectional) attention. 
- The causal Latte allows very efficient next-token prediction in linear time, compared to quadratic time for standard causal attention.
- Empirically, Latte achieves similar performance to standard attention on various language modeling benchmarks, whilst allowing scaling to much longer sequence lengths.

In summary, the paper introduces an efficient linear-complexity latent attention method to replace the standard quadratic attention in transformers. This facilitates much longer context lengths for transformers.
