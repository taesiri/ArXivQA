# [Polos: Multimodal Metric Learning from Human Feedback for Image   Captioning](https://arxiv.org/abs/2402.18091)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Establishing an automatic evaluation metric for image captioning models that closely aligns with human judgments is critical for advancing their development. 
- However, classic metrics like CIDEr exhibit weak correlation with human judgments. 
- Recent data-driven metrics have shown improved correlation but still face issues in handling hallucinations and generalizing across diverse images/text.

Proposed Solution:
- The paper proposes Polos, a supervised automatic evaluation metric for image captioning models. 
- It introduces Multimodal Metric Learning from Human Feedback (M2LHF), a framework to develop metrics based on human judgments.
- Polos computes scores from multimodal inputs using a parallel feature extraction mechanism with embeddings from contrastive learning. 
- To train Polos, the Polaris dataset is collected which has 131K human judgments from 550 people, 10x larger than standard datasets.

Main Contributions:
- Proposal of Polos metric and M2LHF framework for developing automatic evaluation metrics.
- Introduction of a parallel feature extraction mechanism using CLIP and SimCSE embeddings.
- Construction of Polaris dataset with 131K human judgments from 550 evaluators. 
- State-of-the-art performance of Polos on Composite, Flickr8K, PASCAL-50S, FOIL and Polaris benchmarks, demonstrating its effectiveness.

In summary, the paper presents a novel supervised evaluation metric Polos using a new M2LHF framework and dataset. Experiments show Polos aligns better with human judgments than previous metrics, with increased robustness and generalization capabilities.


## Summarize the paper in one sentence.

 Here is a one-sentence summary of the paper:

The paper proposes Polos, a supervised automatic evaluation metric for image captioning models that achieves state-of-the-art performance by extracting effective features through a parallel mechanism leveraging CLIP and RoBERTa and by training the model to directly predict human judgments based on multimodal inputs using a novel framework called M^2LHF.


## What is the main contribution of this paper?

 According to the paper, the main contributions are:

1) The introduction of Polos, an automatic evaluation metric tailored for image captioning models.

2) The proposal of M^2LHF, a novel framework used to develop a practical metric for image captioning based on human feedback. 

3) The introduction of a parallel feature extraction mechanism that leverages text embeddings from RoBERTa pretrained with SimCSE and vision-language embeddings from CLIP.

4) The construction of the Polaris dataset, containing 131,020 human judgments collected from 550 evaluators on image captions.

5) Achieving state-of-the-art performance on several image captioning benchmarks including Composite, Flickr8K-Expert, Flickr8K-CF, PASCAL-50S, FOIL, and the Polaris dataset itself.

In summary, the main contribution is proposing Polos, a new supervised evaluation metric for image captioning, along with the accompanying framework, feature extraction mechanism, and dataset used to develop it. The metric demonstrates improved correlation with human judgments over previous methods.


## What are the keywords or key terms associated with this paper?

 Based on my review of the paper content, some of the key terms and keywords associated with this paper include:

- Image captioning - The paper focuses on evaluating image captioning models.

- Automatic evaluation metric - The paper proposes Polos, a supervised automatic evaluation metric for image captioning models.

- Multimodal metric learning - The paper introduces a framework called M^2LHF (Multimodal Metric Learning from Human Feedback) for developing metrics based on human feedback. 

- Parallel feature extraction - The paper proposes a parallel feature extraction mechanism that utilizes features from both CLIP and RoBERTa pretrained with SimCSE.

- Polaris dataset - The paper constructs a new dataset called Polaris containing 131K human judgments for evaluating image captioning models.

- State-of-the-art performance - The proposed Polos metric achieves state-of-the-art results on several image captioning benchmarks.

- Robustness - A key focus of the paper is developing a robust and practical automatic evaluation metric for image captioning.

In summary, the key themes are around multimodal evaluation metrics for image captioning, using human judgments and multiple modalities to achieve robust performance.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in the paper:

1. The paper proposes a parallel feature extraction mechanism that utilizes both CLIP and RoBERTa features. Can you explain in more detail how this parallel mechanism works and why it is beneficial over using just CLIP or just RoBERTa features?

2. The M^2LHF framework trains the metric using human judgments as supervision. What are some challenges and considerations when collecting a large-scale human judged dataset like Polaris? How does Polaris compare to existing human judgement datasets?

3. The paper argues that regression models are better suited than ranking models for this image captioning evaluation task. Can you expand more on the limitations of ranking models and why the authors decided regression was more appropriate?

4. What modifications needed to be made to the RUSE method to adapt it for handling multimodal inputs of images and text? How does the proposed parallel feature extraction mechanism build upon RUSE?

5. The metric is evaluated on both in-domain and out-of-domain datasets. Why is out-of-domain evaluation important for assessing the practicality of a supervised metric? What do the out-of-domain results demonstrate about the proposed metric?

6. Could you explain in more detail the results on the FOIL hallucination detection dataset? Why does performance on FOIL provide evidence for a metric's robustness? 

7. The error analysis categorizes 7 types of failures for the proposed method. Which failure type was most common and how could the method potentially be improved to address this in future work?

8. How suitable do you think the proposed Polos metric would be for use in real-world image captioning applications? Are there any practical constraints or limitations not highlighted in the paper? 

9. The paper introduces the Polaris dataset containing human judgments. What further analyses could be performed on Polaris to better understand metrics and gain additional insights?

10. The method achieves state-of-the-art results across several image captioning benchmarks. Based on the information presented, do you think any recent advances could outperform the proposed Polos metric? Why or why not?
