# [Genetic Quantization-Aware Approximation for Non-Linear Operations in   Transformers](https://arxiv.org/abs/2403.19591)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem Statement:
- Non-linear operations like GELU, Softmax, EXP are prevalent in Transformers and incur substantial hardware costs due to high-precision computations (e.g. FP32, INT32). 
- Prior works optimize these operations through piecewise linear (pwl) approximation and lookup table (LUT) storage, but still rely on high-precision arithmetic.
- There is a lack of methods for efficient LUT approximation that are aware of low-bit quantization schemes commonly used in hardware accelerators.

Proposed Solution - GQA-LUT:  
- Proposes a genetic quantization-aware LUT approximation algorithm (GQA-LUT) for non-linear operations.  
- Handles the scaling factors in quantization schemes which significantly impact approximation accuracy.
- Introduces a Rounding Mutation (RM) method to tackle substantial shifts in breakpoints under large scaling factors.
- Enables efficient INT8 LUT approximation while maintaining accuracy.

Key Contributions:
- Analysis of linkage between scaling factors and LUT parameters under integer-only quantization.
- GQA-LUT algorithm that automatically determines LUT approximation parameters with quantization awareness.  
- Rounding mutation strategy to minimize approximation error due to scaling factors.
- Demonstrated superior operator-level accuracy over prior NN-LUT method.
- Achieved negligible <0.1% degradation for Transformer models on segmentation.
- INT8 LUT units reduce area by 81% and power by 79-80% versus FP32/INT32.

In summary, the paper proposes a genetic LUT approximation method called GQA-LUT to efficiently handle non-linear operations in Transformers under low-bit quantization schemes. This is achieved through quantization awareness and a rounding mutation strategy.
