# [Partial Search in a Frozen Network is Enough to Find a Strong Lottery   Ticket](https://arxiv.org/abs/2402.14029)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Strong lottery tickets (SLTs) are subnetworks within a randomly initialized neural network that can achieve high accuracy without any weight training. SLTs only need to store a small amount of information (random weight seed and binary pruning mask) to be reconstructed, allowing efficient specialized hardware inference.
- However, existing SLT search algorithms still require costly backpropagation-based optimization over the entire dense network connections. Recent work shows that searching over a randomly pruned sparse network reduces search space, but limits SLT sparsity range leading to suboptimal accuracy.

Proposed Solution:
- Propose a new method to reduce SLT search space regardless of target SLT sparsity. In addition to random pruning, it also randomly locks some weights at initialization to be permanent parts of the final SLT. Both pruned and locked (frozen) parameters are excluded from the search.
- Show theoretically that SLTs exist in such frozen networks through an extension of the subset-sum approximation technique used in prior work.
- New "partial SLT search" method searches only over the non-frozen part of the network. Allows controlling size and position of search space independently of SLT sparsity.

Main Contributions:
- First method to reduce SLT search space regardless of target sparsity, by random partial freezing (pruning and locking) of source network. Enables efficient search over full sparsity range.
- Theoretical guarantee of SLT existence in frozen networks through novel extension of subset-sum approximation with frozen variables.  
- Experiments show SLTs found in frozen networks achieve better accuracy-search space and accuracy-model size tradeoffs compared to SLTs from dense or sparse networks.
- Special case where SLT in a frozen graph neural network outperforms trained model, while requiring 40x less memory. Demonstrates freezing can boost SLT performance.

In summary, the paper proposes a way to significantly reduce SLT search space and model size by randomly freezing parts of the source network. Both theoretically and empirically shows that accurate SLTs exist in the remaining smaller search space.


## Summarize the paper in one sentence.

 This paper proposes a method to reduce the search space for finding strong lottery tickets by randomly freezing (pruning or locking) parts of the neural network at initialization, and shows experimentally and theoretically that accurate lottery tickets can be found efficiently within such partially frozen networks.


## What is the main contribution of this paper?

 The main contributions of this paper are:

1) It proposes a novel method to reduce the search space for finding strong lottery tickets (SLTs) that works by randomly freezing (pruning or locking) some parameters of the source network. This allows reducing the search space regardless of the desired SLT sparsity.

2) It provides a theoretical result that guarantees the existence of SLTs within such frozen networks, by extending previous subset-sum approximation proofs. 

3) Experimental results demonstrate that the proposed method can find SLTs that achieve better accuracy and model size tradeoffs compared to SLTs found in dense or sparse networks. Specifically, SLTs found in frozen networks improve the accuracy per search space and accuracy per model size ratios.

In summary, the paper introduces a way to reduce SLT search space that also allows finding better SLTs, with support from theory and experiments. This can enable more efficient SLT training and inference.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key keywords and terms associated with this paper are:

- Strong lottery tickets (SLTs)
- Subset-sum approximation
- Random pruning
- Random locking
- Frozen networks
- Partial SLT search
- Search space reduction
- Model compression
- Deep learning

The paper proposes a novel method called "partial SLT search" to reduce the search space for finding strong lottery tickets (SLTs) in neural networks. This is done by randomly pruning and locking some weights in the network, creating a "frozen" network. The existence of SLTs in such frozen networks is theoretically guaranteed through an extension of the subset-sum approximation technique. Experiments show the proposed method can find SLTs with better accuracy and model compression compared to SLTs found in dense or randomly pruned networks. Overall, the key focus is on efficiently finding compact SLT subnetworks with high accuracy by reducing the search space.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 potential in-depth questions about the method proposed in this paper:

1. The paper proposes freezing a random subset of weights in the network to reduce the search space for finding strong lottery tickets (SLTs). What is the intuition behind why freezing weights in this manner would still allow accurate SLTs to be found?

2. How does the proposed "partial SLT search" method differ from prior work that prunes weights at initialization to reduce search space? What advantage does partial search with freezing offer?

3. The paper proves the existence of accurate SLTs within frozen networks using an extension of the subset-sum approximation technique. Can you explain the key idea behind this proof and how it differs from the proof for SLT existence in dense/sparse networks?  

4. What are the differences in how the locking and pruning ratios should be set to position the search space optimally depending on whether the target SLT sparsity is low, medium or high? Explain the reasoning.

5. The experiments show frozen SLTs can outperform trained weight networks on node classification. What property of graph neural networks might freezing help mitigate that leads to this result?

6. How does the accuracy vs search space tradeoff and the accuracy vs model size tradeoff achieved by SLTs found via partial search compare to SLTs from dense and sparse networks?

7. The paper proposes a "freezing mask" to compress the network during inference by encoding the pruning/locking pattern. Explain how this differs from the supermask typically used to represent SLTs and what advantage it offers.

8. What scenarios would be better suited to solely pruning versus a mix of pruning and locking when constructing the frozen network? What factors determine the best approach?

9. The experiments used edge popup to find SLTs. How might the proposed freezing method interact differently with other SLT-finding algorithms like SNIP or GraSP?

10. The paper claims the method can reduce off-chip memory access in specialized hardware. Explain the specifics of how partial SLT search enables more on-chip reconstruction of the model during inference.
