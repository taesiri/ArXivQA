# [Partial Search in a Frozen Network is Enough to Find a Strong Lottery   Ticket](https://arxiv.org/abs/2402.14029)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Strong lottery tickets (SLTs) are subnetworks within a randomly initialized neural network that can achieve high accuracy without any weight training. SLTs only need to store a small amount of information (random weight seed and binary pruning mask) to be reconstructed, allowing efficient specialized hardware inference.
- However, existing SLT search algorithms still require costly backpropagation-based optimization over the entire dense network connections. Recent work shows that searching over a randomly pruned sparse network reduces search space, but limits SLT sparsity range leading to suboptimal accuracy.

Proposed Solution:
- Propose a new method to reduce SLT search space regardless of target SLT sparsity. In addition to random pruning, it also randomly locks some weights at initialization to be permanent parts of the final SLT. Both pruned and locked (frozen) parameters are excluded from the search.
- Show theoretically that SLTs exist in such frozen networks through an extension of the subset-sum approximation technique used in prior work.
- New "partial SLT search" method searches only over the non-frozen part of the network. Allows controlling size and position of search space independently of SLT sparsity.

Main Contributions:
- First method to reduce SLT search space regardless of target sparsity, by random partial freezing (pruning and locking) of source network. Enables efficient search over full sparsity range.
- Theoretical guarantee of SLT existence in frozen networks through novel extension of subset-sum approximation with frozen variables.  
- Experiments show SLTs found in frozen networks achieve better accuracy-search space and accuracy-model size tradeoffs compared to SLTs from dense or sparse networks.
- Special case where SLT in a frozen graph neural network outperforms trained model, while requiring 40x less memory. Demonstrates freezing can boost SLT performance.

In summary, the paper proposes a way to significantly reduce SLT search space and model size by randomly freezing parts of the source network. Both theoretically and empirically shows that accurate SLTs exist in the remaining smaller search space.
