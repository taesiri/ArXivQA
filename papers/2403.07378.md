# [SVD-LLM: Truncation-aware Singular Value Decomposition for Large   Language Model Compression](https://arxiv.org/abs/2403.07378)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
The advancements in large language models (LLMs) have been hindered by their substantial resource demands. Compression techniques such as quantization, pruning, and knowledge distillation require expensive retraining. In contrast, compression techniques based on low-rank approximation like singular value decomposition (SVD) avoid retraining but have two key limitations: (1) Truncating smaller singular values can lead to higher compression loss. (2) Lack of model parameter update after SVD truncation fails to compensate for accuracy degradation, especially at high compression ratios.

Proposed Solution - SVD-LLM:
This paper proposes SVD-LLM, a new SVD-based LLM compression method that addresses the above two limitations. The key ideas are:

1. Truncation-Aware Data Whitening: Derive a whitening matrix from input activation such that truncating the smallest singular values of the whitened weight matrix leads to minimal compression loss. This establishes a direct mapping between singular values and loss.

2. Layer-Wise Closed-Form Update: Progressively update the remaining model parameters layer-by-layer after SVD truncation to adapt to the new activation and compensate for accuracy degradation.

Main Contributions:
- Proposes truncation-aware data whitening strategy supported by theoretical analysis on the direct mapping between singular values and loss.  

- Introduces layer-wise closed-form update to compensate for accuracy drop, especially at high compression ratios.

- Evaluates SVD-LLM on 11 datasets and 7 models from 3 LLM families at 4 scales. It consistently outperforms baselines across different datasets, models and scales.

- Demonstrates superiority in terms of compression speed. SVD-LLM takes 15 mins whereas state-of-the-art ASVD takes 5.5 hours to compress LLaMA-7B at 20% ratio.  

- Shows SVD-LLM enhances performance of other compression methods like quantization and pruning. It also reduces runtime KV cache in addition to model compression.
