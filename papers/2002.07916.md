# [Information Condensing Active Learning](https://arxiv.org/abs/2002.07916)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central hypothesis appears to be that an active learning method which selects points that maximize the mutual information between the model predictions on the acquired points and the remaining unlabeled points will improve model accuracy and uncertainty over methods that maximize information only with respect to the model parameters. Specifically, the paper introduces a new acquisition function called Information Condensing Active Learning (ICAL) which tries to select a batch of points that have maximum statistical dependence, as measured by the Hilbert-Schmidt Independence Criterion (HSIC), between the model's predictions on the batch and the model's predictions on the remaining unlabeled points. The main hypothesis seems to be that by maximizing the information gained about the predictive distribution on the remaining unlabeled points, ICAL will better minimize the model's uncertainty and error on the test distribution compared to prior active learning methods.


## What is the main contribution of this paper?

This paper introduces Information Condensing Active Learning (ICAL), a new batch mode active learning method. The key ideas and contributions are:- ICAL selects a batch of points that maximizes the statistical dependency between the model's predictions on the batch and on the remaining unlabeled points. This aims to acquire labels that provide maximal information about the unlabeled data. - Previous methods like BatchBALD acquire points with high mutual information about the model parameters. ICAL shows through examples that this can increase uncertainty on the unlabeled data, hurting accuracy.- ICAL uses the Hilbert-Schmidt Independence Criterion (HSIC) to efficiently measure dependencies between predictions. This allows scaling to large batches and datasets.- Algorithmic optimizations are introduced, like averaging kernels and greedy selection, to make ICAL efficient. - ICAL is model-agnostic and applicable to any probabilistic model. Experiments show it outperforms prior methods like BatchBALD on image datasets.In summary, the main contribution is proposing ICAL, a new batch active learning approach to acquire labels that directly minimize uncertainty on the unlabeled data, rather than just maximize information about model parameters. Theoretical analysis and experiments demonstrate the advantages of this approach.
