# [RdimKD: Generic Distillation Paradigm by Dimensionality Reduction](https://arxiv.org/abs/2312.08700)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a summary paragraph of the key points from the paper:

This paper proposes RdimKD, a general and effective knowledge distillation framework for compressing neural networks. It relies solely on projecting the teacher's and student's feature maps onto a low-dimensional subspace using a simple projection matrix before applying an L2 loss. This allows the student network to learn valuable information from the teacher while still having flexibility to adapt to its lower capacity. The authors provide three realizations of RdimKD using different projection methods - PCA, autoencoders, and random orthogonal matrices. Experiments across image classification, object detection, semantic segmentation, language understanding, and speech recognition tasks demonstrate that RdimKD boosts student performance over baseline and achieves state-of-the-art or comparable results. A key advantage is the simplicity of RdimKD requiring no extra modules or hyperparameter tuning. The dimensionality reduction concept allows sufficient student freedom while transferring knowledge from the teacher. The strong empirical results highlight the generality, effectiveness, and simplicity of the proposed approach for knowledge distillation.
