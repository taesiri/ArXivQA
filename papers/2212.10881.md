# [In-Sensor &amp; Neuromorphic Computing are all you need for Energy Efficient   Computer Vision](https://arxiv.org/abs/2212.10881)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question appears to be: How can we reduce the energy consumption and bandwidth requirements of computer vision pipelines by integrating spiking neural networks with in-sensor computing approaches?More specifically, the paper proposes an in-sensor computing framework that utilizes spiking neural networks to address the high energy costs and bandwidth bottlenecks associated with transferring image data from sensors to downstream AI processing units. The key ideas are:- Use spiking neural networks (SNNs) which are more efficient than traditional DNNs due to their sparse activations and lack of expensive multiplication operations.- Implement the first layer convolution directly in the image sensor pixels to avoid transferring full image data. This is enabled by proposed analog circuits for convolution and spiking activation.- Reduce the number of output channels from the in-sensor convolution using knowledge distillation during SNN training. This further reduces the bandwidth between sensor and processor.- Overall, the proposed co-design of specialized hardware and tailored SNN training algorithms aims to minimize the total energy consumption of computer vision pipelines. Experiments demonstrate significant reductions in bandwidth, sensor energy, data transfer energy, and total energy compared to baseline approaches.In summary, the central hypothesis is that jointly optimizing sensing hardware and spiking network algorithms can enable more efficient embedded computer vision systems. The paper presents a concrete instantiation and experimental evaluation of this idea.


## What is the main contribution of this paper?

The main contributions of this paper appear to be:1. Proposing an in-sensor computing hardware-software co-design framework for spiking neural networks (SNNs) that reduces the bandwidth and energy consumption for image recognition tasks. 2. Leveraging recently proposed techniques for training one-time-step SNNs that are highly energy-efficient compared to traditional deep neural networks.3. Customizing the image sensor pixel array and periphery to implement analog multi-channel, multi-bit convolution and comparison operations needed for direct-encoded SNN models.4. Using analog correlated double sampling (CDS) circuits to implement positive and negative weights for the in-sensor SNN convolution.5. Proposing a simple 2-transistor analog comparator design that can fuse the batch normalization into the spiking activation function.6. Reducing the number of channels in the in-sensor convolution layer via knowledge distillation during SNN training to further reduce bandwidth.7. Demonstrating their framework reduces bandwidth by up to 96x and total energy by up to 2.32x compared to traditional computer vision processing, with only 3.8% drop in accuracy on ImageNet image classification.In summary, the key contribution is developing a holistic co-design approach spanning algorithms, circuits, and systems to enable very efficient in-sensor computing for SNNs targeting computer vision applications.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper proposes an algorithm-hardware co-design approach for in-sensor computing with spiking neural networks that reduces the bandwidth between image sensing and processing by up to 96x and total energy consumption by up to 2.32x for image recognition tasks, with only a 3.8% drop in accuracy on ImageNet compared to traditional computer vision pipelines.


## How does this paper compare to other research in the same field?

Here are my thoughts on how this paper compares to other research in the field of in-sensor computing for energy efficient computer vision:- The key idea of doing in-sensor computing for computer vision tasks is not entirely new, with prior works like [Chen et al. 2020, Scamp et al. 2020] proposing similar concepts. However, most prior in-sensor computing approaches focused on implementing regular convolutional neural networks (CNNs). - A key novelty of this paper is proposing customized in-sensor computing hardware and algorithms for spiking neural networks (SNNs), which are more hardware friendly and energy efficient compared to CNNs. - Specifically, the paper introduces circuit techniques like using analog correlated double sampling and simple comparators to realize the first spiking convolutional layer within image sensor pixels. This is different from prior in-sensor CNN works that required more complex ADC-based circuits.- The paper also proposes custom SNN training methods like distilling knowledge to reduce the number of channels in the in-sensor convolution layer, which helps to further reduce bandwidth and energy.- Compared to prior SNN hardware works, this paper uniquely targets algorithm-hardware codesign to enable efficient in-sensor computing for SNNs while retaining accuracy.- Evaluations on ImageNet classification show the proposed approach can reduce total energy by up to 2.32x compared to a baseline SNN system, which is a significant improvement over prior in-sensor CNN works.In summary, the key novelty of this paper is in proposing the first customized in-sensor computing solution for SNNs that co-optimizes algorithms and hardware, achieving much better efficiency than prior in-sensor CNN approaches. The circuit techniques and SNN training methods are tailored for efficient in-sensor spiking convolutional layers.


## What future research directions do the authors suggest?

Based on my reading of the paper, some of the key future research directions suggested by the authors include:- Exploring more complex vision tasks beyond image classification, such as object detection and segmentation, to evaluate the in-sensor computing framework. The paper currently focuses on image classification datasets like CIFAR10 and ImageNet.- Incorporating additional SNN layers beyond just the first convolutional layer inside the image sensor. The current approach implements only the first convolutional layer in-sensor. - Further optimizing the SNN models using techniques like pruning and quantization to maximize energy and bandwidth reductions. The paper mentions this as future work.- Expanding the in-sensor computing approach to other sensing modalities beyond vision, such as for audio and tactile sensors. The concept could potentially apply to other sensors that face similar data transfer bottlenecks.- Evaluating the benefits of the approach for applications where the sensor and processing are physically separated, necessitating wireless data transfer. The paper hypothesizes much larger gains in such use cases.- Incorporating programmable weights in the in-sensor convolutions using emerging non-volatile memory devices. Currently, the weights are fixed during manufacturing.- Expanding the simulations and models to include additional components like the region proposal network for object detection frameworks. This could improve the energy and delay modeling.In summary, the key future directions are to evaluate the approach on more complex vision tasks, incorporate more in-sensor layers, further optimize the SNN models, apply the concept to other sensing modalities, evaluate benefits for wireless systems, enable programmable weights, and expand the simulations.
