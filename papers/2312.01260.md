# [Rethinking PGD Attack: Is Sign Function Necessary?](https://arxiv.org/abs/2312.01260)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Neural networks are vulnerable to adversarial attacks - small input perturbations that fool the network. 
- Projected gradient descent (PGD) is a popular white-box attack that uses the "sign" of the gradient to craft adversarial examples. 
- However, the sign function discards gradient magnitude information. Previous attempts at using the raw gradient failed in PGD.

Key Questions:
- What factors determine perturbation quality in PGD? Why does the raw gradient fail? 
- Why is the sign function necessary in L_inf attacks but not L_2 attacks? 
- Can we achieve better attacks without the sign function?

Proposed Solution:
- Provide theoretical analysis showing perturbation change and prior adversarial loss determine gain.
- Empirically show PGD perturbation changes more than raw gradient PGD, explaining its better performance. 
- Propose Raw Gradient Descent (RGD) which uses an unclipped hidden variable for updates. This allows genuine adversary gradient information to be preserved.

Contributions:
- Identify factors that influence PGD attack strength 
- Explain why sign-based PGD outperforms raw gradient PGD
- Propose RGD algorithm that eliminates sign function and outperforms PGD
- Demonstrate RGD improves on PGD, raw PGD and AutoAttack across datasets, models and settings
- Provide useful insights into pros/cons of sign function in adversarial attacks

In summary, the paper provides significant analysis and insights into sign-based attacks, proposes an improved raw gradient algorithm, and shows consistent improvements over current state-of-the-art attacks.
