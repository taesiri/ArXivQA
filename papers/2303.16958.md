# [PartManip: Learning Cross-Category Generalizable Part Manipulation   Policy from Point Cloud Observations](https://arxiv.org/abs/2303.16958)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading, the central research question this paper tries to address is: how to learn a generalizable object manipulation policy that can achieve cross-category object manipulation?

Specifically, the authors aim to develop a manipulation policy that can generalize across different object categories, unlike prior work that focused on category-level generalization (e.g. opening different drawer instances). The key motivation is that cross-category generalization is vital for building truly intelligent robots that can manipulate novel objects in unconstrained real-world environments. 

To achieve this goal, the authors propose to leverage "GAParts" - parts that are shared across object categories (e.g. handles, doors) and can be manipulated in a similar way. They develop a large-scale simulation benchmark called PartManip with diverse object categories and realistic settings to facilitate research on this problem. 

The key technical contribution is a generalizable policy learning approach that involves:
1) Training an expert policy on states using part-based canonicalization and rewards.
2) Distilling the expert policy into a student vision-based policy using DAgger and behavior cloning.
3) Using a 3D Sparse UNet backbone and domain adversarial training for cross-category generalization.

Through experiments, the authors demonstrate superior performance on unseen categories compared to prior methods. They also validate the real-world transferability on a physical robot.

In summary, the central hypothesis is that by leveraging parts shared across categories and proposed techniques like canonicalization, state-to-vision distillation and domain adversarial training, the authors can learn policies that achieve the challenging goal of cross-category object manipulation. The PartManip benchmark and results support this hypothesis.


## What is the main contribution of this paper?

 The main contributions of this paper appear to be:

- Introducing a new large-scale, part-based cross-category object manipulation benchmark called PartManip. This benchmark requires manipulating parts like handles and doors across different object categories, posing challenges for generalization. 

- Proposing a method for learning a generalizable vision-based manipulation policy on this benchmark, which involves:

1) Training a state-based expert policy using part-based canonicalization and part-aware rewards. This canonicalization transforms the state into the coordinate frame of the target part to reduce variability. 

2) Distilling the expert policy into a vision-based student policy using DAgger and behavior cloning. This allows leveraging the expert while training on real visual observations.

3) Using data augmentations and a 3D Sparse UNet backbone for better generalization.

4) Introducing domain adversarial training for category-invariant features.

- Demonstrating that the proposed method substantially outperforms prior baselines, especially on unseen object categories. The method also succeeds on real-world experiments.

In summary, the main contribution appears to be proposing a new challenging cross-category manipulation benchmark and an effective learning method that combines state-based experts, vision-based policy distillation, data augmentations, and domain adaptation techniques to achieve good generalization performance.
