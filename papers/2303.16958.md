# [PartManip: Learning Cross-Category Generalizable Part Manipulation   Policy from Point Cloud Observations](https://arxiv.org/abs/2303.16958)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading, the central research question this paper tries to address is: how to learn a generalizable object manipulation policy that can achieve cross-category object manipulation?

Specifically, the authors aim to develop a manipulation policy that can generalize across different object categories, unlike prior work that focused on category-level generalization (e.g. opening different drawer instances). The key motivation is that cross-category generalization is vital for building truly intelligent robots that can manipulate novel objects in unconstrained real-world environments. 

To achieve this goal, the authors propose to leverage "GAParts" - parts that are shared across object categories (e.g. handles, doors) and can be manipulated in a similar way. They develop a large-scale simulation benchmark called PartManip with diverse object categories and realistic settings to facilitate research on this problem. 

The key technical contribution is a generalizable policy learning approach that involves:
1) Training an expert policy on states using part-based canonicalization and rewards.
2) Distilling the expert policy into a student vision-based policy using DAgger and behavior cloning.
3) Using a 3D Sparse UNet backbone and domain adversarial training for cross-category generalization.

Through experiments, the authors demonstrate superior performance on unseen categories compared to prior methods. They also validate the real-world transferability on a physical robot.

In summary, the central hypothesis is that by leveraging parts shared across categories and proposed techniques like canonicalization, state-to-vision distillation and domain adversarial training, the authors can learn policies that achieve the challenging goal of cross-category object manipulation. The PartManip benchmark and results support this hypothesis.


## What is the main contribution of this paper?

 The main contributions of this paper appear to be:

- Introducing a new large-scale, part-based cross-category object manipulation benchmark called PartManip. This benchmark requires manipulating parts like handles and doors across different object categories, posing challenges for generalization. 

- Proposing a method for learning a generalizable vision-based manipulation policy on this benchmark, which involves:

1) Training a state-based expert policy using part-based canonicalization and part-aware rewards. This canonicalization transforms the state into the coordinate frame of the target part to reduce variability. 

2) Distilling the expert policy into a vision-based student policy using DAgger and behavior cloning. This allows leveraging the expert while training on real visual observations.

3) Using data augmentations and a 3D Sparse UNet backbone for better generalization.

4) Introducing domain adversarial training for category-invariant features.

- Demonstrating that the proposed method substantially outperforms prior baselines, especially on unseen object categories. The method also succeeds on real-world experiments.

In summary, the main contribution appears to be proposing a new challenging cross-category manipulation benchmark and an effective learning method that combines state-based experts, vision-based policy distillation, data augmentations, and domain adaptation techniques to achieve good generalization performance.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the key points from the paper:

The paper introduces PartManip, a large-scale cross-category part manipulation benchmark in simulation, and proposes a method involving state-based expert learning with part canonicalization, augmented state-to-vision policy distillation, a 3D Sparse UNet backbone, and domain adversarial training, which achieves strong performance on manipulating diverse objects and generalizing to unseen categories.


## How does this paper compare to other research in the same field?

 Here are some key points comparing this paper to other research in robotic manipulation and generalizable skills:

- The paper introduces a new large-scale benchmark, PartManip, for learning cross-category generalizable manipulation skills. This pushes the limits of previous benchmarks like ManiSkill that focused on generalization within a single object category. The diversity and size of PartManip is a significant advance.

- The paper tackles a more realistic task setting than prior work. PartManip uses only sparse point cloud observations without additional oracle information like part segmentations. This makes the tasks closer to real-world applications.

- The method combines expert-guided RL training with student visual policy learning via distillation. This builds on prior work using vision-based policy learning and knowledge distillation, adapting the techniques to the more challenging cross-category setting.

- The part-based canonicalization proposed is a novel way to capture relative geometries and reduce variability during RL training. This and the part-aware rewards seem crucial for learning effective policies on the diverse PartManip environments.

- Using 3D Sparse U-Nets as a backbone is not entirely new but is an effective architecture choice for handling the complexity of the 3D visual observations. Optimizing its efficiency is an engineering contribution.  

- The use of domain adversarial training to learn more domain/category-invariant features is an interesting application of this technique in the context of generalization.

Overall, the work makes significant progress on vision-based cross-category manipulation skills by combining and extending many recent ideas, and by introduction of the large and challenging PartManip benchmark. The gains over prior methods are impressive. The real-world experiments also demonstrate promising sim-to-real transfer.


## What future research directions do the authors suggest?

 The paper suggests a few potential future research directions:

- Improving performance on unseen object instances and categories. The method shows promising results on cross-category generalization but there is still room for improvement, especially on novel objects. Developing techniques to further boost generalization is an important direction.

- Addressing the sim2real gap. There is a significant difference between simulated environments and the real world in terms of point cloud inputs and physics. Bridging this gap is critical for deploying the learned policies on real robots. Using sim2real transfer techniques like domain randomization could help.

- Exploring more complex tasks. The current work focuses on relatively simple pick-and-place style tasks. Extending to more complex multi-step tasks like assembling or packing could be an interesting direction.

- Learning from more realistic sensory inputs. The current method relies on 3-view point clouds which may not be readily available on real robots. Learning from more limited visual observations using monocular RGB or RGB-D data could make the approach more practical. 

- Combining model-based and model-free approaches. The paper uses a pure model-free reinforcement learning approach. Incorporating model-based planning especially for physics modeling could improve sample efficiency and performance.

- Manipulating more diverse objects. The objects currently considered have relatively simple geometries. Expanding the object sets to include more complex shapes and topologies could better showcase generalization abilities.

In summary, the key suggested directions are: improving generalization, addressing sim2real gaps, extending to more complex tasks and objects, learning from more limited/realistic sensory inputs, and combining model-based and model-free techniques. Advancing research in these areas could significantly extend the capabilities of generalizable robotic manipulation.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the key points in the paper:

This paper introduces PartManip, a large-scale cross-category part manipulation benchmark composed of 11 object categories, 494 objects, and 1432 tasks across 6 task classes in simulation. The benchmark uses sparse-view point clouds as input without oracle part segmentation, making it more realistic than prior work like ManiSkill. The authors propose a two-stage training framework to tackle the challenge of learning a generalizable vision-based manipulation policy. First, they train a state-based expert policy using part-canonicalization and part-aware rewards. Then they distill the expert policy into a vision-based student policy using DAgger and behavior cloning. To boost generalization, they utilize a 3D Sparse UNet backbone and introduce domain adversarial training. Experiments demonstrate superior performance over baselines, especially on unseen categories, with over 20% higher success on OpenDoor and OpenDrawer. The method also succeeds on novel objects in the real world. The benchmark and code have been publicly released.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the paper:

This paper introduces a large-scale, part-based cross-category object manipulation benchmark called PartManip. The benchmark is composed of 11 object categories, 494 objects, and 1432 tasks across 6 task classes (e.g. OpenDoor, OpenDrawer). Compared to previous benchmarks like ManiSkill, PartManip is more realistic and diverse - it uses sparse-view point clouds as input without any oracle segmentation masks, and has significantly more objects. The cross-category nature of the benchmark requires agents to generalize skills like opening doors learned on storage furniture to other categories like ovens or safes. This presents challenges due to large gaps in geometry and appearance across categories.

To tackle the difficulties of vision-based policy learning on this benchmark, the authors propose a two-stage training framework. First, they train a state-based expert policy using part-canonicalization and part-aware rewards. Then, they distill this into a vision-based student policy using DAgger and behavior cloning. They also utilize a 3D Sparse UNet backbone for handling diverse objects, data augmentation and domain adversarial training for better generalization, and optimize the Sparse UNet implementation for faster training. Experiments show their method significantly outperforms prior methods, especially on unseen categories where it improves success rates by over 20% on OpenDoor and OpenDrawer tasks. The benchmark and code have been publicly released.
