# [Hierarchical Recurrent Adapters for Efficient Multi-Task Adaptation of   Large Speech Models](https://arxiv.org/abs/2403.19709)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Full fine-tuning of large pre-trained models for downstream tasks is expensive as the entire model specializes on a single task. The per-task parameter overhead becomes as large as all model weights, making it unscalable for large number of tasks.
- Existing efficient adapters like residual adapters, LoRA, BitFit have high per-task parameter overhead for large scale multi-task adaptations.

Proposed Solution:
- Introduce Hierarchical Recurrent Adapter (HRA) module for efficient adaptation of large speech models.
- HRA has a shared recurrent controller network and multiple task-specific adapter heads. This reduces per-task overhead.
- HRA parameters are shared across layers of pretrained model for model-wise efficiency.
- Simple linear and FFN heads experimented as adapter heads.

Key Contributions:
- Achieve improved model-wise efficiency via recurrent adapter parameters across layers. 
- Modular adaptation model by having separate controller and task heads.
- Better task-wise efficiency by having small task-specific heads with shared controller.

Experiments:
- Tested HRA on single and multi-task ASR using a 2B parameter pretrained speech model.
- In single task, HRA matches full fine-tuning performance with 8-12x less parameters.
- In multi-task, HRA gets comparable performance to full fine-tuning while having sub-linear parameter growth as tasks increase.
- Ablations done on adapter head designs and controller RNN architectures.

In summary, the paper introduces a hierarchical and recurrent adapter approach for large speech model adaptation that is highly parameter and computation efficient in both single and multi-task settings.
