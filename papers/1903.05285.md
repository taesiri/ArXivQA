# [All You Need is a Few Shifts: Designing Efficient Convolutional Neural   Networks for Image Classification](https://arxiv.org/abs/1903.05285)

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper addresses is how to design efficient convolutional neural networks using only a few shift operations. The key points are:- Shift operations can provide spatial information communication in CNNs, but not every shift is necessary. Many just cause redundant memory movement. - The authors propose a "Sparse Shift Layer" (SSL) that learns to sparsify shift operations, eliminating unnecessary shifts. This reduces memory movement and speeds up inference.- Through experiments, the authors find that only a small fraction of feature maps need to be shifted to maintain model accuracy. This shows that just a few shift operations are sufficient for spatial communication.- To enable training of discrete shifts, the authors propose a quantization-aware shift learning method. This keeps shifts integer during inference but allows gradient-based learning.- A new network architecture called FE-Net is proposed to fully exploit the limited model capacity when using sparse shifts. - On ImageNet classification, FE-Net with SSL achieves state-of-the-art accuracy compared to other compact networks built with depthwise separable convolutions. This demonstrates the efficiency of SSL as a basic building block.In summary, the central hypothesis is that efficient convolutional networks can be constructed using only a small number of learned shift operations, instead of heavy use of depthwise separable convolutions. The SSL and FE-Net architectures provide evidence supporting this.


## What is the main contribution of this paper?

The main contributions of this paper are:1. It introduces a new basic component called Sparse Shift Layer (SSL) to build efficient convolutional neural networks. The key idea is to sparsify shift operations by adding a displacement penalty during training, so that only a small number of feature maps need to be shifted. This eliminates redundant memory movements. 2. It proposes a quantization-aware shift learning method to make shift operations differentiable during training while still using efficient integer shifts during inference. This avoids the need for interpolation.3. It designs an improved network architecture called Fully Exploited Network (FE-Net) that progressively involves more feature maps in computations as the layer increases. This helps maximize the use of the limited network capacity. 4. Experiments show SSL is very efficient. The resulting networks outperform counterparts built with depthwise separable convolutions in accuracy, FLOPs and inference speed on ImageNet. For example, they achieve 75% top-1 accuracy on ImageNet with only 563M Multiply-Add operations, the first time compact networks have achieved this accuracy without depthwise separable convolutions.In summary, the paper introduces SSL as an efficient new building block for convolutional neural networks, and shows it can surpass networks built with the commonly used depthwise separable convolutions. The findings suggest promising new directions for further improving network design.
