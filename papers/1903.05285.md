# [All You Need is a Few Shifts: Designing Efficient Convolutional Neural   Networks for Image Classification](https://arxiv.org/abs/1903.05285)

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper addresses is how to design efficient convolutional neural networks using only a few shift operations. The key points are:- Shift operations can provide spatial information communication in CNNs, but not every shift is necessary. Many just cause redundant memory movement. - The authors propose a "Sparse Shift Layer" (SSL) that learns to sparsify shift operations, eliminating unnecessary shifts. This reduces memory movement and speeds up inference.- Through experiments, the authors find that only a small fraction of feature maps need to be shifted to maintain model accuracy. This shows that just a few shift operations are sufficient for spatial communication.- To enable training of discrete shifts, the authors propose a quantization-aware shift learning method. This keeps shifts integer during inference but allows gradient-based learning.- A new network architecture called FE-Net is proposed to fully exploit the limited model capacity when using sparse shifts. - On ImageNet classification, FE-Net with SSL achieves state-of-the-art accuracy compared to other compact networks built with depthwise separable convolutions. This demonstrates the efficiency of SSL as a basic building block.In summary, the central hypothesis is that efficient convolutional networks can be constructed using only a small number of learned shift operations, instead of heavy use of depthwise separable convolutions. The SSL and FE-Net architectures provide evidence supporting this.


## What is the main contribution of this paper?

The main contributions of this paper are:1. It introduces a new basic component called Sparse Shift Layer (SSL) to build efficient convolutional neural networks. The key idea is to sparsify shift operations by adding a displacement penalty during training, so that only a small number of feature maps need to be shifted. This eliminates redundant memory movements. 2. It proposes a quantization-aware shift learning method to make shift operations differentiable during training while still using efficient integer shifts during inference. This avoids the need for interpolation.3. It designs an improved network architecture called Fully Exploited Network (FE-Net) that progressively involves more feature maps in computations as the layer increases. This helps maximize the use of the limited network capacity. 4. Experiments show SSL is very efficient. The resulting networks outperform counterparts built with depthwise separable convolutions in accuracy, FLOPs and inference speed on ImageNet. For example, they achieve 75% top-1 accuracy on ImageNet with only 563M Multiply-Add operations, the first time compact networks have achieved this accuracy without depthwise separable convolutions.In summary, the paper introduces SSL as an efficient new building block for convolutional neural networks, and shows it can surpass networks built with the commonly used depthwise separable convolutions. The findings suggest promising new directions for further improving network design.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper introduces a Sparse Shift Layer (SSL) as an efficient alternative to depthwise separable convolution for building compact neural networks, showing that only a few shift operations are needed for spatial communication; combined with a redesigned network architecture, SSL achieves state-of-the-art accuracy and speed on ImageNet compared to other compact networks.


## How does this paper compare to other research in the same field?

Here are my thoughts on how this paper compares to other research in efficient convolutional neural network design:- The main novelty is introducing the Sparse Shift Layer (SSL) as an efficient alternative to depthwise separable convolutions. The key idea is that only a small subset of feature maps needs to be shifted to enable spatial information communication. This allows eliminating many unnecessary shift operations and memory movement.- The paper shows SSL can outperform networks built with depthwise separable convolutions, which have been the dominant approach in many recent compact network designs like MobileNet and ShuffleNet. This is the first time a non-depthwise convolution approach achieves state-of-the-art accuracy and efficiency.- The paper demonstrates SSL works well in a redesigned network architecture called FE-Net, which progressively involves more features maps in computation as depth increases. This further improves efficiency and accuracy compared to prior work.- The quantization-aware shift learning method enables end-to-end training of networks with SSL while quantizing the shifts back to integers for efficient inference. This is analogous to how quantization methods train low-bit networks.- Extensive ablation studies validate the effects of SSL, shift sparsity, and the network architecture. The ImageNet experiments demonstrate the approach generalizes well to large-scale problems and achieves impressive accuracy (75-76%) at low computational complexity.- Overall, this paper makes excellent progress on efficient network design by introducing SSL and FE-Net. The efficiency gains over depthwise separable convolutions are clearly demonstrated. It outperforms prior compact networks including recent neural architecture search methods. The SSL concept is simple yet powerful, and could inspire more research in this direction.


## What future research directions do the authors suggest?

Based on my reading of the paper, the main future research directions suggested by the authors are:- Further explore the use of sparse shift layers (SSL) for building efficient convolutional neural networks. The authors show SSL can be a powerful alternative to depthwise separable convolutions, so they suggest more research into novel network architectures using SSL.- Apply SSL in neural architecture search (NAS) frameworks. Since SSL is shown to be an efficient basic component, the authors suggest incorporating it into NAS systems to see if it can help discover even better architectures. - Explore other applications and tasks where SSL could be beneficial beyond image classification. The authors demonstrate SSL for image classification tasks, but suggest trying it on other vision tasks as well as non-vision domains like speech and NLP.- Analyze the theoretical properties of networks built with SSL. The authors provide extensive empirical analysis of SSL, but suggest further theoretical analysis to understand why it is so effective and how to design networks leveraging its strengths.- Combine SSL with other methods like channel pruning, low-bit quantization, etc. The authors show SSL works well with squeeze-and-excitation modules, and suggest exploring how it could complement other compression and acceleration techniques.- Develop specialized hardware implementations that can maximize the efficiency of SSL. Since SSL relies on shifting operations, custom hardware tuned for this could further improve its speed.In summary, the main directions are utilizing SSL in novel networks and tasks, theoretical analysis of SSL, and specialized hardware for SSL. The authors show strong potential for SSL and suggest ways to leverage it even more effectively.


## Summarize the paper in one paragraph.

Here is a one paragraph summary of the paper:This paper introduces a new efficient component for convolutional neural networks called Sparse Shift Layer (SSL). SSL sparsifies the shift operation in a network by adding penalties during training to eliminate unnecessary shifts. Experiments show that only a small fraction of feature maps need to be shifted to maintain accuracy, significantly reducing memory movement and speeding up inference. The authors also propose a quantization-aware shift learning method to enable differentiation while keeping shifts discrete during inference. To fully exploit SSL, a network architecture is designed where only a subset of channels are transformed at each layer, progressively mixing more channels as depth increases. On ImageNet, networks built with SSL can surpass those built with depthwise separable convolutions in accuracy and speed. With 563M multiply-adds, SSL nets achieve 75.0% top-1 accuracy, outperforming MobileNets and ShuffleNets. This demonstrates the potential of SSL as an alternative to depthwise convolution for building efficient ConvNets.


## Summarize the paper in two paragraphs.

Here is a two paragraph summary of the paper:This paper introduces a new basic component called Sparse Shift Layer (SSL) to construct efficient convolutional neural networks. The authors show that only a few shift operations are actually needed to provide the spatial information communication in convolutional networks. They add a shift operation penalty during training to induce sparsity and eliminate redundant memory movement. Experiments on CIFAR and ImageNet show that networks with SSL can achieve high accuracy and fast inference speed with only a small fraction of feature maps shifted. To further improve performance, the authors propose a quantization-aware shift learning method to enable true shift operation during inference while keeping it trainable. They also design a new network architecture called FE-Net that progressively involves more feature maps into computation at each layer to better exploit the limited network capacity. Experiments demonstrate that FE-Net with SSL can surpass networks using depthwise separable convolutions like MobileNet in terms of accuracy and speed. The compact network achieves 75.0% top-1 accuracy on ImageNet with only 563M multiply-adds, which is state-of-the-art for this computational budget. This shows SSL can be an efficient alternative to depthwise convolution for building lightweight neural networks.


## Summarize the main method used in the paper in one paragraph.

The paper proposes a sparse shift layer (SSL) to construct efficient convolutional neural networks with only a few shift operations. The key ideas are:- Add a shift operation penalty during training to induce sparsity and eliminate redundant memory movements. Experiments show that only a small portion of feature maps need to be shifted to maintain accuracy.- Use a quantization-aware shift learning method to enable differentiation while keeping the inference benefit of integer shift operations. - Design a network architecture that progressively involves more feature maps into computation as depth increases, improving feature diversity and capacity utilization.- Experiments on CIFAR and ImageNet show SSL can build compact networks that outperform counterparts using depthwise separable convolutions in accuracy and speed. The approach provides an efficient alternative to separable convolutions for mobile applications.
