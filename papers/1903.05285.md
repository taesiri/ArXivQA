# [All You Need is a Few Shifts: Designing Efficient Convolutional Neural   Networks for Image Classification](https://arxiv.org/abs/1903.05285)

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper addresses is how to design efficient convolutional neural networks using only a few shift operations. The key points are:- Shift operations can provide spatial information communication in CNNs, but not every shift is necessary. Many just cause redundant memory movement. - The authors propose a "Sparse Shift Layer" (SSL) that learns to sparsify shift operations, eliminating unnecessary shifts. This reduces memory movement and speeds up inference.- Through experiments, the authors find that only a small fraction of feature maps need to be shifted to maintain model accuracy. This shows that just a few shift operations are sufficient for spatial communication.- To enable training of discrete shifts, the authors propose a quantization-aware shift learning method. This keeps shifts integer during inference but allows gradient-based learning.- A new network architecture called FE-Net is proposed to fully exploit the limited model capacity when using sparse shifts. - On ImageNet classification, FE-Net with SSL achieves state-of-the-art accuracy compared to other compact networks built with depthwise separable convolutions. This demonstrates the efficiency of SSL as a basic building block.In summary, the central hypothesis is that efficient convolutional networks can be constructed using only a small number of learned shift operations, instead of heavy use of depthwise separable convolutions. The SSL and FE-Net architectures provide evidence supporting this.
