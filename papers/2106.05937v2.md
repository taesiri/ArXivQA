# [Fair Normalizing Flows](https://arxiv.org/abs/2106.05937v2)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question is: 

How can we learn fair representations of data that provably guarantee sensitive attributes cannot be recovered, while maintaining high utility for downstream prediction tasks?

The key ideas and contributions are:

- Proposing Fair Normalizing Flows (FNF), a new method for learning fair data representations. FNF models the encoder as a normalizing flow, which allows computing the exact likelihood of representations.

- Leveraging the exact likelihood computation to minimize the statistical distance between representations of different sensitive groups. This allows deriving theoretical guarantees on the maximum unfairness (adversarial accuracy) of downstream predictors.

- Experimental evaluation showing FNF can substantially increase provable fairness on real-world datasets like Adult, Compas, Crime etc. without significantly sacrificing accuracy.

- Demonstrating additional benefits of FNF's invertibility such as algorithmic recourse and transfer learning.

In summary, the paper introduces Fair Normalizing Flows to address the problem of learning provably fair representations that are resistant to adversarial sensitive attribute recovery. Theoretical analysis and experiments validate FNF's effectiveness.
