# [LiRank: Industrial Large Scale Ranking Models at LinkedIn](https://arxiv.org/abs/2402.06859)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Large-scale ranking models face challenges in training efficiency, storage, inference speed, and ease of deployment to production. Researchers have proposed various state-of-the-art architectures like DCNv2, but directly applying them brings issues like overfitting, divergence, and diminished returns. 

Proposed Solution - LiRank Framework
- The paper proposes the LiRank framework to effectively combine different architectures like Residual DCN, Dense Gating, Transformers into a unified production ranking model.

Key Contributions:
- Residual DCN: An improved DCNv2 architecture with added attention and residual connections for better feature interactions.
- Isotonic Calibration Layer: Learns calibration within the DNN model during training to align predicted probabilities with real-world occurrences.  
- Optimization Methods: Techniques like incremental training, Transformer history modeling, explore/exploit strategies to improve model quality.
- Training Optimization: 4D model parallelism, optimized data loading, etc to increase training speed.  
- Compression Methods: Quantization and vocabulary compression using QR hashing and MurmurHash for large model deployment.

Applications:
- The methods were deployed in Feed ranking, Job recommendations and Ads CTR prediction at LinkedIn.
- Resulted in 0.5% more Feed sessions, 1.76% more qualified job applicants and 4.3% better Ads CTR.

The paper provides practical insights into developing and deploying large-scale deep ranking models, sharing numerous architecture improvements and training optimization techniques.


## Summarize the paper in one sentence.

 Here is a one sentence summary of the key points from the paper:

The paper introduces LinkedIn's LiRank large-scale deep ranking framework, which combines novel modeling components like Residual DCN and isotonic calibration with production optimization techniques for recommendation systems, achieving significant gains across LinkedIn services like Feed, Jobs, and Ads.


## What is the main contribution of this paper?

 Based on my understanding, the main contributions of this paper are:

1. It proposes a novel Residual DCN layer, which is an improvement over the DCNv2 architecture by adding attention and residual connections. 

2. It proposes a novel isotonic calibration layer that can be trained jointly within the deep learning model to perform calibration, improving model accuracy.

3. It provides customizations of deep learning based exploit/explore methods for production use.

4. It shares insights and practical solutions for effectively combining and tuning state-of-the-art architectures like Residual DCN, Dense Gating, Transformers etc. into a unified large-scale ranking model. It discusses challenges like diminishing returns, overfitting, divergence etc. faced in integrating these architectures.

5. It provides methods for compressing and quantizing large deep ranking models to enable effective deployment through techniques like model quantization and vocabulary compression.

6. It summarizes deployment experiences and results of applying these ranking models at large scale in domains like Feed Ranking, Job Recommendations and Ads CTR prediction at LinkedIn. The techniques discussed resulted in metrics improvements like 0.5% more feed sessions, 1.76% more qualified job applicants and 4.3% better Ads CTR.

In summary, the paper makes modeling and optimization contributions for large-scale ranking systems, while also sharing practical deployment insights.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, here are some of the key terms and keywords that seem most relevant:

- Large scale ranking models
- Deep neural networks
- Residual DCN (novel architecture modification proposed in the paper)
- Isotonic calibration layer (novel method proposed for model calibration)
- Model compression techniques (quantization, vocabulary compression) 
- Multi-task learning
- Training optimizations (4D model parallelism, incremental training, etc.)
- LinkedIn use cases (Feed ranking, Jobs recommendations, Ads CTR prediction)
- Production deployments and lessons learned

The paper discusses innovations in deep learning ranking models, with a focus on architectures, optimizations, and compression techniques to enable large scale deployment. It shares case studies from LinkedIn's production systems for feed, jobs, and ads, including experiment results. Key terms cover the modeling contributions, training/serving optimizations, and production learnings when building industrial recommendation systems.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 potential in-depth questions about the methods proposed in this paper:

1. The paper proposes a novel Residual DCN layer that adds attention and residual connections to the DCNv2 architecture. Can you explain in more detail how the attention mechanism works in this architecture? How does it help improve the model's feature learning capability?

2. The paper introduces an isotonic calibration layer that is trained jointly within the deep learning model. What is the motivation behind developing this customized layer rather than using traditional post-hoc calibration methods? How does joint training help improve calibration and overall model performance?

3. Incremental training using the Hessian matrix is utilized in the paper to mitigate catastrophic forgetting. What are the computational challenges with directly computing the full Hessian matrix? How does using just the diagonal elements help alleviate this issue? 

4. The paper employs a Transformer-Encoder based architecture for modeling member interaction history. What are the advantages of using self-attention for history modeling compared to RNN based approaches? How was the Transformer customized for efficiency?

5. For exploration/exploitation, the paper adapts a Bayesian linear regression method on the last layer weights. Why is limiting the Bayesian treatment to just the final layer effective? What are the posterior probability computations that are avoided compared to full Bayesian neural networks?

6. Wide popularity features are incorporated using separate random effect models. Why are these models more suitable for highly volatile identifiers compared to the main feed ranking model? How frequently are they retrained to adapt to shifts?

7. What motivates the use of multi-task learning in the feed ranking model? What architectural variants for multi-task learning were explored? What were the tradeoffs observed in terms of performance vs efficiency?  

8. What are some of the key challenges identified in directly predicting dwell times? How does the threshold based long-dwell classifier address these issues by adapting to evolving user consumption patterns?

9. Explain the quotient-remainder based embedding table compression technique used in the paper. How does it allow the use of unlimited vocabularies without collisions? What aggregation methods were found to work best?

10. What motivated the use of post-training quantization for embedding tables? Why is the middle-max variant preferred over min-max quantization in this application? How does quantization improve generalization performance?
