# [m2mKD: Module-to-Module Knowledge Distillation for Modular Transformers](https://arxiv.org/abs/2402.16918)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Training modular neural networks poses optimization challenges due to sparse connectivity between modules. Using knowledge distillation from pretrained monolithic models could help, but conventional distillation methods fail when applied to modular networks due to unique architectures and enormous parameters.

Proposed Solution: 
- The paper proposes a module-to-module knowledge distillation (m2mKD) approach to transfer knowledge between modules of the teacher and student models.

- The teacher model is first trained in a modular way using Deep Incubation. It is split into modules that are incubated using a smaller meta model. 

- Student modules are encouraged to mimic the behaviour of the corresponding teacher modules by combining them independently with the meta model and comparing outputs.

- Only the student module parameters are updated during this process. The distilled student modules are then used to initialize the full modular student model.

Key Contributions:

- First work to investigate knowledge distillation specifically for modular neural networks. Identifies challenges in this context.

- Proposes the module-to-module knowledge distillation (m2mKD) approach to address challenges, enables transforming monolithic models into modular ones.

- Demonstrates m2mKD improves performance of modular models - NACs and MoEs. Up to 5.6% better IID accuracy on TinyImageNet. Up to 4.2% better OOD robustness on TinyImageNet-R.

- Establishes feasibility of using Deep Incubation to train teacher modules in a modular way instead of just separating a pretrained monolithic model.

- Algorithm is general, does not impose restrictions on teacher or student model architectures. Can handle irregular distillation scenarios beyond monolithic-to-modular.

In summary, the paper introduces a novel knowledge distillation approach specifically tailored for modular neural networks to improve their training and performance by transferring knowledge in a module-to-module manner.
