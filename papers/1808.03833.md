# [Self-Supervised Model Adaptation for Multimodal Semantic Segmentation](https://arxiv.org/abs/1808.03833)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be: How can we enable convolutional neural networks for semantic segmentation to dynamically adapt the fusion of multimodal features in an optimal, self-supervised manner according to factors like object category, spatial location, and scene context?The key hypothesis appears to be that explicitly modeling the correlation between multimodal feature maps before fusion will allow the network to selectively emphasize the most informative features from each modality while suppressing less useful features. This should lead to more effective exploitation of complementary multimodal information compared to naive fusion techniques like concatenation. The authors propose a Self-Supervised Model Adaptation (SSMA) approach with a novel SSMA block that recalibrates and fuses modality-specific feature maps in a probabilistic, activation-driven way. By learning the fusion adaptively in a self-supervised fashion, the goal is to make the multimodal segmentation model more robust to different environments, conditions, and modalities.In summary, the central research question is how to enable optimal, dynamic, self-supervised adaptation of multimodal fusion in CNNs for semantic segmentation. The key hypothesis is that explicitly modeling feature correlations will allow selective emphasis of useful complementary features from each modality.
