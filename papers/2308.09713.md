# [Dynamic 3D Gaussians: Tracking by Persistent Dynamic View Synthesis](https://arxiv.org/abs/2308.09713)

## Design 10 in-depth questions about method proposed in this paper and avoid simple question.

 Here are 10 potential in-depth questions about the method proposed in this paper:

1. The paper introduces the idea of using Dynamic 3D Gaussians to model dynamic scenes. How do the properties of Gaussians (e.g. being soft and having infinite support) help with modeling dynamic scenes compared to alternative representations like points?

2. The paper enforces physical plausibility of the Dynamic 3D Gaussian reconstruction using local rigidity, local rotational similarity, and long-term local isometry losses. Can you explain the intuition behind each of these losses and how they help regularize the optimization? 

3. Tracking emerges naturally from the persistent Dynamic 3D Gaussian representation without requiring any correspondence supervision. Why does enforcing persistence of the Gaussians' attributes enable accurate tracking to emerge?

4. The ablation study shows that the local rigidity loss is crucial for good tracking performance. Why might directly enforcing rigidity between neighbouring Gaussians be more effective than relying on the rendering loss alone to induce rigidity?

5. How does explicitly modeling rotation with the Dynamic 3D Gaussians allow for recovering full 6-DOF motion compared to a point-based representation? What applications does having the rotational motion enable?

6. The Dynamic 3D Gaussian representation lends itself well to editing applications like attaching novel views to keypoints or propagating edits across time. What properties of the representation make these applications straightforward?

7. What are the tradeoffs between this method's particle-based Lagrangian representation versus grid-based Eulerian representations for modeling dynamic scenes? How do they compare in terms of correspondence and novel view synthesis?

8. The method requires a multi-camera capture setup and does not work off-the-shelf on monocular video. How could you extend the approach to handle monocular video input? What challenges does monocular input present?

9. The paper mentions real-time rendering applications as an advantage of this method. How is the rendering process efficient compared to other neural rendering techniques? Why is real-time feasible?

10. What are some limitations of this method? For example, how could it be extended to handle dynamic scenes with objects entering and leaving the field of view?
