# [Dynamic 3D Gaussians: Tracking by Persistent Dynamic View Synthesis](https://arxiv.org/abs/2308.09713)

## Design 10 in-depth questions about method proposed in this paper and avoid simple question.

 Here are 10 potential in-depth questions about the method proposed in this paper:

1. The paper introduces the idea of using Dynamic 3D Gaussians to model dynamic scenes. How do the properties of Gaussians (e.g. being soft and having infinite support) help with modeling dynamic scenes compared to alternative representations like points?

2. The paper enforces physical plausibility of the Dynamic 3D Gaussian reconstruction using local rigidity, local rotational similarity, and long-term local isometry losses. Can you explain the intuition behind each of these losses and how they help regularize the optimization? 

3. Tracking emerges naturally from the persistent Dynamic 3D Gaussian representation without requiring any correspondence supervision. Why does enforcing persistence of the Gaussians' attributes enable accurate tracking to emerge?

4. The ablation study shows that the local rigidity loss is crucial for good tracking performance. Why might directly enforcing rigidity between neighbouring Gaussians be more effective than relying on the rendering loss alone to induce rigidity?

5. How does explicitly modeling rotation with the Dynamic 3D Gaussians allow for recovering full 6-DOF motion compared to a point-based representation? What applications does having the rotational motion enable?

6. The Dynamic 3D Gaussian representation lends itself well to editing applications like attaching novel views to keypoints or propagating edits across time. What properties of the representation make these applications straightforward?

7. What are the tradeoffs between this method's particle-based Lagrangian representation versus grid-based Eulerian representations for modeling dynamic scenes? How do they compare in terms of correspondence and novel view synthesis?

8. The method requires a multi-camera capture setup and does not work off-the-shelf on monocular video. How could you extend the approach to handle monocular video input? What challenges does monocular input present?

9. The paper mentions real-time rendering applications as an advantage of this method. How is the rendering process efficient compared to other neural rendering techniques? Why is real-time feasible?

10. What are some limitations of this method? For example, how could it be extended to handle dynamic scenes with objects entering and leaving the field of view?


## What is the main contribution of this paper?

 The main contribution of this paper is a method for simultaneous dynamic 3D scene modeling, novel view synthesis, and 6 degree-of-freedom (6-DOF) tracking using Dynamic 3D Gaussians. Specifically:

- They represent dynamic 3D scenes as collections of 3D Gaussians whose positions and orientations change over time, but with consistent color, opacity and size.

- They introduce differentiable rendering of these Dynamic 3D Gaussians to optimize them to match input camera views across time, enabling dynamic novel view synthesis.

- They enforce physical priors like local rigidity during optimization so the Gaussians track the same physical points over time, resulting in persistent dense 6-DOF tracking as a byproduct of novel view synthesis. 

- They demonstrate state-of-the-art performance on synthetic and real datasets for novel view synthesis and especially long-term dense tracking in both 2D and 3D.

- The decomposability of the Gaussians enables creative downstream applications like attaching cameras to scene elements, editing across time, and combining scenes.

In summary, the key contribution is using persistent Dynamic 3D Gaussians for unified dynamic scene reconstruction, novel view synthesis, and 6-DOF tracking in a differentiable rendering framework. The results and applications showcase the promise of this representation.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the key points from the paper:

The paper proposes a method for simultaneously modeling and rendering dynamic 3D scenes from multi-view video while achieving persistent dense 6-DOF tracking, by representing scenes with particles called Dynamic 3D Gaussians that move and rotate over time with enforced physical consistency.


## How does this paper compare to other research in the same field?

 Here are a few key points of comparison between this paper and other research in dynamic 3D scene modeling, novel view synthesis, and tracking:

- The use of 3D Gaussians to represent the scene is novel compared to prior work like NeRF which uses MLPs or grid-based approaches. Gaussians provide a natural decomposition of the scene into particles that can move independently. 

- Modeling full 6-DOF motion including rotation with 3D Gaussians is also new compared to point-based representations used in some prior work. This allows imposing physical priors on rotation as well as translation.

- Emergence of tracking from view synthesis optimization alone, without any flow or correspondence supervision, sets it apart from methods that require ground truth tracks or optical flow as input.

- Real-time optimization and rendering during test time is much faster than many learning-based approaches that require offline training. Enables applications requiring real-time performance.

- Multi-view input used here provides more accurate metric reconstruction compared to monocular methods. But monocular capability could be a promising extension.

- Lack of any training data beyond the test frames makes it more general than approaches that require large labeled tracking datasets. However, it can currently only track visible points unlike learning methods.

- Creative editing applications like attaching cameras to scene points are enabled by the decomposable Gaussian representation. Unseen in most prior dynamic reconstruction work.

Overall, the combination of accuracy, speed, generalizability and creative applications make this paper unique compared to prior work on similar tasks. The Gaussian representation and emergence of tracking from optimization seem to be the biggest innovations.
