# [Modeling Collaborator: Enabling Subjective Vision Classification With   Minimal Human Effort via LLM Tool-Use](https://arxiv.org/abs/2403.02626)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem: 
Developing classifiers for subjective or nuanced visual concepts traditionally requires substantial manual effort to annotate training data. Recent techniques like Agile Modeling still require users to spend significant time (30+ minutes) monotonously labeling thousands of images per concept. This is tedious and labor-intensive.

Solution:
The paper proposes "Modeling Collaborator" to train subjective image classifiers with minimal human effort. The key ideas are:

1) Replace manual labeling with natural language interactions to decompose complex concepts into simpler objective sub-components. This leverages humans' inherent ability for logical reasoning.

2) Use recent advances in foundation models (LLMs and VLMs) to automatically label training data by asking the models visual questions. Specifically, an LLM orchestrates a VQA model andCaptioning model to assess images. 

3) Only ask users to manually validate a small set of 100 seed images to adapt the concept over time.

4) Use unlabeled public images and the automatic annotator to generate distillation training data to train a lightweight deployable model.

Contributions:

- New training framework to develop subjective vision classifiers with 10-20x less human effort than prior art.

- Demonstrates state-of-the-art performance over zero-shot baselines like CLIP on classifying nuanced concepts. Matches performance of crowd-sourced models but without any human labeling effort.

- Analyzes model performance as more user feedback and auto-labeled data is progressively added. Provides intuitions into the tradeoffs.

The approach helps overcome the expensive annotation barrier for developing real-world vision classifiers for content moderation, aesthetic prediction etc. It has the potential to enable end-user custom classifiers.
