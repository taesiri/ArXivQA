# [Achieving Pareto Optimality using Efficient Parameter Reduction for DNNs   in Resource-Constrained Edge Environment](https://arxiv.org/abs/2403.10569)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Existing AI models require intensive computational resources only available in the cloud, causing issues like service interruptions, latency, and performance problems. 
- There is a need to shift towards edge computing for training machine learning models. However, modern deep learning architectures are designed to be computationally intensive, making training on resource-constrained edge devices challenging.

Proposed Solution:
- Optimize an existing deep neural network architecture (Xception) using compact network design strategies to improve hardware efficiency and facilitate on-device training.
- Employ techniques from SqueezeNet like replacing 3x3 filters with 1x1 and decreasing input channels to reduce number of parameters while preserving accuracy.
- Evaluate optimized Xception model on image classification with Caltech-101 dataset and PCB defect detection task.

Key Contributions:
- Present an optimization of DNNs for efficient hardware utilization to enable training on edge devices.
- Implement optimization on Xception architecture and evaluate performance in terms of accuracy, memory usage and latency.
- Achieve Pareto optimality - high accuracy comparable to original Xception and lower memory consumption.
- Explore benefits of transfer learning on resource utilization by comparing pretrained vs non-pretrained models.

In summary, the paper proposes an optimized Xception model using compact network design that achieves objectives of high accuracy and improved hardware efficiency. Experiments on edge devices and analysis of results validate the feasibility of model optimization for facilitating on-device training.
