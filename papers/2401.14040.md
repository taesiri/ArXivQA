# [(Chat)GPT v BERT: Dawn of Justice for Semantic Change Detection](https://arxiv.org/abs/2401.14040)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Evaluating the ability of transformer language models like BERT and (Chat)GPT to detect semantic change over time. Specifically on two diachronic Word-In-Context (WiC) tasks - TempoWiC (short-term changes in tweets) and HistoWiC (long-term changes in historical text).

Proposed Solution:  
- Compared performance of BERT and ChatGPT/GPT on the two WiC tasks using different prompting strategies (zero-shot, few-shot, many-shot) and temperature values (for GPT). 
- Introduced HistoWiC task adapted from SemEval Lexical Semantic Change benchmark.
- Focused evaluation on using (Chat)GPT in an automatic way with templated prompts.

Key Findings:
- With reasonable prompts, (Chat)GPT performance is comparable but lower than BERT's on HistoWiC. But significantly worse on TempoWiC.  
- GPT through API outperforms ChatGPT web interface for the tasks.
- Lower GPT temperature values result in better performance.
- In-context learning through few-shot prompting helps for TempoWiC but not for HistoWiC.

Main Contributions:
- First comprehensive evaluation of (Chat)GPT's potential for semantic change detection. 
- Demonstrated completely automatic evaluation of (Chat)GPT responses using templated prompts.
- Introduced new HistoWiC benchmark for long-term semantic change.
- Showed GPT API better than ChatGPT web interface for tasks. 
- Compared prompting strategies and temperature values for (Chat)GPT.
- Revealed deficiencies compared to BERT but showed potential for improvements.

The paper systematically probes (Chat)GPT's capabilities on semantic change detection and introduces a new evaluation benchmark. While highlighting current limitations, it opens up promising future research directions.


## Summarize the paper in one sentence.

 The paper evaluates the potential of GPT models like ChatGPT for semantic change detection through Word-in-Context tasks, finding BERT currently outperforms GPT but GPT shows promise, especially for detecting long-term lexical changes.


## What is the main contribution of this paper?

 The main contribution of this paper is an empirical investigation into the use of ChatGPT and GPT 3.5 for detecting semantic change, through evaluations on the TempoWiC and a newly introduced HistoWiC task. Specifically, the paper:

- Presents the first attempt to assess the potential of (Chat)GPT models for studying semantic change.

- Introduces HistoWiC, a novel diachronic evaluation benchmark based on historical text to assess models' ability to detect long-term semantic changes. 

- Compares the performance of (Chat)GPT and BERT on the TempoWiC and HistoWiC tasks under various setups (e.g. different prompting strategies and temperature values).

- Finds that while (Chat)GPT achieves comparable performance to BERT on HistoWiC, it performs significantly worse on TempoWiC which involves detecting short-term semantic changes.

- Concludes that BERT currently provides a more effective and robust solution than (Chat)GPT for modeling semantic change, challenging the belief that ChatGPT makes BERT outdated technology.

In summary, the main contribution is an in-depth empirical analysis to evaluate the potential of using (Chat)GPT models, compared to BERT, for computationally modeling semantic change.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and keywords associated with it are:

- Semantic change detection
- Word-in-Context (WiC) task
- Temporal WiC (TempoWiC) 
- Historical WiC (HistoWiC)
- Transformer-based language models
- BERT
- (Chat)GPT
- Lexical semantic change
- Semantic variation
- Diachronic text

The paper focuses on evaluating the potential of (Chat)GPT models, specifically ChatGPT and GPT 3.5, for the task of semantic change detection. It introduces a new Historical WiC (HistoWiC) task adapted from an existing semantic change detection benchmark. The performance of (Chat)GPT on TempoWiC and HistoWiC is compared to BERT models. Key aspects examined include different prompting strategies for (Chat)GPT and the impact of temperature values. The goal is to assess whether off-the-shelf (Chat)GPT can outperform fine-tuned BERT models for recognizing lexical semantic changes in both contemporary (TempoWiC) and historical (HistoWiC) textual data.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in this paper:

1. The paper introduces a new dataset called HistoWiC for evaluating long-term semantic change detection. How was this dataset created and what were the steps involved in adapting the existing LSC data for the WiC task format?

2. The paper evaluates both the GPT API and ChatGPT web interface. What are some key differences between these two methods of accessing the models? What are the tradeoffs between using the API vs the web interface?  

3. The paper tests (Chat)GPT under varying temperature settings. What is the impact of temperature on the model outputs and why does the paper experiment with different values? How does temperature affect deterministic versus creative responses?

4. The prompting strategies (zero-shot, few-shot, many-shot) have very different results on the TempoWiC and HistoWiC tasks. What accounts for these differences? Why would in-context learning be more or less effective for short-term versus long-term semantic change?

5. The paper finds BERT outperforms (Chat)GPT on both tasks. What specifically does the false negative rate analysis show regarding where (Chat)GPT struggles in detecting semantic change compared to BERT? 

6. The message history experiments highlight interesting findings about how conversation context impacts (Chat)GPT's effectiveness over time. How do the fixed and sliding window strategies aim to test these effects? What questions remain about how to optimize message history?

7. The paper evaluates ranking semantic change using the LSC data. Why does (Chat)GPT perform so poorly on this compared to the classification tasks? What does this suggest about the scope of its pre-trained knowledge?

8. The BERT layer analysis shows an interesting divergence between TempoWiC favoring later layers while HistoWiC favors earlier layers. What might account for this and what are the implications for selecting embeddings for semantic change tasks?

9. The paper argues that while (Chat)GPT does not beat BERT, its potential should still be explored further as the models continue to evolve. What are 1-2 promising future directions suggested by the analysis? What enhancements could improve (Chat)GPT's capabilities?

10. What are some key limitations of the study evaluation approach and results? How could the analysis be expanded to provide more robust and generalizable assessments of (Chat)GPT for semantic change detection?
