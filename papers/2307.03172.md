# [Lost in the Middle: How Language Models Use Long Contexts](https://arxiv.org/abs/2307.03172)

## What is the central research question or hypothesis that this paper addresses?

This paper explores how language models use long input contexts when performing downstream tasks that require identifying and using relevant information within the provided context. The central hypothesis is that language models often struggle to effectively leverage long input contexts. In particular, the authors hypothesize that model performance will degrade when relevant information is located in the middle of long input contexts, compared to cases where relevant information is at the beginning or end.To test this, the paper presents controlled experiments on two tasks - multi-document question answering and key-value retrieval - that require accessing relevant information from input contexts. The authors modulate context length by adding more documents/key-value pairs and change the position of relevant info by reordering. The key findings are:- Performance on both tasks shows a U-shaped curve based on position of relevant info, with higher performance when relevant info is at the start/end versus the middle.- Performance decreases as input contexts grow longer, even for extended-context models. - Encoder-decoder models are more robust to position changes for shorter contexts but still struggle on longer sequences.In summary, the central hypothesis is that language models struggle to effectively leverage long contexts, especially when relevant information is in the middle. The controlled experiments on multi-document QA and key-value retrieval aim to test this hypothesis.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contribution is an empirical analysis of how well language models are able to use long input contexts. The authors evaluate various language models on two tasks - multi-document question answering and key-value retrieval - that require identifying and using relevant information within long input texts. They find that model performance tends to be highest when relevant information is located at the very beginning or end of the input context, and degrades significantly when models must access and use information in the middle of long contexts. This demonstrates a "U-shaped" performance curve as the position of relevant information is varied.In addition, the authors show that model performance decreases substantially as the input context grows longer, even for models designed specifically to handle longer contexts. To better understand these results, the paper investigates the effects of model architecture, query-aware contextualization, and instruction tuning. It also includes a case study on open-domain QA to analyze the tradeoff between providing more context and increasing the reasoning load.Overall, the key contribution is a set of controlled experiments and analysis that sheds light on the limitations of current language models in effectively utilizing long contexts, even as models are scaled to handle longer texts. The work introduces new evaluation protocols and benchmarks to measure progress in this area.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key findings from this paper:The paper analyzes how well language models use long input contexts for tasks like question answering and information retrieval, finding that performance degrades when models must access relevant information in the middle of long contexts and as context length increases, even for models designed to handle long contexts.
