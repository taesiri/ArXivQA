# [Lost in the Middle: How Language Models Use Long Contexts](https://arxiv.org/abs/2307.03172)

## What is the central research question or hypothesis that this paper addresses?

This paper explores how language models use long input contexts when performing downstream tasks that require identifying and using relevant information within the provided context. The central hypothesis is that language models often struggle to effectively leverage long input contexts. In particular, the authors hypothesize that model performance will degrade when relevant information is located in the middle of long input contexts, compared to cases where relevant information is at the beginning or end.To test this, the paper presents controlled experiments on two tasks - multi-document question answering and key-value retrieval - that require accessing relevant information from input contexts. The authors modulate context length by adding more documents/key-value pairs and change the position of relevant info by reordering. The key findings are:- Performance on both tasks shows a U-shaped curve based on position of relevant info, with higher performance when relevant info is at the start/end versus the middle.- Performance decreases as input contexts grow longer, even for extended-context models. - Encoder-decoder models are more robust to position changes for shorter contexts but still struggle on longer sequences.In summary, the central hypothesis is that language models struggle to effectively leverage long contexts, especially when relevant information is in the middle. The controlled experiments on multi-document QA and key-value retrieval aim to test this hypothesis.
