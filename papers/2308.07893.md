# [Memory-and-Anticipation Transformer for Online Action Understanding](https://arxiv.org/abs/2308.07893)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is: How can we develop a more effective approach for online action detection and anticipation that better models the full temporal context, including past, present and future actions/events?The key hypothesis is that modeling the circular interdependence between memory (past events) and anticipation (future events) will lead to improved performance on online action detection and anticipation tasks compared to prior memory-based approaches. The paper proposes a new memory-anticipation-based paradigm and model architecture called Memory-and-Anticipation Transformer (MAT) to address this question. MAT aims to capture the complete temporal structure spanning history, present and future through circular interaction between memory and anticipation.In summary, the paper hypothesizes that:- Modeling anticipation can enhance memory representations and vice versa due to their circular dependence. - A memory-anticipation based approach will outperform memory-only based methods on online action detection and anticipation benchmarks.The experiments aim to validate these hypotheses by evaluating MAT on several datasets across both tasks.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contributions appear to be:1. Proposing a new memory-anticipation-based paradigm for online action detection and anticipation. The key idea is to model the circular dependencies between memory (past) and anticipation (future) to capture the entire temporal structure.2. Presenting a unified architecture called Memory-and-Anticipation Transformer (MAT) that can process online action detection and anticipation in a unified manner, spanning both training and inference.3. Demonstrating state-of-the-art performance of MAT on four challenging benchmarks for online action detection and anticipation tasks (TVSeries, THUMOS'14, HDD, and EPIC-Kitchens-100).In summary, the key novelty seems to be introducing the concept of memory-anticipation circular dependence and implementing it in a unified transformer architecture that can handle both online action detection and anticipation effectively. The experimental results validate the benefits of the proposed approach over previous memory-based methods that only model limited historical context.
