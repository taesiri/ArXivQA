# [TRAMS: Training-free Memory Selection for Long-range Language Modeling](https://arxiv.org/abs/2310.15494)

## What is the central research question or hypothesis that this paper addresses?

 The central research question this paper aims to address is how to improve the efficiency and effectiveness of memory usage in long-range transformer language models like Transformer-XL. 

Specifically, the key hypotheses are:

1. There is room for improvement in Transformer-XL's memory utilization - a large portion of the memories are not fully utilized.

2. It is possible to select a smaller subset of high-quality memories in a training-free manner that can lead to better performance compared to using all memories.

3. A simple metric based on the norms and angles of the reformulated keys can be used to identify important memories independent of the queries.

The overall goal is to develop a training-free memory selection technique called TRAMS that can plug into Transformer-XL to select the most useful memories for attention computation. This is expected to reduce the quadratic computation complexity in self-attention while improving model performance on long-range language modeling benchmarks.

In summary, the central hypothesis is that smarter selection of memories can boost model performance without additional training or parameters. TRAMS aims to achieve this via a simple yet effective memory scoring and selection approach.


## What is the main contribution of this paper?

 The main contribution of this paper is proposing TRAMS, a training-free memory selection method for long-range language modeling with Transformers. Specifically:

- They analyze the memory usage in Transformer-XL and find that a large portion of the memories are not fully utilized, motivating exploring better memory selection strategies.

- They reformulate the attention calculation to obtain query and key representations that allow selecting important memories independent of the queries. 

- They propose a simple metric based on the norms and angles of the reformulated keys for selecting valuable memories before computing the attention, without needing extra training.

- They evaluate TRAMS on word-level WikiText-103 and character-level enwik8 datasets by plugging it into Transformer-XL. Results show improved performance without extra parameters, indicating the efficacy of the proposed training-free memory selection.

- Analysis provides insights into the importance of different layers for memory selection, and shows TRAMS improves memory utilization by increasing the average attention probability compared to Transformer-XL.

In summary, the main contribution is developing a simple yet effective training-free method to select valuable memories for attention computation in Transformers, helping alleviate their limitations in long-range dependency modeling. The memory selection mechanism is model-agnostic and could be integrated into other memory-augmented Transformers.
