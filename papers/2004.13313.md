# [Modularized Transfomer-based Ranking Framework](https://arxiv.org/abs/2004.13313)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be:Can we design a modular Transformer architecture for information retrieval that achieves state-of-the-art effectiveness while being faster and more interpretable than existing black-box Transformer rankers?The key hypotheses appear to be:1) Existing Transformer rankers like BERT simultaneously perform representation and interaction when processing a concatenated query-document input. 2) By disentangling representation and interaction into separate modules, we can pre-compute representations offline and keep online computation focused only on the interaction. This can make ranking faster.3) Separating representation and interaction also assigns clear roles to different components, making the model more interpretable. 4) This modular architecture can match the effectiveness of previous monolithic Transformer rankers.To summarize, the central research question is around developing a modular Transformer ranking framework that is efficient, interpretable, and effective compared to prior monolithic approaches. The key hypotheses focus on how decomposing ranking into representation and interaction modules can help achieve these goals.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contributions seem to be:1. Proposing the MORES framework which modularizes the Transformer ranker into separate modules for text representation and interaction. This enables faster ranking by pre-computing representations offline and only doing lightweight interaction online. 2. Demonstrating that MORES can match the effectiveness of state-of-the-art BERT rankers on a large supervised ranking dataset, while being up to 120x faster due to representation reuse.3. Showing via domain adaptation experiments that MORES' modular design does not hurt transfer ability, allowing it to be used in low-resource settings.4. Using the modular design to gain new understanding about Transformer rankers. The experiments reveal differences between adapting representations vs interaction across domains. The attention analysis also sheds light on the dual role of query-document attention - understanding the query and matching query-document tokens.5. Proposing techniques to effectively train and initialize MORES using BERT weights, without needing expensive pre-training.In summary, the main contribution seems to be proposing the interpretable and efficient MORES framework as a replacement for BERT rankers, while retaining effectiveness. The modular design also enables better understanding of Transformer ranking models.


## How does this paper compare to other research in the same field?

Here are some ways this paper compares to other research in natural language processing and information retrieval:- The key contribution is proposing a modularized architecture for Transformer-based ranking models, separating representation and interaction. This is a novel way to structure Transformer rankers compared to prior work like BERT for ranking which uses the standard Transformer encoder architecture. - It shows the benefits of modularization and decomposition for efficiency and interpretability. Other work has explored model efficiency, but this connects efficiency gains directly to the module decomposition. The analysis of separate representation and interaction modules for interpretability purposes is also novel.- It explores different strategies to precompute and reuse document representations for faster query evaluation. Building query-agnostic offline representations is a common technique in IR, but exploring this in the context of Transformer rankers is new.- The adaptation experiments provide insight about domain transferability of different model components. Most prior work looks at adapting full models. Analyzing adaptation of modules separately is an interesting analysis.- Overall, the paper pushes Transformer ranking research in a novel direction of modular model architecture. It connects this decomposition to benefits like efficiency, interpretability, and transferability that prior BERT ranking research did not explore. The analysis and techniques around precomputing representations are also tailored to the proposed modules.In summary, the key novelty is the proposed modular architecture itself. The paper does a nice job motivating it and exploring the unique advantages it provides compared to standard Transformer ranker models. The efficiency, analysis, and adaptation experiments are enabled specifically by the modular decomposition.
