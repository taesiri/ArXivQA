# [Interactive Cartoonization with Controllable Perceptual Factors](https://arxiv.org/abs/2212.09555)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question seems to be: 

How can we enable user control and interactivity in deep learning-based cartoon stylization methods?

The key points are:

- Existing deep cartoon stylization methods like CartoonGAN, AnimeGAN, etc. focus only on end-to-end translation from photo to cartoon style. They do not allow user control over the stylization process. 

- The authors propose a new method called Cartooner that allows control over texture (stroke thickness, abstraction level) and color attributes during cartoon stylization.

- This is achieved by having separate texture and color decoders in the model architecture. The texture decoder has a novel "texture controller" module to adjust stroke and abstraction. The color decoder is trained with a new HSV augmentation strategy.

- Together, these innovations allow users to interactively guide the cartoon stylization by controlling texture and color properties. This improves on prior deep methods that offer no controllability.

- Experiments show Cartooner produces higher quality stylizations than previous methods, while enabling diverse user-guided outputs.

In summary, the key hypothesis is that allowing user interactivity during deep cartoon stylization can improve on prior end-to-end approaches, both in terms of output quality and diversity. The proposed Cartooner method aims to demonstrate this.
