# [PlaSma: Making Small Language Models Better Procedural Knowledge Models   for (Counterfactual) Planning](https://arxiv.org/abs/2305.19472)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be:How can we develop smaller, more efficient language models with strong capabilities for procedural and counterfactual planning?The key points related to this question appear to be:- Procedural planning (decomposing high-level goals into coherent, ordered steps) is an important task but currently relies on large, expensive models. - The authors propose a framework called PlaSma to impart planning abilities to smaller LMs through "symbolic procedural knowledge distillation" and a "verifier-guided decoding algorithm."- They introduce a new task called "counterfactual planning" which requires revising plans to accommodate realistic constrained situations. - Experiments show their approach allows much smaller LMs (100s of millions of parameters vs billions) to match or exceed the performance of larger teacher models on planning tasks.So in summary, the central hypothesis appears to be that with the right training framework, distillation, and inference algorithms, orders of magnitude smaller LMs can be competitive with giant LMs on procedural and counterfactual planning. The paper aims to demonstrate this capability.


## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be:Can smaller language models be endowed with procedural knowledge and (counterfactual) planning capabilities that are comparable to large language models?The key points related to this question are:- The paper introduces an approach called PlaSma that uses symbolic procedural knowledge distillation and an inference-time algorithm to impart planning abilities to small language models. - The authors argue that while large language models (LLMs) show promising performance on procedural planning tasks, their computational costs and issues with reproducibility limit wider adoption.- PlaSma aims to show that much smaller, more accessible models can be competitive with large models on planning through knowledge distillation and tailored decoding.- The paper proposes a novel counterfactual planning task that requires adapting plans to constrained, real-world situations. This tests models' ability to reason about counterfactuals.- Through experiments on planning and counterfactual planning, the authors demonstrate that their distilled small models (770M to 11B parameters) improve over the original large teacher model and approach the performance of models 16x larger.So in summary, the central hypothesis is that smaller, more efficient models can match large model performance on planning tasks when equipped with the right training framework and inference algorithms. The results provide evidence that this is achievable through the proposed PlaSma approach.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contributions appear to be:1. Presenting a novel framework called PlaSma for imparting procedural planning capabilities in small language models, through symbolic procedural knowledge distillation and an inference-time decoding algorithm. 2. Introducing a new task called Counterfactual Planning that involves revising a plan to accommodate realistic counterfactual scenarios and constraints.3. Creating a large-scale procedural planning dataset called CoPlan through knowledge verbalization from large language models. The dataset contains goals, plans, conditions, and counterfactual plans.4. Showing through experiments that much smaller models (770M-11B parameters) distilled using the proposed framework can compete with and often surpass their larger teacher models on procedural planning tasks, including the proposed counterfactual planning.5. Demonstrating the application of the distilled small models to embodied agents through significantly improved planning performance in a simulated household environment compared to prior work using GPT-3.In summary, the main contribution is a novel framework to impart procedural planning abilities in small LMs, which includes multi-task distillation objectives, guided decoding, a new counterfactual planning task, and an accompanying diverse procedural planning dataset. The results show smaller models can achieve comparable or better performance than large models on planning tasks.


## How does this paper compare to other research in the same field?

Here are a few thoughts on how this paper compares to other research in procedural planning:- The focus on using smaller, more accessible models sets it apart from much prior work that relies on large language models like GPT-3. Developing methods to impart planning abilities to smaller models could make the technology more widely usable.- Introducing the novel counterfactual planning and plan revision tasks is an interesting extension beyond the standard goal-based planning setup. Generating plans that can adapt to changed situations is an important direction.- The overall framework of knowledge distillation and inference-time decoding seems quite unique for this task. Many prior approaches formulate planning as a text generation problem and don't incorporate additional structures.- The proposed dataset collection process leverages LLMs in a creative way to obtain a large dataset without expensive human annotation. This could enable scaling up the data for future research.- Evaluation in the VirtualHome simulated environment demonstrates the applicability of the method to embodied agents. This is a useful validation of real-world viability.- In terms of limitations, there isn't much analysis of the diversity or coverage of the collected goals and plans. Ensuring variety is important.Overall, the work introduces valuable innovations in terms of the tasks, models, and methodology. If the promises around accessibility hold up, it could expand applications of AI planning technology. Testing generalization and biases will be important future directions.


## How does this paper compare to other research in the same field?

Here are a few key ways this paper compares to other related research in the field:- The paper introduces a new dataset, CoPlan, for studying procedural planning and counterfactual planning. This provides a valuable new resource for researchers compared to existing planning datasets like ProScript. - The paper proposes a novel approach called PlaSma that uses symbolic procedural knowledge distillation and an inference-time algorithm to impart planning abilities to small language models. This is a unique technique compared to prior work like language model prompting or code generation based methods.  - The paper evaluates both standard planning and counterfactual planning tasks. Introducing and benchmarking counterfactual planning is novel compared to most prior work that focused only on standard goal-based planning.- The paper shows smaller distilled models can match or exceed the performance of much larger models like GPT-3. This demonstrates the value of distillation and the inference algorithm compared to reliance solely on large scale models.  - The paper demonstrates strong performance on an embodied planning environment (VirtualHome), significantly outperforming prior work with GPT-3. This shows the method's promise for real-world robotics applications.In summary, key novelties of this paper compared to related work include the new dataset for planning, the distillation and inference algorithm approach, evaluating counterfactual planning, and strong results beating large models and in embodied settings. The paper makes good progress advancing research on accessible and practical planning with language models.


## What future research directions do the authors suggest?

Based on my reading of the paper, some of the main future research directions suggested by the authors include:- Developing methods to make procedural knowledge models more sample efficient, so they can acquire knowledge from less data. The authors suggest exploring techniques like meta-learning and multi-task learning.- Exploring different verbalization strategies besides few-shot prompting to extract procedural knowledge from large language models. This could help produce higher quality and more diverse data.- Studying how to best transfer procedural knowledge to even smaller and more efficient models like mobile or on-device models. - Extending the counterfactual planning tasks to more complex goal formulations and constraints. This could better approximate real-world applications.- Incorporating additional modalities like vision, for more situated procedural planning and reasoning. This could enable applications like robotic agents.- Developing more realistic environments like simulations to evaluate procedural planning models instead of just language metrics. This could reveal how models might perform in the real world.- Exploring self-supervised techniques so models can continue to acquire procedural knowledge and planning strategies from experience in the environment, instead of relying solely on human annotations.So in summary, some of the key directions involve improving sample efficiency and knowledge quality, transferring to smaller models, extending the counterfactual planning setup, incorporating other modalities, using more realistic environments for evaluation, and enabling more autonomous knowledge acquisition.
