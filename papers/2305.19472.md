# [PlaSma: Making Small Language Models Better Procedural Knowledge Models   for (Counterfactual) Planning](https://arxiv.org/abs/2305.19472)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be:How can we develop smaller, more efficient language models with strong capabilities for procedural and counterfactual planning?The key points related to this question appear to be:- Procedural planning (decomposing high-level goals into coherent, ordered steps) is an important task but currently relies on large, expensive models. - The authors propose a framework called PlaSma to impart planning abilities to smaller LMs through "symbolic procedural knowledge distillation" and a "verifier-guided decoding algorithm."- They introduce a new task called "counterfactual planning" which requires revising plans to accommodate realistic constrained situations. - Experiments show their approach allows much smaller LMs (100s of millions of parameters vs billions) to match or exceed the performance of larger teacher models on planning tasks.So in summary, the central hypothesis appears to be that with the right training framework, distillation, and inference algorithms, orders of magnitude smaller LMs can be competitive with giant LMs on procedural and counterfactual planning. The paper aims to demonstrate this capability.


## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be:Can smaller language models be endowed with procedural knowledge and (counterfactual) planning capabilities that are comparable to large language models?The key points related to this question are:- The paper introduces an approach called PlaSma that uses symbolic procedural knowledge distillation and an inference-time algorithm to impart planning abilities to small language models. - The authors argue that while large language models (LLMs) show promising performance on procedural planning tasks, their computational costs and issues with reproducibility limit wider adoption.- PlaSma aims to show that much smaller, more accessible models can be competitive with large models on planning through knowledge distillation and tailored decoding.- The paper proposes a novel counterfactual planning task that requires adapting plans to constrained, real-world situations. This tests models' ability to reason about counterfactuals.- Through experiments on planning and counterfactual planning, the authors demonstrate that their distilled small models (770M to 11B parameters) improve over the original large teacher model and approach the performance of models 16x larger.So in summary, the central hypothesis is that smaller, more efficient models can match large model performance on planning tasks when equipped with the right training framework and inference algorithms. The results provide evidence that this is achievable through the proposed PlaSma approach.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contributions appear to be:1. Presenting a novel framework called PlaSma for imparting procedural planning capabilities in small language models, through symbolic procedural knowledge distillation and an inference-time decoding algorithm. 2. Introducing a new task called Counterfactual Planning that involves revising a plan to accommodate realistic counterfactual scenarios and constraints.3. Creating a large-scale procedural planning dataset called CoPlan through knowledge verbalization from large language models. The dataset contains goals, plans, conditions, and counterfactual plans.4. Showing through experiments that much smaller models (770M-11B parameters) distilled using the proposed framework can compete with and often surpass their larger teacher models on procedural planning tasks, including the proposed counterfactual planning.5. Demonstrating the application of the distilled small models to embodied agents through significantly improved planning performance in a simulated household environment compared to prior work using GPT-3.In summary, the main contribution is a novel framework to impart procedural planning abilities in small LMs, which includes multi-task distillation objectives, guided decoding, a new counterfactual planning task, and an accompanying diverse procedural planning dataset. The results show smaller models can achieve comparable or better performance than large models on planning tasks.
