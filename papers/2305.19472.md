# [PlaSma: Making Small Language Models Better Procedural Knowledge Models   for (Counterfactual) Planning](https://arxiv.org/abs/2305.19472)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be:How can we develop smaller, more efficient language models with strong capabilities for procedural and counterfactual planning?The key points related to this question appear to be:- Procedural planning (decomposing high-level goals into coherent, ordered steps) is an important task but currently relies on large, expensive models. - The authors propose a framework called PlaSma to impart planning abilities to smaller LMs through "symbolic procedural knowledge distillation" and a "verifier-guided decoding algorithm."- They introduce a new task called "counterfactual planning" which requires revising plans to accommodate realistic constrained situations. - Experiments show their approach allows much smaller LMs (100s of millions of parameters vs billions) to match or exceed the performance of larger teacher models on planning tasks.So in summary, the central hypothesis appears to be that with the right training framework, distillation, and inference algorithms, orders of magnitude smaller LMs can be competitive with giant LMs on procedural and counterfactual planning. The paper aims to demonstrate this capability.


## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be:Can smaller language models be endowed with procedural knowledge and (counterfactual) planning capabilities that are comparable to large language models?The key points related to this question are:- The paper introduces an approach called PlaSma that uses symbolic procedural knowledge distillation and an inference-time algorithm to impart planning abilities to small language models. - The authors argue that while large language models (LLMs) show promising performance on procedural planning tasks, their computational costs and issues with reproducibility limit wider adoption.- PlaSma aims to show that much smaller, more accessible models can be competitive with large models on planning through knowledge distillation and tailored decoding.- The paper proposes a novel counterfactual planning task that requires adapting plans to constrained, real-world situations. This tests models' ability to reason about counterfactuals.- Through experiments on planning and counterfactual planning, the authors demonstrate that their distilled small models (770M to 11B parameters) improve over the original large teacher model and approach the performance of models 16x larger.So in summary, the central hypothesis is that smaller, more efficient models can match large model performance on planning tasks when equipped with the right training framework and inference algorithms. The results provide evidence that this is achievable through the proposed PlaSma approach.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contributions appear to be:1. Presenting a novel framework called PlaSma for imparting procedural planning capabilities in small language models, through symbolic procedural knowledge distillation and an inference-time decoding algorithm. 2. Introducing a new task called Counterfactual Planning that involves revising a plan to accommodate realistic counterfactual scenarios and constraints.3. Creating a large-scale procedural planning dataset called CoPlan through knowledge verbalization from large language models. The dataset contains goals, plans, conditions, and counterfactual plans.4. Showing through experiments that much smaller models (770M-11B parameters) distilled using the proposed framework can compete with and often surpass their larger teacher models on procedural planning tasks, including the proposed counterfactual planning.5. Demonstrating the application of the distilled small models to embodied agents through significantly improved planning performance in a simulated household environment compared to prior work using GPT-3.In summary, the main contribution is a novel framework to impart procedural planning abilities in small LMs, which includes multi-task distillation objectives, guided decoding, a new counterfactual planning task, and an accompanying diverse procedural planning dataset. The results show smaller models can achieve comparable or better performance than large models on planning tasks.


## How does this paper compare to other research in the same field?

Here are a few thoughts on how this paper compares to other research in procedural planning:- The focus on using smaller, more accessible models sets it apart from much prior work that relies on large language models like GPT-3. Developing methods to impart planning abilities to smaller models could make the technology more widely usable.- Introducing the novel counterfactual planning and plan revision tasks is an interesting extension beyond the standard goal-based planning setup. Generating plans that can adapt to changed situations is an important direction.- The overall framework of knowledge distillation and inference-time decoding seems quite unique for this task. Many prior approaches formulate planning as a text generation problem and don't incorporate additional structures.- The proposed dataset collection process leverages LLMs in a creative way to obtain a large dataset without expensive human annotation. This could enable scaling up the data for future research.- Evaluation in the VirtualHome simulated environment demonstrates the applicability of the method to embodied agents. This is a useful validation of real-world viability.- In terms of limitations, there isn't much analysis of the diversity or coverage of the collected goals and plans. Ensuring variety is important.Overall, the work introduces valuable innovations in terms of the tasks, models, and methodology. If the promises around accessibility hold up, it could expand applications of AI planning technology. Testing generalization and biases will be important future directions.


## How does this paper compare to other research in the same field?

Here are a few key ways this paper compares to other related research in the field:- The paper introduces a new dataset, CoPlan, for studying procedural planning and counterfactual planning. This provides a valuable new resource for researchers compared to existing planning datasets like ProScript. - The paper proposes a novel approach called PlaSma that uses symbolic procedural knowledge distillation and an inference-time algorithm to impart planning abilities to small language models. This is a unique technique compared to prior work like language model prompting or code generation based methods.  - The paper evaluates both standard planning and counterfactual planning tasks. Introducing and benchmarking counterfactual planning is novel compared to most prior work that focused only on standard goal-based planning.- The paper shows smaller distilled models can match or exceed the performance of much larger models like GPT-3. This demonstrates the value of distillation and the inference algorithm compared to reliance solely on large scale models.  - The paper demonstrates strong performance on an embodied planning environment (VirtualHome), significantly outperforming prior work with GPT-3. This shows the method's promise for real-world robotics applications.In summary, key novelties of this paper compared to related work include the new dataset for planning, the distillation and inference algorithm approach, evaluating counterfactual planning, and strong results beating large models and in embodied settings. The paper makes good progress advancing research on accessible and practical planning with language models.


## What future research directions do the authors suggest?

Based on my reading of the paper, some of the main future research directions suggested by the authors include:- Developing methods to make procedural knowledge models more sample efficient, so they can acquire knowledge from less data. The authors suggest exploring techniques like meta-learning and multi-task learning.- Exploring different verbalization strategies besides few-shot prompting to extract procedural knowledge from large language models. This could help produce higher quality and more diverse data.- Studying how to best transfer procedural knowledge to even smaller and more efficient models like mobile or on-device models. - Extending the counterfactual planning tasks to more complex goal formulations and constraints. This could better approximate real-world applications.- Incorporating additional modalities like vision, for more situated procedural planning and reasoning. This could enable applications like robotic agents.- Developing more realistic environments like simulations to evaluate procedural planning models instead of just language metrics. This could reveal how models might perform in the real world.- Exploring self-supervised techniques so models can continue to acquire procedural knowledge and planning strategies from experience in the environment, instead of relying solely on human annotations.So in summary, some of the key directions involve improving sample efficiency and knowledge quality, transferring to smaller models, extending the counterfactual planning setup, incorporating other modalities, using more realistic environments for evaluation, and enabling more autonomous knowledge acquisition.


## What future research directions do the authors suggest?

Based on my reading of the paper, some of the main future research directions suggested by the authors include:- Exploring different methods for acquiring procedural knowledge from language models, beyond few-shot prompting. The authors mention reinforcement learning or introspective methods as possible alternatives.- Applying the approach to multimodal settings by incorporating visual affordances and environment semantics. This could improve planning for embodied agents.- Extending the framework to include more diverse and inclusive goals that cover a wider range of cultures and backgrounds. - Developing better evaluation metrics and benchmarks to measure procedural and commonsense reasoning abilities. The authors note limitations with using only automatic metrics.- Combining symbolic knowledge distillation with other methods like introspective reasoning or neuro-symbolic techniques to impart stronger reasoning abilities.- Scaling up the approach to even larger language models and assessing the impact on planning abilities.- Exploring different decoder architectures to enhance the model's causal and temporal reasoning.- Studying how to transfer procedural knowledge to new tasks and domains through meta-learning or lifelong learning approaches.- Applying the method to related reasoning tasks like instruction following, multi-step question answering, process modeling etc.In summary, the main directions involve developing better methods for acquiring and representing procedural knowledge, scaling up the approach, improving evaluation, combining symbolic knowledge with other reasoning techniques, and transferring knowledge to new tasks and settings.


## Summarize the paper in one paragraph.

Here is a one paragraph summary of the paper:The paper proposes a new approach called PlaSma for imparting procedural planning capabilities to small language models. The key ideas are symbolic procedural knowledge distillation and an inference-time decoding algorithm. For knowledge distillation, a large teacher model first generates procedural knowledge in the form of goals, plans, conditions and counterfactual plans. This data is then used to train smaller student models via task-specific and multi-task distillation objectives. To enable more structured reasoning during inference, they incorporate a step-wise verifier into the decoding process to guide beam search towards more coherent and accurate plans. The approach is evaluated on original and proposed counterfactual planning tasks. The results show that much smaller models can effectively compete and often surpass the performance of their larger teacher models and baselines like GPT-3. Overall, the work provides a promising direction for developing smaller yet powerful models for procedural planning through symbolic knowledge transfer and inference-time algorithms.


## Summarize the paper in one paragraph.

Here is a one paragraph summary of the key points from the paper:The paper presents PlaSma, a novel approach to impart procedural planning capabilities in small language models. PlaSma uses symbolic procedural knowledge distillation to enhance the implicit knowledge in small models and an inference-time algorithm to enable more structured reasoning. The authors introduce three procedural planning tasks: goal-based planning, counterfactual planning, and plan revision. For data collection, they use GPT-3's few-shot abilities to generate a large dataset of goals, plans, conditions and counterfactual plans called CoPlan. This dataset is used to train smaller student models via task-specific and multi-task distillation objectives. To improve the coherence and logical sequencing of the generated plans, they incorporate a step-wise verifier into beam search during decoding. Experiments show that much smaller models distilled with PlaSma outperform their larger teacher models in both standard and counterfactual planning settings. The best 11B parameter student matches human performance and is competitive with 175B models on the goal-based planning task. In counterfactual planning, the distilled student achieves 93% validity according to humans. Overall, PlaSma provides an effective approach to impart planning abilities in small, accessible language models through symbolic knowledge distillation and inference-time algorithms.


## Summarize the paper in two paragraphs.

Here is a two paragraph summary of the paper:The paper presents PlaSma, a novel approach to impart procedural planning capabilities in small language models. The authors propose a two-pronged framework combining symbolic procedural knowledge distillation and an inference-time decoding algorithm. For knowledge distillation, they first use a large teacher model to generate a dataset of goals, plans, conditions and counterfactual plans called CoPlan. This dataset is then used to train smaller student models via task-specific and multi-task distillation objectives. To further enhance the students, the authors incorporate a step-wise verifier in beam search during inference to guide the model to generate more coherent and accurate plans. In addition, the paper introduces two new counterfactual planning tasks requiring the model to generate or revise plans based on a condition constraint. Experiments show the smaller distilled models can compete with and often surpass their larger teacher models on both the original and counterfactual planning tasks. The best 11B student model achieves over 90% validity on counterfactual planning based on human evaluation. Overall, the work provides an effective framework to impart procedural planning abilities in smaller, more accessible models.In summary, the key contributions are: (1) a symbolic procedural knowledge distillation pipeline to verbalize and transfer planning knowledge into small models, (2) an inference-time decoding algorithm using a step-wise verifier to improve reasoning (3) introduction of counterfactual planning tasks (4) the CoPlan dataset of goals, plans and conditions (5) strong experimental results showing smaller models can match or exceed larger teacher models in planning abilities. The proposed techniques and dataset provide valuable resources to advance research into commonsense planning with smaller language models.


## Summarize the main method used in the paper in one paragraph.

Here is a one paragraph summary of the main method used in the paper:The paper presents PlaSma, a framework to impart procedural planning capabilities to small language models. PlaSma has two main components: symbolic procedural knowledge distillation, and an inference-time decoding algorithm. For knowledge distillation, the authors first use a large teacher model (GPT-3) to generate a dataset of goals, plans, conditions and counterfactual plans. This dataset, called CoPlan, is then used to fine-tune smaller student models on planning tasks via task-specific and multi-task distillation objectives. To further improve the students' reasoning capabilities, the authors incorporate a step-wise verifier into the decoding process to guide the model to generate more coherent and accurate plans. The verifier scores candidate next steps based on semantic validity. Overall, PlaSma shows that much smaller distilled student models can match or exceed the performance of their larger teacher models and even GPT-3 on procedural planning tasks.


## Summarize the main method used in the paper in one paragraph.

Here is a one paragraph summary of the main method used in the paper:The paper presents PlaSma, a framework for endowing small language models with procedural planning capabilities. The key components of PlaSma are symbolic procedural knowledge distillation and an inference-time decoding algorithm. For knowledge distillation, the authors first use a large teacher model (GPT-3) to generate a dataset of goals, plans, conditions, and counterfactual plans. They filter this dataset to retain only high-quality examples using critic models trained on human annotations. The resulting dataset, CoPlan, is then used to train smaller student models via task-specific and multi-task distillation objectives. To enhance the students' reasoning capabilities, the authors propose a novel verifier-guided step-wise beam search algorithm. This decoding strategy incorporates an independent verifier model that scores candidate next steps based on semantic coherence and temporal validity. The verifier guides the search to favor more logical and complete plans. Experiments demonstrate that the distilled student models outperform the teacher and are competitive with much larger models like GPT-3. The verifier-guided decoding further improves performance across different student sizes. Overall, PlaSma presents an effective approach to impart procedural planning abilities in small, accessible language models.
