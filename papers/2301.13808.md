# [Large Language Models are Versatile Decomposers: Decompose Evidence and   Questions for Table-based Reasoning](https://arxiv.org/abs/2301.13808)

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper addresses is:  How can we effectively enable large language models (LLMs) to perform table-based reasoning by decomposing both the evidence (tables) and questions?Specifically, the paper proposes a method called DATER (Decompose Evidence And Questions for effective Table-based REasoning) to address two key challenges:1) Huge tables with many irrelevant rows/columns interfere with reasoning. The paper decomposes evidence by predicting relevant row/column indexes to extract a small sub-table containing only relevant information.2) Complex questions scatter important information across the question text. The paper decomposes questions into simpler sub-questions by generating intermediate SQL queries as a bridge, avoiding hallucinations. The overall hypothesis is that decomposing both evidence and questions this way will enable more effective table-based reasoning by large language models. Experiments on 3 datasets (including outperforming humans on TabFact) support this hypothesis.


## What is the main contribution of this paper?

The main contributions of this paper are:1. The authors propose to leverage large language models (LLMs) as versatile decomposers to decompose both evidence (tables) and questions for more effective table-based reasoning. 2. For evidence decomposition, they use the LLM to extract a relevant sub-table from a large table by predicting the row and column indexes most relevant to answering the question. This reduces interference from irrelevant table information.3. For question decomposition, they propose a "parsing-execution-filling" strategy to break down complex questions into simpler logical and numerical sub-questions. This involves generating intermediate SQL queries to retrieve reliable information from the table. 4. They evaluate their approach on three benchmark table reasoning datasets - TabFact, WikiTableQuestions, and FetaQA. The proposed method achieves significantly better performance than competitive baselines across all datasets. Notably, it surpasses human performance on TabFact for the first time.5. In addition to strong overall results, their approach also provides some interpretability by generating the sub-table and sub-questions used for reasoning.In summary, the key contribution is using LLMs for interpretable and effective decomposition of evidence and questions to improve complex table-based reasoning. The model achieves new state-of-the-art performance on multiple standard benchmarks.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper: The paper proposes a method called Dater that uses large language models to decompose complex evidence tables and questions into simpler components, in order to improve performance on table-based reasoning tasks like fact verification and question answering.


## How does this paper compare to other research in the same field?

Here are a few key ways this paper compares to other related work in table-based reasoning:- This paper focuses on using large language models (LLMs) for table-based reasoning, while most prior work has relied on fine-tuning task-specific models. The key innovation is using LLMs like Codex in an in-context learning setup without any model tuning.- The paper proposes a new method called Dater that decomposes the evidence table and question into simpler components before reasoning. This makes it easier for LLMs to handle complex multi-step reasoning over large tables. Most prior work does not explicitly decompose in this way.- The decomposition methods are designed specifically for LLMs. For example, the "parsing-execution-filling" strategy for question decomposition uses SQL as an intermediate representation to avoid LLM hallucination issues. - Experiments show Dater outperforms prior published results on TabFact, WikiTableQuestions, and FetaQA benchmarks. Notably it exceeds human performance on TabFact. This demonstrates the power of LLM-based in-context learning for table reasoning.- In addition to strong overall performance, Dater provides some level of interpretability by generating intermediate sub-evidence tables and sub-questions. This contrasts with end-to-end black-box models.In summary, the key novelties are using LLMs for table reasoning, introducing evidence/question decomposition strategies tailored for LLMs, and showing decomposition can make LLMs more effective and interpretable on this task compared to prior published work. The results suggest LLMs have untapped potential for structured reasoning.


## What future research directions do the authors suggest?

Based on my reading of the paper, here are some of the main future research directions suggested by the authors:- Exploring more fine-grained alignment between tables and questions by performing step-by-step reasoning over tables. The current evidence decomposition method extracts indexes of rows/columns as a whole via the LLM without considering the chain-of-thought characteristic of the question. - Extending the approach to other table-based reasoning tasks beyond fact verification and question answering, such as table-to-text generation. The methods could potentially help generate more focused and coherent text.- Applying the approach to other structured data formats beyond tables, such as knowledge graphs, to help decompose complex reasoning over both text and graphs.- Exploring other reliable question decomposition strategies beyond the current SQL-based method. For example, leveraging executable symbolic languages like Prolog.- Evaluating the approach when using even more powerful LLMs beyond Codex, as model scale has been shown to confer stronger reasoning abilities.- Developing methods to automatically generate high-quality prompting examples to avoid costly manual annotation. This could help scale up the approach.- Analyzing the sample efficiency and generalization ability of the approach when using limited prompting examples. More systematic probing is needed.- Conducting human evaluations to assess the interpretability and traceability of the decomposed evidence and questions.In summary, the main future directions are exploring more fine-grained reasoning alignment, extending the approach to new tasks and data types, evaluating larger models, automating prompting generation, and analyzing sample efficiency, generalization ability and interpretability.


## Summarize the paper in one paragraph.

Here is a one paragraph summary of the key points from the paper:The paper proposes a method called Dater for effective table-based reasoning using large language models (LLMs). Dater employs LLMs as versatile decomposers to decompose both the evidence (tables) and questions. First, it reduces huge tables into smaller sub-tables containing only relevant information by predicting related row and column indexes. Second, it decomposes complex questions into simpler sub-questions using a "parsing-execution-filling" strategy, where an intermediate SQL query acts as a bridge to generate reliable numerical and logical sub-questions. Finally, the model reasons over the decomposed sub-evidence and sub-questions to produce the final answer. Experiments on three benchmark datasets show Dater substantially outperforms previous methods, even surpassing human performance on TabFact. A key advantage is Dater provides interpretability by generating interpretable sub-evidence and sub-questions.


## Summarize the paper in two paragraphs.

Here is a two paragraph summary of the paper:This paper proposes a method called Dater for effective table-based reasoning using large language models (LLMs). The key idea is to decompose both the evidence (tables) and questions into simpler components before reasoning over them. For evidence decomposition, the paper utilizes an LLM to extract a relevant sub-table by predicting which rows and columns are needed to answer the question. This reduces interference from irrelevant information in large tables. For question decomposition, the paper introduces a "parsing-execution-filling" strategy that converts questions into SQL queries, executes them to get reliable results, and fills these back into the question decomposition. Finally, the LLM performs reasoning using the simplified evidence and question components. Experiments on three benchmark datasets show Dater substantially outperforms prior methods, even surpassing human performance on the TabFact dataset. A key advantage is interpretability from the decomposed components. Future work includes more fine-grained evidence decomposition based on reasoning chains.


## Summarize the main method used in the paper in one paragraph.

Here is a one paragraph summary of the main method used in the paper:This paper proposes a method called Dater that leverages large language models (LLMs) to decompose both evidence (tables) and questions for effective table-based reasoning. First, it uses the LLM to extract a sub-evidence table by predicting relevant row and column indexes. This filters out irrelevant information from large tables. Second, it decomposes complex questions into simpler sub-questions using a "parsing-execution-filling" strategy: it generates an intermediate SQL query corresponding to the logic of the question, executes this SQL on the table to get results, and fills these results back into the sub-questions. Finally, it performs reasoning over the decomposed sub-evidence and sub-questions using in-context learning with a few examples. Experiments on three datasets show Dater substantially outperforms prior methods, including exceeding human performance on the TabFact dataset. A key advantage is Dater provides interpretability by generating intermediate sub-evidence and sub-questions.
