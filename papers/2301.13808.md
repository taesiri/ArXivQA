# Large Language Models are Versatile Decomposers: Decompose Evidence and   Questions for Table-based Reasoning

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper addresses is:  How can we effectively enable large language models (LLMs) to perform table-based reasoning by decomposing both the evidence (tables) and questions?Specifically, the paper proposes a method called DATER (Decompose Evidence And Questions for effective Table-based REasoning) to address two key challenges:1) Huge tables with many irrelevant rows/columns interfere with reasoning. The paper decomposes evidence by predicting relevant row/column indexes to extract a small sub-table containing only relevant information.2) Complex questions scatter important information across the question text. The paper decomposes questions into simpler sub-questions by generating intermediate SQL queries as a bridge, avoiding hallucinations. The overall hypothesis is that decomposing both evidence and questions this way will enable more effective table-based reasoning by large language models. Experiments on 3 datasets (including outperforming humans on TabFact) support this hypothesis.


## What is the main contribution of this paper?

The main contributions of this paper are:1. The authors propose to leverage large language models (LLMs) as versatile decomposers to decompose both evidence (tables) and questions for more effective table-based reasoning. 2. For evidence decomposition, they use the LLM to extract a relevant sub-table from a large table by predicting the row and column indexes most relevant to answering the question. This reduces interference from irrelevant table information.3. For question decomposition, they propose a "parsing-execution-filling" strategy to break down complex questions into simpler logical and numerical sub-questions. This involves generating intermediate SQL queries to retrieve reliable information from the table. 4. They evaluate their approach on three benchmark table reasoning datasets - TabFact, WikiTableQuestions, and FetaQA. The proposed method achieves significantly better performance than competitive baselines across all datasets. Notably, it surpasses human performance on TabFact for the first time.5. In addition to strong overall results, their approach also provides some interpretability by generating the sub-table and sub-questions used for reasoning.In summary, the key contribution is using LLMs for interpretable and effective decomposition of evidence and questions to improve complex table-based reasoning. The model achieves new state-of-the-art performance on multiple standard benchmarks.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper: The paper proposes a method called Dater that uses large language models to decompose complex evidence tables and questions into simpler components, in order to improve performance on table-based reasoning tasks like fact verification and question answering.
