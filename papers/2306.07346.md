# [Learning to Mask and Permute Visual Tokens for Vision Transformer   Pre-Training](https://arxiv.org/abs/2306.07346)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question seems to be: 

How can we improve upon existing self-supervised pre-training objectives like masked image modeling (MIM) to learn better visual representations for downstream tasks?

The authors motivate this question by identifying some limitations of the prevalent MIM approach, which masks random patches of an image and tries to reconstruct those patches based on the unmasked context. Specifically, MIM introduces input noise during pre-training and assumes independence between reconstructed patches. 

To address these issues, the authors propose a novel pre-training technique called Masked and Permuted Vision Transformer (MaPeT), which combines masking and permutation strategies along with additional position embeddings. The key ideas are:

- Use permuted (rather than masked) predictions to capture inter-patch dependencies in an auto-regressive manner

- Incorporate learnable position embeddings to provide full position information during pre-training, avoiding discrepancies with fine-tuning

- Employ two-stream self-attention to implement the permutation strategy in a vision transformer backbone

So in summary, the central hypothesis is that the proposed MaPeT pre-training approach can learn improved visual representations compared to MIM and other existing methods by overcoming some of their limitations. The experiments aim to validate this hypothesis.


## What is the main contribution of this paper?

 Based on my reading of the paper, the main contributions appear to be:

- Proposing a novel self-supervised pre-training approach called MaPeT (Masked and Permuted Vision Transformer) that combines masked and permuted image modeling strategies. 

- Introducing a permutation-based pre-training objective called Permuted Image Modeling (PIM) that autoregressively predicts permuted image patches while maintaining contextual bi-directionality.

- Presenting Masked and Permuted pre-training for Vision Transformers (MaPeT) that uses PIM to capture intra-patch dependency and takes auxiliary position information as input to ensure the model sees a full sequence of patches at each target position.

- Proposing a new discrete tokenizer called $k$-CLIP that employs discretized CLIP features as visual tokens without requiring training an autoencoder, enabling pre-training without labeled data.

- Conducting experiments under fair and consistent settings to evaluate the proposed method against baselines and competitors. The results demonstrate MaPeT's competitive performance on ImageNet image classification.

In summary, the main contribution appears to be the proposal of a novel pre-training approach MaPeT that combines masked and permuted image modeling along with a new tokenizer $k$-CLIP, and showing through fair experiments that this method achieves strong performance on downstream vision tasks.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the key points from the paper:

The paper proposes a new self-supervised pre-training approach called Masked and Permuted Vision Transformer (MaPeT) that combines masked and permuted image modeling strategies along with learnable masked tokens representing positional information to overcome limitations of standard masked image modeling approaches for vision transformers.
