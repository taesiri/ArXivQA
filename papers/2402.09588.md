# [Emerging Opportunities of Using Large Language Models for Translation   Between Drug Molecules and Indications](https://arxiv.org/abs/2402.09588)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Drug discovery is a costly and time-consuming process. Generating new drug molecules that can treat specific diseases or targets could greatly benefit the drug discovery pipeline and provide better treatments for patients.
- Large language models (LLMs) have shown promise for various NLP tasks by leveraging large amounts of textual data. This paper explores using LLMs for translating between drug molecules (represented as SMILES strings) and drug indications (text descriptions of what conditions the drug treats).  

Proposed Solution:
- The authors propose and evaluate two new tasks: (1) drug-to-indication: generating the indications for a drug given its SMILES string, and (2) indication-to-drug: generating a SMILES string for a drug that treats a set of input indications.
- Several variants of the MolT5 model are tested on these tasks using datasets from ChEMBL and DrugBank. Experiments are done with different model sizes, train/test splits, and fine-tuning approaches.
- A custom tokenizer is also developed to better handle SMILES string inputs by separating them into individual bonds and molecules. A small MolT5 model is pretrained from scratch using this tokenizer.

Key Results:
- Larger MolT5 models generally perform better on both tasks across metrics like BLEU, ROUGE, Text2Mol similarity. Fine-tuning tends to hurt performance.
- Custom tokenizer model shows promise, benefiting from fine-tuning on the drug-to-indication task. Performance on indication-to-drug is more mixed. 
- Overall, translating between SMILES strings and indications remains a challenging task. The signal between drugs and indications is inherently poor. More data and better representation learning could help.

Main Contributions:
- Formulation of two new tasks: drug-to-indication and indication-to-drug translation.
- Thorough evaluation of MolT5 capabilities on these tasks using multiple models, datasets and metrics. 
- Development of a custom tokenizer to better handle SMILES string inputs.
- Analysis of the limitations of existing methods on these tasks and suggestions for future work to build better representations.
