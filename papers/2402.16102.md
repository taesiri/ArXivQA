# [Interpreting Predictive Probabilities: Model Confidence or Human Label   Variation?](https://arxiv.org/abs/2402.16102)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Modern NLP systems output a predictive probability distribution over possible outcomes. 
- This distribution is sometimes interpreted as the system's confidence in its prediction (P1), and sometimes as capturing variability in human annotations (P2).  
- Using the same distribution for both purposes is ambiguous and limiting.

Proposed Solutions:
- P1 (model confidence): Train separate modules to predict model errors, use approximate Bayesian methods, or techniques like conformal prediction. Evaluate via selective prediction.  
- P2 (human variability): Collect diverse annotated data, model subpopulations of annotators, understand sources of label variation. Not all variability is desirable, so also study annotation errors.

Contributions:
- Identify two distinct perspectives (P1 and P2) on the role of predictive probabilities.
- Discuss merits and limitations of each perspective. P1: calibration metrics have issues. P2: modeling human variability is difficult.  
- Recommend best of both worlds: disentangle representations of uncertainty about model confidence vs. human perspectives.
- Highlight tools and exciting directions towards more trustworthy and fairer NLP systems.

In summary, the paper clearly distinguishes two roles of predictive probabilities, argues both contribute to trustworthy and fair AI, and recommends modeling them separately, providing concrete suggestions towards improving NLP systems.
