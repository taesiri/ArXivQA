# [Estimating Unknown Population Sizes Using the Hypergeometric   Distribution](https://arxiv.org/abs/2402.14220)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- The hypergeometric distribution models discrete count data when sampling occurs without replacement from a finite population. It captures the dependence between category counts unlike the multinomial distribution.
- Estimating the parameters of this distribution is important for modeling many real-world scenarios involving finite populations and sparse/under-sampled data. 
- However, there is a gap in existing methods to estimate the distribution when both the total population size and the size of constituent categories are unknown, which is a common problem setting.

Proposed Solution:
- The authors propose a novel method to estimate the unknown hypergeometric population sizes using maximum likelihood. 
- They use a continuous relaxation of the likelihood to enable optimization. The estimates are thresholded to satisfy constraints.
- The method is extended to a VAE framework to model mixtures of hypergeometric distributions conditioned on a latent variable. This handles under-sampled, high-dimensional sparse count data.

Key Contributions:
- First empirical demonstration that jointly estimating total population size and category sizes is tractable, even with severe under-sampling.
- Proposed method outperforms multinomial and Poisson likelihoods in accuracy of estimates and learning useful latent representations.
- Demonstrated applications in NLP (estimating passage complexity) and biology (recovering true gene expression counts in single-cell data).
- Addresses gap in literature regarding hypergeometric parameter estimation. Enables improved modeling of finite, dependent count data.

In summary, the paper proposes a novel hypergeometric likelihood-based method to estimate unknown population sizes from under-sampled count data. Experiments show it accurately recovers parameters and outperforms other likelihoods. The method promises modeling improvements in many real-world scenarios involving dependent discrete data and unknown population sizes.


## Summarize the paper in one sentence.

 This paper proposes a method to estimate the sizes of unknown discrete populations that make up an overall population using the hypergeometric distribution, and demonstrates its effectiveness on problems like inferring latent vocabularies and recovering missing gene transcript counts.


## What is the main contribution of this paper?

 According to the paper, the main contribution is:

1) Proposing a novel method to estimate the unknown population sizes for a mixture of discrete distributions using the hypergeometric likelihood. This addresses a gap in the literature, as previous work has not tackled this estimation challenge in the presence of severe under-sampling.

2) Providing the first empirical demonstration that this inference problem is tractable. Through data simulation, the authors show that the true population sizes can be accurately recovered using maximum likelihood estimation with the hypergeometric likelihood.

3) Successful application of the proposed method to two real-world problems: (i) estimating the complexity of latent vocabularies underlying reading passages in NLP, and (ii) accurately recovering the true number of gene transcripts from sparse, under-sampled single-cell genomics data.

In summary, the key contribution is a new method for estimating unknown population sizes of discrete distributions using the hypergeometric likelihood, with empirical evidence this challenging inference problem is tractable, and demonstration of its usefulness on real-world applications.


## What are the keywords or key terms associated with this paper?

 Based on my review of the paper, some of the key terms and concepts are:

- Hypergeometric distribution - A discrete probability distribution that models sampling without replacement from a finite population divided into categories. Used to model the data generating process.

- Maximum likelihood estimation - Method used to estimate the parameters of the hypergeometric distribution, specifically the unknown total population size and sizes of the constituent categories.

- Variational autoencoder (VAE) - Used to model a mixture of hypergeometric distributions conditioned on a continuous latent variable. Allows information sharing between similar observations.

- Count data - The type of data modeled, consisting of counts of discrete elements/events.

- Under-sampling - A key assumption is that the true distribution is severely under-sampled.

- Applications:
  - NLP (natural language processing) - Used to estimate latent vocabulary size and complexity underlying text passages
  - Single-cell genomics - Used to estimate true gene expression counts from sparse, noisy measurements

- Benchmark likelihoods: Multinomial distribution and Poisson distribution - Compared to the hypergeometric likelihood in experiments.

So in summary, the key focus is using the hypergeometric likelihood in a VAE framework to estimate the parameters of under-sampled finite discrete distributions, with applications to count data from domains like NLP and biology.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in this paper:

1) The paper proposes using a continuous relaxation of the hypergeometric distribution to enable differentiable optimization. What are the tradeoffs of this approximation compared to using the true discrete distribution? Could Gumbel-softmax have been used instead?

2) How does the choice of violation penalty $C_{viol}$ affect the optimization process and final estimates? Was any analysis done to set this hyperparameter?

3) The method seems to struggle more when the true distributions have very similar fraction distributions (Fig 8). What modifications could make the model more robust in this challenging case? 

4) Was any experiment done with $K>3$ categories? How does inference complexity scale with larger $K$?

5) For the reading complexity application, what other NLP tasks could this method of latent vocabulary estimation be useful for? Could it complement language models?

6) The method seems very sample inefficient. What modifications could improve sample efficiency for faster convergence?

7) The variance of estimates seems higher for more severely under-sampled data. How can the model uncertainty be quantified? 

8) The method recovers well-separated mixtures. What changes would allow recovering a continuum of distributions, rather than discrete clusters?

9) The model assumes independence between trials. How could temporal or grouping information be incorporated?

10) What other likelihood functions, such as negative binomial, could be worth exploring for this problem setting? How do the modeling assumptions differ?
