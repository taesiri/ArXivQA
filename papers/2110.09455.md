# [TLDR: Twin Learning for Dimensionality Reduction](https://arxiv.org/abs/2110.09455)

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper addresses is how to design a dimensionality reduction technique that leverages recent advances in self-supervised learning to be more scalable and performant. Specifically, the paper proposes Twin Learning for Dimensionality Reduction (TLDR), a method that aims to learn a lower-dimensional representation that preserves local neighborhoods from a higher-dimensional input space. The key ideas are:1. Using nearest neighbors to define pairs of related points whose proximity should be preserved through the dimensionality reduction mapping.2. Employing the Barlow Twins objective, a recently proposed self-supervised learning loss, to learn the mapping. This loss encourages invariance across the pairs while reducing redundancy in the learned features. 3. Adding a projector head, another concept from self-supervised learning, to map representations to a higher-dimensional space where the loss is computed.The main hypothesis is that by combining these ideas, TLDR can learn better dimensionality reductions than traditional techniques like PCA and other manifold learning methods, while remaining simple and scalable. Experiments on image retrieval and document retrieval tasks support this hypothesis, showing gains over PCA baselines.In summary, the paper explores how ideas from self-supervised visual representation learning can be adapted and modified for the distinct goal of unsupervised dimensionality reduction across generic data types. The central aim is improving scalability and performance compared to existing manifold learning techniques.
