# [Efficient Online Data Mixing For Language Model Pre-Training](https://arxiv.org/abs/2312.02406)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a high-quality paragraph summarizing the key points of the paper:

This paper proposes a novel method called Online Data Mixing (ODM) for optimizing the data mixing strategy during language model pretraining. ODM formulates the data mixing problem as a multi-armed bandit, where each dataset is viewed as an "arm" and optimized for maximum information gain using the training loss as a reward signal. Unlike prior methods like DoReMi that use static mixing weights, ODM dynamically adapts the sampling distribution over datasets to focus on those that provide the most learning signal. Experiments using a 1B parameter model trained on The Pile dataset demonstrate ODM's efficiency, reaching the final perplexity of the next best method with 19% fewer iterations. ODM also improves performance, lowering perplexity by 4.8% over The Pile weights and boosting accuracy on MMLU by 1.9% over DoReMi. The paper shows ODM is highly efficient, adding only a negligible 0.000007% overhead during pretraining. Overall, ODM provides an effective online approach to data mixing that maximizes information gain while introducing minimal computational burden.
