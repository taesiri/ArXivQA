# [TERA: Self-Supervised Learning of Transformer Encoder Representation for   Speech](https://arxiv.org/abs/2007.06028)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is: How can we design an effective self-supervised learning method to pre-train Transformer encoder models on speech, such that the pre-trained models can learn useful representations for various downstream speech tasks?The key ideas and contributions of the paper are:- Proposes a novel self-supervised speech representation learning method called TERA (Transformer Encoder Representations from Alteration).- Introduces three types of alterations along time, frequency, and magnitude axes to formulate the self-supervised pre-training task. The model learns by reconstructing the original speech from the altered version. - Evaluates TERA extensively on multiple downstream tasks including phoneme classification, keyword spotting, speaker recognition, and speech recognition (ASR). Shows that TERA achieves strong performance compared to previous self-supervised methods.- Studies the effect of different alterations and shows each teaches the model a distinct aspect of speech. The time alteration improves phoneme and ASR performance, frequency alteration improves speaker recognition, and magnitude alteration increases data diversity.- Explores different ways to transfer the pre-trained model to downstream tasks, including feature extraction and fine-tuning.- Analyzes the effect of pre-training data quantity, choice of acoustic features, model sizes, transfer learning between datasets, etc. Provides insights on how different factors affect self-supervised speech pre-training.In summary, the central hypothesis is that the proposed TERA method, which pre-trains Transformers by reconstructing speech altered along time/frequency/magnitude dimensions, can learn effective representations for diverse speech tasks. The paper provides comprehensive experiments and analysis to validate this idea.


## What is the main contribution of this paper?

The main contributions of this paper are:- Proposes a novel self-supervised speech representation learning method called TERA (Transformer Encoder Representations from Alteration). - Introduces three types of alterations for self-supervised pre-training: time alteration, frequency alteration, and magnitude alteration. The model learns by reconstructing the original speech frames from the altered versions. - Evaluates TERA extensively on downstream tasks including phoneme classification, keyword spotting, speaker recognition, and automatic speech recognition (ASR). Shows that TERA improves over baseline features and other self-supervised methods.- Studies the effect of different alterations and finds each teaches the model a distinct characteristic of speech. Time alteration helps with phonetic content, frequency alteration with speaker identity, and magnitude alteration increases diversity.- Explores different ways to transfer the pre-trained model to downstream tasks, including feature extraction and fine-tuning. Finds smaller models better for feature extraction and larger models better for fine-tuning.- Analyzes the effect of pre-training with different amounts of data. Shows some masked reconstruction methods don't benefit from more noisy data while TERA does. - Studies pre-training on different acoustic features and shows it significantly impacts downstream performance.- Demonstrates the transferability of TERA from pre-training dataset (LibriSpeech) to unseen downstream dataset (TIMIT) for both phoneme classification and ASR.In summary, the main contribution is proposing the TERA framework and pre-training scheme, along with extensive experiments analyzing different aspects of self-supervised learning for speech.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper proposes a self-supervised speech representation learning method called TERA, which learns bidirectional representations by reconstructing altered speech inputs where time, frequency, and magnitude information are corrupted.


## How does this paper compare to other research in the same field?

Here are some key ways this paper compares to related work in self-supervised speech representation learning:- Most prior work focuses on reconstruction along the temporal axis only (e.g. autoencoding, masked prediction). This paper proposes reconstruction along 3 orthogonal axes - time, frequency, and magnitude.- Many prior works evaluate on classification tasks only. This paper evaluates on a more comprehensive set of tasks - phone classification, keyword spotting, speaker recognition, and ASR.- This paper explores different ways to incorporate the pre-trained model into downstream tasks - representation extraction and fine-tuning. Most prior work looks at only one approach. - This paper studies the impact of pre-training data amount. It finds masked reconstruction methods like Mockingjay don't always benefit from more unlabeled data, while the proposed TERA does. - This paper explores pre-training with different acoustic features like MFCC, FBANK, fMLLR. It shows the choice of feature impacts performance. Most prior works use only log Mel features.- This paper finds smaller models better for representation extraction and larger models better for fine-tuning. This provides insights on model size choice.- This paper demonstrates the approach works well even with a mismatch between pre-training and downstream datasets. Most prior works assume the datasets are from the same domain.In summary, key contributions compared to prior work are the novel reconstruction along multiple axes, more comprehensive evaluation, analysis of pre-training factors like data, features, model size, and showing robustness to domain shift. The paper provides new insights into self-supervised speech representation learning.
