# [TERA: Self-Supervised Learning of Transformer Encoder Representation for   Speech](https://arxiv.org/abs/2007.06028)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is: How can we design an effective self-supervised learning method to pre-train Transformer encoder models on speech, such that the pre-trained models can learn useful representations for various downstream speech tasks?The key ideas and contributions of the paper are:- Proposes a novel self-supervised speech representation learning method called TERA (Transformer Encoder Representations from Alteration).- Introduces three types of alterations along time, frequency, and magnitude axes to formulate the self-supervised pre-training task. The model learns by reconstructing the original speech from the altered version. - Evaluates TERA extensively on multiple downstream tasks including phoneme classification, keyword spotting, speaker recognition, and speech recognition (ASR). Shows that TERA achieves strong performance compared to previous self-supervised methods.- Studies the effect of different alterations and shows each teaches the model a distinct aspect of speech. The time alteration improves phoneme and ASR performance, frequency alteration improves speaker recognition, and magnitude alteration increases data diversity.- Explores different ways to transfer the pre-trained model to downstream tasks, including feature extraction and fine-tuning.- Analyzes the effect of pre-training data quantity, choice of acoustic features, model sizes, transfer learning between datasets, etc. Provides insights on how different factors affect self-supervised speech pre-training.In summary, the central hypothesis is that the proposed TERA method, which pre-trains Transformers by reconstructing speech altered along time/frequency/magnitude dimensions, can learn effective representations for diverse speech tasks. The paper provides comprehensive experiments and analysis to validate this idea.


## What is the main contribution of this paper?

The main contributions of this paper are:- Proposes a novel self-supervised speech representation learning method called TERA (Transformer Encoder Representations from Alteration). - Introduces three types of alterations for self-supervised pre-training: time alteration, frequency alteration, and magnitude alteration. The model learns by reconstructing the original speech frames from the altered versions. - Evaluates TERA extensively on downstream tasks including phoneme classification, keyword spotting, speaker recognition, and automatic speech recognition (ASR). Shows that TERA improves over baseline features and other self-supervised methods.- Studies the effect of different alterations and finds each teaches the model a distinct characteristic of speech. Time alteration helps with phonetic content, frequency alteration with speaker identity, and magnitude alteration increases diversity.- Explores different ways to transfer the pre-trained model to downstream tasks, including feature extraction and fine-tuning. Finds smaller models better for feature extraction and larger models better for fine-tuning.- Analyzes the effect of pre-training with different amounts of data. Shows some masked reconstruction methods don't benefit from more noisy data while TERA does. - Studies pre-training on different acoustic features and shows it significantly impacts downstream performance.- Demonstrates the transferability of TERA from pre-training dataset (LibriSpeech) to unseen downstream dataset (TIMIT) for both phoneme classification and ASR.In summary, the main contribution is proposing the TERA framework and pre-training scheme, along with extensive experiments analyzing different aspects of self-supervised learning for speech.
