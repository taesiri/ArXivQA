# [TERA: Self-Supervised Learning of Transformer Encoder Representation for   Speech](https://arxiv.org/abs/2007.06028)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is: How can we design an effective self-supervised learning method to pre-train Transformer encoder models on speech, such that the pre-trained models can learn useful representations for various downstream speech tasks?The key ideas and contributions of the paper are:- Proposes a novel self-supervised speech representation learning method called TERA (Transformer Encoder Representations from Alteration).- Introduces three types of alterations along time, frequency, and magnitude axes to formulate the self-supervised pre-training task. The model learns by reconstructing the original speech from the altered version. - Evaluates TERA extensively on multiple downstream tasks including phoneme classification, keyword spotting, speaker recognition, and speech recognition (ASR). Shows that TERA achieves strong performance compared to previous self-supervised methods.- Studies the effect of different alterations and shows each teaches the model a distinct aspect of speech. The time alteration improves phoneme and ASR performance, frequency alteration improves speaker recognition, and magnitude alteration increases data diversity.- Explores different ways to transfer the pre-trained model to downstream tasks, including feature extraction and fine-tuning.- Analyzes the effect of pre-training data quantity, choice of acoustic features, model sizes, transfer learning between datasets, etc. Provides insights on how different factors affect self-supervised speech pre-training.In summary, the central hypothesis is that the proposed TERA method, which pre-trains Transformers by reconstructing speech altered along time/frequency/magnitude dimensions, can learn effective representations for diverse speech tasks. The paper provides comprehensive experiments and analysis to validate this idea.
