# [Second-Order Fine-Tuning without Pain for LLMs:A Hessian Informed   Zeroth-Order Optimizer](https://arxiv.org/abs/2402.15173)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
Fine-tuning large language models (LLMs) is important for adapting them to specific downstream tasks. However, conventional first-order optimizers like Adam have high GPU memory consumption due to backpropagation. Recent works use memory-efficient zeroth-order optimizers like MeZO with just forward passes, but they suffer from slow convergence due to heterogeneity of curvatures across parameters.  

Proposed Solution: 
This paper proposes HiZOO, a diagonal Hessian-informed zeroth order optimizer for fine-tuning LLMs. HiZOO estimates the diagonal Hessian using forward passes to account for heterogeneity of curvatures. Specifically:

1) It estimates the diagonal Hessian by accessing loss values at perturbed parameters based on zeroth-order oracles. This requires only one extra forward pass per step compared to MeZO.

2) The estimated diagonal Hessian is used to scale the parameter updates during gradient descent. This allows faster convergence by adjusting step sizes based on curvatures.

3) Theoretical analysis shows HiZOO can converge to optimum under common assumptions and handles heterogeneity effectively.

Main Contributions:

1) First work to incorporate diagonal Hessian estimation in zeroth-order optimizer for fine-tuning LLMs to address curvature heterogeneity.

2) Requires only one extra forward pass but accelerates convergence by 5x on average over MeZO.

3) Achieves better accuracy than MeZO in most cases, across model types, scales and tasks. For example, reduces training steps by 30x on SST-2 while achieving 3% higher accuracy.

4) Compatible with full-tuning, LoRA and prefix tuning methods. Outperforms MeZO consistently in most configurations.  

5) Visualizations on test functions demonstrate HiZOO's effectiveness in handling heterogeneity of curvatures.

In summary, HiZOO is a principled way to accelerate zeroth-order fine-tuning of LLMs by adjusting for curvature heterogeneity using diagonal Hessian estimation.
