# [$R^{2}$Former: Unified $R$etrieval and $R$eranking Transformer for Place   Recognition](https://arxiv.org/abs/2304.03410)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading, the key points of this paper are:

- Proposes a unified framework for visual place recognition (VPR) that handles both image retrieval and reranking using only transformer models. The framework is called R2Former.

- Shows that vision transformer tokens can serve as effective local features for reranking/local matching, performing comparably or better than CNN local features. 

- Introduces a novel transformer-based reranking module that models feature correlation and attention information to determine if an image pair matches the same location.

- Achieves state-of-the-art VPR performance on major datasets while having significantly faster inference speed and lower memory footprint compared to prior reranking methods like RANSAC.

- Demonstrates the flexibility of the reranking module by integrating it with different CNN and transformer backbones.

The key hypothesis seems to be that transformers and their tokens can be adapted to effectively handle both global retrieval and local reranking for place recognition, challenging the need for separate CNN-based components. The paper provides extensive experiments to validate this hypothesis and the advantages of the unified R2Former framework.
