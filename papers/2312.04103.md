# [Enhancing the Rationale-Input Alignment for Self-explaining   Rationalization](https://arxiv.org/abs/2312.04103)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a summary paragraph of the key points from the paper:

This paper identifies and addresses an important problem called "rationale shift" in the popular self-explaining AI technique called rationalization. Rationalization works by having two neural network models - a generator and a predictor - cooperate in a game to produce explanations. However, the paper shows both empirically and theoretically that the generator can sometimes select rationales (explanations) that semantically deviate from the original input, while the predictor still makes accurate predictions. This causes the predictor to provide misleading feedback to the generator. To address this rationale shift problem, the authors propose a novel rationalization architecture called DAR, which employs a discriminator module pretrained on the full inputs to ensure alignment between the selected rationales and original inputs. Extensive experiments on real-world and synthetic datasets demonstrate that DAR can significantly improve the quality of the rationales compared to prior state-of-the-art methods. Theoretical analysis also supports why the pretrained discriminator enables overcoming the rationale shift issue. By identifying this previously unknown problem and proposing an effective solution, this work makes important progress towards building more transparent and trustworthy AI systems.
