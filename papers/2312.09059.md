# [Auto-Prox: Training-Free Vision Transformer Architecture Search via   Automatic Proxy Discovery](https://arxiv.org/abs/2312.09059)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a high-quality paragraph summarizing the key points of the paper:

This paper proposes Auto-Prox, a novel framework for automatically discovering effective zero-cost proxies to enable efficient training-free vision transformer (ViT) architecture search. A major limitation of existing training-free methods is the reliance on manually designed proxies that lack generalizability across tasks. To address this, the authors first construct a ViT benchmark (ViT-Bench-101) containing diverse architectures and performance metrics across multiple datasets. Leveraging this, they define a comprehensive search space of potential zero-cost proxies, representing candidates as computation graphs over ViT statistics and mathematical operations. A joint correlation metric is introduced as the objective to search for proxies that generalize well across datasets. An evolutionary approach is employed, augmented with an elitism-preservation strategy to prevent deterioration. Experiments demonstrate that Auto-Prox discovers high-quality proxies surpassing state-of-the-art methods in both cross-dataset ranking correlation and downstream model accuracy after architecture search. The method provides an automated and efficient way to find effective proxies for training-free ViT search that transfers well across varying domains.


## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper "Auto-Prox: Training-Free Vision Transformer Architecture Search via Automatic Proxy Discovery":

Problem:
- Designing better Vision Transformer (ViT) architectures is critical for performance, but manual search is impractical given the huge design spaces. Neural Architecture Search (NAS) can automate this but training-based NAS is computationally expensive.  
- Training-free NAS methods use proxies to predict model accuracy without training, but rely on manally designed, dataset-specific proxies limiting generalization.

Solution - Auto-Prox:
- Proposes a framework to automatically search for generic zero-cost proxies that predict ViT accuracy across datasets. 

Key Ideas:
- Constructs ViT-Bench-101 with ViTs and accuracy on multiple datasets to evaluate proxy-accuracy correlation.  
- Defines search space of proxies comprising ViT statistics, mathematical operations and computation graphs.
- Introduces Joint Correlation Metric (JCM) to measure proxy consistency across datasets. Optimizes JCM to improve generalization.
- Employs elitism-preservation strategy to efficiently explore the search space while preventing deterioration.

Main Contributions:
- Proposes a from-scratch automatic framework to discover proxies without manual effort or dataset-specific biases. Improves generalization across datasets.
- Constructs ViT-Bench-101 dataset with 500 ViTs from two search spaces evaluated on four datasets. Enables standardized proxy evaluation.
- Achieves state-of-the-art performance in terms of ranking correlation and outperforms hand-crafted proxies.
- Discovered proxy used for efficient training-free NAS, finding ViTs that achieve high accuracy across datasets.

In summary, the paper makes ViT architecture search more automated, efficient and generic via an automatic proxy discovery framework called Auto-Prox.
