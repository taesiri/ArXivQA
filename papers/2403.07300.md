# [Taming Pre-trained LLMs for Generalised Time Series Forecasting via   Cross-modal Knowledge Distillation](https://arxiv.org/abs/2403.07300)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem: 
Existing methods for time series forecasting usually train models from scratch using limited temporal data, which prevents their generalization. Although recent works have introduced pre-trained language models (LLMs) to alleviate this issue, they typically treat LLMs as well-initialized forecasting models and directly take time series as input, ignoring the inherent modality gap between temporal and textual data. This modality mismatch constrains the ability of LLMs for time series forecasting.

Proposed Solution:
This paper proposes a novel framework called LLaTA that fully exploits the capabilities of LLMs for time series forecasting via cross-modal knowledge distillation. Specifically, it consists of two branches - a temporal modal branch for processing time series, and a textual modal branch that employs the layers from pre-trained LLMs. To bridge the modality gap, temporal tokens are first projected to the same latent space as textual tokens. Then both static and dynamic knowledge in LLMs are transferred to the temporal branch through designed losses and modules:

1) Static knowledge in frozen word embeddings is extracted via principal component analysis and aligned to temporal tokens using cross-attention. This facilitates better representation learning. 

2) Dynamic input-dependent knowledge is migrated through a feature regularization loss and a modal consistency loss to capture contextual relations and output coherence across modalities.

Main Contributions:

1) Proposes a novel cross-modal distillation framework LLaTA that mitigates the temporal-textual modality mismatch and enhances LLM exploitation for time series forecasting.

2) Introduces mechanisms to transfer both static and dynamic knowledge from pre-trained LLMs via well-designed losses and components. 

3) Extensive experiments show state-of-the-art performance on diverse forecasting tasks and significant improvements in generalization ability under few-shot and zero-shot settings.
