# [Learning Embeddings with Centroid Triplet Loss for Object Identification   in Robotic Grasping](https://arxiv.org/abs/2404.06277)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem: 
- Object perception is crucial for applications like robotic bin picking, product verification, multi-order picking, etc. But handling the millions of unique objects in supply chains is challenging for current perception methods.
- Traditional detection methods assume fixed object sets at training time. Changes require expensive retraining. 
- Recent advancements enable zero-shot segmentation but lack scalable object identification to associate segments with object classes.

Proposed Solution:
- Use a generic zero-shot segmentation model like Segment Anything (SAM) to get object segments from images.
- Train an identification backbone with centroid triplet loss (CTL) on a large-scale dataset like ARMBench. CTL allows flexible input sizes by aggregating features to centroids. 
- Match gallery images to query segments using cosine similarity in the CTL feature space for object identification.

Contributions:
- Approach to train identification backbones with CTL on large datasets.
- Evaluation establishing new SOTA on ARMBench object identification.
- Integrated pipeline for unseen object detection combining SAM and identification backbone.
- Evaluation on HOPE showing performance comparable to methods trained with object data.

In summary, they propose an object identification pipeline leveraging recent advances in zero-shot segmentation and the flexibility of the centroid triplet loss. When combined, this enables scalable detection and identification of unseen objects without requiring retraining, advancing the state of the art on the problem of perceiving previously unseen objects.
