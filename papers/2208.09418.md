# [SAFARI: Versatile and Efficient Evaluations for Robustness of   Interpretability](https://arxiv.org/abs/2208.09418)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the main research question seems to be: How can we systematically evaluate the robustness of explanations/interpretations from deep learning models against adversarial perturbations?

The authors highlight two key issues:

1) Explanations can be fooled or manipulated by small adversarial perturbations to the input, even if the model prediction remains unchanged. This indicates a lack of robustness in the explanation methods.

2) There is a lack of comprehensive evaluation metrics and methods to systematically assess the robustness of explanations from different explanation techniques. 

To address these issues, the authors propose:

- Two new evaluation metrics: worst-case interpretation discrepancy and probabilistic interpretation robustness. These provide complementary perspectives.

- Efficient estimation methods for the metrics based on genetic algorithms and subset simulation. These are applicable to diverse explanation methods in a black-box manner.

- Demonstrate the utility of the proposed techniques on tasks like ranking explanation methods by robustness and selecting training schemes to improve robustness.

So in summary, the main research contribution is a comprehensive and versatile evaluation framework to assess the robustness of explanations against adversarial manipulations. This provides a way to benchmark different explanation techniques and select models/training methods that improve robustness.
