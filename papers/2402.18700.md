# [Learning to Compress Prompt in Natural Language Formats](https://arxiv.org/abs/2402.18700)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Large language models (LLMs) have limited ability to process long context inputs, which impairs their efficiency and scalability. 
- Existing works compress prompts into soft prompts, but soft prompts have poor transferability across different LLMs.

Proposed Solution: 
- Propose a framework called "Natural Language Prompt Encapsulation" (Nano-Capsulator) to compress lengthy prompts into shorter natural language "Capsule Prompts".
- Use a semantics preserving loss to retain meaning during compression. 
- Use a reward function with length constraints to optimize utility of compressed prompts.
- Capsule Prompts have concise natural language format for better LLM transferability.

Main Contributions:
- Propose a novel prompt compression framework to reduce prompt lengths by 81.4% into natural language Capsule Prompts.
- Capsule Prompts decrease inference latency by up to 4.5x and cut costs by 80.1% while preserving accuracy.  
- Capsule Prompts have strong transferability across diverse LLMs and unseen downstream datasets.
- Framework is broadly applicable to different types of prompts including few-shot chain of thoughts and reading comprehension passages.

In summary, the paper introduces an effective prompt compression framework to encapsulate lengthy contexts into much shorter natural language Capsule Prompts that have better LLM transferability, preserve utility, and enhance efficiency.
