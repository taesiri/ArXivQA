# [Lumen: Unleashing Versatile Vision-Centric Capabilities of Large   Multimodal Models](https://arxiv.org/abs/2403.07304)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
Existing large multimodal models (LMMs) focus on high-level visual content comprehension and reasoning, but lack capabilities for fundamental visual perception tasks like object detection, instance segmentation and pose estimation. Recent works adapt these tasks' outputs to fit the text generation format of LMMs, but this overlooks tasks' intrinsic characteristics and hinders learning of visual perception. 

Proposed Solution:
The paper proposes Lumen, an LMM architecture that decouples task-agnostic and task-specific learning. It has two stages:

1) Task-Agnostic Matching Stage: Unify tasks by reformulating them into a matching problem between instruction text and image regions, outputting a heatmap. The heatmap indicates cross-modal matching probabilities regardless of end task.

2) Task-Specific Decoding Stage: Route the shared heatmap representation to lightweight specialized decoders to generate task outputs (boxes, masks, points, etc.). The routing is based on a task token predicted in stage 1.

By concentrating firstly on multimodal comprehension then specialized decoding, Lumen promotes learning of visual perception capabilities.

Main Contributions:
1) Propose Lumen, an LMM architecture that decouples task-agnostic and task-specific learning to enhance vision capabilities.
2) Significantly expand capabilities of LMMs to tasks like detection, segmentation, pose estimation.
3) Outperform previous LMM methods on COCO detection by a large margin. Achieve comparable performance to specialist models on several tasks.
4) Demonstrate excellent generalization ability to unseen datasets and tasks in experiments.

In summary, the paper presents an effective way to unlock vision-centric capabilities in LMMs by decoupled training, outperforming previous works. The high versatility and generalization highlight Lumen's potential.
