# [Tree-Planted Transformers: Large Language Models with Implicit Syntactic   Supervision](https://arxiv.org/abs/2402.12691)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Large language models (LLMs) have achieved great success through scalability on large datasets, but have inefficient training. 
- Syntactic language models (SLMs) can be trained efficiently with syntactic supervision, but have trouble scaling up.
- There is a need to develop a syntactic large language model (SLLM) architecture that combines the scalability of LLMs and the efficiency of SLMs.

Proposed Solution:
- The paper proposes a novel method called "tree-planting" which implicitly plants syntactic trees into the attention weights of transformer LMs. 
- This results in Tree-Planted Transformers (TPTs) which learn syntax on small treebanks via tree-planting, and then scale up on large text corpora with syntactic scaffolding.

Key Contributions:
- Introduction of tree-planting method to implicitly guide transformer LM attention weights to reflect syntactic structures, without needing explicit syntactic supervision.
- Development of TPTs that significantly outperform comparable SLMs on targeted syntactic evaluations, despite lacking explicit supervision.
- Demonstration that tree-planting and TPTs are a promising foundation for developing SLLMs that have both the scalability of LLMs and the efficiency of SLMs.
- Analysis showing single tree-planted head works best, and need to balance prediction and syntax loss, indicating potential for more human-like TPTs.

In summary, the paper proposes an implicit syntax-guiding method to develop TPTs that can efficiently learn and scale-up syntactic knowledge in language models. The results validate tree-planting and TPTs as a way forward for SLLMs.
