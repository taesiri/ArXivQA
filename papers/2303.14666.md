# [Generalization Matters: Loss Minima Flattening via Parameter   Hybridization for Efficient Online Knowledge Distillation](https://arxiv.org/abs/2303.14666)

## What is the central research question or hypothesis that this paper addresses?

 The central research question this paper addresses is how to improve generalization performance in online knowledge distillation (OKD) by promoting flatter loss minima. 

The key hypothesis is that constructing a hybrid-weight model (HWM) via stochastic convex combination of student model parameters and minimizing its training loss can guide the students to converge to flatter loss minima, thus improving generalization.

In summary, the paper proposes a new OKD method called OKDPH that utilizes parameter hybridization to explicitly optimize for flatter minima and better generalization, in contrast to prior OKD methods that focus on designing sophisticated modules or architectures.
