# [A Comprehensive Evaluation of Parameter-Efficient Fine-Tuning on   Software Engineering Tasks](https://arxiv.org/abs/2312.15614)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
As pre-trained models (PTMs) become larger, fully fine-tuning all their parameters on downstream tasks can be very computationally expensive. Parameter-efficient fine-tuning (PEFT) methods have been proposed to address this by freezing most PTM parameters and only fine-tuning a small number of additional parameters. However, there has been no comprehensive evaluation of different PEFT methods on various PTMs for software engineering tasks.

Solution:
This paper conducts an extensive empirical study evaluating 5 popular PEFT methods (Houlsby, Pfeiffer, Parallel, Prefix, LoRA) on 8 PTMs, including both text-only models like T5 and code-specific models like CodeBERT. They are evaluated on 4 representative SE tasks: clone detection, defect detection, code search and code translation. Three key research questions are explored:

1) Effectiveness of applying PEFT to text-only vs code-specific PTMs 
2) Impact of model size on PEFT performance
3) Influence of model architecture on PEFT methods

Both effectiveness (accuracy metrics) and efficiency (GPU usage, training time) are analyzed quantitatively.

Key Findings:
- Parallel PEFT method consistently performs the best overall
- PEFT more effective for transfer learning from text-only PTMs than code PTMs 
- Increasing model size hurts PEFT performance on some models 
- Encoder-decoder models better for PEFT on code search, encoder-only better for defect detection
- PEFT saves 10-30% GPU resources versus full fine-tuning
- PEFT shows potential to reduce training time but not guaranteed

Overall, the extensive experiments provide valuable insights into applying different PEFT methods on various PTMs for SE tasks. The models and code are publicly available to build upon this work.
