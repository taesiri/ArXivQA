# [A Framework for Bilevel Optimization on Riemannian Manifolds](https://arxiv.org/abs/2402.03883)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
The paper considers bilevel optimization problems where both the upper and lower level variables are constrained to lie on Riemannian manifolds. Specifically, the problem has the following form:

\begin{align*}
\min_{x \in \mathcal{M}_x} & \ f(x, y^*(x)) \\
\textrm{s.t. } y^*(x) = \argmin_{y \in \mathcal{M}_y} & \ g(x,y)  
\end{align*}

Here $\mathcal{M}_x, \mathcal{M}_y$ are Riemannian manifolds, $f,g$ are real-valued functions, and $y^*(x)$ represents the optimal solution to the lower level problem for a given $x$. The lower level function $g(x,y)$ is assumed to be geodesically strongly convex in $y$ which ensures a unique optimal $y^*(x)$.

Proposed Solution:
- Derives the Riemannian hypergradient using the implicit function theorem on manifolds. This relies on the invertibility of the Riemannian Hessian of $g$ w.r.t. $y$.
- Proposes 4 strategies to estimate the hypergradient without directly computing the Hessian inverse:
    1. Conjugate gradient method 
    2. Truncated Neumann series
    3. Automatic differentiation
    4. Computing the inverse explicitly (when possible)
- Provides upper bounds on the hypergradient estimation error for each strategy.
- Introduces the Riemannian Hypergradient Descent (RHGD) algorithm to solve the bilevel problem using exponential/retraction updates.
- Analyzes the convergence and complexity of RHGD under different hypergradient approximation schemes.
- Extends the framework and analysis to stochastic bilevel optimization.

Main Contributions:

- First work to study bilevel optimization problems where both upper and lower level variables are constrained on Riemannian manifolds.

- Derives the Riemannian hypergradient and proposes computationally efficient approximation strategies along with error bounds.

- Provides convergence guarantees for the RHGD algorithm using different hypergradient estimators. Matches the complexity of Euclidean algorithms in terms of condition number $\kappa$.

- Generalizes the framework to allow stochastic objectives and retraction mappings.

- Demonstrates the effectiveness of modeling bilevel problems on manifolds through experiments on hyper-representation learning, meta learning and domain adaptation.

In summary, the paper develops a principled optimization framework for solving challenging bilevel problems on Riemannian manifolds with convergence guarantees. The proposed algorithms and analysis provide a foundation for modeling complex bilevel tasks involving manifold constraints.
