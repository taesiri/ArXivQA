# [Efficient Language Model Architectures for Differentially Private   Federated Learning](https://arxiv.org/abs/2403.08100)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Federated learning (FL) trains models on decentralized data located on devices without sharing the data. It has applications like training language models.  
- In FL, SGD is commonly used as the client optimizer due to its efficiency. However, for language models, adaptive optimizers like Adam are preferred centralizedly for better performance.
- So the paper asks: can we modify language models to work well with SGD in FL, getting the benefits of both?

Proposed Solution:
- The paper proposes using scale invariant architectures that are robust to changes in parameter scale during training. 
- For LSTM models, they propose a novel Coupled Input Forget Gate (CIFG) LSTM called SI-CIFG with modified sigmoid and tanh activations to make them scale invariant.
- They also use an existing scale invariant Transformer with modified attention. 
- These models can converge faster with SGD and achieve better performance in FL.

Main Contributions:
- Proposes SI-CIFG, a novel scale invariant LSTM architecture optimized for FL.
- Shows SI-CIFG outperforms standard CIFG significantly in large-scale simulation and production FL experiments.
- Applies existing scale invariant Transformer to FL and shows benefits over standard Transformer.
- Shows the proposed architectures also help when training with differential privacy in FL.
- Demonstrates language models can be optimized for effective SGD training, getting benefits of efficiency without losing adaptive optimizer performance.

In summary, the paper proposes innovative scale invariant architectures that can train language models effectively in FL using SGD, with gains in convergence speed, stability, and performance over standard architectures.
