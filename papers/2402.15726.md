# [CLIPose: Category-Level Object Pose Estimation with Pre-trained   Vision-Language Knowledge](https://arxiv.org/abs/2402.15726)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem: 
Most existing category-level 6D object pose estimation methods rely on learning object category information from limited 3D point cloud data. However, the scale of current 3D datasets is small due to the high cost of 3D data collection and annotation. As a result, the extracted category features may not be sufficiently comprehensive to handle complex real-world scenarios with diverse intra-category variation. 

Proposed Solution:
This paper proposes CLIPose, a novel framework that utilizes a pre-trained vision-language model called CLIP to extract more robust category-specific features by aligning representations from point clouds, images, and texts. Specifically:

1) Input triplets are prepared containing the point cloud, image patch, and text description of an object. 

2) Features from the three modalities are extracted via encoders and aligned in a shared feature space using contrastive learning losses. This allows transferring semantic knowledge from images and texts to enhance point cloud feature learning.

3) CLIP's image encoder is fine-tuned using prompt tuning to make it more sensitive to pose information. Texts are augmented with pose parameters to provide further pose guidance.

4) The point cloud features are used to estimate the 6D pose and reconstruct symmetries. Contrastive estimation loss between image and text representations enhances category feature learning.

Main Contributions:
- First framework to apply CLIP for category-level 6D object pose estimation to leverage knowledge transfer across modalities.

- Aligns representations from point clouds, images and texts via contrastive learning for robust category-specific feature learning.

- Introduces prompt-tuning of CLIP and pose-augmented textual descriptions to make CLIP more pose-sensitive.

- Achieves state-of-the-art performance on REAL275 and CAMERA25 benchmarks while running in real-time. Provides new insights for real open-world pose estimation.
