# [Zero-Shot Cross-Lingual Reranking with Large Language Models for   Low-Resource Languages](https://arxiv.org/abs/2312.16159)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Large language models (LLMs) have shown promise for document reranking tasks, but their effectiveness for low-resource languages is not well studied. 
- Specifically, it's unclear how well LLMs perform cross-lingual reranking between English and African languages compared to monolingual reranking scenarios.

Methodology:
- Use 3 LLMs - RankGPT4, RankGPT3.5, and open-source RankZephyr for listwise reranking experiments.  
- Test on CIRAL benchmark with English queries and passages in 4 African languages.
- Compare cross-lingual reranking, monolingual reranking with document translation to English, and monolingual reranking with LLM-generated query translations.

Key Findings:
- Reranking is most effective when done fully in English using document translations. Up to 9 pt higher NDCG than reranking natively in African languages.
- Cross-lingual reranking is competitive depending on LLM's multilinguality. RankGPT4 does well likely due to better multilingual capabilities.
- RankGPT4 reranking improves when using its own translations vs Google translations, indicating correspondence between translation quality and ranking accuracy.

Main Contributions:
- First comprehensive evaluation of modern LLMs for cross-lingual and monolingual low-resource language reranking.
- Analysis of impact of using LLM's own translations on downstream ranking performance.
- Demonstrates capabilities of LLMs for low-resource IR, but significant room for improvement remains compared to English.
