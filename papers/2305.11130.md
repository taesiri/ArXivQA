# SimOAP: Improve Coherence and Consistency in Persona-based Dialogue   Generation via Over-sampling and Post-evaluation

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper addresses is how to improve coherence and consistency in persona-based dialogue generation. Specifically, the authors aim to develop a simple and effective strategy that can improve coherence and consistency for any pretrained dialogue model without needing to retrain the model or modify its architecture. The key hypothesis is that language models can produce more coherent and consistent responses if they generate and evaluate a large set of candidate responses, rather than just greedily selecting the top response. However, large-scale sampling brings additional computational costs. To address this, the authors propose a two-stage approach called SimOAP:1) Over-sampling stage: Use existing dialogue models to generate a large and diverse set of candidate responses efficiently via distillation and compression techniques.2) Post-evaluation stage: Select the best response from the candidates using multiple metrics to evaluate coherence and consistency.So in summary, the core research question is how to improve coherence and consistency in persona-based dialogue without expensive retraining, through efficient large-scale sampling and effective automatic evaluation. The hypothesis is that language models already have the capability to generate good responses, but standard greedy decoding fails to select them.


## What is the main contribution of this paper?

The main contributions of this paper are:1. The authors verify that high-probability responses generated by dialogue models are not necessarily better than low-probability responses. They show that dialogue models can generate good responses, but these good responses are often not selected. 2. The authors propose a simple two-stage method called SimOAP (over-sampling and post-evaluation) to improve coherence and consistency in persona-based dialogue generation. This method is model-agnostic.3. In the over-sampling stage, they use existing dialogue models to generate a large number of diverse candidate responses. Compressed or distilled models are used to reduce the extra time cost. 4. In the post-evaluation stage, they select the best response from the large set of candidates using multiple well-designed automatic evaluation metrics - TF-IDF for coherence and natural language inference for consistency.5. Through experiments on the PersonaChat dataset, they show that SimOAP improves existing dialogue models and outperforms baseline strategies in both automatic and human evaluations. The method also works well even when using smaller compressed models.In summary, the main contribution is proposing and validating a simple plug-and-play two-stage strategy to improve coherence and consistency in dialogue generation by over-sampling from existing models and applying careful post-evaluation, without the need for retraining models or designing complex objectives.
