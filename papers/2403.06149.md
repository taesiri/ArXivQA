# [Can Large Language Models Automatically Score Proficiency of Written   Essays?](https://arxiv.org/abs/2403.06149)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Automated essay scoring (AES) aims to automatically evaluate the quality of written essays to save time for teachers and provide feedback to students.
- Despite decades of research, there is still room for improvement in AES systems. 
- Recently, large language models (LLMs) like ChatGPT and Llama have shown impressive natural language abilities, but their potential for AES is unknown.

Methodology:
- The authors evaluate ChatGPT and Llama on the ASAP dataset for holistic and trait-based AES.
- They design 4 prompts with increasing elaborations to study the impact of prompt engineering. 
- They compare performance to state-of-the-art models and analyze consistency across prompts.

Key Findings:  
- Prompt engineering significantly impacts performance, but the best prompt depends on the LLM and task.
- ChatGPT demonstrates higher consistency than Llama.  
- Both LLMs lag behind state-of-the-art in terms of scoring accuracy.
- However, LLMs provide meaningful feedback to potentially improve essay quality.

Main Contributions:
- First study to comprehensively evaluate ChatGPT and Llama for automated essay scoring.
- Analysis of impact of prompt engineering strategies. 
- Comparison of scoring accuracy against state-of-the-art models.
- Demonstration of LLMs' potential for generating helpful writing feedback.

The study reveals strengths and weaknesses of LLMs for this task. While scoring accuracy lags behind other models, their linguistic knowledge could be beneficial for providing writing feedback to students.
