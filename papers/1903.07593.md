# [Learning Correspondence from the Cycle-Consistency of Time](https://arxiv.org/abs/1903.07593)

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper addresses is how to learn visual correspondence from raw unlabeled video in a self-supervised manner. The key ideas and hypotheses are:- Visual correspondence is crucial for visual reasoning, but obtaining ground truth supervision is expensive and limited.- There is inherent correspondence between observations in raw video adjacent in time due to the continuity of the visual world. This can provide free supervision through temporal cycle consistency.- By tracking patches backwards and forwards in time in a learned feature space and optimizing for cycle consistency, a model can learn a feature representation useful for establishing correspondences. - The learned representation can support correspondence at various levels, from pixel-level optical flow to mid-level patch correspondence to high-level object tracking, without specific fine-tuning.So in summary, the main hypothesis is that cycle consistency in time is a powerful self-supervised signal that can be used to learn visual correspondence from scratch on raw video data. The key idea is that correspondence requires a feature space that is invariant to transformations; by optimizing cycle consistency the model is encouraged to develop such feature representations.


## What is the main contribution of this paper?

The main contribution of this paper is presenting a self-supervised method to learn visual correspondence from unlabeled video by optimizing for temporal cycle-consistency. Specifically:- The authors propose a method to learn a feature representation by tracking image patches along cycles in time in video. By tracking a patch forward and backward in time and minimizing the inconsistency between start and end points, the model learns features useful for correspondence.- They introduce cycle-consistent tracking losses based on forward-backward tracking that allow learning from longer cycles as well as skip connections to deal with occlusions or drift. - The proposed framework is used to learn a spatial feature representation in a deep convnet for mid-level correspondence. This is evaluated without finetuning on propagation tasks like video object segmentation, human pose estimation, semantic segmentation, and optical flow estimation.- Experiments demonstrate the effectiveness of the learned features on these tasks compared to self-supervised baselines. The method performs competitively with supervised approaches despite being trained only on raw indoor videos without labels or finetuning.In summary, the key contribution is a self-supervised framework to learn visual correspondence from unlabeled video through optimizing cycle-consistency in time, and demonstrating its effectiveness for mid-level feature learning and propagation tasks. The main value is in providing a way to learn representations for correspondence without human annotation.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper proposes a self-supervised method to learn visual correspondence from unlabeled video by using cycle-consistency in time as the supervisory signal.


## How does this paper compare to other research in the same field?

Here are a few key points in comparing this paper to other research on learning visual correspondence from video:- The main novelty is using cycle-consistency in time as a supervisory signal for learning visual representations from raw video in a completely self-supervised manner. Previous self-supervised video representation learning methods like Wang et al. and Pathak et al. relied on off-the-shelf tracking tools, limiting what could be learned. - The idea of exploiting cycle-consistency as a learning signal is inspired by prior work on image alignment, 3D shape matching, etc. However, this paper is the first to employ cycle-consistency across multiple time steps in video.- For the model architecture, the spatial feature encoder uses a standard ResNet backbone, but the differential tracking module during training is deliberately kept simple to encourage the representation to do the heavy lifting. - Experiments demonstrate the acquired representation transfers well to correspondence tasks like video object segmentation, pose propagation, semantic segmentation, and optical flow, without any fine-tuning. This generalization is a key advantage compared to supervised representation learning.- The performance is strong compared to prior self-supervised video representation learning methods. It even exceeds optical flow methods on some metrics despite not being trained on pixel-level objectives. However, there is still a gap compared to fully supervised methods.In summary, this paper makes a nice contribution in advancing self-supervised visual correspondence learning from video by exploiting cycle-consistency over time, and demonstrates strong transferability of the learned representations. The results are a promising step toward learning from the abundant correspondence information in unlabeled video in a scalable manner.
