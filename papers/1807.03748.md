# [Representation Learning with Contrastive Predictive Coding](https://arxiv.org/abs/1807.03748)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is: How can we develop an unsupervised learning approach to extract useful high-level representations from high-dimensional data across different modalities?The key hypotheses are:1) Predicting the future in latent space using powerful autoregressive models encourages the model to capture information that is maximally useful for predicting future samples.2) Using a probabilistic contrastive loss based on noise-contrastive estimation allows the model to focus on capturing mutual information between the target and context while remaining tractable. 3) This approach can learn useful representations on diverse data types including speech, images, text, and reinforcement learning environments.In summary, the central research question is about developing a universal unsupervised learning approach for representation learning, with the key ideas being latent predictive coding and contrastive estimation. The hypothesis is that this method can work well across very different domains.


## What is the main contribution of this paper?

This paper proposes a new unsupervised learning approach called Contrastive Predictive Coding (CPC). The key ideas are:- Encode high-dimensional observations (e.g. images, audio, text) into a compact latent space using a neural network encoder. - Use powerful autoregressive models to summarize the latent representations and predict future latents. - Train the model end-to-end with a probabilistic contrastive loss based on noise contrastive estimation. This induces the model to capture information in the latent space that is useful for predicting future observations.- Show that CPC works well across four diverse domains (speech, images, text, RL environments) by evaluating the learned representations on downstream tasks. CPC achieves state-of-the-art or competitive results compared to previous unsupervised methods.In summary, the main contribution is proposing a universal unsupervised learning framework based on predictive coding and contrastive losses that learns useful representations from high-dimensional data across different modalities. The simplicity and strong performance across domains is a key advantage compared to prior work.
