# [PEEB: Part-based Image Classifiers with an Explainable and Editable   Language Bottleneck](https://arxiv.org/abs/2403.05297)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem: Existing fine-grained image classifiers for domains like bird species suffer from four key limitations: (1) They are inherently black-box models without built-in explainability. (2) Some claim explainability by learning prototypes or concepts but these are not editable or interpretable by humans. (3) Textual concept-based models operate at the image level so it's unclear what visual details match the text. (4) Most models require training images or demonstration images. 

Proposed Solution: The paper proposes PEEB - an explainable and editable fine-grained bird classifier. PEEB expresses the class name into a set of pre-defined textual descriptors of the visual parts of that bird species. It then matches the visual embeddings of detected parts in the input image to the textual embeddings of descriptors in each class to compute a classification score. This allows it to classify images of new species specified only by the textual descriptors, without needing any training images.

Key Contributions:

(1) Shows CLIP classifiers rely heavily on class names - accuracy drops 10x when names are removed or replaced.

(2) Proposes a novel part-based architecture with textual descriptors that outperforms CLIP classifiers by 8-29% on CUB/NABirds/iNat.

(3) Outperforms recent text concept-based classifiers that use pre-defined concepts, highlighting the power of editable domain-specific descriptors.

(4) Achieves state-of-the-art 88.8% accuracy among explainable CUB classifiers using supervised learning, competitive with the best CUB classifiers.

(5) Allows defining new classes textually at test time without re-training. Demonstrates this by editing descriptors of an unseen bluebird species to correctly classify it.

(6) Releases Birds-11K, an unprecedentedly large dataset of 440K images spanning 11K bird species collected from 7 sources and annotated with textual part descriptors automatically generated by GPT-4.


## Summarize the paper in one sentence.

 Here is a one-sentence summary of the paper:

The paper proposes PEEB, an explainable and editable fine-grained image classifier that matches visual parts of images to textual descriptors provided by humans or GPT-4, outperforming CLIP-based and prototype-based classifiers while enabling users to modify class definitions without retraining.


## What is the main contribution of this paper?

 The main contributions of this paper are:

1. It proposes PEEB, an explainable and editable image classifier for fine-grained bird classification. PEEB matches visual embeddings of detected bird parts to textual descriptors provided by humans or GPT-4, enabling it to classify images without needing any training images. This also allows users to edit the textual descriptors to define new classes without retraining.

2. It introduces a new large-scale bird image dataset called Bird-11K with around 290K images spanning 11K species, which is unprecedented in scale and diversity. This facilitates large-scale pre-training for vision-language tasks.

3. Through extensive experiments, the paper shows that PEEB outperforms other explainable methods like concept bottlenecks and prototypical part networks in both zero-shot and supervised settings. It also demonstrates stronger generalization capability compared to prior arts in the conventional zero-shot setting.

4. The paper applies PEEB to dog classification on a newly constructed Dog-140 dataset and shows that it achieves state-of-the-art performance among explainable methods while being comparable to top black-box classifiers.

In summary, the main contribution is an explainable and editable classifier called PEEB that pushes state-of-the-art in fine-grained image classification while enabling human interpretability and editability. The large-scale Bird-11K dataset and demonstration of PEEB's effectiveness across birds and dogs are other key contributions.


## What are the keywords or key terms associated with this paper?

 Based on my review of the paper, here are some of the key terms and concepts associated with it:

- Fine-grained classification
- Bird classification 
- Explainable AI (XAI)
- Editable classifiers
- Textual descriptors
- Part-based classifiers
- Vision-language models
- Natural language bottleneck
- Zero-shot learning
- Generalized zero-shot learning
- Bird-11K dataset
- PEEB (Part-based Explainable and Editable classifier with Language Bottleneck)
- OWL-ViT (Open World Object Detector ViT)
- Matching visual and textual embeddings
- Contrastive learning

The paper proposes PEEB, a novel bird classification model that uses textual descriptors of visual parts to enable explainability and editability without retraining. It relies on OWL-ViT to detect bird parts and match them to descriptive embeddings. The model is pretrained on a large-scale dataset called Bird-11K and demonstrates strong performance on CUB and NABirds datasets, outperforming prior concept-based and part-based classifiers. Key aspects are its interpretability, ability for users to edit class definitions without retraining, and generalization capability.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 potential in-depth questions about the method proposed in this paper:

1. The paper proposes using part-based textual descriptors of birds generated by GPT-4 for fine-grained classification. What are some potential issues with relying on AI-generated descriptors, and how might the authors address these limitations?

2. The PEEB model matches visual part embeddings to textual descriptors for classification. How does this approach for combining vision and language modalities compare to other multimodal fusion techniques? What are the unique advantages?

3. The authors demonstrate PEEB's ability to edit descriptors and define new classes without retraining. What modifications would be needed to scale this capability to adding a large number of new classes? Are there scenarios where retraining might still be necessary?

4. How does the two-stage pre-training approach on the Birds-11K dataset impact what the model learns compared to end-to-end training? What motivated this choice over joint training of all components?

5. The paper introduces the Birds-11K dataset spanning 11,000 bird species. What considerations went into curating such a large-scale, fine-grained dataset? What steps were taken to ensure diversity and balance across different species?  

6. The PEEB model relies on an object detector backbone for localizing bird parts. How does the choice of object detector impact overall performance? Would a specialized bird part detector be better suited?

7. The authors demonstrate PEEB's ability to classify novel bird species in a zero-shot setting. What factors contribute most to its generalization capability? How might performance change as the number of unseen classes increases drastically?

8. How does PEEB, which operates on local part features, compare to global image classification models in terms of sample efficiency and data needs? What differences would you expect in real-world deployment?

9. The paper focuses on fine-grained bird classification, but also shows promising results on dogs. What considerations would be necessary to apply PEEB more broadly to other animal classification domains?

10. The text encoder is kept frozen during PEEB's training while other components get updated. What effect does this choice have? How does it influence what language knowledge gets retained?
