# [Reinforcement Learning from Delayed Observations via World Models](https://arxiv.org/abs/2403.12309)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
Reinforcement learning (RL) algorithms typically assume that agents receive immediate feedback about the effects of their actions. However, in many real-world applications, observations can be delayed due to physical or technological constraints. These delays challenge the fundamental assumption in RL and can significantly impact the performance of algorithms. Prior work has studied delays in fully observable Markov decision processes (MDPs), but delays in partially observable MDPs (POMDPs) have received little attention. POMDPs with delays, where observations provide partial information about latent state transitions, are an important practical setting for applying RL.

Proposed Solution: 
This paper proposes using world models, which have shown success in integrating past observations and learning latent dynamics, to address observation delays in POMDPs. World models learn a latent state representation along with dynamics and observation models. The key insight is that world models reduce delayed POMDPs to delayed MDPs in the latent space. Thus, concepts from delayed MDPs can be combined with world model techniques for POMDPs. 

The authors adapt the model-based RL algorithm Dreamer V3 as a case study. Two main strategies are explored: 1) An extended actor that conditions the policy on the latent state and past actions, converting the problem into an extended MDP, 2) A latent actor that imagines the current latent state and conditions the policy only on that. Additionally, a simpler memoryless actor and an agnostic baseline are evaluated.

Contributions:
- Establish a link between delays in MDPs and POMDPs via world models, allowing novel combinations of techniques
- Propose three methods to incorporate world models into delayed RL 
- Conduct extensive experiments in both vector and visual environments. For the first time, benchmark delayed RL on visual control tasks.
- Demonstrate that world models can effectively handle partial observability under delays, where existing approaches struggle. The extended actor approach improves performance by ~30% over the agnostic baseline.

In summary, this paper pioneers the application of world models to tackle a key practical challenge in RL - observation delays. The proposed techniques offer a simple yet powerful way to achieve delay-robust learning in visually rich and partially observed environments.
