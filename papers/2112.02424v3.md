# [Variational Wasserstein gradient flow](https://arxiv.org/abs/2112.02424v3)

## What is the central research question or hypothesis that this paper addresses?

This paper proposes a scalable numerical algorithm for computing Wasserstein gradient flows for optimization problems defined over probability densities. The key ideas are:1) Using a variational formulation of the objective function leveraging f-divergences. This allows evaluating the objectives in terms of samples without explicit density estimation. 2) Applying the JKO scheme to the variational formulation. Each JKO step involves solving a min-max stochastic optimization problem over a transport map and a dual function parameterized by neural networks.3) Theoretical analysis showing the variational objective satisfies certain properties like moment matching and relations to integral probability metrics.The central hypothesis is that the proposed variational JKO scheme can efficiently compute Wasserstein gradient flows for high-dimensional distributions, avoiding the need for spatial discretization as in traditional methods. This is demonstrated through numerical experiments on sampling, PDEs, and image datasets.In summary, the key novelty is the variational reformulation combined with JKO scheme to develop a scalable and meaningful algorithm for Wasserstein gradient flows applicable to empirical distributions.


## What is the main contribution of this paper?

The main contribution of this paper is developing a scalable numerical algorithm to implement the Wasserstein gradient flow for objective functions in the form of $f$-divergence. The key ideas are:1. Using a variational formulation of the objective function in terms of an approximate $f$-divergence defined over a restricted function class. This allows evaluating the objective using samples without density estimation.2. Applying the JKO scheme on the variational formulation to obtain a sequence of proximal problems involving optimization over transport maps and dual functions. These are solved via iterative updates using stochastic gradient methods. 3. Theoretical analysis showing the variational objective satisfies certain properties like moment matching and stability with respect to perturbations.4. Demonstrating the effectiveness of the proposed method on sampling tasks, PDE solutions, and image generation problems. The method scales well to high dimensions compared to prior works based on input convex neural networks.In summary, the paper develops a practical and scalable way to compute Wasserstein gradient flows by combining variational objectives, JKO scheme, and stochastic optimization with neural network function representations. The method applies broadly to objectives expressible as $f$-divergences.
