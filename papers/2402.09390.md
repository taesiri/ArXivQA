# [HGOT: Hierarchical Graph of Thoughts for Retrieval-Augmented In-Context   Learning in Factuality Evaluation](https://arxiv.org/abs/2402.09390)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key aspects of the paper:

Problem:
- Large language models (LLMs) like ChatGPT suffer from a tendency to "hallucinate" or generate factually incorrect or ungrounded responses. This is a major reliability and trust issue, especially for retrieval-augmented in-context learning where models leverage external information to enhance their responses.

Proposed Solution:
- The paper introduces a Hierarchical Graph of Thoughts (HGOT) framework that constructs a structured, multi-layered graph to allow more organized and efficient sourcing and incorporation of relevant information to reduce hallucinations.

- It utilizes the emergent planning capabilities of LLMs to break down complex questions into smaller, more manageable sub-queries using a divide-and-conquer strategy.

- It improves the self-consistency majority voting method to assess thought quality using citation recall and precision metrics. Answers are weighted based on citation quality of thoughts.

- A scoring mechanism is proposed to evaluate retrieved passages based on frequency of citation, quality of citations, self-consistency confidence, and retrieval module ranking.

Main Contributions:

- Investigates LLM's emergent planning ability for graph construction in retrieval-augmented in-context learning.

- Enhances thought quality assessment by incorporating citation recall and precision metrics. Weights answers by citation quality of thoughts.

- Proposes passage scoring mechanism considering citation frequency/quality, self-consistency confidence, and retrieval ranking.

- Experiments on FEVER, Open-SQuAD and HotPotQA datasets show HGOT outperforms methods like DSP, ReAct, Self-Ask etc. by up to 7%, demonstrating efficacy in improving factuality.

In summary, the proposed HGOT framework structurally organizes sourced information and evaluates quality of thoughts and passages to significantly enhance factuality of LLM responses for retrieval-augmented in-context learning.
