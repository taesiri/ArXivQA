# [GeneCIS: A Benchmark for General Conditional Image Similarity](https://arxiv.org/abs/2306.07969)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is: How can we train and evaluate vision models to flexibly adapt to different notions of image similarity, as specified by explicit textual conditions?The key points are:- Humans understand a wide variety of notions of similarity between images, and can adapt their judgments based on context. However, most current vision models assume a single, fixed notion of similarity. - Prior work has studied conditional similarity in constrained domains like fashion or birds. This paper aims to tackle conditional similarity in general natural images.- The challenges are (1) collecting suitable data to train for diverse notions of similarity and (2) benchmarking models on an open-ended set of similarity conditions.- The paper proposes the GeneCIS benchmark with 4 datasets covering different types of conditional similarity tasks, designed for zero-shot evaluation.- It also proposes a method to automatically mine training data from image-caption datasets, without exhaustive human annotation of similarity notions.- Experiments show their method outperforms baselines on GeneCIS. Performance on GeneCIS is also weakly correlated with ImageNet accuracy, indicating it measures a different capability.In summary, the central hypothesis is that we can train and evaluate vision models to exhibit flexible, general conditional similarity judgments on natural images in a zero-shot manner, using automatically mined training data. The GeneCIS benchmark and data mining method are proposed to study this question.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contributions seem to be:1) Proposing the GeneCIS benchmark for evaluating models on general conditional image similarity. This benchmark contains four diverse datasets that require adapting to different notions of similarity based on a given condition. 2) Showing that powerful vision models like CLIP struggle on GeneCIS when evaluated in a zero-shot manner, suggesting it measures a different capability than standard vision tasks. Also showing performance on GeneCIS is only weakly correlated with ImageNet accuracy.3) Proposing a scalable method to train conditional similarity models by automatically mining information from large image-caption datasets. This avoids the need for exhaustive human annotations of similarity notions.4) Demonstrating their proposed method provides substantial improvements over baselines on GeneCIS and generalizes to unseen retrieval tasks, even surpassing supervised state-of-the-art on MIT-States despite zero-shot evaluation.In summary, the main contributions seem to be 1) proposing a new benchmark and framework for evaluating conditional similarity, 2) showing limitations of existing methods on this benchmark, and 3) developing a scalable solution that provides gains on the benchmark and related tasks. The key ideas seem to be evaluating zero-shot generalization and using abundantly available image-caption data.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper proposes a new benchmark and method for training models to understand diverse notions of image similarity based on textual conditions, showing performance gains over baselines on this and related retrieval tasks.


## How does this paper compare to other research in the same field?

Here are a few thoughts on how this paper compares to other research in the field of conditional image similarity:- The proposed GeneCIS benchmark provides a new way to evaluate models on their ability to adapt to diverse notions of similarity based on textual conditions. Other existing benchmarks like those in compositional learning and composed image retrieval focus on more constrained domains or closed sets of conditions/transformations. GeneCIS covers a broader range of conditions in a zero-shot setting.- The method of mining training data from image-caption datasets is novel compared to prior work that relies on manually specified conditions or annotations for a limited set of notions of similarity. The proposed mining approach is more scalable and provides more diversity in the conditions seen during training. - Evaluating on GeneCIS, the authors show that standard vision-language models like CLIP struggle, suggesting these models have limited flexibility for conditional similarity despite their strong performance on other vision tasks. This highlights that conditional similarity poses a distinct and challenging problem.- Compared to prior conditional similarity networks that are specialized for certain domains or trained in a supervised manner, the proposed model trained via mining is a more general approach applicable to natural images and able to handle a wide variety of conditions not seen during training.- The gains shown on related CIR benchmarks help validate the utility of the proposed training approach. Achieving state-of-the-art on MIT-States without training on that dataset is notable and shows the transferability of the learned conditional similarity functions.In summary, the GeneCIS benchmark and mining-based training approach advance the specific area of conditional similarity in some novel directions compared to existing work, while also highlighting the limitations of current vision-language models for flexible notions of similarity. The proposed ideas help drive progress on an important but relatively less explored problem.


## What future research directions do the authors suggest?

Based on my reading of the paper, some of the main future research directions suggested by the authors include:- Scaling up the amount of training data. The authors show that their performance improves when training on more mined image-caption triplets, suggesting their approach can benefit from larger datasets. They mention the potential to utilize even larger caption datasets like LAION-5B with 5 billion images.- Exploring different modalities for the conditioning information. The paper focuses on using text conditions, but the authors suggest exploring other input modalities like sketches or attribute vectors. - Extending the types of transformations induced by the condition. Most of their mined triplets involve object/scene modifications, but they suggest prompting more complex transformations like viewpoint or illumination changes.- Generalizing to unseen compositions of conditions. Their model is evaluated in a zero-shot setting, but further generalization tests could involve new combinations of conditions at test time.- Scaling model capacity for more complex reasoning. The authors use a standard CLIP model, but suggest exploring larger vision-language architectures.- Learning conditional generation models. The authors point out their work focuses on retrieval, while image generation conditioned on text edits is also an important direction.- Exploring conditional similarity for other vision tasks like detection and segmentation. The authors suggest the ideas could be useful for making existing vision models more flexible.In summary, the main future directions focus on scaling up the data, model capacity, and diversity of conditions and tasks, to advance conditional similarity learning. Evaluating generalization to new compositions at test time is also highlighted.


## Summarize the paper in one paragraph.

Here is a one paragraph summary of the paper:The paper introduces the GeneCIS benchmark for evaluating models on general conditional image similarity. Conditional image similarity refers to assessing how similar two images are given some external condition like a text description. The paper argues that most representation learning methods assume a single fixed notion of similarity, whereas humans understand many dynamic notions of similarity. GeneCIS contains four conditional image retrieval datasets covering different types of similarity conditions. The authors show that powerful CLIP models struggle on GeneCIS and that performance is only weakly correlated with ImageNet accuracy. They propose a method to train conditional similarity models by automatically mining training data from image-caption datasets. Their method provides substantial gains over baselines on GeneCIS. The paper demonstrates the challenges in evaluating and acquiring training data for conditional similarity, and presents a scalable solution that offers promising directions for future work.


## Summarize the paper in two paragraphs.

Here is a two paragraph summary of the key points from the paper:The paper proposes GeneCIS, a new benchmark to evaluate models on their ability to adapt to different notions of image similarity based on textual conditions. GeneCIS contains four datasets that cover different types of conditional similarity tasks, such as focusing on a particular attribute or object, or specifying a change in an attribute or object. The datasets are designed for zero-shot evaluation to test a model's generalization ability. The authors find that current vision-language models like CLIP struggle on GeneCIS and performance is only weakly correlated with ImageNet accuracy, unlike many other vision tasks, highlighting the value of this new benchmark. The authors also propose a method to train models for conditional similarity by automatically mining information from image-caption datasets. Their approach extracts subject-predicate-object relationships from captions and constructs training triplets relating reference and target images via mined conditions. They show this method provides substantial gains over CLIP baselines on GeneCIS. The model also improves over zero-shot baselines on existing composed image retrieval benchmarks like CIRR and MIT-States, even surpassing prior state-of-the-art on MIT-States despite zero-shot evaluation. The gains from increasing training triplets suggest their mining strategy is promising for learning general conditional similarity.


## Summarize the main method used in the paper in one paragraph.

Here is a one paragraph summary of the main method used in the paper:The paper proposes a method for training general conditional image similarity models by automatically mining triplet training data from large-scale image-caption datasets. The method extracts 'Subject' -> 'Predicate' -> 'Object' relationships from the captions using an off-the-shelf scene graph parser. It constructs training triplets by taking an image with a randomly sampled relationship from the dataset as a reference image, then sampling another image with the same subject but different object as the target image. The condition linking the reference and target is defined as the concatenated predicate and object from the target relationship. This allows it to automatically generate diverse conditional similarity training data without exhaustive human annotation. The triplets are used to train a contrastive model with separate encoders for images and text conditions and a learned composition module. The resulting model is evaluated zero-shot on the proposed GeneCIS benchmark for conditional similarity.
