# [GeneCIS: A Benchmark for General Conditional Image Similarity](https://arxiv.org/abs/2306.07969)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is: How can we train and evaluate vision models to flexibly adapt to different notions of image similarity, as specified by explicit textual conditions?The key points are:- Humans understand a wide variety of notions of similarity between images, and can adapt their judgments based on context. However, most current vision models assume a single, fixed notion of similarity. - Prior work has studied conditional similarity in constrained domains like fashion or birds. This paper aims to tackle conditional similarity in general natural images.- The challenges are (1) collecting suitable data to train for diverse notions of similarity and (2) benchmarking models on an open-ended set of similarity conditions.- The paper proposes the GeneCIS benchmark with 4 datasets covering different types of conditional similarity tasks, designed for zero-shot evaluation.- It also proposes a method to automatically mine training data from image-caption datasets, without exhaustive human annotation of similarity notions.- Experiments show their method outperforms baselines on GeneCIS. Performance on GeneCIS is also weakly correlated with ImageNet accuracy, indicating it measures a different capability.In summary, the central hypothesis is that we can train and evaluate vision models to exhibit flexible, general conditional similarity judgments on natural images in a zero-shot manner, using automatically mined training data. The GeneCIS benchmark and data mining method are proposed to study this question.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contributions seem to be:1) Proposing the GeneCIS benchmark for evaluating models on general conditional image similarity. This benchmark contains four diverse datasets that require adapting to different notions of similarity based on a given condition. 2) Showing that powerful vision models like CLIP struggle on GeneCIS when evaluated in a zero-shot manner, suggesting it measures a different capability than standard vision tasks. Also showing performance on GeneCIS is only weakly correlated with ImageNet accuracy.3) Proposing a scalable method to train conditional similarity models by automatically mining information from large image-caption datasets. This avoids the need for exhaustive human annotations of similarity notions.4) Demonstrating their proposed method provides substantial improvements over baselines on GeneCIS and generalizes to unseen retrieval tasks, even surpassing supervised state-of-the-art on MIT-States despite zero-shot evaluation.In summary, the main contributions seem to be 1) proposing a new benchmark and framework for evaluating conditional similarity, 2) showing limitations of existing methods on this benchmark, and 3) developing a scalable solution that provides gains on the benchmark and related tasks. The key ideas seem to be evaluating zero-shot generalization and using abundantly available image-caption data.
