# [GeneCIS: A Benchmark for General Conditional Image Similarity](https://arxiv.org/abs/2306.07969)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is: How can we train and evaluate vision models to flexibly adapt to different notions of image similarity, as specified by explicit textual conditions?The key points are:- Humans understand a wide variety of notions of similarity between images, and can adapt their judgments based on context. However, most current vision models assume a single, fixed notion of similarity. - Prior work has studied conditional similarity in constrained domains like fashion or birds. This paper aims to tackle conditional similarity in general natural images.- The challenges are (1) collecting suitable data to train for diverse notions of similarity and (2) benchmarking models on an open-ended set of similarity conditions.- The paper proposes the GeneCIS benchmark with 4 datasets covering different types of conditional similarity tasks, designed for zero-shot evaluation.- It also proposes a method to automatically mine training data from image-caption datasets, without exhaustive human annotation of similarity notions.- Experiments show their method outperforms baselines on GeneCIS. Performance on GeneCIS is also weakly correlated with ImageNet accuracy, indicating it measures a different capability.In summary, the central hypothesis is that we can train and evaluate vision models to exhibit flexible, general conditional similarity judgments on natural images in a zero-shot manner, using automatically mined training data. The GeneCIS benchmark and data mining method are proposed to study this question.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contributions seem to be:1) Proposing the GeneCIS benchmark for evaluating models on general conditional image similarity. This benchmark contains four diverse datasets that require adapting to different notions of similarity based on a given condition. 2) Showing that powerful vision models like CLIP struggle on GeneCIS when evaluated in a zero-shot manner, suggesting it measures a different capability than standard vision tasks. Also showing performance on GeneCIS is only weakly correlated with ImageNet accuracy.3) Proposing a scalable method to train conditional similarity models by automatically mining information from large image-caption datasets. This avoids the need for exhaustive human annotations of similarity notions.4) Demonstrating their proposed method provides substantial improvements over baselines on GeneCIS and generalizes to unseen retrieval tasks, even surpassing supervised state-of-the-art on MIT-States despite zero-shot evaluation.In summary, the main contributions seem to be 1) proposing a new benchmark and framework for evaluating conditional similarity, 2) showing limitations of existing methods on this benchmark, and 3) developing a scalable solution that provides gains on the benchmark and related tasks. The key ideas seem to be evaluating zero-shot generalization and using abundantly available image-caption data.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper proposes a new benchmark and method for training models to understand diverse notions of image similarity based on textual conditions, showing performance gains over baselines on this and related retrieval tasks.


## How does this paper compare to other research in the same field?

Here are a few thoughts on how this paper compares to other research in the field of conditional image similarity:- The proposed GeneCIS benchmark provides a new way to evaluate models on their ability to adapt to diverse notions of similarity based on textual conditions. Other existing benchmarks like those in compositional learning and composed image retrieval focus on more constrained domains or closed sets of conditions/transformations. GeneCIS covers a broader range of conditions in a zero-shot setting.- The method of mining training data from image-caption datasets is novel compared to prior work that relies on manually specified conditions or annotations for a limited set of notions of similarity. The proposed mining approach is more scalable and provides more diversity in the conditions seen during training. - Evaluating on GeneCIS, the authors show that standard vision-language models like CLIP struggle, suggesting these models have limited flexibility for conditional similarity despite their strong performance on other vision tasks. This highlights that conditional similarity poses a distinct and challenging problem.- Compared to prior conditional similarity networks that are specialized for certain domains or trained in a supervised manner, the proposed model trained via mining is a more general approach applicable to natural images and able to handle a wide variety of conditions not seen during training.- The gains shown on related CIR benchmarks help validate the utility of the proposed training approach. Achieving state-of-the-art on MIT-States without training on that dataset is notable and shows the transferability of the learned conditional similarity functions.In summary, the GeneCIS benchmark and mining-based training approach advance the specific area of conditional similarity in some novel directions compared to existing work, while also highlighting the limitations of current vision-language models for flexible notions of similarity. The proposed ideas help drive progress on an important but relatively less explored problem.


## What future research directions do the authors suggest?

Based on my reading of the paper, some of the main future research directions suggested by the authors include:- Scaling up the amount of training data. The authors show that their performance improves when training on more mined image-caption triplets, suggesting their approach can benefit from larger datasets. They mention the potential to utilize even larger caption datasets like LAION-5B with 5 billion images.- Exploring different modalities for the conditioning information. The paper focuses on using text conditions, but the authors suggest exploring other input modalities like sketches or attribute vectors. - Extending the types of transformations induced by the condition. Most of their mined triplets involve object/scene modifications, but they suggest prompting more complex transformations like viewpoint or illumination changes.- Generalizing to unseen compositions of conditions. Their model is evaluated in a zero-shot setting, but further generalization tests could involve new combinations of conditions at test time.- Scaling model capacity for more complex reasoning. The authors use a standard CLIP model, but suggest exploring larger vision-language architectures.- Learning conditional generation models. The authors point out their work focuses on retrieval, while image generation conditioned on text edits is also an important direction.- Exploring conditional similarity for other vision tasks like detection and segmentation. The authors suggest the ideas could be useful for making existing vision models more flexible.In summary, the main future directions focus on scaling up the data, model capacity, and diversity of conditions and tasks, to advance conditional similarity learning. Evaluating generalization to new compositions at test time is also highlighted.
