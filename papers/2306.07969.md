# [GeneCIS: A Benchmark for General Conditional Image Similarity](https://arxiv.org/abs/2306.07969)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is: How can we train and evaluate vision models to flexibly adapt to different notions of image similarity, as specified by explicit textual conditions?The key points are:- Humans understand a wide variety of notions of similarity between images, and can adapt their judgments based on context. However, most current vision models assume a single, fixed notion of similarity. - Prior work has studied conditional similarity in constrained domains like fashion or birds. This paper aims to tackle conditional similarity in general natural images.- The challenges are (1) collecting suitable data to train for diverse notions of similarity and (2) benchmarking models on an open-ended set of similarity conditions.- The paper proposes the GeneCIS benchmark with 4 datasets covering different types of conditional similarity tasks, designed for zero-shot evaluation.- It also proposes a method to automatically mine training data from image-caption datasets, without exhaustive human annotation of similarity notions.- Experiments show their method outperforms baselines on GeneCIS. Performance on GeneCIS is also weakly correlated with ImageNet accuracy, indicating it measures a different capability.In summary, the central hypothesis is that we can train and evaluate vision models to exhibit flexible, general conditional similarity judgments on natural images in a zero-shot manner, using automatically mined training data. The GeneCIS benchmark and data mining method are proposed to study this question.
