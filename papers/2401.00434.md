# [GeoGalactica: A Scientific Large Language Model in Geoscience](https://arxiv.org/abs/2401.00434)

## What is the main contribution of this paper?

 This paper presents GeoGalactica, a 30 billion parameter language model specialized for the geoscience domain. The main contributions include:

1. Construction of a large geoscience text corpus (GeoCorpus) containing 65 billion tokens, which is claimed to be the largest geoscience-specific corpus. This was used to further pre-train the Galactica model.

2. Creation of GeoSignal v2, an expanded instruction tuning dataset with 1 million samples for supervised fine-tuning. This dataset focuses on geoscience knowledge and reasoning. 

3. Development of data cleaning and preparation pipelines for converting raw geoscience data (PDFs, etc.) into a format suitable for model training. These tools are open-sourced.

4. Further pre-training and supervised fine-tuning of the 30B parameter Galactica model using the geoscience data. This resulted in the GeoGalactica model which outperforms prior geoscience LLMs.

5. Extensive quantitative evaluation using GeoBench and qualitative human evaluation demonstrating GeoGalactica's state-of-the-art performance on various geoscience NLP tasks.

6. Release of the GeoGalactica pre-training checkpoints and training details to promote research into specialized scientific language models.

In summary, the key innovation is the construction of a very large domain-specific model and data ecosystem to significantly advance the capabilities of LLMs for geoscience applications.
