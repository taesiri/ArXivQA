# [EERO: Early Exit with Reject Option for Efficient Classification with   limited budget](https://arxiv.org/abs/2402.03779)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Increasing complexity of deep learning models leads to high computational costs during inference. Methods are needed to reduce these costs while maintaining accuracy.
- Early exit strategies place auxiliary prediction heads throughout a network allowing easier instances to exit early. But determining optimal exit points is challenging. 

Proposed Solution: 
- The paper frames early exiting as a classification with reject option problem. This allows formulating an optimal reject rule to decide when an instance should exit at an auxiliary head.
- For a single auxiliary head, the reject rule is based on a threshold on the head's prediction confidence to control accuracy and budget tradeoff. 
- For multiple heads, an aggregation procedure with exponential weights is proposed to optimize joint empirical risk under a global budget constraint.

Key Contributions:
- Proposes EERO method to translate early exiting to reject option classification and derive optimal rules.
- Provides theoretical analysis for single head case including data-driven threshold calibration and budget control.
- Extends approach to multiple heads using aggregation for risk minimization under budget constraints.
- Demonstrates EERO on ResNet, ConvNext and MSDNet architectures over CIFAR and ImageNet, showing accuracy improvements and strict budget adherence. 

In summary, the paper makes significant contributions in reconceptualizing early exit as a reject option classification problem. This enables formulating optimal decision rules for single and multiple exit points to enhance efficiency while meeting budget constraints. Experiments validate benefits across diverse model architectures.
