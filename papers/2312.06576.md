# [HyPE-GT: where Graph Transformers meet Hyperbolic Positional Encodings](https://arxiv.org/abs/2312.06576)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a high-quality paragraph summarizing the key points of the paper:

This paper proposes a novel framework called HyPE-GT for generating positional encodings (PEs) in hyperbolic space to enhance graph transformers. The framework allows creating multiple categories of PEs by selecting different combinations of modules for PE initialization (Laplacian or random walk PEs), manifold type (hyperboloid or Poincare ball), and hyperbolic network for learning PEs (HNN or HGCN). This provides practitioners with diverse PE options to optimize for different tasks. The PEs are combined with standard graph transformer layers to incorporate positional information. Additionally, the framework mitigates over-smoothing in deep GNNs by re-purposing the PEs to augment intermediate node representations. Comprehensive experiments on molecular, co-author, and co-purchase benchmarks demonstrate the effectiveness of hyperbolic PEs in boosting deep GNN performance. Key results show HyPE-GT achieving top accuracy on multiple datasets compared to state-of-the-art methods. The framework also significantly enhances deep GCNs, JK-Nets and GCNIIs across different depths. Overall, the paper presents a highly flexible and performant approach to learn task-optimized PEs.
