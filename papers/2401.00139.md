# [Is Knowledge All Large Language Models Needed for Causal Reasoning?](https://arxiv.org/abs/2401.00139)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Large language models (LLMs) like ChatGPT have shown remarkable capabilities across various tasks, but their potential for causal reasoning remains unclear. 
- Prior works have evaluated LLMs' causal abilities but suffer from either overly simple or complex evaluation settings that do not reveal their true capacities.  
- It is unknown whether LLMs rely more on the contextual knowledge provided or their inherent knowledge when making causal inferences.

Proposed Solution:  
- The paper introduces a causal attribution model to quantify the influence of contextual knowledge versus inherent knowledge on LLM causal reasoning using "do-calculus".
- Counterfactual scenarios are generated by manipulating the input components (context, embedded knowledge in variables, numerical data) to estimate their attribution scores. 
- Experiments are designed to systematically remove knowledge or data to evaluate LLMs' reasoning capacities in their absence.

Main Contributions:
- Finding 1: LLMs strongly rely on domain knowledge for accurate causal reasoning, reflected in high attribution scores for knowledge. With adequate knowledge, they demonstrate sound causal logic.  
- Finding 2: Numerical data plays a more limited role. LLMs show basic but flawed reasoning without knowledge, incorrectly relating correlation to causation.
- The method allows comprehensive comparison of different LLMs' reasoning capacities across datasets. It also enables analysis of issues like order bias.
- Overall, the paper supports that knowledge is critical for LLM causal reasoning through a robust evaluation approach. The framework has broad utility for interpretable and trustworthy LLMs.

In summary, the paper tackles the open question of how LLMs perform causal reasoning via a principled attribution model and experimental design. The key insight is LLMs' reliance on domain knowledge for accurate causal inferences, while data alone leads to limitations. The approach facilitates detailed analyses to uncover strengths and issues in LLMs' reasoning processes.
