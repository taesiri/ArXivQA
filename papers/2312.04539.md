# [Self-Guided Open-Vocabulary Semantic Segmentation](https://arxiv.org/abs/2312.04539)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a detailed paragraph summarizing the key points of the paper:

This paper proposes a novel self-guided open-vocabulary semantic segmentation framework called Self-Seg. The goal is to automatically determine relevant objects in an image and segment them at the pixel level, without requiring any predefined classes or textual input from the user. The core of Self-Seg is a module called BLIP-Cluster-Caption (BCC), which clusters local BLIP embeddings from an image to identify semantic regions, captions each region to generate noun phrases describing objects, and passes these nouns as class names to an off-the-shelf open-vocabulary segmentor like X-Decoder. BCC allows creating segmentation masks and discovering relevant classes in a self-guided manner. The paper also contributes an LLM-based Open-Vocabulary Evaluator (LOVE) to map predicted open-vocabulary classes to dataset classes for evaluation. Experiments on PASCAL VOC, ADE20K and Cityscapes demonstrate state-of-the-art performance on self-guided segmentation, and competitive results compared to methods that require ground truth classes. Key advantages are the ability to exhaustively identify visual concepts including rare objects, the model-agnostic nature via modularity, and requiring no finetuning. Limitations include occasional incorrect or overly generic captions and competition between related classes. Overall, Self-Seg advances self-supervised segmentation and reasoning of visual scenes.
