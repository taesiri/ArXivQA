# [Multi-modal Attribute Prompting for Vision-Language Models](https://arxiv.org/abs/2403.00219)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper "Multi-modal Attribute Prompting for Vision-Language Models":

Problem:
- Large pre-trained vision-language models (VLMs) like CLIP struggle in few-shot scenarios due to relying solely on global image and text features, overlooking multi-modal attribute characteristics. 
- This limits the model's ability to perceive fine-grained visual details and generalize to unseen classes.

Proposed Solution: 
- The paper proposes a Multi-modal Attribute Prompting (MAP) method by jointly exploring textual attribute prompting, visual attribute prompting and attribute-level alignment.

Key Contributions:

1. Textual Attribute Prompting:
- Generate class-specific textual descriptions using GPT-3.5 language model as queries. 
- Construct multiple textual attribute prompts for each class by combining context words, class name and attribute descriptions.

2. Visual Attribute Prompting: 
- Introduce learnable visual attribute prompts to interact with image tokens and aggregate regional features.
- Propose Adaptive Visual Attribute Enhancement (AVAE) module to refine initial visual prompts using selected textual prompts as guidance.  

3. Attribute-Level Alignment:
- Formulate alignment between visual and textual attribute prompts as an Optimal Transport problem.
- Achieve precise alignment to handle disruptions in images.

- The integration of textual prompts, visual prompts and attribute alignment provides multi-level robust alignment between images and text.

- Extensive experiments show state-of-the-art performance on 11 datasets for few-shot classification, base-to-novel generalization, domain generalization and cross-dataset evaluation.
