# [Are LLMs Good Cryptic Crossword Solvers?](https://arxiv.org/abs/2403.12094)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Cryptic crosswords involve clever wordplay and are challenging even for modern NLP models, with previous works reporting low accuracy (less than 10%) compared to human performance. 
- The abilities of large language models (LLMs) on solving cryptic crosswords have not been tested before. This paper aims to benchmark the performance of popular LLMs on this task.

Methodology:
- The authors evaluated 3 LLMs - LLaMA, Mistral (open-source models), and ChatGPT (closed-source) on the task of solving cryptic clues.
- They tested the models under different settings: zero-shot learning using different prompt formulations, few-shot learning using 3/10 random or indicator examples, and fine-tuning the open-source models on the task using various train/test splits.
- Two datasets from previous works were used: one from The Guardian and another called Cryptonite. Care was taken to create test sets that avoid answer memorization by models.

Key Results:
- Providing elaborate prompts did not help the models much; few-shot learning brings small improvements.
- Mistral outperformed LLaMA on average across different settings. ChatGPT also did better than open-source models.
- Models struggled with constrained test sets designed to avoid memorization, showing their limited reasoning abilities.
- Fine-tuning helped to an extent, but models are still far below human-level performance.

Main Contributions:
- Established benchmark results for several popular LLMs on the challenging cryptic crossword task under various settings. 
- Created additional constrained test sets that require deeper language understanding.
- Showed that despite recent progress, cracking cryptic puzzles automatically still remains an open challenge for LLMs.

Limitations are the small set of models tested, English-only experiments, and constrained prompting approaches. Future work may explore better prompting techniques, curriculum learning for LLMs, and modular architectures to improve solving different types of wordplay.
