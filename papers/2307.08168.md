# [Feedback is All You Need: Real-World Reinforcement Learning with   Approximate Physics-Based Models](https://arxiv.org/abs/2307.08168)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be: How can we efficiently and reliably optimize control policies for real-world robotic systems using limited amounts of real-world data?The key hypothesis appears to be:By systematically exploiting an approximate physics-based model to simultaneously design 1) a policy gradient estimator and 2) a stabilizing low-level feedback controller embedded within the policy class, we can overcome key challenges in policy optimization and learn high-performance policies using limited real-world data.Specifically, the paper proposes using the model derivatives to get efficient policy gradient estimates, while the embedded low-level controller designed with the model prevents errors in these estimates from exploding over long horizons. This allows the method to efficiently learn from limited real-world data despite potential inaccuracies in the model.The experiments then aim to validate whether this approach can efficiently learn control policies on real robotic hardware using only a few minutes of real-world data, even when the model has substantial inaccuracies or mismatches from the true system dynamics.In summary, the central hypothesis is that by carefully leveraging even an approximate physical model, the proposed method can enable reliable and data-efficient policy optimization on real-world robotic systems. The experiments aim to validate this hypothesis.
