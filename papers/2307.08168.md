# [Feedback is All You Need: Real-World Reinforcement Learning with   Approximate Physics-Based Models](https://arxiv.org/abs/2307.08168)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be: How can we efficiently and reliably optimize control policies for real-world robotic systems using limited amounts of real-world data?The key hypothesis appears to be:By systematically exploiting an approximate physics-based model to simultaneously design 1) a policy gradient estimator and 2) a stabilizing low-level feedback controller embedded within the policy class, we can overcome key challenges in policy optimization and learn high-performance policies using limited real-world data.Specifically, the paper proposes using the model derivatives to get efficient policy gradient estimates, while the embedded low-level controller designed with the model prevents errors in these estimates from exploding over long horizons. This allows the method to efficiently learn from limited real-world data despite potential inaccuracies in the model.The experiments then aim to validate whether this approach can efficiently learn control policies on real robotic hardware using only a few minutes of real-world data, even when the model has substantial inaccuracies or mismatches from the true system dynamics.In summary, the central hypothesis is that by carefully leveraging even an approximate physical model, the proposed method can enable reliable and data-efficient policy optimization on real-world robotic systems. The experiments aim to validate this hypothesis.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contributions are:1) The paper introduces a novel policy gradient-based framework for efficiently training robot control policies with limited real-world data. The key ideas are leveraging an approximate physics-based model to construct both the policy gradient estimator and a low-level stabilizing controller embedded within the policy architecture. 2) Theoretical analysis provides insight into how the proposed approach helps overcome key challenges with standard policy gradient methods, including exploding gradients and ill-conditioning. The analysis shows how the presence of stabilizing low-level feedback improves the smoothness properties and reduces the variance of the optimization problem.3) Hardware experiments demonstrate the approach can reliably learn high-performance control policies for a small RC car and a quadruped robot using only a few minutes of real-world data. This showcases the method's ability to overcome substantial model inaccuracies and learn effectively despite limited real-world interaction.In summary, the main contribution is a systematic framework for efficiently and reliably training policies on physical systems by exploiting simple models and embedded feedback, with theoretical and empirical validation. The proposed techniques help enable direct policy learning on robotic hardware despite imperfect modeling and limited data.
