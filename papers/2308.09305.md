# [Human Part-wise 3D Motion Context Learning for Sign Language Recognition](https://arxiv.org/abs/2308.09305)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be: How can we improve sign language recognition by learning part-wise 3D motion context and utilizing both 2D and 3D pose information?The key hypotheses appear to be:1) Learning part-wise motion context is beneficial for sign language recognition compared to only using whole-body motion. The paper proposes using alternating part-wise and whole-body Transformers to capture both intra-part and inter-part motion contexts.2) Jointly utilizing 2D and 3D pose information can improve performance compared to using only 2D or only 3D poses. The paper proposes early fusion by concatenating 2D, 3D positional, and 3D rotational poses. So in summary, the main goals are to exploit part-wise motion contexts and effectively combine 2D and 3D poses to achieve state-of-the-art sign language recognition performance. The methods and experiments aim to validate these hypotheses.


## What is the main contribution of this paper?

This paper proposes a human part-wise motion context learning framework called P3D for sign language recognition. The main contributions are:1. Learning part-wise motion context using alternating layers of part-wise encoding Transformers (PET) and whole-body encoding Transformers (WET). PET encodes motion contexts from each body part, while WET merges them into a unified context. This allows capturing both intra-part and inter-part motion contexts. Experiments show this part-wise learning benefits sign language recognition performance.2. Employing pose ensemble by joint-wise concatenation of 2D and 3D poses to utilize their complementary information. This allows exploiting rich motion context from 3D pose while keeping 2D pose's benefits. Experiments show significant performance gains from the pose ensemble, with P3D outperforming previous methods by a large margin when using both 2D and 3D poses.3. Achieving state-of-the-art performance on the WLASL benchmark. P3D outperforms previous pose-based methods using just 2D pose. With pose ensemble, it surpasses previous state-of-the-art or achieves comparable performance to RGB-based methods.In summary, the main contributions are the part-wise motion context learning, exploiting 2D and 3D poses via ensemble, and superior performance on sign language recognition. The proposed P3D framework advances the state-of-the-art for pose-based sign language recognition.
