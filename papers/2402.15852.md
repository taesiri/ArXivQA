# [NaVid: Video-based VLM Plans the Next Step for Vision-and-Language   Navigation](https://arxiv.org/abs/2402.15852)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
Vision-and-Language Navigation (VLN) requires agents to navigate in diverse, unseen environments by following free-form linguistic instructions. While much progress has been made in simulated environments, VLN models still struggle to generalize across datasets and to the real world. This is due to reliance on imperfect inputs like odometry, depth sensors, and maps. 

Proposed Solution: 
This paper proposes NaVid, the first video-based Vision Language Model (VLM) for VLN that only requires RGB video as input. NaVid encodes each frame with an instruction-queried token to extract instruction-relevant features and multiple instruction-agnostic tokens to globally encode visual details. By modeling trajectories as video, NaVid provides richer spatial-temporal context compared to prior works. The model is trained on 550K navigation samples from VLN-CE and 665K captioning samples. 

Key Contributions:
- NaVid achieves SOTA results on VLN-CE benchmarks without any odometry, depth sensor, or map inputs - only RGB video. This advances towards more realistic human-like VLN.

- NaVid demonstrates superior cross-dataset generalization, improving R2Râ†’RxR success from 16.8% to 23.8% absolutely over prior arts.

- In real-world tests across diverse scenes, NaVid completes 84% of simple instructions and 48% of complex instructions requiring several steps. This showcases impressive sim-to-real transferability.  

- As the first VLM approach to continuous VLN, NaVid sets a strong benchmark for future research into leveraging large models for more generalizable embodied agents. Its video-based formulation provides a more practical paradigm compared to discrete modeling.

In summary, by harnessing a VLM trained on abundant data, NaVid advances VLN research through its SOTA performance, generalization ability, and realistic formulation, representing an exciting step towards deployable VLN.
