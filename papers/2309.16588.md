# [Vision Transformers Need Registers](https://arxiv.org/abs/2309.16588)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the main research question seems to be:How can we alleviate artifacts and improve the quality of feature maps in vision transformer models, both supervised and self-supervised?The key hypotheses proposed in the paper are:1) Modern vision transformers exhibit "outlier" tokens during inference that correspond to artifacts in feature maps and attention maps.2) These outliers appear because the model is repurposing redundant/uninformative patches to store and process global information. 3) Explicitly providing the model with additional "register" tokens as buffers will isolate this behavior and avoid collateral side effects like loss of local information in the patch tokens.4) Adding registers will remove artifacts, smooth feature maps, improve performance on dense prediction tasks, and enable methods like unsupervised object discovery.So in summary, the main research question is how to improve feature maps in vision transformers, and the central hypothesis is that providing register tokens can isolate the model's behavior of recycling redundant patches, avoiding artifacts and improving representation quality. The paper aims to demonstrate and validate these hypotheses through quantitative and qualitative experiments.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contributions appear to be:- Identifying and characterizing artifacts in the feature maps of supervised and self-supervised vision transformers, in the form of high-norm "outlier" tokens appearing primarily in background regions of images. - Proposing an interpretation that models are recycling redundant tokens to store and process global image information during inference.- Validating this interpretation by showing that the outlier tokens contain less local patch information but more global image information compared to normal tokens.- Demonstrating that simply appending additional "register" tokens to the input sequence fixes the artifact problem by providing dedicated tokens for the model to use for internal computations. - Showing that fixing the artifacts with register tokens improves performance on dense prediction tasks, enables usage with object discovery methods, and results in smoother and more interpretable feature maps and attention maps.So in summary, the main contribution appears to be identifying artifacts in vision transformers and proposing a simple and effective solution via register tokens to mitigate the artifacts, which improves model interpretability and performance on certain downstream tasks.
