# [Vision Transformers Need Registers](https://arxiv.org/abs/2309.16588)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question seems to be: 

Why do vision transformers like DINOv2 exhibit artifacts in their attention maps during inference, and how can this phenomenon be mitigated?

The key hypotheses appear to be:

1) The artifacts correspond to high-norm "outlier" tokens that arise during training and inference of sufficiently large vision transformer models.

2) These outliers represent redundant patches that the model learns to overwrite in order to store and process global information. 

3) Providing the model with explicit "register" tokens as placeholders for this global processing will isolate this behavior and eliminate the artifacts in the original patch tokens.

So in summary, the paper is investigating the underlying causes of attention map artifacts in vision transformers, with a focus on DINOv2, and proposing the use of register tokens to mitigate this issue. The central hypothesis seems to be that dedicating explicit registers for global processing will prevent the model from overwriting patch tokens for this purpose, resulting in cleaner attention maps.


## What is the main contribution of this paper?

 Based on the abstract and section titles, it seems the main contributions of this paper are:

1. Identifying and characterizing artifacts (high-norm outlier tokens) in the feature maps of supervised and self-supervised vision transformers.

2. Proposing a simple solution to mitigate these artifacts by providing additional "register" tokens that the model can use for internal computations instead of repurposing spatial feature tokens. 

3. Demonstrating that this solution removes the artifacts, leads to smoother feature maps, and improves performance on dense prediction tasks and compatibility with object discovery methods.

4. Showing that the phenomenon occurs not just in self-supervised models like DINOv2 but also in supervised models like DeiT-III and CLIP.

So in summary, the key contributions appear to be identifying, explaining, and mitigating artifacts in vision transformer feature maps using register tokens, and showing this improves feature quality and downstream performance across both self-supervised and supervised training paradigms.


## How does this paper compare to other research in the same field?

 Based on my understanding, here are a few thoughts on how this paper compares to other research in the same field:

- This paper focuses specifically on identifying and characterizing artifacts in feature maps of vision transformers (ViTs), both supervised and self-supervised models. This is a fairly focused investigation on a specific phenomenon - artifacts in ViTs - that doesn't seem to have been extensively studied before. 

- The paper provides both quantitative analysis (e.g. probing outlier tokens, evaluating on downstream tasks) and qualitative visualizations to demonstrate the presence of these artifacts and their effect. The combination of hard performance numbers and intuitive visuals make the findings convincing.

- The proposed solution of adding "register" tokens draws inspiration from memory transformers in NLP, but applies the idea to mitigate artifacts in vision transformers. Adaptating techniques across modalities is an interesting approach.

- The paper tests the proposed solution on a range of model types - supervised (DeiT), self-supervised (DINOv2), and cross-modal (CLIP). Evaluating across different training paradigms demonstrates the generality of the artifacts and solution.

- The analysis builds on recent advancements in self-supervised visual representation learning, especially the DINO family of methods. The findings contribute specifically to this sub-field of representation learning for computer vision.

- The paper identifies limitations of current vision transformers, which could guide improvement efforts. It also demonstrates improved performance on downstream tasks, showing practical benefits.

Overall, I would say the paper provides focused analysis of an artifact, proposes a tailored solution, thoroughly evaluates across settings, and builds on recent advancements - contributing specific insights to representation learning for vision. The approach seems novel compared to related work.


## What future research directions do the authors suggest?

 Based on my reading of the paper, some of the main future research directions suggested by the authors include:

- Developing methods to encourage more diversity and interpretability in the patterns learned by register tokens. The authors note that different registers exhibit some interesting but unregularized attention patterns in their experiments. Exploring ways to explicitly regularize or influence the behavior of registers could be beneficial.

- Analyzing the impact of different training configurations on the emergence of artifacts and outliers. The authors point out they have not fully determined what factors in the training process lead to artifacts appearing in some models like DINOv2 but not others like original DINO. Further study on how hyperparameters, model size, etc affect this could provide useful insights.

- Applying a similar analysis to other model architectures besides Vision Transformers. The authors focus their study on ViT models, but suggest expanding the investigation to see if similar artifacts appear in CNNs and other architectures during self-supervised training.

- Testing the impact of registers in other domains like NLP. The use of memory and registers proved beneficial for vision models - studying if they could also help with things like reducing peakiness in attention maps for language models is suggested.

- Exploring other potential uses and interpretations of registers. The authors provide one explanation for the role registers play, but suggest further analysis could reveal other ways they are functioning and whether they can be exploited for different purposes.

In summary, the main future directions include better understanding registers, expanding the study to other models and domains, and continuing to explore how registers could be used in new ways. The overall goal is gaining more insight into these artifacts and how to mitigate them.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the paper:

The paper identifies and characterizes artifacts, corresponding to high-norm tokens, that appear in the feature maps of both supervised and self-supervised vision transformer (ViT) networks during inference. These outlier tokens primarily arise in low-informative background areas of images and are repurposed by the model for internal computations, while containing little of the original local information from their input patches. The authors propose that the model learns to recognize redundant patches, discards their local information, and recycles the tokens to aggregate global image information. To alleviate this issue, they propose appending dedicated "register" tokens to the input sequence that the model can utilize for its internal computations, rather than overwriting existing patch tokens. Adding these register tokens is shown to remove the artifacts entirely, improve model performance on dense prediction tasks, enable usage of the features for object discovery, and provide smoother feature maps and attention maps.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the paper:

This paper identifies and characterizes artifacts in the feature maps of both supervised and self-supervised Vision Transformer (ViT) networks. The artifacts correspond to high-norm tokens that appear primarily in low-informative background areas of images during inference. They are repurposed by the network for internal computations. The authors first analyze where and when these artifacts appear during training. They show the artifacts emerge in sufficiently large networks after substantial training. Probing reveals the artifacts hold little local information about pixels or spatial location but substantial global information about the image. 

The authors hypothesize networks reuse low-information patches as computational scratchpads to increase efficiency. To test this, they propose appending dedicated "register" tokens to isolate this behavior. Experiments show models trained with registers do not develop artifacts. Registered models achieve equal or better performance on downstream tasks while producing smoother feature maps. This enables improved performance on object discovery tasks. The proposed register tokens provide a simple solution for removing undesirable artifacts in Vision Transformers.


## Summarize the main method used in the paper in one paragraph.

 Here is a one paragraph summary of the main method used in the paper:

This paper proposes adding additional "register" tokens to the input sequence of vision transformers in order to provide the model with slots to store and process global image information during inference. The authors observe that large vision transformer models tend to repurpose certain low-informative image patches into high-norm outlier tokens that hold less local information but more global image information. They hypothesize that explicitly providing extra register tokens will allow the model to store global information in these slots rather than overwriting local patch tokens. They show that adding a small number of register tokens to the input sequence eliminates the outlier artifact tokens and enables the model to achieve improved performance on dense prediction tasks as well as produce smoother feature maps. The register tokens emerge with some natural diversity in their attention patterns without any explicit regularization. Overall, this simple architectural modification removes undesirable artifacts and improves vision transformer performance.


## What problem or question is the paper addressing?

 Based on my reading of the paper, it seems the main problem the authors are addressing is the presence of artifacts or undesirable irregularities in the feature maps and attention maps of vision transformers. 

Specifically, the paper identifies these artifacts in both supervised and self-supervised vision transformer models like DeiT, CLIP, and DINO/DINOv2. The artifacts appear as high-norm "outlier" tokens that contain less local image information but more global information. 

The authors hypothesize that large, sufficiently trained vision transformer models are learning to recognize redundant tokens/patches and repurposing them to store and process global image information. However, this causes irregularities in the feature maps.

To address this, the authors propose adding explicit "register" tokens that the model can use for storing and processing global information instead of reusing redundant patch tokens. This avoids the artifacts and improves model performance.

In summary, the main problem is the presence of artifacts in vision transformer feature maps caused by models reusing redundant patches, and the solution is adding dedicated register tokens to isolate this behavior.


## What are the keywords or key terms associated with this paper?

 Based on skimming the paper, some of the key terms and concepts that seem most relevant are:

- Vision transformers - The paper focuses on analyzing and improving vision transformer models like ViT, DeiT, CLIP, DINO/DINOv2.

- Self-supervised learning - The paper studies artifacts and improvements particularly for self-supervised vision models like DINOv2.

- Artifacts - A main focus is understanding and mitigating artifacts like high-norm "outlier" tokens that appear in vision transformer feature maps. 

- Attention maps - The artifacts manifest in attention maps, which are analyzed visually. Cleaner attention maps are a goal.

- Object discovery - Methods like LOST that rely on vision transformer attention maps are evaluated. Improving attention maps improves object discovery.

- Registers - The proposed method introduces new "register" tokens to provide dedicated storage for the model and mitigate artifacts.

- Feature norms - Quantitatively analyzing feature map token norms distributions allows detecting outliers.

- Linear probing - Linear models on features are probed to understand what different tokens represent.

- Dense prediction - Tasks like segmentation and depth estimation evaluate local feature quality, which is improved.

So in summary, the key terms cover vision transformers, self-supervised learning, artifacts, attention maps, object discovery, registers, feature norms, linear probing, and dense prediction.


## Based on the paper, what questions should we ask to create a comprehensive summary of it? Please list at least 10 questions.

 Here are 10 potential questions to ask to create a comprehensive summary of a research paper:

1. What is the main research question or problem being addressed in the paper?

2. What are the key goals or objectives of the research? 

3. What is the theoretical background or prior work that motivates this research?

4. What methods, data, and analyses were used to conduct the research?

5. What were the main findings or results of the study?

6. What conclusions were drawn from the results? How do they relate back to the original research goals?

7. What are the limitations of the study that impact the interpretation of the findings?

8. What are the main contributions or implications of this work? How does it extend prior research?

9. What future work does the paper suggest is needed in this research area?

10. How does this paper fit into the broader context of the field? What open questions remain?

Asking questions like these should help summarize the key information about the paper's background, methods, results, and impacts. Additional targeted questions may be needed for papers on specific topics or using particular techniques. The goal is to extract the core elements needed to concisely convey the essence of the paper.


## Design 10 in-depth questions about method proposed in this paper and avoid simple question.

 Here are 10 potential in-depth questions about the method proposed in the paper:

1. The paper proposes using additional "register" tokens during pre-training of vision transformers to provide dedicated storage for aggregating global information. What motivated this proposed approach? How does it differ from prior techniques for incorporating external memory in transformers?

2. The authors hypothesize that large, sufficiently trained vision transformers learn to repurpose redundant/uninformative tokens as memory slots for global information aggregation. What evidence supports this hypothesis? Are there other potential explanations for the artifact tokens observed? 

3. How exactly are the additional register tokens incorporated during pre-training? Do they receive gradients and get updated like other tokens? What happens to them during downstream transfer learning?

4. The paper shows that using 1-2 register tokens is sufficient to mitigate artifacts. But performance keeps improving with more registers. What factors determine the optimal number of registers? Is there a risk of overparameterization?

5. The register tokens exhibit some natural diversity in their learned attention maps. How is this emergent behavior enabled? Does it relate to slot attention mechanisms? Should register diversity be explicitly encouraged?

6. The paper focuses on vision transformers, but does the register approach generalize to other transformer architectures? What unique aspects of vision transformers make them susceptible to these artifacts during pre-training?

7. How do the learned representations with registers compare to the original artifacts qualitatively and quantitatively? Are they capturing complementary global information despite the local attention?

8. For downstream tasks, when and why should register-enhanced pretrained models be preferred over original models? What types of tasks stand to benefit the most?

9. The registers improve performance of object discovery algorithms like LOST. Could the explicit storage be leveraged more directly to improve unsupervised object segmentation? 

10. The paper studies ImageNet-scale datasets. How do dataset characteristics like size, noise, domain, etc. impact emergence of artifacts and the benefits of registers?
