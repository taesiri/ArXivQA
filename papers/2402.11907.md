# [Direct Large Language Model Alignment Through Self-Rewarding Contrastive   Prompt Distillation](https://arxiv.org/abs/2402.11907)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem: 
- Aligning large language models (LLMs) with human expectations without relying on costly human-annotated preference data is an important open challenge. 

Proposed Solution - Direct Large Model Alignment (DLMA):
- Leverages contrastive prompt pairs to let the LLM generate preference response pairs (one superior, one inferior) to queries.  
- Introduces a probability-based self-rewarding score to evaluate the quality of the self-generated response pairs by comparing generation likelihoods under contrastive prompts.
- Optimizes the LLM with the self-rewarding scores using a direct preference optimization (DPO) method that is efficient and stable.

Main Contributions:
- Demonstrates probability-based evaluation is more effective than text-generation based evaluation for assessing self-generated preference data quality.  
- Proposes the DLMA method for aligning LLMs without human annotation through self-rewarding contrastive prompt distillation.
- Experiments show DLMA surpasses existing approaches on alignment benchmarks, achieving better performance than even human-annotation based methods.
- Provides analysis confirming self-rewarding score accuracy and preservation of text quality after alignment.

In summary, the paper makes notable contributions in enabling unsupervised alignment of LLMs to human preferences through an automatic self-rewarding framework. The proposed DLMA method sets the stage for further work on safe and scalable language model alignment.
