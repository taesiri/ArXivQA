# [On Exploring the Reasoning Capability of Large Language Models with   Knowledge Graphs](https://arxiv.org/abs/2312.00353)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a high-quality summary paragraph of the key points from the paper:

This paper explores the ability of Large Language Models (LLMs) like GPT-3 to perform reasoning on knowledge graphs using their internal knowledge gained during pre-training. Two research questions are investigated: 1) To what extent LLMs can accurately recall facts and relations from their internal knowledge graphs, and 2) How well LLMs can infer implicit relations from a given context paragraph. The authors evaluate LLMs on tasks like tail entity prediction, relation prediction, relation extraction, and contextual path finding between entities. They introduce metrics to measure accuracy as well as different types of hallucinations made by models. The results demonstrate LLMs are able to recall factual knowledge to a reasonable degree and infer relations from context, with more advanced models like GPT-4 performing the best. However, there is still room for improvement to reduce content and ontology hallucinations during reasoning. The preliminary analysis provides motivation for further research leveraging LLMs' internal knowledge graphs for knowledge retrieval and reasoning problems.
