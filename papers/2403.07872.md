# [Rethinking Generative Large Language Model Evaluation for Semantic   Comprehension](https://arxiv.org/abs/2403.07872)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem Statement
- Evaluating large language models (LLMs) is crucial but challenging due to their open-ended nature and non-uniqueness of reasonable answers.  
- Current LLM evaluations center around multiple choice question answering (MCQA) for convenience, but it has limitations:
   1) Inconsistency between MCQA and practical open-ended question answering
   2) Varying MCQA evaluation strategies across models
   3) Discrepancies between MCQA predictions and open-ended responses  
   4) Mismatch with real-world usage scenarios

Proposed Solution  
- Introduce RWQ-Elo rating system that evaluates LLMs through two-player contests, mirroring real-world usage
   - Compile "Real-World Questions" (RWQ) benchmark of 20,772 authentic user queries
   - Randomly pair two LLMs to respond to RWQ questions 
   - Use GPT-4 as judge to determine winner/loser/tie based on response quality
   - Assign Elo rating to each LLM, adjusted after each contest round

- Compare 24 LLMs including GPT-4, GPT-3.5, Gemini-Pro, LLaMA families
- Analyze system stability, relation to other leaderboards like AlpacaEval, and schema for registering new models

Main Contributions
- Comprehensive study on limitations of prevalent MCQA evaluation
- Introduction of RWQ-Elo system for reliable/scalable LLM assessment 
- RWQ benchmark of 20,772 real-world user queries across diverse domains
- In-depth experiments on 24 SOTA models to showcase RWQ-Elo's effectiveness

The paper makes notable contributions in highlighting issues with existing LLM evaluation approaches, and proposing an improved competitive rating system that better reflects practical usage. The RWQ benchmark and extensive experiments on latest models also represent valuable assets to the community.
