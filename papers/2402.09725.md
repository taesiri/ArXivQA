# [Improving Non-autoregressive Machine Translation with Error Exposure and   Consistency Regularization](https://arxiv.org/abs/2402.09725)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Non-autoregressive machine translation (NAT) models generate translations in parallel, improving speed but struggling to model target sentence dependencies. 
- A popular NAT approach is conditional masked language modeling (CMLM), which iteratively masks and predicts low-confidence tokens.
- However, CMLM suffers from exposure bias between training and inference due to distribution mismatch of observed tokens (ground truth vs model-predicted).

Proposed Solution: 
- The paper proposes error exposure and consistency regularization (EECR) to address the training-inference mismatch in CMLM.

- Error Exposure: Construct mixed sequences by replacing some ground truth tokens with model-predicted ones during training. Supervise predictions of masked tokens under these imperfect observations to expose model to potential inference errors.

- Consistency Regularization: Add symmetric KL divergence between masked token distributions under different observing scenarios as optimization objective. Enforces prediction consistency across views.

Main Contributions:
- Proposes a simple and general strategy of combining error exposure and consistency regularization to address exposure bias issue in conditional masked language models like CMLM.

- Achieves improved performance over CMLM baselines with reduced repetition rate, showing effectiveness. Best model variant achieves comparable results to autoregressive Transformer.

- Analyses reveal the approach leads to more consistent behavior between training and inference. Probability distributions of predictions become more similar across ground truth and noisy observing scenarios.

- Has wide applicability to different conditional masked language models. Tested improvements on both CMLM and CMLMC models over several translation datasets.
