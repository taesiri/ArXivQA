# [Do Input Gradients Highlight Discriminative Features?](https://arxiv.org/abs/2102.12781)

## What is the central research question or hypothesis that this paper addresses?

 From my reading of the paper, the central research question seems to be: Do input gradients of neural network models accurately highlight discriminative features for a given prediction task? 

The key hypothesis that the paper investigates is:

Assumption (A): Input coordinates with larger gradient magnitude are more relevant for model prediction compared to input coordinates with smaller gradient magnitude.

The authors test the validity of this common assumption behind many input gradient based explanation methods like Integrated Gradients and SmoothGrad. Their main approach is to introduce an evaluation framework called DiffROAR to directly compare the predictive power of input coordinates with top vs. bottom gradient magnitudes.

Through experiments on image classification datasets and theory on a synthetic dataset, the authors provide evidence that:

1) Input gradients of standard models can grossly violate assumption (A), incorrectly highlighting unimportant features. 

2) Input gradients of adversarially trained robust models better satisfy assumption (A) by accurately highlighting task-relevant features.

3) A key reason standard models violate (A) is due to "feature leakage", where input gradients highlight spurious features from other instances. 

Overall, the paper systematically investigates if and when input gradients satisfy assumption (A) which underlies their use in many popular explanation methods.


## What is the main contribution of this paper?

 Based on my understanding, the main contributions of this paper are:

1. It introduces an evaluation framework called DiffROAR to test the common assumption that input gradient magnitude highlights discriminative features for model prediction. The framework compares the predictive power of features with the highest vs. lowest input gradient magnitudes.

2. It applies DiffROAR to show that input gradients of standard neural networks on image classification tasks often violate the assumption by suppressing discriminative features. However, input gradients of adversarially robust models satisfy the assumption reasonably well.

3. It introduces BlockMNIST, a semi-real dataset based on MNIST that encodes a priori knowledge of discriminative features. Analysis on this dataset validates the findings from real image datasets and reveals "feature leakage" in input gradients of standard models. 

4. It provides theoretical analysis on a simplified version of BlockMNIST to rigorously demonstrate feature leakage in standard models and how adversarial training helps mitigate this problem.

In summary, the paper systematically studies the fidelity of input gradients using empirical evaluation on real and semi-real datasets combined with theoretical analysis. It reveals counter-intuitive behaviors of input gradients of standard models and sheds light on why adversarial training improves attribution quality.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the key points in the paper:

The paper introduces an evaluation framework and semi-real dataset to test the common assumption that input gradient magnitude highlights discriminative features, and finds that while this assumption is violated by standard models, it holds reasonably well for adversarially robust models.


## How does this paper compare to other research in the same field?

 This paper presents an evaluation framework and analysis of input gradient attributions in image classification models. Here are some key ways it relates to other work in explainable AI:

- It focuses specifically on testing the common assumption that input gradient magnitude highlights discriminative features for prediction. Many papers propose new attribution methods, but fewer works directly probe the validity of assumptions underlying popular techniques like input gradients.

- The proposed evaluation framework DiffROAR adapts the remove-and-retrain (ROAR) methodology to directly compare top vs bottom input gradient attributions. Other evaluation frameworks like ROAR measure overall feature importance rather than testing the relative importance of top vs bottom features.

- The analysis identifies "feature leakage" in input gradients of standard models as a reason for violating the common assumption. The concept of feature leakage has not been explicitly studied before in relation to input gradient fidelity. 

- The semi-real BlockMNIST dataset enables empirical verification of findings by encoding ground truth knowledge of discriminative features. Most prior work studies real-world datasets where ground truth features are unknown.

- The simplified synthetic dataset allows rigorous theoretical characterization of feature leakage phenomenon in standard and robust models. Other theoretical analyses focus on different aspects like implicit regularization.

- The paper thoroughly analyzes input gradients specifically, whereas most work studies multiple attribution methods. Given the fundamental role of input gradients, focusing exclusively on them is an interesting distinction.

In summary, this work makes conceptual contributions by formalizing and testing common assumptions made in interpretation methods. The proposed frameworks serve as "sanity checks" to audit the fidelity of explanations. The empirical and theoretical identification and analysis of feature leakage also provides novel insight into the limitations of input gradient attributions.


## What future research directions do the authors suggest?

 Based on my reading of the paper, here are some of the future research directions the authors suggest:

- Further analysis of other commonly-used attribution methods beyond vanilla input gradients to understand if they exhibit behaviors like feature leakage. The authors focused exclusively on input gradients due to their fundamental role in many attribution methods, but suggest extending the analysis to other methods as well.

- Further investigation into how adversarial training helps mitigate feature leakage in input gradient attributions. The authors identified feature leakage as a key reason why standard models' input gradients violate assumption (A), but do not fully characterize how adversarial training fixes this issue.

- Development of additional synthetic datasets like the simplified BlockMNIST that allow theoretical analysis of feature leakage and related phenomena. The authors propose the simplified BlockMNIST as a model system to study these concepts rigorously, and suggest creating other similar synthetic datasets amenable to theoretical analysis.

- Formalization and falsification of other common assumptions made in interpretability research. The authors highlight the need to state assumptions clearly and test them empirically, as with assumption (A) in this work. They encourage the community to identify and rigorously evaluate other intuitive assumptions.

- Extension of the DiffROAR evaluation framework to other domains beyond image classification. The authors develop DiffROAR for evaluating assumption (A) on image data, but suggest expanding it to other data types like time series, text, etc.

In summary, the authors advocate for more rigorous theoretical analysis of attribution methods and their behaviors using simplified model systems, stricter statement and evaluation of assumptions in interpretability, and expansion of evaluation frameworks like DiffROAR to thoroughly test these assumptions across domains.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the key points in the paper:

The paper introduces a new evaluation framework called DiffROAR to analyze whether input gradient attributions satisfy the assumption that input coordinates with larger gradient magnitudes are more relevant for model prediction compared to those with smaller magnitudes. Experiments using DiffROAR on image classification benchmarks suggest that input gradients of standard models violate this assumption whereas robust models satisfy it reasonably well. To further investigate this, the authors introduce BlockMNIST, a semi-real dataset encoding a priori knowledge of discriminative features. Analysis on BlockMNIST confirms findings from real datasets - input gradients of standard models exhibit "feature leakage" and highlight non-discriminative features while robust models suppress leakage. Finally, the authors theoretically demonstrate feature leakage for standard models on a simplified BlockMNIST dataset, proving input gradients do not highlight instance-specific features and thus violate the key assumption. Overall, the work motivates formally testing assumptions in interpretability and shows the evaluation framework and semi-real datasets can serve as sanity checks for attribution methods.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the key points in the paper:

The paper introduces an evaluation framework called DiffROAR to analyze whether input gradient attributions satisfy a key assumption that is made by many feature attribution methods. The premise is that input coordinates with larger gradient magnitude are more relevant for the model's prediction compared to those with smaller gradient magnitude. 

The authors take a multi-pronged approach to systematically study input gradient attributions. First, they introduce DiffROAR, which builds on the ROAR framework, to test the premise on real-world image classification datasets. The experiments show that input gradients of standard models violate the premise, a phenomenon termed Feature Inversion in Gradients (FIG), while gradients of adversarially robust models satisfy the premise. Next, the authors design a semi-real MNIST-based dataset called BlockMNIST that encodes a priori knowledge of discriminative features. Analyzing this dataset validates the findings on real data and reveals that standard models exhibit "feature leakage," highlighting discriminative features from other instances. Finally, the authors introduce a simplified theoretical version of BlockMNIST and prove that standard models demonstrate FIG due to feature leakage while adversarially robust models do not. In summary, the work provides empirical and theoretical evidence that standard models violate the key premise behind input gradient feature attribution, while robust models satisfy the premise.


## Summarize the main method used in the paper in one paragraph.

 Here is a one paragraph summary of the main method used in the paper:

The paper introduces a new evaluation framework called DiffROAR to analyze the fidelity of input gradient attributions in highlighting discriminative features. DiffROAR builds on the remove-and-retrain (ROAR) methodology to account for masking-induced distribution shifts. Given an attribution scheme A, architecture M, and level k, DiffROAR computes the difference between predictive power of top-k A and bottom-k A unmasking schemes. A positive DiffROAR score indicates the attribution scheme satisfies the key assumption that higher gradient magnitude coordinates are more useful for prediction. Experiments apply DiffROAR to evaluate input gradients of standard and robust MLPs/CNNs on image classification benchmarks like CIFAR-10. The paper also analyzes a semi-real MNIST-based dataset called BlockMNIST where ground truth discriminative features are known a priori. This is used to validate and explain empirical findings regarding input gradients. Finally, a simplified version of BlockMNIST is analyzed theoretically to formally demonstrate the feature leakage phenomenon in input gradients of standard models.


## What problem or question is the paper addressing?

 The paper is addressing the validity of a key assumption made in several popular post-hoc interpretability methods for neural networks. Specifically, many gradient-based attribution methods, like input gradients, assume that:

(A) Coordinates with larger input gradient magnitude are more relevant for model prediction compared to coordinates with smaller input gradient magnitude. 

In other words, the magnitude of input gradients is assumed to highlight discriminative, task-relevant features. 

The paper takes a multi-pronged approach to systematically test the validity of this assumption (A) on real-world image classification datasets as well as synthetic datasets where ground truth features are known. The overall goal is to empirically and theoretically analyze whether input gradient attributions satisfy or violate assumption (A).


## What are the keywords or key terms associated with this paper?

 Based on a brief review of the paper, some of the key terms and concepts include:

- Deep learning
- Neural networks
- Convolutional neural networks (CNNs)
- Adversarial examples 
- Adversarial training
- Robustness
- Interpretability
- Attribution methods
- Input gradients
- Assumption A
- Feature inversion in gradients (FIG)
- Fixing feature inversion via robustness (RF-FIG)  
- Real-world image datasets (CIFAR-10, Imagenet-10 etc)
- Semi-real datasets (BlockMNIST)  
- Synthetic datasets
- Remove and Retrain (ROAR) framework
- DiffROAR evaluation framework

The main focus of the paper seems to be evaluating the fidelity of input gradient-based attribution methods for explaining model predictions, especially comparing standard and adversarially trained models. The key assumption evaluated is whether input gradient magnitude correlates with feature importance, through proposed frameworks like DiffROAR. The notion of "feature inversion in gradients" (FIG) and mitigating it via adversarial training to get "robustness fixes FIG" (RF-FIG) also appears central. The combination of real-world datasets, semi-real datasets like BlockMNIST, and synthetic datasets provides an interesting multi-pronged empirical and theoretical analysis.
