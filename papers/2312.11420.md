# [Tuning LayerNorm in Attention: Towards Efficient Multi-Modal LLM   Finetuning](https://arxiv.org/abs/2312.11420)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Finetuning large language models (LLMs) into multi-modal LLMs (MLLMs) that can handle vision-language tasks poses a major computational challenge. 
- Full parameter finetuning requires substantial compute resources. Existing efficient methods like LoRA sacrifice performance.

Proposed Solution: 
- The paper proposes finetuning only the Layer Normalization (LayerNorm) components within the attention blocks of the LLM decoder. 
- This is inspired by viewing MLLM creation as a domain adaptation task from text to multi-modal. LayerNorm helps transition knowledge.

Main Contributions:
- Show LayerNorm finetuning alone achieves comparable or better performance than full finetuning and significantly outperforms LoRA, while requiring far fewer trainable parameters.
- Find that with just 0.004% trainable parameters from LayerNorm (termed LayerNorm-simple), strong performance is still obtained.
- Discover image-grounded conversational data is much more effective for MLLM finetuning than other types of data.
- Analysis provides intuitions into why LayerNorm works well - it acts as an effective domain adaptor, improves model expressiveness, and exhibits beneficial optimization properties.
- Overall, demonstrate that both data and parameters can be substantially reduced for efficient high-performance MLLM finetuning via the LayerNorm approach. The work helps pave the way for more accessible MLLM adoption.
