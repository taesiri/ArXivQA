# [Safe Deep RL in 3D Environments using Human Feedback](https://arxiv.org/abs/2201.08102)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be:Is the ReQueST approach feasible for safe deep reinforcement learning in complex 3D environments using only human feedback (no simulator or procedural reward function)?Specifically, the authors aim to determine:1) If a high-quality pixel-based dynamics model can be learned from human trajectories to enable meaningful feedback. 2) If the data requirements for the human trajectories and feedback are viable in terms of quantity and quality.The overall goal is to show that ReQueST can enable training an RL agent with close to zero instances of unsafe behavior, using only human data. The paper tests this approach on a 3D first-person object collection task and compares it to standard RL algorithms.In summary, the central hypothesis is that ReQueST is a viable approach for safe RL in complex 3D environments relying solely on human feedback, and the paper aims to demonstrate this through experiments on a 3D object collection task.


## What is the main contribution of this paper?

The main contribution of this paper is demonstrating that the ReQueST method for safe reinforcement learning is feasible in complex 3D environments using real human data. Specifically:- They show that a high-quality pixel-based dynamics model can be learned from human demonstrator trajectories in a 3D first-person environment, and this model can be used to generate simulated rollouts for human feedback. - They collect reward sketches from humans on simulated trajectories and use these to train a reward model.- They show that using the learned dynamics and reward models, they can deploy an agent with model predictive control that achieves competent performance on an apple collection task with 10-20x fewer instances of unsafe behavior compared to a standard RL algorithm.- They analyze the data requirements and show performance degrades gracefully with less data. They also examine failure modes when the models are imperfect.Overall, this paper demonstrates that the ReQueST framework can scale to complex 3D environments and real human data, enabling learning from human feedback with reduced unsafe behavior during training. The key technical contributions are showing this is possible with high-dimensional pixel observations and analyzing the data needs and failure modes.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence TL;DR summary of the paper:The paper demonstrates that the ReQueST algorithm for safe deep reinforcement learning is feasible in complex 3D environments, enabling training of competent agents with orders of magnitude fewer instances of unsafe behavior compared to standard RL, using only safe human demonstrations and reward feedback.


## How does this paper compare to other research in the same field?

Here are a few key ways this paper compares to other research on safe deep reinforcement learning in 3D environments:- Data Sources: This paper uses entirely human-provided data to train the dynamics and reward models, unlike much prior work that relies more heavily on simulated data. Collecting sufficient real human data at scale is a key contribution.- Environment Complexity: The 3D environment used here appears more visually complex than environments in most prior safe RL papers, with photorealistic rendering. This tests whether pixel-based models can scale.- Feedback Mechanism: The paper uses reward sketches as the primary feedback signal, rather than more common choices like rankings/comparisons or binary safe/unsafe labels. This demonstrates the viability of high-bandwidth human feedback.- Performance Metrics: Safety violations are evaluated rigorously, both during training and deployment. Many papers focus only on deployment performance. Evaluating safety during training is critical for safe exploration techniques.- Agent Training: The paper uses model predictive control at deployment time rather than full end-to-end RL training. This simplifies training but limits the final performance level.- Key Limitations: The environment complexity is still limited compared to real-world settings. And the agent relies heavily on human data collection, which may not always be feasible.Overall, this paper makes excellent progress on testing whether human-in-the-loop approaches like ReQueST can scale to complex 3D environments. But many open challenges remain for real-world application of these ideas. The paper offers an exciting proof-of-concept rather than an end-to-end solution ready for deployment.


## What future research directions do the authors suggest?

Based on my reading of the paper, here are some of the key future research directions suggested by the authors:- Applying ReQueST in real-world environments using real robots, to evaluate how well the approach scales beyond simulated 3D environments. The authors acknowledge that real-world evaluation is an important next step.- Improving the quality and efficiency of the learned neural simulator models. The authors note there is room for progress on the video prediction quality and data requirements of the dynamics models. Architectural improvements could help.- Comparing ReQueST to other safe exploration methods like constrained RL. The authors suggest this could be an informative comparison.- Evaluating the human data requirements and scaling behavior more thoroughly across a wider range of environments and tasks. While the authors did some analysis of this, more work is needed to fully understand the practical data needs.- Exploring whether unsupervised pretraining of dynamics models could help mitigate the reliance on large human datasets. The authors propose this as a potential way to reduce human data needs for some tasks.- Developing better ways to evaluate when the models and data are "good enough" for safe deployment. The authors faced challenges in model validation.- Using more sophisticated forms of human oversight during training, like debates between humans, to help scalability. The authors suggest this could address limitations around human ability to recognize unsafe behavior.- Exploring whether incentives to tamper exist during the trajectory optimization process while training reward models. The authors outline a hypothetical failure case around this.In summary, key directions are real-world tests, improving simulators, comparisons to other methods, better understanding of scaling, reducing human data needs, validation techniques, oversight mechanisms, and analyzing incentives.


## Summarize the paper in one paragraph.

Here is a one paragraph summary of the paper:The paper proposes a method called ReQueST (reward query synthesis via trajectory optimization) for safe deep reinforcement learning in complex 3D environments. The key idea is to first learn a neural simulator of the environment from safe human trajectories. This learned simulator is then used to efficiently learn a reward model by asking humans for feedback on hypothesized trajectories, avoiding the need to actually execute unsafe trajectories in the real environment during training. The method is demonstrated in a 3D first-person object collection task, where it enables training an agent with much lower instances of unsafe behavior compared to standard RL. The neural simulator and reward model are trained using only human data, showing the approach can work with imperfect real-world data. Overall, the paper shows that the ReQueST framework is feasible for safe exploration in complex 3D environments using data sourced entirely from human contractors.
