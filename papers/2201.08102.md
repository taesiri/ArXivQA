# [Safe Deep RL in 3D Environments using Human Feedback](https://arxiv.org/abs/2201.08102)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be:Is the ReQueST approach feasible for safe deep reinforcement learning in complex 3D environments using only human feedback (no simulator or procedural reward function)?Specifically, the authors aim to determine:1) If a high-quality pixel-based dynamics model can be learned from human trajectories to enable meaningful feedback. 2) If the data requirements for the human trajectories and feedback are viable in terms of quantity and quality.The overall goal is to show that ReQueST can enable training an RL agent with close to zero instances of unsafe behavior, using only human data. The paper tests this approach on a 3D first-person object collection task and compares it to standard RL algorithms.In summary, the central hypothesis is that ReQueST is a viable approach for safe RL in complex 3D environments relying solely on human feedback, and the paper aims to demonstrate this through experiments on a 3D object collection task.


## What is the main contribution of this paper?

The main contribution of this paper is demonstrating that the ReQueST method for safe reinforcement learning is feasible in complex 3D environments using real human data. Specifically:- They show that a high-quality pixel-based dynamics model can be learned from human demonstrator trajectories in a 3D first-person environment, and this model can be used to generate simulated rollouts for human feedback. - They collect reward sketches from humans on simulated trajectories and use these to train a reward model.- They show that using the learned dynamics and reward models, they can deploy an agent with model predictive control that achieves competent performance on an apple collection task with 10-20x fewer instances of unsafe behavior compared to a standard RL algorithm.- They analyze the data requirements and show performance degrades gracefully with less data. They also examine failure modes when the models are imperfect.Overall, this paper demonstrates that the ReQueST framework can scale to complex 3D environments and real human data, enabling learning from human feedback with reduced unsafe behavior during training. The key technical contributions are showing this is possible with high-dimensional pixel observations and analyzing the data needs and failure modes.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence TL;DR summary of the paper:The paper demonstrates that the ReQueST algorithm for safe deep reinforcement learning is feasible in complex 3D environments, enabling training of competent agents with orders of magnitude fewer instances of unsafe behavior compared to standard RL, using only safe human demonstrations and reward feedback.


## How does this paper compare to other research in the same field?

Here are a few key ways this paper compares to other research on safe deep reinforcement learning in 3D environments:- Data Sources: This paper uses entirely human-provided data to train the dynamics and reward models, unlike much prior work that relies more heavily on simulated data. Collecting sufficient real human data at scale is a key contribution.- Environment Complexity: The 3D environment used here appears more visually complex than environments in most prior safe RL papers, with photorealistic rendering. This tests whether pixel-based models can scale.- Feedback Mechanism: The paper uses reward sketches as the primary feedback signal, rather than more common choices like rankings/comparisons or binary safe/unsafe labels. This demonstrates the viability of high-bandwidth human feedback.- Performance Metrics: Safety violations are evaluated rigorously, both during training and deployment. Many papers focus only on deployment performance. Evaluating safety during training is critical for safe exploration techniques.- Agent Training: The paper uses model predictive control at deployment time rather than full end-to-end RL training. This simplifies training but limits the final performance level.- Key Limitations: The environment complexity is still limited compared to real-world settings. And the agent relies heavily on human data collection, which may not always be feasible.Overall, this paper makes excellent progress on testing whether human-in-the-loop approaches like ReQueST can scale to complex 3D environments. But many open challenges remain for real-world application of these ideas. The paper offers an exciting proof-of-concept rather than an end-to-end solution ready for deployment.
