# [Safe Deep RL in 3D Environments using Human Feedback](https://arxiv.org/abs/2201.08102)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be:Is the ReQueST approach feasible for safe deep reinforcement learning in complex 3D environments using only human feedback (no simulator or procedural reward function)?Specifically, the authors aim to determine:1) If a high-quality pixel-based dynamics model can be learned from human trajectories to enable meaningful feedback. 2) If the data requirements for the human trajectories and feedback are viable in terms of quantity and quality.The overall goal is to show that ReQueST can enable training an RL agent with close to zero instances of unsafe behavior, using only human data. The paper tests this approach on a 3D first-person object collection task and compares it to standard RL algorithms.In summary, the central hypothesis is that ReQueST is a viable approach for safe RL in complex 3D environments relying solely on human feedback, and the paper aims to demonstrate this through experiments on a 3D object collection task.
