# [Text-guided Explorable Image Super-resolution](https://arxiv.org/abs/2403.01124)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
The paper introduces the problem of zero-shot text-guided exploration of solutions to open-domain image super-resolution. The goal is to allow users to explore diverse, semantically accurate image reconstructions that preserve data consistency with low-resolution inputs for arbitrary large downsampling factors, without explicitly training models for these degradations. This is an ill-posed problem with multiple valid solutions.

Methods: 
The paper proposes two approaches:

1. Modifying text-to-image (T2I) diffusion models for zero-shot super-resolution by incorporating reconstruction guidance or null-space consistency in the multi-stage sampling process. Specifically, Imagen and DALL-E models are adapted using diffusion posterior sampling (DPS), pseudoinverse-guided diffusion (PiGDM), and denoising diffusion null-space models (DDNM).

2. Incorporating CLIP image-text similarity guidance along with DDNM null-space consistency for zero-shot diffusion restoration. An energy function based on CLIP embeddings is incorporated to guide sampling towards text consistency.

Contributions:

- First work to explore open-domain image super-resolution guided by text prompts in a zero-shot setting, allowing sampling of diverse solutions.

- Adaptation of state-of-the-art T2I models for zero-shot restoration through appropriate modification of the multi-stage sampling process.

- Incorporation of CLIP guidance along with DDNM to improve text consistency.

- Extensive experiments demonstrate improved diversity and text-consistency over existing methods, ability to recover complex scene details from text, and exploration of solutions by varying textual attributes.

The work opens up promising directions for user-guided exploration of image recovery through language.
