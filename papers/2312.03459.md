# [F3-Pruning: A Training-Free and Generalized Pruning Strategy towards   Faster and Finer Text-to-Video Synthesis](https://arxiv.org/abs/2312.03459)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a high-quality paragraph summarizing the key points of the paper:

This paper proposes F$^3$-Pruning, a training-free and generalized pruning strategy to accelerate inference for text-to-video (T2V) synthesis models while maintaining or even improving video quality. By analyzing the inference process of transformer and diffusion based T2V models, the authors identify redundancy in the temporal attention modules commonly used to model relations between frames. They propose aggregating temporal attention values across layers/timesteps into an Aggregate Attention Score (AAS) and pruning weights below a fixed ratio $\alpha$ of smallest AAS. Experiments on two T2V models, the transformer-based CogVideo and diffusion-based Tune-A-Video, demonstrate 1.35x speedup of CogVideo on UCF-101 with 22% FVD improvement and consistently enhanced quality on other datasets. As a generalized strategy not requiring retraining or model-specific design, F$^3$-Pruning effectively accelerates T2V inference while assuring quality and alignment to text across model types. Key strengths are its training-free simplicity, generalizability, and dual benefits of faster inference and improved video quality from redistribution of redundant temporal attention.
