# [Nonsmooth Implicit Differentiation: Deterministic and Stochastic   Convergence Rates](https://arxiv.org/abs/2403.11687)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem Statement:
- The paper studies the problem of efficiently computing the derivative or Jacobian of the solution map of a parametric fixed point equation. Specifically, it considers fixed point equations where the mapping function $\Phi$ is not necessarily differentiable everywhere, making the problem more challenging. 

- Key applications that motivate this problem include hyperparameter optimization, meta-learning, and data poisoning attacks in machine learning, where the fixed point equation arises from optimality conditions of an underlying optimization problem.

- The paper considers both a deterministic setting where $\Phi$ can be evaluated explicitly, and a stochastic setting where $\Phi$ has a composite form involving an expectation that is estimated via a stochastic oracle.

Proposed Solutions:

- For the deterministic setting, the paper analyzes two methods - Iterative Differentiation (ITD) and Approximate Implicit Differentiation (AID). ITD differentiates through the iterations of a fixed point solver, while AID solves a linear system based on the implicit function theorem.

- For the stochastic setting, the paper proposes Nonsmooth Stochastic Implicit Differentiation (NSID), which approximates the Jacobian-vector product using stochastic estimates of the constituent mappings in $\Phi$.

Main Contributions:

- Provides non-asymptotic linear convergence rates for ITD and AID in the nonsmooth setting, revealing AID's faster convergence. Identification of cases where AID significantly outperforms ITD.

- First analysis of stochastic AID with proven convergence rates. Introduction of NSID method and analysis showing it matches best known rates for smooth setting.

- Demonstration on bilevel optimization problems in hyperparameter optimization and adversarial attacks that the analysis translates to rates for approximating derivatives of the bilevel objective.

- Experiments confirming superior empirical performance of AID over ITD, and fast convergence of NSID validating the theory.
