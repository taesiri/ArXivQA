# [Objects that Sound](https://arxiv.org/abs/1712.06651)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central research questions/hypotheses of this paper are:1. Can audio and visual embeddings be learned in an unsupervised manner, using only audio-visual correspondence from videos, that enable effective cross-modal retrieval (e.g. using an image to search for related sounds)?2. Can a network be designed that is capable of localizing the sound source in an image, i.e. answering "which object in the image is making the sound?", using only unsupervised audio-visual correspondence as training.To summarize, the key goals are:- Develop unsupervised methods for learning aligned audio and visual embeddings suitable for cross-modal retrieval.- Design a network architecture that can localize sounding objects in images using only unsupervised audio-visual correspondence, without any labels.The authors use the audio-visual correspondence (AVC) task as a form of self-supervision to train the networks. This involves determining whether an audio clip and image frame correspond or not based on whether they are from the same video or different videos.The paper introduces two main network architectures:- The AVE-Net for learning aligned audio and visual embeddings for retrieval.- The AVOL-Net for localizing sounding objects using a multiple instance learning framework.Both are trained on the AVC task on unlabeled video data to achieve the desired functions.


## What is the main contribution of this paper?

The main contribution of this paper is designing deep neural network architectures that can learn useful audio-visual representations in a completely unsupervised manner, using only the correspondence between audio and video streams in videos as a supervisory signal. Specifically, the paper makes the following key contributions:- It proposes the Audio-Visual Embedding Network (AVE-Net), which can learn aligned audio and visual embeddings suitable for cross-modal retrieval tasks like querying images using audio, or retrieving sounds for a query image. Experiments show this network outperforms prior unsupervised and even supervised methods on audio-visual retrieval.- It develops the Audio-Visual Object Localization Network (AVOL-Net) which can localize the object in an image that is producing a given sound, using only the audio as input during inference. This demonstrates semantic grounding of sounds to visual objects in a completely unsupervised way.- The paper explores different network architectures for ingesting multiple visual frames, or optical flow along with a frame, as inputs. It finds that using more visual context does not necessarily help for learning better embeddings on this unsupervised task.- It provides insights into preventing shortcuts and "cheating" behaviors when training with this self-supervised correspondence objective, which could inform future work.In summary, the key innovation is in designing ConvNet architectures that enable new cross-modal capabilities like retrieval and localization to emerge when trained without any labels, using only the natural alignment between images and sounds in videos. The impressive results demonstrate the power of self-supervision from video for audio-visual understanding.
