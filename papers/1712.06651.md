# [Objects that Sound](https://arxiv.org/abs/1712.06651)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central research questions/hypotheses of this paper are:1. Can audio and visual embeddings be learned in an unsupervised manner, using only audio-visual correspondence from videos, that enable effective cross-modal retrieval (e.g. using an image to search for related sounds)?2. Can a network be designed that is capable of localizing the sound source in an image, i.e. answering "which object in the image is making the sound?", using only unsupervised audio-visual correspondence as training.To summarize, the key goals are:- Develop unsupervised methods for learning aligned audio and visual embeddings suitable for cross-modal retrieval.- Design a network architecture that can localize sounding objects in images using only unsupervised audio-visual correspondence, without any labels.The authors use the audio-visual correspondence (AVC) task as a form of self-supervision to train the networks. This involves determining whether an audio clip and image frame correspond or not based on whether they are from the same video or different videos.The paper introduces two main network architectures:- The AVE-Net for learning aligned audio and visual embeddings for retrieval.- The AVOL-Net for localizing sounding objects using a multiple instance learning framework.Both are trained on the AVC task on unlabeled video data to achieve the desired functions.
