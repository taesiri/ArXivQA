# [Objects that Sound](https://arxiv.org/abs/1712.06651)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central research questions/hypotheses of this paper are:1. Can audio and visual embeddings be learned in an unsupervised manner, using only audio-visual correspondence from videos, that enable effective cross-modal retrieval (e.g. using an image to search for related sounds)?2. Can a network be designed that is capable of localizing the sound source in an image, i.e. answering "which object in the image is making the sound?", using only unsupervised audio-visual correspondence as training.To summarize, the key goals are:- Develop unsupervised methods for learning aligned audio and visual embeddings suitable for cross-modal retrieval.- Design a network architecture that can localize sounding objects in images using only unsupervised audio-visual correspondence, without any labels.The authors use the audio-visual correspondence (AVC) task as a form of self-supervision to train the networks. This involves determining whether an audio clip and image frame correspond or not based on whether they are from the same video or different videos.The paper introduces two main network architectures:- The AVE-Net for learning aligned audio and visual embeddings for retrieval.- The AVOL-Net for localizing sounding objects using a multiple instance learning framework.Both are trained on the AVC task on unlabeled video data to achieve the desired functions.


## What is the main contribution of this paper?

The main contribution of this paper is designing deep neural network architectures that can learn useful audio-visual representations in a completely unsupervised manner, using only the correspondence between audio and video streams in videos as a supervisory signal. Specifically, the paper makes the following key contributions:- It proposes the Audio-Visual Embedding Network (AVE-Net), which can learn aligned audio and visual embeddings suitable for cross-modal retrieval tasks like querying images using audio, or retrieving sounds for a query image. Experiments show this network outperforms prior unsupervised and even supervised methods on audio-visual retrieval.- It develops the Audio-Visual Object Localization Network (AVOL-Net) which can localize the object in an image that is producing a given sound, using only the audio as input during inference. This demonstrates semantic grounding of sounds to visual objects in a completely unsupervised way.- The paper explores different network architectures for ingesting multiple visual frames, or optical flow along with a frame, as inputs. It finds that using more visual context does not necessarily help for learning better embeddings on this unsupervised task.- It provides insights into preventing shortcuts and "cheating" behaviors when training with this self-supervised correspondence objective, which could inform future work.In summary, the key innovation is in designing ConvNet architectures that enable new cross-modal capabilities like retrieval and localization to emerge when trained without any labels, using only the natural alignment between images and sounds in videos. The impressive results demonstrate the power of self-supervision from video for audio-visual understanding.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence TL;DR summary of the paper:The paper presents two unsupervised neural networks for learning aligned image and audio representations from unlabeled videos - one for cross-modal retrieval and one for localizing objects that sound in images based only on the audio.


## How does this paper compare to other research in the same field?

Here is a summary of how this paper on learning audio-visual embeddings compares to related work:- This paper focuses on learning cross-modal audio-visual representations in an unsupervised manner using only the natural alignment between sounds and images in video as supervision. Other works have also explored learning audio-visual representations from video in an unsupervised way, such as Owens et al. 2016, Arandjelovic et al. 2017, and Aytar et al. 2017. - A key contribution of this paper is designing network architectures specifically for cross-modal retrieval and audio-visual localization tasks. For retrieval, they introduce the AVE-Net which aligns audio and visual embeddings using a distance-based loss. This is different from prior works like Arandjelovic et al. 2017 that simply concatenated features. For localization, they propose the AVOL-Net based on a multiple instance learning approach to identify spatial regions corresponding to sound. This goes beyond previous works that only demonstrated associations between whole images and audio.- Compared to supervised approaches like Izadinia et al. 2017 and Hershey et al. 2017 that use annotated data, this work shows the viability of learning useful audio-visual representations without manual labels. However, the representations may not be as semantically precise as with full supervision.- An interesting finding of this work is that using multiple input frames did not necessarily improve performance, indicating that motion cues can provide shortcuts that prevent learning high-level semantics. This highlights the importance of careful architecture design in self-supervised learning.- Overall, this paper pushes forward unsupervised cross-modal learning through novel network designs tailored for retrieval and localization. The techniques demonstrate promising results on large, real-world video data without using any manual labels.In summary, the key innovations of this work compared to prior art are the specialized network architectures for cross-modal tasks, demonstration of localization abilities, and analysis illuminating the challenges of learning from video data. The techniques offer a path towards robust audio-visual understanding without costly labeled data.


## What future research directions do the authors suggest?

The authors suggest several future research directions in the conclusion and future work section:- Modifying the AVOL-Net to have an explicit soft attention mechanism, rather than the max-pooling currently used, for audio-visual object localization. - Exploring whether using information from multiple audio channels could aid localization, though this has some challenges such as requiring known calibration and being prone to noise.- Developing approaches to enable the AVOL-Net to detect entire objects rather than just the discriminative parts that produce sound.- Extending the ideas to the video domain by incorporating motion information.- Applying the approach to other sensing modalities beyond audio-visual, such as tactile signals.- Testing the robustness of the method on more diverse and unconstrained data.- Combining the approach with some labeled data in a semi-supervised framework.- Using the learnt representations for downstream tasks like audio-visual action recognition.- Exploring how the learnt representations could enable robots to learn about objects and their sounds in the real world.In summary, the main future directions are improving the localization, making the models more robust with less supervision, and applying them to robotics/embodied agents. The core idea of using audio-visual correspondence as self-supervision has potential for further exploration.


## Summarize the paper in one paragraph.

Here is a one paragraph summary of the key points from the paper:The paper proposes two new deep learning architectures for cross-modal learning using audio and visual data from unlabeled videos. First, the Audio-Visual Embedding Network (AVE-Net) is designed to learn aligned audio and visual embeddings that enable cross-modal retrieval, outperforming prior methods on an AudioSet subset. Second, the Audio-Visual Object Localization Network (AVOL-Net) localizes objects in images that are producing a given sound, using a multiple instance learning approach to identify relevant image regions without requiring bounding box supervision. Both models are trained from scratch using an audio-visual correspondence task as a self-supervised objective. Key results show successful cross-modal retrieval and localization of a diverse set of sounding objects. The work demonstrates how representation learning and localization can be achieved from unlabeled videos by designing appropriate deep network architectures trained with audio-visual correspondence.


## Summarize the paper in two paragraphs.

Here is a two paragraph summary of the paper:The paper proposes new neural network architectures for cross-modal learning between images and audio. The first architecture, called AVE-Net, is designed for cross-modal retrieval. It takes an input image and 1 second of audio and processes them through separate vision and audio subnetworks. The outputs are 128-dimensional embeddings that are aligned through the use of the Euclidean distance between them as the only fusion. This forces the representations to be compatible for retrieval across modalities. Experiments show this approach outperforms baselines on retrieval tasks using the AudioSet dataset. The second architecture, AVOL-Net, is designed for localizing which objects in an image are producing a given sound. It builds on AVE-Net but keeps a higher resolution output in the vision subnetwork. The audio embedding is compared to visual embeddings at each spatial location using scalar products to produce a similarity map. This map highlights regions correlated with the sound. Through multiple instance learning, the network learns to localize sounding objects without any supervision on object locations or identities. Results demonstrate it can localize a wide variety of instruments, tools, and other sounding objects in challenging real videos. Both methods illustrate the promise of self-supervision from audio-visual correspondence for representation learning.


## Summarize the main method used in the paper in one paragraph.

Here is a one paragraph summary of the main method used in the paper:The paper presents a neural network-based approach for learning cross-modal audio-visual embeddings and localizing objects that sound in images, using only unlabelled videos for training. The key idea is to train networks using an audio-visual correspondence (AVC) objective, where pairs of image frames and audio clips from the same video are treated as positives, while pairs from different videos are negatives. To enable cross-modal retrieval, an Audio-Visual Embedding Network (AVE-Net) is proposed that computes the correspondence score as the Euclidean distance between normalized audio and visual embeddings. This forces the embeddings to become aligned across modalities. For localizing sounding objects, an Audio-Visual Object Localization Network (AVOL-Net) is introduced that uses a multiple instance learning framework - region-level visual features are compared to the audio embedding and maximal similarity indicates the sounding object's location. Both networks are optimized end-to-end using the AVC task as the objective. The main novelty is in designing appropriate network architectures that can learn cross-modal retrieval and sound source localization abilities from scratch using only the weak supervision of audio-visual correspondence.


## What problem or question is the paper addressing?

The paper is addressing the problem of learning visual and audio representations and their alignment from unlabeled videos. Specifically, it focuses on two objectives:1. Learning networks that can embed audio and visual inputs into a common space suitable for cross-modal retrieval. This allows querying across modalities, e.g. using an image to search for related sounds.2. Learning a network that can localize the object making a sound in an image, given just the audio signal. This aims to answer the question "which object in the image is making this sound?".The key idea is to train the networks using only the weak supervision of audio-visual correspondence from unlabeled videos, i.e. whether an image frame and audio clip correspond or not. This cross-modal self-supervision enables learning the desired representations and alignments without manual annotations.The main contributions are:- Showing that aligned audio and visual embeddings can be learned to enable both within-mode (e.g. audio-audio) and cross-modal (e.g. image-audio) retrieval.- Exploring architectures for cross-modal retrieval and sound source localization trained on the audio-visual correspondence task.- Demonstrating that semantic objects making sounds in images can be localized using only the audio, without access to motion or flow information. - Providing guidance on avoiding shortcuts in the data preparation that could allow "cheating" on the correspondence task.In summary, the paper introduces techniques to learn from unlabeled video to perform cross-modal retrieval and localize sounding objects, using only the weak supervision of audio-visual correspondence. The key innovation is the network architectures and training frameworks designed for these tasks.
