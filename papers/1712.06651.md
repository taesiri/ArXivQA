# [Objects that Sound](https://arxiv.org/abs/1712.06651)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central research questions/hypotheses of this paper are:1. Can audio and visual embeddings be learned in an unsupervised manner, using only audio-visual correspondence from videos, that enable effective cross-modal retrieval (e.g. using an image to search for related sounds)?2. Can a network be designed that is capable of localizing the sound source in an image, i.e. answering "which object in the image is making the sound?", using only unsupervised audio-visual correspondence as training.To summarize, the key goals are:- Develop unsupervised methods for learning aligned audio and visual embeddings suitable for cross-modal retrieval.- Design a network architecture that can localize sounding objects in images using only unsupervised audio-visual correspondence, without any labels.The authors use the audio-visual correspondence (AVC) task as a form of self-supervision to train the networks. This involves determining whether an audio clip and image frame correspond or not based on whether they are from the same video or different videos.The paper introduces two main network architectures:- The AVE-Net for learning aligned audio and visual embeddings for retrieval.- The AVOL-Net for localizing sounding objects using a multiple instance learning framework.Both are trained on the AVC task on unlabeled video data to achieve the desired functions.


## What is the main contribution of this paper?

The main contribution of this paper is designing deep neural network architectures that can learn useful audio-visual representations in a completely unsupervised manner, using only the correspondence between audio and video streams in videos as a supervisory signal. Specifically, the paper makes the following key contributions:- It proposes the Audio-Visual Embedding Network (AVE-Net), which can learn aligned audio and visual embeddings suitable for cross-modal retrieval tasks like querying images using audio, or retrieving sounds for a query image. Experiments show this network outperforms prior unsupervised and even supervised methods on audio-visual retrieval.- It develops the Audio-Visual Object Localization Network (AVOL-Net) which can localize the object in an image that is producing a given sound, using only the audio as input during inference. This demonstrates semantic grounding of sounds to visual objects in a completely unsupervised way.- The paper explores different network architectures for ingesting multiple visual frames, or optical flow along with a frame, as inputs. It finds that using more visual context does not necessarily help for learning better embeddings on this unsupervised task.- It provides insights into preventing shortcuts and "cheating" behaviors when training with this self-supervised correspondence objective, which could inform future work.In summary, the key innovation is in designing ConvNet architectures that enable new cross-modal capabilities like retrieval and localization to emerge when trained without any labels, using only the natural alignment between images and sounds in videos. The impressive results demonstrate the power of self-supervision from video for audio-visual understanding.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence TL;DR summary of the paper:The paper presents two unsupervised neural networks for learning aligned image and audio representations from unlabeled videos - one for cross-modal retrieval and one for localizing objects that sound in images based only on the audio.


## How does this paper compare to other research in the same field?

Here is a summary of how this paper on learning audio-visual embeddings compares to related work:- This paper focuses on learning cross-modal audio-visual representations in an unsupervised manner using only the natural alignment between sounds and images in video as supervision. Other works have also explored learning audio-visual representations from video in an unsupervised way, such as Owens et al. 2016, Arandjelovic et al. 2017, and Aytar et al. 2017. - A key contribution of this paper is designing network architectures specifically for cross-modal retrieval and audio-visual localization tasks. For retrieval, they introduce the AVE-Net which aligns audio and visual embeddings using a distance-based loss. This is different from prior works like Arandjelovic et al. 2017 that simply concatenated features. For localization, they propose the AVOL-Net based on a multiple instance learning approach to identify spatial regions corresponding to sound. This goes beyond previous works that only demonstrated associations between whole images and audio.- Compared to supervised approaches like Izadinia et al. 2017 and Hershey et al. 2017 that use annotated data, this work shows the viability of learning useful audio-visual representations without manual labels. However, the representations may not be as semantically precise as with full supervision.- An interesting finding of this work is that using multiple input frames did not necessarily improve performance, indicating that motion cues can provide shortcuts that prevent learning high-level semantics. This highlights the importance of careful architecture design in self-supervised learning.- Overall, this paper pushes forward unsupervised cross-modal learning through novel network designs tailored for retrieval and localization. The techniques demonstrate promising results on large, real-world video data without using any manual labels.In summary, the key innovations of this work compared to prior art are the specialized network architectures for cross-modal tasks, demonstration of localization abilities, and analysis illuminating the challenges of learning from video data. The techniques offer a path towards robust audio-visual understanding without costly labeled data.
