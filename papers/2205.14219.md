# [Controllable Text Generation with Neurally-Decomposed Oracle](https://arxiv.org/abs/2205.14219)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper text, it seems the central research question is how to control auto-regressive language models to satisfy desired attributes during text generation. The key ideas and contributions in addressing this research question include:- Proposing a framework to decompose sequence-level oracle supervision into token-level guidance for controlling text generation models. This allows steering the base auto-regressive model towards satisfying the desired attributes.- The token-level guidance is approximated by an auxiliary neural model called NADO which is trained on data sampled from the base model. This helps align NADO better with the base model's distribution.- Providing theoretical analysis on how the approximation quality of NADO affects the controllable generation results. - Demonstrating the effectiveness of the proposed framework on two tasks - lexically constrained text generation and machine translation with formality control. The results show the framework can efficiently guide the base model to satisfy specified attributes while maintaining high text quality.So in summary, the central hypothesis is that by decomposing sequence-level constraints into token-level guidance and approximating it via a NADO model trained on base model samples, we can achieve effective control over text generation attributes without compromising quality or modifying the base model. The experimental results on constrained text generation and machine translation tasks seem to validate this hypothesis.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contributions seem to be:- Proposing a general framework (NADO) for controllable text generation that can decompose sequence-level oracles into token-level guidance to steer auto-regressive language models.- Theoretical analysis showing how the approximation quality of NADO affects the controllable generation results.- An optimal closed-form solution for incorporating the token-level guidance into the base model for controllable generation, based on posterior regularization. - Demonstrating the effectiveness of the proposed framework on two tasks: lexically constrained text generation and machine translation with formality control. The results show the framework can efficiently guide the base model to satisfy specified attributes while maintaining high quality.- The framework is flexible, treating the base model and oracle functions as black boxes. It does not require refactoring or fine-tuning the base model.- NADO is trained on data sampled from the base model rather than requiring auxiliary labeled data. This helps align NADO better with the base model.In summary, the key contribution appears to be proposing a general, flexible framework for controllable text generation that decomposes sequence-level constraints into token-level guidance for steering language models, with both theoretical analysis and empirical demonstrations of effectiveness. The framework is model-agnostic and data-efficient.


## How does this paper compare to other research in the same field?

Here are some thoughts on how this paper compares to other research in the field of controllable text generation:- The paper proposes a general framework (NADO) for controlling large autoregressive language models using only sequence-level supervision. This differs from many existing methods that require some amount of supervision at the token level (e.g. FUDGE, GeDI). Relying only on sequence-level supervision makes the framework more flexible.- The key idea is to decompose the sequence-level control objective into token-level guidance that can steer the base language model. This decomposition is approximated neurally rather than derived analytically. Other recent work like NeuroLogic and A*esque decoding focuses more on specialized constrained decoding algorithms. - The paper theoretically analyzes how the quality of the neural approximation affects the controllable generation results. This helps justify the approach and makes the limitations clearer. Formal analysis of this type seems less common in related work.- For training the neural control model, the paper emphasizes the importance of sampling from the base language model to match distributions. Methods like FUDGE and PPLM that use auxiliary models trained on different data appear more likely to suffer from distribution mismatch issues.- The framework is model-agnostic and black-box, applicable to any base language model without refactoring. Related work is often more tailored to specific model architectures. The NADO model itself is also quite simple (standard seq2seq).- Experiments demonstrate the framework can handle different types of control objectives, like lexical constraints and formality changes in translation. The results significantly outperform prior specialized methods in terms of generation quality.Overall, the paper introduces a simple but general framework for controllable generation that relies only on sequence-level supervision. The theoretical analysis and emphasis on distribution matching via sampling seem like notable contributions compared to related work. The model-agnostic black-box approach is also distinctive.


## What future research directions do the authors suggest?

Based on my reading of the paper, here are some of the main future research directions suggested by the authors:- Exploring different methods for decomposing and approximating the sequence-level oracle, beyond the neural network approach proposed in this work. The authors suggest analyzing properties of different oracles to understand which are more amenable to decomposition and approximation.- Applying the proposed framework to broader application scenarios, such as reducing societal biases like gender or racial bias in text generation. The generality of the framework makes it well-suited for these types of controls.- Analyzing the impact of base model quality on the improvement gained from the proposed NADO method. The authors suggest NADO is able to better guide higher quality base models, so investigating this relationship could be informative.- Extending the framework to handle soft constraints, which allow controlling the generation to meet constraints only some of the time rather than always. The authors provide some initial theoretical results in this direction.- Exploring the connection between the optimization approach proposed here and reinforcement learning methods. The appendix mentions this connection so it could be interesting to further develop.- Applying the method to base models beyond pretrained language models, to continue demonstrating the generality of the framework across model types and architectures.So in summary, the main suggested directions are enhancing the decomposition and approximation components, applying the method to new use cases and constraints, better understanding its theoretical properties, and extending it to other base model types. The overall focus is on developing, analyzing and demonstrating the flexibility of the proposed framework.


## Summarize the paper in one paragraph.

Here is a one paragraph summary of the paper:The paper proposes a framework called Neurally-Decomposed Oracle (NADO) for controllable text generation with large pre-trained auto-regressive language models like GPT-2. Given a pre-trained base model and a sequence-level oracle function indicating whether a desired attribute is satisfied, NADO decomposes the oracle into token-level guidance to steer the base model during text generation. Specifically, an auxiliary neural model is trained to approximate the token-level guidance using examples sampled from the base model, without needing additional labeled data. During generation, the token distribution from the base model is modified at each step based on the guidance from NADO to satisfy the oracle constraints. Theoretical analysis is provided on how the approximation quality of NADO affects the controllable generation results. Experiments on lexical constrained text generation and machine translation with formality control show that NADO can effectively guide the base model to satisfy specified attributes while maintaining high quality and fluency. The framework is general, flexible, and does not require refactoring or fine-tuning the base model.


## Summarize the paper in two paragraphs.

Here is a two paragraph summary of the paper:This paper proposes a framework called NeurAlly-Decomposed Oracle (NADO) for controllable text generation with auto-regressive models like GPT. The key idea is to decompose a sequence-level oracle function that specifies a desired attribute (e.g. keywords that should appear in the text) into token-level guidance signals. Specifically, they formulate the sequence generation process as an optimization problem based on posterior regularization. The optimal token-level guidance signals that conform to the sequence-level oracle are derived in closed form, but are intractable to compute exactly. Therefore, NADO is proposed, which is a small neural network trained to approximate the token-level guidance signals. It is trained on examples sampled from the base model to be controlled, using the sequence-level oracle function as supervision. At inference time, NADO provides the token-level guidance signals to modify the token distributions output by the base model at each step, in order to bias it towards satisfying the sequence-level oracle. Experiments on lexical constrained text generation and machine translation with formality control demonstrate that NADO can efficiently guide the base model to satisfy the required attributes while maintaining high quality generations.In summary, the key ideas proposed in this paper are: 1) Formulating controllable text generation as an optimization problem with posterior regularization to derive optimal token-level guidance signals from a sequence-level oracle. 2) Using a small neural network called NADO to approximate the intractable token-level guidance signals. 3) Training NADO on examples sampled from the base model to align it well and avoid distribution mismatch issues. 4) Using NADO to modify the base model's token distributions at each step to satisfy the sequence-level control objective. Experiments show this is an efficient way to achieve control over large autoregressive models like GPT while maintaining generation quality.


## Summarize the main method used in the paper in one paragraph.

Here is a one paragraph summary of the main method used in the paper:The paper proposes a framework called NeurAlly-Decomposed Oracle (NADO) for controllable text generation with auto-regressive language models. Given a pre-trained language model and a sequence-level oracle function indicating whether a text satisfies certain constraints, NADO decomposes the oracle into token-level guidance. Specifically, it trains an auxiliary neural model to approximate the optimal token-level distributions that incorporate the oracle constraints, using examples sampled from the base language model. During inference, the auxiliary model provides token-level guidance to modify the distributions from the base model to generate sequences satisfying the constraints. The auxiliary model is trained with a cross-entropy loss to match the ground-truth token-level guidance, as well as a regularizer to enforce auto-regressive consistency. Theoretical analysis is provided on how the approximation quality affects the controllability. Experiments on lexical constrained text generation and machine translation with formality control demonstrate the effectiveness of NADO in guiding the base model towards satisfying given constraints while maintaining high generation quality.


## What problem or question is the paper addressing?

Based on my reading of the paper, it appears to be addressing the problem of how to control auto-regressive language models to satisfy certain desired attributes during text generation. Some key aspects:- Auto-regressive language models like GPT-2 have shown impressive text generation capabilities, but controlling the attributes of the generated text remains challenging. Approaches like fine-tuning require a lot of data and are inefficient with large models. - The paper proposes a method to steer a pre-trained auto-regressive model to satisfy given sequence-level attributes specified by an "oracle" function. The key ideas are:1) Decomposing the sequence-level oracle into token-level guidance that can guide the model's output distribution at each step.2) Approximating the token-level guidance using an auxiliary neural model called NADO, trained on data sampled from the base model.3) Incorporating NADO into the base model using a probabilistic formulation to modify the output distribution based on the guidance.- This allows flexible control over text generation attributes based on any given sequence-level oracle, without refining or retraining the base model.- Theoretical analysis is provided on how the approximation quality of NADO affects the controllable generation performance.- Experiments on lexical constrained text generation and machine translation with formality control demonstrate that the proposed method can efficiently guide the base model to satisfy desired attributes while maintaining generation quality.In summary, the key problem addressed is how to control text generation attributes from large auto-regressive language models in an efficient and flexible way using limited supervision. The proposed method provides a way to do this by decomposing and approximating the control objective for incorporation into the base model.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Based on my reading of the paper text, here is a one sentence summary: The paper proposes a framework called NeurAlly-Decomposed Oracle (NADO) to control auto-regressive text generation models by decomposing sequence-level oracle supervision into token-level guidance approximated by a neural network trained on examples sampled from the base model.
