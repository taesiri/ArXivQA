# Controllable Text Generation with Neurally-Decomposed Oracle

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper text, it seems the central research question is how to control auto-regressive language models to satisfy desired attributes during text generation. The key ideas and contributions in addressing this research question include:- Proposing a framework to decompose sequence-level oracle supervision into token-level guidance for controlling text generation models. This allows steering the base auto-regressive model towards satisfying the desired attributes.- The token-level guidance is approximated by an auxiliary neural model called NADO which is trained on data sampled from the base model. This helps align NADO better with the base model's distribution.- Providing theoretical analysis on how the approximation quality of NADO affects the controllable generation results. - Demonstrating the effectiveness of the proposed framework on two tasks - lexically constrained text generation and machine translation with formality control. The results show the framework can efficiently guide the base model to satisfy specified attributes while maintaining high text quality.So in summary, the central hypothesis is that by decomposing sequence-level constraints into token-level guidance and approximating it via a NADO model trained on base model samples, we can achieve effective control over text generation attributes without compromising quality or modifying the base model. The experimental results on constrained text generation and machine translation tasks seem to validate this hypothesis.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contributions seem to be:- Proposing a general framework (NADO) for controllable text generation that can decompose sequence-level oracles into token-level guidance to steer auto-regressive language models.- Theoretical analysis showing how the approximation quality of NADO affects the controllable generation results.- An optimal closed-form solution for incorporating the token-level guidance into the base model for controllable generation, based on posterior regularization. - Demonstrating the effectiveness of the proposed framework on two tasks: lexically constrained text generation and machine translation with formality control. The results show the framework can efficiently guide the base model to satisfy specified attributes while maintaining high quality.- The framework is flexible, treating the base model and oracle functions as black boxes. It does not require refactoring or fine-tuning the base model.- NADO is trained on data sampled from the base model rather than requiring auxiliary labeled data. This helps align NADO better with the base model.In summary, the key contribution appears to be proposing a general, flexible framework for controllable text generation that decomposes sequence-level constraints into token-level guidance for steering language models, with both theoretical analysis and empirical demonstrations of effectiveness. The framework is model-agnostic and data-efficient.
