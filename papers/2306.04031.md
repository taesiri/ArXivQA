# [Certified Reasoning with Language Models](https://arxiv.org/abs/2306.04031)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be: How can we develop methods that combine the flexibility of language models with the reliability of formal logical reasoning? More specifically, the paper is investigating how to integrate language models like GPT-3 with formal theorem proving tools in order to perform more reliable and verifiable multi-step reasoning. The key ideas explored are:- Developing "guide" tools that language models can invoke to constrain generation to valid outputs according to logical rules. - Implementing a specific guide called LogicGuide that interfaces with an external theorem prover and allows a language model to make certifiably sound inferences.- Evaluating LogicGuide on question answering and reasoning tasks to see if it improves accuracy and mitigates issues like content bias. - Using LogicGuide for safer self-improvement of models by training only on certified reasoning chains.So in summary, the central hypothesis is that guide tools like LogicGuide can make language model reasoning more accurate and trustworthy by integrating symbolic logical guarantees while retaining the flexibility of natural language generation. The experiments aim to validate whether LogicGuide actually achieves those goals in practice.


## What is the main contribution of this paper?

The main contribution of this paper seems to be introducing a framework of "guide tools" for language models, and specifically a tool called LogicGuide that allows language models to perform logically sound reasoning. The key ideas are:- Guide tools allow language models to invoke external constraints during text generation. When a guide tool is triggered, it defines a set of valid outputs the language model can generate.- LogicGuide is a specific guide tool that interfaces with an external logical reasoning system. It allows the language model to formalize assumptions, set proof goals, and make logically valid inferences. - LogicGuide significantly improves the accuracy of language models like GPT-3 and LLaMA on multi-step natural language reasoning tasks. It helps avoid unsound inferences and reduces problematic "content effects".- LogicGuide enables more effective self-improvement of language models through learning from their own certified reasoning chains. This avoids reinforcing possible reasoning mistakes.Overall, guide tools like LogicGuide provide a way to combine the flexibility of large language models with the reliability of formal logical systems for improved reasoning capabilities. The main contribution is showing how this hybrid approach can work in practice.
