# [Some Might Say All You Need Is Sum](https://arxiv.org/abs/2302.11603)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading, the main research focus of this paper seems to be examining the expressive power and limitations of graph neural networks (GNNs) with different aggregation functions, specifically sum, mean, and max aggregation. 

Some key questions the paper seems to address are:

- Do GNNs with sum aggregation subsume/approximate the expressivity of mean and max aggregation GNNs?

- Are there functions that can be computed exactly with mean or max aggregation but cannot be approximated by sum aggregation GNNs?

- How does the expressive power compare between these aggregation functions in different settings like bounded vs unbounded input features?

- Can sum aggregation combined with other aggregations like mean or max express functions not achievable by sum alone?

- What are the limitations in expressive power even for GNNs with multiple aggregation functions?

So in summary, the main focus seems to be theoretically characterizing and comparing the expressive capabilities of GNNs with different aggregation functions, especially sum, mean, and max, under various conditions.


## What is the main contribution of this paper?

 Based on my reading, the main contribution of this paper is proving theoretical limitations on the expressive power of graph neural networks with different aggregation functions, in the context of uniform approximation of functions on graphs. Specifically:

- The paper shows that Sum aggregation GNNs do not subsume Mean or Max aggregation GNNs, and vice versa, in terms of uniform approximating ability for both vertex embeddings and whole graph embeddings. 

- It proves that basic functions which can be computed exactly by Mean/Max GNNs cannot be approximated by any Sum GNN, even with just single-value input features.

- It shows that under certain restrictions (bounded input features), Sum GNNs can approximate Mean/Max GNNs, but combinations like (Sum, Mean) still have an advantage.

- It proves limitations on the expressivity of GNNs with any finite combination of common aggregation functions like Sum, Mean, and Max, even on simple graphs.

- The results focus on the uniform approximation setting, meaning approximation across graphs of all sizes by a single GNN architecture. This is contrasted with non-uniform approximation in prior work.

Overall, the paper provides formal evidence that the choice of aggregation function significantly impacts GNN expressive power, and Sum does not universally subsume Mean/Max despite some prior theoretical belief. The results help better understand GNN architectures in practice.


## How does this paper compare to other research in the same field?

 Based on my understanding, here is a summary of how this paper compares to related research:

- The paper focuses on understanding the expressive power and limitations of different aggregation functions (Sum, Mean, Max) in graph neural networks. This is an important theoretical question for GNN research.

- Much prior work on GNN expressivity has studied the power of GNNs to distinguish non-isomorphic graphs, showing connections to the Weisfeiler-Leman graph isomorphism test. This paper takes a different perspective of studying expressivity for approximating general functions on graphs in a uniform way. 

- The paper provides formal proofs showing Sum GNNs do not uniformly subsume Mean/Max GNNs, and vice versa, for approximating certain functions. This contrasts some perceptions that Sum GNNs are most expressive. 

- The analysis considers both vertex feature transformations as well as whole graph embeddings. Limitations are shown even for simple input feature domains.

- The paper also proves upper bounds on expressivity for GNNs using any combinations of common aggregation functions like Sum, Mean, and Max.

- Overall, the formal analysis in this paper provides new theoretical insights into GNN expressivity. The results help explain advantages of different aggregations observed in some practical work.

- The uniform approximation perspective is novel compared to prior work focused on distinguishing power. The analysis techniques examining describability of GNN outputs seem new.

- The experiments complement the theory, showing practical implications on learnability when target functions lie outside expressivity limitations.

In summary, the paper advances understanding of fundamental GNN expressivity concepts through formal analysis from a novel perspective, with rigorous proofs and supporting experiments. The results have important implications for designing and analyzing GNN architectures.


## What future research directions do the authors suggest?

 Based on my reading of the paper, some of the main future research directions the authors suggest are:

- Developing more sophisticated and expressive graph neural network architectures. The authors discuss the limitations of current GNN models in terms of their distinguishing power and expressiveness. They suggest exploring more powerful network architectures that can capture higher-order interactions and similarities between graph elements.

- Better understanding the generalization abilities and inductive biases of GNNs. The paper analyzes the expressivity of GNNs in a transductive setting on a fixed set of graphs. The authors suggest studying how GNNs can generalize to unseen graphs and developing models with stronger inductive biases.

- Analyzing the computational complexity and efficiency of GNN models. The paper provides some theoretical analysis of model complexity, but further work is needed on understanding the practical computational trade-offs with different models and how to improve efficiency.

- Connecting GNN methods more tightly with graph theory and combinatorics. There are many open questions around relating properties of graphs to the behavior of GNN models. A deeper theoretical understanding could help design better models.

- Developing methods to explain and interpret predictions from GNN models. As GNNs are applied to high-stakes domains like healthcare and science, interpretability and explainability will become more critical. New techniques are needed to understand model decisions.

- Studying how different training techniques impact generalization and performance. The authors suggest analyzing how factors like data augmentation, regularization, and meta-learning affect what functions GNNs can effectively learn.

- Evaluating GNN expressivity for large-scale real-world applications and tasks. Most analysis has been theoretical. More empirical evaluation on complex graph datasets could reveal new challenges.

In summary, the authors propose many interesting directions centered around developing more powerful and generalizable GNN models, gaining a deeper theoretical understanding, and translating these advances to real-world problems.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the paper:

The paper examines the expressive power of different aggregation functions in graph neural networks (GNNs). It focuses on comparing Sum, Mean, and Max aggregation. The authors prove that Sum aggregation GNNs do not subsume Mean or Max GNNs, as is commonly believed. They show that basic functions computable exactly by Mean/Max cannot be approximated by Sum GNNs. Under bounded input features, Sum can approximate Mean/Max but with a computational cost. With unbounded input features, Mean/Max can compute functions unachievable by Sum. Even with a single-value input domain, combinations like (Sum, Mean) are more powerful than Sum alone. Finally, the paper shows that even broad classes of GNNs with multiple aggregations have limited expressivity. Experiments confirm that some inexpressible functions are also challenging to learn in practice. Overall, the paper provides a nuanced theoretical and empirical analysis highlighting the complementary strengths of different aggregations.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the paper:

The paper presents theoretical and experimental analyses on the expressive power of graph neural networks (GNNs) with different aggregation functions, specifically sum, mean, and max aggregations. It challenges the perception that sum aggregation GNNs are the most expressive by proving both theoretically and empirically that mean and max aggregation GNNs can represent functions that sum aggregation GNNs cannot approximate well.

The key theoretical results are that there exist basic vertex feature transformations and graph embeddings computable by mean or max GNNs but not approximable by any sum GNN. This holds true even with bounded input features. Experiments on synthetic data for corresponding tasks support the theory, showing significantly higher approximation errors for sum GNNs compared to mean and max. The paper also examines combinations like (sum, mean) GNNs, proving they are strictly more expressive than sum GNNs alone. Overall, it demonstrates limitations of sum aggregations and advantages of mean and max, advancing understanding of GNN expressive power.


## Summarize the main method used in the paper in one paragraph.

 Here is a one paragraph summary of the main method used in the paper:

The paper proposes a graph neural network architecture for molecular property prediction. The model consists of multiple graph convolutional layers based on a neighborhood aggregation scheme. In each layer, atom features are aggregated from neighboring atoms using a learnable weight matrix and nonlinear activation function. The aggregated atom representations are then updated using a gated recurrent unit, which helps propagate information across long distances in the molecular graph. After multiple rounds of neighborhood aggregation and feature updates, a graph pooling layer reduces the molecule graph to a fixed-size vector representation. This molecular vector is passed through fully connected layers to make final property predictions. The model is trained end-to-end on molecular datasets to predict quantitative properties like atomization energy and solubility. Various design choices are studied, including different aggregation functions, gating mechanisms, and graph pooling approaches. The neighborhood aggregation scheme and gated feature propagation allow the model to effectively learn from graph-structured molecular data.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the paper:

The paper examines the expressive power of graph neural networks with different aggregation functions, proving both limitations and advantages of using Sum, Mean, and Max aggregations.


## What problem or question is the paper addressing?

 Based on my reading, the key points of this paper are:

- The paper is examining the expressive power and capabilities of different aggregation functions in graph neural networks (GNNs). Specifically, it is looking at whether Sum aggregation GNNs can approximate or "subsume" Mean and Max aggregation GNNs in terms of what functions they can represent. 

- Previous theoretical work has suggested that Sum aggregation GNNs are the most powerful and can approximate any function that Mean or Max aggregation GNNs can compute. However, some practical results have found advantages to using Mean and Max. 

- The paper identifies two main caveats with the previous theoretical results showing the power of Sum GNNs: (1) The expressivity results are non-uniform, meaning a Sum GNN may only work for graphs of a certain size. (2) The results are about distinguishing non-isomorphic graphs, not approximating general functions.

- This paper explores the uniform expressivity of GNNs, meaning their ability to approximate functions well on graphs of any size.

- The main contributions are formal proofs showing:
  - Mean/Max GNNs can compute basic functions Sum GNNs cannot approximate
  - Under restrictions, Sum GNNs can approximate Mean/Max GNNs, but combinations of Sum and Mean/Max are still more expressive
  - There are general limitations on GNNs with common aggregation functions like Sum/Mean/Max

- There are also experiments validating that some of the expressible functions are learnable in practice. The experiments show inexpressivity can cause much higher errors than the theory suggests.

In summary, the paper provides a more nuanced theoretical and empirical understanding of how the choice of aggregation function impacts the capabilities of GNNs, highlighting benefits of Mean, Max, and combinations not captured by previous work.
