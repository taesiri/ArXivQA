# [Some Might Say All You Need Is Sum](https://arxiv.org/abs/2302.11603)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading, the main research focus of this paper seems to be examining the expressive power and limitations of graph neural networks (GNNs) with different aggregation functions, specifically sum, mean, and max aggregation. 

Some key questions the paper seems to address are:

- Do GNNs with sum aggregation subsume/approximate the expressivity of mean and max aggregation GNNs?

- Are there functions that can be computed exactly with mean or max aggregation but cannot be approximated by sum aggregation GNNs?

- How does the expressive power compare between these aggregation functions in different settings like bounded vs unbounded input features?

- Can sum aggregation combined with other aggregations like mean or max express functions not achievable by sum alone?

- What are the limitations in expressive power even for GNNs with multiple aggregation functions?

So in summary, the main focus seems to be theoretically characterizing and comparing the expressive capabilities of GNNs with different aggregation functions, especially sum, mean, and max, under various conditions.


## What is the main contribution of this paper?

 Based on my reading, the main contribution of this paper is proving theoretical limitations on the expressive power of graph neural networks with different aggregation functions, in the context of uniform approximation of functions on graphs. Specifically:

- The paper shows that Sum aggregation GNNs do not subsume Mean or Max aggregation GNNs, and vice versa, in terms of uniform approximating ability for both vertex embeddings and whole graph embeddings. 

- It proves that basic functions which can be computed exactly by Mean/Max GNNs cannot be approximated by any Sum GNN, even with just single-value input features.

- It shows that under certain restrictions (bounded input features), Sum GNNs can approximate Mean/Max GNNs, but combinations like (Sum, Mean) still have an advantage.

- It proves limitations on the expressivity of GNNs with any finite combination of common aggregation functions like Sum, Mean, and Max, even on simple graphs.

- The results focus on the uniform approximation setting, meaning approximation across graphs of all sizes by a single GNN architecture. This is contrasted with non-uniform approximation in prior work.

Overall, the paper provides formal evidence that the choice of aggregation function significantly impacts GNN expressive power, and Sum does not universally subsume Mean/Max despite some prior theoretical belief. The results help better understand GNN architectures in practice.


## How does this paper compare to other research in the same field?

 Based on my understanding, here is a summary of how this paper compares to related research:

- The paper focuses on understanding the expressive power and limitations of different aggregation functions (Sum, Mean, Max) in graph neural networks. This is an important theoretical question for GNN research.

- Much prior work on GNN expressivity has studied the power of GNNs to distinguish non-isomorphic graphs, showing connections to the Weisfeiler-Leman graph isomorphism test. This paper takes a different perspective of studying expressivity for approximating general functions on graphs in a uniform way. 

- The paper provides formal proofs showing Sum GNNs do not uniformly subsume Mean/Max GNNs, and vice versa, for approximating certain functions. This contrasts some perceptions that Sum GNNs are most expressive. 

- The analysis considers both vertex feature transformations as well as whole graph embeddings. Limitations are shown even for simple input feature domains.

- The paper also proves upper bounds on expressivity for GNNs using any combinations of common aggregation functions like Sum, Mean, and Max.

- Overall, the formal analysis in this paper provides new theoretical insights into GNN expressivity. The results help explain advantages of different aggregations observed in some practical work.

- The uniform approximation perspective is novel compared to prior work focused on distinguishing power. The analysis techniques examining describability of GNN outputs seem new.

- The experiments complement the theory, showing practical implications on learnability when target functions lie outside expressivity limitations.

In summary, the paper advances understanding of fundamental GNN expressivity concepts through formal analysis from a novel perspective, with rigorous proofs and supporting experiments. The results have important implications for designing and analyzing GNN architectures.


## What future research directions do the authors suggest?

 Based on my reading of the paper, some of the main future research directions the authors suggest are:

- Developing more sophisticated and expressive graph neural network architectures. The authors discuss the limitations of current GNN models in terms of their distinguishing power and expressiveness. They suggest exploring more powerful network architectures that can capture higher-order interactions and similarities between graph elements.

- Better understanding the generalization abilities and inductive biases of GNNs. The paper analyzes the expressivity of GNNs in a transductive setting on a fixed set of graphs. The authors suggest studying how GNNs can generalize to unseen graphs and developing models with stronger inductive biases.

- Analyzing the computational complexity and efficiency of GNN models. The paper provides some theoretical analysis of model complexity, but further work is needed on understanding the practical computational trade-offs with different models and how to improve efficiency.

- Connecting GNN methods more tightly with graph theory and combinatorics. There are many open questions around relating properties of graphs to the behavior of GNN models. A deeper theoretical understanding could help design better models.

- Developing methods to explain and interpret predictions from GNN models. As GNNs are applied to high-stakes domains like healthcare and science, interpretability and explainability will become more critical. New techniques are needed to understand model decisions.

- Studying how different training techniques impact generalization and performance. The authors suggest analyzing how factors like data augmentation, regularization, and meta-learning affect what functions GNNs can effectively learn.

- Evaluating GNN expressivity for large-scale real-world applications and tasks. Most analysis has been theoretical. More empirical evaluation on complex graph datasets could reveal new challenges.

In summary, the authors propose many interesting directions centered around developing more powerful and generalizable GNN models, gaining a deeper theoretical understanding, and translating these advances to real-world problems.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the paper:

The paper examines the expressive power of different aggregation functions in graph neural networks (GNNs). It focuses on comparing Sum, Mean, and Max aggregation. The authors prove that Sum aggregation GNNs do not subsume Mean or Max GNNs, as is commonly believed. They show that basic functions computable exactly by Mean/Max cannot be approximated by Sum GNNs. Under bounded input features, Sum can approximate Mean/Max but with a computational cost. With unbounded input features, Mean/Max can compute functions unachievable by Sum. Even with a single-value input domain, combinations like (Sum, Mean) are more powerful than Sum alone. Finally, the paper shows that even broad classes of GNNs with multiple aggregations have limited expressivity. Experiments confirm that some inexpressible functions are also challenging to learn in practice. Overall, the paper provides a nuanced theoretical and empirical analysis highlighting the complementary strengths of different aggregations.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the paper:

The paper presents theoretical and experimental analyses on the expressive power of graph neural networks (GNNs) with different aggregation functions, specifically sum, mean, and max aggregations. It challenges the perception that sum aggregation GNNs are the most expressive by proving both theoretically and empirically that mean and max aggregation GNNs can represent functions that sum aggregation GNNs cannot approximate well.

The key theoretical results are that there exist basic vertex feature transformations and graph embeddings computable by mean or max GNNs but not approximable by any sum GNN. This holds true even with bounded input features. Experiments on synthetic data for corresponding tasks support the theory, showing significantly higher approximation errors for sum GNNs compared to mean and max. The paper also examines combinations like (sum, mean) GNNs, proving they are strictly more expressive than sum GNNs alone. Overall, it demonstrates limitations of sum aggregations and advantages of mean and max, advancing understanding of GNN expressive power.


## Summarize the main method used in the paper in one paragraph.

 Here is a one paragraph summary of the main method used in the paper:

The paper proposes a graph neural network architecture for molecular property prediction. The model consists of multiple graph convolutional layers based on a neighborhood aggregation scheme. In each layer, atom features are aggregated from neighboring atoms using a learnable weight matrix and nonlinear activation function. The aggregated atom representations are then updated using a gated recurrent unit, which helps propagate information across long distances in the molecular graph. After multiple rounds of neighborhood aggregation and feature updates, a graph pooling layer reduces the molecule graph to a fixed-size vector representation. This molecular vector is passed through fully connected layers to make final property predictions. The model is trained end-to-end on molecular datasets to predict quantitative properties like atomization energy and solubility. Various design choices are studied, including different aggregation functions, gating mechanisms, and graph pooling approaches. The neighborhood aggregation scheme and gated feature propagation allow the model to effectively learn from graph-structured molecular data.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the paper:

The paper examines the expressive power of graph neural networks with different aggregation functions, proving both limitations and advantages of using Sum, Mean, and Max aggregations.


## What problem or question is the paper addressing?

 Based on my reading, the key points of this paper are:

- The paper is examining the expressive power and capabilities of different aggregation functions in graph neural networks (GNNs). Specifically, it is looking at whether Sum aggregation GNNs can approximate or "subsume" Mean and Max aggregation GNNs in terms of what functions they can represent. 

- Previous theoretical work has suggested that Sum aggregation GNNs are the most powerful and can approximate any function that Mean or Max aggregation GNNs can compute. However, some practical results have found advantages to using Mean and Max. 

- The paper identifies two main caveats with the previous theoretical results showing the power of Sum GNNs: (1) The expressivity results are non-uniform, meaning a Sum GNN may only work for graphs of a certain size. (2) The results are about distinguishing non-isomorphic graphs, not approximating general functions.

- This paper explores the uniform expressivity of GNNs, meaning their ability to approximate functions well on graphs of any size.

- The main contributions are formal proofs showing:
  - Mean/Max GNNs can compute basic functions Sum GNNs cannot approximate
  - Under restrictions, Sum GNNs can approximate Mean/Max GNNs, but combinations of Sum and Mean/Max are still more expressive
  - There are general limitations on GNNs with common aggregation functions like Sum/Mean/Max

- There are also experiments validating that some of the expressible functions are learnable in practice. The experiments show inexpressivity can cause much higher errors than the theory suggests.

In summary, the paper provides a more nuanced theoretical and empirical understanding of how the choice of aggregation function impacts the capabilities of GNNs, highlighting benefits of Mean, Max, and combinations not captured by previous work.


## What are the keywords or key terms associated with this paper?

 Based on my review, some of the key terms and concepts in this paper include:

- Graph neural networks (GNNs) - The paper examines the expressivity and approximation power of different types of graph neural networks, which are a popular deep learning method for graph-structured data.

- Aggregation functions - A key component of GNNs are the aggregation functions, such as sum, mean, and max, that aggregate neighbor information. The choice of aggregation impacts the expressivity.

- Uniform vs non-uniform expressivity - The paper makes a distinction between uniform expressivity (one model works for all graph sizes) and non-uniform (different model per size). It focuses on uniform notions. 

- Subsumption - Whether one class of GNNs can uniformly approximate or "subsume" another class by choosing appropriate parameters.

- Approximation - The paper examines additive approximation of functions on graphs by different GNN classes.

- Theoretical analysis - The paper gives theoretical proofs and analysis, rather than empirical results, for the expressivity and approximation powers of different GNN classes.

So in summary, the key focus is on theoretical analysis of uniform expressivity and approximation abilities of graph neural networks using different aggregation functions. Key terms include aggregation, uniform expressivity, subsumption, and approximation.


## Based on the paper, what questions should we ask to create a comprehensive summary of it? Please list at least 10 questions.

 Here are 10 potential questions to ask to create a comprehensive summary of the paper:

1. What is the main purpose or research question of the paper? 

2. What methods did the authors use to address the research question? What data did they analyze?

3. What were the key findings or results of the study? 

4. Did the results support or contradict the original hypothesis or expectations?

5. What are the main conclusions of the paper? How do the authors interpret the results?

6. What are the limitations of the study that the authors acknowledge?

7. How does this study relate to previous work in the field? How does it support or challenge prior research?  

8. What are the main contributions or significance of this work?

9. What future research directions do the authors suggest based on this study?

10. Are there any ethical considerations related to the research that should be discussed?

Asking these types of questions should help summarize the key information about the purpose, methodology, findings, conclusions, limitations, and significance of the research. Additional questions could probe deeper into the specific details and analyses presented depending on the paper topic and content. The goal is to understand the big picture as well as the important details.


## Design 10 in-depth questions about method proposed in this paper and avoid simple question.

 Here are 10 potential in-depth questions about the method proposed in the paper:

1. The paper proposes an approach for graph neural networks based on using sum aggregation. What are the key advantages and disadvantages of using sum aggregation compared to other aggregation functions like mean or max pooling?

2. The paper shows sum aggregation GNNs can approximate mean and max aggregation GNNs under certain conditions. What are these conditions and why do they enable this approximation? How broadly applicable are these conditions?

3. The paper proves there are functions computable by mean/max GNNs that cannot be approximated by sum GNNs in certain cases. Can you explain one of these cases and the intuition behind why sum GNNs fail? 

4. What modifications or extensions to the sum GNN architecture proposed in the paper could potentially increase its expressive power? For example, allowing multiple aggregation functions or more complex neural network layers.

5. The paper focuses on theoretical expressivity, but how might the choice of aggregation impact practical performance and trainability of GNN models? Does the theory align with observed empirical results?

6. The paper uses specific synthetic graph datasets in the experiments. How could the experimental validation be strengthened by using more real-world graph data? What factors should be considered in designing experiments?

7. The paper defines uniform vs non-uniform expressivity. Why is uniform expressivity important for graph learning? What are the limitations of only considering non-uniform expressivity?

8. How does the notion of uniform approximation in this paper compare to the universal approximation theorems for neural networks? What parallels can be drawn?

9. The paper proves limitations on the expressivity of GNNs using certain aggregation functions. Do you think these limitations can be overcome by using much deeper GNN models? Why or why not?

10. The paper focuses on sum, mean and max aggregation but there are other aggregators like LSTM or pooling operations. How might the theoretical analysis need to change to handle more complex aggregators?


## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a high-quality paragraph summarizing the key points of the paper:

This paper examines the expressive power of graph neural networks (GNNs) using different aggregation functions, focusing on the uniform notion of expressivity where a single GNN must be able to handle graphs of any size. It proves that neither Sum nor Mean/Max aggregation GNNs subsume one another in terms of expressivity. Specifically, basic functions computable by Mean/Max cannot be approximated by Sum GNNs, while with unbounded input features, functions computable by Mean/Max cannot be approximated by Sum. However, with bounded input features, Sum GNNs can approximate Mean/Max GNNs, though at a size cost. Interestingly, even with single-value input features, combinations of Sum and Mean/Max are strictly more expressive than Sum alone. Experiments on synthetic data demonstrate that in practice, limitations in expressivity often translate to higher error than the theory suggests. Overall, the paper provides a rigorous characterization of GNN expressivity, highlighting limitations of Sum and advantages of Mean/Max aggregations, dispelling common beliefs about their relative power. The results have significant implications for designing more powerful GNN architectures in the future.


## Summarize the paper in one sentence.

 This paper proves that Sum, Mean, and Max Graph Neural Networks have incomparable expressive power under uniform approximation of graph functions.


## Summarize the paper in one paragraphs.

 Here is a one paragraph summary of the key points made in the paper:

This paper explores the expressive power of different graph neural network (GNN) aggregation functions, focusing on the common choices of Sum, Mean, and Max aggregation. It proves that in the uniform expressivity setting, where a single GNN must work on graphs of all sizes, Sum GNNs do not strictly subsume Mean/Max GNNs nor vice versa. Specifically, the paper shows basic functions like neighbor count that are easy for Sum GNNs are inexpressible for Mean/Max GNNs. However, other functions on unbounded input features are expressible for Mean/Max but not Sum GNNs. Even on bounded input features, the paper proves (Sum, Mean) GNNs outperform Sum GNNs alone. Experiments on synthetic data confirm Sum GNNs fail to generalize OOD, whereas Mean GNNs succeed. Overall, the paper makes important theoretical contributions clarifying the incomparable expressive power of different GNN aggregations, with implications for improved model design.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 potential in-depth questions about the method proposed in this paper:

1. The paper shows that Mean and Max GNNs cannot approximate the neighbors-sum function, which Sum GNNs can compute exactly. Why is this function inexpressible by Mean and Max GNNs? What property of the neighbors-sum function enables Sum GNNs to represent it?

2. The paper proves that Sum GNNs can approximate Mean and Max GNNs given bounded input features. What is the key idea behind the construction that allows Sum GNNs to emulate Mean/Max under these conditions? What are the limitations of this approximation result?

3. The paper shows that with unbounded input features, Mean/Max GNNs can compute functions that are inapproximable by Sum GNNs. What example function do they use to prove this? Why does allowing unbounded features enable new expressivity for Mean/Max but not for Sum?

4. Even with a finite input domain, the paper shows that a combination of Sum and Mean/Max can be more expressive than Sum alone. What graph construction do they use to prove this? Why can't Sum GNNs approximate the function defined on this graph construction?

5. The paper defines "uniform expressivity" and contrasts it with "non-uniform expressivity" used in some prior work. What is the difference between these notions? Why is uniform expressivity a stronger theoretical guarantee?

6. How does the paper formally define the approximation and subsumption relations between different GNN classes? What are some subtleties in comparing GNN expressivity based on these definitions?

7. The paper examines graph embedding functions in addition to vertex embedding functions. Why is graph embedding expressivity an important complementary notion to vertex embedding expressivity? 

8. How does the paper extend their characterization from Mean/Max GNNs to the broader class of Multiple Uniform Polynomial Aggregation (MUPA) GNNs? What limitation do they prove on this larger class?

9. What graph constructions are introduced in the paper? How do properties of these graphs allow proving limitations on different GNN classes?

10. The paper includes some experiments on synthetic data. How well do the practical results align with the theoretical limitations proved in the paper? What are some ways the experiments provide additional insights?
