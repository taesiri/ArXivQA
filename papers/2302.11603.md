# [Some Might Say All You Need Is Sum](https://arxiv.org/abs/2302.11603)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the main research focus of this paper seems to be examining the expressive power and limitations of graph neural networks (GNNs) with different aggregation functions, specifically sum, mean, and max aggregation. Some key questions the paper seems to address are:- Do GNNs with sum aggregation subsume/approximate the expressivity of mean and max aggregation GNNs?- Are there functions that can be computed exactly with mean or max aggregation but cannot be approximated by sum aggregation GNNs?- How does the expressive power compare between these aggregation functions in different settings like bounded vs unbounded input features?- Can sum aggregation combined with other aggregations like mean or max express functions not achievable by sum alone?- What are the limitations in expressive power even for GNNs with multiple aggregation functions?So in summary, the main focus seems to be theoretically characterizing and comparing the expressive capabilities of GNNs with different aggregation functions, especially sum, mean, and max, under various conditions.


## What is the main contribution of this paper?

Based on my reading, the main contribution of this paper is proving theoretical limitations on the expressive power of graph neural networks with different aggregation functions, in the context of uniform approximation of functions on graphs. Specifically:- The paper shows that Sum aggregation GNNs do not subsume Mean or Max aggregation GNNs, and vice versa, in terms of uniform approximating ability for both vertex embeddings and whole graph embeddings. - It proves that basic functions which can be computed exactly by Mean/Max GNNs cannot be approximated by any Sum GNN, even with just single-value input features.- It shows that under certain restrictions (bounded input features), Sum GNNs can approximate Mean/Max GNNs, but combinations like (Sum, Mean) still have an advantage.- It proves limitations on the expressivity of GNNs with any finite combination of common aggregation functions like Sum, Mean, and Max, even on simple graphs.- The results focus on the uniform approximation setting, meaning approximation across graphs of all sizes by a single GNN architecture. This is contrasted with non-uniform approximation in prior work.Overall, the paper provides formal evidence that the choice of aggregation function significantly impacts GNN expressive power, and Sum does not universally subsume Mean/Max despite some prior theoretical belief. The results help better understand GNN architectures in practice.
