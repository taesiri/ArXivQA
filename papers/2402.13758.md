# [Factual Consistency Evaluation of Summarisation in the Era of Large   Language Models](https://arxiv.org/abs/2402.13758)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Assessing factual consistency (FC) of automatically generated text summaries is important to avoid spreading misinformation. 
- Existing FC evaluation methods have limitations in performance, efficiency, explainability and generalizability to summaries generated by large language models (LLMs).
- There is a lack of FC evaluation benchmarks for LLM-generated summaries, especially in domain-specific areas like clinical texts.

Proposed Solution:
- Introduced TreatFact, a new dataset of 170 LLM-generated summaries of clinical trial abstracts, annotated by experts for factual consistency.
- Comprehensively evaluated 11 LLMs for FC assessment capability across news and clinical domains. Analyzed impact of factors like model size, prompts, pre-training data and fine-tuning.
- Compared LLMs to state-of-the-art FC evaluation methods on existing news dataset and newly introduced clinical dataset.

Key Findings:
- Proprietary LLMs (ChatGPT, GPT-4) outperform existing methods but open-source LLMs lag behind. 
- Increasing model size, pre-training data and fine-tuning helps boost performance of open-source LLMs.
- All examined methods struggle with FC evaluation on TreatFact clinical summaries, highlighting it as a challenging benchmark.  

Main Contributions:
- First FC benchmark focused on LLM-generated clinical summaries (TreatFact).
- Systematic analysis of using LLMs for FC evaluation across domains.
- Insights into enhancing open-source LLM performance on FC evaluation.
- Identification of major challenges in FC assessment of LLM clinical summaries.

In summary, the paper makes multiple contributions towards advancing factual consistency evaluation research, especially for summaries generated by state-of-the-art LLMs across different domains.
