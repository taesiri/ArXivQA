# [You Need Multiple Exiting: Dynamic Early Exiting for Accelerating   Unified Vision Language Model](https://arxiv.org/abs/2211.11152)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be:How to efficiently accelerate inference speed of unified vision language models with minimal performance loss using early exiting strategies?The key points are:1) Unified vision language models (e.g. based on transformers like OFA) achieve great performance but are slow for inference. 2) Early exiting strategies have been used to speed up inference in other domains, but have not been explored for unified vision language models.3) Applying existing early exiting strategies naively results in performance loss.4) The paper proposes a new framework called MuE that enables early exiting in both encoder and decoder of unified models, while minimizing performance loss. 5) MuE does this by:- Decomposing modalities in the encoder for flexible exiting.- Using layer similarity rather than classifiers for exit decisions. - Adding layer-wise task loss to maintain performance.6) Experiments on SNLI-VE and COCO show MuE reduces inference time significantly (up to 50%) with minimal drop in accuracy.In summary, the key hypothesis is that the proposed MuE framework can enable efficient early exiting in unified vision-language models, which was not possible effectively before. The paper seems to validate this hypothesis through experiments on two datasets.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contributions appear to be:1. Proposing a novel early exiting strategy called MuE for unified vision language models to improve inference efficiency. MuE allows dynamically skipping layers in both the encoder and decoder based on layer-wise input similarities. 2. Introducing a modality decomposition mechanism that decomposes the vision and language modalities in the encoder. This allows flexibly skipping different modalities for different tasks, further improving efficiency.3. Designing a layer-wise task loss that links each decoder layer to the final task. This helps maintain task performance when significant reductions in computation are required.4. Demonstrating through experiments on SNLI-VE and MS COCO datasets that MuE can reduce computation by up to 50% and 40% respectively, while maintaining 99% and 96% of the original performance.In summary, the main contribution appears to be proposing a new early exiting strategy called MuE that can efficiently skip layers in both the encoder and decoder of unified vision language models, aided by modality decomposition and a layer-wise loss. Experiments show MuE can significantly reduce computation with minimal impact on performance.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper proposes a new method called MuE that improves the inference efficiency of unified vision-language models by dynamically skipping layers in both the encoder and decoder based on layer similarity, and introduces a layer-wise loss to help maintain performance when significant reductions in computation are required.
