# [ICLN: Input Convex Loss Network for Decision Focused Learning](https://arxiv.org/abs/2403.01875)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem: 
In decision-making problems under uncertainty, prediction and optimization are often treated as separate tasks. Prediction-focused learning (PFL) trains a model to make accurate predictions, while decision-focused learning (DFL) trains a model to make predictions that lead to good decisions for the task. Computing gradients for DFL requires differentiating through the optimization problem, which can be intractable. Existing surrogate DFL methods either restrict the optimization domain or build local surrogate losses per data instance, requiring lots of computation.

Proposed Solution:
This paper proposes Input Convex Loss Network (ICLN), a novel global surrogate loss for general DFL problems. ICLN uses Input Convex Neural Networks to guarantee convexity with respect to some inputs while maintaining a global structure. This allows ICLN to represent the task loss for any DFL problem using a single learned loss function, without needing to hand-engineer problem-specific losses. 

Two variants are introduced: 
1) ICLN-L: Learns a local convex loss around each prediction instance 
2) ICLN-G: Learns a global loss that is convex for sampled neighbor predictions but not necessarily for original predictions.

Both only require a pre-processing sampling stage to generate prediction/task loss pairs. The learned loss can then be used to train any predictive model by taking gradients.

Main Contributions:
- Proposes ICLN, the first general, global surrogate loss for DFL requiring only a single learned loss function
- Guarantees convexity for sampled prediction neighbors through Input Convex NN architecture
- Achieves state-of-the-art performance across inventory, knapsack and portfolio optimization tasks
- Requires orders of magnitude fewer loss samples than methods learning local loss models
- Provides ability to perform general DFL without problem-specific loss engineering

In summary, ICLN contribution is a novel, efficient and flexible surrogate DFL approach that can work on any decision-making problem with much less samples than prior arts.


## Summarize the paper in one sentence.

 This paper proposes Input Convex Loss Networks (ICLN), a novel global surrogate loss function for decision-focused learning that is convex with respect to prediction samples near the true instances but not necessarily convex overall, enabling efficient end-to-end training of predictive models for stochastic optimization problems.


## What is the main contribution of this paper?

 Based on my understanding, the main contribution of this paper is proposing a novel global surrogate loss function called Input Convex Loss Network (ICLN) for decision-focused learning. Specifically:

- ICLN learns the mapping from predictions to task loss via a neural network that is guaranteed to be convex for some inputs. This avoids having to handcraft surrogate losses for each problem.

- Two versions are proposed - ICLN-L which learns local task loss representations for each instance, and ICLN-G which learns a single global task loss representation. 

- ICLN-G in particular requires significantly fewer training samples than prior methods, reducing the time complexity for training machine learning models in decision-focused learning paradigms.

- The effectiveness and flexibility of ICLN is demonstrated on three different stochastic decision-making problems - inventory stock, knapsack, and portfolio optimization. On all three tasks, ICLN variants outperform previous surrogate loss approaches.

In summary, the key contribution is developing a general, handcraft-free surrogate loss function based on input convex neural networks, which reduces training complexity for decision-focused learning across a range of optimization problems.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and keywords associated with this paper include:

- Decision-focused learning (DFL)
- Prediction-focused learning (PFL) 
- Task loss
- Surrogate loss
- Input convex loss network (ICLN)
- ICLN-L (local task loss representation)
- ICLN-G (global task loss representation)  
- Input convex neural network (ICNN)
- Inventory stock problem
- Knapsack problem
- Portfolio optimization 

The paper proposes a new method called Input Convex Loss Network (ICLN) for decision-focused learning. It has two variants - ICLN-L for local task loss modeling and ICLN-G for global task loss modeling. The key idea is to learn a surrogate loss function that approximates the task loss using input convex neural networks. The method is evaluated on three decision-making problems - inventory stock, knapsack, and portfolio optimization. The terms and keywords reflect the key concepts, proposed methods, and experiment settings in the paper.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in the paper:

1) The paper proposes two variants of the Input Convex Loss Network (ICLN) - ICLN-L and ICLN-G. What are the key differences between these two variants and what are the tradeoffs? 

2) For ICLN-L, the paper generates prediction samples near each instance $y_i$ to train a loss model $\mathcal{L}_{\psi_i}$ that is convex in the neighborhood of $y_i$. How does ensuring convexity in the neighborhood of each instance help in training better predictive models?

3) The paper claims ICLN-G requires significantly fewer samples than other methods like LODL and NN. Why does ensuring convexity only in the neighborhood of the sampled predictions $\tilde{y}_i^{(k)}$ still allow effective training with fewer samples?

4) How exactly does the partial input convex architecture used in ICLN-G ensure convexity in the sampled prediction $\tilde{y}_i^{(k)}$ while not necessarily ensuring convexity in the actual instance $y_i$? 

5) The inventory stock problem in the experiments does not have true probability labels due to the demand stochasticity. How does the sampling procedure differ in this problem compared to the knapsack and portfolio optimization problems?

6) For problems that do have access to true labels like knapsack and portfolio optimization, is the performance difference between ICLN-L and ICLN-G more significant compared to the inventory problem? Why?

7) The paper claims the sampling stage is pre-processable and parallelizable. What modifications can be made to leverage parallel compute to offset the sampling cost, especially for ICLN-L?  

8) Can ideas from few-shot or meta-learning be incorporated so that ICLN-G gives good performance with even fewer samples? How can we avoid overfitting in that setting?

9) How straightforward is it to apply ICLN to other decision-focused learning problems beyond inventory, knapsack and portfolio optimization covered in the paper?

10) The paper focuses on stochastic problems. How can we extend ICLN to settings where the uncertainty is from an adversarial agent rather than stochastic?
