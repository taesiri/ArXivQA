# [Investigating Bias Representations in Llama 2 Chat via Activation   Steering](https://arxiv.org/abs/2402.00402)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem: 
Large language models (LLMs) like Llama 2 Chat can perpetuate societal biases related to gender, race, and religion. It is important to assess these models' robustness to biased behaviors before deploying them in sensitive applications. 

Methodology:  
The authors perform red teaming on Llama 2 Chat using activation steering to elicit biased responses. Specifically, they use contrastive activation addition (CAA) to add bias steering vectors and subtract refusal steering vectors. The bias vectors are constructed from the StereoSet dataset and custom gender bias prompts formatted as A/B multiple choice questions.  

Key Findings:
- Llama 2 Chat exhibits inherent gender bias despite fine-tuning and reinforcement learning from human feedback (RLHF) 
- Attempts to steer the model toward other biases triggers refusal responses
- Bypassing refusal responses reveals the model can generate racist, sexist and religiously intolerant text
- All types of bias vectors are negatively correlated with refusal
- In the chat model, different forms of bias (gender, race, religion) have high cosine similarity (0.8) indicating conflation of these concepts 

Main Contributions:
- Demonstrates efficacy of integrated refusal vectors for red teaming LLMs
- Reveals high correlation between bias vectors in Llama 2 Chat, suggesting RLHF causes the model to jointly represent multiple biases in similar ways rather than understanding them as distinct concepts
- Provides analysis of relationships between bias and refusal vectors
- Showcases activation steering as an insightful diagnostic test for model weaknesses

In summary, this paper uncovers inherent societal biases in Llama 2 Chat and that RLHF seems to increase conflation of bias types. It also emphasizes the importance of red teaming strategies like activation steering to ensure safe and ethical LLM development.


## Summarize the paper in one sentence.

 This paper investigates inherent gender bias in Llama 2 Chat and finds that reinforcement learning from human feedback causes the model to conflate different forms of societal bias, raising questions about its nuanced understanding.


## What is the main contribution of this paper?

 Based on the abstract and conclusion, the main contributions of this paper appear to be:

1) Employing activation steering via contrastive activation addition to probe and uncover inherent gender bias in the Llama 2 7B Chat model, even after reinforcement learning from human feedback (RLHF).

2) Demonstrating that attempting to steer the model towards biased outputs triggers refusal responses, and showing there is a predictable negative correlation between bias and refusal vectors. 

3) Revealing that RLHF causes the Llama 2 7B Chat model to increase the similarity in its internal representations of different forms of societal bias (gender, race, religion), indicating it may lose nuanced understanding of distinct bias concepts and instead associate them together.

4) Providing insights into effective red teaming of language models using activation steering, especially emphasizing the importance of integrating a refusal vector to bypass safeguards. 

In summary, the main contribution seems to be using activation steering to uncover and analyze inherent biases in the Llama 2 Chat model, including the effects of RLHF on its bias representations. The authors also contribute best practices for adversarial testing of biases.


## What are the keywords or key terms associated with this paper?

 Based on a review of the paper's content, some of the key terms and keywords associated with this paper include:

- Large language models (LLMs)
- Activation steering 
- Contrastive activation addition
- Societal bias
- Gender bias
- Racial bias
- Religious bias  
- Reinforcement learning from human feedback (RLHF)
- Red-teaming
- Refusal steering vectors
- Robustness testing

The paper investigates bias representations in the Llama 2 7B Chat large language model using activation steering. It looks specifically at gender, racial, and religious biases. Key methods include contrastive activation addition to steer model outputs and refusal steering to bypass guardrails. The analysis explores relationships between different types of societal bias vectors and refusal vectors. It also evaluates the impact of reinforcement learning from human feedback on bias representations. Overall, the paper demonstrates an approach to red-teaming and robustness testing of LLMs with regards to societal biases.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in this paper:

1. The paper utilizes activation steering to probe biases in Llama 2 7B Chat. What are the key advantages of using activation steering over other bias probing techniques? What limitations does it have?

2. The paper constructs steering vectors by taking the average difference in activations between stereotype and anti-stereotype prompts. How robust is this approach to noise in the activations? Could improved pairing or scaling of the prompts lead to more effective steering vectors?  

3. The paper finds that applying racial bias steering vectors also impacts gender and religious biases. Does this indicate the model represents all biases similarly? Or could it be an artifact of the specific steering vector used?

4. The refusal vector is crucial for bypassing the model's guardrails. What techniques could make this vector more robust? Are there risks associated with subtracting the refusal vector?

5. The gender bias vectors have the lowest correlation with refusal vectors. Does this accurately reflect the relative severity of gender bias in the model versus other biases? Why might gender bias be less associated with refusal?

6. Reinforcement learning appears to increase similarity between different types of bias vectors. What mechanism leads to this effect? Is it an inevitable outcome of reducing biased responses via RL?  

7. Are the patterns uncovered with activation steering likely to generalize to other forms of model probing? What empirical evidence is needed to confirm the findings?

8. The paper notes racial steering vectors were most effective due to larger sample size. How can we determine the optimal data requirements for producing robust and transferable steering vectors? 

9. Would collecting steering data in different domains lead to vectors that reveal different aspects of bias representations? How should we select domains to construct informative steering vectors?

10. The paper focuses solely on steering at the token level. Could other techniques like manipulating attention or retrieving closest activations provide additional insights? What are the tradeoffs to explore?
