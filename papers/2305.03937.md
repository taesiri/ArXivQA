# [Residual Prompt Tuning: Improving Prompt Tuning with Residual   Reparameterization](https://arxiv.org/abs/2305.03937)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is: How can we improve prompt tuning, a parameter-efficient method for adapting large pretrained language models, through reparameterization of the prompt embeddings?Specifically, the authors propose a method called "Residual Prompt Tuning" which reparameterizes the prompt embeddings by passing them through a shallow network with a residual connection before prepending them to the input. The key ideas are:1) Reparameterizing the prompts gives more flexibility compared to directly optimizing the prompt embeddings. 2) Using a residual connection speeds up optimization and improves performance compared to reparameterization without skip connections.3) After training, the reparameterization network can be discarded, so only the modified prompt embeddings need to be retained.The main hypothesis is that Residual Prompt Tuning will improve the performance and robustness of prompt tuning across tasks, models, and hyperparameters, while retaining the parameter efficiency of prompt tuning. Experiments on SuperGLUE benchmark tasks support this hypothesis.In summary, the paper introduces and evaluates a simple but effective method to boost prompt tuning through residual reparameterization of the prompt embeddings. The central hypothesis is that this reparameterization approach will enhance prompt tuning.
