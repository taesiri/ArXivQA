# [Residual Prompt Tuning: Improving Prompt Tuning with Residual   Reparameterization](https://arxiv.org/abs/2305.03937)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is: How can we improve prompt tuning, a parameter-efficient method for adapting large pretrained language models, through reparameterization of the prompt embeddings?Specifically, the authors propose a method called "Residual Prompt Tuning" which reparameterizes the prompt embeddings by passing them through a shallow network with a residual connection before prepending them to the input. The key ideas are:1) Reparameterizing the prompts gives more flexibility compared to directly optimizing the prompt embeddings. 2) Using a residual connection speeds up optimization and improves performance compared to reparameterization without skip connections.3) After training, the reparameterization network can be discarded, so only the modified prompt embeddings need to be retained.The main hypothesis is that Residual Prompt Tuning will improve the performance and robustness of prompt tuning across tasks, models, and hyperparameters, while retaining the parameter efficiency of prompt tuning. Experiments on SuperGLUE benchmark tasks support this hypothesis.In summary, the paper introduces and evaluates a simple but effective method to boost prompt tuning through residual reparameterization of the prompt embeddings. The central hypothesis is that this reparameterization approach will enhance prompt tuning.


## What is the main contribution of this paper?

The main contribution of this paper is proposing a new method called Residual Prompt Tuning to improve prompt tuning, which is a parameter-efficient approach for adapting large pre-trained language models to downstream tasks. Specifically, the key ideas and contributions are:- Proposing to reparameterize the soft prompt embeddings in prompt tuning using a shallow residual network. This gives the model more flexibility to learn better prompt representations.- Showing that Residual Prompt Tuning substantially improves performance over standard prompt tuning across multiple models and datasets. It achieves over 7 points higher average score on SuperGLUE benchmark compared to prompt tuning.- Demonstrating that Residual Prompt Tuning makes prompt tuning much more robust to hyperparameters like learning rate and prompt initialization. It also speeds up convergence.- Showing the effectiveness of Residual Prompt Tuning in low-data regimes, improving over prompt tuning in few-shot settings.- Providing a simple and efficient method to get better performance from prompt tuning without needing pretraining on other tasks or changing the model architecture.In summary, the main contribution is proposing Residual Prompt Tuning, a novel way to reparameterize and optimize soft prompts, which leads to significant gains over standard prompt tuning in various settings. The improved performance, robustness and efficiency of prompt tuning is the primary contribution.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Based on my understanding, the key point of the paper is proposing a new method called Residual Prompt Tuning to improve the performance and stability of prompt tuning for parameter-efficient tuning of large language models. The main idea is to reparameterize the soft prompt embeddings using a shallow residual network before feeding them into the frozen language model. This allows more flexible optimization and makes prompt tuning more robust. Overall, the paper shows that Residual Prompt Tuning significantly boosts prompt tuning performance across different models and datasets.In one sentence, I would summarize it as: The paper proposes Residual Prompt Tuning, a method to reparameterize soft prompts via a residual network, which substantially improves prompt tuning performance and stability across models and datasets.
