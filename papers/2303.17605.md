# [SparseViT: Revisiting Activation Sparsity for Efficient High-Resolution   Vision Transformer](https://arxiv.org/abs/2303.17605)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question appears to be: How can we leverage activation sparsity to improve the efficiency of high-resolution vision transformers, while retaining their accuracy?Specifically, the authors propose a method called SparseViT to revisit activation sparsity in the context of recent window-based vision transformers (ViTs) like Swin Transformer. Their key ideas include:- Performing window-level activation pruning in ViTs to achieve actual speedup, since window attentions are naturally batched over windows.- Using a sparsity-aware adaptation method during training to accommodate the model to activation sparsity.- Employing an evolutionary search algorithm to efficiently explore the best layerwise sparsity configuration under a latency constraint. The main hypothesis seems to be that by carefully incorporating activation sparsity into ViTs in this manner, it is possible to reduce computation and improve efficiency for high-resolution image recognition tasks, while maintaining the models' accuracy. The paper aims to demonstrate this through experiments on monocular 3D object detection, 2D instance segmentation, and 2D semantic segmentation.In summary, the central research question is about improving efficiency of vision transformers via activation sparsity, and the key hypothesis is that this can be done without sacrificing accuracy by using the SparseViT method proposed in the paper.


## What is the main contribution of this paper?

The main contribution of this paper is proposing SparseViT, a method to leverage activation sparsity for efficient high-resolution vision transformers. The key ideas are:- Revisiting activation sparsity in the context of recent window-based vision transformers (ViTs) like Swin Transformer. Unlike CNNs, window attention is naturally batched, so window-level activation pruning can achieve actual speedup.- Introducing sparsity-aware adaptation during training, which randomly samples different layerwise sparsity configurations. This adapts the model to sparsity without expensive retraining. - Using evolutionary search to efficiently explore the vast search space and find the optimal per-layer sparsity ratios under latency constraints.- Achieving 1.5x, 1.4x and 1.3x speedup on monocular 3D detection, 2D instance segmentation and 2D semantic segmentation respectively, with negligible to no accuracy drop.In summary, the paper shows that activation sparsity can be effectively leveraged to accelerate high-resolution ViTs, by designing techniques like sparsity-aware adaptation and evolutionary search to find optimal layerwise configurations. This allows preserving high-resolution details efficiently.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper proposes SparseViT, a vision transformer model that uses evolutionary search to determine optimal per-layer activation sparsity ratios and adapts the model to sparsity during training, achieving speedups of 1.5x, 1.4x, and 1.3x on monocular 3D object detection, 2D instance segmentation, and 2D semantic segmentation tasks compared to the dense baseline with minimal loss of accuracy.
