# [LLM-Planner: Few-Shot Grounded Planning for Embodied Agents with Large   Language Models](https://arxiv.org/abs/2212.04088)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question is: How can large language models (LLMs) be used as few-shot planners for embodied agents to follow natural language instructions in visually-perceived environments? 

More specifically, the authors aim to investigate:

- Whether LLMs can be adapted as few-shot high-level planners for embodied agents through careful prompt design and other techniques like dynamic example retrieval and logit biases. 

- If the high-level plans generated by LLMs can be further improved through a novel grounded re-planning algorithm that incorporates perceived objects from the environment into the prompt to make the plans more grounded.

- How an LLM-based planner performs in complex, diverse, and partially observable environments compared to existing methods, especially under a true few-shot setting with only 100 training examples.

So in summary, the main hypothesis is that by properly designing prompts and dynamically grounding LLMs with environment perceptions, they can be adapted as sample-efficient high-level planners for embodied agents to follow instructions across diverse tasks and environments. The experiments aim to validate whether this approach works in practice.


## What is the main contribution of this paper?

 The main contribution of this paper is proposing a new method called LLM-Planner that uses large language models (LLMs) as few-shot planners for embodied agents. The key ideas and contributions are:

- Proposes a novel way to leverage LLMs like GPT-3 for few-shot planning for embodied agents, enabling the agent to learn new tasks with only a few examples.

- Introduces a grounded re-planning algorithm that allows the LLM planner to dynamically adjust plans based on perceptions from the environment, making the plans more grounded.

- Achieves strong few-shot performance on a challenging embodied AI benchmark (ALFRED) despite using less than 0.5% of the full training data. Existing methods fail completely under the same few-shot setting.

- Provides extensive analyses and ablations to validate the effectiveness of the proposed techniques like grounded re-planning and dynamic example retrieval. 

- Demonstrates how to properly integrate the LLM planner into existing embodied agents like HLSM to empower them with few-shot planning capabilities.

In summary, the key innovation is using LLMs for few-shot planning for embodied agents and enhancing LLMs with grounding to produce more dynamic and grounded plans. This allows developing extremely sample-efficient agents that can learn new tasks quickly.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the key points from the paper:

The paper proposes LLM-Planner, a method that uses large language models to generate high-level plans for embodied agents to follow natural language instructions, and introduces a grounded re-planning algorithm to dynamically adjust plans based on environmental perception, achieving strong performance on the ALFRED benchmark with only a small fraction of training data.


## How does this paper compare to other research in the same field?

 Here are my thoughts on how this paper compares to other research in the field of few-shot language-grounded navigation:

- The key contribution of this paper is using large language models (LLMs) as few-shot planners for embodied agents following natural language instructions. This is a relatively new direction compared to most prior work on embodied AI and vision-and-language navigation.

- Most prior work relies on training neural network models end-to-end with a lot of paired language-trajectory demonstrations. This is costly to collect. In contrast, this paper shows that with careful prompt design and grounding techniques, LLMs can achieve strong few-shot performance with only 100 demonstration examples, which is orders of magnitude less data than prior work.

- This paper is most related to other recent works that also explore using LLMs for language-grounded navigation, such as SayCan, JARVIS, and ProgPrompt. However, those works make strong assumptions about knowing the action space or object affordances a priori, which limits their applicability. This paper proposes more general techniques without such assumptions.

- Compared to SayCan, a pioneering work in this direction, this paper shows significantly better results on the complex ALFRED benchmark by using the LLM as a generative model to directly produce plans instead of ranking a fixed set of candidate skills. The grounded replanning technique also provides an important ability to dynamically adjust plans based on the environment.

- The proposed LLM-Planner demonstrates strong generalization across diverse environments and task types in ALFRED. In contrast, most prior works only evaluate on simpler or more limited settings. The techniques are also model-agnostic and can empower any embodied agent.

- An interesting direction for future work is exploring different choices of LLMs beyond GPT-3, such as Codex or PaLM. The prompt design techniques may also generalize to other embodied AI domains beyond navigation. Overall, this paper opens up an exciting new research direction on combining LLMs with physical grounding and replanning for sample-efficient embodied agents.


## What future research directions do the authors suggest?

 Based on my reading of the paper, some of the main future research directions suggested by the authors include:

- Exploring other large language models beyond GPT-3, such as Codex, to see if they can further improve performance on high-level planning for embodied agents. The authors mention Codex as a promising model to try.

- Improving prompt design to better guide the language model to generate high-quality, grounded plans. The authors note prompt design is important for unleashing the power of large language models.

- Developing more advanced methods for physical grounding and dynamic re-planning during task execution. The authors propose a simple method of adding observed objects to the prompt for re-planning, but more sophisticated techniques could be explored.

- Evaluating the approach on more complex and diverse environments beyond ALFRED. The authors note their method should generalize but more experimentation on different environments would be useful. 

- Using better low-level planners and perception modules. The authors note these components from the baseline model currently limit end-to-end performance on some tasks, so integrating with more advanced alternatives could further improve results.

- Exploring whether similar techniques could allow fast adaption and transfer learning to new tasks with only a few examples. The authors suggest their approach could enable versatile agents that learn new tasks quickly.

Overall, the core suggested directions are developing more advanced prompt engineering strategies, integrating with improved perception and control modules, testing on more complex environments, and leveraging the approach for faster adaptation and transfer to new tasks. The authors propose their method opens the door to developing extremely sample-efficient embodied agents.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the paper:

This paper proposes LLM-Planner, a new method that leverages large language models (LLMs) to do few-shot planning for embodied agents. The key idea is to use LLMs to generate high-level plans (sequences of subgoals like "Pick up apple" and "Go to microwave") from natural language instructions and only a few examples of instruction-plan pairs. To make the plans grounded in the environment, LLM-Planner dynamically re-prompts the LLM during execution to re-plan based on objects observed in the current environment. Experiments on the ALFRED benchmark for embodied instruction following show that with less than 0.5% of the full training data, LLM-Planner integrated with existing embodied agents can match or exceed the performance of various baselines trained on the full dataset. This demonstrates the potential of harnessing LLMs for extremely sample-efficient learning and planning for embodied agents.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the paper:

The paper proposes a new method, LLM-Planner, that uses large language models (LLMs) as high-level planners for embodied agents in vision-and-language navigation tasks. The key idea is to leverage the few-shot learning capabilities of LLMs like GPT-3 to generate high-level plans for completing complex household tasks based on natural language instructions and a few examples. 

The authors design an appropriate prompt and retrieve similar examples to provide context for the LLM to generate plausible high-level plans. They further propose a grounded re-planning algorithm that allows the agent to dynamically adjust the plan based on objects perceived in the environment. This helps to ground the LLM's plan to the physical environment. Experiments on the challenging ALFRED benchmark show that with less than 0.5% of the full training data, LLM-Planner integrated with an existing agent architecture achieves competitive performance compared to models trained on the entire dataset. The proposed method significantly reduces the annotation cost and improves the sample efficiency for training embodied agents.


## Summarize the main method used in the paper in one paragraph.

 Here is a one paragraph summary of the main method used in the paper:

The paper proposes LLM-Planner, a method that leverages large language models (LLMs) to do few-shot planning for embodied agents in vision-and-language navigation tasks. LLM-Planner uses the LLM GPT-3 to generate high-level plans consisting of navigation and interaction subgoals from natural language instructions and a few demo examples. To make the plans grounded to the physical environment, LLM-Planner incorporates a novel grounded re-planning algorithm that prompts GPT-3 again during execution to replan based on objects detected in the environment. Prompt engineering techniques like dynamic example retrieval and logit biases are used to further improve the LLM's few-shot performance. LLM-Planner is integrated with an existing navigation agent and evaluated on the challenging ALFRED benchmark. Using only 0.5% of the full training data, LLM-Planner achieves competitive performance to state-of-the-art methods trained on the entire dataset. The work demonstrates the potential of leveraging LLMs for extremely sample-efficient planning for embodied agents.


## What problem or question is the paper addressing?

 The paper is addressing the problem of developing embodied agents that can follow natural language instructions to complete tasks in simulated environments. Specifically, it focuses on improving the sample efficiency and versatility of such agents, so they can learn new tasks quickly with limited training data and handle diverse tasks and environments.

The key limitations the paper identifies with existing methods are:

- They require large amounts of labeled training data (instruction-trajectory pairs) to learn each new task. This hinders developing truly versatile agents that can handle many different tasks.

- They have poor sample efficiency and struggle to learn new tasks quickly from limited examples.

- They struggle to handle diverse tasks across different environments, often making assumptions about the environments being known or restricted.

To overcome these limitations, the paper proposes a new method called LLM-Planner that leverages large language models (LLMs) to do one-shot or few-shot planning for embodied agents. The key ideas are:

- Use LLMs like GPT-3 in an in-context learning setup to generate high-level plans from natural language instructions and a few examples, instead of requiring supervised training data.

- Enhance the LLM with physical grounding by dynamically prompting it again during execution to re-plan based on objects perceived in the environment. This produces more grounded plans tailored to the current environment.

- Integrate the LLM planner with existing embodied agents in a modular way without needing to modify their perception modules or low-level controllers.

The key research question is whether large language models can be adapted through careful prompting and grounding to do high-quality one-shot planning for embodied agents, reducing the dependence on large supervised training sets. The paper aims to demonstrate this through systematic experiments on a complex benchmark environment.
