# LLM-Planner: Few-Shot Grounded Planning for Embodied Agents with Large   Language Models

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is: How can large language models (LLMs) be used as few-shot planners for embodied agents to follow natural language instructions in visually-perceived environments? More specifically, the authors aim to investigate:- Whether LLMs can be adapted as few-shot high-level planners for embodied agents through careful prompt design and other techniques like dynamic example retrieval and logit biases. - If the high-level plans generated by LLMs can be further improved through a novel grounded re-planning algorithm that incorporates perceived objects from the environment into the prompt to make the plans more grounded.- How an LLM-based planner performs in complex, diverse, and partially observable environments compared to existing methods, especially under a true few-shot setting with only 100 training examples.So in summary, the main hypothesis is that by properly designing prompts and dynamically grounding LLMs with environment perceptions, they can be adapted as sample-efficient high-level planners for embodied agents to follow instructions across diverse tasks and environments. The experiments aim to validate whether this approach works in practice.


## What is the main contribution of this paper?

The main contribution of this paper is proposing a new method called LLM-Planner that uses large language models (LLMs) as few-shot planners for embodied agents. The key ideas and contributions are:- Proposes a novel way to leverage LLMs like GPT-3 for few-shot planning for embodied agents, enabling the agent to learn new tasks with only a few examples.- Introduces a grounded re-planning algorithm that allows the LLM planner to dynamically adjust plans based on perceptions from the environment, making the plans more grounded.- Achieves strong few-shot performance on a challenging embodied AI benchmark (ALFRED) despite using less than 0.5% of the full training data. Existing methods fail completely under the same few-shot setting.- Provides extensive analyses and ablations to validate the effectiveness of the proposed techniques like grounded re-planning and dynamic example retrieval. - Demonstrates how to properly integrate the LLM planner into existing embodied agents like HLSM to empower them with few-shot planning capabilities.In summary, the key innovation is using LLMs for few-shot planning for embodied agents and enhancing LLMs with grounding to produce more dynamic and grounded plans. This allows developing extremely sample-efficient agents that can learn new tasks quickly.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper proposes LLM-Planner, a method that uses large language models to generate high-level plans for embodied agents to follow natural language instructions, and introduces a grounded re-planning algorithm to dynamically adjust plans based on environmental perception, achieving strong performance on the ALFRED benchmark with only a small fraction of training data.


## How does this paper compare to other research in the same field?

Here are my thoughts on how this paper compares to other research in the field of few-shot language-grounded navigation:- The key contribution of this paper is using large language models (LLMs) as few-shot planners for embodied agents following natural language instructions. This is a relatively new direction compared to most prior work on embodied AI and vision-and-language navigation.- Most prior work relies on training neural network models end-to-end with a lot of paired language-trajectory demonstrations. This is costly to collect. In contrast, this paper shows that with careful prompt design and grounding techniques, LLMs can achieve strong few-shot performance with only 100 demonstration examples, which is orders of magnitude less data than prior work.- This paper is most related to other recent works that also explore using LLMs for language-grounded navigation, such as SayCan, JARVIS, and ProgPrompt. However, those works make strong assumptions about knowing the action space or object affordances a priori, which limits their applicability. This paper proposes more general techniques without such assumptions.- Compared to SayCan, a pioneering work in this direction, this paper shows significantly better results on the complex ALFRED benchmark by using the LLM as a generative model to directly produce plans instead of ranking a fixed set of candidate skills. The grounded replanning technique also provides an important ability to dynamically adjust plans based on the environment.- The proposed LLM-Planner demonstrates strong generalization across diverse environments and task types in ALFRED. In contrast, most prior works only evaluate on simpler or more limited settings. The techniques are also model-agnostic and can empower any embodied agent.- An interesting direction for future work is exploring different choices of LLMs beyond GPT-3, such as Codex or PaLM. The prompt design techniques may also generalize to other embodied AI domains beyond navigation. Overall, this paper opens up an exciting new research direction on combining LLMs with physical grounding and replanning for sample-efficient embodied agents.
