# [High-Fidelity Guided Image Synthesis with Latent Diffusion Models](https://arxiv.org/abs/2211.17084)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading, the main research question this paper aims to address is: 

How can we perform high-fidelity guided image synthesis from user scribbles using latent diffusion models, without requiring paired training data or modifications to model architecture/training?

Specifically, the key hypotheses appear to be:

1) Existing inversion-based approaches for guided synthesis with diffusion models suffer from an intrinsic domain gap problem, resulting in outputs that lack details and resemble simplistic representations. 

2) Modelling the guided synthesis output as the solution to a constrained optimization problem can help resolve this domain gap issue and achieve highly realistic outputs in a single diffusion sampling pass.

3) An approximate solution to this constrained optimization can be obtained through simple gradient descent, by minimizing the distance between the painted image and reference scribbles, while regularizing the output to lie close to the text-conditioned sample space.

4) Defining cross-attention based correspondence between text tokens and scribble regions allows controlling semantics without conditional training.

The paper aims to validate these hypotheses through proposed methods GradOP and GradOP+, along with qualitative and quantitative experiments on guided image synthesis from text prompts and user scribbles. The key novelty lies in formulating and approximately solving the optimization problem to bridge domain gap without modifying model training.
