# [Forward Learning of Graph Neural Networks](https://arxiv.org/abs/2403.11004)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper "Forward Learning of Graph Neural Networks":

Problem:
- Graph neural networks (GNNs) have become very popular and achieved great success across various applications involving graph data. However, the standard way of training GNNs relies on backpropagation (BP), which has several limitations that constrain the scalability, parallelism and flexibility of learning GNNs. 
- Key constraints of BP include: requiring storage of activations for the backward pass, dependence on non-local signals for parameter updates, updates only after full forward pass in reverse order. These make BP biologically implausible.

Proposed Solution: 
- The paper develops a novel forward learning framework called "ForwardGNN" to train GNNs without relying on BP.
- It extends and improves upon the recent Forward-Forward (FF) algorithm designed for image classification to enable effective forward-only learning of GNNs.

Main Contributions:
- Proposes two techniques to adapt FF for graph data/GNNs: extending node features using labels, extending graph structure using virtual nodes.
- Develops a single forward pass approach that generates learning signals using available labels, removing the need for explicit negative samples as in FF.
- Enables each GNN layer to learn from both bottom-up and top-down signals without backpropagation.
- Evaluation on node classification and link prediction tasks demonstrates ForwardGNN matches or outperforms BP, while being more memory-efficient and scalable.
- Opens up the possibility of biologically plausible and flexible forward-only training of GNNs.

In summary, the paper makes significant contributions towards enabling scalable and flexible forward learning of graph neural networks, without relying on the constraints introduced by backpropagation. The proposed ForwardGNN framework shows strong performance compared to standard BP in the experiments.
