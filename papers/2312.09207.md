# [WikiMuTe: A web-sourced dataset of semantic descriptions for music audio](https://arxiv.org/abs/2312.09207)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a summary paragraph of the key points from the paper:

This paper introduces WikiMuTe, a new publicly available dataset of free-form textual descriptions paired with music audio, extracted from Wikipedia articles using a text-mining pipeline. The dataset contains 9000 tracks with rich semantic descriptions covering topics like genre, style, mood, instrumentation, etc. To demonstrate the usefulness of this data, the authors train a multi-modal model to match text and audio by learning joint representations. This model is evaluated on two tasks - tag-based music retrieval using the labels as search queries, and zero-shot music auto-tagging using established benchmark datasets. The results show competitive performance, underscoring the value of web-sourced data. However, models trained on manually created datasets like MusicCaps still achieve higher scores. The authors find that applying cross-modal relevance filtering to remove irrelevant text improves results. They hypothesize that expanding the web-mining to more sources can further enhance performance despite noisiness, as larger data size seems to outweigh quality issues. Overall, this paper makes a case for using rich free-form textual descriptions for complex music analysis, enabled by web-sourced datasets.


## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- There is a lack of openly accessible text-music datasets to train models for complex cross-modal tasks like text-to-music retrieval and music generation. 
- Manually labeling music with text descriptions is laborious and does not scale. 
- Automatically generated texts may not reflect the richness of human descriptions.

Proposed Solution:
- Present WikiMuTe, a new web-sourced dataset of 9,000 music tracks paired with textual descriptions extracted from Wikipedia articles using text mining.
- The descriptions include both short phrases (aspects) and longer sentences covering topics like genre, style, mood, instrumentation etc.
- A 3-stage pipeline is used to compile the dataset: 
   1) Collect music samples and text from Wikipedia 
   2) Extract relevant textual descriptions using a text-mining system
   3) Filter text-music pairs based on cross-modal relevance scoring
- Show applications of the dataset by training a text-audio matching model on two tasks: tag-based retrieval and auto-tagging

Main Contributions:
- Introduction of WikiMuTe, a large-scale open text-music dataset with rich and diverse textual descriptions of music content
- Demonstration of a scalable web-sourcing and text-mining approach to create such datasets
- Analysis of tradeoffs compared to other labeled/synthetic datasets
- Competitive results on multiple music retrieval and tagging tasks, showing usefulness of web-sourced data

The paper provides a novel way of harnessing online encyclopedic knowledge to obtain textual supervision at scale for music analysis models. The proposed dataset and benchmarking helps quantify tradeoffs and guides future efforts in this emerging cross-modal research direction.
