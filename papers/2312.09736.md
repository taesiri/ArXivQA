# [HEAR: Hearing Enhanced Audio Response for Video-grounded Dialogue](https://arxiv.org/abs/2312.09736)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Video-grounded dialogue (VGD) systems aim to answer questions based on a video, audio, and dialogue history input. 
- Existing VGD systems tend to ignore the audio information when generating responses, especially for audio-related questions. This is coined as the "deaf response" problem.

Proposed Solution: 
- The paper proposes the Hearing Enhanced Audio Response (HEAR) framework to enhance a VGD system's ability to listen and respond appropriately to audio.

Main Components:
1) Sensible Audio Listening (SAL): Selectively attends to audio based on audio-relatedness of the question. It has two components:
   - Keyword-based audio sensing: Masks video if keywords related to audio are present.
   - Semantic neural estimator: Learns to predict if a question is audio-related based on meaning.

2) Reconstructive Listening Enhancement (RLE): Enhances audio representations by reconstructing randomly masked audio using surrounding video/audio context. It establishes a "reconstruction upper bound" via additional surrounding masking to ensure the reconstructions are semantically meaningful.

Contributions:
- Proposes HEAR framework to enhance listening capabilities of VGD systems in a model-agnostic way
- Introduces SAL module for selective audio attention based on question semantics
- Introduces RLE module to improve audio representations by establishing reconstruction upper bounds
- Achieves state-of-the-art results on VGD datasets and shows consistent improvements when applied to other VGD models

Let me know if you need any clarification or have additional questions on the summary! I'm happy to provide more details.
