# [Video Representation Learning by Recognizing Temporal Transformations](https://arxiv.org/abs/2007.10730)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is:How can we learn video representations that better capture motion and temporal dynamics, without requiring manual annotations? The key hypotheses appear to be:1) Training a model to discriminate between videos and their temporally transformed versions will force it to learn about motion and dynamics in order to solve the pretext task.2) Designing temporal transformations that require observing long frame sequences will result in representations that capture long-range dynamics. 3) Features learned this way will transfer better to downstream tasks like action recognition compared to features learned through supervised training on action labels, by virtue of modeling motion and dynamics more accurately.The authors propose a self-supervised approach for video representation learning based on distinguishing a number of temporal transformations of videos, including speed changes, random permutations, periodic motions, and temporal warping. They design the transformations to require modeling long-range dynamics, and show through experiments that the learned features transfer better to action recognition compared to supervised pre-training and other self-supervised approaches.


## What is the main contribution of this paper?

The main contribution of this paper is introducing a novel self-supervised learning approach to learn video representations that capture motion dynamics. The key ideas are:- Proposing a pretext task of training a neural network to distinguish a video from its temporally transformed versions, including speed changes, random permutations, periodic motions, and temporal warps. - Showing that temporal transformations that require observing long-range dynamics (many frames) yield better video representations compared to transformations that can be identified from only a few frames.- Achieving state-of-the-art transfer learning performance on action recognition by pre-training on the proposed pretext task and transferring features to UCF101 and HMDB51 datasets.- Demonstrating qualitatively and quantitatively that the learned features better capture temporal dynamics compared to supervised pre-training, by visualizations and performance on time-related tasks like video synchronization.In summary, the key contribution is a novel self-supervised learning approach for video representation learning that trains models to recognize temporal transformations, especially long-range dynamics, leading to features that capture motion well and achieve excellent performance when transferred to action recognition.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper proposes a novel self-supervised learning approach for video representation learning by training a model to distinguish between different temporal transformations of a video, with the goal of learning features that capture the natural dynamics of the video.
