# [Mirror Descent Policy Optimization](https://arxiv.org/abs/2005.09814)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the key research question this paper aims to address is how to develop efficient reinforcement learning algorithms inspired by the theory of mirror descent (MD). Specifically, the authors propose a new algorithm called "mirror descent policy optimization" (MDPO) which iteratively updates the policy by approximately solving a trust-region optimization problem. The objective function consists of two terms: 1) A linear approximation of the standard RL objective 2) A proximity term that restricts consecutive policies to remain close to each other. The policy update is done by taking multiple gradient steps on this objective function.The authors derive on-policy and off-policy variants of MDPO and highlight the connections to existing algorithms like TRPO, PPO and SAC. The central hypothesis seems to be that deriving RL algorithms based on the principles of MD can lead to methods that are simple, efficient, and achieve state-of-the-art performance. The experiments aim to validate this hypothesis by benchmarking MDPO against TRPO, PPO and SAC on continuous control tasks.In summary, the key research question is whether using the theory of MD to derive RL algorithms can lead to improved performance and useful insights compared to existing approaches like TRPO, PPO and SAC. The MDPO algorithm and experiments are designed to test this hypothesis.
