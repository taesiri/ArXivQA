# [Mirror Descent Policy Optimization](https://arxiv.org/abs/2005.09814)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the key research question this paper aims to address is how to develop efficient reinforcement learning algorithms inspired by the theory of mirror descent (MD). Specifically, the authors propose a new algorithm called "mirror descent policy optimization" (MDPO) which iteratively updates the policy by approximately solving a trust-region optimization problem. The objective function consists of two terms: 1) A linear approximation of the standard RL objective 2) A proximity term that restricts consecutive policies to remain close to each other. The policy update is done by taking multiple gradient steps on this objective function.The authors derive on-policy and off-policy variants of MDPO and highlight the connections to existing algorithms like TRPO, PPO and SAC. The central hypothesis seems to be that deriving RL algorithms based on the principles of MD can lead to methods that are simple, efficient, and achieve state-of-the-art performance. The experiments aim to validate this hypothesis by benchmarking MDPO against TRPO, PPO and SAC on continuous control tasks.In summary, the key research question is whether using the theory of MD to derive RL algorithms can lead to improved performance and useful insights compared to existing approaches like TRPO, PPO and SAC. The MDPO algorithm and experiments are designed to test this hypothesis.


## What is the main contribution of this paper?

The main contribution of this paper is proposing an efficient RL algorithm called mirror descent policy optimization (MDPO). Here are the key points about MDPO and its contributions:- MDPO is inspired by the theoretical connections shown recently between mirror descent (MD), a first-order optimization method, and trust-region policy optimization algorithms in RL. - MDPO iteratively updates the policy by approximately solving a trust-region optimization problem. The objective has two terms: a linear approximation of the RL objective, and a proximity term that keeps consecutive policies close. - The paper derives on-policy and off-policy variants of MDPO. It emphasizes design choices motivated by the theory of MD in RL.- For on-policy MDPO, the update rule resembles TRPO but does not enforce the trust region constraint explicitly. It shows MDPO performs better than PPO and on-par with TRPO, without needing explicit constraint enforcement.- Off-policy MDPO bears similarity to SAC but keeps policies closer via the proximity term. Experiments show it matches or exceeds SAC's performance.- MDPO offers a simple and unified way to understand TRPO, PPO and SAC as approximations to theoretical trust-region updates. The theory-inspired design allows it to achieve state-of-the-art performance.In summary, the key contribution is proposing MDPO as an efficient, scalable RL algorithm grounded in theory, that matches or exceeds the performance of popular methods like TRPO, PPO and SAC across various continuous control tasks.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence TL;DR summary of the paper "Mirror Descent Policy Optimization":The paper proposes a new reinforcement learning algorithm called Mirror Descent Policy Optimization (MDPO) that is derived from the theory of mirror descent optimization and aims to approximately solve a trust region constrained policy optimization problem by taking multiple gradient steps on the objective function.


## How does this paper compare to other research in the same field?

Here is a brief analysis of how the paper "Mirror Descent Policy Optimization" compares to other research in reinforcement learning:- The paper proposes novel on-policy and off-policy algorithms (MDPO) for policy optimization that are inspired by the mirror descent algorithm from convex optimization. This connects policy optimization in RL more directly to optimization theory. Most other policy optimization methods like PPO, TRPO, SAC etc. have been proposed more from an empirical/heuristic perspective. - The paper shows MDPO has comparable or better performance than state-of-the-art methods like TRPO, PPO and SAC on several continuous control tasks. This demonstrates the effectiveness of the optimization-driven approach taken in MDPO. Most prior RL papers evaluate algorithms on performance only.- The paper provides new theoretical insights into popular algorithms like TRPO, PPO and SAC by analyzing their connections to MDPO and the optimization formulations used in MDPO. This kind of theoretical analysis and interpretation of existing empirical algorithms is relatively uncommon in RL research.- MDPO uses simpler optimization procedures compared to natural gradient in TRPO or clipping in PPO, while achieving better or equal performance. Most research focuses on proposing new complex algorithms. Simplicity of MDPO while preserving performance is novel.- The paper considers different designs like on-policy vs off-policy, forward vs reverse KL, hard vs soft MDPs in a principled way based on the MD theory. Most RL papers do not provide this level of analysis to motivate algorithmic design choices.In summary, connecting policy optimization to mirror descent, the extensive theoretical analysis relating MDPO to existing algorithms, and the simplicity yet effectiveness of MDPO distinguish this paper from most other RL research. The paper brings a fresh optimization-driven perspective to policy optimization.


## What future research directions do the authors suggest?

Here are some key future research directions suggested in the paper:- Evaluate MDPO algorithms on more complex and realistic problems. The paper focused on standard continuous control tasks, so testing on more challenging environments would be useful. - Explore using MDPO for discrete action spaces. The algorithms were presented for continuous actions, but adapting them to work with discrete actions like in Atari games could be beneficial. Comparing to algorithms like DQN and PPO in discrete domains would be interesting.- Investigate other choices of Bregman divergence beyond KL divergence. The use of Tsallis entropy led to improved performance, so studying other divergences induced by convex functions could further enhance MDPO.- Incorporate exploration into the MD-based policy updates. Theoretical work has looked at adding exploration bonuses to MD updates, so applying this to MDPO may improve performance in complex environments.- Provide theoretical guarantees for MDPO in the parametric policy setting. The current theory applies to tabular MDPO, so extending the analysis to the practical parametric policy case would be useful. - Scale MDPO to even more complex tasks like robotics control. Testing the algorithms on higher-dimensional continuous control problems would be a good direction.- Compare MDPO to a wider range of RL algorithms beyond TRPO, PPO and SAC. For example, how does it fare compared to Q-learning methods?In summary, the main suggested directions are scaling MDPO to more complex and realistic tasks, adapting it to discrete actions, analyzing and incorporating exploration, extending the theory, and comparing it to an even broader set of RL algorithms.
