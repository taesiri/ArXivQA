# [EquiformerV2: Improved Equivariant Transformer for Scaling to   Higher-Degree Representations](https://arxiv.org/abs/2306.12059)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be:How can equivariant Transformers like Equiformer be scaled up to higher degrees of equivariant representations? The authors start with Equiformer and aim to modify it to work effectively with higher degree equivariant features. Equiformer and other equivariant GNNs have been limited to low degrees due to the computational complexity of tensor products. The authors investigate whether the Equiformer architecture can be adapted to leverage higher degree features using more efficient tensor product operations.Specifically, the main hypotheses appear to be:- Replacing the tensor products in Equiformer with eSCN convolutions will allow scaling to higher degree features.- Additional architectural improvements (attention renormalization, separable S2 activation, separable layer norm) will enable the model to better leverage the representation power of higher degrees. - The proposed EquiformerV2 architecture incorporating these modifications will outperform prior methods when evaluated on tasks like energy and force prediction for molecules.In summary, the central research aim is developing an equivariant Transformer that can effectively utilize higher degree equivariant representations, which have been inaccessible to prior equivariant models. The authors propose specific architectural changes to achieve this.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contributions appear to be:- Proposing an improved equivariant Transformer model called EquiformerV2 for modeling 3D atomistic systems. The improvements include using more efficient eSCN convolutions to incorporate higher-degree tensor representations, and architectural innovations like attention re-normalization, separable S2 activation, and separable layer normalization.- Demonstrating that EquiformerV2 outperforms previous state-of-the-art methods on the large-scale OC20 benchmark for predicting atomic forces and energies. EquiformerV2 achieves up to 12% lower force error and 4% lower energy error compared to prior works.- Showing that EquiformerV2 offers better speed-accuracy trade-offs compared to prior invariant and equivariant models on OC20. At the same inference speed, it is 8% more accurate than eSCN.- Demonstrating that when used in the AdsorbML algorithm for adsorption energy calculations, EquiformerV2 achieves higher success rates while requiring fewer DFT calculations compared to previous methods.In summary, the main contribution is proposing EquiformerV2, an improved equivariant Transformer model that leverages architectural innovations to better incorporate higher-degree tensor representations. Experiments demonstrate state-of-the-art performance on modeling atomic systems compared to prior equivariant and invariant models.


## How does this paper compare to other research in the same field?

Here are a few thoughts on how this paper compares to other research in the field of equivariant graph neural networks for 3D molecular and materials modeling:- The key contribution is adapting efficient tensor product operations from the eSCN paper to work well with the more expressive Equiformer architecture. Prior works like eSCN and SCN focused on simpler message passing network architectures. Showing these efficient operations can be effectively incorporated into Transformers is an important advance.- The paper demonstrates strong performance on the large and diverse OC20 benchmark. Many prior works focused on smaller datasets like QM9 or MD17. Validation on more realistic and complex chemical systems is an important direction, and this paper advances progress there.- The architectural improvements like attention re-normalization, separable S2 activation, and separable layer norm seem crucial to make scaling up to higher degree representations effective. The ablations verify these contributions. This highlights that effectively modeling higher degrees likely requires adapting the network architecture.- The speed and accuracy trade-offs compared to prior works are state-of-the-art. Being able to tune this trade-off is important for applications. The improved computational efficiency is promising.- Transferability to smaller QM9 and MD17 datasets is not evaluated. An open question is whether benefits of higher degrees and larger models overfit on smaller benchmarks. Pre-training helps, but further analysis would be interesting.- Broader limitations of working with 3D molecular graphs and invariance to translation/permutation remain. Alternative representations like 3D point clouds are worth exploring. Equivariance to rotation is still an important inductive bias though.In summary, the paper makes excellent progress advancing equivariant Graph Networks for molecular modeling by effectively scaling them to higher degrees of representations and larger datasets. The results are state-of-the-art, and analysis of architectural choices is thorough. Open questions remain about transferability and limitations of 3D graph representations.
