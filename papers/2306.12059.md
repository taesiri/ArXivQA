# [EquiformerV2: Improved Equivariant Transformer for Scaling to   Higher-Degree Representations](https://arxiv.org/abs/2306.12059)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be:How can equivariant Transformers like Equiformer be scaled up to higher degrees of equivariant representations? The authors start with Equiformer and aim to modify it to work effectively with higher degree equivariant features. Equiformer and other equivariant GNNs have been limited to low degrees due to the computational complexity of tensor products. The authors investigate whether the Equiformer architecture can be adapted to leverage higher degree features using more efficient tensor product operations.Specifically, the main hypotheses appear to be:- Replacing the tensor products in Equiformer with eSCN convolutions will allow scaling to higher degree features.- Additional architectural improvements (attention renormalization, separable S2 activation, separable layer norm) will enable the model to better leverage the representation power of higher degrees. - The proposed EquiformerV2 architecture incorporating these modifications will outperform prior methods when evaluated on tasks like energy and force prediction for molecules.In summary, the central research aim is developing an equivariant Transformer that can effectively utilize higher degree equivariant representations, which have been inaccessible to prior equivariant models. The authors propose specific architectural changes to achieve this.
