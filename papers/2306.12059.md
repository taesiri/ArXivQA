# [EquiformerV2: Improved Equivariant Transformer for Scaling to   Higher-Degree Representations](https://arxiv.org/abs/2306.12059)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be:How can equivariant Transformers like Equiformer be scaled up to higher degrees of equivariant representations? The authors start with Equiformer and aim to modify it to work effectively with higher degree equivariant features. Equiformer and other equivariant GNNs have been limited to low degrees due to the computational complexity of tensor products. The authors investigate whether the Equiformer architecture can be adapted to leverage higher degree features using more efficient tensor product operations.Specifically, the main hypotheses appear to be:- Replacing the tensor products in Equiformer with eSCN convolutions will allow scaling to higher degree features.- Additional architectural improvements (attention renormalization, separable S2 activation, separable layer norm) will enable the model to better leverage the representation power of higher degrees. - The proposed EquiformerV2 architecture incorporating these modifications will outperform prior methods when evaluated on tasks like energy and force prediction for molecules.In summary, the central research aim is developing an equivariant Transformer that can effectively utilize higher degree equivariant representations, which have been inaccessible to prior equivariant models. The authors propose specific architectural changes to achieve this.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contributions appear to be:- Proposing an improved equivariant Transformer model called EquiformerV2 for modeling 3D atomistic systems. The improvements include using more efficient eSCN convolutions to incorporate higher-degree tensor representations, and architectural innovations like attention re-normalization, separable S2 activation, and separable layer normalization.- Demonstrating that EquiformerV2 outperforms previous state-of-the-art methods on the large-scale OC20 benchmark for predicting atomic forces and energies. EquiformerV2 achieves up to 12% lower force error and 4% lower energy error compared to prior works.- Showing that EquiformerV2 offers better speed-accuracy trade-offs compared to prior invariant and equivariant models on OC20. At the same inference speed, it is 8% more accurate than eSCN.- Demonstrating that when used in the AdsorbML algorithm for adsorption energy calculations, EquiformerV2 achieves higher success rates while requiring fewer DFT calculations compared to previous methods.In summary, the main contribution is proposing EquiformerV2, an improved equivariant Transformer model that leverages architectural innovations to better incorporate higher-degree tensor representations. Experiments demonstrate state-of-the-art performance on modeling atomic systems compared to prior equivariant and invariant models.
