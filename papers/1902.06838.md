# SC-FEGAN: Face Editing Generative Adversarial Network with User's Sketch   and Color

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper addresses is how to develop an image editing system that can generate realistic and high-quality edits to facial images based on free-form user inputs like masks, sketches, and colors. The key hypothesis is that a neural network model trained on appropriate data with proper loss functions can produce synthetic facial images that reflect a user's desired edits, as specified through free-form mask, sketch, and color inputs.Some key points:- The paper proposes a new system called SC-FEGAN that allows intuitive editing of facial images using free-form user inputs. - It aims to improve upon limitations of prior work like awkward edges, inability to handle large missing regions, and lack of user control.- The proposed model is an end-to-end trainable convolutional network that incorporates style loss for better results when large regions are missing.- It uses a U-Net architecture with gated convolutions and trained adversarially along with other loss functions.- The training data incorporates free-form masks, sketches, and color maps to enable the model to handle free-form inputs.So in summary, the main research goal is developing an intuitive facial image editing system using a specially designed deep learning model and appropriate training procedure. The key hypothesis is that this approach can enable high-quality editing from free-form user inputs.


## What is the main contribution of this paper?

The main contributions of this paper are:- They propose a novel image editing system called SC-FEGAN that can generate realistic face images from user inputs of free-form mask, sketch, and color. - They design an end-to-end trainable convolutional network architecture for SC-FEGAN. It uses a U-Net structure with gated convolutional layers and trained with a combination of losses (per-pixel, perceptual, style, total variance, and GAN loss) to produce high quality results.- They create appropriate training data of sketches, colors, and free-form masks based on eye positions and face segmentation instead of stereotyped inputs. This enables the system to handle user sketch and color input for editing.- They show that their system can intuitively edit face images by changing hair, eyes, mouth, etc. even when large portions are erased. It can also generate reasonable results from small inputs due to learning details like earrings from the training data.- Compared to previous approaches, their system produces higher quality results in shape, structure and details for large missing regions in faces through the use of sketch, color, and free-form mask as input.In summary, the main contribution is an end-to-end trainable network for high-quality face image editing that utilizes free-form user inputs of sketch, color, and mask. The design of the architecture, losses, and training data enables intuitive editing even for large missing facial regions.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

The paper presents a novel face image editing system called SC-FEGAN that can generate realistic and high quality edited images from a user's free-form mask, sketch, and color inputs. The system uses a fully convolutional network with style and perceptual losses to enable intuitive editing even with large missing regions.


## How does this paper compare to other research in the same field?

Here are a few key ways this paper on face editing GANs compares to other related work:- Uses free-form sketch and color input from the user, allowing more flexible and intuitive editing compared to fixed masks or input formats. This builds on ideas from DeepFillv2 and FaceShop.- Proposes a U-Net generator architecture with gated convolutions, which is simpler and faster than multi-stage coarse-to-fine architectures like in DeepFill. Shows this can still produce quality results.- Uses a SN-PatchGAN discriminator adapted from previous work, avoiding awkward seam artifacts.- Adds perceptual and style loss terms when training, in addition to basic GAN loss. Helps generate better details and quality especially for large edited regions. - Evaluates on higher resolution 512x512 images from CelebA-HQ, while much previous work was limited to lower resolutions like 128x128 or 256x256.- Shows examples of editing entire regions like hair and generating faces from scratch based on sketch/color. Most prior work focused more on smaller hole filling tasks.- Compares quantitatively and visually to DeepFill and FaceShop methods, showing improved performance and quality of edits.Overall, it builds incrementally on a lot of previous ideas while demonstrating higher resolution editing, flexibility via sketch/color input, simplified architecture, and improved quality of generation through new losses. Pushes forward the capabilities of face editing with GANs.


## What future research directions do the authors suggest?

The paper suggests a few future research directions:- Extending the system to handle edits and generation of full body images rather than just faces. The authors note that bodies have more complex structures and training the system on full bodies could be an interesting challenge.- Exploring alternative loss functions or network architectures that could further improve the quality and realism of the generated images. The authors mention this could help make details like hair even more realistic.- Incorporating semantic or contextual guidance into the system, rather than just low-level sketch and color information. This could potentially allow for more intuitive and meaningful edits.- Adding user controls to specify higher level attributes like age, gender, expression etc. to steer the image generation process. - Investigating interactive or iterative editing interfaces that allow the user to progressively refine the generated image.- Applying the system to other image editing tasks beyond face completion, such as retouching, morphing, or style transfer.- Exploring ways to reduce reliance on large training datasets, such as through few-shot or zero-shot learning techniques.In summary, the main future directions are extending the system to full body image generation, exploring improvements to image quality and realism, incorporating more semantic guidance, adding user controls, developing interactive interfaces, and applying the approach to other editing tasks. Reducing the data requirements is also noted as an important challenge.
