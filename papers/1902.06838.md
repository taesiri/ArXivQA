# [SC-FEGAN: Face Editing Generative Adversarial Network with User's Sketch   and Color](https://arxiv.org/abs/1902.06838)

## What is the central research question or hypothesis that this paper addresses?

 The central research question this paper addresses is how to develop an image editing system that can generate realistic and high-quality edits to facial images based on free-form user inputs like masks, sketches, and colors. 

The key hypothesis is that a neural network model trained on appropriate data with proper loss functions can produce synthetic facial images that reflect a user's desired edits, as specified through free-form mask, sketch, and color inputs.

Some key points:

- The paper proposes a new system called SC-FEGAN that allows intuitive editing of facial images using free-form user inputs. 

- It aims to improve upon limitations of prior work like awkward edges, inability to handle large missing regions, and lack of user control.

- The proposed model is an end-to-end trainable convolutional network that incorporates style loss for better results when large regions are missing.

- It uses a U-Net architecture with gated convolutions and trained adversarially along with other loss functions.

- The training data incorporates free-form masks, sketches, and color maps to enable the model to handle free-form inputs.

So in summary, the main research goal is developing an intuitive facial image editing system using a specially designed deep learning model and appropriate training procedure. The key hypothesis is that this approach can enable high-quality editing from free-form user inputs.


## What is the main contribution of this paper?

 The main contributions of this paper are:

- They propose a novel image editing system called SC-FEGAN that can generate realistic face images from user inputs of free-form mask, sketch, and color. 

- They design an end-to-end trainable convolutional network architecture for SC-FEGAN. It uses a U-Net structure with gated convolutional layers and trained with a combination of losses (per-pixel, perceptual, style, total variance, and GAN loss) to produce high quality results.

- They create appropriate training data of sketches, colors, and free-form masks based on eye positions and face segmentation instead of stereotyped inputs. This enables the system to handle user sketch and color input for editing.

- They show that their system can intuitively edit face images by changing hair, eyes, mouth, etc. even when large portions are erased. It can also generate reasonable results from small inputs due to learning details like earrings from the training data.

- Compared to previous approaches, their system produces higher quality results in shape, structure and details for large missing regions in faces through the use of sketch, color, and free-form mask as input.

In summary, the main contribution is an end-to-end trainable network for high-quality face image editing that utilizes free-form user inputs of sketch, color, and mask. The design of the architecture, losses, and training data enables intuitive editing even for large missing facial regions.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 The paper presents a novel face image editing system called SC-FEGAN that can generate realistic and high quality edited images from a user's free-form mask, sketch, and color inputs. The system uses a fully convolutional network with style and perceptual losses to enable intuitive editing even with large missing regions.


## How does this paper compare to other research in the same field?

 Here are a few key ways this paper on face editing GANs compares to other related work:

- Uses free-form sketch and color input from the user, allowing more flexible and intuitive editing compared to fixed masks or input formats. This builds on ideas from DeepFillv2 and FaceShop.

- Proposes a U-Net generator architecture with gated convolutions, which is simpler and faster than multi-stage coarse-to-fine architectures like in DeepFill. Shows this can still produce quality results.

- Uses a SN-PatchGAN discriminator adapted from previous work, avoiding awkward seam artifacts.

- Adds perceptual and style loss terms when training, in addition to basic GAN loss. Helps generate better details and quality especially for large edited regions. 

- Evaluates on higher resolution 512x512 images from CelebA-HQ, while much previous work was limited to lower resolutions like 128x128 or 256x256.

- Shows examples of editing entire regions like hair and generating faces from scratch based on sketch/color. Most prior work focused more on smaller hole filling tasks.

- Compares quantitatively and visually to DeepFill and FaceShop methods, showing improved performance and quality of edits.

Overall, it builds incrementally on a lot of previous ideas while demonstrating higher resolution editing, flexibility via sketch/color input, simplified architecture, and improved quality of generation through new losses. Pushes forward the capabilities of face editing with GANs.


## What future research directions do the authors suggest?

 The paper suggests a few future research directions:

- Extending the system to handle edits and generation of full body images rather than just faces. The authors note that bodies have more complex structures and training the system on full bodies could be an interesting challenge.

- Exploring alternative loss functions or network architectures that could further improve the quality and realism of the generated images. The authors mention this could help make details like hair even more realistic.

- Incorporating semantic or contextual guidance into the system, rather than just low-level sketch and color information. This could potentially allow for more intuitive and meaningful edits.

- Adding user controls to specify higher level attributes like age, gender, expression etc. to steer the image generation process. 

- Investigating interactive or iterative editing interfaces that allow the user to progressively refine the generated image.

- Applying the system to other image editing tasks beyond face completion, such as retouching, morphing, or style transfer.

- Exploring ways to reduce reliance on large training datasets, such as through few-shot or zero-shot learning techniques.

In summary, the main future directions are extending the system to full body image generation, exploring improvements to image quality and realism, incorporating more semantic guidance, adding user controls, developing interactive interfaces, and applying the approach to other editing tasks. Reducing the data requirements is also noted as an important challenge.


## Summarize the paper in one paragraph.

 The paper presents SC-FEGAN, a face editing generative adversarial network that takes as input a user's free-form mask, sketch, and color strokes. The proposed network uses a U-net architecture with gated convolutional layers and is trained with a combination of GAN loss, perceptual loss, style loss, and total variance loss. It generates high-quality and realistic edits even for large erased portions of the image. A key contribution is the creation of appropriate sketch and color domain training data using free-form masking and face segmentation. Comparisons to recent approaches show improved image quality and structure for free-form masks. Results demonstrate intuitive editing capabilities for facial features and hairstyles. Interestingly, the model can also generate reasonable earring details due to the training data preparation. Overall, SC-FEGAN enables easy and flexible face image editing with user sketch and color input.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the paper:

This paper presents a novel face image editing system called SC-FEGAN that allows users to edit face images by providing free-form mask, sketch, and color inputs. The system uses a fully convolutional generative adversarial network (GAN) that can be trained end-to-end. The generator network is based on a U-net architecture with gated convolutional layers, which enables it to produce high-quality 512x512 results. The discriminator uses a spectral normalized PatchGAN structure. The system is trained not just with adversarial and perceptual losses, but also with a style loss term that helps generate realistic details even when large portions of the image are missing or edited. 

The authors demonstrate the effectiveness of their approach compared to recent inpainting techniques like DeepFill v1 and v2. The additional style loss and free-form user inputs (mask, sketch, color) allow their model to plausibly generate missing facial structures and details like hair, eyes, and earrings. Interesting examples include being able to draw an earring that doesn't exist in the original image. Comparisons also show their U-net generator performs better than a coarse-to-fine network, and works well even with total image deletion. The user study shows their SC-FEGAN approach allows easy editing and outperforms existing tools for novice users.


## Summarize the main method used in the paper in one paragraph.

 The main method used in this paper is a face editing generative adversarial network (GAN) called SC-FEGAN that allows users to edit faces in images by providing free-form mask, sketch, and color inputs. The key components of SC-FEGAN are:

- A generator network based on a U-net architecture with gated convolutional layers that takes as input an incomplete image, mask, sketch, color map, and noise vector. It outputs an edited face image.

- A discriminator network based on a spectral normalized PatchGAN that discriminates between real and fake/edited images. 

- Training losses including pixel-wise loss, perceptual loss using VGG features, style loss, total variation loss, and GAN loss. The style loss based on Gram matrices is critical for handling large missing regions like hair.

- A free-form mask, sketch, and color dataset created from CelebA-HQ faces using positional eye masks, edge detection, and semantic segmentation. This better matches potential user inputs.

- End-to-end training of the generator and discriminator networks. The generator takes user inputs to edit face images in a realistic way.

In summary, SC-FEGAN allows intuitive face editing through free-form mask, sketch, and color inputs mapped to a U-net generator and PatchGAN discriminator trained with losses suited for large missing regions. The customized training data also helps produce realistic edits.


## What problem or question is the paper addressing?

 The paper is addressing the problem of face image editing using generative adversarial networks (GANs). Specifically, it aims to develop an image editing system that can generate realistic edits to faces based on a user's free-form input of masks, sketches, and colors. 

The key questions/goals of the paper are:

- How to develop an end-to-end trainable GAN system that can generate high-quality and realistic edits to face images based on free-form user input? 

- How to make the system responsive to user sketches and colors to guide the image generation?

- How to enable editing of large portions of the image, like changing the entire hairstyle, while still generating realistic results?

- How to improve on limitations of prior work like awkward edges, unrealistic details, and inability to handle large edits. 

To address these, the paper proposes a new network architecture called SC-FEGAN that combines things like gated convolutions, style loss, and SN-PatchGAN discriminator. The design choices aim to improve quality and better incorporate the user input.

So in summary, the key focus is developing an intuitive face image editing GAN that can generate realistic results guided by free-form user input like sketches and colors.
