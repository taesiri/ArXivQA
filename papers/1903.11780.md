# [Wasserstein Dependency Measure for Representation Learning](https://arxiv.org/abs/1903.11780)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central hypothesis appears to be that using the Wasserstein distance instead of KL divergence in mutual information estimators for representation learning will result in more complete learned representations. Specifically, the authors argue:- Mutual information maximization for representation learning is fundamentally limited because tight lower bounds on mutual information require sample sizes exponential in the mutual information. This makes them problematic for high mutual information datasets.- KL divergence based mutual information estimators are prone to only capturing a few factors of variation in the data, leading to incomplete representations.- Using the Wasserstein distance instead of KL divergence will mitigate this issue and learn more complete representations. The Wasserstein distance accounts for the underlying metric of the data distribution, unlike KL divergence.To test this hypothesis, the paper:- Demonstrates cases where mutual information estimation fails to learn complete representations, both in theory and experiments.- Introduces the Wasserstein dependency measure as an alternative objective using Wasserstein distance.- Shows experimentally that their proposed Wasserstein predictive coding technique learns more complete representations on designed tasks where incomplete representations are problematic.So in summary, the central hypothesis is that using Wasserstein distances instead of KL divergence for representation learning will learn more complete representations by overcoming limitations of mutual information maximization. The experiments aim to demonstrate this hypothesis.


## What is the main contribution of this paper?

The main contributions of this paper appear to be:- Demonstrating limitations of mutual information-based representation learning approaches, both theoretically and empirically. The paper shows that these methods can learn incomplete representations that capture only a few factors of variation when the mutual information is large. - Proposing the Wasserstein dependency measure as an alternative to mutual information for representation learning. This uses the Wasserstein distance instead of KL divergence.- Introducing Wasserstein predictive coding (WPC), a practical approximation of the Wasserstein dependency measure using Lipschitz regularization from GAN literature. - Showing experimentally that WPC mitigates the incomplete representation problem of mutual information approaches and learns more complete representations on a number of designed tasks as well as on real datasets like CelebA.- Analyzing the effect of dataset size, minibatch size, and neural network inductive biases, and showing WPC is more robust in various settings.So in summary, the main contribution appears to be identifying limitations of mutual information approaches for representation learning, proposing the Wasserstein dependency measure as a solution, and providing experimental evidence that a practical approximation can learn more complete representations.


## How does this paper compare to other research in the same field?

Here are a few key points about how this paper compares to other research in representation learning:- It focuses on limitations of mutual information-based representation learning methods like contrastive predictive coding (CPC). Prior work has shown CPC can learn useful representations, but this paper argues it struggles with high mutual information data.- It proposes an alternative called the Wasserstein dependency measure that uses the Wasserstein distance instead of KL divergence. This is motivated by the theoretical benefits of Wasserstein vs KL for dependency measurement. - It introduces a practical method called Wasserstein predictive coding (WPC) to approximate the proposed Wasserstein dependency measure. WPC combines ideas from CPC and Wasserstein GAN regularization.- Experiments across several synthetic and real datasets demonstrate WPC mitigates issues with CPC, leading to better representation learning. WPC seems robust to factors like dataset size and batch size.- The techniques focus on unsupervised representation learning. Much prior work has explored supervised representation learning, but unsupervised settings remain challenging. The proposed ideas could help advance unsupervised methods.- For estimation, the paper uses simple gradient penalty regularization from WGAN. More advanced Lipschitz constraint techniques could further improve WPC performance.Overall, this paper makes both theoretical and practical contributions for unsupervised representation learning. It identifies limitations of existing mutual information methods, proposes the Wasserstein dependency measure as an alternative, and provides promising experimental results with WPC. More work is needed to scale up WPC, but it seems a worthwhile direction.
