# [Wasserstein Dependency Measure for Representation Learning](https://arxiv.org/abs/1903.11780)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central hypothesis appears to be that using the Wasserstein distance instead of KL divergence in mutual information estimators for representation learning will result in more complete learned representations. Specifically, the authors argue:- Mutual information maximization for representation learning is fundamentally limited because tight lower bounds on mutual information require sample sizes exponential in the mutual information. This makes them problematic for high mutual information datasets.- KL divergence based mutual information estimators are prone to only capturing a few factors of variation in the data, leading to incomplete representations.- Using the Wasserstein distance instead of KL divergence will mitigate this issue and learn more complete representations. The Wasserstein distance accounts for the underlying metric of the data distribution, unlike KL divergence.To test this hypothesis, the paper:- Demonstrates cases where mutual information estimation fails to learn complete representations, both in theory and experiments.- Introduces the Wasserstein dependency measure as an alternative objective using Wasserstein distance.- Shows experimentally that their proposed Wasserstein predictive coding technique learns more complete representations on designed tasks where incomplete representations are problematic.So in summary, the central hypothesis is that using Wasserstein distances instead of KL divergence for representation learning will learn more complete representations by overcoming limitations of mutual information maximization. The experiments aim to demonstrate this hypothesis.
