# [BenchLMM: Benchmarking Cross-style Visual Capability of Large Multimodal   Models](https://arxiv.org/abs/2312.02896)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a high-quality paragraph summarizing the key points of the paper:

This paper proposes BenchLMM, a novel benchmark to assess large multimodal models (LMMs) on their capability of reasoning images with different styles, including artistic styles (cartoon, painting, etc.), sensor styles (infrared, X-ray, etc.), and application styles (autonomous driving, defect detection, etc.). The paper reveals that existing LMMs, even the commercial model GPT-4V, generally underperform on non-common image styles compared to regular photographic images. Also, superior performance on common images does not guarantee better reasoning capability on other image styles. To enhance LMMs, the authors propose a simple yet effective style prompt enhancement method by instructing LMMs to predict image style first before answering questions. Moreover, through an error reflection analysis, the authors find commercially stronger LMMs like GPT-4V can provide detailed reasoning processes to self-diagnose mistakes, whereas open-source LMMs merely rephrase correct answers, showing intelligent LMMs should have such capability. The proposed benchmark and analysis offer comprehensive tools and insights to facilitate future LMM research towards more intelligent and versatile reasoning.
