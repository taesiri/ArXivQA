# [New logarithmic step size for stochastic gradient descent](https://arxiv.org/abs/2404.01257)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem Statement
The paper addresses the challenge of selecting an appropriate step size (learning rate) for the stochastic gradient descent (SGD) algorithm when training deep neural networks. The step size value significantly impacts the convergence rate of SGD. If too large, SGD may not reach the optimal solution, and if too small, it converges slowly or gets stuck in local minima. 

Existing step size schemes have limitations - constant step size lacks adaptation; decaying step sizes converge slowly initially; periodic step sizes like cosine annealing need manual tuning of hyperparameters.

Proposed Solution  
The paper proposes a new logarithmic decay step size for SGD with periodic warm restarts. The key ideas are:

- The step size decays as per $\eta_t = \eta_0(1 - \frac{\ln t}{\ln T})$ where T is the restart period. This ensures larger step sizes initially and smaller ones later for faster convergence.

- Periodic warm restarts reinitialize the step size to the maximal value $\eta_0$, providing fresh momentum to escape local minima.

- Theoretical analysis shows this logarithmic decay step size achieves the optimal $O(1/\sqrt{T})$ convergence rate for non-convex smooth functions.

Main Contributions
- The proposed logarithmic step size has desirable properties - slower decay than exponential/polynomial schedules, yet faster than cosine; and assigns higher selection probability to final iterations.

- Establishes the optimal $O(1/\sqrt{T})$ convergence rate for SGD with the new step size by milder assumptions on initial step size compared to prior arts.

- Empirical evaluation on MNIST, CIFAR10 and CIFAR100 datasets demonstrates superior performance over state-of-the-art step size schemes. For CIFAR100, achieves 0.9% better test accuracy compared to cosine step size when using CNNs.

In summary, the paper presents a novel logarithmic step size scheme for SGD that enjoys strong theoretical convergence guarantees and also demonstrates improved empirical performance over existing methods on benchmark deep learning tasks.
