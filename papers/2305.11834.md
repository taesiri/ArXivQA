# Pengi: An Audio Language Model for Audio Tasks

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be: How can we develop a general-purpose audio language model capable of handling both open-ended and close-ended audio tasks without needing additional fine-tuning or task-specific model extensions?The key hypotheses appear to be:1) Framing all audio tasks as text-generation tasks and using a unified model architecture and training procedure can enable good performance on diverse audio tasks.2) Using audio task templates inspired by instruction tuning during training can help the model learn useful representations and conditioning behaviors for different audio tasks. 3) Combining an audio encoder, text encoder, mapping networks and a pre-trained causal language model can create a model capable of handling both open-ended generation tasks like audio captioning as well as close-ended tasks like classification through text matching.So in summary, the main research direction seems to be exploring how to create a versatile audio language model for general audio understanding by framing tasks as conditional text generation problems and leveraging instruction tuning, transfer learning and a unified model architecture. The performance of this model across a range of open and closed-ended audio tasks is then used to evaluate the potential of this approach.


## What is the main contribution of this paper?

The main contributions of this paper are:1. Proposing Pengi, a novel Audio Language Model (ALM) that can perform both open-ended and close-ended audio tasks using a unified architecture and training procedure. Pengi is the first ALM capable of handling such diverse audio tasks. 2. Framing all audio tasks as text generation tasks, where the model takes audio and text as input and generates text as output. This allows converting any audio task into the audio-text-to-text format.3. Designing audio task templates for training inspired by instruction tuning from NLP. This allows the model to learn from diverse tasks and improve its audio representations.4. Evaluating Pengi extensively on 21 downstream tasks from different audio domains. Pengi achieves state-of-the-art results on several open-ended and close-ended tasks, establishing a strong baseline for general-purpose ALMs.5. Demonstrating that connecting language models with audio models is a promising direction towards building versatile models for general audio understanding.In summary, the main contribution is proposing and benchmarking Pengi, the first general-purpose Audio Language Model that can perform well on both open-ended and close-ended audio tasks using a unified architecture.
