# [Are Targeted Messages More Effective?](https://arxiv.org/abs/2403.06817)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem: 
The paper studies two versions of graph neural networks (GNNs) - 1-sided GNNs (1-GNNs) where messages passed between nodes depend only on the source node state, and 2-sided GNNs (2-GNNs) where messages depend on the states of both the source and target nodes. The key question studied is whether 2-GNNs are more expressive than 1-GNNs in terms of the computational power and functions they can represent.

The problem is motivated by the fact that both 1-GNNs and 2-GNNs are used in practice, with a perception that 2-GNNs are more powerful, but limited theoretical understanding of their relative expressivity. The study also connects to distinguishing two logical fragments - modal and guarded fragments - of first-order logic with counting.

Proposed Solution and Contributions:

1. The paper proves that in the uniform setting, there exists a query computable by a 2-GNN that cannot be computed by any 1-GNN. This is true for GNNs using SUM aggregation and establishes that 2-GNNs are strictly more expressive.

2. For non-uniform expressivity, where GNNs can vary with input size, the paper leverages known logical characterizations to show that 1-GNNs and 2-GNNs have equivalent power. This holds even for polynomial-size, bounded-depth GNN families. 

3. A main technical contribution is a proof that the modal and guarded fragments of first-order logic with counting have equivalent expressivity over undirected graphs. This implies the equivalence between 1-GNNs and 2-GNNs in non-uniform settings. The proof uses novel ideas like hashing sets to signatures.

4. For GNNs using MAX or MEAN aggregation, the paper shows 1-GNNs can simulate 2-GNNs, proving equivalent expressivity.

In summary, the paper provides a comprehensive study of the expressivity of 1-GNNs versus 2-GNNs, in both uniform and non-uniform settings, using logical tools. The results reveal a nuanced picture of their relationship and multiple technical contributions towards separating or equating their power.


## Summarize the paper in one sentence.

 This paper studies the expressive power of graph neural networks with 1-sided versus 2-sided messages, as well as the related modal and guarded fragments of first-order logic with counting, showing that the modal and guarded fragments have equal expressive power which implies that 1-sided and 2-sided GNNs have the same expressivity in certain non-uniform settings, while in the uniform setting 2-sided GNNs can be strictly more expressive than 1-sided GNNs when using SUM aggregation.


## What is the main contribution of this paper?

 This paper studies the expressive power of two versions of graph neural networks (GNNs):

1. 1-GNNs, where messages passed between nodes depend only on the state of the source node.

2. 2-GNNs, where messages depend on the states of both the source and target nodes. 

The key contributions are:

1. Proving that in the uniform setting, there is a query expressible by a 2-GNN that is not expressible by any 1-GNN. This separation relies on using SUM aggregation and does not hold for MEAN or MAX aggregation.

2. Proving that in non-uniform settings, where the GNN can depend on the graph size, 1-GNNs and 2-GNNs have the same expressive power. This holds even for polynomial-size GNNs with bounded depth. 

3. Connecting these GNN variants to modal and guarded fragments of first-order logic with counting (FOC) and proving that these fragments have the same expressive power. This implies the equivalence in the non-uniform setting.

4. Introducing new proof techniques for lower bounds on GNN expressiveness, based on invariants about "nice" polynomials and hashing sets to signatures.

So in summary, the paper provides adetailed comparison of different GNN architectures and their logical equivalents, separating them in some cases and unifying them in others. The logical connection also enables transfer of expressivity results between the ML and logic settings.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper summary, some of the key terms and concepts include:

- Graph neural networks (GNNs)
- Message passing 
- 1-sided GNNs vs 2-sided GNNs
- Uniform vs non-uniform expressivity
- Modal vs guarded fragments of first-order logic 
- Counting logic
- Hashing techniques
- Inexpressibility results
- Weisfeiler-Leman algorithm

The paper studies the expressive power and limitations of 1-sided and 2-sided graph neural networks, which differ in how messages passed between nodes depend on the source and target states. It relates this to modal and guarded fragments of first-order logic with counting. Key findings include:

- 2-GNNs are more expressive than 1-GNNs for uniform expressivity with SUM aggregation 
- 1-GNNs and 2-GNNs have equivalent non-uniform expressivity
- Modal and guarded fragments of first-order logic with counting have equal expressive power
- New hashing techniques and inexpressibility results for GNNs not based on the Weisfeiler-Leman algorithm

So in summary, key terms revolve around GNN variants, logics, expressivity notions, and proof techniques for separating models.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 potential in-depth questions about the method proposed in this paper:

1. The paper states that 2-GNNs express the query that selects all vertices of a graph that have a neighbour of larger degree, while this query is not expressible by a 1-GNN. Why is this specific query an effective one for demonstrating this separation in expressivity? Are there other similar queries that would also demonstrate this separation?

2. The proof that the query is not expressible by a 1-GNN relies on a notion of "fast convergence" to a polynomial. What is the intuition behind this concept and why is it useful for the proof technique? 

3. The paper shows that 1-GNNs and 2-GNNs have equivalent expressivity in certain non-uniform settings. What are the implications of having equivalent expressivity in these settings but not in a uniform setting? When is each type of expressivity result more relevant?

4. Theorem 3.1 states that for MEAN or MAX aggregation, 1-GNNs can simulate 2-GNNs. What is it about SUM aggregation specifically that enables the separation between 1-GNNs and 2-GNNs?  

5. The proof technique involves designing a 1-GNN to simulate a 2-GNN using one-hot encodings of state values. What is the intuition behind why one-hot encodings are useful here? Are there any limitations or downsides?

6. The paper draws interesting connections between GNN expressivity and modal/guarded fragments of first-order logic. Are there other logics that have interesting connections to GNN architectures that could be further explored?

7. Could the hashing technique used in the proof that the modal and guarded fragments have equal expressivity be applied in other contexts? What are its limitations?

8. Is the model considered in the paper (undirected vertex-labeled graphs, restriction to unary queries) representative of how GNNs are used in practice? How robust are the results to modifications to the model?  

9. The paper leaves open whether recurrent 2-GNNs could be more expressive than recurrent 1-GNNs. What challenges arise in resolving this question that do not come up in the feedforward setting?

10. From an applications perspective, what are the most significant practical implications of the main theoretical results concerning relative expressivity of 1-GNNs and 2-GNNs?
