# [LEACE: Perfect linear concept erasure in closed form](https://arxiv.org/abs/2306.03819)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is:How can we optimally edit a representation X to remove linearly encoded information about a protected attribute Z, while minimizing collateral damage to other useful information in X?The key contributions and hypotheses appear to be:1) A set of theoretical results showing the equivalence between several conditions related to linearly guarding Z in X, including:- X linearly guards Z - The trivial constant predictor achieves optimal loss for all convex losses- The classconditional means of X given Z are all equal - The cross-covariance between X and Z is zero2) A closed-form solution called LEACE which provably finds the optimal affine edit to X that removes linear correlations with Z while minimizing mean squared error under any seminorm. 3) A method called "concept scrubbing" which applies LEACE sequentially to all layers of a neural network to remove target concepts.The central hypothesis seems to be that LEACE will allow more surgical removal of concepts from neural networks compared to prior work like INLP, better preserving useful information while still effectively guarding concepts of interest. The paper aims to demonstrate this via experiments on debiasing and amnesic probing.Does this seem like an accurate high-level summary of the core research question and contributions? Let me know if you need any clarification or have additional thoughts!


## What is the main contribution of this paper?

This paper introduces LEACE (LEAst-squares Concept Erasure), a new method for concept erasure that has the following key features:- It provides a closed-form solution to erase the information about a protected attribute Z from a representation X, by making an affine transformation to X that minimizes the mean squared distance while guaranteeing that linear classifiers cannot predict Z from the transformed representation.- It proves this solution is optimal with respect to all norms induced by inner products, including Euclidean and Mahalanobis norms. - It shows that three prior methods (SAL, Mean Projection, and Fair PCA) achieve perfect linear erasure, but are suboptimal in terms of representational distance. - It applies LEACE through a new technique called "concept scrubbing" which removes target concepts from every layer of a neural network.- It demonstrates the effectiveness of LEACE for reducing gender bias in BERT and accurately measuring reliance of LMs on part-of-speech using concept scrubbing.The key innovation is deriving the closed-form LEACE solution, proving its optimality, and showing it enables surgical interventions in neural networks to remove specific concepts while minimizing collateral damage. This improves over prior work which required optimization and caused unnecessary changes to useful features unrelated to the concept being erased.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper introduces LEACE, a closed-form method for perfectly removing linear correlations between a representation and a target variable while minimally changing the representation, theoretically characterizes the set of transformations that achieve perfect linear erasure, and applies LEACE to language models via a procedure called concept scrubbing which removes target information from every layer.
