# [Cross-BERT for Point Cloud Pretraining](https://arxiv.org/abs/2312.04891)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a high-quality paragraph summarizing the key points of the paper:

This paper proposes Cross-BERT, a novel cross-modal framework for self-supervised pretraining of 3D point clouds using 2D images. It addresses the challenges of effectively integrating information from different modalities through two specially designed pretraining tasks: Point-Image Alignment (PIA) and Masked Cross-modal Modeling (MCM). PIA aligns the features between modalities using both inter-modality and intra-modality contrastive objectives to obtain discriminative yet aligned representations. MCM improves the conventional mask modeling task by incorporating high-level semantic guidance from the cross-modal interactions. Extensive experiments on various downstream tasks like classification, segmentation, and detection demonstrate that Cross-BERT produces high-quality and versatile 3D representations by smoothly reconstructing the masked tokens. It outperforms prior state-of-the-art methods on multiple datasets across both synthetic and real-world scenarios. The results highlight the effectiveness of leveraging 2D knowledge to complement 3D point clouds and utilizing the advantages of BERT-style pretraining across modalities.
