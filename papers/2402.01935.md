# [Code Representation Learning At Scale](https://arxiv.org/abs/2402.01935)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem Statement:
- Recent large code language models have shown promising results on downstream code generation tasks using very limited pretraining corpora compared to large natural language models. However, code representation learning lags behind in leveraging large corpora to develop general-purpose programming language embedding models. 

- The standard masking strategy from BERT of randomly replacing 80% tokens with [MASK], 10% with random tokens and keeping 10% unchanged leads to deteriorated representations for code as it can alter semantics and structure.

Methodology:
- Propose CodeSage, a bidirectional Transformer encoder trained with two-stage pretraining on a vast corpus of 237M code files and 75M text-code pairs across 9 languages.

- Stage 1: Jointly train on identifier deobfuscation (DOBF) to predict masked identifier names and a customized masking strategy without replacing tokens to retain structure. DOBF encodes semantics and structure while the masking strategy learns beyond just identifiers.

- Stage 2: Further enhance representations via bimodal contrastive learning on constructed hard positives (text snippets, function body pairs with signatures/return statements removed) and hard negatives (close yet incorrect examples in embedding space).

Main Contributions:
- Developed effective token-level denoising scheme retaining structure for pretraining code models at scale. DOBF and no-replacement masking complement each other.

- Demonstrated the importance of constructing hard positives and hard negatives for contrastive learning. Removing shortcuts in text-code pairs and approximating hard negatives boosts performance.  

- Established SoTA off-the-shelf models across variety of downstream tasks through two-stage pretraining fueled by vast corpora. Detailed analysis provided on factors contributing to representation learning.

- Released CodeSage models to facilitate future research on leveraging large corpora for programming languages.
