# [Source-free Video Domain Adaptation by Learning Temporal Consistency for   Action Recognition](https://arxiv.org/abs/2203.04559)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is:How can we perform source-free video domain adaptation (SFVDA) to enable video models to adapt to new target domains without accessing the original source training data? The key hypothesis proposed in the paper is:Learning temporal consistency across local temporal features, by enforcing feature consistency and source prediction consistency, will allow learning effective overall temporal features for SFVDA. Attending to local features with higher source prediction confidence will further help align the adapted model to the source data distribution.In summary, the main goals are:1) Formulate the novel problem of SFVDA which aims to tackle privacy issues in conventional VUDA. 2) Propose a method called ATCoN which learns temporal consistency via feature and prediction consistency to obtain effective overall temporal features for SFVDA.3) Validate ATCoN's efficacy empirically and demonstrate state-of-the-art performance on various cross-domain action recognition benchmarks.


## What is the main contribution of this paper?

The main contribution of this paper is proposing a novel method called Attentive Temporal Consistent Network (ATCoN) to tackle the problem of Source-Free Video Domain Adaptation (SFVDA). The key points are:1. The paper formulates a new problem setting called SFVDA, which aims to adapt a video model trained on source data to a target domain without accessing the source data. This addresses privacy concerns with standard video domain adaptation. 2. The paper proposes ATCoN to address SFVDA. The key ideas are:- Learning temporal consistency between local temporal features extracted from the video via two objectives - feature consistency and source prediction consistency. This allows extracting effective overall temporal features. - Attending to local features using relevance weights based on their prediction confidence on the source classifier. This aligns the features better to the source.3. Experiments on multiple video domain adaptation benchmarks demonstrate state-of-the-art performance of ATCoN for SFVDA compared to prior source-free domain adaptation methods.4. Ablation studies validate the efficacy of the temporal consistency learning and local feature weighting mechanisms in ATCoN.In summary, the main contribution is proposing a novel method ATCoN to effectively tackle the new problem of source-free video domain adaptation by learning temporal consistency and attending to confident local features.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points in the paper:The paper proposes a novel method called Attentive Temporal Consistent Network (ATCoN) to tackle the challenging problem of Source-Free Video Domain Adaptation (SFVDA) for action recognition, where ATCoN learns temporal consistency across video clips via feature consistency and source prediction consistency losses and attends to more relevant clips when aggregating the video-level representation.


## How does this paper compare to other research in the same field?

Here are a few key points about how this paper compares to other research in video domain adaptation:- This is the first work to tackle the problem of source-free video domain adaptation (SFVDA). Prior works have studied source-free domain adaptation for images, but adapting videos without source data access is more challenging due to the additional temporal features. This paper formulates a new realistic and important problem setting.- The proposed method ATCoN introduces a novel approach of learning temporal consistency via feature consistency and source prediction consistency losses. This is a unique way to obtain effective temporal features without access to source data or labels. Most prior domain adaptation works rely on aligning source and target features directly.- Experiments across multiple video DA benchmarks show ATCoN outperforms prior state-of-the-art source-free DA methods by a large margin. The consistent gains highlight the benefits of the temporal consistency modeling for adapting videos.- Compared to standard unsupervised video DA methods that use source data, ATCoN achieves competitive or even better accuracy. This is impressive given the more challenging setting without source data access.- Ablation studies validate the contributions of the main components of ATCoN - the temporal consistency losses and the local relevance weighting. This provides insight into what drives the model's effectiveness.Overall, this paper makes both conceptual and technical contributions to the field. It opens up a new research direction in privacy-preserving video domain adaptation. The temporal consistency modeling also provides a novel perspective compared to existing feature alignment techniques for DA. The comprehensive experiments demonstrate the promise of this approach for source-free video adaptation.
