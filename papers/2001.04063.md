# ProphetNet: Predicting Future N-gram for Sequence-to-Sequence   Pre-training

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central hypothesis is that predicting future n-grams during pre-training of a sequence-to-sequence model will improve the model's performance on downstream natural language generation tasks compared to only predicting the next token as is typically done. The key claims of the paper are:- Introducing an objective of predicting future n-grams during pre-training forces the model to plan ahead rather than just relying on strong local correlations.- The proposed ProphetNet architecture, which uses n-stream self-attention to enable efficient n-gram prediction during pre-training, achieves state-of-the-art results on summarization and question generation benchmarks.- Pre-training ProphetNet on a large 160GB corpus leads to significant gains over strong baselines like BART and T5 which used more pre-training data.So in summary, the central hypothesis is that future n-gram prediction is a better pre-training objective for sequence-to-sequence models compared to next token prediction, and the ProphetNet architecture realizes this idea effectively. The experimental results on downstream NLG tasks support these claims.


## What is the main contribution of this paper?

The main contributions of this paper are:1. It proposes ProphetNet, a new sequence-to-sequence pre-training model for natural language generation tasks. 2. It introduces a novel self-supervised objective called future n-gram prediction, which predicts the next n tokens simultaneously based on the previous context. This encourages the model to plan ahead and capture longer-term dependencies.3. It proposes the n-stream self-attention mechanism to efficiently implement the future n-gram prediction during training. This includes a main stream and n predicting streams.4. It shows state-of-the-art results on CNN/DailyMail, Gigaword, and SQuAD question generation tasks compared to models pre-trained on the same datasets.5. It achieves new state-of-the-art results on CNN/DailyMail and Gigaword using only 1/3 and 1/5 of the pre-training data that previous models used.In summary, the key innovation is the future n-gram prediction objective and n-stream self-attention during pre-training. This improves model planning and dependency modeling for natural language generation. The results show significant improvements over previous pre-trained models, especially with less pre-training data.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper presents ProphetNet, a new sequence-to-sequence pre-training model that introduces a novel self-supervised objective called future n-gram prediction and an n-stream self-attention mechanism to predict the next n tokens based on previous context, achieving state-of-the-art results on summarization and question generation tasks.


## How does this paper compare to other research in the same field?

Here are my thoughts on how this paper compares to other research on sequence-to-sequence pre-training models:- This paper introduces a new self-supervised objective called future n-gram prediction, which predicts the next n tokens at each time step during pre-training. This is different from most prior work like BERT, GPT, etc. which only predict the next token. The future n-gram prediction encourages better long-term planning and prevents overfitting on local correlations.- The paper proposes a novel n-stream self-attention mechanism to achieve this future n-gram prediction efficiently during training, while still being able to convert to standard autoregressive generation during inference. This allows leveraging the future n-gram prediction for better pre-training, without sacrificing inference efficiency.- The paper shows strong empirical results on multiple NLG datasets by pre-training on a 16GB corpus or a 160GB corpus. On 160GB pre-training, ProphetNet achieves SOTA results on CNN/DailyMail and Gigaword using much less pre-training data than models like T5, BART or PEGASUS.- Compared to other recent pre-training methods like BART and MASS, ProphetNet introduces the novel future n-gram prediction objective. The core architecture is still a standard Transformer encoder-decoder, unlike T5 which explores various architectural changes.- The scale of pre-training data used in this work (max 160GB) is smaller than several other recent efforts like T5 (750GB), PEGASUS (3.8TB). But the results show ProphetNet can make very efficient use of pre-training data.Overall, the paper makes useful innovations in the pre-training objectives and self-attention mechanism over standard methods, while keeping the overall model architecture simple. The strong results on multiple NLG datasets highlight the benefits of the proposed pre-training approach.


## What future research directions do the authors suggest?

The authors suggest a few potential future research directions:- Applying ProphetNet to more downstream NLU and NLG tasks. The paper mainly evaluated ProphetNet on summarization and question generation, so testing it on a broader range of tasks could be interesting.- Pre-training ProphetNet with other pre-training objectives besides the masked language modeling used in the paper. They suggest the model could potentially benefit from other self-supervised tasks. - Pre-training ProphetNet on larger datasets like the 750GB C4 corpus used to train T5. They showed impressive results pre-training on much less data than models like BART and T5, so scaling up could lead to further gains.- Exploring different model architectures in addition to the Transformer encoder-decoder structure they used. The paper focuses on the future n-gram prediction objective, but the model architecture could likely be improved as well.- Analyzing what linguistic abilities and long-term dependencies ProphetNet learns compared to standard autoregressive models. The paper hypothesizes ProphetNet should learn better long-term planning but does not deeply analyze what specifically it learns.- Incorporating techniques like reinforcement learning and mutual information maximization that have been explored to model future dependencies. ProphetNet uses a simple n-gram prediction approach, but more complex methods could be beneficial.In summary, the main future directions are applying ProphetNet more broadly, using larger pre-training data, analyzing what linguistic abilities it learns, and exploring architectural or modeling enhancements to better capture long-term dependencies.


## Summarize the paper in one paragraph.

Here is a one paragraph summary of the paper:This paper presents ProphetNet, a new sequence-to-sequence pre-training model for natural language generation tasks. ProphetNet introduces a novel self-supervised objective called future n-gram prediction, where the model predicts the next n tokens simultaneously based on previous context at each time step. This explicit future n-gram prediction encourages the model to plan ahead and prevents overfitting on local correlations. ProphetNet employs an n-stream self-attention mechanism to efficiently predict the future n-grams during training while still being able to convert to a regular autoregressive Transformer for inference. The authors pre-train ProphetNet using two datasets: a 16GB corpus (BookCorpus + Wikipedia) and a 160GB corpus. The pre-trained ProphetNet achieves state-of-the-art results on the CNN/DailyMail and Gigaword summarization benchmarks as well as the SQuAD question generation dataset, outperforming models pretrained on the same datasets. ProphetNet also achieves new SOTA on CNN/DailyMail and Gigaword compared to models pretrained on much larger datasets, demonstrating the effectiveness and efficiency of the proposed future n-gram prediction pretraining approach.
