# ProphetNet: Predicting Future N-gram for Sequence-to-Sequence   Pre-training

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central hypothesis is that predicting future n-grams during pre-training of a sequence-to-sequence model will improve the model's performance on downstream natural language generation tasks compared to only predicting the next token as is typically done. The key claims of the paper are:- Introducing an objective of predicting future n-grams during pre-training forces the model to plan ahead rather than just relying on strong local correlations.- The proposed ProphetNet architecture, which uses n-stream self-attention to enable efficient n-gram prediction during pre-training, achieves state-of-the-art results on summarization and question generation benchmarks.- Pre-training ProphetNet on a large 160GB corpus leads to significant gains over strong baselines like BART and T5 which used more pre-training data.So in summary, the central hypothesis is that future n-gram prediction is a better pre-training objective for sequence-to-sequence models compared to next token prediction, and the ProphetNet architecture realizes this idea effectively. The experimental results on downstream NLG tasks support these claims.


## What is the main contribution of this paper?

The main contributions of this paper are:1. It proposes ProphetNet, a new sequence-to-sequence pre-training model for natural language generation tasks. 2. It introduces a novel self-supervised objective called future n-gram prediction, which predicts the next n tokens simultaneously based on the previous context. This encourages the model to plan ahead and capture longer-term dependencies.3. It proposes the n-stream self-attention mechanism to efficiently implement the future n-gram prediction during training. This includes a main stream and n predicting streams.4. It shows state-of-the-art results on CNN/DailyMail, Gigaword, and SQuAD question generation tasks compared to models pre-trained on the same datasets.5. It achieves new state-of-the-art results on CNN/DailyMail and Gigaword using only 1/3 and 1/5 of the pre-training data that previous models used.In summary, the key innovation is the future n-gram prediction objective and n-stream self-attention during pre-training. This improves model planning and dependency modeling for natural language generation. The results show significant improvements over previous pre-trained models, especially with less pre-training data.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper presents ProphetNet, a new sequence-to-sequence pre-training model that introduces a novel self-supervised objective called future n-gram prediction and an n-stream self-attention mechanism to predict the next n tokens based on previous context, achieving state-of-the-art results on summarization and question generation tasks.
