# [Improving Spatiotemporal Self-Supervision by Deep Reinforcement Learning](https://arxiv.org/abs/1807.11293)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is: How can reinforcement learning be used to improve spatiotemporal self-supervision for learning powerful visual representations?More specifically, the paper proposes using deep reinforcement learning to learn an adaptive policy for sampling permutations during self-supervised training. The goal is to sample permutations that are most useful for the network to learn good representations at each stage of training. This allows the permutations to adapt to the evolving state of the network.The key hypothesis is that sampling permutations in an adaptive, non-random way during self-supervised training will lead to improved visual representations compared to using random permutations.In summary, the paper presents a reinforcement learning framework for adaptive permutation sampling to improve self-supervised visual representation learning in both the spatial and temporal domains. The central hypothesis is that this adaptive approach will outperform standard random permutation sampling.


## What is the main contribution of this paper?

The main contribution of this paper is developing a reinforcement learning-based approach to improve spatiotemporal self-supervision for learning convolutional neural network (CNN) feature representations. Specifically, the key ideas/contributions are:- Proposing a method to jointly learn from spatial and temporal shuffling as complementary self-supervision tasks for unsupervised representation learning. - Using a policy network based on reinforcement learning to dynamically propose permutations (shufflings) for training the main CNN network. The policy adapts the permutations based on the state of the CNN being trained.- Showing improved performance on benchmark tasks compared to prior self-supervision methods that use fixed random permutations. The learned features transfer better to tasks like image classification, object detection, segmentation, etc.- Analyzing the effect of the proposed approach through ablations and visualizations. Combining spatial and temporal tasks improves over individual tasks. The policy helps select useful permutations leading to faster improvement.So in summary, the main contribution is developing an adaptive reinforcement learning-based approach for sampling permutations to improve self-supervision from spatial and temporal ordering tasks. This leads to learning better general visual representations in an unsupervised manner.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper proposes using deep reinforcement learning to train a policy network that adaptively selects spatiotemporal permutations for self-supervised learning of convolutional neural network representations.


## How does this paper compare to other research in the same field?

Here are a few key points comparing this paper to other related work in self-supervised learning:- Combines spatial and temporal ordering tasks into a joint spatiotemporal self-supervision framework. Most prior work has focused on either spatial or temporal ordering tasks separately. Jointly training on both provides complementary signals.- Introduces a reinforcement learning policy to adaptively sample permutations during training based on the model's current state. Other self-supervision methods use fixed random permutations. The policy allows more informative permutations to be selected as the model evolves.- Achieves competitive results on standard benchmarks like ImageNet classification and Pascal VOC transfer tasks compared to prior self-supervision methods. The joint spatiotemporal training and adaptive permutation sampling appear to provide better representations.- Uses a relatively simple backbone architecture (CaffeNet) compared to some other recent self-supervision papers that use deeper ResNet models. This suggests the approach itself provides gains above just model size.- Provides ablation studies and analysis to isolate the improvements from the joint training and adaptive permutations. This demonstrates the value of the key components.- The method does require some extra computation for the policy network and validation, but analysis shows it is relatively small compared to the base self-supervised training.Overall, the paper makes notable contributions in adapting self-supervision for ordering tasks by training jointly over space and time and learning a adaptive permutation policy. The results demonstrate this provides better representations than prior work while still using a simple base architecture. The approach seems promising for further exploration.


## What future research directions do the authors suggest?

Based on my reading of the paper, some key future research directions suggested by the authors include:- Exploring other reinforcement learning algorithms besides policy gradient, such as Q-learning, to optimize the permutation selection policy. The authors used policy gradient but mention Q-learning as another option.- Trying more complex and deeper network architectures beyond CaffeNet. The authors used a relatively simple CNN architecture and suggest experimenting with more advanced and deeper models.- Evaluating the approach on larger datasets beyond Imagenet and UCF101/HMDB51. The authors demonstrate results on standard benchmarks but suggest applying the method to larger-scale datasets.- Combining the spatiotemporal self-supervision approach with other self-supervised tasks like colorization, context prediction, etc. The authors propose jointly training spatial and temporal shuffle tasks, but suggest combining with other tasks too.- Exploring curriculum learning schedules predicted by the policy, rather than randomly sampled permutations. The policy currently does not follow a curriculum, but learning one is suggested. - Reducing the computational overhead of the policy training, which currently requires additional validation. More efficient approximations to compute the policy's state and reward are suggested.- Applying the reinforcement learning framework to other computer vision problems besides self-supervision. The authors propose it for optimizing permutations here but suggest it may benefit other vision tasks.In summary, the main future directions are exploring other algorithms, network architectures, datasets, task combinations, curriculum learning, efficiency improvements, and applications of deep reinforcement learning to vision.


## Summarize the paper in one paragraph.

The paper proposes an approach for improving spatiotemporal self-supervision of convolutional neural networks (CNNs) by using deep reinforcement learning. The key ideas are:- Self-supervision based on ordering tasks like spatial image shuffling or temporal video frame shuffling provide a rich training signal for CNNs. Previous works have randomly sampled permutations for shuffling, but optimal permutations likely depend on the state of the network being trained. - A reinforcement learning agent is proposed to learn a policy for adaptively selecting permutations based on the network state. The policy receives the validation error of permutations as state, and improvement in validation error over a baseline as reward. - Experiments show the learned adaptive permutation selection improves unsupervised feature learning on standard benchmarks like image classification, retrieval, detection and segmentation. Combining spatial and temporal shuffling tasks also improves over individual tasks. The approach is computationally efficient, with only 40% overhead compared to basic self-supervised training.In summary, this paper introduces an elegant method to make self-supervision by shuffling more adaptive and learn even better visual features, by using reinforcement learning to optimize the core shuffling permutations based on the network's state during training.
