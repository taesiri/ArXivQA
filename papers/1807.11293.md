# [Improving Spatiotemporal Self-Supervision by Deep Reinforcement Learning](https://arxiv.org/abs/1807.11293)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is: How can reinforcement learning be used to improve spatiotemporal self-supervision for learning powerful visual representations?More specifically, the paper proposes using deep reinforcement learning to learn an adaptive policy for sampling permutations during self-supervised training. The goal is to sample permutations that are most useful for the network to learn good representations at each stage of training. This allows the permutations to adapt to the evolving state of the network.The key hypothesis is that sampling permutations in an adaptive, non-random way during self-supervised training will lead to improved visual representations compared to using random permutations.In summary, the paper presents a reinforcement learning framework for adaptive permutation sampling to improve self-supervised visual representation learning in both the spatial and temporal domains. The central hypothesis is that this adaptive approach will outperform standard random permutation sampling.


## What is the main contribution of this paper?

The main contribution of this paper is developing a reinforcement learning-based approach to improve spatiotemporal self-supervision for learning convolutional neural network (CNN) feature representations. Specifically, the key ideas/contributions are:- Proposing a method to jointly learn from spatial and temporal shuffling as complementary self-supervision tasks for unsupervised representation learning. - Using a policy network based on reinforcement learning to dynamically propose permutations (shufflings) for training the main CNN network. The policy adapts the permutations based on the state of the CNN being trained.- Showing improved performance on benchmark tasks compared to prior self-supervision methods that use fixed random permutations. The learned features transfer better to tasks like image classification, object detection, segmentation, etc.- Analyzing the effect of the proposed approach through ablations and visualizations. Combining spatial and temporal tasks improves over individual tasks. The policy helps select useful permutations leading to faster improvement.So in summary, the main contribution is developing an adaptive reinforcement learning-based approach for sampling permutations to improve self-supervision from spatial and temporal ordering tasks. This leads to learning better general visual representations in an unsupervised manner.
