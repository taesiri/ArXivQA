# [Patches Are All You Need?](https://arxiv.org/abs/2201.09792)

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper addresses is whether the strong performance of vision transformers like ViT may result more from using patch embeddings as the input representation, rather than the transformer architecture itself. The paper proposes that the use of patch embeddings, which split the image into patches and embed them, may be a critical factor behind the performance of newer architectures like ViT. The authors develop a very simple convolutional architecture called ConvMixer that operates directly on patches like ViT, but uses only standard convolutions instead of attention. The main result is that despite its simplicity, ConvMixer outperforms ViT, MLP-Mixer, and some variants on similar data regimes, in addition to outperforming classical CNNs like ResNet. This suggests the patch representation itself, rather than novel operations like self-attention, may drive much of the performance of these new architectures.In summary, the central hypothesis is that the use of patch embeddings is critical to the strong performance of vision transformers, more so than the transformer architecture. The ConvMixer architecture is proposed to test this idea. Its competitive performance helps demonstrate the importance of the patch representation.
