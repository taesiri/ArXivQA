# [Continual Learning in Neural Networks](https://arxiv.org/abs/1910.02718v2)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading, the central research question that this paper addresses is: 

How can continual learning methods be extended to work in an online setting without task boundaries or an i.i.d. assumption?

The paper proposes a system to deploy the importance weight regularization approach MAS in an online continual learning setting. Specifically, it investigates:

1) When to update the importance weights in an online setting without clear task boundaries. They propose using the loss surface to detect "plateaus" indicating stable learning regimes.

2) Which data to use to update importance weights. They propose keeping a small buffer of hard samples as a summary of previous history. 

3) How to accumulate importance weights at each update. They propose a cumulative moving average.

The main hypothesis is that with this protocol, the MAS method can be effectively adapted to an online continual learning setting with non-i.i.d. streaming data and without known task boundaries. The experiments on face recognition and robot navigation aim to validate the effectiveness of the proposed system.


## What is the main contribution of this paper?

 The paper "Continual Learning Through Synaptic Intelligence" by Friedemann Zenke, Ben Poole and Surya Ganguli makes the following key contributions to continual learning:

1. It proposes a method called "Synaptic Intelligence" (SI) for preventing catastrophic forgetting in neural networks trained on a sequence of tasks. The key idea is to estimate the importance of each synaptic connection (i.e. each weight parameter) based on how much its changes contribute to the changes in the overall loss function during training. Connections deemed more important for previous tasks are then more heavily regularized when training on new tasks.

2. The paper shows that SI leads to significantly less forgetting on previous tasks compared to common baselines like fine-tuning or elastic weight consolidation (EWC), when tested on sequences of image classification tasks.

3. It provides a theoretical justification for SI based on Bayesian online learning, showing the connection between the SI update rules and maintaining an online posterior distribution over the weights.

4. The paper introduces a new benchmark for continual learning called Split MNIST, where the MNIST dataset is split into multiple tasks based on digit labels. This allows more systematic evaluation compared to prior works that used different datasets for each task.

5. The ability of SI to prevent catastrophic forgetting is demonstrated on various continual learning benchmarks including Split MNIST, Permuted MNIST and CIFAR-100. SI achieves state-of-the-art performance on these benchmarks compared to prior methods.

In summary, the key contribution is the proposal of Synaptic Intelligence, a novel and effective method for regulating the plasticity of synaptic connections to prevent catastrophic forgetting in continual learning settings. Both theoretically grounded and empirically demonstrated, SI provides a way for neural networks to learn continually from a stream of data across multiple tasks.
