# [Architectural Backdoors in Neural Networks](https://arxiv.org/abs/2206.07840)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question seems to be:

Can neural network architectures themselves be modified to hide backdoors? 

The authors investigate whether an adversary can use modifications to neural network architectures, rather than just manipulating the trained weights, to introduce backdoors that force the model to behave undesirably in the presence of a specific trigger. 

The key hypothesis appears to be that by making small changes to the architecture using common components, attackers can introduce backdoors that will persist even if the model is later retrained by a defender with clean data and weights reinitialized. Previous backdoor attacks rely on manipulating weights, so they can be removed by retraining. The authors hypothesize architectural backdoors will survive retraining and be more stealthy.

In summary, the main research question is whether architectural backdoors are a viable attack technique, and the key hypothesis seems to be that they can persist through retraining, unlike previous weight-based backdoor attacks. The paper aims to demonstrate and formally characterize architectural backdoors as a new threat vector.


## What is the main contribution of this paper?

 Based on my reading of the abstract and introduction, the main contributions of this paper appear to be:

1. Introducing a new class of backdoor attacks against neural networks, where the backdoor is hidden in the model architecture rather than the weights. This makes the backdoor more robust as it can survive complete retraining.

2. Demonstrating how to construct these "model architecture backdoors" (MABs) using common components like pooling layers. The authors formalize the requirements for a successful MAB.

3. Evaluating MAB attacks on computer vision benchmarks under different threat models. They show MABs can survive retraining, unlike previous backdoor attacks that rely on manipulated weights.

In summary, the key contribution seems to be proposing and evaluating a new type of backdoor attack that exploits the model architecture rather than weights. This attack is shown to be robust to retraining, making it a concerning vulnerability. The authors also analyze the requirements for architectural backdoors and discuss possible defenses.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the paper:

The paper introduces a new class of backdoor attacks against neural networks where the backdoor is hidden inside the model architecture itself rather than the weights, allowing it to survive complete retraining on new datasets.


## How does this paper compare to other research in the same field?

 Here are a few thoughts on how this paper compares to other related work:

- This paper introduces a new class of backdoor attacks called "model architecture backdoors" (MAB) that manipulate the model architecture itself rather than just the weights. This is a novel approach compared to most prior backdoor attacks like BadNets and poisoning attacks that modify the weights. 

- The authors state that the only related prior work on architecture backdoors are two very recent papers by Goldwasser et al. (2022) and Li et al. (2021). Those papers showed some initial examples of how model architectures could be manipulated but this paper provides a much more thorough investigation and formalization of architectural backdoors.

- Compared to those prior papers on architecture backdoors, this paper demonstrates attacks across a wider range of threat models, including surviving full retraining of the model. Most prior backdoor attacks are removed if the model is retrained, so the architectural backdoors pose a new threat.

- The paper provides an in-depth analysis of the requirements for successful architectural backdoors, such as the need for a direct input-output path and asymmetric components. This level of formalization and understanding of the attack vector is novel.

- The attacks are demonstrated across multiple vision datasets and models to show the generalizability of the threat. Many prior backdoor papers only show results on 1-2 datasets.

- The connections made to neural architecture search (NAS) and complex automatically designed networks are an interesting insight about how architectural backdoors could be hidden even more sneakily in the future.

Overall, this paper provides a comprehensive and rigorous treatment of a new class of backdoor attacks, representing a notable advance over the small amount of related prior work. The formalization and demonstration across models and datasets makes a strong case that architectural backdoors pose a real threat worth further study.


## What future research directions do the authors suggest?

 Based on my reading of the paper, here are some of the main future research directions suggested by the authors:

- Further investigate the space of possible architectural backdoors, especially in complex models designed by neural architecture search. The authors showed a proof of concept, but there may be other types of architectural backdoors that could be designed.

- Develop methods to detect and defend against architectural backdoors. The authors mentioned some simple heuristics like rejecting models with direct input-output connections, but more research is needed into robust defenses.

- Study whether architectural backdoors could be automatically generated using neural architecture search techniques. The authors tried a simple modification to DARTS but were not able to generate survivable backdoors, suggesting more work is needed in this area. 

- Extend the work to other domains beyond computer vision. The authors demonstrated architectural backdoors in image classifiers, but these types of attacks may also be possible in NLP, speech recognition, and other areas.

- Analyze the requirements for asymmetric components to enable targeted architectural backdoors. The attacks in this paper were untargeted, but the authors suggest architectural modifications that could allow targeted misclassifications.

- Develop techniques to explain and interpret complex auto-generated neural architectures to make inspection easier. This could aid in detecting architectural backdoors.

In summary, the authors lay out architectural backdoors as a new threat vector for neural networks and suggest a number of avenues for future work to understand, detect, and mitigate these types of attacks. Their work opens up a new subfield at the intersection of neural architecture search, model security, and robustness.


## Summarize the paper in one paragraph.

 The paper introduces a new class of backdoor attacks against neural networks, where the backdoor is hidden inside the model architecture rather than the weights. The authors show how an attacker can slightly modify a model's architecture using common components to introduce backdoors that survive complete retraining on new datasets. This makes the backdoors dataset-agnostic. The architecture backdoors link the input directly to the output and use asymmetric components to enable targeted misclassification. The authors demonstrate attacks on computer vision benchmarks under three threat models, showing the architectural backdoors persist after retraining unlike previous weight-based backdoors. They formalize the requirements for successful architectural backdoors, relating them to interpretability and showing how they enable possible defenses. Overall, the paper highlights a concerning vulnerability in reusing model architectures that survives retraining, unlike previous approaches, and urges further research into this space.


## Summarize the main method used in the paper in one paragraph.

 Here is a one paragraph summary of the main method used in the paper:

The paper introduces a new class of backdoor attacks against neural networks called model architecture backdoors (MAB). The key idea is that rather than manipulating the weights of a model to insert a backdoor, the attacker makes small modifications to the model architecture itself using common components. For example, they replace an adaptive average pooling layer with a "malicious" version that can detect a specific backdoor trigger in the input image and force large activations, changing the model's predictions. The authors show how to construct these backdoors to link the input directly to the output and make them weight-agnostic, so they persist even if the model is retrained or fine-tuned on new datasets by the victim. The attacks are demonstrated on image classification benchmarks under different threat models. Unlike previous backdoor attacks based on modifying weights, MAB survives complete retraining of the model parameters, making it a serious threat. The authors formalize the requirements for successful MAB and discuss possible defenses.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the paper:

The paper introduces a new class of backdoor attacks against neural networks, called model architecture backdoors (MAB). Unlike previous backdoor attacks that modify the trained weights, MAB embeds the backdoor in the model architecture itself. The authors show how an attacker can slightly modify a neural network architecture, using only common components, to introduce a backdoor. The key insight is that the backdoor can be implemented in a way that is weight-agnostic, meaning it persists even if the model is later retrained by a defender. 

The authors demonstrate the effectiveness of MAB attacks across different threat models. They show that MAB backdoors survive complete retraining on new datasets, unlike weight-based backdoors which are removed through retraining. The requirements for a successful MAB attack are formalized, such as the need for a direct path from input to output to avoid the backdoor being negated by learnable parameters. Possible defenses against MAB are also discussed, such as rejecting models with suspicious input-output paths. The vulnerabilities introduced by complex neural architectures are highlighted as an area requiring further research. Overall, the paper demonstrates a new class of dataset-agnostic backdoors that pose a real threat.


## What problem or question is the paper addressing?

 Based on my reading of the abstract and introduction, this paper introduces a new class of backdoor attacks against neural networks, called model architecture backdoors (MAB). The key problem it is addressing is:

- Existing backdoor attacks rely on manipulating the trained weights of a neural network model. However, these backdoors could potentially be removed if the model is retrained or fine-tuned on new data by a defender. 

- The authors investigate an alternative approach where the backdoor is embedded in the model architecture itself rather than the weights. The goal is to create a backdoor that persists even if the model is retrained from scratch.

The main research questions seem to be:

- Can an attacker modify a neural network's architecture to embed backdoors that survive complete retraining on new datasets?

- How can such architectural backdoors be constructed? What are the requirements to make them robust and operational?

- How prevalent is this threat - do architectural backdoors pose a real vulnerability across different model architectures and training settings?

So in summary, this paper introduces the concept of architectural backdoors as a new attack vector, demonstrates how to construct them, evaluates their robustness across settings, and aims to raise awareness of this threat. The key novelty is making backdoors survive weight reinitializations via the architecture.
