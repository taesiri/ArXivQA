# [Architectural Backdoors in Neural Networks](https://arxiv.org/abs/2206.07840)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question seems to be:

Can neural network architectures themselves be modified to hide backdoors? 

The authors investigate whether an adversary can use modifications to neural network architectures, rather than just manipulating the trained weights, to introduce backdoors that force the model to behave undesirably in the presence of a specific trigger. 

The key hypothesis appears to be that by making small changes to the architecture using common components, attackers can introduce backdoors that will persist even if the model is later retrained by a defender with clean data and weights reinitialized. Previous backdoor attacks rely on manipulating weights, so they can be removed by retraining. The authors hypothesize architectural backdoors will survive retraining and be more stealthy.

In summary, the main research question is whether architectural backdoors are a viable attack technique, and the key hypothesis seems to be that they can persist through retraining, unlike previous weight-based backdoor attacks. The paper aims to demonstrate and formally characterize architectural backdoors as a new threat vector.
