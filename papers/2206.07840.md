# [Architectural Backdoors in Neural Networks](https://arxiv.org/abs/2206.07840)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question seems to be:

Can neural network architectures themselves be modified to hide backdoors? 

The authors investigate whether an adversary can use modifications to neural network architectures, rather than just manipulating the trained weights, to introduce backdoors that force the model to behave undesirably in the presence of a specific trigger. 

The key hypothesis appears to be that by making small changes to the architecture using common components, attackers can introduce backdoors that will persist even if the model is later retrained by a defender with clean data and weights reinitialized. Previous backdoor attacks rely on manipulating weights, so they can be removed by retraining. The authors hypothesize architectural backdoors will survive retraining and be more stealthy.

In summary, the main research question is whether architectural backdoors are a viable attack technique, and the key hypothesis seems to be that they can persist through retraining, unlike previous weight-based backdoor attacks. The paper aims to demonstrate and formally characterize architectural backdoors as a new threat vector.


## What is the main contribution of this paper?

 Based on my reading of the abstract and introduction, the main contributions of this paper appear to be:

1. Introducing a new class of backdoor attacks against neural networks, where the backdoor is hidden in the model architecture rather than the weights. This makes the backdoor more robust as it can survive complete retraining.

2. Demonstrating how to construct these "model architecture backdoors" (MABs) using common components like pooling layers. The authors formalize the requirements for a successful MAB.

3. Evaluating MAB attacks on computer vision benchmarks under different threat models. They show MABs can survive retraining, unlike previous backdoor attacks that rely on manipulated weights.

In summary, the key contribution seems to be proposing and evaluating a new type of backdoor attack that exploits the model architecture rather than weights. This attack is shown to be robust to retraining, making it a concerning vulnerability. The authors also analyze the requirements for architectural backdoors and discuss possible defenses.
