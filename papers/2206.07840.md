# [Architectural Backdoors in Neural Networks](https://arxiv.org/abs/2206.07840)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question seems to be:

Can neural network architectures themselves be modified to hide backdoors? 

The authors investigate whether an adversary can use modifications to neural network architectures, rather than just manipulating the trained weights, to introduce backdoors that force the model to behave undesirably in the presence of a specific trigger. 

The key hypothesis appears to be that by making small changes to the architecture using common components, attackers can introduce backdoors that will persist even if the model is later retrained by a defender with clean data and weights reinitialized. Previous backdoor attacks rely on manipulating weights, so they can be removed by retraining. The authors hypothesize architectural backdoors will survive retraining and be more stealthy.

In summary, the main research question is whether architectural backdoors are a viable attack technique, and the key hypothesis seems to be that they can persist through retraining, unlike previous weight-based backdoor attacks. The paper aims to demonstrate and formally characterize architectural backdoors as a new threat vector.


## What is the main contribution of this paper?

 Based on my reading of the abstract and introduction, the main contributions of this paper appear to be:

1. Introducing a new class of backdoor attacks against neural networks, where the backdoor is hidden in the model architecture rather than the weights. This makes the backdoor more robust as it can survive complete retraining.

2. Demonstrating how to construct these "model architecture backdoors" (MABs) using common components like pooling layers. The authors formalize the requirements for a successful MAB.

3. Evaluating MAB attacks on computer vision benchmarks under different threat models. They show MABs can survive retraining, unlike previous backdoor attacks that rely on manipulated weights.

In summary, the key contribution seems to be proposing and evaluating a new type of backdoor attack that exploits the model architecture rather than weights. This attack is shown to be robust to retraining, making it a concerning vulnerability. The authors also analyze the requirements for architectural backdoors and discuss possible defenses.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the paper:

The paper introduces a new class of backdoor attacks against neural networks where the backdoor is hidden inside the model architecture itself rather than the weights, allowing it to survive complete retraining on new datasets.


## How does this paper compare to other research in the same field?

 Here are a few thoughts on how this paper compares to other related work:

- This paper introduces a new class of backdoor attacks called "model architecture backdoors" (MAB) that manipulate the model architecture itself rather than just the weights. This is a novel approach compared to most prior backdoor attacks like BadNets and poisoning attacks that modify the weights. 

- The authors state that the only related prior work on architecture backdoors are two very recent papers by Goldwasser et al. (2022) and Li et al. (2021). Those papers showed some initial examples of how model architectures could be manipulated but this paper provides a much more thorough investigation and formalization of architectural backdoors.

- Compared to those prior papers on architecture backdoors, this paper demonstrates attacks across a wider range of threat models, including surviving full retraining of the model. Most prior backdoor attacks are removed if the model is retrained, so the architectural backdoors pose a new threat.

- The paper provides an in-depth analysis of the requirements for successful architectural backdoors, such as the need for a direct input-output path and asymmetric components. This level of formalization and understanding of the attack vector is novel.

- The attacks are demonstrated across multiple vision datasets and models to show the generalizability of the threat. Many prior backdoor papers only show results on 1-2 datasets.

- The connections made to neural architecture search (NAS) and complex automatically designed networks are an interesting insight about how architectural backdoors could be hidden even more sneakily in the future.

Overall, this paper provides a comprehensive and rigorous treatment of a new class of backdoor attacks, representing a notable advance over the small amount of related prior work. The formalization and demonstration across models and datasets makes a strong case that architectural backdoors pose a real threat worth further study.


## What future research directions do the authors suggest?

 Based on my reading of the paper, here are some of the main future research directions suggested by the authors:

- Further investigate the space of possible architectural backdoors, especially in complex models designed by neural architecture search. The authors showed a proof of concept, but there may be other types of architectural backdoors that could be designed.

- Develop methods to detect and defend against architectural backdoors. The authors mentioned some simple heuristics like rejecting models with direct input-output connections, but more research is needed into robust defenses.

- Study whether architectural backdoors could be automatically generated using neural architecture search techniques. The authors tried a simple modification to DARTS but were not able to generate survivable backdoors, suggesting more work is needed in this area. 

- Extend the work to other domains beyond computer vision. The authors demonstrated architectural backdoors in image classifiers, but these types of attacks may also be possible in NLP, speech recognition, and other areas.

- Analyze the requirements for asymmetric components to enable targeted architectural backdoors. The attacks in this paper were untargeted, but the authors suggest architectural modifications that could allow targeted misclassifications.

- Develop techniques to explain and interpret complex auto-generated neural architectures to make inspection easier. This could aid in detecting architectural backdoors.

In summary, the authors lay out architectural backdoors as a new threat vector for neural networks and suggest a number of avenues for future work to understand, detect, and mitigate these types of attacks. Their work opens up a new subfield at the intersection of neural architecture search, model security, and robustness.


## Summarize the paper in one paragraph.

 The paper introduces a new class of backdoor attacks against neural networks, where the backdoor is hidden inside the model architecture rather than the weights. The authors show how an attacker can slightly modify a model's architecture using common components to introduce backdoors that survive complete retraining on new datasets. This makes the backdoors dataset-agnostic. The architecture backdoors link the input directly to the output and use asymmetric components to enable targeted misclassification. The authors demonstrate attacks on computer vision benchmarks under three threat models, showing the architectural backdoors persist after retraining unlike previous weight-based backdoors. They formalize the requirements for successful architectural backdoors, relating them to interpretability and showing how they enable possible defenses. Overall, the paper highlights a concerning vulnerability in reusing model architectures that survives retraining, unlike previous approaches, and urges further research into this space.


## Summarize the main method used in the paper in one paragraph.

 Here is a one paragraph summary of the main method used in the paper:

The paper introduces a new class of backdoor attacks against neural networks called model architecture backdoors (MAB). The key idea is that rather than manipulating the weights of a model to insert a backdoor, the attacker makes small modifications to the model architecture itself using common components. For example, they replace an adaptive average pooling layer with a "malicious" version that can detect a specific backdoor trigger in the input image and force large activations, changing the model's predictions. The authors show how to construct these backdoors to link the input directly to the output and make them weight-agnostic, so they persist even if the model is retrained or fine-tuned on new datasets by the victim. The attacks are demonstrated on image classification benchmarks under different threat models. Unlike previous backdoor attacks based on modifying weights, MAB survives complete retraining of the model parameters, making it a serious threat. The authors formalize the requirements for successful MAB and discuss possible defenses.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the paper:

The paper introduces a new class of backdoor attacks against neural networks, called model architecture backdoors (MAB). Unlike previous backdoor attacks that modify the trained weights, MAB embeds the backdoor in the model architecture itself. The authors show how an attacker can slightly modify a neural network architecture, using only common components, to introduce a backdoor. The key insight is that the backdoor can be implemented in a way that is weight-agnostic, meaning it persists even if the model is later retrained by a defender. 

The authors demonstrate the effectiveness of MAB attacks across different threat models. They show that MAB backdoors survive complete retraining on new datasets, unlike weight-based backdoors which are removed through retraining. The requirements for a successful MAB attack are formalized, such as the need for a direct path from input to output to avoid the backdoor being negated by learnable parameters. Possible defenses against MAB are also discussed, such as rejecting models with suspicious input-output paths. The vulnerabilities introduced by complex neural architectures are highlighted as an area requiring further research. Overall, the paper demonstrates a new class of dataset-agnostic backdoors that pose a real threat.


## What problem or question is the paper addressing?

 Based on my reading of the abstract and introduction, this paper introduces a new class of backdoor attacks against neural networks, called model architecture backdoors (MAB). The key problem it is addressing is:

- Existing backdoor attacks rely on manipulating the trained weights of a neural network model. However, these backdoors could potentially be removed if the model is retrained or fine-tuned on new data by a defender. 

- The authors investigate an alternative approach where the backdoor is embedded in the model architecture itself rather than the weights. The goal is to create a backdoor that persists even if the model is retrained from scratch.

The main research questions seem to be:

- Can an attacker modify a neural network's architecture to embed backdoors that survive complete retraining on new datasets?

- How can such architectural backdoors be constructed? What are the requirements to make them robust and operational?

- How prevalent is this threat - do architectural backdoors pose a real vulnerability across different model architectures and training settings?

So in summary, this paper introduces the concept of architectural backdoors as a new attack vector, demonstrates how to construct them, evaluates their robustness across settings, and aims to raise awareness of this threat. The key novelty is making backdoors survive weight reinitializations via the architecture.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and concepts are:

- Backdoor attacks - The paper introduces a new class of backdoor attacks against neural networks, where the backdoor is hidden inside the model architecture rather than the weights.

- Model architecture backdoors (MAB) - The name given to the architectural backdoors proposed in this paper. They are modifications to the model architecture that cause it to behave differently in the presence of a secret trigger.

- Weight-agnostic backdoors - MAB backdoors are designed to be weight-agnostic, meaning they persist even if the model is retrained or the weights are changed. This makes them more robust than backdoors embedded in the weights.

- Activation engineering - One of the key techniques used to construct MABs. It involves modifying activation functions in the model to detect triggers and dramatically change the model's behavior. 

- Neural architecture search (NAS) - The paper explores using NAS techniques to automatically discover architectures vulnerable to backdoors.

- Threat models - The paper evaluates MABs under different threat models like direct use of a backdoored model, fine-tuning, and full retraining.

- Requirements for operational backdoors - The paper formalizes requirements like direct input-output paths and asymmetric components that are needed for robust backdoors.

- Defenses - Possible defenses against MABs are discussed, like rejecting models with input-output connections and inspecting architectures.

In summary, the key focus is on backdoors that exploit the model architecture itself rather than the weights, with analysis on constructing them and defending against them.


## Design 10 in-depth questions about method proposed in this paper and avoid simple question.

 Here are 10 potential in-depth questions about the methods proposed in the paper:

1. The paper proposes using architectural backdoors rather than modifying weights to inject backdoors. Why might an architectural backdoor be more robust than modifying weights directly? What are the trade-offs?

2. The malicious adaptive average pooling (AAP) layer is key to the architectural backdoor. Why was the AAP layer chosen as the insertion point for the backdoor? What properties make it a good choice?

3. The paper mentions the backdoor should have a low false positive rate. Why is this important? How does the use of checkerboard patterns rather than solid white blocks help with this?

4. The architectural backdoor connects the input image directly to the AAP layer. Why is this direct connection critical? What issues could arise without it? 

5. The paper argues architectural backdoors should be weight-agnostic. Why is this property desirable? What techniques help make the backdoor weight-agnostic in this work?

6. For targeted attacks, the paper claims architectural backdoors need asymmetric components. Explain why asymmetry is required and propose how you might add asymmetry to enable targeted attacks.

7. The architectural backdoor reduces accuracy when the trigger is present but maintains good task accuracy otherwise. Explain the trade-offs in optimizing these two metrics from an attacker's perspective. 

8. The paper demonstrates detecting architectural backdoors by identifying unusual connections. Propose other potential methods to detect or mitigate architectural backdoors.

9. The DARTS experiments suggest the search space was too limited to find effective backdoors automatically. Suggest ways the search space could be expanded to improve automated discovery of architectural backdoors.

10. The paper focuses on computer vision tasks. Discuss how architectural backdoors could be developed and deployed effectively for other data modalities like text or audio.


## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a summary paragraph of the paper:

This paper introduces a new class of backdoor attacks against neural networks, where the backdoor is hidden inside the model architecture rather than the weights. The authors show how an attacker can manipulate common model architectures like AlexNet and VGG by adding malicious components that detect a specific backdoor trigger in the input and force misclassification. Unlike previous backdoor attacks based on data or weight manipulation, these architectural backdoors survive complete retraining of the model weights on new datasets. The authors formalize the requirements for successful architectural backdoors, like having a direct path from input to output and using weight-agnostic components. They evaluate attacks on image classification datasets under different threat models. Architectural backdoors cause a significant drop in accuracy (to random chance) when the trigger is present, even after retraining, while other backdoor methods fail. The authors discuss limitations and potential defenses, like rejecting models with suspicious input-output paths. Overall, this work demonstrates a dangerous new attack vector via model architectures that persists through common backdoor mitigations.


## Summarize the paper in one sentence.

 The paper introduces architectural backdoors, a new class of backdoor attacks against neural networks where the backdoor is hidden inside the model architecture rather than the weights. The backdoors can survive complete retraining from scratch.


## Summarize the paper in one paragraphs.

 Here is a one paragraph summary of the paper:

This paper introduces a new class of backdoor attacks against neural networks where the backdoor is hidden inside the model architecture rather than the weights. The authors show how an attacker can manipulate the model architecture to make it sensitive to a specific backdoor trigger, such that the model will misclassify inputs containing the trigger. Unlike previous backdoor attacks that rely on manipulating the weights, these architectural backdoors can survive complete retraining of the model from scratch. The authors describe the requirements for constructing robust architectural backdoors, such as having a direct path from input to output and using weight-agnostic components. They evaluate their architectural backdoor attacks on image classification benchmarks under different threat models, showing the attacks are effective at reducing model accuracy in the presence of a trigger while maintaining good performance on clean inputs. The paper demonstrates that architectural backdoors represent a real threat that can persist through common defense strategies like retraining.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 potential in-depth questions about the paper:

1. The paper proposes a new class of backdoor attacks called Model Architecture Backdoors (MAB). How is this fundamentally different from previous backdoor attacks like BadNets and differs in the threat model? What are the key advantages of MAB over previous attacks?

2. The paper mentions three main requirements for a successful MAB attack - having a direct input-output path, a weight-agnostic detector, and asymmetric components. Why are each of these important for the attack to be robust and survive retraining? What would happen if any of these requirements were not met? 

3. The activation engineering method is used to construct the MAB by chaining together exponential activations and pooling operations. Walk through how this allows a spatial trigger like a checkerboard pattern to be detected. How was the activation engineering refined from the initial naive version to make the MAB more robust?

4. The authors attempt to use neural architecture search (NAS) to automatically find architectures with backdoors. Why was this challenging? What limitations of the search space used and optimization approach caused the backdoors found by NAS to not be very effective?

5. The paper evaluates MAB under three different threat models - direct use, fine-tuning, and full retraining. Walk through the experiments and results under each setting. How does MAB compare to BadNets and handcrafted backdoors in each case?

6. Figure 5 shows how the MAB survives fine-tuning, while BadNets and handcrafted backdoors are removed. Explain why fine-tuning has this effect for weight-based backdoors but not MAB.

7. The paper mentions limitations and defenses for MAB attacks. Discuss approaches like visual inspection, interval bound propagation, and preventing direct input-output paths that could potentially detect or protect against architectural backdoors.

8. Table 1 shows that on IMDB-Wiki, MAB causes the model to always classify an input as Will Smith when the trigger is present. Explain why MAB leads to an untargeted attack in this case while a targeted attack would be harder to achieve. 

9. The SHAP analysis demonstrates how MAB focuses exclusively on the trigger region while other attacks use more diffuse features. Explain how this matches the goals and construction of MAB.

10. This paper opens up a new avenue of thinking about backdoors in neural network architectures. What directions are there for future work building on this? What other kinds of architectural manipulations could lead to harmful behaviors?
