# [Architectural Backdoors in Neural Networks](https://arxiv.org/abs/2206.07840)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question seems to be:

Can neural network architectures themselves be modified to hide backdoors? 

The authors investigate whether an adversary can use modifications to neural network architectures, rather than just manipulating the trained weights, to introduce backdoors that force the model to behave undesirably in the presence of a specific trigger. 

The key hypothesis appears to be that by making small changes to the architecture using common components, attackers can introduce backdoors that will persist even if the model is later retrained by a defender with clean data and weights reinitialized. Previous backdoor attacks rely on manipulating weights, so they can be removed by retraining. The authors hypothesize architectural backdoors will survive retraining and be more stealthy.

In summary, the main research question is whether architectural backdoors are a viable attack technique, and the key hypothesis seems to be that they can persist through retraining, unlike previous weight-based backdoor attacks. The paper aims to demonstrate and formally characterize architectural backdoors as a new threat vector.


## What is the main contribution of this paper?

 Based on my reading of the abstract and introduction, the main contributions of this paper appear to be:

1. Introducing a new class of backdoor attacks against neural networks, where the backdoor is hidden in the model architecture rather than the weights. This makes the backdoor more robust as it can survive complete retraining.

2. Demonstrating how to construct these "model architecture backdoors" (MABs) using common components like pooling layers. The authors formalize the requirements for a successful MAB.

3. Evaluating MAB attacks on computer vision benchmarks under different threat models. They show MABs can survive retraining, unlike previous backdoor attacks that rely on manipulated weights.

In summary, the key contribution seems to be proposing and evaluating a new type of backdoor attack that exploits the model architecture rather than weights. This attack is shown to be robust to retraining, making it a concerning vulnerability. The authors also analyze the requirements for architectural backdoors and discuss possible defenses.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the paper:

The paper introduces a new class of backdoor attacks against neural networks where the backdoor is hidden inside the model architecture itself rather than the weights, allowing it to survive complete retraining on new datasets.


## How does this paper compare to other research in the same field?

 Here are a few thoughts on how this paper compares to other related work:

- This paper introduces a new class of backdoor attacks called "model architecture backdoors" (MAB) that manipulate the model architecture itself rather than just the weights. This is a novel approach compared to most prior backdoor attacks like BadNets and poisoning attacks that modify the weights. 

- The authors state that the only related prior work on architecture backdoors are two very recent papers by Goldwasser et al. (2022) and Li et al. (2021). Those papers showed some initial examples of how model architectures could be manipulated but this paper provides a much more thorough investigation and formalization of architectural backdoors.

- Compared to those prior papers on architecture backdoors, this paper demonstrates attacks across a wider range of threat models, including surviving full retraining of the model. Most prior backdoor attacks are removed if the model is retrained, so the architectural backdoors pose a new threat.

- The paper provides an in-depth analysis of the requirements for successful architectural backdoors, such as the need for a direct input-output path and asymmetric components. This level of formalization and understanding of the attack vector is novel.

- The attacks are demonstrated across multiple vision datasets and models to show the generalizability of the threat. Many prior backdoor papers only show results on 1-2 datasets.

- The connections made to neural architecture search (NAS) and complex automatically designed networks are an interesting insight about how architectural backdoors could be hidden even more sneakily in the future.

Overall, this paper provides a comprehensive and rigorous treatment of a new class of backdoor attacks, representing a notable advance over the small amount of related prior work. The formalization and demonstration across models and datasets makes a strong case that architectural backdoors pose a real threat worth further study.


## What future research directions do the authors suggest?

 Based on my reading of the paper, here are some of the main future research directions suggested by the authors:

- Further investigate the space of possible architectural backdoors, especially in complex models designed by neural architecture search. The authors showed a proof of concept, but there may be other types of architectural backdoors that could be designed.

- Develop methods to detect and defend against architectural backdoors. The authors mentioned some simple heuristics like rejecting models with direct input-output connections, but more research is needed into robust defenses.

- Study whether architectural backdoors could be automatically generated using neural architecture search techniques. The authors tried a simple modification to DARTS but were not able to generate survivable backdoors, suggesting more work is needed in this area. 

- Extend the work to other domains beyond computer vision. The authors demonstrated architectural backdoors in image classifiers, but these types of attacks may also be possible in NLP, speech recognition, and other areas.

- Analyze the requirements for asymmetric components to enable targeted architectural backdoors. The attacks in this paper were untargeted, but the authors suggest architectural modifications that could allow targeted misclassifications.

- Develop techniques to explain and interpret complex auto-generated neural architectures to make inspection easier. This could aid in detecting architectural backdoors.

In summary, the authors lay out architectural backdoors as a new threat vector for neural networks and suggest a number of avenues for future work to understand, detect, and mitigate these types of attacks. Their work opens up a new subfield at the intersection of neural architecture search, model security, and robustness.


## Summarize the paper in one paragraph.

 The paper introduces a new class of backdoor attacks against neural networks, where the backdoor is hidden inside the model architecture rather than the weights. The authors show how an attacker can slightly modify a model's architecture using common components to introduce backdoors that survive complete retraining on new datasets. This makes the backdoors dataset-agnostic. The architecture backdoors link the input directly to the output and use asymmetric components to enable targeted misclassification. The authors demonstrate attacks on computer vision benchmarks under three threat models, showing the architectural backdoors persist after retraining unlike previous weight-based backdoors. They formalize the requirements for successful architectural backdoors, relating them to interpretability and showing how they enable possible defenses. Overall, the paper highlights a concerning vulnerability in reusing model architectures that survives retraining, unlike previous approaches, and urges further research into this space.
