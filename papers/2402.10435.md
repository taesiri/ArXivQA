# [Dynamic Patch-aware Enrichment Transformer for Occluded Person   Re-Identification](https://arxiv.org/abs/2402.10435)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper "Dynamic Patch-aware Enrichment Transformer for Occluded Person Re-Identification":

Problem: 
Person re-identification (re-ID) aims to match people across different camera views. While progress has been made in re-ID for visible people, performance drops significantly when people are occluded, which is common in real-world scenarios. Existing methods rely on external semantic or pose detectors to align body parts, but they are error-prone for complex multi-pedestrian occlusions. Therefore, effectively distinguishing human bodies from occlusions without additional detectors remains an open challenge.

Solution - Dynamic Patch-aware Enrichment Transformer (DPEFormer):
1) Dynamic Patch Token Selection Module (DPSM) automatically pinpoints informative human patches from all patch tokens using a label-guided proxy token and similarity ranking. This acts as attention to focus on useful body regions while ignoring occluded regions.

2) Feature Blending Module (FBM) fuses the global image features with local part features selected by DPSM through cross-attention. This enriches the features by blending global context and local detail information.  

3) Realistic Occlusion Augmentation (ROA) leverages segmented masks to synthesize diverse and realistic occlusion images during training. This uses no manual annotation and improves robustness.

Main Contributions:
- Proposes end-to-end architecture for occluded re-ID that distinguishes human bodies from occlusions automatically without any external detector
- Introduces dynamic patch token selection attention to focus on useful body regions 
- Enriches features by blending global context and local details via cross-attention
- Generates realistic occlusion images for augmentation using segmented masks 

Experiments show DPEFormer outperforms state-of-the-art occluded re-ID methods by a large margin on benchmark datasets, proving its effectiveness. The framework does not rely on any additional annotation and has good generalizability.
