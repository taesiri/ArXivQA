# [Full Parameter Fine-tuning for Large Language Models with Limited   Resources](https://arxiv.org/abs/2306.09782)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question/hypothesis appears to be:Can full parameter fine-tuning of large language models be enabled on limited computational resources through algorithmic and engineering optimizations? The authors aim to explore techniques to enable full parameter fine-tuning of large language models (LLMs) with billions of parameters on resource-constrained hardware like a single machine with 8 GPUs. Their goal is to lower the threshold for training LLMs and make them more accessible to smaller labs/companies. To address this, they propose a new optimizer called LOMO that reduces memory usage and enables full parameter tuning of LLMs within limited resources. The main hypotheses seem to be:- SGD can successfully fine-tune the full parameters of LLMs, despite issues that previously hindered its widespread usage.- By fusing gradient computation and parameter update in one step, LOMO can significantly reduce memory usage compared to standard approaches.- With memory optimization techniques like LOMO, full parameter fine-tuning of large LLMs is feasible on resource-limited hardware.So in summary, the central research question/hypothesis is around enabling full parameter LLM fine-tuning on limited resources through an optimized training process, with LOMO being the proposed technique. The authors aim to experimentally validate this hypothesis.
