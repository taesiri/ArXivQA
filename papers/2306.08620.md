# [Anticipatory Music Transformer](https://arxiv.org/abs/2306.08620)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central research question this paper addresses is:How can we construct generative models of music that can be controlled by conditioning on an asynchronous correlated process (such as a melody line)?Specifically, the paper focuses on the problem of "infilling" - generating a complete sequence of musical events conditioned on a subset of user-specified events. The authors motivate this as an important capability for building controllable generative models that can support human creativity and music co-creation. To address this question, the paper introduces a method called "anticipation" for interleaving the conditioning control sequence with the generated event sequence in an autoregressive model. This allows the model to anticipate upcoming control events and account for them when generating new events.The authors apply anticipation to train "anticipatory infilling models" on a large dataset of MIDI music, and demonstrate that these models can perform infilling tasks like generating accompaniments conditioned on a melody line. The anticipatory models match unconditional generation performance of autoregressive baselines, while unlocking new conditioned generation capabilities.So in summary, the central hypothesis is that by structuring conditional generation using anticipation, the paper's proposed models can unlock new control capabilities for symbolic music generation without sacrificing quality or flexibility of the underlying generative model.


## What is the main contribution of this paper?

The main contribution of this paper is introducing a method called "anticipation" for constructing controllable generative models of temporal point processes. Specifically, the paper focuses on applying anticipation to build models for symbolic music generation that can perform infilling control tasks like generating accompaniments to a given melody.The key ideas are:- Anticipation involves interleaving sequences of events (e.g. notes in a melody) and controls (e.g. notes in an accompaniment) such that controls appear close in the sequence to the events they should influence. This allows building autoregressive models that can anticipate upcoming controls.- They propose an "arrival-time" encoding of music that represents notes as triplets of (arrival time, duration, pitch). This facilitates anticipation since subsequences can be reordered while preserving semantics. - They apply anticipation to build "anticipatory infilling models" that can generate complete music conditioned on a subset of user-provided control notes. These models are trained on the Lakh MIDI dataset.- Experiments show the anticipatory models match regular autoregressive models in unconditional generation, while gaining the ability to perform infilling control tasks. Human evaluators even prefer some anticipatory accompaniments over real human-composed music.So in summary, the main contribution is introducing the anticipation technique to build controllable autoregressive models of music that can perform conditional generation tasks like infilling while maintaining quality of unconditional generation. The results on musical accompaniment showcase the capabilities enabled by this approach.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper introduces a method called anticipation for constructing controllable generative models of musical sequences that can be conditioned asynchronously on controls (such as a melody line) by interleaving the event sequence with the control sequence in a particular order.
