# [Anticipatory Music Transformer](https://arxiv.org/abs/2306.08620)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central research question this paper addresses is:How can we construct generative models of music that can be controlled by conditioning on an asynchronous correlated process (such as a melody line)?Specifically, the paper focuses on the problem of "infilling" - generating a complete sequence of musical events conditioned on a subset of user-specified events. The authors motivate this as an important capability for building controllable generative models that can support human creativity and music co-creation. To address this question, the paper introduces a method called "anticipation" for interleaving the conditioning control sequence with the generated event sequence in an autoregressive model. This allows the model to anticipate upcoming control events and account for them when generating new events.The authors apply anticipation to train "anticipatory infilling models" on a large dataset of MIDI music, and demonstrate that these models can perform infilling tasks like generating accompaniments conditioned on a melody line. The anticipatory models match unconditional generation performance of autoregressive baselines, while unlocking new conditioned generation capabilities.So in summary, the central hypothesis is that by structuring conditional generation using anticipation, the paper's proposed models can unlock new control capabilities for symbolic music generation without sacrificing quality or flexibility of the underlying generative model.


## What is the main contribution of this paper?

The main contribution of this paper is introducing a method called "anticipation" for constructing controllable generative models of temporal point processes. Specifically, the paper focuses on applying anticipation to build models for symbolic music generation that can perform infilling control tasks like generating accompaniments to a given melody.The key ideas are:- Anticipation involves interleaving sequences of events (e.g. notes in a melody) and controls (e.g. notes in an accompaniment) such that controls appear close in the sequence to the events they should influence. This allows building autoregressive models that can anticipate upcoming controls.- They propose an "arrival-time" encoding of music that represents notes as triplets of (arrival time, duration, pitch). This facilitates anticipation since subsequences can be reordered while preserving semantics. - They apply anticipation to build "anticipatory infilling models" that can generate complete music conditioned on a subset of user-provided control notes. These models are trained on the Lakh MIDI dataset.- Experiments show the anticipatory models match regular autoregressive models in unconditional generation, while gaining the ability to perform infilling control tasks. Human evaluators even prefer some anticipatory accompaniments over real human-composed music.So in summary, the main contribution is introducing the anticipation technique to build controllable autoregressive models of music that can perform conditional generation tasks like infilling while maintaining quality of unconditional generation. The results on musical accompaniment showcase the capabilities enabled by this approach.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper introduces a method called anticipation for constructing controllable generative models of musical sequences that can be conditioned asynchronously on controls (such as a melody line) by interleaving the event sequence with the control sequence in a particular order.


## How does this paper compare to other research in the same field?

Here are a few key ways this paper on anticipatory music transformers compares to other related work on controllable generative modeling and music generation:- The method of anticipation for asynchronous conditioning is novel compared to prior work. Similar ideas like orderless NADE and XLNet use global permutations of the factorization order, while this paper proposes local reordering.- For music generation, the focus on symbolic music and preserving musicality without sacrificing autoregressive performance distinguishes this work. Much recent work has focused on the audio domain or encoder-decoder architectures which are harder to scale. - The musical infilling application is related to prior work on span infilling and harmonization, but generating full, asynchronous accompaniments conditioned on a melody is a more challenging task that better showcases the capabilities of anticipation.- The scale of models trained on the full Lakh MIDI dataset and the human evaluations comparing generation quality are more rigorous than evaluations in some related work.- Overall, the paper makes a compelling case that anticipation is a useful technique for temporal point processes that integrates well with autoregressive modeling. The results on music data suggest this approach unlocks new creative possibilities while maintaining sample quality. The comparisons situate this as a promising research direction relative to alternative generative modeling paradigms.In summary, the novelty of the anticipation technique and its effective application to symbolic music generation via infilling models are the key distinguishing features compared to relevant prior research. The paper makes a nice contribution in advancing controllable generative modeling.


## What future research directions do the authors suggest?

Based on my reading of the paper, some of the key future research directions suggested by the authors include:- Systematic ablation studies on the anticipation interval hyperparameter δ. The authors did not have the resources to thoroughly explore different values of δ, so they suggest this as an area for future work.- Training and evaluating even larger-scale models on more data. The authors note they likely did not saturate performance on the Lakh MIDI dataset, implying bigger models and more data could further improve results.- Extending the methods to other musical traditions beyond Western music. The authors explicitly call for building models of non-Western music genres and note this may require collecting more data first. - Applying anticipation to generate symbolic music conditioned on localized text (e.g. lyrics). The authors mention this as an intriguing possibility based on recent work controlling music generation with text in the audio domain.- Further studying anticipatory infilling models' capabilities through user interaction. The authors propose the prior over infilling controls could facilitate more complex interactive workflows, which they leave for future work.- Analyzing inter-annotator agreement for the human evaluations. The authors collected annotations from multiple workers but defer this analysis.- Accounting for variability in training, e.g. different splits or random seeds. The authors did not do this due to compute constraints.So in summary, the main directions mentioned are: more systematic studies of model hyperparameters, scaling up models and data, extending to new domains like non-Western music, integrating textual conditioning, interaction studies, annotator analysis, and accounting for training variability.
