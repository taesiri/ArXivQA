# [Qwen-Audio: Advancing Universal Audio Understanding via Unified   Large-Scale Audio-Language Models](https://arxiv.org/abs/2311.07919)

## Summarize the paper in one sentence.

 The paper introduces Qwen-Audio, a large-scale multi-task audio-language model capable of universal audio understanding without task-specific fine-tuning, and Qwen-Audio-Chat, an interactive chat model built upon Qwen-Audio via instruction fine-tuning, enabling flexible dialogue interaction from both audio and text inputs.


## Summarize the paper in one paragraphs.

 Here is a one paragraph summary of the paper:

The paper introduces Qwen-Audio and Qwen-Audio-Chat, a set of large-scale audio-language models for universal audio understanding and flexible instruction following interaction. Qwen-Audio aims to train a unified model capable of handling diverse audio types and tasks by scaling up multi-task pre-training using over 30 datasets across speech, sound, music and various languages. To address the challenge of variation in textual labels across datasets, a hierarchical multi-task training framework with shared and specified tags is proposed to enable knowledge sharing while avoiding interference. Without any task-specific fine-tuning, Qwen-Audio achieves state-of-the-art performance on multiple benchmarks, showcasing its universal audio understanding abilities. Building upon Qwen-Audio, Qwen-Audio-Chat is developed via supervised instruction fine-tuning to align with human intent, supporting multi-turn dialogues with both audio and text inputs. The models promote the development of the audio-text multimodal community through open-sourced code and models.


## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a high-quality paragraph summarizing the key points of the paper:

The paper introduces Qwen-Audio, a large-scale audio-language model capable of universal audio understanding across diverse tasks, languages, and audio types. To achieve this, the authors perform multi-task pretraining on over 30 datasets covering speech, natural sounds, music, and songs. A key challenge they address is the variation in textual labels across different datasets, which can lead to interference. They propose a multi-task training framework that conditions the decoder on hierarchical tags to encourage knowledge sharing while avoiding interference. Without any task-specific fine-tuning, Qwen-Audio achieves state-of-the-art results on test sets of Aishell1, CochlScene, ClothoAQA, and VocalSound. The authors also introduce Qwen-Audio-Chat via supervised instruction fine-tuning to enable flexible dialogues involving both audio and text. The models represent fundamental advancements in universal audio understanding and audio-language interaction capabilities. Through open-sourcing, the authors aim to promote further progress in this exciting field.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the paper:

The paper introduces Qwen-Audio, a large-scale multitask audio-language model capable of universal audio understanding without task-specific fine-tuning, and Qwen-Audio-Chat, an interactive chat model that supports flexible audio and text input in dialogues.
