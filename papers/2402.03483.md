# [SWAG: Storytelling With Action Guidance](https://arxiv.org/abs/2402.03483)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Automated long-form story generation with large language models (LLMs) can produce coherent content, but struggles to make stories interesting and engaging. 
- Existing methods like end-to-end generation or reinforcement learning for control have limitations.

Proposed Solution - Storytelling With Action Guidance (SWAG):  
- Formulates storytelling as a search problem - finding the optimal path through possible stories given a premise.  
- Uses a two-model feedback loop for generation:
   - One LLM generates story content.
   - Another auxiliary LLM chooses next best "action" to steer the story.
- Trains action discriminator (AD) LLM on dataset of story action preferences from GPT-4 and Mixtral-8x7B. Applies supervised pre-training and direct preference optimization to align AD LLM.

Key Contributions:
- Introduces a simple yet effective method for controllable and engaging automated story generation using modular LLMs.
- Demonstrates strong performance - SWAG substantially outperforms previous end-to-end approaches when evaluated by GPT-4 and humans.
- Achieves state-of-the-art results; SWAG with only open-source LLMs surpasses GPT-3.5-Turbo. 
- Enables flexible human-AI collaboration by allowing users to customize action space or intervene in story generation.
- Establishes a new paradigm for creative content generation using iterative LLM feedback loops.

In summary, the key innovation is the interactive two-model structure for exploring the space of possible stories. By using an auxiliary LLM to guide story progression, the approach allows control over quality and direction leading to more compelling and surprising narrative.


## Summarize the paper in one sentence.

 Here is a one sentence summary of the paper:

The paper proposes Storytelling With Action Guidance (SWAG), a novel approach for automated long-form story generation that uses a two-model feedback loop - one LLM to generate story content, and another auxiliary LLM to choose optimal actions to steer the story - resulting in more engaging and higher quality stories compared to previous end-to-end generation techniques.


## What is the main contribution of this paper?

 The main contribution of this paper is proposing Storytelling With Action Guidance (SWAG), a novel approach to long-form story generation using large language models (LLMs). Specifically:

- SWAG reduces story writing to a search problem through a two-model feedback loop: one LLM generates story content, another auxiliary LLM chooses the next best "action" to steer the story's future direction. This allows for improved control and more engaging content compared to end-to-end generation.

- The paper introduces a method for training an action discriminator LLM (AD LLM) on human preferences for story actions, using techniques like supervised fine-tuning and direct preference optimization. 

- Through quantitative analysis and human evaluation, the paper shows SWAG can substantially outperform previous end-to-end story generation techniques in terms of interestingness, surprise, and coherence.

- The proposed SWAG pipeline is highly modular and can work with any LLM. The paper demonstrates strong performance using only open-source models, surpassing proprietary models like GPT-3.5-Turbo.

In summary, the main contribution is proposing and evaluating SWAG, a modular and effective approach to controllable long-form story generation through iterative LLM guidance.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in this paper:

1. How exactly does the action discriminator LLM (AD LLM) work to choose the next best action for continuing the story? What is the architecture and training procedure used?

2. The paper mentions using both supervised fine-tuning (SFT) and direct preference optimization (DPO) to train the AD LLM. What are the differences between these two techniques and why are both used? 

3. What specifically does the "story state" represent in the context of this method? How is it updated and passed between the story generation LLM and the AD LLM?

4. The preference dataset used to train the AD LLM seems critical to the success of this approach. What strategies were used to collect diverse and unbiased training data from LLMs like GPT-4?

5. Could you elaborate on how the LongLoRA technique enables efficient fine-tuning of the AD LLM on long context sequences? How does this impact the overall method?

6. In the ablation experiments, what insights were gained by testing different combinations of models for the story generation LLM and AD LLM? How did the match between them impact performance?

7. The action space consists of 50 predefined story actions. How might using more fine-grained and diverse actions impact the quality and variability of story generations?

8. The method seems very flexible in allowing human interaction/collaboration during story generation. Can you describe how a user could intervene and guide the creative process?

9. What are some of the biggest limitations of the current method in terms of models tested, action space constraints, evaluation datasets etc.? How could these be addressed in future work?

10. Do you think this feedback-based technique could be applied successfully to other generative tasks beyond storytelling? What other applications seem promising?
