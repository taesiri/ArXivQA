# [Hufu: A Modality-Agnositc Watermarking System for Pre-Trained   Transformers via Permutation Equivariance](https://arxiv.org/abs/2403.05842)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Pre-trained Transformer models are valuable assets but face threats of theft and unauthorized usage. Watermarking is an important technique to protect model ownership. 
- However, existing watermarking methods are customized for specific models/tasks, not unified across modalities. They rely on trigger samples which can be exposed.  
- The goal is to design a modular, trigger-free watermarking framework for Transformers.

Method - Hufu:
- Key idea: Leverage permutation equivariance in Transformers - model gives same output for permuted inputs if weights are correspondingly permuted.
- Thus a model can learn two sets of weights - one for normal usage, another (permutation of the first) for watermarking.
- Watermark by fine-tuning model on data with secret permutation P. Output features are restored by P^{-1 and matched to secret vector sk via a decoder. 
- Embed watermark in shadow model to make extraction robust even from stolen models.
- Extract watermark by feeding permuted input to model. Restore output order by P^{-1, decode with G to match sk.

Contributions:
- Discovered permutation equivariance property in Transformers, enabling two weight sets in one model.
- Designed Hufu - a novel, modular, trigger-free watermarking system for Transformers.
- Watermark relies on model structure rather than data triggers, hence more robust.
- Evaluated on Vision Transformers, BERT and GPT. Showed efficiency, perfect effectiveness, minimal impact on tasks, and robustness to attacks.
- With task/modality-agnostic design, Hufu is promising for unified IP protection of various Transformers.
