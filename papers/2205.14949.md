# [HiViT: Hierarchical Vision Transformer Meets Masked Image Modeling](https://arxiv.org/abs/2205.14949)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading, the key research question addressed in this paper is: 

How can we design an efficient hierarchical vision transformer architecture that enjoys both high performance and training speed in masked image modeling (MIM)?

The authors note that hierarchical vision transformers like Swin Transformer have superior performance but slow training speed for MIM due to operations like shifting window attention. On the other hand, standard vision transformers like ViT have fast training but poorer performance. 

To get the best of both worlds, the paper proposes a new architecture called HiViT that modifies Swin Transformer to remove unnecessary local inter-unit operations that hinder computational efficiency for MIM. The goal is to achieve a hierarchical vision transformer that is high-performing, flexible, and fast for masked image modeling.

In summary, the central hypothesis is that by eliminating certain local inter-unit operations from hierarchical vision transformers, it is possible to attain both strong accuracy and high training speed in the MIM paradigm. The design of HiViT aims to test this hypothesis.


## What is the main contribution of this paper?

 Based on my reading of the paper, the main contributions appear to be:

- The proposal of HiViT, a hierarchical vision transformer architecture designed to combine the strengths of both plain (e.g. ViT) and hierarchical (e.g. Swin) vision transformers for masked image modeling. 

- Removing redundant "local inter-unit operations" from hierarchical vision transformers like Swin Transformer to make them more efficient and flexible for masked image modeling. This involves modifications like aligning the masking unit size with the main stages' token size, removing later stages, and replacing early-stage window attentions with MLPs.

- Empirical evaluations showing HiViT can outperform both ViT and Swin Transformer on ImageNet classification with fully supervised training. It also shows advantages for masked image modeling, achieving better accuracy and 1.9x faster training than Swin Transformer.

- Demonstrations that removing the local inter-unit operations does not hurt performance on various vision tasks including classification, detection, and segmentation. This validates the flexibility of HiViT for different applications.

In summary, the main contribution appears to be proposing HiViT to make hierarchical vision transformers more compatible with masked image modeling, while retaining their performance advantages over plain vision transformers like ViT. The modifications to remove unnecessary local operations are shown to provide benefits across multiple vision tasks.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the key point in the paper:

The paper proposes a hierarchical vision transformer architecture called HiViT that achieves both high efficiency and good performance in masked image modeling for self-supervised visual pre-training by removing unnecessary local inter-unit operations.


## How does this paper compare to other research in the same field?

 Here are a few key points on how this paper compares to other related research:

- This paper presents HiViT, a new hierarchical vision transformer architecture for masked image modeling (MIM). Other recent works have also explored combining hierarchical vision transformers like Swin Transformer with MIM, such as SimMIM. A key difference is that HiViT is designed to enable the efficient training techniques of MAE to be applied, by removing unnecessary 'local inter-unit operations' that would prevent masking patches at the input stage.

- The paper shows that HiViT outperforms vanilla ViT and competes with or exceeds Swin Transformer in terms of accuracy on ImageNet classification with supervised training. This suggests the modifications help incorporate useful inductive biases from Swin while maintaining flexibility.

- For self-supervised MIM pre-training, the paper shows HiViT can match or exceed the accuracy of MAE and SimMIM after fine-tuning, while training significantly faster than SimMIM. This demonstrates the benefits of HiViT's design for efficient MIM training.

- The paper includes ablation studies that provide insight into the impact of different architectural design choices in HiViT. For example, removing the late stages of Swin Transformer improved accuracy, while using relative position encodings was important for good performance. 

- For transfer learning, the paper shows strong results for HiViT on object detection in COCO and semantic segmentation in ADE20K, competitive with or exceeding other recent MIM methods. This helps validate the learned representations transfer effectively.

Overall, a key contribution is showing that the efficiency and flexibility of ViT for MIM can be combined with the inductive biases of hierarchical vision transformers through careful architecture design in HiViT. The results highlight the potential of HiViT as a strong backbone for self-supervised representation learning and downstream tasks.


## What future research directions do the authors suggest?

 Based on my reading of the paper, some key future research directions suggested by the authors include:

- Developing more flexible architecture designs that go beyond the constraints of having a fixed masking unit size. The authors note that a limitation of their method is that the masking unit size is fixed, which requires choosing a single "main stage". They suggest modifying low-level code (e.g. CUDA) to support arbitrary and variable grouping of tokens as a possible solution.

- Exploring different recovery targets in masked image modeling besides pixel recovery. The authors focus on pixel-level recovery but note other targets like discrete visual tokens could be promising directions.

- Applying the HiViT architecture to other self-supervised learning methods beyond masked image modeling. The authors designed HiViT specifically for efficiency in MIM, but suggest it could be beneficial for other self-supervised methods too.

- Extending HiViT to other computer vision tasks, particularly ones relying on modeling long-range dependencies in images. The authors highlight HiViT's potential for tasks where long-range relationship modeling is important.

- Developing training techniques tailored for HiViT, such as regularization methods. The authors use standard training techniques for HiViT but suggest exploring HiViT-specific training advances.

- Combining the benefits of HiViT and hybrid vision transformers. The authors note hybrid designs using both convolutional and transformer modules achieve top results, suggesting exploring integrating HiViT with convolutional modules.

In summary, the main future directions are around exploring more flexible architectures, extending HiViT to other methods and tasks, and developing tailored training techniques to further improve HiViT's capabilities. The overarching goal is developing more advanced transformer architectures for computer vision.
