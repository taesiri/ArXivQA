# Iteratively Prompt Pre-trained Language Models for Chain of Thought

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question addressed is: How can we equip pre-trained language models (PLMs) with the ability to develop a "chain of thought" for complex multi-step reasoning? Specifically, the authors are investigating how to elicit relevant knowledge from PLMs step-by-step in order to perform multi-step inference to answer complex queries. This involves iteratively prompting the PLM to recall the necessary knowledge statements to derive the answer.The key hypotheses are:- An iterative prompting framework where knowledge is elicited from the PLM incrementally will be more effective than non-iterative approaches for complex multi-step reasoning.- A context-aware prompting approach that dynamically generates prompts conditioned on the current step's context will be superior to prior prompting methods that use static prompts for this iterative elicitation.So in summary, the central research question is how to guide PLMs to develop a logical chain of thought via iterative context-aware prompting in order to perform complex multi-step reasoning. The core hypotheses focus on the benefits of the iterative prompting framework and a context-aware prompt generation method.


## What is the main contribution of this paper?

The main contribution of this paper is proposing an iterative prompting framework to guide pre-trained language models to perform multi-step reasoning. Specifically:- They propose an iterative prompting scheme where the pre-trained language model (PLM) is prompted repeatedly to recall a series of relevant knowledge facts needed to answer a complex query. This mimics how humans develop a "chain of thought".- They design a context-aware prompter module that dynamically generates prompts based on the current context (query + previously recalled facts). This allows the prompts to vary across reasoning steps and integrate necessary context.- They conduct experiments on multi-hop QA and commonsense reasoning datasets. Results show their approach outperforms prior prompting methods by large margins and approaches fine-tuning performance, while keeping the PLM parameters frozen.- They perform analysis to demonstrate the faithfulness of the learned prompting behavior, ruling out exploitation of spurious patterns. This helps validate that their method genuinely guides the PLM through multi-step reasoning.In summary, the key contribution is an iterative prompting framework with an adaptive context-aware prompter to elicit knowledge from PLMs for complex multi-step inference, together with supporting experiments and analysis. The work helps advance prompting as a technique for reasoning over implicit knowledge in large PLMs.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the main idea in the paper:The paper proposes an iterative prompting framework to progressively elicit knowledge stored in pre-trained language models and guide them to develop a logical "chain of thought" for complex reasoning tasks.


## How does this paper compare to other research in the same field?

Here are a few key ways this paper compares to other related work:- This paper explores iterative prompting to help pre-trained language models (PLMs) perform multi-step reasoning by eliciting a "chain of thought". Other work has shown PLMs struggle at complex reasoning despite having a lot of knowledge, so this aims to address that gap.- The proposed iterative prompting framework progressively prompts the PLM to recall relevant knowledge across multiple steps for a given reasoning task. This differs from prior prompting work that focused on single-step factual knowledge retrieval.- The context-aware prompter model is designed to dynamically generate prompts conditioned on the evolving context at each reasoning step. This aims to capture the variability needed across steps, unlike static/context-agnostic prompting methods.- Experiments compare iterative prompting against non-iterative baselines using fine-tuning and prompting methods like prompt/prefix tuning on multi-hop QA and commonsense reasoning datasets. Results show benefits of the iterative scheme and context-aware prompter.- Analysis examines faithfulness of prompting, including test-train overlap analysis and random model/embedding probes to evaluate spurious pattern exploitation. This helps interpret the validity of results.- The focus is on knowledge elicitation and reasoning from a fixed PLM, rather than end task performance. So it differs from dataset-specific reasoning methods that incorporate other specialized components.Overall, the key novelty is in iterative prompting to elicit chained reasoning from PLMs, enabled by the context-aware dynamic prompter. Experiments demonstrate strengths over non-iterative and static prompting baselines. The faithfulness analysis also helps advance understanding of learned prompting behaviors.
