# AugESC: Dialogue Augmentation with Large Language Models for Emotional   Support Conversation

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be: How can large language models be leveraged to augment crowdsourced dialogue data and improve downstream dialogue models' generalization ability? Specifically, the authors aim to address the limitations of current crowdsourced dialogue datasets, which are expensive to collect and limited in scale and topic coverage. They hypothesize that large language models can be used to automatically augment dialogue data through a dialogue completion approach, thereby improving generalization of downstream models trained on this data. The key research questions/hypotheses appear to be:- Can treating dialogue augmentation as a dialogue completion task and leveraging large LMs lead to effective augmentation with reasonable quality?- Can the augmented dataset, AugESC, improve generalization capability of downstream dialogue models to open-domain topics compared to only using the original crowdsourced data?- How does this dialogue completion approach compare to strong baselines for dialogue augmentation like simulated conversation between separately trained models?- What is the quality of the augmented dialogues produced by this approach compared to crowdsourced and simulated dialogues based on human evaluation?So in summary, the central hypothesis is that large LMs can augment limited dialogue data through dialogue completion to improve generalization, and the key questions surround the effectiveness and quality of this approach. The human and automatic evaluations aim to validate the utility of the augmented dataset and compare dialogue quality across different augmentation methods.


## What is the main contribution of this paper?

The main contribution of this paper is proposing an effective approach for augmenting dialogue data using large language models. Specifically:- The paper presents an approach that treats dialogue augmentation as a dialogue completion task. By fine-tuning a large language model (GPT-J) on sample dialogues and then prompting it to complete full dialogues from various starting points, the authors are able to generate a large amount of augmented dialogues.- Using this approach, the authors construct AugESC, an augmented dataset for the emotional support conversation (ESC) task, which has 45x more dialogues and greatly extended topic coverage compared to the original crowdsourced ESConv dataset.- Through comprehensive human evaluation, the authors demonstrate that their approach produces higher quality augmented dialogues than strong baselines. The quality of AugESC is shown to be comparable to the crowdsourced ESConv. - Further human interactive evaluation verifies that post-training dialogue models on AugESC substantially improves their generalization capability to open-domain topics unseen during training.In summary, the key contribution is proposing an effective dialogue augmentation approach via language model prompting, constructing a high-quality augmented dataset AugESC, and demonstrating its utility in improving dialogue models' generalization ability. The results highlight the potential of leveraging large language models to alleviate data scarcity issues for dialogue tasks.
