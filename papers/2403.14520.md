# [Cobra: Extending Mamba to Multi-Modal Large Language Model for Efficient   Inference](https://arxiv.org/abs/2403.14520)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Existing multimodal large language models (MLLMs) rely on Transformer networks which have quadratic computational complexity, making them inefficient for deployment. 

Proposed Solution:
- The paper proposes Cobra, a novel MLLM that uses the Mamba language model as its backbone instead of Transformer networks. Mamba has linear computational complexity.

- Cobra integrates visual information into Mamba by using a vision encoder consisting of DINOv2 and SigLIP ViT models. The visual representations are projected into the same latent space as the language representations.

- Different modal fusion schemes are explored to optimize the integration of visual and linguistic information within Mamba. The best performing approach concatenates the output of DINOv2 and SigLIP encoders.

Main Contributions:

1. Cobra introduces a MLLM with linear computational complexity by using Mamba as the backbone, instead of quadratic Transformer networks. This improves efficiency.

2. The paper investigates different modal fusion schemes to integrate visual information into Mamba effectively. DINOv2 + SigLIP combination works the best.  

3. Experiments show Cobra achieves very competitive performance compared to models like LLaVA, with much fewer parameters. It also excels in spatial relationship judgments and overcoming visual hallucinations.

4. Cobra performs 3-4x faster inference than MobileVLM v2 and TinyLLaVA with similar model size. The speedup comes from the linear complexity of Mamba.

In summary, Cobra explores a new direction of using linear complexity models as the backbone for Multimodal LLMs. The efficient fusion with visual data, combined with competitive performance establishes it as an effective approach for this task.
