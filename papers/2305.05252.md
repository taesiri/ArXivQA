# Distilling Script Knowledge from Large Language Models for Constrained   Language Planning

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question/hypothesis seems to be:How to improve the constrained language planning ability of large language models (LLMs), and effectively transfer such ability to smaller, more specialized models through dataset distillation. Specifically, the authors aim to:- Define and establish the novel task of "constrained language planning", where the goal is to generate goal-oriented scripts that faithfully adhere to specific constraints. This advances language planning from just abstract goals to more complex, real-world specific goals.- Evaluate LLMs on constrained language planning and show they tend to generate fluent but unfaithful scripts. Propose an "over-generate then filter" approach to improve faithfulness.- Use the improved LLMs to generate a high-quality "CoScript" dataset of constrained language planning examples. Show this dataset can effectively train smaller models for the task, achieving performance comparable to LLMs.So in summary, the core hypothesis is that LLMs can be improved at constrained language planning via better prompting and dataset distillation, enabling specialized smaller models to acquire strong constrained planning abilities. The paper aims to demonstrate this through empirical experiments and analysis.


## What is the main contribution of this paper?

Based on my reading, the main contributions of this paper are:1. It defines the novel problem of constrained language planning, which focuses on generating goal-oriented scripts for specific goals with multi-facet constraints, advancing language planning research beyond just abstract goals.2. It proposes an over-generate-then-filter approach to improve the constrained language planning ability of large language models (LLMs). Experiments show this method boosts the constraint faithfulness of LLM-generated scripts by 26%. 3. It uses the improved LLMs to generate a new constrained script dataset called CoScript, which contains 55,000 high-quality scripts. This dataset enables training specialized models for constrained language planning.4. Experiments demonstrate that models trained on CoScript, like T5, can achieve strong constrained language planning performance comparable or even surpassing LLMs like GPT-3 and InstructGPT. This shows the value of the CoScript dataset.5. The work provides empirical analysis of the constrained language planning behavior of LLMs. It identifies their common failure modes like ignoring constraints, guides the methodology design, and offers insights on this underexplored topic.In summary, the key contribution is defining the novel constrained language planning problem, developing methods to improve LLM performance on it, constructing a dataset to train specialized models, and extensive empirical analysis around this new direction. The proposed techniques and dataset open up avenues for future research on language planning for complex goals.
