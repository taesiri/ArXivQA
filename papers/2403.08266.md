# [Sketch2Manga: Shaded Manga Screening from Sketch with Diffusion Models](https://arxiv.org/abs/2403.08266)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Manga screening (adding screentones to manga sketches) is a tedious process for artists. There are no existing automatic methods tailored for manga screening that can generate high-quality shaded screentones. 
- Existing sketch-to-manga methods rely on additional inputs like reference images or produce low-quality unshaded screentones.
- Existing illustration-to-manga methods struggle with rich color regions and consistently generate manga with low contrast or unnatural shading.

Proposed Solution:
- A sketch-to-manga framework with a color illustration as an intermediary between the sketch and manga domains.
- A two-step approach:
   1) Generate a color illustration from the sketch using a text-to-image diffusion model.
   2) Generate a manga with shaded screentones from the color illustration's intensity map using a tailored latent diffusion model finetuned on high-quality manga data.
- Finetune the VAE and U-Net components of Stable Diffusion with a manga dataset and custom loss (without LPIP loss).
- Intensity map of color illustration used as conditional input during diffusion to align screentones with shading.  
- An adaptive scaling method to integrate the generated screentones into the color illustration while preserving shading details.

Main Contributions:
- First fully automatic sketch-to-manga approach for generating high-quality manga with shaded, high-frequency screentones.
- A finetuned latent diffusion model conditioned on intensity guidance capable of producing quality screentones.
- An effective two-step intermediary approach over direct generation.
- An adaptive scaling technique to properly blend screentones and illustrations.

In summary, the paper proposes a tailored framework to automatically convert sketches to manga through an intermediary color illustration using intensitiy-guided diffusion models and adaptive scaling. Both qualitative and quantitative evaluations demonstrate superior manga generation ability compared to state-of-the-art methods.
