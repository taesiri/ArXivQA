# [Rebuilding ROME : Resolving Model Collapse during Sequential Model   Editing](https://arxiv.org/abs/2403.07175)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem Statement
- Recent work has shown that the popular model editing method called Rank-One Model Editing (ROME) suffers from "disabling edits" - certain edits that cause sudden loss of model performance, inability to recall edited facts, and model collapse. These disabling edits limit the use of ROME for sequential editing, where multiple edits are made to the same model. 

- Two metrics are proposed to identify disabling edits - (1) normalized update matrix norm (|Delta|) and (2) generation entropy. Using these metrics, it is shown that disabling edits exist when using the CounterFact dataset for editing but not when using the zsRE dataset.

Proposed Solution  
- The authors rebuild the ROME codebase from scratch. Surprisingly, with their reimplementation (called r-ROME), the problem of disabling edits and model collapse goes away, allowing large-scale sequential editing.

- Sequential editing experiments on GPT-J 6B and GPT2-XL 1.5B show that the original ROME implementation leads to high |Delta| and sudden model collapse, whereas r-ROME leads to lower |Delta| and gradual performance decay, enabling extensive sequential editing.

Key Contributions
- Show that disabling edits and model collapse only happen when editing using the CounterFact dataset and not zsRE dataset
- Provide a more stable reimplementation of ROME (r-ROME) that resolves model collapse, allowing large-scale sequential editing
- Open source r-ROME codebase for the research community
