# [Fixed Random Classifier Rearrangement for Continual Learning](https://arxiv.org/abs/2402.15227)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem Statement
The paper addresses the problem of catastrophic forgetting in neural networks, where networks forget previously learned knowledge after training on new tasks. Specifically, it looks at continual learning for visual classification tasks with a shared backbone and task-specific classifiers. It notes that most current methods focus on constraining the backbone to mitigate forgetting, while underestimating the role of the classifiers. 

Key Idea
The key idea is that the norm of the equivalent one-class classifier, formed by differencing the two binary classifiers, affects the forgetting level. classifiers with higher norms magnify variation in the backbone's latent representations, leading to more forgetting. 

Proposed Solution
The paper proposes a two-stage algorithm called Fixed Random Classifier Rearrangement (FRCR):

1. Replace standard classifiers with fixed random ones, constraining classifier norms without impacting performance. This also implicitly prevents overfitting to old tasks.

2. Rearrange the entries of new task classifiers to reduce drift in old task latent representations. This is done by minimizing correlation between old and new one-class classifiers, approximately orthogonalizing the search directions.

Experiments and Results
Experiments were conducted on split versions of MNIST, FashionMNIST and CIFAR10. FRCR gave significantly improved performance over baselines like EWC and Stable SGD on continuity metrics like average accuracy and forgetting rate. Further analysis visualized the benefits of fixed classifiers and confirmed reduced latent representation drift.

Main Contributions
- Identified connection between equivalent one-class classifier norms and forgetting levels
- Proposed simple yet effective FRCR algorithm using fixed random classifiers and rearrangement to constrain classifier norms and representational drift
- Demonstrated strong performance, outperforming regularization-based and replay-free methods on multiple datasets
