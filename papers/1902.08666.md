# [From open learners to open games](https://arxiv.org/abs/1902.08666)

## What is the central research question or hypothesis that this paper addresses?

The central research question of this paper is establishing a formal connection between open learners (a framework for machine learning systems) and open games (a framework for game theory). Specifically, the paper proves that there is a faithful symmetric monoidal functor from the category of open learners to the category of open games. The key findings are:- There is a canonical way to view any neural network (or other simple machine learning system) as a fragment of a game, where each parameter acts as a player trying to optimize its strategy. - The gradient descent dynamics of a neural network can be encoded in the best response relation of the corresponding game.- This allows translating concepts between machine learning and game theory, like composing networks and composing games.So in summary, the main contribution is rigorously connecting these two frameworks mathematically, suggesting many potential applications of using game theory to analyze machine learning and vice versa. The functor constructed provides the formal link between the two theories.


## What is the main contribution of this paper?

The main contribution of this paper is proving that there is a faithful symmetric monoidal functor from the category of open learners to the category of open games. This establishes a formal connection between machine learning and game theory by showing that any supervised neural network (without feedback) can be viewed canonically as an open game. Specifically, the paper defines a functor F from the category of open learners Learn to the category of open games OG. It shows that:- F maps identities and composition in Learn to identities and composition in OG, so it defines a functor. - F is faithful, meaning morphisms that are equal in OG must have originated from equal morphisms in Learn. - F preserves monoidal product, so it is a symmetric monoidal functor. - F maps the counit morphisms in Learn to the counit morphisms in OG.The main significance is that this functorial mapping provides a way to interpret any neural network or learning algorithm as a game, with parameters acting as players and gradient descent encoded in the best response relation. This opens up many possibilities for using game theory to analyze machine learning systems.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper proves there is a canonical way to view any sufficiently simple neural network as a fragment of a game, with parameters controlled by players and gradient descent encoded in the best response relation, and suggests this link between machine learning and game theory could lead to new hybrid techniques combining strengths of both fields.
