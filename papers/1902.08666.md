# [From open learners to open games](https://arxiv.org/abs/1902.08666)

## What is the central research question or hypothesis that this paper addresses?

 The central research question of this paper is establishing a formal connection between open learners (a framework for machine learning systems) and open games (a framework for game theory). Specifically, the paper proves that there is a faithful symmetric monoidal functor from the category of open learners to the category of open games. 

The key findings are:

- There is a canonical way to view any neural network (or other simple machine learning system) as a fragment of a game, where each parameter acts as a player trying to optimize its strategy. 

- The gradient descent dynamics of a neural network can be encoded in the best response relation of the corresponding game.

- This allows translating concepts between machine learning and game theory, like composing networks and composing games.

So in summary, the main contribution is rigorously connecting these two frameworks mathematically, suggesting many potential applications of using game theory to analyze machine learning and vice versa. The functor constructed provides the formal link between the two theories.


## What is the main contribution of this paper?

 The main contribution of this paper is proving that there is a faithful symmetric monoidal functor from the category of open learners to the category of open games. This establishes a formal connection between machine learning and game theory by showing that any supervised neural network (without feedback) can be viewed canonically as an open game. 

Specifically, the paper defines a functor F from the category of open learners Learn to the category of open games OG. It shows that:

- F maps identities and composition in Learn to identities and composition in OG, so it defines a functor. 

- F is faithful, meaning morphisms that are equal in OG must have originated from equal morphisms in Learn. 

- F preserves monoidal product, so it is a symmetric monoidal functor. 

- F maps the counit morphisms in Learn to the counit morphisms in OG.

The main significance is that this functorial mapping provides a way to interpret any neural network or learning algorithm as a game, with parameters acting as players and gradient descent encoded in the best response relation. This opens up many possibilities for using game theory to analyze machine learning systems.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the paper:

The paper proves there is a canonical way to view any sufficiently simple neural network as a fragment of a game, with parameters controlled by players and gradient descent encoded in the best response relation, and suggests this link between machine learning and game theory could lead to new hybrid techniques combining strengths of both fields.


## How does this paper compare to other research in the same field?

 Here are a few key points about how this paper relates to other research on open learners and open games:

- It establishes a formal connection between the categories of open learners and open games by constructing a faithful symmetric monoidal functor from the former to the latter. This links two bodies of research that appear very similar but were developed independently.

- The paper suggests that viewing neural networks as fragments of games via this functor could allow techniques from game theory to be applied to analyzing and designing neural networks and other machine learning systems. This proposes new potential applications building on the formal link.

- The paper points out that the distinction between open learners and open games is that open games involve counterfactual reasoning via continuations, while open learners only have single output values. This highlights a key conceptual difference between the theories.

- The author suggests many directions for future work exploiting the connection, including decomposing game players into neural networks, using learning to approximate equilibria, designing GANs and multi-agent learning systems, and creating hybrid theories. This outlines an ambitious research program based on the link.

- Overall, the paper establishes a formal foundation connecting two previously disjoint theories, and proposes this be used as the basis for a variety of new applications and research directions integrating machine learning and game theory. The functor itself is the core conceptual contribution.

In summary, the paper relates the two fields in a novel way and sets out an extensive agenda for future work building on that relationship. The core contribution is the formal functor mapping open learners to open games.


## What future research directions do the authors suggest?

 The paper suggests several future research directions:

- Analyzing what kinds of games can arise from neural networks and other learning algorithms using game theory techniques. 

- Decomposing game players into learning algorithms to get more realistic models of learning agents.

- Using gradient descent in games to efficiently approximate equilibria.

- Formalizing GANs as games between neural networks. 

- Systematically designing new GAN-like systems with game theory.

- Creating hybrid systems combining game theory, multiagent systems, and machine learning.

- Factoring open games and open learners to get a more modular proof relating them.

- Applying game theory and multiagent systems to improve machine learning.

- Using machine learning to find equilibria in games.

- Creating a unified theory combining game theory, multiagent systems, and machine learning.

The key suggestions are using each field to improve the others, creating hybrid systems, and finding more modular theoretical connections between them.


## Summarize the paper in one paragraph.

 This paper establishes a formal connection between open learners, which model neural networks and other machine learning systems, and open games, which are a framework for compositional game theory. It proves that there is a faithful symmetric monoidal functor F from the category of open learners to the category of open games. This means any neural network can be canonically viewed as a game, with parameters corresponding to players' strategies and gradient descent corresponding to best response dynamics. The paper suggests this link could allow techniques from game theory to analyze machine learning systems and machine learning techniques like neural networks to approximate equilibria in games. It poses several open questions about applying ideas across the two fields, like using games to structure equilibrium-finding programs and using neural networks as more realistic game players.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the paper:

This paper establishes a formal connection between the categories of open learners and open games. Open learners, proposed by Fong, Spivak and Tuyéras, are a general framework for supervised learning systems that includes neural networks as a special case. The category of open learners has neural networks as morphisms, with composition and monoidal product corresponding to sequential and parallel composition. Open games, proposed by Hedges et al., are a framework for game theory based on rational players and best response dynamics. The category of open games also has composition and monoidal product. 

The main contribution of this paper is to construct a faithful symmetric monoidal functor from the category of open learners to the category of open games. This shows that any supervised neural network can be canonically viewed as an open game, with parameters acting as players and gradient descent dynamics encoded in the best response relation. The author suggests several potential applications of this link between machine learning and game theory, such as using game theory to analyze neural networks, approximating economic equilibria with neural networks, and formalizing the game dynamics in generative adversarial networks. Overall, the functor establishes a precise connection between the compositional structure of neural networks and games.


## Summarize the main method used in the paper in one paragraph.

 Here is a one paragraph summary of the main method used in the paper:

The paper defines a functor F from the category of open learners to the category of open games. An open learner is a supervised learning system represented categorically, including neural networks as a special case. An open game is a fragment of a non-cooperative game used in compositional game theory. The functor F associates each open learner, consisting of parameters and an update rule, to an open game where the parameters become players' strategies and the update rule becomes a best response relation. This establishes a formal mathematical connection showing that any neural network can be viewed canonically as a game where gradient descent is represented in the best response dynamics. The functor is shown to be faithful, preserving identities and composition. The paper suggests this link could lead to new techniques applying game theory to analyze neural networks and new ways to design learning algorithms based on games.


## What problem or question is the paper addressing?

 The paper is establishing a formal connection between the theory of open learners and the theory of open games. Open learners, developed by Fong, Spivak and Tuyéras, provide a categorical model of machine learning systems like neural networks that captures things like backpropagation and gradient descent. Open games, developed by Hedges and others, provide a compositional model of game theory. 

The main contribution is proving that there is a faithful symmetric monoidal functor from the category of open learners to the category of open games. This means any open learner (i.e. a supervised machine learning system without feedback) can be canonically viewed as an open game. 

Roughly speaking, each parameter in the learning system acts as a "player" in the game, and the game's best response relation encodes the dynamics of gradient descent and parameter updating.

The paper suggests this link could lead to new techniques and applications combining machine learning and game theory, like using learning dynamics to find equilibria in games, or framing GANs and other multi-agent learning systems game-theoretically. It also poses some open theoretical questions about finding a simpler, more modular proof of the functor.

Overall, the main problem addressed is establishing and beginning to explore an unexpected formal similarity between two theories - open learners and open games. The functor is the technical device used to demonstrate the connection.
