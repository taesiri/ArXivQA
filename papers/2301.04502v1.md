# [Pruning Compact ConvNets for Efficient Inference](https://arxiv.org/abs/2301.04502v1)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research questions addressed are:

1) RQ1: Pruning to improve computation vs. performance tradeoff. Can a model obtained by pruning a larger FBNetV3 model M1 (optimized using NAS) achieve higher generalization performance than a smaller FBNetV3 model M2 when the pruned model has the same number of FLOPs as M2?

2) RQ2: Pruning as an efficient paradigm. When a larger FBNetV3 model M1 is available and computational resources are limited, is pruning a faster and less computationally expensive approach to obtain a model with higher accuracy at a desired computation level (FLOPs) than running a full-fledged architecture search?

In summary, the authors are interested in understanding whether pruning can further optimize neural networks that have already been optimized through NAS techniques like the FBNetV3 family of models. Specifically, they want to see if pruning larger FBNetV3 models can achieve better accuracy-computation tradeoffs than smaller FBNetV3 models optimized directly through NAS. They also want to evaluate if pruning is a more efficient way to obtain high-accuracy low-FLOP models compared to rerunning the costly NAS procedures.
