# Controllable Text Generation with Language Constraints

## What is the central research question or hypothesis that this paper addresses?

The central research question addressed in this paper is how to control text generation in large language models using natural language constraints. The paper introduces a new task called Cognac where the goal is to generate text that conforms to a specified topic and avoids violating certain constraints, with both being provided in plain English rather than as special control codes. The key hypothesis is that explicitly querying the language model itself to obtain internal guidance on what tokens to generate or avoid based on the provided topic and constraints will allow better control over text generation compared to prior approaches.The paper proposes a method called CognacGen that generates guidance terms for the given topic/constraint using the language model, and then uses techniques like prefix tuning to distill this guidance into the model. At test time, the guidance is used to modify the model's token probabilities to steer generation. The central hypothesis is that leveraging the model's own knowledge through self-guidance will outperform alternative approaches like fine-tuning or using classifiers for controlled text generation.In summary, the key research question is whether self-guidance based on a language model's internal knowledge can enable better control over text generation with natural language constraints compared to prior techniques. The paper introduces a new task and dataset to study this question, and proposes the CognacGen method to test the hypothesis.


## What is the main contribution of this paper?

The main contribution of this paper is developing a method for controllable text generation in language models using natural language constraints. Specifically:- The paper introduces a new benchmark task called Cognac that provides topics and constraints in natural language along with example text, and challenges models to generate text conforming to both. The datasets are created using WordNet and Wikidata to provide knowledge-focused constraints.- The paper proposes a method called CognacGen that leverages the language model's own knowledge to guide generation. It uses prefix tuning to distill guidance from the LM itself on how to generate text conforming to a topic and avoiding a constraint. - Three forms of guidance are proposed - binary verifier, top-k tokens, and textual examples. The textual example guidance uses a trie-based generation method to properly incorporate multi-token guidance.- Experiments show CognacGen outperforms competitive baselines like PPLM and self-debiasing, especially when evaluating on unseen natural language instructions. The analysis provides insights into the benefits of the proposed guidance mechanisms.In summary, the main contribution is the Cognac benchmark and CognacGen method for controlled text generation using the language model's own knowledge and natural language constraints. This allows for more precise control compared to prior attribute-based constraints.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points in the paper:The paper proposes a new task called Cognac for controllable text generation in language models using natural language constraints, introduces corresponding datasets based on WordNet and Wikidata, and develops a method called CognacGen that leverages the language model's own knowledge via self-guidance and prefix tuning to steer generation towards conforming to specified topics and constraints.


## How does this paper compare to other research in the same field?

Here are a few key ways this paper compares to other research on controllable text generation:- The use of natural language for specifying topics and constraints is more flexible than many prior approaches that rely on predefined control codes or attributes. Allowing users to describe constraints in natural language makes it easier to apply these methods to new domains.- The datasets created from WordNet and Wikidata introduce more knowledge-intensive constraints compared to prior work that often focuses on high-level concepts like sentiment or toxicity. The knowledge-based constraints strike a balance between broad attributes and narrow lexical constraints.- The proposed methods do not require retraining language models, unlike control code training approaches like CTRL. The prefix tuning approach allows leveraging a language model's own knowledge to guide generation, without expensive retraining.- The focus on compositional constraints sets it apart from prior work on single attribute constraints. Guiding the model to stay on topic while avoiding certain content makes the task more challenging.- The results demonstrate these methods can work well even for large language models. Prior work on decoding modifications have been applied mostly to smaller T5 models. The gains over strong baselines like GPT-3 are noteworthy.Overall, the use of natural language control, knowledge-focused datasets, prefix tuning for guidance, and compositional constraints make this work unique compared to prior research on controllable text generation. The proposals also seem promising based on the empirical results, despite limitations of the knowledge bases used.


## What future research directions do the authors suggest?

Based on my reading of the paper, here are some key future research directions suggested by the authors:- Scaling up their proposed method \sysname{} to even larger pre-trained language models. The authors note that while their method uses a 1.5B parameter model, it could likely benefit from scaling to larger models like GPT-3 or InstructGPT.- Improving the guidance models to provide higher quality guidance examples for modifying the base LM's probabilities during generation. The authors suggest the oracle model provides an upper bound on performance, indicating there is room to improve the guidance quality.- Generalizing the approach to other languages besides English. The authors note their techniques should be applicable to other languages as well, though this needs to be verified experimentally. - Applying the method to other domains beyond the WordNet and Wikidata knowledge bases used in the paper. The authors designed their benchmark to require knowledge-intensive constraints, but other knowledge sources could reveal new challenges.- Developing better evaluation metrics and datasets to properly assess controllable text generation models. The authors designed new metrics for their proposed benchmark, but note there is still substantial room for improvement in how we evaluate these models.- Combining their approach with other methods like reinforcement learning or iterative refinement to further improve control over text generation. The prefix-tuning approach provides a simple but effective way to modify an LM's generations.In summary, the key future directions focus on scaling up their approach, improving the quality of guidance, generalizing to new languages/domains, and developing better evaluation methods for this challenging task of controllable text generation.
