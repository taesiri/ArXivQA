# Controllable Text Generation with Language Constraints

## What is the central research question or hypothesis that this paper addresses?

The central research question addressed in this paper is how to control text generation in large language models using natural language constraints. The paper introduces a new task called Cognac where the goal is to generate text that conforms to a specified topic and avoids violating certain constraints, with both being provided in plain English rather than as special control codes. The key hypothesis is that explicitly querying the language model itself to obtain internal guidance on what tokens to generate or avoid based on the provided topic and constraints will allow better control over text generation compared to prior approaches.The paper proposes a method called CognacGen that generates guidance terms for the given topic/constraint using the language model, and then uses techniques like prefix tuning to distill this guidance into the model. At test time, the guidance is used to modify the model's token probabilities to steer generation. The central hypothesis is that leveraging the model's own knowledge through self-guidance will outperform alternative approaches like fine-tuning or using classifiers for controlled text generation.In summary, the key research question is whether self-guidance based on a language model's internal knowledge can enable better control over text generation with natural language constraints compared to prior techniques. The paper introduces a new task and dataset to study this question, and proposes the CognacGen method to test the hypothesis.


## What is the main contribution of this paper?

The main contribution of this paper is developing a method for controllable text generation in language models using natural language constraints. Specifically:- The paper introduces a new benchmark task called Cognac that provides topics and constraints in natural language along with example text, and challenges models to generate text conforming to both. The datasets are created using WordNet and Wikidata to provide knowledge-focused constraints.- The paper proposes a method called CognacGen that leverages the language model's own knowledge to guide generation. It uses prefix tuning to distill guidance from the LM itself on how to generate text conforming to a topic and avoiding a constraint. - Three forms of guidance are proposed - binary verifier, top-k tokens, and textual examples. The textual example guidance uses a trie-based generation method to properly incorporate multi-token guidance.- Experiments show CognacGen outperforms competitive baselines like PPLM and self-debiasing, especially when evaluating on unseen natural language instructions. The analysis provides insights into the benefits of the proposed guidance mechanisms.In summary, the main contribution is the Cognac benchmark and CognacGen method for controlled text generation using the language model's own knowledge and natural language constraints. This allows for more precise control compared to prior attribute-based constraints.
