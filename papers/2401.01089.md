# [Quokka: An Open-source Large Language Model ChatBot for Material Science](https://arxiv.org/abs/2401.01089)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem Statement:
- Existing large language models (LLMs) like ChatGPT lack domain-specific knowledge and are not open-sourced. There is a need for an open-source LLM specialized for the materials science domain.

Proposed Solution:  
- The authors introduce Quokka, an open-source LLM family optimized for materials science by continuing pretraining LLaMA-2 on 1 million materials science papers from S2ORC dataset.
- Two foundation models are released - Quokka-7B and Quokka-13B for building materials science NLP models. 
- Two chatbots are released - Quokka-7B-Chat and Quokka-13B-Chat for dialogues.

Methodology:
- Initial pretraining on 1 million materials science papers to teach domain knowledge
- Instruction tuning on 3344 instructions to improve context-aware responses

Key Outcomes:
- Release of 4 open-source models - Quokka-7B, Quokka-13B, Quokka-7B-Chat, Quokka-13B-Chat
- Specialization for materials science with domain pretraining 
- Dialogue ability via instruction tuning

Future Work:
- Expand instruction set for better instruction following
- Add multimodality for understanding vision input

Overall, the paper introduces much-needed open-source LLMs for the materials science domain, with comprehensive domain pretraining and instruction tuning. Four models are released to benefit the research community.


## Summarize the paper in one sentence.

 This paper presents Quokka, an open-source chatbot for materials science that is optimized by pretraining LLaMa language models on over 1 million materials science research articles and then instruction tuning for dialog abilities.


## What is the main contribution of this paper?

 The main contribution of this paper is the development and release of Quokka, an open-source language model family optimized for the materials science domain. Specifically:

1) The paper introduces Quokka-7B and Quokka-13B, which are variants of the LLaMA-2 language model that have been further pre-trained on over 1 million materials science academic articles from the S2ORC dataset. These foundation models can be used by the research community for building materials science NLP applications.

2) The paper also releases Quokka-7B-Chat and Quokka-13B-Chat, which are chatbot models created by instruction tuning of the foundation models, enabling them to have dialogues and answer questions related to materials science.

3) All four model checkpoints are being openly released to benefit the materials science research community. This includes the costly pre-trained foundation models, not just the final chatbots. 

4) The methodology involves first pre-training the LLaMA-2 base models on domain documents to inject materials science knowledge, followed by instruction tuning to impart conversational and question-answering abilities.

In summary, the main contribution is advancing language modeling capabilities for the materials science domain by releasing optimized and reusable model checkpoints, as well as describing the training methodology.


## What are the keywords or key terms associated with this paper?

 Based on my review of the paper, some of the key terms and concepts associated with this work include:

- Quokka - The name of the open-source large language model chatbot developed in this research for the materials science domain.

- LLaMa-2 - The foundation language model from Meta that Quokka builds upon through further pretraining and instruction tuning. 

- Materials science - The specific domain that Quokka is optimized for through pretraining on over 1 million materials science papers.

- S2ORC - The academic corpus of over 1 million materials science papers used to enhance Quokka's understanding of the field. 

- Instruction tuning - The process used to refine Quokka's ability to interpret queries and instructions related to materials science.

- Chatbot - Quokka is designed as a conversational agent or chatbot to assist with materials science questions and dialogue.

- Open-source - The Quokka model checkpoints are being publicly released to benefit the research community.

So in summary, key terms cover the Quokka chatbot itself, the underlying LLaMa-2 foundation, the materials science domain focus, the training methodology, and the open-source nature of the models.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in the paper:

1. The paper mentions using the S2ORC academic corpus to enhance the model's understanding of materials science. What considerations went into selecting this specific corpus over other options? How large is it and what types of papers are included?

2. The paper talks about mixing the material corpus with 10% general knowledge data. What was the rationale behind using 10% specifically? Were any experiments done to determine the optimal ratio? 

3. The pretraining uses an initial learning rate of 2e-5. How was this hyperparameter value determined? Were other values tested and what impact did it have on model performance?

4. The paper uses an FSDP pipeline and techniques like bf16 and flash attention to improve training speed. Can you elaborate more on how these methods work and quantify the speedup achieved from using them?

5. What quality checks or evaluations were done during the pretraining phase to track progress and model capability? Were perplexity or other metrics tracked?

6. The instruction tuning uses a combination of general and material science specific instructions. What was the process for curating or generating these instructions? How were they validated?

7. The instruction tuning appears to use more epochs than the pretraining phase. What was the motivation behind using 15 epochs here? Was any overfitting analysis done?

8. The paper shows nice loss curve plots for pretraining and instruction tuning. Were any other quantitative metrics or quality evaluations done after these stages to validate model performance?

9. The case studies primarily show text generation examples. Were there any quantitative or human evaluations done to measure the model's ability to understand and respond accurately to materials science questions? 

10. The conclusion mentions future plans for multimodality. What types of multimodal data could be relevant for this application and how might they be incorporated into the model?
