# [Class Adaptive Network Calibration](https://arxiv.org/abs/2211.15088)

## What is the central research question or hypothesis that this paper addresses?

 This paper proposes a new method called Class Adaptive Label Smoothing (CALS) to improve the calibration of deep neural networks during training. The main hypothesis is that using class-wise adaptive penalties instead of a single global penalty for enforcing calibration constraints can lead to better performance, especially for datasets with many categories or class imbalance. 

The key research questions addressed are:

- How can we introduce class-specific weighting when enforcing calibration constraints during training, rather than using a single global hyperparameter as in prior work?

- Can an adaptive optimization strategy based on Augmented Lagrangian Multipliers allow class-wise weights to be learned automatically from data during training? 

- Does this class-adaptive approach improve calibration over prior global penalty methods, particularly for large-scale and class-imbalanced datasets?

Overall, the central hypothesis is that class-adaptive calibration regularization can outperform prior global regularization schemes for training deep networks that are both accurate and well-calibrated, especially for challenging real-world datasets. The paper aims to demonstrate this via the proposed CALS method and evaluation on various benchmarks.
