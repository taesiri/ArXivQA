# [FrameQuant: Flexible Low-Bit Quantization for Transformers](https://arxiv.org/abs/2403.06082)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper "FrameQuant: Flexible Low-Bit Quantization for Transformers":

Problem:
- Transformer models like vision transformers (ViTs) and large language models (LLMs) achieve state-of-the-art performance on many tasks but have a large compute and memory footprint. This makes their deployment expensive and infeasible on many devices.
- Existing post-training quantization methods can compress models to 4 bits with some loss in performance. But quantizing to very low bits like 2 bits leads to a significant drop in accuracy.

Proposed Solution:
- The paper proposes FrameQuant, a new post-training quantization method based on fusion frames from harmonic analysis. 
- Key idea is to compute a redundant overcomplete fusion frame representation of the weight matrices and quantize the weights in that representation space instead of directly quantizing the weights.
- This allows fractional number of bits, e.g. 2.1 or 2.2 bits, providing flexibility to balance model compression and quality.
- Fusion frame representations provide robustness against quantization noise due to redundancy. Known error bounds and optimal recovery guarantees exist.

Method Details:
- Compute fusion frame representations of layer inputs/outputs. Quantize the transformed weight matrix to minimize proxy loss between full-precision and quantized outputs.
- Use weight clipping at 2-sigma level before quantization to improve stability.
- Reconstruction uses straightforward synthesis operator. Can also use Wiener filter for optimal MSE reduction.

Results:
- Extensive experiments on 15 ViT architectures and LLMs like OPT and Llama.
- Consistently outperforms state-of-the-art with ~2 bit models, achieving 2.2 bits with fusion frame redundancy of 1.1x. 
- Significantly closes gap with full-precision performance across models and datasets.

Main Contributions:
- First work exploring fusion frames for model quantization
- Enables flexible low-bit quantization with fractional bits 
- Robust performance with ~2 bits for Vision Transformers and LLMs
- Consistent improvements over all baselines
