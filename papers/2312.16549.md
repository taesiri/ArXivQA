# [How Robust are LLMs to In-Context Majority Label Bias?](https://arxiv.org/abs/2312.16549)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem Statement:
- The paper investigates the robustness of in-context learning (ICL) in large language models (LLMs) to shifts in label distribution, referred to as "majority label bias". 
- Previous works showed ICL is susceptible to such biases, but did not fully examine model performance across varying class proportions.
- Understanding model robustness to skewed label distributions is important since real-world data often contains inherent biases.

Proposed Solution:
- Conduct a comprehensive study on LLMs' robustness boundaries to majority label bias using public text classification datasets. 
- Vary distribution of labels in the prompt from balanced to highly skewed.
- Compare multiple model sizes (7B to 40B parameters) and effect of adding task instructions.
- Define a "Robustness Boundary" metric to quantify tolerance.

Key Contributions:
- Contrary to claims about lack of robustness, LLMs show resilience with Robustness Boundary of 80-100% for binary classification.  
- Larger models and task instructions further improve robustness.
- Performance degrades significantly for multi-class problems.
- Study provides guidance on model selection and prompt design strategies when dealing with biased data.

In summary, the paper provides a comprehensive analysis of how robust different LLMs are to skewed label distributions during in-context learning for text classification tasks. The key findings highlight model capabilities and limitations that can inform real-world deployment.
