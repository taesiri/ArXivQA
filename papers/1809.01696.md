# [TVQA: Localized, Compositional Video Question Answering](https://arxiv.org/abs/1809.01696)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the main research focus is on presenting a new video question answering (VQA) dataset called TVQA and providing baseline experiments and models for this new dataset. Specifically:- The paper introduces the TVQA dataset, which contains over 150K question-answer pairs on 21K video clips from 6 popular TV shows. The key features of TVQA highlighted are:1) It is large-scale and contains natural videos with rich dynamics and social interactions. 2) The video clips are relatively long (60-90 secs) with associated dialogues.3) The questions are compositional, requiring localization of relevant moments as well as multimodal understanding.- The paper analyzes the dataset statistics and properties. It also compares TVQA to other existing VQA datasets.- The paper proposes two VQA tasks on this dataset - QA on grounded clips and QA on full clips. It provides several baseline methods and a multi-stream end-to-end neural network model.- Experiments are presented comparing the baselines and the proposed model. The best result is achieved by the multi-stream model jointly using subtitles, regional visual features, and visual concept features.In summary, the central research contribution is the introduction and analysis of the new TVQA dataset for multimodal video QA, along with baseline experiments and models as a benchmark for future work. The paper focuses on dataset construction and empirical methodology more than novel technical contributions.


## What is the main contribution of this paper?

The main contribution of this paper is the introduction of the TVQA dataset, which is a large-scale video question answering dataset based on 6 popular TV shows. The key advantages of TVQA highlighted in the paper are:- It contains 152,545 human-written QA pairs from 21,793 video clips spanning over 460 hours of video. This makes it a large-scale and natural video QA dataset.- The video clips are relatively long at 60-90 seconds, containing more social interactions and activities compared to previous video QA datasets. - The questions are compositional, consisting of a main question and a temporal grounding part (e.g. "before", "after"). This encourages questions that require temporal reasoning and moment localization within the clips.- The dialogue (character names + subtitles) is provided for each clip. Understanding the relationship between the dialogue and the questions/answers is crucial for answering many questions correctly.- Timestamps are provided to localize the moments relevant to answering each question.In addition to introducing the dataset, the paper also provides benchmark experiments and propose a multi-stream end-to-end neural network model for the video QA task on this dataset.In summary, the key contribution is the introduction and analysis of the large-scale, natural, temporally compositional, and multimodal TVQA dataset to spur further research progress in video question answering.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper introduces TVQA, a new large-scale video question answering dataset based on 6 popular TV shows, containing over 150K human-written QA pairs from 21K video clips, designed to require compositional understanding of both video and subtitle content in order to answer questions.


## How does this paper compare to other research in the same field?

Here are a few key points in comparing this paper to other research in video question answering:- The paper introduces a new large-scale video QA dataset called TVQA, which is one of the largest to date with over 150K QA pairs from 21K clips spanning 460 hours of video. Other major video QA datasets are generally smaller in scale.- TVQA uses natural videos from 6 popular TV shows across 3 genres. Other datasets often use movies, cartoons, or less natural video sources. Using realistic videos is advantageous for developing algorithms that can work in the real world.- The questions in TVQA require understanding both the visual content in the videos as well as the dialogue subtitles. Many other datasets focus more on just visual or just textual reasoning. TVQA aims to be truly multimodal.- The paper proposes a compositional question format with a main question and a temporal grounding part. This encourages complex questions requiring visual localization. Other datasets tend to have simpler question formats.- The paper provides localization supervision with start/end timestamps for each question. Some other datasets like TGIF-QA also provide this, but many don't have temporal localization.- The paper gives benchmark results for baselines as well as a multi-stream neural network model. Many other papers introduce a new dataset but do not thoroughly evaluate strong baselines on it.Overall, the key distinguishing factors of this paper seem to be the scale, realism and multimodality of the TVQA dataset, as well as the effort to provide thorough analysis and baselines for the new dataset. The compositional questions are also an innovative component.
