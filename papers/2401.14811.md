# [On the Limitations of Markovian Rewards to Express Multi-Objective,   Risk-Sensitive, and Modal Tasks](https://arxiv.org/abs/2401.14811)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem Statement:
- The paper studies the expressivity of standard Markovian reward functions in reinforcement learning (RL) for incentivizing agent behavior. 
- It focuses on three broad classes of tasks that cannot be adequately expressed by scalar, Markovian rewards:
  1) Multi-objective RL (MORL) tasks with non-linear combinations of multiple rewards
  2) Risk-sensitive RL tasks modeled using non-affine utility functions 
  3) "Modal" RL tasks that depend on what transitions are possible from a state rather than what occurs.

Proposed Solutions:  
- Provides necessary and sufficient conditions for when a MORL problem can be reduced to scalar RL - only when the combination of rewards is a linear weighting. This means most interesting MORL tasks cannot be scalarized.
- Proves only affine transformations of the return function G are possible to express as Markovian rewards. So standard risk-averse utility functions like exponential, isoelastic, quadratic cannot be expressed.
- Introduces "modal" RL tasks and shows they typically cannot be formalized by Markovian rewards either. 

Main Contributions:
- Quantifies limitations in the semantic expressivity of Markovian reward functions.
- Shows MORL and risk-sensitive RL require fundamentally different solutions from standard RL.
- Highlights modal RL tasks as an interesting new direction.
- Discusses approaches for solving some tasks that cannot be represented by Markovian rewards.
- Clarifies implicit assumptions and applicability of common RL techniques.
- Motivates further research into extending expressivity of RL frameworks.

In summary, the paper provides both theoretical and practical insights about what standard scalar, Markovian reward functions can and cannot express. This contributes to a more nuanced understanding of the RL formalism.


## Summarize the paper in one sentence.

 This paper studies the expressivity of scalar, Markovian reward functions in reinforcement learning and identifies limitations in their ability to represent multi-objective, risk-sensitive, and modal tasks.


## What is the main contribution of this paper?

 This paper examines the expressivity of scalar, Markovian reward functions in reinforcement learning (RL) for capturing three broad classes of tasks:

1) Multi-objective RL tasks, where the aim is to learn a single policy that achieves an optimal trade-off amongst multiple reward signals. 

2) Risk-sensitive RL tasks, where the aim is to encourage risk-averse behavior by using concave utility functions of the rewards. 

3) Modal RL tasks, a newly introduced class, where the evaluation is based not only on the distribution of trajectories generated by a policy but also on what the agent could have done along those trajectories.

The main contributions are:

- Deriving necessary and sufficient conditions for when a single-policy multi-objective RL problem can be expressed with a scalar, Markovian reward function. It is shown this is only possible when the multi-objective trade-off corresponds to a linear weighting of the individual rewards.

- Proving that only affine transformations of the trajectory return function are possible to express with Markovian rewards, ruling out common concave utility functions used to model risk aversion. 

- Introducing modal RL tasks, arguing they are useful for robustness, and showing that most cannot be expressed by scalar, Markovian rewards.

In summary, the paper demonstrates fundamental limitations in the expressivity of Markovian reward functions for capturing a broad range of useful and intuitive tasks. This clarifies the assumptions behind standard RL techniques and shows where more specialized methods are necessary.


## What are the keywords or key terms associated with this paper?

 Based on my analysis, some of the key terms and concepts associated with this paper include:

- Multi-objective reinforcement learning (MORL): The paper examines single-policy MORL problems where the aim is to learn a single policy that achieves an optimal trade-off amongst multiple reward signals.

- Risk-sensitive reinforcement learning: The paper analyzes whether risk-averse objectives modeled using concave utility functions can be expressed as Markovian reward functions. 

- Modal tasks/problems: The paper introduces and defines a new class of "modal" RL tasks that depend not just on what distribution of trajectories an agent generates, but also on what it could have done along those trajectories.

- Markovian reward functions: The paper studies the expressivity and limitations of standard scalar, Markovian reward functions in RL for capturing different classes of tasks/problems like multi-objective, risk-sensitive, and modal ones.

- Policy orderings: The paper primarily defines a "task" as corresponding to an ordering over policies that encodes a preference relation. It examines if different classes of tasks can be expressed by Markovian rewards that induce the correct policy ordering.

- Affine transformations: One of the key results is that only affine transformations of the trajectory return function can be expressed as Markovian reward functions.

Some other relevant concepts are affordances, Probabilistic Computational Tree Logic (PCTL), value functions, constrained MDPs, etc. Let me know if you need any clarification or have additional questions!


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in this paper:

1. The paper defines "modal tasks" as tasks that evaluate agents not only based on the trajectories they generate, but also based on what they could have done along those trajectories. Can you provide some concrete examples of modal tasks that would be useful in practice? What kinds of capabilities would an agent need to perform well on these tasks?

2. The affordance-based MDP framework proposed for solving modal tasks allows the reward to depend on the optimal value functions of "affordance" MDPs. What restrictions need to be placed on these affordance MDPs and value functions to ensure the resulting modal task is still learnable? 

3. The affordance-based algorithm maintains separate Q-functions for the modal task and each affordance MDP. What are the advantages and disadvantages of this approach compared to using a single integrated Q-function? Under what conditions would one approach be preferred over the other?

4. The paper argues that using off-policy updates is crucial for the affordance Q-functions to ensure they converge to their true values. Why are on-policy updates insufficient here? And does the modal task Q-function also require off-policy updates?

5. What modifications could be made to the exploration strategy used by the affordance-based algorithm to ensure all relevant parts of the state space are sufficiently explored? How can we balance exploitation and exploration when part of the reward depends on learned value functions?

6. The affordance framework only allows the reward to depend on state features and optimal value functions. What are some examples of potentially useful dependencies that this excludes? How difficult would it be to extend the framework and algorithm to allow for things like dependence on optimal policies or temporal logic specifications?

7. The framework requires specifying a vector of value functions the reward can depend on. How should the number of affordances be chosen in practice? What problems can arise from having too many or too few affordances?

8. What types of function approximators would be most suitable for estimating the different value functions used by the algorithm? How should the different functions be represented to enable effective learning while keeping the optimization tractable?

9. The affordance Q-functions are updated separately using an off-policy method like Q-learning. What problems can arise when applying off-policy RL algorithms with function approximation? How should these be addressed?

10. For theoretical convergence guarantees, the paper cites a tabular RL algorithm result. What practical issues might arise when applying the affordance framework with nonlinear function approximators that could prevent convergence or reduce performance? How could the algorithm be revised to improve convergence?
