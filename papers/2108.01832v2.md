# [Offline Decentralized Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2108.01832v2)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading, the central research question addressed in this paper is:

How to enable agents to learn high-performing and coordinated policies from decentralized offline datasets in multi-agent reinforcement learning? 

Specifically, the paper proposes a framework to tackle two key challenges:

1) The transition dynamics experienced by each agent can change significantly during learning as other agents update their policies, causing a mismatch between the offline dataset and the real environment dynamics.

2) With decentralized learning on independently collected datasets, agents may have very different value estimates for the same states, leading to uncoordinated policies.

To address these issues, the paper introduces two techniques:

- Value deviation, which optimistically increases the transition probabilities of high-value next states to reduce the underestimation of potentially good actions. 

- Transition normalization, which normalizes the transition probabilities to build consensus among agents on state values and promote coordination.

By combining these two techniques, the proposed framework enables agents to learn policies that perform well and coordinate with each other, using only decentralized offline datasets.

In summary, the central hypothesis is that value deviation and transition normalization can reduce the negative impacts of transition bias and miscoordination in offline decentralized multi-agent reinforcement learning. The experiments aim to validate whether the proposed framework can improve performance over baselines in this challenging setting.


## What is the main contribution of this paper?

 The main contribution of this paper is proposing a framework for offline decentralized multi-agent reinforcement learning. The key ideas are:

1. Introducing value deviation to optimistically increase the transition probabilities of high-value next states. This helps mitigate the underestimation of potential good actions that are hidden due to poor behavior policies of other agents. 

2. Introducing transition normalization to normalize the transition probabilities and make agents have similar estimates of state values. This helps improve coordination among agents.

3. Theoretically proving the convergence of Q-learning under the non-stationary transition dynamics introduced by value deviation and transition normalization.

4. Empirically showing that the framework can be easily integrated with existing offline RL algorithms like BCQ, CQL, and TD3+BC. Experiments demonstrate substantial improvements over baselines in a variety of multi-agent tasks.

In summary, this is the first work to address the challenging problem of offline decentralized multi-agent RL. It proposes a general framework to handle the mismatches between offline dataset and online execution through value deviation and transition normalization. Theoretically and empirically it shows the effectiveness of the framework.
