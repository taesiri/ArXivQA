# [Precise Zero-Shot Dense Retrieval without Relevance Labels](https://arxiv.org/abs/2212.10496)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the abstract and introduction, the central hypothesis of this paper seems to be:Dense retrieval models require large amounts of relevance labeled data for training, which may not always be available. The authors propose an alternative approach called HyDE that does not require relevance labels and can work in a zero-shot setting by utilizing a generative language model and unsupervised contrastive document encoder. Specifically, the key hypothesis appears to be that decomposing dense retrieval into a generative task (generating hypothetical relevant documents with an LM) and a similarity task (encoding documents with a contrastive encoder) can allow effective zero-shot retrieval without needing relevance supervision. The generative LM can capture relevance patterns by generating example documents, while the contrastive encoder maps documents into a shared space where similarity indicates relevance.So in summary, the central hypothesis is that the proposed HyDE approach can enable precise zero-shot dense retrieval without requiring relevance labels, by splitting the task between a generative LM and unsupervised contrastive encoder. The authors aim to show this works well across diverse tasks and languages.


## What is the main contribution of this paper?

Based on my reading of the paper abstract, the main contribution appears to be proposing a new method called HyDE (Hypothetical Document Embeddings) for zero-shot dense retrieval that does not require any relevance labels or supervision. Specifically, the key ideas of HyDE seem to be:- It uses an instruction-following language model (e.g. InstructGPT) to generate a "hypothetical" document that captures relevance patterns for a given query, even though this document is not real and may contain false details. - It then encodes this hypothetical document using an unsupervised contrastively learned encoder (e.g. Contriever) to get a document embedding vector. The encoder serves as a "lossy compressor" to filter out incorrect details. - This document vector is then used to retrieve the most similar real documents from the corpus based on vector similarity, leveraging the document-document similarities learned by the encoder.So in summary, the main contribution appears to be proposing this method to decompose dense retrieval into a generative relevance modeling task and a document similarity encoding task, avoiding the need for explicit relevance supervision. The results show HyDE can significantly outperform previous unsupervised methods like Contriever across various tasks and languages.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key point from the paper: The paper proposes a new zero-shot dense retrieval method called HyDE that pivots through hypothetical document embeddings generated by an instruction-following language model to enable effective retrieval without any relevance labels.


## How does this paper compare to other research in the same field?

Here are the key points on how this paper compares to other related work in zero-shot dense retrieval:- This paper tackles the challenging problem of fully zero-shot dense retrieval where no relevance labels are available for training. Most prior work assumes access to some labeled data like MS-MARCO for training. - The proposed HyDE method is completely unsupervised, without fine-tuning or training any models. It uses an instruction-following LM like GPT-3 and a contrastive encoder like Contriever. Both models are used out-of-the-box without modification.- HyDE pivots through hypothetical documents generated by the LM to create query embeddings. It offloads relevance modeling to the powerful generative capabilities of LLMs. This is a novel way of approaching zero-shot retrieval.- Other methods like TART require fine-tuning the encoders with task instructions. HyDE keeps the encoder fixed and handles instructions only through the LM.- HyDE performs retrieval purely based on document similarity without directly modeling query-document scores. The generation and encoding steps essentially cast retrieval as NLU and NLG tasks.- HyDE doesn't require any custom training, indices or search algorithms. It can leverage standard dense retrieval pipelines based on MIPS.- In contrast, generative retrieval methods like DSI and SEAL require training generative models on relevance data and may need custom search indices.- Experiments show HyDE significantly outperforms previous state-of-the-art like Contriever on diverse tasks and languages. It is competitive with supervised models.In summary, HyDE proposes a novel unsupervised dense retrieval approach using LLMs and contrastive learning. It demonstrates strong zero-shot performance without requiring relevance labels or model training.
