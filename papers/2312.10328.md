# [Deriving Rewards for Reinforcement Learning from Symbolic Behaviour   Descriptions of Bipedal Walking](https://arxiv.org/abs/2312.10328)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Generating complex physical movements like bipedal walking from high-level symbolic descriptions is challenging in robotics and AI. It requires combining numerical optimization techniques like reinforcement learning (RL) with symbolic representations.
- Defining good reward functions for RL that capture the desired behaviour is difficult and most approaches rely on trial-and-error. 

Proposed Solution:
- Propose a novel approach to systematically derive a reward function from a symbolic description of the desired walking gait. 
- Model the intended walking gait as a hybrid automaton which specifies a sequence of different configurations (state-space orthants) the walker should transition through.
- Use this hybrid automaton model to define an "orthant reward" that incentivizes the walker to follow the specified state sequence.

Experiments and Results:
- Apply approach on a simulated compass gait walker using PPO RL algorithm.
- Compare orthant reward to sparse and velocity-based rewards.
- Orthant reward leads to faster training times and higher final walking speeds than other rewards.
- Combining orthant reward with a velocity reward works best, reducing training times and increasing walking distance.

Main Contributions:
- A principled methodology to derive reward functions from symbolic behaviour descriptions using hybrid automata.
- Introduction of a novel "orthant reward" for bipedal walking based on hybrid automaton model.
- Demonstrating faster training and better performance using orthant reward on simulated compass walker.
- Proposed technique serves as a blueprint for translating symbolic descriptions into rewards for RL.

The key novelty is the use of hybrid automata to model symbolically described behaviours and systematically transform them into shaped rewards for RL. Experiments show this automata-based reward shaping can improve learning of complex physical behaviours like bipedal walking.


## Summarize the paper in one sentence.

 This paper proposes a novel approach to derive reward functions for reinforcement learning of bipedal walking from symbolic behavior descriptions, by modeling the intended system behavior as a hybrid automaton over state space orthants.


## What is the main contribution of this paper?

 According to the paper, the main contribution is:

A principled way to derive reward functions from symbolic descriptions of desired behavior. Specifically, the paper proposes translating an informal description of desired behavior into a formal model using hybrid automata. This formal model reduces the search space for reinforcement learning by restricting it to desired state sequences. The reduced search space allows formulating an improved reward function to guide the learning process.

The paper demonstrates this approach on bipedal walking of a compass gait model. A symbolic description of the different walking phases is translated into a hybrid automaton over state space orthants. This automaton is then used to define a reward term that incentivizes the compass walker to follow the specified gait cycle. Experiments show this speeds up learning and increases the resulting walking speed over baseline rewards.

In summary, the main contribution is a systematic methodology to transform symbolic behavior descriptions into improved reward functions for reinforcement learning agents. The simplicity of the approach should make it applicable to other problems where defining good rewards is difficult.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and keywords associated with it are:

- Reinforcement learning
- Reward function design
- Bipedal walking
- Compass gait walker
- Hybrid automata
- Symbolic behavior descriptions
- Orthant rewards
- Reduced state space
- Faster training
- Improved walking speed

The paper proposes a novel approach to deriving reward functions for reinforcement learning from symbolic descriptions of desired behaviors, and applies this method to bipedal walking of a compass gait walker robot. Key elements include modeling the walking behavior as transitions between orthants of the state space using a hybrid automaton formalism, and defining an orthant reward to incentivize following this formal model. This is shown to reduce the search space and lead to faster training and improved walking speed compared to baseline rewards.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in this paper:

1. The paper proposes formalizing the gait cycle as a hybrid automaton over state space orthants. What are the key benefits of using a hybrid automaton formulation compared to other formal methods? How does it help in reducing the state space?

2. The reward function formulation uses the hybrid automaton structure to provide rewards. Explain the intuition behind the orthant reward term and how it helps guide the policy search. What are other potential ways the hybrid automaton could be utilized in defining reward functions?

3. The method is applied to a compass gait walker model. What are the assumptions made in this model and what would be required to apply this approach to more complex bipedal systems? How can the increased complexity be handled?

4. The paper argues this is a principled way to translate symbolic descriptions to numerical rewards. What are the limitations of current inverse reinforcement learning methods that this approach addresses? Whatsymbolic aspects can be captured through the hybrid automaton formulation?  

5. The combined reward function of orthant and forward velocity terms performs the best. Analyze why the combination works better compared to each individual term. Does this provide any insight into designing good reward functions in general?

6. What safety guarantees can be provided by using the hybrid automaton formalization that may not be possible with learned policies otherwise? How can the safety verification be performed based on this model?

7. The method is model-based in that the hybrid automaton structure needs to be defined apriori. How can this be extended to generate such formal structures automatically from observations or demonstrations?

8. The hybrid automaton here models an ideal or desired behavior. How can this approach handle deviations or recoveries from perturbations during policy execution? Would modeling error states help?

9. The evaluation is performed in simulation on a compass gait walker. What challenges need to be addressed before deploying such learned policies on real hardware? Would sim-to-real transfer methods help?

10. The paper discusses potential extensions like curriculum learning and combination with inverse RL. Explain how these can build on the core idea presented. What other extensions or enhancements do you envision for this approach?
