# [Deriving Rewards for Reinforcement Learning from Symbolic Behaviour   Descriptions of Bipedal Walking](https://arxiv.org/abs/2312.10328)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Generating complex physical movements like bipedal walking from high-level symbolic descriptions is challenging in robotics and AI. It requires combining numerical optimization techniques like reinforcement learning (RL) with symbolic representations.
- Defining good reward functions for RL that capture the desired behaviour is difficult and most approaches rely on trial-and-error. 

Proposed Solution:
- Propose a novel approach to systematically derive a reward function from a symbolic description of the desired walking gait. 
- Model the intended walking gait as a hybrid automaton which specifies a sequence of different configurations (state-space orthants) the walker should transition through.
- Use this hybrid automaton model to define an "orthant reward" that incentivizes the walker to follow the specified state sequence.

Experiments and Results:
- Apply approach on a simulated compass gait walker using PPO RL algorithm.
- Compare orthant reward to sparse and velocity-based rewards.
- Orthant reward leads to faster training times and higher final walking speeds than other rewards.
- Combining orthant reward with a velocity reward works best, reducing training times and increasing walking distance.

Main Contributions:
- A principled methodology to derive reward functions from symbolic behaviour descriptions using hybrid automata.
- Introduction of a novel "orthant reward" for bipedal walking based on hybrid automaton model.
- Demonstrating faster training and better performance using orthant reward on simulated compass walker.
- Proposed technique serves as a blueprint for translating symbolic descriptions into rewards for RL.

The key novelty is the use of hybrid automata to model symbolically described behaviours and systematically transform them into shaped rewards for RL. Experiments show this automata-based reward shaping can improve learning of complex physical behaviours like bipedal walking.
