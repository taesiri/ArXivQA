# [RC-DARTS: Resource Constrained Differentiable Architecture Search](https://arxiv.org/abs/1912.12814)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is how to efficiently perform neural architecture search under resource constraints. Specifically, the paper aims to develop a method that can automatically search for neural network architectures that maximize accuracy while satisfying constraints on model size, computational complexity, and/or inference speed. The key hypotheses are:- Neural architecture search can be formulated as a constrained optimization problem with accuracy as the objective and resource usage as constraints.- By using a continuous relaxation of the architecture representation along with an iterative projection method, this constrained optimization can be solved efficiently using gradient descent techniques. - Applying this approach with a multi-level search strategy and specialized connection cells results in neural architectures that achieve state-of-the-art trade-offs between accuracy and resource usage.In summary, the central research question is how to perform efficient automated neural architecture search with resource constraints, and the key hypothesis is that by formulating it as a constrained optimization problem and using continuous relaxation and iterative projection, an efficient and effective solution can be developed. The experiments aim to validate that this approach can discover architectures that achieve accuracies comparable to or exceeding state-of-the-art hand-designed and automatically searched networks while satisfying constraints on model size, FLOPs, etc.


## What is the main contribution of this paper?

The main contributions of this paper are:1. It proposes RC-DARTS, a novel framework for one-shot neural architecture search under resource constraints. RC-DARTS aims to learn customized network architectures that maximize accuracy while satisfying user-defined constraints on model size, FLOPs, etc.2. It formulates the architecture search as a constrained optimization problem and proposes an iterative projection method to efficiently solve it. This allows joint optimization of architecture parameters and weights through gradient descent while satisfying the resource constraints. 3. It introduces a multi-level search strategy to learn different architectures for different layers based on their effects on resource costs and accuracy. It also learns a new connection cell to better tradeoff between accuracy and costs.4. Experiments on CIFAR-10 and ImageNet show RC-DARTS achieves state-of-the-art accuracy under similar resource constraints. It learns lightweight models using 3 orders of magnitude less computation than prior NAS methods.In summary, the key contribution is a one-shot NAS method that can efficiently learn customized architectures under user-defined resource constraints. This is useful for applications where models need to be tailored for specific hardware platforms.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper proposes a novel neural architecture search framework called RC-DARTS that efficiently learns customized network architectures under user-defined resource constraints by formulating the task as a constrained optimization problem and using an iterative projection method along with a multi-level search strategy.


## How does this paper compare to other research in the same field?

Here are a few key points on how this paper compares to other research in the field of neural architecture search:- The paper focuses on efficient neural architecture search under resource constraints, which is an important problem for real-world applications with limited compute budgets. Many prior NAS methods do not explicitly incorporate resource constraints during the search process.- The proposed RC-DARTS method builds upon the DARTS algorithm by formulating the search as a constrained optimization problem. This allows resource costs like FLOPs and parameters to be directly optimized during the architecture search.- RC-DARTS utilizes an iterative projection method to solve the constrained optimization problem. This is a novel approach compared to prior NAS methods. The iterative projection enables efficient search while satisfying the resource constraints.- The paper introduces a multi-level search strategy, which is a new technique allowing different architectures to be learned for layers at different depths in the network. This helps balance accuracy and resource costs across the layers.- Experiments show RC-DARTS can find compact architectures with comparable or better accuracy than prior NAS methods, while using significantly less computation for the search process. For example, RC-DARTS used 1 GPU day while NASNet used 1800 GPU days.- RC-DARTS achieves strong results compared to prior work on compact architecture search like DPP-Net and MnasNet. It demonstrates better accuracy-efficiency trade-offs.Overall, the paper presents innovations in formulating NAS as a constrained optimization problem, the iterative projection solution, and multi-level search. This allows more efficient search for architectures specialized to a target resource budget. The empirical results validate RC-DARTS can discover accurate and efficient models.


## What future research directions do the authors suggest?

Based on my reading of the paper, some of the main future research directions suggested by the authors are:- Exploring using a neural network to learn the mapping from network architecture hyperparameters to a more complex resource cost, such as inference speed on specific hardware platforms. The current method uses a simple continuous approximation for resource costs like FLOPs and parameters. Learning a mapping could allow optimizing for more complex platform-specific costs.- Applying the RC-DARTS framework to additional domains beyond image classification, such as object detection, segmentation, etc. The authors demonstrate RC-DARTS for image classification, but suggest it could be useful for other vision tasks.- Incorporating more advanced architectural components into the RC-DARTS search space, such as squeeze-and-excitation blocks, inverted residuals, etc. The authors note the candidate operations used currently are separate from the proposed method, so more advanced operations could be integrated.- Exploring different formulations of the resource constraints, such as using different lower/upper bounds or adding penalties to the loss. The current constraints are simple lower/upper bounds, but other constraint formulations could be investigated.- Applying RC-DARTS to settings with multiple constraints simultaneously, beyond just FLOPs and parameters. The authors formulate it for multiple constraints but experiment with two, so validating with more complex constraints is suggested.- Investigating warm-start and transfer learning techniques to further improve the efficiency of the search process. The authors use a simple warm start technique but suggest more advanced methods could help.In summary, the main directions are enhancing the modeling of resource costs, applying RC-DARTS to new domains/tasks, integrating new operations, exploring constraint formulations, using more constraints, and improving search efficiency. The core RC-DARTS method provides a strong foundation for future work in these areas.


## Summarize the paper in one paragraph.

This paper proposes RC-DARTS, a resource constrained differentiable architecture search method for one-shot NAS under resource constraints. RC-DARTS builds on DARTS by formulating the problem as a constrained optimization problem with resource constraints on model size and FLOPs. An iterative projection method is proposed to solve the constrained optimization problem. A multi-level search strategy is also introduced to adaptively learn architectures for different layers. Experiments on CIFAR-10 and ImageNet show RC-DARTS achieves state-of-the-art accuracy under similar resource constraints compared to other NAS methods. The key novelty is formulating NAS as a constrained optimization problem to directly learn architectures satisfying target resource constraints.


## Summarize the paper in two paragraphs.

Here is a two paragraph summary of the paper:This paper proposes RC-DARTS, a neural architecture search method that learns architectures under user-defined resource constraints such as FLOPs and model size. The method builds upon DARTS by formulating the problem as a constrained optimization problem with the resource constraints added. An iterative projection algorithm is proposed to solve this constrained optimization problem. Specifically, it alternates between an unconstrained optimization step to maximize validation accuracy and a projection step to satisfy the resource constraints. The method also employs a multi-level search strategy, where architectures are learned separately for layers at different depths in the network. This allows the method to find pareto-optimal architectures across all layers. Experiments on CIFAR-10 and ImageNet demonstrate the effectiveness of RC-DARTS. It achieves state-of-the-art accuracy under similar resource constraints compared to prior NAS methods. The learned architectures have smaller model size and FLOPs than DARTS while achieving comparable or better accuracy. The results show RC-DARTS can automatically learn lightweight architectures suitable for resource constrained applications like mobile platforms. The method only requires 1 GPU day for architecture search, allowing efficient one-shot NAS under constraints.
