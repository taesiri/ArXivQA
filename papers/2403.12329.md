# [FedFisher: Leveraging Fisher Information for One-Shot Federated Learning](https://arxiv.org/abs/2403.12329)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Federated learning aims to train a global model across multiple decentralized devices/clients, while keeping local data private on each device. However most existing federated algorithms (like FedAvg) require multiple rounds of communication back and forth between the server and clients, which can be prohibitive in many real world settings.

- This paper addresses the problem of one-shot federated learning where we want to aggregate local models trained only once at each client, to generate a high quality global model. However most existing one-shot algorithms suffer from issues like poor performance and privacy leakage.

Proposed Solution: 
- The paper proposes two new algorithms called FedFisher and FedFisher-K that leverage ideas from Fisher information approaches, in order to combine local models in a one-shot manner. 

- Specifically, the key idea is to view the problem of combining local models as finding the mode of the global posterior distribution over models. Then under a mean field approximation, they show this is equivalent to minimizing the sum of squared `Fisher distances' between the global model and each local model.

- Based on this insight, FedFisher uses local Fisher information to appropriately weight different local models during aggregation. FedFisher-K uses local Kronecker factored approximations of the Fisher information.

- The paper provides theoretical analysis to show that under certain assumptions, FedFisher variants can recover the global model almost as well as algorithms that train for multiple rounds.

Main Contributions:
- Proposes a new perspective for one-shot federated learning based on finding the mode of the global posterior distribution. This leads to the use of Fisher information for model aggregation.

- Introduces two practical algorithms FedFisher and FedFisher-K that operationalize these ideas and show strong empirical performance compared to existing baselines across multiple datasets.

- Provides generalization bounds for FedFisher that match existing results for centralized learning up to an additive factor accounting for client heterogeneity.

- Demonstrates improved privacy with FedFisher-K compared to other one-shot algorithms via an inversion attack experiment.
