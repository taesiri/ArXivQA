# [Chimera: A Lossless Decoding Method for Accelerating Large Language   Models Inference by Fusing all Tokens](https://arxiv.org/abs/2402.15758)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
Large language models (LLMs) have shown impressive capabilities on various tasks. However, their widespread deployment is hindered by the inefficient auto-regressive decoding process. Existing methods like parallel decoding heads provide acceleration but suffer from low accuracy compared to auto-regressive decoding. 

Proposed Solution: 
The paper proposes Chimera, a novel framework for speculative decoding to accelerate LLM inference while maintaining accuracy. The key ideas are:

1) Lightweight Draft Model: Predicts multiple future tokens efficiently by capturing short-range dependencies using a trigram encoder, and long-range dependencies by distilling knowledge from the original LLM's representations.

2) Residual Decoding Heads: Generate tokens at different future positions. They utilize both the lightweight draft model's and original LLM's representations to improve accuracy.

Main Contributions:

- Proposes lightweight yet accurate draft model for speculative decoding by distilling knowledge from original LLM.

- Achieves 2.7x speedup on average over auto-regressive decoding on Vicuna and LlaMA models with negligible accuracy drop.

- Analyzes the inference time bottleneck, indicating verification takes up increasing proportion at higher model size.

- Performs comprehensive analysis - evaluates on multiple datasets, decoding methods and model variations. Demonstrates consistent and significant acceleration.

In summary, the paper presents Chimera that can accelerate LLMs by 2.7x, while maintaining high accuracy. The main ideas are capturing dependencies effectively in the draft model, and distilling knowledge from the original LLM.
