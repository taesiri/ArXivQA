# CLEVR-Dialog: A Diagnostic Dataset for Multi-Round Reasoning in Visual   Dialog

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the main research question this paper seems to address is:How can we construct a large-scale, fully annotated, diagnostic dataset for studying multi-round reasoning and visual coreference resolution in visually-grounded dialog?The key aspects related to this research question are:- Existing large-scale visual dialog datasets (like VisDial) are collected on real images, making it hard to extensively annotate the state of images/dialogs and study sub-tasks like coreference resolution in isolation.- The authors develop a new dataset called CLEVR-Dialog by combining:(a) CLEVR rendered images which have full scene graph annotations by construction. (b) A dialog grammar grounded in these scene graphs to generate multi-round dialogs.- This combination results in exhaustively annotated dialogs where the dialog state is fully known. This enables targeted analysis like evaluating visual coreference resolution as a function of coreference distance.- The authors benchmark existing models on CLEVR-Dialog and perform novel analysis like assessing textual/visual grounding of the best model. Such analysis is only possible on their fully annotated diagnostic dataset.In summary, the key research contribution is the construction of CLEVR-Dialog to enable in-depth analysis and diagnosis of visuo-linguistic reasoning in visual dialog, which is not possible on large real datasets due to lack of annotations.


## What is the main contribution of this paper?

Based on the abstract and introduction, the main contributions of this paper appear to be:1. The development of CLEVR-Dialog, a new diagnostic dataset for studying multi-round reasoning in visually-grounded dialog. The key features of CLEVR-Dialog are:- It is built by combining CLEVR images with exhaustive scene graph annotations with a dialog grammar grounded in those scene graphs. - It contains over 4 million fully-annotated question-answer pairs in 425K dialogs on 85K images.- The dialogs are designed to require temporal/multi-round reasoning over the conversation history. 2. Using CLEVR-Dialog to analyze visual dialog models, especially on the challenging task of visual coreference resolution. The authors show a breakdown of model performance based on coreference distance and question type. 3. Providing strong baselines on CLEVR-Dialog by benchmarking several existing visual dialog models. The best performing model (CorefNMN) gets 68% accuracy.4. Demonstrating diagnostic analyses enabled by the full annotations in CLEVR-Dialog, including both qualitative visualization and quantitative evaluation of textual/visual grounding for the CorefNMN model.In summary, the key contribution appears to be the new diagnostic CLEVR-Dialog dataset that allows in-depth analysis of reasoning and grounding in visual dialog models. The benchmarking of models and case studies demonstrate its utility.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper introduces a new synthetic visual dialog dataset called CLEVR-Dialog that is generated from scene graphs and is fully annotated, allowing detailed analysis of multi-round reasoning and coreference resolution by dialog agents.
