# [Africa-Centric Self-Supervised Pre-Training for Multilingual Speech   Representation in a Sub-Saharan Context](https://arxiv.org/abs/2404.02000)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Popular self-supervised learning (SSL) models for speech are mainly pre-trained on few high-resource languages, causing over-representation of those languages. 
- African languages have unique typological characteristics and are under-resourced, hence severely affected by this imbalance.

Proposed Solution:
- Present the first SSL speech model trained exclusively on African languages to produce better representations for downstream tasks in African languages.
- Collected ~60K hours of speech in 21 languages/dialects from broadcast news and street interviews in Sub-Saharan Africa.
- Employed HuBERT base (90M parameters) as the pre-training model with two iterations of self-supervised pre-training using the African speech dataset.
- Evaluated on Sub-Saharan African (SSA) subset of the FLEURS-102 benchmark containing 20 languages in two scenarios:
   - Fine-tuned the pre-trained model for speech recognition. 
   - Fine-tuned for language identification (LID).

Main Contributions:
- First open-source SSL speech model pre-trained solely on Sub-Saharan African speech, showing effectiveness for African languages.
- Achieves competitive speech recognition performance to FLEURS top model while using 7x less pre-training data and 6x less parameters.
- Outperforms FLEURS baselines by over 22% in accuracy on LID task, demonstrating strong language representations.
- Takes a step towards more cost-effective and robust pre-trained models specialized for African languages compared to large generic models.
