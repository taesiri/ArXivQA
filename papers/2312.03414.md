# [Compressed Context Memory For Online Language Model Interaction](https://arxiv.org/abs/2312.03414)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a high-quality paragraph summarizing the key points of the paper:

This paper proposes a novel context compression method called Compressed Context Memory (CCM) for efficient online inference of Transformer language models. As context continually accumulates during online interactions, CCM compresses new context into a compact memory space using conditional LoRA adapters integrated into the model. This compressed memory is then utilized for efficient inference, reducing memory and computation costs. CCM employs parallelizable training strategies involving memory updates and attention masking. Experiments across conversation, personalization, and multi-task scenarios demonstrate CCM's superior performance over fixed-context compression methods. With only 1/5 the context memory, CCM achieves equivalent accuracy to full context models. Further analyses highlight CCM's memory-efficiency benefits, including 13x higher throughput on memory-constrained GPUs. Overall, CCM enables language models to continually process expanding contexts during online inference with minimal overhead.
