# [Towards Faithful and Robust LLM Specialists for Evidence-Based   Question-Answering](https://arxiv.org/abs/2402.08277)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Large language models (LLMs) like GPT-3 still struggle with evidence-based question answering, where they need to answer questions by citing relevant sources. Specifically, existing LLMs have issues with citing the correct sources (source quality) and truthfully representing the information from the sources (answer attributability).

- Open-sourced LLMs perform even worse than commercial LLMs on evidence-based QA. So there is a need for methods to improve the performance of open-sourced LLMs. 

- Prior work on instruction tuning of LLMs using synthesized data faces challenges with data quality and model overfitting.

Proposed Solution:
- The authors propose a pipeline to synthesize a diversified evidence-based QA dataset called SynSciQA at scale. The pipeline also has filters to refine the data and ensure higher quality, leading to SynSciQA+ and SynSciQA++.

- They collect 4 test sets with different distances to the training data to benchmark in-distribution and out-of-distribution (OOD) performance.

- They fine-tune open-sourced LLMs like LLama and Zephyr on SynSciQA datasets and evaluate on the test sets.

Main Contributions:
- Show data quality matters more than quantity for evidence-based QA fine-tuning. The quality filters lead to better performance.

- Demonstrate synthetic data fine-tuning reliably improves both in-distribution and OOD performance on evidence-based QA.

- Find that overfitting exists during fine-tuning. Performance on synthetic dev set strongly correlates with OOD test performance.

- Propose reproducibility benchmarks, metrics and analyses to advance research on more traceable QA with open-sourced LLMs.
