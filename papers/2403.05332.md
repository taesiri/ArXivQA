# [Degradation Resilient LiDAR-Radar-Inertial Odometry](https://arxiv.org/abs/2403.05332)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
Enabling autonomous robots to operate robustly in challenging environments with degraded perception is an important capability. Key issues include handling geometrically self-similar environments where LiDAR-based methods struggle, as well as environments with visual obscurants like fog or dust that severely limit LiDAR range and quality. Robust odometry is crucial to avoid failures.  

Proposed Solution:
The paper proposes a tightly-coupled sensor fusion approach called "Degradation Resilient LiDAR-Radar-Inertial Odometry (DR-LRIO)" that leverages the complementary strengths of LiDAR, radar and IMU. The key ideas are:

1) Use a factor graph optimization over a sliding window to fuse IMU, LiDAR and radar. Custom factor formulations are tailored to each modality.

2) For LiDAR, add individual feature factors instead of a single 6DOF transform. This conveys non-degenerate information even in degraded conditions.

3) For radar, estimate 3D velocity using RANSAC on Doppler measurements and add a velocity factor to the graph. This aids velocity estimation and constraints drift.

4) In fog, add more factors for ground plane features visible to LiDAR to improve altitude and yaw drift.

Main Contributions:

1) Novel factor graph architecture and formulations to tightly fuse LiDAR, radar and IMU with resilience to degradation.

2) LiDAR feature factors that preserve non-degenerate information when LiDAR is impaired. 

3) Radar velocity factor that improves velocity estimation and reduces drift.

4) Evaluation in challenging real-world environments on an aerial robot: (a) geometrically self-similar tunnel (b) fog-filled hallway. Proposed method outperforms LiDAR-only and radar-only baselines.

In summary, the paper offers a complete system for multi-modal odometry that is resilient to degraded perception by leveraging the synergy between LiDAR, radar and IMU through a tightly-coupled estimation framework. Both algorithmic and experimental contributions demonstrate the capability.
