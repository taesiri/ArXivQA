# [Accurate LoRA-Finetuning Quantization of LLMs via Information Retention](https://arxiv.org/abs/2402.05445)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem: 
- Large language models (LLMs) have excellent performance but very high computational/memory requirements, making deployment difficult. 
- Quantization can compress LLMs to lower bit widths but causes significant accuracy drops.
- Existing LoRA finetuning quantization methods still result in suboptimal accuracy for quantized LLMs. Main issues are:
  - Quantization causes immense information loss and representation capability degradation.
  - Finetunable LoRA has limited representation capacity to recover lost information.

Proposed Solution - IR-QLoRA:
- Information Calibration Quantization (ICQ): Calibrates quantizers to maximize information entropy, retaining maximal mutual information between original and quantized weights. This enhances representation capability.
- Information Elastic Connection (IEC): Introduces parameter-free connections to diversify LoRA's transformation ability, giving access to original representation information. This expands LoRA's representation capacity.

Main Contributions:
- Proposes ICQ to calibrate quantizers via entropy maximization to retain information and enhance representation ability of quantized weights.
- Proposes IEC to construct flexible LoRA connections for utilizing original representation information, expanding adaptation ability.
- Achieves state-of-the-art accuracy for quantized LLMs with LoRA across LLaMA and LLaMA2 families, especially at very low 2-4 bit quantization. 
- Demonstrates excellent versatility - can flexibly boost existing LoRA finetuning quantization frameworks for LLMs.
- Provides substantial accuracy gains with minimal overhead in parameters and training time.

In summary, the paper makes significant advances in accurate and efficient LoRA finetuning quantization for LLMs via an information retention perspective. The IR-QLoRA framework with ICQ and IEC consistently pushes performance limits across models, bit widths and frameworks.
