# [Investigating the Robustness of Vision Transformers against Label Noise   in Medical Image Classification](https://arxiv.org/abs/2402.16734)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Label noise in medical image datasets hampers supervised deep learning methods and hurts model generalizability and test performance. 
- Recent works have focused on making models robust to label noise using CNN backbones, but transformers (ViTs) have not been explored.

Approach:
- Investigate robustness of ViTs against label noise in medical image classification and compare to CNNs. 
- Use two medical datasets - COVID-DU-Ex (X-ray images) and NCT-CRC-HE-100K (histopathology) - injected with varying label noise rates.
- Train ViT and CNN (ResNet18) with standard cross-entropy loss and compare performance. Also test with Co-teaching method.
- Additionally, use two self-supervised pretraining methods (MAE and SimMIM) on ViTs before supervised noisy training.

Key Findings:
- Without pretraining, ViTs are more prone to overfitting and perform worse than CNNs, especially for larger ViT models.
- Self-supervised pretraining boosts ViTs' robustness against label noise. Pretraining is crucial for ViTs to work effectively with label noise.
- When used in the Co-teaching method, pretrained ViTs significantly outperform both CNNs and non-pretrained ViTs.

Main Contributions:
- First work to systematically analyze transformer architectures' (ViTs) robustness against label noise in medical image classification.
- Demonstrate importance of self-supervised pretraining for ViTs to handle label noise effectively and achieve better performance than CNNs.
- Establish strong baseline for using ViTs in learning with noisy labels for medical imaging tasks.

In summary, the key insight is that ViTs can be highly robust against label noise if properly pretrained in a self-supervised manner before supervised noisy training.
