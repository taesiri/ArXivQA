# [Empower Large Language Model to Perform Better on Industrial   Domain-Specific Question Answering](https://arxiv.org/abs/2305.11541)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, it seems the central research question or hypothesis is:How can we augment large language models (LLMs) with domain-specific knowledge to improve their performance on domain-specific question answering tasks? The authors propose that existing methods to enhance LLMs using retrieval or external modules have limitations. Their hypothesis is that a new model interaction paradigm where a small LLM provides generated domain knowledge to a larger LLM can be more effective. To test this, they construct a new cloud domain QA dataset (MSQA) since there is limited availability of domain-specific QA benchmarks. Using this dataset, they show their proposed model interaction approach outperforms retrieval methods in providing accurate, domain-knowledge-enriched answers.In summary, the key research question is how to effectively equip LLMs with relevant domain knowledge. The authors hypothesize and demonstrate that having a small LLM provide generated domain knowledge can achieve this better than current methods. Evaluating this on the new domain QA dataset MSQA is central to testing their approach.


## What is the main contribution of this paper?

Based on my review, the main contributions of this paper appear to be:1. The construction of a new cloud domain QA dataset called MSQA, which contains over 23k question-answering pairs from the Microsoft Q&A forum. This seems to be the first dataset focused specifically on the cloud domain.2. A proposed model interaction paradigm that involves fine-tuning a small language model on domain documentation to align it with domain knowledge, and using this model at runtime to provide domain-specific knowledge to larger LLMs to improve their performance on domain tasks. 3. Evaluation of the proposed model interaction approach on the new MSQA dataset, showing it can outperform commonly used retrieval-based methods for incorporating domain knowledge.4. Introduction of new metrics tailored for evaluating long-length answers in domain-specific QA.So in summary, the key contributions seem to be the new cloud QA dataset, the proposed model interaction paradigm to infuse LLMs with domain knowledge, evaluation of this approach on the dataset, and new domain QA evaluation metrics. The model interaction approach appears to be the core novel method proposed and evaluated in the paper.
