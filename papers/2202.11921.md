# [Auto-scaling Vision Transformers without Training](https://arxiv.org/abs/2202.11921)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question it tries to address is: How to automate the design and scaling of Vision Transformer (ViT) architectures in an efficient and principled manner, while also reducing the heavy computational cost of training ViTs?In particular, the key research questions/challenges the paper tries to tackle are:1) How to efficiently and automatically design a good "seed" ViT topology without relying on tedious manual design or blind architecture search? 2) How to scale up the widths and depths of a ViT architecture in a principled way to generate model variants of different sizes?3) How to reduce the tremendous computational cost and data required for training ViTs compared to CNNs?To summarize, the central focus of the paper is developing an efficient automated framework called As-ViT to address the lack of principled design and scaling methods for ViTs, while also making ViT training more efficient. The key ideas proposed include:- Leveraging network complexity metrics like expected length distortion for fast, training-free design of a promising "seed" ViT topology.- Using the same complexity metrics to automatically scale up the seed topology into wider and deeper variants. - Introducing progressive elastic re-tokenization during training to reduce computational cost by over 50% while maintaining accuracy.So in essence, the paper aims to tackle the automated, efficient design and scaling of ViTs along with reducing the high training cost, which are important open problems for the ViT architecture.


## What is the main contribution of this paper?

Based on my reading of the paper, here is a summary of the main contribution:The paper proposes As-ViT, an automated framework for designing and scaling up Vision Transformer (ViT) architectures in an efficient and principled manner, without tedious manual effort. Specifically, the key contributions are:1) It automates both the design of a "seed" ViT topology and the scaling of its depth and width to generate ViT variants of different sizes. This is done in a very efficient manner via a training-free search approach guided by network complexity metrics. 2) It provides the first comprehensive study of different complexity metrics for ViTs, finding that expected length distortion provides the best trade-off between computation cost and correlation to actual performance.3) It enables efficient training of ViTs via a progressive re-tokenization scheme that gradually reduces the number of tokens in early training stages to save computation.4) The overall framework achieves strong performance on ImageNet classification and COCO detection without any manual architecture crafting. The entire automated design and scaling process takes only around 12 GPU hours.In summary, the key innovation is the proposal of an end-to-end framework that can automatically design ViT architectures from scratch and scale them up in an efficient and principled way, replacing tedious manual efforts. The automated design is enabled by a fast training-free search approach based on network complexity metrics.
