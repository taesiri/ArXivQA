# [Small Temperature is All You Need for Differentiable Architecture Search](https://arxiv.org/abs/2306.06855)

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper addresses is how to close the gap between the relaxed supernet in training and the pruned final architecture in evaluation in differentiable architecture search (DARTS). The key hypothesis is that utilizing small temperature to sparsify the architecture parameter distribution during training will make the supernet training closer to the final discrete architecture evaluation, thus alleviating issues like unfair advantages of skip connections.The main contributions are:1) Proposing sparse-noisy softmax to allow small temperature training without gradient saturation. 2) Developing an exponential temperature schedule to better control the sparsity.3) Designing an entropy-based adaptive scheme to dynamically decay the temperature based on architecture entropy.Overall, the paper aims to enhance DARTS through controlling temperature and sparsity to narrow the gap between relaxed supernet training and discrete architecture evaluation. The main hypothesis is that sparser architecture parameters will reduce issues like skip connection unfairness in DARTS.


## What is the main contribution of this paper?

The main contribution of this paper is proposing methods to alleviate the mismatch between the relaxed continuous supernet during training and the discrete final network after pruning in differentiable architecture search (DARTS). Specifically:- They propose sparse-noisy (sn) softmax to avoid gradient saturation when using small temperature in softmax during training. This allows the use of small temperature to sparsify the architecture parameter distribution while still propagating gradients. - They propose an exponential temperature schedule (ETS) to smoothly control the sparsity of the architecture parameter distribution over training compared to naive linear decay.- They propose an entropy-based dynamic decay (EDD) scheme to adaptively control the temperature decay based on the sparsity of the architecture parameters and the training epoch. This avoids having to manually tune a fixed decay schedule.- They show their EDD method eliminates the performance collapse issue in DARTS, achieves state-of-the-art results on multiple datasets and search spaces, and is more robust than less flexible alternatives when transferring search configurations across spaces and datasets.In summary, the key contribution is using techniques like sn-softmax, ETS, and EDD to sparsify the architecture parameters during DARTS training, which reduces the train-test mismatch and improves performance. The methods are simple yet effective enhancements to the DARTS approach.
