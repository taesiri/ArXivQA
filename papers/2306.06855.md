# [Small Temperature is All You Need for Differentiable Architecture Search](https://arxiv.org/abs/2306.06855)

## What is the central research question or hypothesis that this paper addresses?

 The central research question this paper addresses is how to close the gap between the relaxed supernet in training and the pruned final architecture in evaluation in differentiable architecture search (DARTS). 

The key hypothesis is that utilizing small temperature to sparsify the architecture parameter distribution during training will make the supernet training closer to the final discrete architecture evaluation, thus alleviating issues like unfair advantages of skip connections.

The main contributions are:

1) Proposing sparse-noisy softmax to allow small temperature training without gradient saturation. 

2) Developing an exponential temperature schedule to better control the sparsity.

3) Designing an entropy-based adaptive scheme to dynamically decay the temperature based on architecture entropy.

Overall, the paper aims to enhance DARTS through controlling temperature and sparsity to narrow the gap between relaxed supernet training and discrete architecture evaluation. The main hypothesis is that sparser architecture parameters will reduce issues like skip connection unfairness in DARTS.


## What is the main contribution of this paper?

 The main contribution of this paper is proposing methods to alleviate the mismatch between the relaxed continuous supernet during training and the discrete final network after pruning in differentiable architecture search (DARTS). Specifically:

- They propose sparse-noisy (sn) softmax to avoid gradient saturation when using small temperature in softmax during training. This allows the use of small temperature to sparsify the architecture parameter distribution while still propagating gradients. 

- They propose an exponential temperature schedule (ETS) to smoothly control the sparsity of the architecture parameter distribution over training compared to naive linear decay.

- They propose an entropy-based dynamic decay (EDD) scheme to adaptively control the temperature decay based on the sparsity of the architecture parameters and the training epoch. This avoids having to manually tune a fixed decay schedule.

- They show their EDD method eliminates the performance collapse issue in DARTS, achieves state-of-the-art results on multiple datasets and search spaces, and is more robust than less flexible alternatives when transferring search configurations across spaces and datasets.

In summary, the key contribution is using techniques like sn-softmax, ETS, and EDD to sparsify the architecture parameters during DARTS training, which reduces the train-test mismatch and improves performance. The methods are simple yet effective enhancements to the DARTS approach.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the key points from the paper:

The paper proposes methods to sparsify the architecture parameter distribution in differentiable architecture search (DARTS) using small temperature and adaptive decay scheduling, in order to alleviate issues like performance collapse caused by mismatch between the dense supernet and the sparse final architecture.


## How does this paper compare to other research in the same field?

 Here are a few key points comparing this paper to other research on differentiable architecture search (DARTS):

- The main focus of this paper is on closing the gap between the relaxed supernet in training and the pruned final architecture in evaluation in DARTS. Many other papers have also identified this architecture mismatch as an issue, but this paper specifically tackles it through sparsifying the architecture distribution using small temperatures.

- Methods like Fair DARTS and GAEA have also aimed to alleviate the architecture mismatch issue by promoting sparsity. However, this paper proposes more direct and tailored techniques like sparse-noisy softmax and exponential temperature scheduling to achieve sparser distributions.

- Some other works like Single-Path NAS and SparseNAS combine multi-path and single-path training. This paper aims to stay fully differentiable but make the distributions closer to single-path through temperatures rather than directly training single-path.

- Compared to regularization methods like DARTS+ and SmoothDARTS, this paper uses a different approach of sparsification rather than adding regularization terms. The evaluations show it is competitive or better than these regularization techniques.

- The method requires minimal additional hyperparameters (mainly just lambda) compared to more complex counterparts like DrNAS, R-DARTS, etc. And it has very low overhead in terms of search time.

- The technique results in strong performance across multiple spaces and datasets, demonstrating its general efficacy. The experiments also do ablation studies to analyze the impact of different components.

In summary, this paper provides a simple yet effective way of sparsifying the architecture distributions in DARTS to mitigate the train-test discrepancy. It is compared extensively to prior arts and analyzed thoroughly. The approach is competitive while being efficient and straightforward.


## What future research directions do the authors suggest?

 Based on my reading of the paper, some key future research directions suggested by the authors are:

- Develop more advanced architectures and search spaces for neural architecture search (NAS). The paper focuses on standard cell-based NAS benchmarks like NAS-Bench-201 and DARTS search space. The authors suggest exploring more complex and advanced architectures and search spaces as an area for future work.

- Improve search algorithms and training techniques for NAS. The paper proposes methods to improve the DARTS algorithm specifically. The authors suggest developing better search algorithms and training techniques as a general area for advancement in NAS research.

- Enhance transferability of architectures found by NAS. The paper shows transferring architectures found on CIFAR-10 to ImageNet. Improving how well architectures transfer across different datasets is noted as an important direction.

- Reduce computational costs of NAS. The authors point out NAS remains computationally expensive. Reducing search costs through more efficient search algorithms and training methods is highlighted as an impactful research direction.

- Theoretically understand NAS algorithms better. The paper provides empirical analysis of the proposed method. Developing better theoretical understanding of why NAS algorithms succeed or fail is noted as valuable future work.

- Apply NAS to more applications. The experiments focus on image classification. Applying NAS to new domains like object detection, segmentation, NLP, etc. is suggested as a useful research direction.

In summary, the main future directions mentioned are: developing better architectures/search spaces, improving search algorithms and training techniques, enhancing transferability, reducing computational costs, strengthening theoretical understanding, and expanding NAS to new applications. Advancing along these lines can significantly advance neural architecture search research according to the authors.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the paper:

The paper proposes methods to alleviate the mismatch between the relaxed supernet trained in DARTS and the discrete architecture obtained after pruning. First, they propose a sparse-noisy (sn) softmax to allow the use of small temperatures for a sparse distribution while avoiding gradient saturation. Second, they develop an exponential temperature schedule (ETS) to better control the sparsity during training compared to linear schedules. Finally, they propose an entropy-based dynamic decay (EDD) scheme that adapts the temperature decay based on the sparsity of the architecture parameters. Experiments on multiple datasets and search spaces demonstrate that their approach effectively solves the performance collapse issue in DARTS while achieving superior results. The adaptive nature of EDD also provides better robustness compared to less flexible decay schemes. Overall, the paper focuses on sparsifying the architecture parameter distribution to narrow the gap between the relaxed supernet and discrete final architecture in DARTS.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the key points from the paper:

This paper proposes techniques to sparsify the architecture parameter distribution in differentiable architecture search (DARTS) in order to alleviate the mismatch between the relaxed supernet during training and the discrete final architecture after pruning. The authors first propose a sparse-noisy softmax which uses a small temperature for the feedforward computation to obtain a sparse output distribution, while using a larger temperature in the backwards pass to avoid gradient saturation. They then propose an exponential temperature schedule which decays the temperature in the exponential rather than linear space to better control the sparsity. Finally, they introduce an entropy-based adaptive decay scheme which dynamically adjusts the temperature decay based on the entropy of the architecture parameter distribution. 

The authors evaluate their method, EDD-DARTS, on several datasets and search spaces. It is shown to outperform DARTS and several other NAS methods in terms of test accuracy across multiple benchmarks. Analyses demonstrate that EDD-DARTS achieves much sparser architecture parameter distributions during training compared to baselines. This helps alleviate the train-test discrepancy in DARTS by making the supernet closer to the final discrete architecture. The results validate the efficacy of sparsifying the architecture parameters for more effective differentiable NAS.


## Summarize the main method used in the paper in one paragraph.

 The paper presents a method to improve differentiable architecture search (DARTS) by sparsifying the architecture parameters during training. The key ideas are:

1) They propose a sparse-noisy softmax (sn-softmax) to alleviate gradient saturation when using small temperatures in softmax to sparsify the architecture parameters. Sn-softmax uses a small temperature for the forward pass to get a sparse output, but a higher temperature in the backward pass to avoid gradient saturation. 

2) They propose an exponential temperature schedule (ETS) to smoothly control the sparsity of the architecture parameters during training. ETS schedules the temperature in the exponential space to avoid sudden changes in sparsity.

3) They propose an entropy-based dynamic decay (EDD) scheme to automatically adapt the temperature schedule based on the sparsity of the architecture parameters. EDD increases the decay strength when the parameters are less sparse and reduces it when they become too sparse.

Overall, the method sparsifies the architecture parameters during DARTS training to reduce the gap between the trained supernet and the final discretized architecture. This alleviates issues like unfair advantages of skip connections and architecture mismatch after discretization. Experiments show the method effectively improves DARTS performance across multiple datasets and search spaces.


## What problem or question is the paper addressing?

 The paper is addressing the issue of mismatch between the relaxed continuous architecture space used during training and the discrete architecture space used during evaluation in differentiable architecture search (DARTS). 

The key points are:

- DARTS relaxes the discrete architecture search space to a continuous one during training by allowing weighted combinations of candidate operations on each edge. After training the "supernet", the final architecture is obtained by pruning away all but the strongest operation on each edge. 

- This pruning can result in a mismatch between the supernet trained and the final architecture evaluated. The supernet may learn to rely on weaker operations that get pruned away.

- The paper proposes to address this issue by sparsifying the architecture parameter distribution during training, so the supernet relies less on weak operations. This makes the supernet closer to the final discrete architecture.

- They do this via three main contributions:

1) Sparse-noisy (sn) softmax to allow using small temperatures for sparsity while avoiding gradient saturation.

2) Exponential temperature scheduling (ETS) to smoothly control sparsity over training. 

3) Entropy-based dynamic decay (EDD) to automatically adapt the temperature schedule based on entropy of the architecture parameters.

- Experiments show their method (EDD-DARTS) eliminates the performance collapse issue in DARTS and achieves state-of-the-art performance on multiple datasets and search spaces.

In summary, the key focus is closing the gap between the relaxed supernet and discrete final architecture in DARTS via sparsifying the architecture parameters during training. EDD-DARTS provides an effective way to achieve this sparsification.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and main points are:

- Differentiable architecture search (DARTS): This is a gradient-based neural architecture search method that relaxes the discrete search space to a continuous one for more efficient optimization. The paper focuses on enhancing DARTS.

- Architecture parameters vs operation weights: The architecture parameters determine the mix of operations on each edge of the search cell. The operation weights are the normal network weights for each operation. 

- Supernet training vs final architecture evaluation: In DARTS, a "supernet" with all candidate operations is trained. The final architecture prunes away all but the most likely operations. 

- Architecture mismatch: There can be a mismatch between the supernet trained with multiple operations and the final single-path architecture. This can lead to performance discrepancies.

- Sparse architecture parameters: Having a sparse set of architecture parameters, with clear winners, can alleviate the architecture mismatch issue.

- Sparse-noisy softmax: A proposed method to allow sharp, sparse softmax outputs for feedforward but avoid gradient saturation in backpropagation. 

- Exponential temperature schedule: A schedule for the softmax temperature to smoothly control the sparsity of the architecture parameters.

- Entropy-based dynamic decay: An adaptive scheme to decay the temperature based on the entropy of the architecture parameters and training progress, requiring only one hyperparameter.

In summary, the key focus is enhancing DARTS training to avoid architecture mismatch issues by dynamically sparsifying the architecture parameters using techniques like sparse-noisy softmax and exponential temperature scheduling.


## Based on the paper, what questions should we ask to create a comprehensive summary of it? Please list at least 10 questions.

 Here are 10 potential questions to ask to create a comprehensive summary of the paper:

1. What is the key problem addressed in the paper?

2. What is differentiable architecture search (DARTS) and how does it work? 

3. What issue with DARTS does the paper identify regarding mismatch between the relaxed supernet in training and the pruned finalnet in evaluation?

4. What does the paper propose to close the gap between the relaxed supernet and pruned finalnet?

5. How does the paper formulate sparse-noisy softmax to alleviate gradient saturation? 

6. How does the paper propose an exponential temperature schedule to better control the outbound distribution?

7. What is the entropy-based adaptive scheme proposed in the paper and how does it help achieve enhancement for DARTS?

8. What datasets, search spaces, and baseline methods were used to evaluate the proposed approach? 

9. What were the key results and comparisons to baselines in the experiments?

10. What conclusions does the paper draw about the efficiency and efficacy of the proposed enhancements to DARTS?

Asking these types of questions while reading the paper will help ensure a comprehensive understanding of the key points and details necessary to summarize it effectively. Let me know if you need any clarification on these questions or have additional questions to add.


## Design 10 in-depth questions about method proposed in this paper and avoid simple question.

 Here are 10 potential in-depth questions about the method proposed in this paper:

1. The paper proposes using a small temperature in softmax to obtain a sparse architecture parameter distribution. However, a small temperature can lead to gradient saturation during training. How does the proposed sparse-noisy (sn) softmax address this issue?

2. The paper claims sn-softmax requires minimal additional computation compared to standard softmax. Can you elaborate on the specific computations required and why the overhead is small? 

3. Exponential temperature schedule (ETS) is proposed to control the sparsity of the architecture parameters. How does ETS lead to smoother parameter changes compared to linear temperature scheduling?

4. The entropy-based dynamic decay (EDD) scheme adaptively determines the temperature decay strength. What are the key principles behind how EDD adjusts the decay strength?

5. EDD requires tuning only a single hyperparameter λ. How does λ affect the overall training process and temperature schedule? What strategies can be used to set an appropriate value?

6. The paper shows improved robustness of EDD compared to periodic cyclic decay (PCD) when transferring settings between datasets/search spaces. What causes EDD to generalize better?

7. How exactly does a sparse architecture parameter distribution reduce the gap between the relaxed supernet and discrete final architecture? 

8. What causes architecture mismatch in DARTS and how does the proposed method alleviate this issue? Can you summarize the analysis around Figure 4?

9. How does sn-softmax balance using a small temperature for sparsity while avoiding gradient saturation? Is there a tradeoff?

10. For practical use, what are some key considerations in implementing the overall method? How could it be tuned and optimized further?


## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a high-quality paragraph summarizing the key points of the paper:

This paper proposes enhancements to the Differentiable Architecture Search (DARTS) method for neural architecture search. The key idea is to sparsify the architecture parameter distribution β during DARTS training to alleviate the mismatch between the continuous relaxed supernet used during training and the discrete architecture obtained after one-shot pruning. To achieve this, the authors first propose a sparse-noisy softmax (sn-softmax) to avoid gradient saturation and allow training to continue even when β becomes very sparse. They then introduce an exponential temperature schedule (ETS) that more smoothly controls the sparsity of β over time compared to linear decay. Finally, an entropy-based dynamic decay (EDD) scheme is presented that adaptively determines the temperature decay strength based on the sparsity of β, requiring only one hyperparameter λ to control the training process. Extensive experiments on multiple datasets and search spaces demonstrate that these enhancements effectively eliminate DARTS' tendency to collapse and allow it to achieve state-of-the-art results. The approach adds little computation overhead while greatly improving robustness. By sparsifying architecture parameters during training, the mismatch between training and evaluation is reduced, providing more accurate architecture search.


## Summarize the paper in one sentence.

 Here is a one-sentence summary of the paper:

The paper proposes techniques including sparse-noisy softmax, exponential temperature schedule, and entropy-based dynamic decay to sparsify the architecture parameter distributions in differentiable architecture search, narrowing the gap between the relaxed supernet in training and the discrete final architecture in evaluation to enhance DARTS.


## Summarize the paper in one paragraphs.

 Here is a one paragraph summary of the paper:

This paper proposes enhancements to the Differentiable Architecture Search (DARTS) method for neural architecture search. The key idea is to sparsify the architecture parameter distribution β during training by using a small temperature in the softmax. This helps reduce the gap between the relaxed, multi-path supernet used during training and the discrete, single-path architecture used at evaluation time. To enable training with a small temperature, the authors propose a sparse-noisy softmax (sn-softmax) to alleviate gradient saturation and premature convergence issues. They also introduce an exponential temperature schedule (ETS) to better control the sparsity of β over time. Finally, an entropy-based dynamic decay (EDD) scheme is proposed to adaptively determine the temperature decay strength based on the sparsity of β. Experiments on multiple datasets and search spaces demonstrate that these enhancements effectively eliminate DARTS' tendency to collapse and lead to superior performance. The approach is also efficient, with time costs comparable to or better than prior DARTS variants.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 potential in-depth questions about the method proposed in this paper:

1. The paper proposes using a small temperature in softmax to sparsify the architecture parameter distribution β. Why is sparsifying β important for alleviating the train-test discrepancy in DARTS? What issues does it help mitigate?

2. The paper introduces a "sparse-noisy" softmax (sn-softmax) to deal with gradient saturation when using a small temperature. Can you explain in detail how sn-softmax allows gradients to continue propagating even when softmax converges prematurely? 

3. The exponential temperature schedule (ETS) is proposed to give more fine-grained control over the sparsity of β compared to linear decay. Can you walk through the mathematical justification and formulations for ETS? How does it give more flexibility?

4. The entropy-based dynamic decay (EDD) scheme adaptively determines the temperature decay strength. What is the intuition behind using the entropy of β to guide this? How does taking the training epoch into account lead to more robustness?

5. How does the EDD scheme compare to alternative baselines like a periodic cyclic decay? What are the limitations of setting the full temperature schedule statically beforehand?

6. What role does the hyperparameter λ play in balancing architecture capacity vs trainability? How could you modify λ to tweak this tradeoff?

7. The method is evaluated on various datasets and search spaces. What was the motivation behind this extensive benchmarking? Which cases posed the biggest challenges?

8. Can you analyze the training dynamics plots (Fig 4) and relate the entropy trajectories to differences in discretization accuracy between methods? What explains these patterns?

9. How necessary is using sn-softmax? What drop in performance, if any, occurs with just ETS and EDD? Is the overhead of sn-softmax justified?

10. The paper focuses uniquely on sparsifying β for mitigating DARTS issues. How does this compare / complement other regularization techniques like adding noise or PC-DARTS? Could these be combined?
