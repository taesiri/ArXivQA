# [Small Temperature is All You Need for Differentiable Architecture Search](https://arxiv.org/abs/2306.06855)

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper addresses is how to close the gap between the relaxed supernet in training and the pruned final architecture in evaluation in differentiable architecture search (DARTS). The key hypothesis is that utilizing small temperature to sparsify the architecture parameter distribution during training will make the supernet training closer to the final discrete architecture evaluation, thus alleviating issues like unfair advantages of skip connections.The main contributions are:1) Proposing sparse-noisy softmax to allow small temperature training without gradient saturation. 2) Developing an exponential temperature schedule to better control the sparsity.3) Designing an entropy-based adaptive scheme to dynamically decay the temperature based on architecture entropy.Overall, the paper aims to enhance DARTS through controlling temperature and sparsity to narrow the gap between relaxed supernet training and discrete architecture evaluation. The main hypothesis is that sparser architecture parameters will reduce issues like skip connection unfairness in DARTS.
