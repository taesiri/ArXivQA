# [Small Temperature is All You Need for Differentiable Architecture Search](https://arxiv.org/abs/2306.06855)

## What is the central research question or hypothesis that this paper addresses?

 The central research question this paper addresses is how to close the gap between the relaxed supernet in training and the pruned final architecture in evaluation in differentiable architecture search (DARTS). The key hypothesis is that utilizing small temperature to sparsify the architecture parameter distribution during training will make the supernet training closer to the final discrete architecture evaluation, thus alleviating issues like unfair advantages of skip connections.The main contributions are:1) Proposing sparse-noisy softmax to allow small temperature training without gradient saturation. 2) Developing an exponential temperature schedule to better control the sparsity.3) Designing an entropy-based adaptive scheme to dynamically decay the temperature based on architecture entropy.Overall, the paper aims to enhance DARTS through controlling temperature and sparsity to narrow the gap between relaxed supernet training and discrete architecture evaluation. The main hypothesis is that sparser architecture parameters will reduce issues like skip connection unfairness in DARTS.


## What is the main contribution of this paper?

 The main contribution of this paper is proposing methods to alleviate the mismatch between the relaxed continuous supernet during training and the discrete final network after pruning in differentiable architecture search (DARTS). Specifically:- They propose sparse-noisy (sn) softmax to avoid gradient saturation when using small temperature in softmax during training. This allows the use of small temperature to sparsify the architecture parameter distribution while still propagating gradients. - They propose an exponential temperature schedule (ETS) to smoothly control the sparsity of the architecture parameter distribution over training compared to naive linear decay.- They propose an entropy-based dynamic decay (EDD) scheme to adaptively control the temperature decay based on the sparsity of the architecture parameters and the training epoch. This avoids having to manually tune a fixed decay schedule.- They show their EDD method eliminates the performance collapse issue in DARTS, achieves state-of-the-art results on multiple datasets and search spaces, and is more robust than less flexible alternatives when transferring search configurations across spaces and datasets.In summary, the key contribution is using techniques like sn-softmax, ETS, and EDD to sparsify the architecture parameters during DARTS training, which reduces the train-test mismatch and improves performance. The methods are simple yet effective enhancements to the DARTS approach.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the key points from the paper:The paper proposes methods to sparsify the architecture parameter distribution in differentiable architecture search (DARTS) using small temperature and adaptive decay scheduling, in order to alleviate issues like performance collapse caused by mismatch between the dense supernet and the sparse final architecture.


## How does this paper compare to other research in the same field?

 Here are a few key points comparing this paper to other research on differentiable architecture search (DARTS):- The main focus of this paper is on closing the gap between the relaxed supernet in training and the pruned final architecture in evaluation in DARTS. Many other papers have also identified this architecture mismatch as an issue, but this paper specifically tackles it through sparsifying the architecture distribution using small temperatures.- Methods like Fair DARTS and GAEA have also aimed to alleviate the architecture mismatch issue by promoting sparsity. However, this paper proposes more direct and tailored techniques like sparse-noisy softmax and exponential temperature scheduling to achieve sparser distributions.- Some other works like Single-Path NAS and SparseNAS combine multi-path and single-path training. This paper aims to stay fully differentiable but make the distributions closer to single-path through temperatures rather than directly training single-path.- Compared to regularization methods like DARTS+ and SmoothDARTS, this paper uses a different approach of sparsification rather than adding regularization terms. The evaluations show it is competitive or better than these regularization techniques.- The method requires minimal additional hyperparameters (mainly just lambda) compared to more complex counterparts like DrNAS, R-DARTS, etc. And it has very low overhead in terms of search time.- The technique results in strong performance across multiple spaces and datasets, demonstrating its general efficacy. The experiments also do ablation studies to analyze the impact of different components.In summary, this paper provides a simple yet effective way of sparsifying the architecture distributions in DARTS to mitigate the train-test discrepancy. It is compared extensively to prior arts and analyzed thoroughly. The approach is competitive while being efficient and straightforward.


## What future research directions do the authors suggest?

 Based on my reading of the paper, some key future research directions suggested by the authors are:- Develop more advanced architectures and search spaces for neural architecture search (NAS). The paper focuses on standard cell-based NAS benchmarks like NAS-Bench-201 and DARTS search space. The authors suggest exploring more complex and advanced architectures and search spaces as an area for future work.- Improve search algorithms and training techniques for NAS. The paper proposes methods to improve the DARTS algorithm specifically. The authors suggest developing better search algorithms and training techniques as a general area for advancement in NAS research.- Enhance transferability of architectures found by NAS. The paper shows transferring architectures found on CIFAR-10 to ImageNet. Improving how well architectures transfer across different datasets is noted as an important direction.- Reduce computational costs of NAS. The authors point out NAS remains computationally expensive. Reducing search costs through more efficient search algorithms and training methods is highlighted as an impactful research direction.- Theoretically understand NAS algorithms better. The paper provides empirical analysis of the proposed method. Developing better theoretical understanding of why NAS algorithms succeed or fail is noted as valuable future work.- Apply NAS to more applications. The experiments focus on image classification. Applying NAS to new domains like object detection, segmentation, NLP, etc. is suggested as a useful research direction.In summary, the main future directions mentioned are: developing better architectures/search spaces, improving search algorithms and training techniques, enhancing transferability, reducing computational costs, strengthening theoretical understanding, and expanding NAS to new applications. Advancing along these lines can significantly advance neural architecture search research according to the authors.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the paper:The paper proposes methods to alleviate the mismatch between the relaxed supernet trained in DARTS and the discrete architecture obtained after pruning. First, they propose a sparse-noisy (sn) softmax to allow the use of small temperatures for a sparse distribution while avoiding gradient saturation. Second, they develop an exponential temperature schedule (ETS) to better control the sparsity during training compared to linear schedules. Finally, they propose an entropy-based dynamic decay (EDD) scheme that adapts the temperature decay based on the sparsity of the architecture parameters. Experiments on multiple datasets and search spaces demonstrate that their approach effectively solves the performance collapse issue in DARTS while achieving superior results. The adaptive nature of EDD also provides better robustness compared to less flexible decay schemes. Overall, the paper focuses on sparsifying the architecture parameter distribution to narrow the gap between the relaxed supernet and discrete final architecture in DARTS.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the key points from the paper:This paper proposes techniques to sparsify the architecture parameter distribution in differentiable architecture search (DARTS) in order to alleviate the mismatch between the relaxed supernet during training and the discrete final architecture after pruning. The authors first propose a sparse-noisy softmax which uses a small temperature for the feedforward computation to obtain a sparse output distribution, while using a larger temperature in the backwards pass to avoid gradient saturation. They then propose an exponential temperature schedule which decays the temperature in the exponential rather than linear space to better control the sparsity. Finally, they introduce an entropy-based adaptive decay scheme which dynamically adjusts the temperature decay based on the entropy of the architecture parameter distribution. The authors evaluate their method, EDD-DARTS, on several datasets and search spaces. It is shown to outperform DARTS and several other NAS methods in terms of test accuracy across multiple benchmarks. Analyses demonstrate that EDD-DARTS achieves much sparser architecture parameter distributions during training compared to baselines. This helps alleviate the train-test discrepancy in DARTS by making the supernet closer to the final discrete architecture. The results validate the efficacy of sparsifying the architecture parameters for more effective differentiable NAS.


## Summarize the main method used in the paper in one paragraph.

 The paper presents a method to improve differentiable architecture search (DARTS) by sparsifying the architecture parameters during training. The key ideas are:1) They propose a sparse-noisy softmax (sn-softmax) to alleviate gradient saturation when using small temperatures in softmax to sparsify the architecture parameters. Sn-softmax uses a small temperature for the forward pass to get a sparse output, but a higher temperature in the backward pass to avoid gradient saturation. 2) They propose an exponential temperature schedule (ETS) to smoothly control the sparsity of the architecture parameters during training. ETS schedules the temperature in the exponential space to avoid sudden changes in sparsity.3) They propose an entropy-based dynamic decay (EDD) scheme to automatically adapt the temperature schedule based on the sparsity of the architecture parameters. EDD increases the decay strength when the parameters are less sparse and reduces it when they become too sparse.Overall, the method sparsifies the architecture parameters during DARTS training to reduce the gap between the trained supernet and the final discretized architecture. This alleviates issues like unfair advantages of skip connections and architecture mismatch after discretization. Experiments show the method effectively improves DARTS performance across multiple datasets and search spaces.
