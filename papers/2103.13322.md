# [DNN Quantization with Attention](https://arxiv.org/abs/2103.13322)

## What is the central research question or hypothesis that this paper addresses?

Based on the provided information, it seems the central research question this paper addresses is how to train accurate deep neural networks with low-bit quantized weights and activations. Specifically, the paper proposes a method called "DNN Quantization with Attention" (DQA) to relax the quantization and gradually move from high precision to low precision quantization during training. The key ideas are:- Use a mixture of quantization functions at different precisions (number of bits) during training, instead of just one quantization function. - Learn a set of attention values to interpolate between these quantization functions, rather than using predetermined scaling factors.- Apply a temperature schedule to the attention values to progressively focus on the low-bit quantization function during training.- Add a regularizer to the loss to encourage low-bit quantization functions to be selected.The central hypothesis seems to be that this more relaxed training procedure will allow the network to smoothly transition to low-bit quantization and achieve better accuracy compared to methods that use a fixed low-bit quantization from the start. The experiments aim to validate whether DQA can train accurate low-bit quantized networks and outperform other quantization techniques.
