# [Exploring Sparsity in Graph Transformers](https://arxiv.org/abs/2312.05479)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper "Exploring Sparsity in Graph Transformers":

Problem: 
Graph transformers (GTs) have achieved impressive performance on graph-related tasks. However, GTs have high computational complexity due to the multi-head self-attention mechanism. This hinders their deployment under resource-constrained scenarios. The high redundancy in GT models is a key factor leading to inefficient computation and memory usage. However, pruning techniques tailored for GT models have not been well explored.

Proposed Solution:
This paper proposes a novel Graph Transformer Sparsification (GTSP) framework to reduce the computational costs of GTs by pruning redundant structures across four dimensions:

1) Input graph data: A token selector removes uninformative nodes before feeding them into the model, reducing subsequent computation. 

2) Attention heads: Head importance scores are defined to prune redundant heads. A regrowth scheme reactivates certain pruned heads during training.

3) Model layers: Skip connections are introduced to randomly drop redundant layers, alleviating over-smoothing.

4) Model weights: Gradual magnitude pruning extracts sparse subnetworks. A regrowth scheme prevents premature pruning.  

Masks enable end-to-end optimization of these pruning techniques. GTSP is architecture-agnostic and can easily integrate with existing GT models.

Main Contributions:
- First work proposing a systematic GT compression framework spanning input data, heads, layers and weights. Up to 50% FLOPs reduction with marginal accuracy drop.

- Provide insights on redundancy across GT components and roles of attention heads. Heads focus on distinct structural contexts.

- Demonstrate pruning helps alleviate overfitting and oversmoothing issues in GTs. GTs prioritize informative nodes, enhancing interpretability.

- Extensive experiments on large datasets using GraphTrans, Graphormer and GraphGPS validate effectiveness and generalization ability of GTSP. Up to 30% FLOPs saving and 1.8% accuracy gain.

In summary, this paper makes significant contributions towards compressing graph transformers and offers valuable insights to guide future GT optimization research. The proposed GTSP framework effectively balances efficiency and accuracy.
