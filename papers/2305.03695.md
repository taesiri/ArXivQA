# [Vera: A General-Purpose Plausibility Estimation Model for Commonsense   Statements](https://arxiv.org/abs/2305.03695)

## What is the central research question or hypothesis that this paper addresses?

The central research question addressed in this paper is whether it is possible to build a general-purpose commonsense statement verifier that can estimate the plausibility of natural language statements based on commonsense knowledge. The key hypothesis is that by finetuning a pretrained language model on a diverse collection of correct and incorrect commonsense statements sourced from knowledge bases and question answering datasets, it is possible to develop a model that can effectively judge the plausibility of novel commonsense statements across a wide range of domains and applications.In particular, the paper proposes and evaluates the VERA model, which is designed to output a plausibility score for any given commonsense statement based on the commonsense knowledge encoded in the model parameters. The paper hypothesizes that with an effective training strategy and data collection methodology, VERA will be able to reliably distinguish between true and false commonsense statements. The research questions explored are:1) Can a model finetuned on commonsense training data learn to effectively estimate the plausibility of novel commonsense statements?2) How does VERA compare to existing models and baselines when evaluated on commonsense QA datasets and knowledge filtering tasks?3) Does VERA generalize well to out-of-domain commonsense statements beyond its training data?4) Can VERA help detect commonsense mistakes made by other language models like ChatGPT?Through extensive experiments and analysis, the paper provides evidence that the proposed VERA model achieves state-of-the-art performance on commonsense plausibility estimation and outperforms existing methods, thus supporting the main hypothesis. Evaluating VERA in diverse settings also sheds light on the challenges and future work needed to develop robust verifiers.


## What is the main contribution of this paper?

Based on my understanding, the main contribution of this paper is proposing a general-purpose commonsense statement verification model called VERA. Specifically:- The paper identifies the need for a model that can verify the plausibility of commonsense statements, in order to detect potential errors in text generated by language models. - The authors construct a diverse training dataset of ~7 million correct and incorrect commonsense statements sourced from knowledge bases and QA datasets.- They propose a training approach involving a two-stage process and three training objectives - binary classification, multi-class classification, and supervised contrastive loss.- The proposed VERA model demonstrates strong performance on multiple applications:    - Solving commonsense reasoning problems in a verification format, outperforming models like GPT-3.5 and Flan-T5.    - Filtering noisy commonsense knowledge generated by language models.    - Detecting commonsense mistakes made by ChatGPT.- The scores produced by VERA are well-calibrated, enabling it to express confidence in its predictions.In summary, the main contribution is proposing VERA as an effective general-purpose model for verifying the plausibility of commonsense statements, trained on a large and diverse dataset, which shows promising performance on multiple applications related to commonsense reasoning and knowledge.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence TL;DR of the paper:The paper proposes a general-purpose commonsense statement verifier called VERA which is trained on a diverse collection of correct and incorrect commonsense statements from QA datasets and knowledge bases, and demonstrates strong performance on commonsense reasoning tasks compared to existing models.


## How does this paper compare to other research in the same field?

Here is a summary of how this paper compares to other research in the field of commonsense statement verification:- This paper proposes VERA, a general-purpose commonsense statement verifier. Other work in this field has focused more narrowly on verifying statements from specific domains or sources, such as Symbolic Knowledge Distillation and I2D2 which verify statements generated by those specific methods. VERA aims to verify commonsense statements more broadly.- The paper constructs a large and diverse training set for VERA by sourcing statements from 19 QA datasets and 2 knowledge bases. This is a larger and more varied set of training data than has been used in prior work, which has tended to use smaller domain-specific datasets.- The paper shows that VERA outperforms existing commonsense verifiers like the SKD Critic, I2D2 Critic, and Entailer models when tested on commonsense QA datasets and knowledge filtering tasks. This demonstrates VERA's stronger capabilities compared to prior specialized models.- The paper proposes techniques like multi-stage training, an adversarial multi-class loss, and supervised contrastive learning that seem to improve VERA's accuracy and robustness over baseline approaches. These training innovations aim to make VERA a more capable general verifier.- The paper demonstrates VERA's ability to detect commonsense mistakes made by ChatGPT. Using VERA for this purpose is a novel application of commonsense verification compared to prior work.- Overall, VERA represents a more general, robust, and empirically stronger approach to commonsense statement verification compared to previous specialized models. The variety of data, training techniques, and applications make it a more comprehensive verification solution.In summary, this paper pushes forward the state-of-the-art in commonsense verification research through its more general scope, larger and broader training data, training innovations, and strong empirical results across multiple applications. It represents an advance in developing more capable and reliable commonsense verifiers.


## What future research directions do the authors suggest?

The authors suggest several promising directions for future research:1. Developing methods to verify multi-sentence or long-form statements. The current work focuses on verifying standalone, single-sentence commonsense statements. Extending the approach to handle longer, more complex statements would expand the applicability.2. Making the verifier more robust to syntactic variations. The authors note that the current verifier is not very robust to paraphrasing or negation of the input statements. Developing techniques to improve robustness could lead to more reliable performance. 3. Integrating the verifier into generative LMs. The authors suggest using the verifier as a "reward model" to guide text generation towards more commonsense-compliant outputs. This could be a way to leverage the verifier to directly improve existing generative LMs.4. Evaluating the verifier on broader types of commonsense knowledge. The current work focuses on objective commonsense related to facts about the world. Evaluating on moral, social or other types of commonsense could reveal new research challenges.5. Studying the combination of verification and QA. The authors suggest the two approaches have complementary strengths and weaknesses, so exploring ways to combine them could lead to overall improvements.In summary, the main directions are towards extending the verifier to handle more complex inputs, making it more robust, integrating it into generative LMs, evaluating on broader knowledge types, and combining it with complementary QA-based approaches. Overall, the authors frame commonsense verification as a key challenge for improving AI systems and suggest promising ways forward.


## Summarize the paper in one paragraph.

Here is a one paragraph summary of the paper:The paper proposes VERA, a general-purpose commonsense statement verification model that estimates the plausibility of declarative statements based on commonsense knowledge. VERA is motivated by the observation that even state-of-the-art language models still make simple commonsense mistakes. It is trained on around 7 million commonsense statements constructed from 21 datasets covering diverse domains. Three training objectives are used: binary classification loss, multi-class loss to distinguish nuances, and supervised contrastive loss to learn robust representations. VERA demonstrates strong performance on commonsense reasoning benchmarks, knowledge filtering, and detecting commonsense errors made by models like ChatGPT. It outperforms existing models repurposed for verification and shows good generalization and calibration. The scores could be useful for plausibility estimation. As VERA focuses on single sentences, future work could consider verifying longer, multi-sentence statements.


## Summarize the paper in two paragraphs.

Here is a two paragraph summary of the paper:The paper introduces VERA, a general-purpose commonsense statement verification model. VERA is designed to estimate the plausibility of declarative, natural language statements based on commonsense knowledge. The authors build VERA in response to the lack of good detectors of commonsense errors in text generated by language models. VERA is built on top of T5, a generic pretrained language model, by finetuning it on a large collection of correct and incorrect commonsense statements sourced from knowledge bases and question answering datasets. The training data covers a wide range of commonsense domains. The authors propose a two-stage training process and objectives including binary classification, multi-class classification, and supervised contrastive losses.The authors evaluate VERA on multiple tasks including solving commonsense problems, filtering LM-generated knowledge, and detecting commonsense errors in ChatGPT outputs. VERA outperforms existing models repurposed for verification on seen and unseen commonsense QA benchmarks. It is an effective filter of noisy commonsense knowledge that can improve downstream inference. Through analysis of real-world ChatGPT errors, the authors find VERA can identify many commonsense mistakes made by ChatGPT. The scores from VERA are well-calibrated. The authors argue verification models like VERA can complementgenerative QA models and release VERA to enable further research.


## Summarize the main method used in the paper in one paragraph.

Here is a one paragraph summary of the main method used in the paper:The paper proposes VERA, a general-purpose commonsense statement verifier. VERA is built by finetuning a pretrained language model (T5 or LLaMA) on a diverse collection of around 7 million commonsense statements sourced from knowledge bases and question answering datasets. The statements cover a wide range of commonsense domains and are converted to correct/incorrect pairs. VERA is trained with a combination of three objectives - a binary classification loss, a multi-class loss to distinguish subtle differences, and a supervised contrastive loss to learn robust representations. The training happens in two stages - first on the noisy knowledge base data, then on the higher quality QA data. During inference, VERA applies temperature scaling for calibration. Given an input statement, VERA outputs a score between 0 and 1 reflecting the plausibility of the statement according to commonsense knowledge.


## What problem or question is the paper addressing?

Based on my understanding, the key points are:- Language models (LMs) like ChatGPT have shown impressive capabilities on many tasks, but still make silly commonsense mistakes frequently. This raises concerns about their reliability when deployed in real applications.- There is a lack of good methods to detect commonsense errors in generative LM outputs. Existing commonsense verifiers are limited in scope or don't generalize well. - The paper proposes VERA, a general-purpose model to estimate the plausibility of commonsense statements. It aims to address the limitations of prior work and provide a way to reflect on and potentially correct the commonsense mistakes made by LMs.- VERA is trained on a large and diverse collection of correct and incorrect commonsense statements sourced from knowledge bases and QA datasets. It uses a combination of classification, ranking, and contrastive losses.- Experiments show VERA significantly outperforms existing commonsense verifiers on a wide range of tasks like QA, knowledge filtering, and detecting errors in ChatGPT. It also has good calibration and generalizability.In summary, the key problem is the lack of methods to detect commonsense failures in generative LMs. The paper proposes VERA as an effective general-purpose verifier to address this limitation and enable commonsense verification for improving LM reliability.
