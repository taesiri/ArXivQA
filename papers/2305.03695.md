# [Vera: A General-Purpose Plausibility Estimation Model for Commonsense   Statements](https://arxiv.org/abs/2305.03695)

## What is the central research question or hypothesis that this paper addresses?

The central research question addressed in this paper is whether it is possible to build a general-purpose commonsense statement verifier that can estimate the plausibility of natural language statements based on commonsense knowledge. The key hypothesis is that by finetuning a pretrained language model on a diverse collection of correct and incorrect commonsense statements sourced from knowledge bases and question answering datasets, it is possible to develop a model that can effectively judge the plausibility of novel commonsense statements across a wide range of domains and applications.In particular, the paper proposes and evaluates the VERA model, which is designed to output a plausibility score for any given commonsense statement based on the commonsense knowledge encoded in the model parameters. The paper hypothesizes that with an effective training strategy and data collection methodology, VERA will be able to reliably distinguish between true and false commonsense statements. The research questions explored are:1) Can a model finetuned on commonsense training data learn to effectively estimate the plausibility of novel commonsense statements?2) How does VERA compare to existing models and baselines when evaluated on commonsense QA datasets and knowledge filtering tasks?3) Does VERA generalize well to out-of-domain commonsense statements beyond its training data?4) Can VERA help detect commonsense mistakes made by other language models like ChatGPT?Through extensive experiments and analysis, the paper provides evidence that the proposed VERA model achieves state-of-the-art performance on commonsense plausibility estimation and outperforms existing methods, thus supporting the main hypothesis. Evaluating VERA in diverse settings also sheds light on the challenges and future work needed to develop robust verifiers.


## What is the main contribution of this paper?

Based on my understanding, the main contribution of this paper is proposing a general-purpose commonsense statement verification model called VERA. Specifically:- The paper identifies the need for a model that can verify the plausibility of commonsense statements, in order to detect potential errors in text generated by language models. - The authors construct a diverse training dataset of ~7 million correct and incorrect commonsense statements sourced from knowledge bases and QA datasets.- They propose a training approach involving a two-stage process and three training objectives - binary classification, multi-class classification, and supervised contrastive loss.- The proposed VERA model demonstrates strong performance on multiple applications:    - Solving commonsense reasoning problems in a verification format, outperforming models like GPT-3.5 and Flan-T5.    - Filtering noisy commonsense knowledge generated by language models.    - Detecting commonsense mistakes made by ChatGPT.- The scores produced by VERA are well-calibrated, enabling it to express confidence in its predictions.In summary, the main contribution is proposing VERA as an effective general-purpose model for verifying the plausibility of commonsense statements, trained on a large and diverse dataset, which shows promising performance on multiple applications related to commonsense reasoning and knowledge.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence TL;DR of the paper:The paper proposes a general-purpose commonsense statement verifier called VERA which is trained on a diverse collection of correct and incorrect commonsense statements from QA datasets and knowledge bases, and demonstrates strong performance on commonsense reasoning tasks compared to existing models.
