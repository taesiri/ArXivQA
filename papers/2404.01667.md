# [METAL: Towards Multilingual Meta-Evaluation](https://arxiv.org/abs/2404.01667)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem: Evaluating the text generation capabilities of large language models (LLMs) is challenging. Metrics like ROUGE and BLEU have drawbacks like reliance on exact matches, overemphasis on length, and inability to assess subjective quality. Using humans for evaluation is difficult to scale. Recent works have proposed using LLMs themselves as evaluators but have shown they can be biased, inconsistent, and differ from human judgments. Past works are mostly English-centric.

Proposed Solution: This paper introduces the METAL (Multilingual META-evauation) framework - an end-to-end pipeline for assessing LLMs as evaluators in multilingual scenarios. 

1) A dataset of 1000 summaries covering 10 languages was created with native speaker judgments across 5 metrics, by prompting GPT-4 to generate varying quality summaries. 

2) Evaluations were obtained from GPT-3.5 Turbo, GPT-4 and PaLM2 on these 1000 summaries and compared to human judgments.

Key Contributions:

- Creation of a meticulously curated multilingual meta-evaluation benchmark dataset with 1000 summaries in 10 languages rated by humans on 5 metrics.

- Thorough analysis of 3 SOTA LLMs - GPT-3.5 Turbo, GPT-4 and PaLM2 as evaluators on this dataset. Findings indicate GPT-4 does best across metrics and languages.

- Investigation into alignment between human and LLM reasoning/justifications. Observed that LLMs often provide incorrect reasoning despite accurate scores.

- Demonstrated framework allows rigorous assessment of LLMs as multilingual evaluators. The dataset and framework enable future research into this space.
