# Frame-Recurrent Video Inpainting by Robust Optical Flow Inference

## What is the central research question or hypothesis that this paper addresses?

This paper proposes a new framework for video inpainting, which aims to fill in missing regions in video frames while preserving both spatial details and temporal consistency. The central hypothesis is that combining convolutional LSTM (ConvLSTM) and robust optical flow estimation can effectively model the spatial-temporal structure in videos for high-quality video inpainting. Specifically, the key research questions addressed are:- How to design an architecture that can handle videos of arbitrary sizes and lengths efficiently? The paper proposes using ConvLSTM instead of 3D CNNs to remove restrictions on input sizes.- How to generate accurate optical flows from incomplete video frames to guide ConvLSTM? The paper proposes a two-branch flow estimation module and a flow blending network to obtain robust optical flows. - How to train the network to balance spatial details and temporal consistency? The paper introduces spatial losses (L1, perceptual) and temporal losses (short-term, long-term) during training.- Does the proposed framework outperform state-of-the-art video inpainting methods quantitatively and qualitatively? Experiments on two datasets under different masks demonstrate superiority over previous methods.- How does each component contribute to the final performance? Ablation studies validate the effectiveness of ConvLSTM, robust optical flow estimation, and the training losses.In summary, the key hypothesis is that ConvLSTM and robust optical flow can effectively address the challenges of detail preservation and temporal consistency in video inpainting. The experiments and analysis validate the advantages of the proposed framework.


## What is the main contribution of this paper?

The main contribution of this paper is a new framework for video inpainting to recover missing or corrupted regions in video frames. The key ideas/contributions are:- A ConvLSTM-based architecture that can model the spatial-temporal consistency in videos for inpainting. This avoids limitations of prior 3D CNN methods in terms of memory, computational cost, and handling large motions.- A robust optical flow generation module that produces accurate optical flow from incomplete frames using two flow estimation branches. This enables handling of large motions. - The flow blending network learns to fuse the two estimated optical flows into a robust flow to guide the ConvLSTM module.- The method can handle videos of arbitrary sizes and lengths efficiently in a streaming manner, unlike prior fixed-size approaches.- Experiments on two datasets with different mask types show the proposed method outperforms prior state-of-the-art, especially on complex videos with large motions. - Ablation studies validate the contributions of different components like the ConvLSTM, flow blending, etc.In summary, the key contribution is a new deep learning framework for video inpainting that combines ConvLSTM and robust optical flow estimation to efficiently handle complex real videos of arbitrary sizes and lengths. The robust flow generation and blending are important to enable handling of large motions.
