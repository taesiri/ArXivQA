# [Learning a Low-Rank Feature Representation: Achieving Better Trade-Off   between Stability and Plasticity in Continual Learning](https://arxiv.org/abs/2312.08740)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a high-quality paragraph summarizing the key points of the paper:

This paper proposes a novel training algorithm called LRFR to address the plasticity-stability dilemma in continual learning. The core idea is to learn a low-rank representation of past tasks' features to expand the null space for future task training, enhancing plasticity while preserving stability. Specifically, the authors first mathematically show the relationship between the null space dimension and the rank of past tasks' feature representations. Then a subset of neurons is judiciously selected through a sparsity penalty during pretraining to reduce the rank. Consequently, features of new tasks can be learned in a larger null space of old tasks to improve plasticity. Experiments on CIFAR and TinyImageNet demonstrate state-of-the-art performance. The proposed LRFR algorithm consistently outperforms existing methods by a large margin in terms of average accuracy and backward transfer. The results validate that the learned low-rank structure strikes an improved balance between stability and plasticity in continual learning.
