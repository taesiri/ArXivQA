# [Universal Backdoor Attacks](https://arxiv.org/abs/2312.00157)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem Statement:
- Existing backdoor attacks on image classifiers target a single predetermined class. However, classifiers in practice may be deployed across many use cases and required to classify thousands of classes. 
- Naively targeting each class independently requires poisoning a large portion (>10%) of the training data and is thus infeasible. 

Proposed Solution:
- Introduce "Universal Backdoor Attacks" which enable controlling misclassifications from any source class to any target class using a small increase in poison samples.
- The key insight is that learning a trigger for one class makes the model more vulnerable to learning triggers targeting other classes ("inter-class poison transferability").  
- Triggers are crafted to exploit salient features discovered from a surrogate model. Directions in the classifier's latent space are encoded as binary strings. Each direction corresponds to a class, encoded as a trigger pattern.
- At inference time, target a class by mapping its label to the corresponding latent direction using the surrogate, retrieving the binary code, and generating the encoded trigger to apply on the image.

Main Contributions:
- Show Universal Backdoors are a tangible threat, allowing control of classifiers with thousands of classes by poisoning only 0.15% of training data.
- Introduce a technique to manufacture universal trigger patterns that exploit inter-class poison transferability.
- Demonstrate effectiveness and robustness of attacks by controlling ImageNet classifiers with up to 6000 classes, and show resistance against several defenses.

The key implication is that when training classifiers on web-scale datasets, the integrity of the entire dataset must be protected from data poisoning, not just individual sensitive classes. This is due to the transferability of poison samples between classes.
