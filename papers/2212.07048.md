# [PD-Quant: Post-Training Quantization based on Prediction Difference   Metric](https://arxiv.org/abs/2212.07048)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is: How can we improve post-training quantization (PTQ) methods to achieve higher accuracy, especially in extremely low-bit settings? The key hypotheses behind the proposed PD-Quant method appear to be:1) Using prediction difference (PD) as a metric to determine quantization parameters can better optimize for the final task accuracy compared to commonly used local metrics like MSE. 2) Regularization can help address overfitting issues in PTQ caused by limited calibration data.3) Adjusting the activation distribution of the calibration data (distribution correction) to match batch norm statistics can improve generalization.In summary, the main research questions are around how to improve PTQ accuracy in low-bit regimes, with a focus on using global prediction difference, regularization techniques, and distribution correction to achieve this. The effectiveness of the proposed PD-Quant method in pushing accuracy in 2-bit quantization seems to validate these hypotheses.


## What is the main contribution of this paper?

The main contributions of this paper are:1. It analyzes the limitations of using local metrics like MSE and cosine distance for determining quantization parameters in post-training quantization (PTQ). The paper shows these metrics can lead to suboptimal quantization scaling factors. 2. It proposes a new method called PD-Quant that uses a global prediction difference (PD) loss to determine better quantization parameters. The PD loss compares the prediction difference between the full-precision and quantized models.3. PD-Quant optimizes both quantization scaling factors and rounding values to minimize the PD loss while using regularization techniques to avoid overfitting to the small calibration dataset. 4. It introduces a distribution correction (DC) method to adjust the activation distribution of the calibration data to match the batch normalization statistics of the full training set. This improves generalizability.5. Experiments show PD-Quant achieves state-of-the-art accuracy for PTQ, especially in very low precision settings like 2-bit weights and activations. For example, it improves top-1 accuracy on ImageNet for ResNet-18 2/2-bit from 51.42% to 53.14%.In summary, the key innovation is using a global PD loss instead of local losses for determining quantization parameters in PTQ, along with techniques to avoid overfitting. This provides significant accuracy improvements in low-precision quantized models.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper proposes PD-Quant, a post-training quantization method that determines quantization parameters by using the prediction difference between the full-precision and quantized models rather than just the layer-wise feature difference, and introduces distribution correction to alleviate overfitting to the small calibration set.
