# [Vision Transformer with Deformable Attention](https://arxiv.org/abs/2201.00520)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
Existing vision transformers like ViT suffer from excessive memory and computational costs due to dense attention. Sparse attention methods like in Swin Transformer are data-agnostic and may fail to focus on relevant regions. Ideally, the attention should be flexible to focus on important regions in a data-dependent manner.  

Proposed Solution:
The paper proposes a Deformable Attention Transformer (DAT) with a deformable self-attention module that focuses on relevant regions in images in a data-dependent manner. 

Key ideas:
- Learn a small set of query-agnostic 2D offsets to shift the reference grid points towards important regions based on the input image. This results in shifted keys and values focused on important regions.
- Use bilinear interpolation for differentiable sampling of features at the shifted grid locations.
- Adopt deformable relative position encodings to capture relationships between tokens at continuous deformed locations.  
- Arrange DAT in a hierarchical architecture with deformable attention only in later stages for efficiency.

Main Contributions:
- Proposes the first deformable attention vision backbone that can focus flexibly on important regions.
- Achieves consistently better performance than baselines like Swin on image classification (ImageNet), object detection (COCO) and segmentation (ADE20K) with similar computation cost.  
- Visualizations confirm that deformable attention focuses keys on foreground objects based on the input image content.
- Simple, efficient design that introduces deformable mechanism into vision transformer backbones for the first time as a basic building block.

In summary, the paper presents an efficient deformable attention design for vision transformers that can focus flexibly on important regions in a data-dependent manner and consistently outperforms prior arts across tasks. The visualizations also confirm that deformable attention attends to relevant image regions automatically.
