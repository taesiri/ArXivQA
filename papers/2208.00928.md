# [OmniCity: Omnipotent City Understanding with Multi-level and Multi-view   Images](https://arxiv.org/abs/2208.00928)

## What is the central research question or hypothesis that this paper addresses?

 The central research question of this paper is how to build an omnipotent city dataset with multi-level and multi-view images to enable a variety of tasks for city understanding, reconstruction and simulation. 

Specifically, the paper aims to address the limitations of existing datasets by proposing a new dataset called OmniCity that contains:

- Multi-view satellite images as well as street-level panorama and mono-view images from the same geo-locations in New York City.

- Over 100K pixel-wise annotated images with richer annotation types (e.g. building footprint, height, plane/instance segmentation, land use categories) compared to other datasets. 

- More geo-locations (25K) compared to other multi-view datasets like TorontoCity.

- An efficient annotation pipeline that leverages existing satellite-level labels and view transformations to reduce manual labeling efforts.

The goal is to provide a dataset that can support multiple tasks like segmentation, detection, matching, reconstruction etc. for more holistic city understanding and modeling, overcoming limitations of existing datasets. The paper demonstrates the potential of OmniCity through benchmark experiments on tasks like segmentation and height estimation.


## What is the main contribution of this paper?

 The main contributions of this paper are:

1. Proposing a novel pipeline for efficiently producing diverse pixel-wise annotations on street-level panorama and mono-view images by leveraging existing satellite-level labels and view transformation relations.

2. Building the OmniCity dataset containing over 100K well-aligned satellite and street-level images with richer annotations and more views compared to existing datasets. It includes multi-view satellite images, street-level panorama images, and mono-view images from 25K geo-locations in New York City.

3. Providing extensive baselines and benchmarks for a variety of tasks including building footprint extraction, height estimation, plane/instance/fine-grained segmentation on both satellite and street-level images. 

4. Conducting comprehensive analysis on the impact of view, performance of different methods, and limitations of current methods.

5. Discussing the potential of OmniCity for facilitating new tasks and algorithms for large-scale city understanding, reconstruction and simulation using multi-level and multi-view imagery.

In summary, the key contribution is the proposal of the large-scale OmniCity dataset with diverse imagery and annotations to enable more omnipotent city understanding through a multitude of tasks and views. The efficient annotation pipeline, extensive benchmarks, and results analysis also demonstrate the dataset's usefulness.
