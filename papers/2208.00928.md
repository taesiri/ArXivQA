# [OmniCity: Omnipotent City Understanding with Multi-level and Multi-view   Images](https://arxiv.org/abs/2208.00928)

## What is the central research question or hypothesis that this paper addresses?

 The central research question of this paper is how to build an omnipotent city dataset with multi-level and multi-view images to enable a variety of tasks for city understanding, reconstruction and simulation. 

Specifically, the paper aims to address the limitations of existing datasets by proposing a new dataset called OmniCity that contains:

- Multi-view satellite images as well as street-level panorama and mono-view images from the same geo-locations in New York City.

- Over 100K pixel-wise annotated images with richer annotation types (e.g. building footprint, height, plane/instance segmentation, land use categories) compared to other datasets. 

- More geo-locations (25K) compared to other multi-view datasets like TorontoCity.

- An efficient annotation pipeline that leverages existing satellite-level labels and view transformations to reduce manual labeling efforts.

The goal is to provide a dataset that can support multiple tasks like segmentation, detection, matching, reconstruction etc. for more holistic city understanding and modeling, overcoming limitations of existing datasets. The paper demonstrates the potential of OmniCity through benchmark experiments on tasks like segmentation and height estimation.
