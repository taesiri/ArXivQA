# CoCon: A Self-Supervised Approach for Controlled Text Generation

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, it appears the central research question is how to control the content of text generated by large pretrained language models at a more fine-grained, word/phrase level. The authors motivate the need for more precise content control beyond just high-level attributes like topic and sentiment. They propose an approach called Content-Conditioner (CoCon) to incorporate target content into the generated text by conditioning on that content input. Their core hypothesis seems to be that CoCon can help guide a language model's output text to include specific words/phrases by taking that target content as input.The main research questions around CoCon that the paper seems to address are:- Can CoCon effectively incorporate target content words/phrases into generated text in a smooth, fluent way?- Can CoCon control high-level attributes like topic and sentiment in a zero-shot manner by conditioning on words related to those attributes? - How does CoCon compare to other controlled text generation methods in terms of content similarity and attribute relevance?- Can CoCon condition on multiple content inputs simultaneously to control different aspects of the generated text?- How robust is CoCon's content control across varying strengths of the conditioning through the content bias term?So in summary, the central research focus appears to be investigating how well CoCon can control the content of text at a finer granularity than prior work through conditioning the language model's generation process directly on target content inputs.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contribution appears to be proposing Content-Conditioner (CoCon) as an approach for more fine-grained control over text generation with pretrained language models. Specifically, the key aspects of the contribution seem to be:- CoCon allows conditioning the text generation on desired content at a more fine-grained, word/phrase level rather than just high-level attributes like topic or sentiment. - The CoCon module is trained in a self-supervised manner on text samples from the pretrained LM itself, avoiding the need for labeled data.- The training uses techniques like self reconstruction, cycle reconstruction, and adversarial losses to teach CoCon to incorporate desired content into fluent generated text.- CoCon enables control over both content and high-level attributes like topic and sentiment in a complementary manner. It can also condition on multiple content inputs.- CoCon's modular architecture means it can be flexibly combined with various pretrained LMs without extensive retraining or finetuning of the LM itself.In summary, the core contribution appears to be proposing and demonstrating a method to achieve more fine-grained control over pretrained language model text generation through content conditioning, in a flexible and self-supervised manner.
