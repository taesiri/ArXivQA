# [Lossless Compression with Probabilistic Circuits](https://arxiv.org/abs/2111.11632v2)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the main research question is: can Probabilistic Circuits (PCs) be effectively used as the backbone model for neural lossless image compression? 

More specifically, the authors investigate whether PCs can achieve competitive compression performance in terms of compression rate and speed compared to existing neural compression methods like Variational Autoencoders (VAEs) and normalizing flows. 

The key hypotheses are:

- PCs can be scaled up to model complex image distributions and achieve good likelihoods on benchmark datasets like MNIST and ImageNet.

- The structured and tractable nature of PCs allows developing efficient compression and decompression algorithms that have provably logarithmic time complexity. 

- When integrated with normalizing flows, PCs can significantly improve compression performance over just using normalizing flows alone.

So in summary, the central research question is assessing the potential of PCs for neural lossless compression, in terms of model expressiveness, compression rate, and computational efficiency. The key hypotheses aim to demonstrate the strengths of PCs over existing compression methods on these metrics.


## What is the main contribution of this paper?

 Based on my reading, the main contributions of this paper are:

1. It proposes using Probabilistic Circuits (PCs) as backbone models for lossless compression. PCs are a class of tractable probabilistic models that enable efficient marginal inference. 

2. It develops an efficient compression and decompression algorithm for PCs that takes advantage of their ability to compute arbitrary marginal probabilities quickly. The algorithm is proven to have time complexity O(log(D) * |p|) where D is the dimensionality and |p| is the PC model size.

3. It shows how to scale up the training of PCs like Hidden Chow-Liu Trees (HCLTs) to achieve competitive compression performance on datasets like MNIST and EMNIST. The proposed PC compressor achieves near state-of-the-art bitrates while being much faster than neural compressors like IDF and BitSwap.

4. It demonstrates how PCs can be naturally integrated as prior distributions in existing neural compression frameworks like normalizing flows to improve their compression performance on natural image datasets.

In summary, the main contribution is proposing the use of PCs for lossless compression, developing an efficient coding scheme that exploits their tractability, scaling up PC training to achieve good compression rates, and showing they can enhance existing neural compression methods when used as priors. The key insight is that model tractability matters for efficient compression alongside expressiveness.
