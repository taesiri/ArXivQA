# [Linear Attention via Orthogonal Memory](https://arxiv.org/abs/2312.11135)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Most existing efficient attention mechanisms suffer from an "efficiency degradation" problem when applied to causal language modeling, where their complexity increases from linear to quadratic. This hinders their applicability for long-range language models.
- The problem becomes even more serious for the challenging task of language modeling with unbounded contexts.

Proposed Solution - Linear Attention via Orthogonal Memory (LAVO):

- Employs orthogonal decomposition to compress the context into a fixed-size orthogonal memory. This maximizes distinguishability among the bounded memory units to effectively store global information.

- Further amplifies fine-grained local information by dissecting the context into windows and performing local self-attention within each window before compressing into orthogonal memory. 

- Embeds relative position encoding to improve extrapolation ability on longer contexts.

Main Contributions:

- Proposes LAVO attention to achieve strong performance while maintaining linear complexity even for causal language modeling and unbounded contexts.

- Extensive experiments covering NLP, speech, vision and time series forecasting tasks demonstrate efficiency and efficacy of LAVO in both self-attention and cross-attention settings.

- Significantly outperforms prior linear attention methods in causal language modeling efficiency, extrapolation ability and being the only method that can process 128K length contexts without IO optimization.

- Strong performance in other tasks - state-of-the-art in causal speech synthesis and non-causal summarization. Competitive results in non-causal speech synthesis and best results for cross-attention tasks.

In summary, the paper makes important contributions in developing an efficient attention mechanism to handle long contexts while preserving linear complexity, with broad applicability demonstrated across diverse tasks.
