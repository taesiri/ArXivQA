# [Visual Reinforcement Learning with Imagined Goals](https://arxiv.org/abs/1807.04742)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the abstract and introduction, the central research question this paper addresses is: How can we devise a practical reinforcement learning algorithm that acquires general-purpose skills by setting its own goals and learning from raw sensory inputs like images?The key ideas proposed to address this question are:- Using unsupervised representation learning (variational autoencoders) to obtain a latent embedding of high-dimensional observations like images. - Using this learned embedding to sample goals for "self-supervised practice", provide rewards based on latent distances, and represent goals/states.- Combining goal-conditioned reinforcement learning with off-policy algorithms and retroactive goal relabeling for efficient learning.So in summary, the central hypothesis is that by jointly learning latent representations and goal-conditioned policies, RL agents can learn complex skills directly from high-dimensional observations like images through self-supervised practice, without needing hand-specified rewards or state representations. The method proposed here, called RIG (Reinforcement Learning with Imagined Goals), aims to demonstrate this hypothesis.


## What is the main contribution of this paper?

This paper presents a reinforcement learning method called RIG (Reinforcement Learning with Imagined Goals) for training policies that can accomplish visual goal-reaching tasks specified by raw image observations. The main contributions are:- Using a variational autoencoder (VAE) to learn a latent representation of images that is used for multiple purposes in RL: generating goals for self-supervised practice, providing rewards, and embedding the state/goal images. - A goal relabeling strategy that samples new goals from the VAE latent space to augment off-policy RL data. This improves sample efficiency over prior goal relabeling methods like HER.- Showing that the method can solve simulated vision-based control tasks directly from images, without access to underlying state information. It also scales to real-world robotic tasks.In summary, the key contribution is a framework that combines representation learning with off-policy reinforcement learning to efficiently learn goal-conditioned policies from raw visual inputs, without any manually specified rewards or instrumentation. The VAE representation is critical for enabling imagined goals, implicit rewards, and data augmentation.
