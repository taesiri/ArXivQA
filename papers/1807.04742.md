# [Visual Reinforcement Learning with Imagined Goals](https://arxiv.org/abs/1807.04742)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the abstract and introduction, the central research question this paper addresses is: How can we devise a practical reinforcement learning algorithm that acquires general-purpose skills by setting its own goals and learning from raw sensory inputs like images?The key ideas proposed to address this question are:- Using unsupervised representation learning (variational autoencoders) to obtain a latent embedding of high-dimensional observations like images. - Using this learned embedding to sample goals for "self-supervised practice", provide rewards based on latent distances, and represent goals/states.- Combining goal-conditioned reinforcement learning with off-policy algorithms and retroactive goal relabeling for efficient learning.So in summary, the central hypothesis is that by jointly learning latent representations and goal-conditioned policies, RL agents can learn complex skills directly from high-dimensional observations like images through self-supervised practice, without needing hand-specified rewards or state representations. The method proposed here, called RIG (Reinforcement Learning with Imagined Goals), aims to demonstrate this hypothesis.


## What is the main contribution of this paper?

This paper presents a reinforcement learning method called RIG (Reinforcement Learning with Imagined Goals) for training policies that can accomplish visual goal-reaching tasks specified by raw image observations. The main contributions are:- Using a variational autoencoder (VAE) to learn a latent representation of images that is used for multiple purposes in RL: generating goals for self-supervised practice, providing rewards, and embedding the state/goal images. - A goal relabeling strategy that samples new goals from the VAE latent space to augment off-policy RL data. This improves sample efficiency over prior goal relabeling methods like HER.- Showing that the method can solve simulated vision-based control tasks directly from images, without access to underlying state information. It also scales to real-world robotic tasks.In summary, the key contribution is a framework that combines representation learning with off-policy reinforcement learning to efficiently learn goal-conditioned policies from raw visual inputs, without any manually specified rewards or instrumentation. The VAE representation is critical for enabling imagined goals, implicit rewards, and data augmentation.


## How does this paper compare to other research in the same field?

Based on my understanding of the paper, here is a comparison to other related work in visual reinforcement learning:- Compared to standard model-free RL algorithms like DDPG and TD3 that operate directly on raw pixel observations, this method is substantially more sample efficient and can learn good policies on more complex visual tasks. The key differences are the use of representation learning with the VAE and the imagined goal sampling.- Compared to prior work on learning latent variable models like beta-VAEs and combining them with RL like in World Models, this method does not require access to rewards or expert demonstrations during training. It is a fully unsupervised and self-supervised approach.- Compared to goal-conditioned RL methods like HER and UVFA, this method can handle raw image observations as goals and states. It does not need ground truth state information. The latent relabeling allows for more efficient off-policy learning.- Compared to methods that also use imagined goals like RIG, this provides a more general algorithm that can handle raw image observations without needing instrumentation during training. The imagined goals allow automatic curriculum generation.- Compared to prior model-based RL methods, this is a model-free approach that directly learns the policy and value function. It avoids issues with model bias and drift.- Compared to methods that learn disentangled representations, this uses a simpler VAE model but could incorporate more advanced representation learning.Overall, the main advances are in combining unsupervised representation learning with off-policy goal conditioning and imagined goals for efficient learning of complex vision-based robotic skills. The experiments demonstrate substantially improved performance over prior model-free and goal-conditioned RL algorithms on such tasks.
