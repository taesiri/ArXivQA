# [Feedback Efficient Online Fine-Tuning of Diffusion Models](https://arxiv.org/abs/2402.16359)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Diffusion models can effectively model complex data distributions like images, proteins, molecules etc. However, in many applications we want to fine-tune them to generate high-reward samples that maximize certain desirable properties like aesthetic quality of images or bioactivity of molecules.
- This can be posed as a reinforcement learning problem to fine-tune the model to maximize a reward function. But efficiently discovering high-reward samples is challenging since they may have low probability in the original distribution and there may be many invalid/infeasible samples.  
- The key research problem is how to efficiently explore the manifold of feasible high-reward samples in an online setting where obtaining feedback (rewards) from the real environment (like via wet lab experiments) is expensive.

Proposed Solution:
- The paper proposes a novel algorithm called SEIKO for feedback-efficient online fine-tuning of diffusion models to optimize a reward function.
- The core ideas are:
   (a) Interleave reward learning and diffusion model updates without querying new feedback when updating models.
   (b) Use an uncertainty bonus and KL regularization term to balance exploration vs exploitation and constrain it to feasible regions.
- In each round, new samples are generated and their rewards queried. The reward model and uncertainty model are updated with this data.
- The diffusion model is then updated by optimizing an objective with 3 terms: (i) optimistic rewards, (ii) KL penalty wrt original pretrained model to avoid infeasible samples, (iii) KL penalty wrt previous model iteration to avoid large changes.
- This updated diffusion then focuses sampling in high-reward and novel feasible regions in next round.

Contributions:
- Proposes first algorithm for feedback-efficient RL-based fine-tuning of diffusion models with theoretical guarantees.
- Achieves state-of-the-art performance in optimizing rewards across applications in images, protein sequences and drug molecule generation while using fewer queries.
- Provides regret analysis to formally characterize sample efficiency.
- Empirically demonstrates effectiveness over strong baselines on three domains.

To summarize, the paper makes conceptual and technical contributions in developing a feedback-efficient online RL method specialized for diffusion models by using uncertainty bonuses and KL constraints to enable efficient exploration on feasibility manifolds.
