# [On-Policy Model Errors in Reinforcement Learning](https://arxiv.org/abs/2110.07985)

## What is the central research question or hypothesis that this paper addresses?

This paper addresses the key challenge of model bias in model-based reinforcement learning (MBRL). MBRL methods learn a model of the environment's dynamics from data, and then use that model to simulate experiences for training a policy. However, errors in the learned model tend to compound over multi-step predictions, leading to simulated data that diverges from the true environment. This model bias limits the performance of MBRL methods compared to model-free algorithms. The central hypothesis of this paper is that combining the learned model with a replay buffer of real on-policy transitions can help reduce model bias. The key idea is to use the model to predict how the observed transitions would change for a new state-action pair, rather than directly predicting the next state. This allows the method, called on-policy corrections (OPC), to retain the true on-policy data distribution while still generalizing to new actions via the model.The central research questions addressed are:- Can OPC provably reduce the on-policy model error, i.e. the difference in expected return between the real environment and model when simulating the current policy?- Does OPC lead to improved performance on challenging MBRL benchmarks compared to state-of-the-art methods?- How does data diversity (rollout length, total transitions) affect OPC compared to model-free algorithms?The theoretical analysis shows OPC eliminates the on-policy error for deterministic policies, while experiments demonstrate superior data-efficiency and robustness over strong baselines on continuous control tasks.
