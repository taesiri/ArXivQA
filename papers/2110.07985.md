# [On-Policy Model Errors in Reinforcement Learning](https://arxiv.org/abs/2110.07985)

## What is the central research question or hypothesis that this paper addresses?

This paper addresses the key challenge of model bias in model-based reinforcement learning (MBRL). MBRL methods learn a model of the environment's dynamics from data, and then use that model to simulate experiences for training a policy. However, errors in the learned model tend to compound over multi-step predictions, leading to simulated data that diverges from the true environment. This model bias limits the performance of MBRL methods compared to model-free algorithms. The central hypothesis of this paper is that combining the learned model with a replay buffer of real on-policy transitions can help reduce model bias. The key idea is to use the model to predict how the observed transitions would change for a new state-action pair, rather than directly predicting the next state. This allows the method, called on-policy corrections (OPC), to retain the true on-policy data distribution while still generalizing to new actions via the model.The central research questions addressed are:- Can OPC provably reduce the on-policy model error, i.e. the difference in expected return between the real environment and model when simulating the current policy?- Does OPC lead to improved performance on challenging MBRL benchmarks compared to state-of-the-art methods?- How does data diversity (rollout length, total transitions) affect OPC compared to model-free algorithms?The theoretical analysis shows OPC eliminates the on-policy error for deterministic policies, while experiments demonstrate superior data-efficiency and robustness over strong baselines on continuous control tasks.


## What is the main contribution of this paper?

Based on my understanding, the main contribution of this paper is proposing a new method called "on-policy corrections" (OPC) for model-based reinforcement learning (MBRL). The key ideas are:- MBRL methods suffer from "model bias", where errors in the learned dynamics model compound over multi-step predictions and lead to poor performance. - The paper proposes to combine a learned dynamics model with a replay buffer of observed transitions. The learned model is used to predict how the transitions would change for new state-action pairs. This allows generating more realistic/unbiased data while retaining the model's generalization.- Theoretically, it is shown that OPC can recover the true state distribution and eliminate the on-policy model error that exists in the policy improvement bound.- For deterministic policies, OPC achieves zero model error regardless of the actual model quality. For stochastic policies, the error depends on both model quality and policy variance.- Empirically, OPC is evaluated on continuous control tasks and outperforms a state-of-the-art MBRL method especially on difficult environments like PyBullet. OPC also increases robustness to different state representations.In summary, the key contribution is proposing OPC as a way to combine learned models and replay buffers in MBRL to achieve lower model bias, better generalization, and improved performance over pure model-based or model-free approaches. The method is supported by both theoretical analysis and experimental results.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence TL;DR summary of the paper:The paper proposes a new method called on-policy corrections (OPC) that combines a learned dynamics model with real transition data in model-based reinforcement learning to reduce model bias and enable more accurate long-term predictions.


## How does this paper compare to other research in the same field?

This paper introduces a new method called "on-policy corrections" (OPC) for model-based reinforcement learning. Here is a brief comparison to related work:- Combining model-free and model-based RL: Many prior works have aimed to combine model-free and model-based approaches, such as guided policy search, PETS, STEVE, and MBVE. The key difference in OPC is the specific way it combines a learned model with real transition data to correct on-policy biases.- Model bias in MBRL: Model bias is a known issue in MBRL, and methods like MBPO use short rollouts from real states to mitigate compounding errors. OPC provides a principled way to leverage real transitions to eliminate on-policy model bias.- Theoretical analysis: The paper provides theoretical analysis bounding the on-policy error with OPC, relating it to policy variance. This analysis helps motivate and explain the approach. Prior MBRL theory has focused more on off-policy errors or algorithmic improvement guarantees.- Hyperparameter tuning: Many prior MBRL methods introduce extra hyperparameters to tune the combination of model-free and model-based components (e.g. mix-in ratio). A strength of OPC is it does not require tuning new hyperparameters.- Connection to iterative learning control: The paper draws an interesting connection between OPC and methods from ILC. Making this connection explicit helps relate ideas between the fields.In summary, OPC introduces a novel, theoretically motivated way to combine learned models and real data in MBRL to address model bias, without introducing major new hyperparameters to tune. The analysis and connections to ILC help provide new insights on this approach.


## What future research directions do the authors suggest?

Here are a few key future research directions suggested by the authors:- Developing methods to better understand and quantify model generalization in MBRL. They note this as an important open area that would help further improve MBRL algorithms.- Extending OPC to partially observed environments. The current method applies directly only to MDPs where full state is observed. Adapting the ideas to partial observability is noted as an interesting direction. - Looking at combining OPC with global exploration schemes like HUCRL. OPC is currently more suited to local policy optimization where the policy changes slowly over time. Combining it with more global exploration is noted as a direction.- Extending OPC to handle heteroscedastic noise, where the transition noise can vary significantly across different states and actions. This may require additional assumptions to distinguish model error from state-dependent noise.- Further theoretical analysis to better understand compounding errors with OPC and potentially incorporate branched rollouts to limit horizon over which errors accumulate.- Empirical analysis of how robust OPC is to off-policy data and changes in the data distribution over iterations. The paper currently includes some initial analysis but more investigation is noted as useful.- Comparisons to recent related MBRL methods like MAAC and MPO that were published concurrently. The paper focuses comparisons to PETS and MBPO but notes comparisons to these newer methods could be interesting.So in summary, extending OPC to more complex RL settings, better understanding model generalization, and further theoretical and empirical analysis of the method are highlighted as key future directions by the authors.
