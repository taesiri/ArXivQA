# [Self-Supervised Generalisation with Meta Auxiliary Learning](https://arxiv.org/abs/1901.08933)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be:How can we improve the ability of a primary supervised learning task to generalize, by using auxiliary tasks, without needing any additional labeled data for the auxiliary tasks? The key hypothesis appears to be:By meta-learning to automatically generate optimal labels for an auxiliary task, a primary supervised learning task can be improved without requiring access to any further labeled data.Specifically, the paper proposes a method called Meta AuXiliary Learning (MAXL) which uses two neural networks - a multi-task network that trains on the primary task and auxiliary task, and a label-generation network that generates the labels for the auxiliary task. The core idea is to use the performance of the multi-task network on the primary task to improve the auxiliary labels generated for the next iteration. So there is a feedback loop between the two networks.In this way, the paper explores how optimal auxiliary tasks can be learned in a completely self-supervised manner, removing the need for manual definition or labeling of auxiliary tasks. The central hypothesis is that this meta-learning approach can improve generalization of the primary task, even without any extra labeled data. The experiments aim to validate this hypothesis.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contribution seems to be proposing a new meta-learning algorithm called Meta AuXiliary Learning (MAXL) for automatically generating optimal auxiliary labels to improve the performance of a primary supervised learning task, without needing any additional labeled data. The key ideas are:- Train two neural networks jointly - a multi-task network that trains on the primary task and auxiliary task, and a label-generation network that generates the labels for the auxiliary task.- The loss for the label-generation network incorporates the loss of the multi-task network on the primary task. So the label-generation network learns to produce auxiliary labels that improve performance on the primary task. - This interaction between the two networks allows the model to meta-learn good auxiliary tasks in a self-supervised way, without needing manually-labeled auxiliary data.- They introduce techniques like Masked Softmax and an entropy regularization loss to help the model learn useful auxiliary labels.The main experimental results are:- MAXL improves image classification performance over single-task learning on 7 datasets, without any extra labeled data.- It outperforms other baselines for generating auxiliary labels like random labels or k-means clustering.- It is competitive with manual human-provided auxiliary labels on CIFAR-100, despite not actually using those human labels.So in summary, the key contribution is proposing MAXL as a way to automate and improve auxiliary learning in a self-supervised manner, removing the need for manual definition and labeling of auxiliary tasks.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points of the paper:The paper proposes a new self-supervised method called Meta Auxiliary Learning (MAXL) to automatically generate optimal auxiliary labels to improve the generalization performance of a primary image classification task, without needing any extra labeled data beyond what is required for the primary task.


## How does this paper compare to other research in the same field?

Here are a few thoughts on how this paper compares to other research in multi-task learning and auxiliary learning:- The key novelty of this paper is the meta-learning approach to automatically generate labels for the auxiliary tasks, without needing any manually labeled data. This removes the reliance on domain expertise or extra labeled data that most prior work on supervised auxiliary learning requires. It also provides more flexibility than prior work on unsupervised auxiliary learning which is limited to certain predefined tasks.- The proposed MAXL algorithm has some similarities to other meta-learning methods like MAML, which also optimize model parameters to be easily adaptable for a new task through a double gradient update. However, MAXL is novel in using this meta-learning specifically for the purpose of creating optimal auxiliary tasks.- Compared to related work like that of Zhang et al. on meta-learned auxiliary data selection, MAXL generates the auxiliary data itself rather than selecting from a pool of candidates. This removes the need for any manually labeled auxiliary data.- The experiments systematically compare MAXL against single-task baselines and other approaches for auxiliary label generation. The results demonstrate clear improvements from MAXL, validating its ability to automatically produce useful auxiliary supervision without human input.- The visualizations provide some interesting insights into the auxiliary labels learned by MAXL. They find only some human-interpretable structure, suggesting the auxiliary tasks capture shared structure to benefit the primary task in a way that may not be intuitively understandable.Overall, this paper proposes a novel meta-learning framework for fully automated auxiliary learning, removing key limitations of prior work. The strength of the results across multiple datasets validate that the approach can automatically generate auxiliary supervision that improves primary task performance. It's an intriguing new direction for making models generalize better without additional human input.
