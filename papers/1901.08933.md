# [Self-Supervised Generalisation with Meta Auxiliary Learning](https://arxiv.org/abs/1901.08933)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be:How can we improve the ability of a primary supervised learning task to generalize, by using auxiliary tasks, without needing any additional labeled data for the auxiliary tasks? The key hypothesis appears to be:By meta-learning to automatically generate optimal labels for an auxiliary task, a primary supervised learning task can be improved without requiring access to any further labeled data.Specifically, the paper proposes a method called Meta AuXiliary Learning (MAXL) which uses two neural networks - a multi-task network that trains on the primary task and auxiliary task, and a label-generation network that generates the labels for the auxiliary task. The core idea is to use the performance of the multi-task network on the primary task to improve the auxiliary labels generated for the next iteration. So there is a feedback loop between the two networks.In this way, the paper explores how optimal auxiliary tasks can be learned in a completely self-supervised manner, removing the need for manual definition or labeling of auxiliary tasks. The central hypothesis is that this meta-learning approach can improve generalization of the primary task, even without any extra labeled data. The experiments aim to validate this hypothesis.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contribution seems to be proposing a new meta-learning algorithm called Meta AuXiliary Learning (MAXL) for automatically generating optimal auxiliary labels to improve the performance of a primary supervised learning task, without needing any additional labeled data. The key ideas are:- Train two neural networks jointly - a multi-task network that trains on the primary task and auxiliary task, and a label-generation network that generates the labels for the auxiliary task.- The loss for the label-generation network incorporates the loss of the multi-task network on the primary task. So the label-generation network learns to produce auxiliary labels that improve performance on the primary task. - This interaction between the two networks allows the model to meta-learn good auxiliary tasks in a self-supervised way, without needing manually-labeled auxiliary data.- They introduce techniques like Masked Softmax and an entropy regularization loss to help the model learn useful auxiliary labels.The main experimental results are:- MAXL improves image classification performance over single-task learning on 7 datasets, without any extra labeled data.- It outperforms other baselines for generating auxiliary labels like random labels or k-means clustering.- It is competitive with manual human-provided auxiliary labels on CIFAR-100, despite not actually using those human labels.So in summary, the key contribution is proposing MAXL as a way to automate and improve auxiliary learning in a self-supervised manner, removing the need for manual definition and labeling of auxiliary tasks.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points of the paper:The paper proposes a new self-supervised method called Meta Auxiliary Learning (MAXL) to automatically generate optimal auxiliary labels to improve the generalization performance of a primary image classification task, without needing any extra labeled data beyond what is required for the primary task.
