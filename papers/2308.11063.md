# [MetaGCD: Learning to Continually Learn in Generalized Category Discovery](https://arxiv.org/abs/2308.11063)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the main research question seems to be: How can we enable a model to continually discover and learn new object categories from unlabeled data over time, while maintaining performance on previously learned categories?Specifically, the paper proposes a new problem setting called "Continuous Generalized Category Discovery" (C-GCD) where a model encounters a continual stream of unlabeled data containing both known classes it has seen before, as well as novel classes it has not seen. The goal is for the model to incrementally expand its knowledge by discovering and learning these new classes over time, without forgetting the old ones (avoiding catastrophic forgetting). To address this, the paper presents a meta-learning based approach called MetaGCD. The key ideas are:- Using the offline labeled data to simulate the online incremental learning process via episodic training. This aligns the offline training objective with the online evaluation protocol to optimize for incremental novel class discovery without forgetting. - Employing contrastive learning on the unlabeled data to discriminate between instances and absorb correlated samples. A soft neighborhood contrastive method is proposed to adaptively mine positive samples.- Formulating a bi-level optimization strategy based on MAML to directly learn how to incrementally discover novel classes from unlabeled data while minimizing forgetting of old classes.In summary, the main research question is how to achieve continual generalized category discovery (C-GCD) in a realistic incremental learning setting, which the paper addresses through a meta-learning approach to optimize directly for the target objectives.


## What is the main contribution of this paper?

This paper proposes a new method called MetaGCD for continual generalized category discovery (C-GCD). The key contributions are:- It considers a realistic setting for real-world applications where a model trained on pre-defined classes continually encounters unlabeled data containing both known and novel classes. The goal is to incrementally discover novel classes while maintaining performance on known classes. - It proposes a meta-learning approach to fully exploit the labeled data during offline training. Instead of just pre-training a model representation, it trains an initialization directly optimized for the downstream continual learning task. This aligns the offline training objective with the online evaluation protocol.- It proposes a soft neighborhood contrastive learning method to explore relationships between data instances and adaptively absorb more correlated samples as soft positives. This facilitates novel class discovery.- It establishes strong baselines and demonstrates superior performance of the proposed MetaGCD method through extensive experiments on CIFAR and Tiny ImageNet datasets. The method achieves much higher accuracy, especially on detecting novel classes, with less hand-engineered components compared to prior arts.In summary, the key novelty is in formulating a more realistic continual learning setting and developing an end-to-end meta-learning solution optimized for this setting. The experiments demonstrate the effectiveness of MetaGCD for incremental novel class discovery without forgetting known classes.
