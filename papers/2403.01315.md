# [Near-optimal Per-Action Regret Bounds for Sleeping Bandits](https://arxiv.org/abs/2403.01315)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- The paper studies the sleeping bandits problem, where in each round a subset of arms are active/available and the learner only competes with the best available arm.
- Prior works obtained suboptimal $O(K\sqrt{TA\ln K})$ regret bounds by reducing to sleeping experts and minimizing sleeping internal regrets. 
- The goal is to directly minimize the per-action pseudo/high-probability regret and obtain near optimal bounds.

Proposed Solutions and Contributions:

1. SB-EXP3 Algorithm:
- Adapts EXP3 to sleeping bandits using a new potential function over cumulative weights of all previously active arms.  
- Obtains near optimal $O(\sqrt{TA\ln G_T})$ regret bound where $G_T\leq K$ is number of distinct arms.  
- First high probability regret bound for sleeping bandits.
- Algorithm and analysis extend to generalized setting of experts reporting confidences.

2. FTARL Algorithm: 
- Follow-The-Active-and-Regularized Leader approach gives $O(\sqrt{T\sqrt{AK}})$ regret using Tsallis entropy.
- Strictly generalizes non-sleeping bandit algorithms/analyses to sleeping setting.

3. Bandits with Sleeping Experts (SE-EXP4 Algorithm):
- Adapts EXP4 to handle expert advice from sleeping experts. 
- Shows implications on deriving adaptive/tracking regret bounds for non-sleeping bandits.

4. Lower bound:
- Shows inherent limitation that no algorithm can achieve both optimal and per-action sublinear regret bounds.

The algorithms directly minimize per-action regret and match minimax lower bounds up to logarithmic factors in the sleeping bandits setting. The analyses also imply new regret bounds in related non-sleeping bandit settings.


## Summarize the paper in one sentence.

 This paper derives near-optimal per-action regret bounds for sleeping bandits, where both the sets of available arms and their losses are chosen adversarially in each round.


## What is the main contribution of this paper?

 Based on my understanding, the main contributions of this paper are:

1. It derives near-optimal per-action regret bounds for sleeping bandits, where both the sets of available arms and their losses are chosen adversarially in each round. Specifically, it presents algorithms with $O(\sqrt{TA\ln K})$ and $O(\sqrt{T\sqrt{AK}})$ regret bounds.

2. It generalizes existing bandit algorithms like EXP3, EXP3-IX, and FTRL to the sleeping bandits setting. The bounds strictly generalize previous results in standard non-sleeping bandits.

3. It shows how the techniques for sleeping bandits can be used to derive adaptive and tracking regret bounds for standard bandits. This provides new proofs for some existing bounds.

4. It extends the results to bandits with advice from sleeping experts by generalizing the EXP4 algorithm. This also leads to new proofs of adaptive and tracking regret bounds in the standard setting.

5. It studies the bandit version of experts that report their confidences, handles it using sleeping bandits algorithms, and proves new confidence regret bounds.

6. It proves a lower bound showing that no algorithm can have both an optimal per-action regret and a per-action strongly adaptive regret sublinear in the number of active rounds for each arm.

In summary, the main contribution is advancing the theory and algorithms for sleeping bandits to obtain near-optimal bounds, while also showing implications and connections to related settings like standard bandits and bandits with expert advice.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and concepts are:

- Sleeping bandits/sleeping experts: A variant of the multi-armed bandit problem where the set of available arms varies over time. Only a subset of "awake" arms with losses are revealed in each round.

- Per-action regret: Regret notion that compares the cumulative loss of the learner against the loss of a single best arm over only the rounds when that arm was awake/active. 

- Minimax optimal regret: Regret that matches known lower bounds, up to constant factors.

- High probability regret bounds: With high probability over the randomness of the algorithm, the regret is upper bounded by the stated quantity.

- Adaptive regret: Regret over a contiguous time interval compared to a fixed arm over that interval.

- Tracking regret: Regret against a shifting sequence of arms.

- Generalized EXP3 and FTRL algorithms: Adaptations of the EXP3 and Follow-the-Regularized-Leader algorithms to the sleeping bandits setting. 

- Doubling trick: Method for dynamically tuning parameters like learning rate without knowing time horizon or other quantities ahead of time.

- Confidence regret: Regret notion when experts/arms report confidences instead of just being awake/asleep.

So in summary, key terms cover problem settings like sleeping bandits, performance metrics like per-action and adaptive regret, algorithms, analysis tools like high probability and doubling trick bounds, and extensions like confidence feedback.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 potential in-depth questions about the method proposed in this paper:

1. The paper proposes a novel algorithm called SB-EXP3 for sleeping bandits. How does SB-EXP3 differ from standard EXP3 and why are these differences necessary for the sleeping bandits setting?

2. One key aspect of SB-EXP3 is the use of the potential function $\tilde{Q}_t$. Explain the intuition behind using $\tilde{Q}_t$ instead of the normalization factor $W_t$, and how does this help bound the regret?

3. Lemma 3 provides an upper bound on the growth of the potential function $\tilde{Q}_t$. Walk through the key steps of the proof and explain how decomposing $\tilde{Q}_t$ by active and inactive arms leads to the final bound. 

4. How does the analysis of SB-EXP3 lead to both pseudo-regret and high probability regret bounds? What is the key property allowing this?

5. The paper shows that SB-EXP3 generalizes EXP3 bounds for non-sleeping bandits. What additional challenges arise in the analysis for sleeping bandits compared to non-sleeping bandits?

6. Explain the doubling trick used in Algorithm 2 and Theorem 5 to remove the dependency on knowing $G_T$ and $\sum_{t=1}^T A_t$ beforehand. What is the intuition behind applying a two-level doubling trick?

7. Walk through the analysis of FTARL and explain the motivation behind assigning non-zero losses to inactive arms. What role does Lemma 4 play here?

8. Compare the bounds of SB-EXP3 and FTARL. In what scenarios would you prefer one over the other and why?

9. Explain how the analysis of SB-EXP3 is adapted in Section 5 to provide bounds that depend on the cumulative confidence instead of number of rounds. What is the key insight enabling this?

10. The lower bound in Section 6 shows limitations on simultaneously attaining optimal and near-optimal per-action regret bounds. Interpret the implication of this lower bound constructively.
