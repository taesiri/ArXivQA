# [Sorted LLaMA: Unlocking the Potential of Intermediate Layers of Large   Language Models for Dynamic Inference Using Sorted Fine-Tuning (SoFT)](https://arxiv.org/abs/2309.08968)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the main research question this paper seeks to address is: How can we enhance the performance of large language models (LLMs) across layers to optimize their usage, leveraging their inherent modularity, without incurring substantial additional costs?More specifically, the key questions investigated are:i) Do the intermediate layers resulting from Supervised Fine-Tuning (SFT) of an LLM generate accurate and meaningful outputs? ii) Does SFT exhibit a sorted behavior, with later layers producing more accurate outputs than earlier layers?iii) How can we enhance this sorted behavior with minimal cost?The authors' main hypothesis seems to be that the conventional reliance solely on the last layer's outputs is suboptimal, and that the potential of intermediate layers can be unlocked to optimize LLM efficiency and performance across layers. They propose applying Sorted Fine-Tuning (SoFT) in place of SFT to test this hypothesis.In summary, this paper centers around investigating and improving the representation learning and generative capabilities of intermediate layers in LLMs through an efficient tuning approach, challenging the standard practice of only leveraging the last layer.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contributions appear to be:- Extending the SortedNet method for tuning auto-regressive language models for generative tasks by sharing a single LLM head layer among sub-models.- Generating 8 nested sub-models, ranging from 12 to 40 layers, from LLaMa2 13B by applying Sorted Fine-Tuning (SoFT) on the Stanford Alpaca dataset and at a cost equivalent to Supervised Fine-Tuning (SFT). - Evaluating the performance of the sub-models of a LLaMA2 13B and demonstrating the effectiveness of SortedNet tuning in enhancing the ability of intermediate layers for text generation through extensive evaluation.In summary, the key contribution is showing how the SortedNet approach can be adapted for generative language models like LLaMA to create efficient sub-models that unlock the potential of intermediate layers. This enables dynamic inference capabilities without any pretraining or modification of the original model architecture. The experiments on LLaMA 13B demonstrate the promise of this approach.
