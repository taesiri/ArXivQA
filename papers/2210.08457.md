# [Scratching Visual Transformer's Back with Uniform Attention](https://arxiv.org/abs/2210.08457)

## What is the central research question or hypothesis that this paper addresses?

 The central research question this paper seeks to address is: What is the role of the density of attention in Vision Transformers (ViTs)? 

Specifically, the authors examine whether ViT models learn dense or sparse attention maps, and hypothesize that ViTs have a preference for dense spatial interactions despite the difficulty of learning dense attention maps via gradient descent. To test this, they propose a simple module called Context Broadcasting (CB) that manually inserts uniform attention to supply additional dense interactions and observe if this helps ViT performance and learning.

The key hypothesis is that ViTs benefit from additional dense spatial interactions, even though dense attention maps are hard to optimize. By manually inserting uniform attention with the CB module, the authors aim to test if:

1) This reduces the density of the original attention maps, suggesting ViTs are trying hard to learn dense maps themselves. 

2) This increases model capacity and generalization ability, implying the extra dense interactions are beneficial.

In essence, the paper investigates the role of attention density in ViTs and hypothesizes dense interactions are vital yet difficult to learn, so manually adding them via the CB module should improve performance. The experiments aim to test this hypothesis.


## What is the main contribution of this paper?

 The main contribution of this paper is proposing a simple method called Context Broadcasting (CB) to inject dense spatial interactions into Vision Transformer (ViT) models. The key points are:

- The paper first analyzes the spatial interactions in ViT models and finds that they tend to learn dense attention maps, which are difficult to optimize. 

- The paper proposes to manually inject dense interactions by broadcasting the globally averaged token to every token. This Context Broadcasting (CB) module is very simple - just one line of code.

- Adding the CB module to ViT models is shown to reduce the density of the original attention maps. This allows the model to divert resources from learning dense attention and improves optimization and generalization.

- The CB module brings consistent gains across image classification on ImageNet and semantic segmentation on ADE20K with negligible computational overhead.

So in summary, the main contribution is a very simple and effective CB module to inject dense spatial interactions into ViTs, which improves their optimization, generalization, and performance. The simplicity and efficacy of the method is the key novelty.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence TL;DR of the paper:

The paper proposes a simple method called Context Broadcasting (CB) that improves Vision Transformers by explicitly supplying uniform attention to provide dense interactions between tokens, which reduces the model's need to learn dense attention maps that are difficult to optimize.
