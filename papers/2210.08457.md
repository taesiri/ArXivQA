# [Scratching Visual Transformer's Back with Uniform Attention](https://arxiv.org/abs/2210.08457)

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper seeks to address is: What is the role of the density of attention in Vision Transformers (ViTs)? Specifically, the authors examine whether ViT models learn dense or sparse attention maps, and hypothesize that ViTs have a preference for dense spatial interactions despite the difficulty of learning dense attention maps via gradient descent. To test this, they propose a simple module called Context Broadcasting (CB) that manually inserts uniform attention to supply additional dense interactions and observe if this helps ViT performance and learning.The key hypothesis is that ViTs benefit from additional dense spatial interactions, even though dense attention maps are hard to optimize. By manually inserting uniform attention with the CB module, the authors aim to test if:1) This reduces the density of the original attention maps, suggesting ViTs are trying hard to learn dense maps themselves. 2) This increases model capacity and generalization ability, implying the extra dense interactions are beneficial.In essence, the paper investigates the role of attention density in ViTs and hypothesizes dense interactions are vital yet difficult to learn, so manually adding them via the CB module should improve performance. The experiments aim to test this hypothesis.
