# [Retention Is All You Need](https://arxiv.org/abs/2304.03103)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the key research goal of this paper is to develop an explainable AI (XAI) based decision support system called HR-DSS to analyze employee attrition and provide insights to help with retention policies. Specifically, the paper proposes using machine learning models like XGBoost to predict employee attrition, and then leveraging explainable AI techniques like SHAP to interpret the predictions and important features driving attrition. The SHAP values are further processed to generate natural language explanations that provide justifications for attrition predictions. The overall vision is to build an interactive decision support system with explainable AI that can assist HR personnel in understanding factors causing employee attrition at both an individual and company-wide level. This system can then help HR units make informed decisions about retention policies and strategies to reduce attrition.In summary, the key research goals are:- Develop machine learning models to predict employee attrition- Apply explainable AI techniques like SHAP to interpret predictions- Generate natural language explanations from SHAP values- Build an interactive decision support system with explainable AI to aid HR retention policies- Provide individual and company-wide insights on attrition factors to assist HR decision making


## What is the main contribution of this paper?

The main contributions of this paper are:- The proposal of an HR decision support system (HR-DSS) that uses explainable AI to provide insights into employee attrition. - Conducting experiments with 8 machine learning models on the IBM HR Analytics Attrition Dataset and selecting XGB as the best performing model with 89.12% accuracy.- Employing the SHAP explainability method on the predictions of the XGB model to identify the key features contributing to employee attrition such as Overtime, StockOptionLevel and MonthlyIncome.- Integrating natural language generation using GPT-3 to generate explanations of the SHAP values in natural language that can be readily understood by HR personnel. - Developing an interactive explainer dashboard that allows HR to analyze the factors affecting attrition for individual employees and tweak parameters to see the effect on retention.- Demonstrating how adjusting dominant features of individual employees through "What-if analysis" in the dashboard can potentially turn attrition into retention.In summary, the main contribution is an end-to-end XAI-driven decision support system called HR-DSS that can provide HR understandable and actionable insights into employee attrition at both the individual and global level to inform retention strategies. The integration of predictive modeling, explainability and an interactive interface makes this system useful for real-world application.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper proposes an explainable AI system called HR-DSS to help HR departments understand and prevent employee attrition by interpreting predictions from machine learning models using techniques like SHAP values and interactive dashboards.


## How does this paper compare to other research in the same field?

This paper presents an explainable AI system called HR-DSS for analyzing employee attrition. Here are some key points in comparing it to other related work:- The authors employ several standard machine learning models like XGBoost, SVM, random forest etc. for predicting employee attrition. Using these simple ML models instead of more complex neural networks aligns with their goal of developing an interpretable system. - To handle the class imbalance in the dataset, they experiment with different oversampling techniques like SMOTE. However, these do not significantly improve results, so they turn to outlier removal and feature weighting instead. The feature engineering helps boost model accuracy.- For explainability, they use the SHAP library rather than just relying on measures like feature importance. SHAP provides local explanations for individual predictions, not just global model interpretation. - The interactive dashboard with what-if analysis allows tweaking employee features to see impact on retention. This enables practical use by HR, beyond just model explanations.- The natural language generation module is novel, generating text explanations from the SHAP values. This further bridges the gap between model interpretability and human resources use.Overall, the use of SHAP, interactive dashboard, and natural language generation differentiates this work from other papers that have applied XAI methods to HR data. The focus is on practical applicability in a decision support system. The simple ML models and data pre-processing also aim to make the system adaptable.


## What future research directions do the authors suggest?

The authors suggest several potential future research directions:- Exploring more advanced deep learning models like CNNs and Graph Neural Networks for attrition prediction. The current work mainly focuses on classical ML models.- Incorporating the natural language explanations directly into the interactive dashboard to provide more intuitive explanations to HR experts. Currently, the NLG is done separately.- Evaluating the approach on real-world HR datasets from companies to analyze the performance in practical settings. The current work uses a synthetic benchmark dataset. - Investigating methods to make the interactive dashboard web-accessible to provide it as a service to HR departments.- Expanding the feature set with more employee information like performance reviews, 360 feedbacks etc. if such data is available. This could improve model accuracy.- Including temporal/sequential modeling to leverage time-series effects in employee behavior over months/years.- Testing different prompt engineering strategies for the language model to generate high-quality and truthful explanations grounded in organizational policies and guidelines.- Exploring adversarial testing of explanations to improve robustness and minimize hallucination risks of the language model.Overall, the authors provide good directions to take this research further both from a technical machine learning perspective as well as tailoring the system to real-world HR usage. Integrating NLG directly into the dashboard and expanding to real-world datasets seem like the most promising next steps.
