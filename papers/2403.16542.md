# [Differentially Private Online Federated Learning with Correlated Noise](https://arxiv.org/abs/2403.16542)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem: 
The paper focuses on online federated learning (OFL), where a server coordinates multiple distributed learners to train a model in real-time using streaming non-iid data from clients. OFL is useful for applications requiring instant decision-making, such as hospital networks. However, a key concern is privacy leakage from the continuously released models. Differential privacy (DP) is a standard technique to preserve privacy, but most works study DP with independent noise injection for offline learning. Using independent noise for online learning reduces utility. Recent works use temporally correlated noise for online learning, but only consider single-machine settings, not OFL.

Proposed Solution:
This paper proposes a DP algorithm for OFL using temporally correlated noise constructed via matrix factorization. To address challenges from DP noise and streaming non-iid data, a perturbed iterate analysis is developed to control the impact of noise on utility. Specifically, the actual variable is decomposed into a virtual variable that subtracts out the DP noise component. The virtual variable, containing gradient information free of DP noise, is analyzed to quantify the effect of noise. Moreover, drift errors caused by local updates are managed under a quasi-strong convexity condition.  

Main Contributions:
- Extends temporally correlated noise techniques from single-machine to federated learning settings
- Develops perturbed iterate analysis to control impact of DP noise and local updates on utility
- Establishes dynamic regret bound that explicitly shows trade-off between utility, privacy parameters, and environment changes
- Removes dependence on environment changes under strong convexity and proves sub-linear static regret 
- Validates efficacy of proposed OFL algorithm on logistic regression problems

In summary, the paper makes important contributions in designing DP algorithms for OFL that use correlated noise to improve utility-privacy trade-offs. The analysis quantifies the impact of key factors and demonstrates advantages over independent noise injection.


## Summarize the paper in one sentence.

 Here is a one-sentence summary of the paper:

The paper proposes a differentially private algorithm for online federated learning that uses temporally correlated noise to improve utility while ensuring privacy, and analytically quantifies the trade-off between utility and privacy via perturbed iterate analysis considering the challenges of local updates with streaming non-iid data.


## What is the main contribution of this paper?

 Based on my understanding, the main contribution of this paper is proposing a novel differentially private algorithm for online federated learning that employs temporally correlated noise to improve the utility while ensuring privacy. Specifically:

- The paper adapts techniques for using temporally correlated noise from the single-machine setting to the more challenging online federated learning setting with multiple learners and local updates. 

- To analyze the impact of the correlated noise along with the drift errors from local updates on utility, the paper develops a perturbed iterate analysis. This allows establishing dynamic regret bounds that explicitly quantify the trade-off between utility and privacy.

- The paper provides both dynamic and static regret bounds. For the quasi-strongly convex case, a sublinear dynamic regret is established. For the strongly convex case, the bound becomes a sublinear static regret.

- The numerical experiments demonstrate the efficacy of the proposed differentially private online federated learning algorithm compared to using independent noise. The results showcase the benefits of exploiting temporal correlations.

In summary, the main novelty is in adapting correlated noise techniques to online federated learning and developing the analysis to handle the combined effect of privacy and system constraints to achieve strong theoretical guarantees.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and concepts include:

- Online federated learning (OFL) - The paper focuses on an online federated learning framework with a server coordinating multiple distributed learners that interact with streaming clients. 

- Streaming non-iid data - The data arriving from clients is non-iid (not independent and identically distributed), even across different time steps for the same learner.

- Differential privacy (DP) - The paper aims to develop a differentially private algorithm to protect the privacy of streaming clients while enabling collaborative learning.  

- Adaptive continuous release - The privacy analysis considers an adaptive adversary that can select data based on all previous model outputs, under continuous release of updated models.

- Temporally correlated noise - The proposed algorithm uses temporally correlated noise for privacy protection, instead of independent noise across iterations. 

- Perturbed iterate analysis - A technique to analyze the impact of DP noise on utility by defining a virtual variable that subtracts the noise.  

- Quasi-strong convexity - An assumption made about the objective functions that is weaker than strong convexity. Enables dynamic regret analysis.

- Dynamic regret - The metric used to evaluate the performance of online learning algorithms by comparing against a sequence of time-varying optimal losses.

In summary, the key focus is on differentially private online federated learning with non-iid streaming data, using temporally correlated noise and perturbed iterate analysis.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 potential in-depth questions about the method proposed in this paper:

1. The paper utilizes a perturbed iterate analysis to control the impact of differential privacy (DP) noise on utility. Can you explain in more detail how this technique works and why it is effective for analyzing the effect of correlated noise?

2. How does the paper handle the drift error caused by local updates with streaming non-iid data? Explain the key ideas that allow managing this error under a quasi-strong convexity condition. 

3. The dynamic regret bound contains a term $C_R$ that captures changes in the optimal solution set over time. What exactly does this term represent and why is it unavoidable in analyzing dynamic regret without strong convexity assumptions?

4. The paper establishes both dynamic and static regret bounds. Compare these bounds and discuss when each one is more suitable to use based on problem assumptions. What are the key differences in the analysis?

5. Explain the matrix factorization approach used to construct the correlated noise and preserve privacy. How does this connect to the sensitivity analysis for differential privacy?

6. How does the choice of matrix factors $\mathbf{B}$ and $\mathbf{C}$ affect the privacy-utility trade-off? What considerations guide the optimization problem formulation to determine these factors? 

7. The paper adapts techniques developed for single-machine online learning to the federated learning setting. What complications arise in the federated case and how does the analysis address them?

8. Compare the dynamic regret bound established in this paper to prior work on distributed online learning without privacy constraints. What additional error terms appear due to privacy protection and how fast do they decay?

9. Discuss the differences in assumptions between the quasi-strongly convex and strongly convex cases. How do the dynamic and static regret bounds change depending on these assumptions? 

10. The numerical experiments show better empirical performance compared to a baseline with independent DP noise. Explain why temporal correlations can improve utility in the online learning setting.
