# [Architectural Strategies for the optimization of Physics-Informed Neural   Networks](https://arxiv.org/abs/2402.02711)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Physics-informed neural networks (PINNs) offer a promising way to solve partial differential equations (PDEs) by incorporating physics into deep learning models. However, PINNs often struggle to train effectively, especially for problems with high-frequency solutions. This is partly attributed to neural networks favoring low-frequency solutions.

- The theory explaining why certain architectural innovations like Gaussian activations perform better is lacking. Additionally, the composite loss landscape of PINNs with both physics and boundary constraints tends to be ill-conditioned, hampering optimization.

Proposed Solutions and Contributions:

1) Provide theoretical basis using Neural Tangent Kernel analysis to demonstrate superior scaling properties of Gaussian activations. Experiments validate Gaussian PINNs outperforming alternatives across PDEs from fluids, physics, etc.

2) Introduce "Equilibrated PINNs", a new architecture using concepts of matrix preconditioning from numerical linear algebra to condition the loss landscape. Equilibrated PINNs dynamically row-equilibrate weight matrices during training to improve conditioning.

- Extensive analysis and experiments provided comparing performance of Gaussian & Equilibrated PINNs against recently proposed methods like Locally Adaptive PINNs, PINNsformer, etc.

- Equilibrated Gaussian PINNs consistently achieve lowest errors and fastest convergence across PDEs, substantiating benefits of architectural innovations proposed. Limitations include longer training time and analysis restricted to boundary condition NTK.

In summary, key innovations are theoretically-grounded Gaussian activations overcoming spectral bias and equilibrated architectures enhancing PINN optimization through preconditioning.
