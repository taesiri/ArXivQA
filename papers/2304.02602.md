# [Generative Novel View Synthesis with 3D-Aware Diffusion Models](https://arxiv.org/abs/2304.02602)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question seems to be: 

How can we develop a novel view synthesis method that is capable of generating realistic novel views from very limited inputs (e.g. a single image), while also being able to generate geometrically consistent renderings over long trajectories far from the original input views?

The key ideas and contributions towards addressing this question appear to be:

- Leveraging recent advances in 2D diffusion models for image synthesis, and adapting them to the task of novel view synthesis by conditioning on input images and camera poses. This allows sampling novel views from the conditional distribution. 

- Incorporating 3D geometry priors in the form of a latent 3D feature volume, extracted from input images, which provides a strong inductive bias for geometric consistency. 

- Formulating this latent feature volume as representing a distribution of possible scene representations rather than just a single scene, which helps handle ambiguity when extrapolating far from inputs.

- Demonstrating this approach generates high quality, geometrically consistent novel views on a variety of datasets including synthetic objects, room-scale scenes, and challenging real-world objects.

So in summary, the main hypothesis is that by combining the generative capabilities of diffusion models with explicit 3D geometric priors, we can develop a novel view synthesis approach that exceeds prior methods in its ability to extrapolate realistically while maintaining consistency.


## What is the main contribution of this paper?

 Based on my reading of the paper, the main contributions seem to be:

- Proposing a novel view synthesis method that extends 2D diffusion models to be 3D-aware by conditioning them on 3D neural features extracted from input image(s). 

- Demonstrating that the proposed 3D feature-conditioned diffusion model can generate realistic novel views from a single input image on a variety of datasets, including objects, rooms, and complex real-world scenes.

- Showing that with the proposed method and sampling strategy, the model can generate long trajectories of realistic, multi-view consistent novel views without suffering from the blurring of regression models or the drift of pure generative models.

In summary, the key contribution appears to be presenting a new way to make diffusion models 3D-aware for novel view synthesis by incorporating geometric priors in the form of 3D neural features. This allows the generative capabilities of diffusion models to be combined with explicit geometry for high-quality view synthesis even from a single image. The experiments show the approach works well across different types of scenes.
