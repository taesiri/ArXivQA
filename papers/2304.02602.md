# [Generative Novel View Synthesis with 3D-Aware Diffusion Models](https://arxiv.org/abs/2304.02602)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question seems to be: How can we develop a novel view synthesis method that is capable of generating realistic novel views from very limited inputs (e.g. a single image), while also being able to generate geometrically consistent renderings over long trajectories far from the original input views?The key ideas and contributions towards addressing this question appear to be:- Leveraging recent advances in 2D diffusion models for image synthesis, and adapting them to the task of novel view synthesis by conditioning on input images and camera poses. This allows sampling novel views from the conditional distribution. - Incorporating 3D geometry priors in the form of a latent 3D feature volume, extracted from input images, which provides a strong inductive bias for geometric consistency. - Formulating this latent feature volume as representing a distribution of possible scene representations rather than just a single scene, which helps handle ambiguity when extrapolating far from inputs.- Demonstrating this approach generates high quality, geometrically consistent novel views on a variety of datasets including synthetic objects, room-scale scenes, and challenging real-world objects.So in summary, the main hypothesis is that by combining the generative capabilities of diffusion models with explicit 3D geometric priors, we can develop a novel view synthesis approach that exceeds prior methods in its ability to extrapolate realistically while maintaining consistency.
