# [Agile Catching with Whole-Body MPC and Blackbox Policy Learning](https://arxiv.org/abs/2306.08205)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be:What are the relative merits of model predictive control (MPC) and blackbox policy optimization for high-speed robotic catching, in terms of reaction time, sample efficiency, robustness to distribution shifts, and versatility in generating whole-body multimodal behaviors?The paper compares an MPC approach using constrained trajectory optimization to a blackbox policy learning approach using zeroth-order optimization. The goal is to provide insights into the trade-offs between these two fundamentally different solution strategies for the challenging task of agile robotic catching. The main axes of comparison are:- Reaction time: How fast can each method react and compute control actions?- Sample efficiency: How much real-world data is needed to train each method?- Robustness: How sensitive is each method to shifts in the ball throwing distribution? - Versatility: How well can each method leverage the robot's whole body (base + arm) to enable multimodal catching behaviors?Through extensive experiments in simulation and the real world, the paper aims to understand the relative strengths and weaknesses of model-based optimal control vs learning-based methods for dexterous, dynamic robot control tasks.


## What is the main contribution of this paper?

The main contribution of this paper is providing insights into the relative merits of two different approaches to high-speed robotic catching:1) Model Predictive Control (MPC) using accelerated constrained trajectory optimization. This represents a "pure control" strategy that is nearly "zero-shot" in terms of data requirements, and handles kinematics, dynamics and constraints well, but can be computationally expensive and sensitive to modeling errors.2) Blackbox policy optimization via reinforcement learning (RL). This represents a "pure learning" approach that can adapt to complex unknown dynamics but is extremely data inefficient. The paper presents a unified experimental evaluation of these two approaches on a real robot platform to highlight trade-offs in reaction time, sample efficiency, robustness to distribution shift, and versatility in generating whole-body multimodal behaviors for agile catching.In summary, the key contribution is providing insights into the subtle trade-offs between model-based optimal control and data-driven reinforcement learning methods for agile robotics through comprehensive experiments comparing MPC and blackbox policy optimization on a real robotic catching task. The paper concludes with proposals to combine the "best of both worlds" in future work.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper compares model predictive control and reinforcement learning for high-speed robotic catching, finding that MPC is more sample efficient and robust but RL achieves higher success rates through simulation training and real-world fine-tuning; they propose combining the approaches in future work.


## How does this paper compare to other research in the same field?

This paper presents an interesting comparison between model predictive control (MPC) and blackbox reinforcement learning (RL) for the task of robotic catching. Here are some key points on how it relates to other work:- Catching/interception is a well-studied problem in robotics, with many prior works using either optimization-based control like MPC or learning-based methods like RL. This paper provides one of the most direct comparisons between the two approaches on the same physical system.- For the MPC approach, the authors use a common framework of formulating catching as a constrained trajectory optimization problem. Their specific method of discretizing the control into a sequence of acceleration/coast phases is pretty standard.- For RL, they use a fairly simple blackbox policy optimization method (BGS) with standard neural network policies. Many other RL papers have explored more sophisticated deep RL algorithms like soft actor-critic or model-based RL.- A key contribution is the real-world experiments and analysis of sim-to-real transfer, out-of-distribution generalization, etc. Most prior work is only evaluated in simulation. The real-world benchmarks and distribution shifts explored here provide valuable insights.- Their analysis of the tradeoffs between MPC and RL is novel. The conclusions on MPC being more robust/generalizable versus RL being more performant with enough data are consistent with intuition, but quantify these tradeoffs on a real system.- Their proposals to combine MPC and RL take advantage of their complementary strengths. Using MPC for coarse planning and RL for local control is a promising direction explored by other recent work as well.In summary, while the individual methods used are not brand new, the head-to-head comparison on real hardware provides unique insights into the tradeoffs between optimization-based and learning-based control for dexterous manipulation skills like catching. The analysis and future directions should inspire more work on combining classical and modern approaches.
