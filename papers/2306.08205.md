# [Agile Catching with Whole-Body MPC and Blackbox Policy Learning](https://arxiv.org/abs/2306.08205)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question seems to be:

What are the relative merits of model predictive control (MPC) and blackbox policy optimization for high-speed robotic catching, in terms of reaction time, sample efficiency, robustness to distribution shifts, and versatility in generating whole-body multimodal behaviors?

The paper compares an MPC approach using constrained trajectory optimization to a blackbox policy learning approach using zeroth-order optimization. The goal is to provide insights into the trade-offs between these two fundamentally different solution strategies for the challenging task of agile robotic catching. The main axes of comparison are:

- Reaction time: How fast can each method react and compute control actions?

- Sample efficiency: How much real-world data is needed to train each method?

- Robustness: How sensitive is each method to shifts in the ball throwing distribution? 

- Versatility: How well can each method leverage the robot's whole body (base + arm) to enable multimodal catching behaviors?

Through extensive experiments in simulation and the real world, the paper aims to understand the relative strengths and weaknesses of model-based optimal control vs learning-based methods for dexterous, dynamic robot control tasks.


## What is the main contribution of this paper?

 The main contribution of this paper is providing insights into the relative merits of two different approaches to high-speed robotic catching:

1) Model Predictive Control (MPC) using accelerated constrained trajectory optimization. This represents a "pure control" strategy that is nearly "zero-shot" in terms of data requirements, and handles kinematics, dynamics and constraints well, but can be computationally expensive and sensitive to modeling errors.

2) Blackbox policy optimization via reinforcement learning (RL). This represents a "pure learning" approach that can adapt to complex unknown dynamics but is extremely data inefficient. 

The paper presents a unified experimental evaluation of these two approaches on a real robot platform to highlight trade-offs in reaction time, sample efficiency, robustness to distribution shift, and versatility in generating whole-body multimodal behaviors for agile catching.

In summary, the key contribution is providing insights into the subtle trade-offs between model-based optimal control and data-driven reinforcement learning methods for agile robotics through comprehensive experiments comparing MPC and blackbox policy optimization on a real robotic catching task. The paper concludes with proposals to combine the "best of both worlds" in future work.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the key points from the paper:

The paper compares model predictive control and reinforcement learning for high-speed robotic catching, finding that MPC is more sample efficient and robust but RL achieves higher success rates through simulation training and real-world fine-tuning; they propose combining the approaches in future work.


## How does this paper compare to other research in the same field?

 This paper presents an interesting comparison between model predictive control (MPC) and blackbox reinforcement learning (RL) for the task of robotic catching. Here are some key points on how it relates to other work:

- Catching/interception is a well-studied problem in robotics, with many prior works using either optimization-based control like MPC or learning-based methods like RL. This paper provides one of the most direct comparisons between the two approaches on the same physical system.

- For the MPC approach, the authors use a common framework of formulating catching as a constrained trajectory optimization problem. Their specific method of discretizing the control into a sequence of acceleration/coast phases is pretty standard.

- For RL, they use a fairly simple blackbox policy optimization method (BGS) with standard neural network policies. Many other RL papers have explored more sophisticated deep RL algorithms like soft actor-critic or model-based RL.

- A key contribution is the real-world experiments and analysis of sim-to-real transfer, out-of-distribution generalization, etc. Most prior work is only evaluated in simulation. The real-world benchmarks and distribution shifts explored here provide valuable insights.

- Their analysis of the tradeoffs between MPC and RL is novel. The conclusions on MPC being more robust/generalizable versus RL being more performant with enough data are consistent with intuition, but quantify these tradeoffs on a real system.

- Their proposals to combine MPC and RL take advantage of their complementary strengths. Using MPC for coarse planning and RL for local control is a promising direction explored by other recent work as well.

In summary, while the individual methods used are not brand new, the head-to-head comparison on real hardware provides unique insights into the tradeoffs between optimization-based and learning-based control for dexterous manipulation skills like catching. The analysis and future directions should inspire more work on combining classical and modern approaches.


## What future research directions do the authors suggest?

 Based on my reading of the paper, here are some of the main future research directions suggested by the authors:

- Combining blackbox optimization and SQP: The authors suggest exploring ways to get the "best of both worlds" from blackbox optimization and SQP, such as using blackbox optimization to optimize the parameters of the SQP solver, using a small blackbox policy to output SQP parameters for each episode, or using a "switch-over" policy that starts with SQP and switches to a blackbox policy right before intercept and cradling. 

- Adaptive nonlinear dynamics prediction: The authors propose introducing tools from adaptive nonlinear dynamics prediction, to enable catching objects with complex aerodynamics like light balls or non-spherical objects. This could help improve modelling of effects like drag and Magnus forces.

- Exploration of other model-based RL techniques: The authors focus on comparing SQP and blackbox optimization, but suggest exploring other model-based RL techniques as well. These could potentially combine the sample efficiency of model-based approaches with the adaptability of data-driven learning.

- Sim-to-real transfer: While sim-to-real transfer is discussed, the authors suggest further work on techniques to improve transfer of policies from simulation to the real world. Things like domain randomization, adaptive simulators, and learned sim-to-real mappings could be promising directions.

- Benchmarking on expanded task distributions: The authors recommend benchmarking the approaches on expanded distributions of ball trajectories, speeds, sizes, shapes, etc. to further explore generalizability.

In summary, the main suggestions are on combining model-based and model-free methods, improving dynamics modelling, sim-to-real transfer, and benchmarking on more diverse task conditions. The overall goal is to build on the insights from this work to develop even more versatile and robust controllers for agile catching tasks.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the key points in the paper:

The paper explores using model predictive control (MPC) and blackbox reinforcement learning (RL) for controlling a robot to catch high speed balls thrown at it. MPC optimizes robot trajectories in real-time using a physics model but can be computationally expensive and sensitive to model errors. RL directly learns a policy from experience but is sample inefficient. Experiments show MPC reacts faster, transfers better to the real robot, and is more robust to different ball throws. However, RL achieves a higher success rate after extensive real robot training. The paper proposes combining them, such as using RL to optimize MPC parameters or switch between MPC and RL at different stages. Overall, it provides insights into tradeoffs between classical control versus learning for agile robotics.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the key points in the paper:

The paper presents a comparative study of model predictive control (MPC) and blackbox reinforcement learning (RL) for the task of high-speed robotic catching. The goal is to track, intercept and cradle balls thrown at speeds up to 5.5 m/s using visual observations and robot proprioception. MPC optimizes robot trajectories by leveraging models of the task constraints and dynamics. It is data-efficient but can be computationally expensive and sensitive to model errors. In contrast, RL directly learns a policy mapping observations to actions, adapting to complex unknown dynamics through experience. However, RL generally requires large amounts of data. 

Through hardware experiments, the paper provides insights into the tradeoffs between the two approaches. MPC shows faster reaction times, better sim-to-real transfer, and more robustness to distribution shifts versus RL. However, with enough real-world fine-tuning, the RL policy achieves the highest success rate. The authors propose combining the approaches, using RL to optimize MPC parameters or as a high-speed supplemental controller. Extensions could incorporate tools for online dynamics adaptation and handling non-spherical objects. Overall, the work offers a nuanced exploration of blending model-based optimal control and data-driven learning for agile robotics.


## Summarize the main method used in the paper in one paragraph.

 Here is a one paragraph summary of the main method used in the paper:

The paper compares two approaches for controlling a robot to catch thrown objects - model predictive control (MPC) and blackbox policy optimization. For MPC, they formulate catching as an optimal control problem that is solved in real-time using sequential quadratic programming. This allows incorporating kinematics, dynamics and constraints while generating trajectories. For the blackbox approach, they train a neural network policy using reinforcement learning and zeroth-order optimization via gradient sensing. This policy maps from observations to actions. The two methods represent different extremes of control - one relying on an accurate model with optimization versus learning a policy in a data-driven way. The paper evaluates their relative performance on metrics like sample efficiency, robustness, multi-modality through experiments on a physical robot.


## What problem or question is the paper addressing?

 Based on my reading, the main problem this paper is addressing is how to develop controllers for robotic catching of high-speed moving objects using visual feedback. Specifically, it is comparing two different approaches - model predictive control (MPC) and blackbox policy optimization - in terms of their relative merits and tradeoffs. The key questions seem to be:

- How can MPC and blackbox policy optimization be applied to solve the agile catching task?

- What are the strengths and weaknesses of each approach in terms of sample efficiency, robustness, versatility for whole-body control, etc? 

- Can the two approaches be combined to get the "best of both worlds"?

The paper aims to provide insights into these questions through theoretical formulation, implementation details, and experimental evaluation on a real robot system. The goal is to understand subtle tradeoffs between model-based optimal control and data-driven reinforcement learning for dexterous, dynamic manipulation skills like catching.


## What are the keywords or key terms associated with this paper?

 Based on reading the paper summary, some key terms and keywords that seem most relevant are:

- Agile robotics - The paper focuses on agile catching by a robot manipulator, which is an example of agile robotics. 

- High speed catching - The paper looks at catching objects thrown at high speeds, which requires agile motions by the robot.

- Model Predictive Control (MPC) - One of the two main approaches explored is model predictive control, which optimizes robot trajectories in real-time based on dynamics models. 

- Blackbox policy optimization - The other main approach is blackbox policy optimization, which uses reinforcement learning to learn policies without relying on models.

- Sample efficiency - A key aspect examined is sample efficiency, i.e. how much data each method requires. MPC is more sample efficient compared to blackbox learning.

- Robustness to distribution shift - The paper looks at robustness of the methods to changes in the ball throwing distribution, where MPC is more robust. 

- Multimodality - The ability to exhibit diverse catching behaviors, where MPC catches more flexibly. 

- Sim-to-real transfer - Transferring policies from simulation to the physical system, where blackbox learning degrades more.

- Combining MPC and learning - The paper proposes combining both methods to get the "best of both worlds".

So in summary, the key terms cover model-based and data-driven control, sample efficiency, sim-to-real transfer, robustness, multimodality, and combining classical and learning-based techniques. The benchmark task is high-speed catching.


## Based on the paper, what questions should we ask to create a comprehensive summary of it? Please list at least 10 questions.

 Here are 10 potential questions to ask in order to summarize the key points of the paper:

1. What is the main problem being addressed in the paper? (Catching high-speed balls with a robot.)

2. What are the two main approaches compared? (Model Predictive Control and Blackbox Policy Optimization.)  

3. What are some key challenges involved in catching high-speed balls? (Unpredictable ball flight, complex physics, short reaction times, etc.)

4. How is the ball trajectory modeled in each approach? 

5. What are the pros and cons highlighted for the MPC approach? (Gracefully handles constraints, sensitive to model errors.)

6. What are the pros and cons highlighted for the blackbox policy optimization approach? (Can adapt to unknown dynamics, data inefficient.)

7. How are the MPC and policy optimized in detail? (Reduction to quadratic programming, use of BGS.)  

8. How is sim-to-real transfer performed and evaluated?

9. What kinds of robustness are evaluated? (Speed variation, yaw angle, hand throwing.)

10. What are some proposed ways to combine the strengths of each approach in future work? (Optimizing MPC parameters with BB, switch-over control policy, etc.)


## Design 10 in-depth questions about method proposed in this paper and avoid simple question.

 Here are 10 potential in-depth questions about the method proposed in the paper:

1. The paper compares model predictive control (MPC) and blackbox policy optimization. What are some key advantages and disadvantages of each approach that lead to their different performance tradeoffs?

2. The MPC approach uses a multi-stage trajectory optimization formulation. How does discretizing the problem into stages help computationally? What are some limitations of this discretization? 

3. The MPC approach uses an asynchronous implementation to handle online replanning. What are the benefits of asynchronous optimization versus a synchronous approach? How does it maintain stability?

4. The blackbox policy optimization uses a two-tower CNN architecture. Why is this architecture chosen? How do the different towers process and integrate different types of input data?

5. The paper mentions using Blackbox Gradient Sensing (BGS) for policy optimization. Why is BGS preferred over other gradient-based or gradient-free optimization methods? What are its strengths and weaknesses?

6. The reward design is different for simulation versus real hardware. What aspects of the reward function needed to change and why? How does this impact sim-to-real transfer?

7. Both methods degrade in performance when transferring from simulation to reality. What factors contribute to this simulation-to-reality gap? How can it be reduced?

8. The MPC approach shows better robustness to distribution shifts versus the blackbox policy. Why does the MPC handle out-of-distribution scenarios better? How can the policy be made more robust?

9. The paper proposes combining aspects of MPC and blackbox optimization. What are some ways these methods could be hybridized? What challenges need to be overcome to get the "best of both worlds"?

10. The methods are evaluated on a ball catching task, but could be applied more broadly. What other agile robotics tasks could these approaches be suited for? Would the performance tradeoffs generalize?


## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a summary paragraph of the key points from the paper:

This paper presents and compares two approaches - model predictive control (MPC) using sequential quadratic programming (SQP) and blackbox policy optimization - for enabling agile catching of high-speed balls thrown at a mobile manipulator. The MPC agent leverages physics-based modeling and online trajectory optimization to plan dynamic motions satisfying constraints. In contrast, the blackbox agent uses deep reinforcement learning to directly map from visual observations to actions, adapting to complex dynamics but requiring much more real-world data. Experiments in simulation and on real hardware demonstrate the tradeoffs: while blackbox optimization can achieve slightly higher catching success after extensive fine-tuning, MPC planning shows superior sim-to-real transfer as well as robustness to significant distribution shift such as slower balls or hand throws. The results motivate future work to combine classical and learning techniques, such as using blackbox optimization to tune MPC parameters online or handover control to a blackbox policy for the final precision catching maneuver. Overall, this careful benchmarking provides insights into performance aspects and transferability in leveraging model-based versus model-free control for agile robot catching.


## Summarize the paper in one sentence.

 Here is a one-sentence summary of the paper:

The paper presents a comparative study of model predictive control and blackbox policy optimization approaches for enabling agile catching of high-speed thrown objects on a mobile manipulator, analyzing performance tradeoffs between the two methods in terms of sample efficiency, sim-to-real transfer, robustness to distribution shifts, and multimodality.


## Summarize the paper in one paragraphs.

 Here is a one paragraph summary of the key points from the paper:

This paper presents and compares two approaches for enabling agile catching of high-speed thrown objects on a 7-DOF mobile manipulator - model predictive control (MPC) using sequential quadratic programming (SQP), representing a "pure control" strategy, and blackbox policy optimization representing a "pure learning" strategy. Through extensive experiments in simulation and on real hardware with a mechanical ball thrower, the authors evaluate performance tradeoffs related to sample efficiency, sim-to-real transfer, robustness to distribution shifts, and multimodal behaviors. They find that while the fine-tuned blackbox agent achieves the highest catching success rate of 85%, the SQP agent is much more robust to varying distributions. The authors conclude by proposing ideas to combine classical and learning-based techniques, such as using blackbox optimization to tune SQP parameters or output them conditioned on state, or having a hybrid switch-over controller.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 potential in-depth questions about the methods proposed in this paper:

1. The paper proposes both model predictive control (MPC) and reinforcement learning (RL) approaches for robotic catching. What are some of the key trade-offs between these two approaches in terms of sample efficiency, robustness, and computational expense? 

2. The MPC formulation uses a multi-stage trajectory optimization approach with "stages" composed of an acceleration phase followed by a cruising phase. What is the justification for this simplified trajectory representation instead of using full trajectory optimization?

3. The paper mentions using a "cradling motion primitive" after catching the ball. What is the purpose of this cradling maneuver and how is it implemented in terms of controlling the robot?

4. For the RL approach, reward shaping seems critical to enable sim-to-real transfer. What are some of the key reward terms used and how do they differ between simulation and the real robot? 

5. How does the MPC approach account for uncertainties/inaccuracies in predicting the ball's trajectory over time? What online adaptations are made to deal with updated estimates?

6. What neural network architecture is used for the RL policy? What considerations went into the design to balance representational capacity and sample efficiency?  

7. The RL policy optimization uses Blackbox Gradient Sensing (BGS). What are the main advantages of this over alternative RL algorithms? How does it balance exploration vs. exploitation?

8. What modifications or enhancements would be needed to apply these methods to catching non-spherical projectiles with more complex aerodynamics?

9. The paper mentions future work on combining MPC and RL. What are some ways these could be fused and what advantages might that provide?

10. Both the MPC and RL approaches seem to depend heavily on accurate state estimation from vision. How might end-to-end methods that directly map visual observations to controls compare to these model-based approaches?
