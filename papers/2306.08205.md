# [Agile Catching with Whole-Body MPC and Blackbox Policy Learning](https://arxiv.org/abs/2306.08205)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be:What are the relative merits of model predictive control (MPC) and blackbox policy optimization for high-speed robotic catching, in terms of reaction time, sample efficiency, robustness to distribution shifts, and versatility in generating whole-body multimodal behaviors?The paper compares an MPC approach using constrained trajectory optimization to a blackbox policy learning approach using zeroth-order optimization. The goal is to provide insights into the trade-offs between these two fundamentally different solution strategies for the challenging task of agile robotic catching. The main axes of comparison are:- Reaction time: How fast can each method react and compute control actions?- Sample efficiency: How much real-world data is needed to train each method?- Robustness: How sensitive is each method to shifts in the ball throwing distribution? - Versatility: How well can each method leverage the robot's whole body (base + arm) to enable multimodal catching behaviors?Through extensive experiments in simulation and the real world, the paper aims to understand the relative strengths and weaknesses of model-based optimal control vs learning-based methods for dexterous, dynamic robot control tasks.


## What is the main contribution of this paper?

The main contribution of this paper is providing insights into the relative merits of two different approaches to high-speed robotic catching:1) Model Predictive Control (MPC) using accelerated constrained trajectory optimization. This represents a "pure control" strategy that is nearly "zero-shot" in terms of data requirements, and handles kinematics, dynamics and constraints well, but can be computationally expensive and sensitive to modeling errors.2) Blackbox policy optimization via reinforcement learning (RL). This represents a "pure learning" approach that can adapt to complex unknown dynamics but is extremely data inefficient. The paper presents a unified experimental evaluation of these two approaches on a real robot platform to highlight trade-offs in reaction time, sample efficiency, robustness to distribution shift, and versatility in generating whole-body multimodal behaviors for agile catching.In summary, the key contribution is providing insights into the subtle trade-offs between model-based optimal control and data-driven reinforcement learning methods for agile robotics through comprehensive experiments comparing MPC and blackbox policy optimization on a real robotic catching task. The paper concludes with proposals to combine the "best of both worlds" in future work.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper compares model predictive control and reinforcement learning for high-speed robotic catching, finding that MPC is more sample efficient and robust but RL achieves higher success rates through simulation training and real-world fine-tuning; they propose combining the approaches in future work.
