# [Agile Catching with Whole-Body MPC and Blackbox Policy Learning](https://arxiv.org/abs/2306.08205)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be:What are the relative merits of model predictive control (MPC) and blackbox policy optimization for high-speed robotic catching, in terms of reaction time, sample efficiency, robustness to distribution shifts, and versatility in generating whole-body multimodal behaviors?The paper compares an MPC approach using constrained trajectory optimization to a blackbox policy learning approach using zeroth-order optimization. The goal is to provide insights into the trade-offs between these two fundamentally different solution strategies for the challenging task of agile robotic catching. The main axes of comparison are:- Reaction time: How fast can each method react and compute control actions?- Sample efficiency: How much real-world data is needed to train each method?- Robustness: How sensitive is each method to shifts in the ball throwing distribution? - Versatility: How well can each method leverage the robot's whole body (base + arm) to enable multimodal catching behaviors?Through extensive experiments in simulation and the real world, the paper aims to understand the relative strengths and weaknesses of model-based optimal control vs learning-based methods for dexterous, dynamic robot control tasks.


## What is the main contribution of this paper?

The main contribution of this paper is providing insights into the relative merits of two different approaches to high-speed robotic catching:1) Model Predictive Control (MPC) using accelerated constrained trajectory optimization. This represents a "pure control" strategy that is nearly "zero-shot" in terms of data requirements, and handles kinematics, dynamics and constraints well, but can be computationally expensive and sensitive to modeling errors.2) Blackbox policy optimization via reinforcement learning (RL). This represents a "pure learning" approach that can adapt to complex unknown dynamics but is extremely data inefficient. The paper presents a unified experimental evaluation of these two approaches on a real robot platform to highlight trade-offs in reaction time, sample efficiency, robustness to distribution shift, and versatility in generating whole-body multimodal behaviors for agile catching.In summary, the key contribution is providing insights into the subtle trade-offs between model-based optimal control and data-driven reinforcement learning methods for agile robotics through comprehensive experiments comparing MPC and blackbox policy optimization on a real robotic catching task. The paper concludes with proposals to combine the "best of both worlds" in future work.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper compares model predictive control and reinforcement learning for high-speed robotic catching, finding that MPC is more sample efficient and robust but RL achieves higher success rates through simulation training and real-world fine-tuning; they propose combining the approaches in future work.


## How does this paper compare to other research in the same field?

This paper presents an interesting comparison between model predictive control (MPC) and blackbox reinforcement learning (RL) for the task of robotic catching. Here are some key points on how it relates to other work:- Catching/interception is a well-studied problem in robotics, with many prior works using either optimization-based control like MPC or learning-based methods like RL. This paper provides one of the most direct comparisons between the two approaches on the same physical system.- For the MPC approach, the authors use a common framework of formulating catching as a constrained trajectory optimization problem. Their specific method of discretizing the control into a sequence of acceleration/coast phases is pretty standard.- For RL, they use a fairly simple blackbox policy optimization method (BGS) with standard neural network policies. Many other RL papers have explored more sophisticated deep RL algorithms like soft actor-critic or model-based RL.- A key contribution is the real-world experiments and analysis of sim-to-real transfer, out-of-distribution generalization, etc. Most prior work is only evaluated in simulation. The real-world benchmarks and distribution shifts explored here provide valuable insights.- Their analysis of the tradeoffs between MPC and RL is novel. The conclusions on MPC being more robust/generalizable versus RL being more performant with enough data are consistent with intuition, but quantify these tradeoffs on a real system.- Their proposals to combine MPC and RL take advantage of their complementary strengths. Using MPC for coarse planning and RL for local control is a promising direction explored by other recent work as well.In summary, while the individual methods used are not brand new, the head-to-head comparison on real hardware provides unique insights into the tradeoffs between optimization-based and learning-based control for dexterous manipulation skills like catching. The analysis and future directions should inspire more work on combining classical and modern approaches.


## What future research directions do the authors suggest?

Based on my reading of the paper, here are some of the main future research directions suggested by the authors:- Combining blackbox optimization and SQP: The authors suggest exploring ways to get the "best of both worlds" from blackbox optimization and SQP, such as using blackbox optimization to optimize the parameters of the SQP solver, using a small blackbox policy to output SQP parameters for each episode, or using a "switch-over" policy that starts with SQP and switches to a blackbox policy right before intercept and cradling. - Adaptive nonlinear dynamics prediction: The authors propose introducing tools from adaptive nonlinear dynamics prediction, to enable catching objects with complex aerodynamics like light balls or non-spherical objects. This could help improve modelling of effects like drag and Magnus forces.- Exploration of other model-based RL techniques: The authors focus on comparing SQP and blackbox optimization, but suggest exploring other model-based RL techniques as well. These could potentially combine the sample efficiency of model-based approaches with the adaptability of data-driven learning.- Sim-to-real transfer: While sim-to-real transfer is discussed, the authors suggest further work on techniques to improve transfer of policies from simulation to the real world. Things like domain randomization, adaptive simulators, and learned sim-to-real mappings could be promising directions.- Benchmarking on expanded task distributions: The authors recommend benchmarking the approaches on expanded distributions of ball trajectories, speeds, sizes, shapes, etc. to further explore generalizability.In summary, the main suggestions are on combining model-based and model-free methods, improving dynamics modelling, sim-to-real transfer, and benchmarking on more diverse task conditions. The overall goal is to build on the insights from this work to develop even more versatile and robust controllers for agile catching tasks.


## Summarize the paper in one paragraph.

Here is a one paragraph summary of the key points in the paper:The paper explores using model predictive control (MPC) and blackbox reinforcement learning (RL) for controlling a robot to catch high speed balls thrown at it. MPC optimizes robot trajectories in real-time using a physics model but can be computationally expensive and sensitive to model errors. RL directly learns a policy from experience but is sample inefficient. Experiments show MPC reacts faster, transfers better to the real robot, and is more robust to different ball throws. However, RL achieves a higher success rate after extensive real robot training. The paper proposes combining them, such as using RL to optimize MPC parameters or switch between MPC and RL at different stages. Overall, it provides insights into tradeoffs between classical control versus learning for agile robotics.


## Summarize the paper in two paragraphs.

Here is a two paragraph summary of the key points in the paper:The paper presents a comparative study of model predictive control (MPC) and blackbox reinforcement learning (RL) for the task of high-speed robotic catching. The goal is to track, intercept and cradle balls thrown at speeds up to 5.5 m/s using visual observations and robot proprioception. MPC optimizes robot trajectories by leveraging models of the task constraints and dynamics. It is data-efficient but can be computationally expensive and sensitive to model errors. In contrast, RL directly learns a policy mapping observations to actions, adapting to complex unknown dynamics through experience. However, RL generally requires large amounts of data. Through hardware experiments, the paper provides insights into the tradeoffs between the two approaches. MPC shows faster reaction times, better sim-to-real transfer, and more robustness to distribution shifts versus RL. However, with enough real-world fine-tuning, the RL policy achieves the highest success rate. The authors propose combining the approaches, using RL to optimize MPC parameters or as a high-speed supplemental controller. Extensions could incorporate tools for online dynamics adaptation and handling non-spherical objects. Overall, the work offers a nuanced exploration of blending model-based optimal control and data-driven learning for agile robotics.


## Summarize the main method used in the paper in one paragraph.

Here is a one paragraph summary of the main method used in the paper:The paper compares two approaches for controlling a robot to catch thrown objects - model predictive control (MPC) and blackbox policy optimization. For MPC, they formulate catching as an optimal control problem that is solved in real-time using sequential quadratic programming. This allows incorporating kinematics, dynamics and constraints while generating trajectories. For the blackbox approach, they train a neural network policy using reinforcement learning and zeroth-order optimization via gradient sensing. This policy maps from observations to actions. The two methods represent different extremes of control - one relying on an accurate model with optimization versus learning a policy in a data-driven way. The paper evaluates their relative performance on metrics like sample efficiency, robustness, multi-modality through experiments on a physical robot.
