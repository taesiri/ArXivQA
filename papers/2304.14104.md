# [Learning Human-Human Interactions in Images from Weak Textual   Supervision](https://arxiv.org/abs/2304.14104)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading, the central research question this paper aims to address is: 

How can we model the diverse and heavy tail of possible human-human interactions from images using only weak supervision from textual captions?

More specifically, the key points are:

- Prior works have treated human-human interactions (HHI) categorically, using a small fixed set of classes. But HHI are diverse and nuanced. 

- The authors propose modeling HHI in images as free text instead of fixed classes. This allows capturing the unlimited variety of possible interactions.

- Learning HHI as free text is challenging due to limited labelled data. The authors use weak supervision from image-caption pairs on the internet.

- They extract pseudo-labels for HHI from noisy internet captions using knowledge distillation and a large language model, without explicit annotation.

- They show these pseudo-labels can train models to effectively predict HHI in images, outperforming prior categorical models.

In summary, the core research question is how to model the long tail of human-human interactions in images as free text, using only weak supervision from readily available image-caption data, rather than small manually annotated datasets. The key novelty is the use of knowledge distillation to create pseudo-labels for this task.


## What is the main contribution of this paper?

 The main contribution of this paper is proposing a new paradigm for learning to understand human-human interactions (HHI) in images as free text, allowing more flexibility than categorical recognition. The key ideas are:

- Modeling HHI as free text instead of fixed categories to capture the vast variety of possible interactions between people. This allows generating descriptions like "dancing" or "shaking hands" instead of forcing a choice from a small set of classes.

- Using knowledge distillation with a large language model on noisy internet image captions to create pseudo-labels describing interactions. This provides training data without expensive manual annotation.

- Introducing the WALDO dataset, a manually annotated benchmark for evaluating HHI understanding models by comparing predicted free text to ground truth descriptions. 

- Showing both qualitatively and quantitatively that models trained on the pseudo-labels can produce more accurate free text for HHI than baseline captioning models. This demonstrates the utility of the proposed pseudo-labelling technique.

In summary, the main contribution is a new paradigm and method for learning to describe the heavy tail of possible human-human interactions in images using weak supervision, instead of being limited to categorical recognition. The pseudo-labelling approach and benchmark dataset enable training models that can generate freer, more accurate textual descriptions of interactions.
