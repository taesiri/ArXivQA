# [Learning Human-Human Interactions in Images from Weak Textual   Supervision](https://arxiv.org/abs/2304.14104)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading, the central research question this paper aims to address is: 

How can we model the diverse and heavy tail of possible human-human interactions from images using only weak supervision from textual captions?

More specifically, the key points are:

- Prior works have treated human-human interactions (HHI) categorically, using a small fixed set of classes. But HHI are diverse and nuanced. 

- The authors propose modeling HHI in images as free text instead of fixed classes. This allows capturing the unlimited variety of possible interactions.

- Learning HHI as free text is challenging due to limited labelled data. The authors use weak supervision from image-caption pairs on the internet.

- They extract pseudo-labels for HHI from noisy internet captions using knowledge distillation and a large language model, without explicit annotation.

- They show these pseudo-labels can train models to effectively predict HHI in images, outperforming prior categorical models.

In summary, the core research question is how to model the long tail of human-human interactions in images as free text, using only weak supervision from readily available image-caption data, rather than small manually annotated datasets. The key novelty is the use of knowledge distillation to create pseudo-labels for this task.


## What is the main contribution of this paper?

 The main contribution of this paper is proposing a new paradigm for learning to understand human-human interactions (HHI) in images as free text, allowing more flexibility than categorical recognition. The key ideas are:

- Modeling HHI as free text instead of fixed categories to capture the vast variety of possible interactions between people. This allows generating descriptions like "dancing" or "shaking hands" instead of forcing a choice from a small set of classes.

- Using knowledge distillation with a large language model on noisy internet image captions to create pseudo-labels describing interactions. This provides training data without expensive manual annotation.

- Introducing the WALDO dataset, a manually annotated benchmark for evaluating HHI understanding models by comparing predicted free text to ground truth descriptions. 

- Showing both qualitatively and quantitatively that models trained on the pseudo-labels can produce more accurate free text for HHI than baseline captioning models. This demonstrates the utility of the proposed pseudo-labelling technique.

In summary, the main contribution is a new paradigm and method for learning to describe the heavy tail of possible human-human interactions in images using weak supervision, instead of being limited to categorical recognition. The pseudo-labelling approach and benchmark dataset enable training models that can generate freer, more accurate textual descriptions of interactions.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the paper:

The paper proposes learning to understand human-human interactions in images by generating free text descriptions of interactions using knowledge distillation and weak supervision from image captions.


## How does this paper compare to other research in the same field?

 This paper presents a new approach for learning to understand human-human interactions (HHI) in images using weak supervision from image captions. Here are some key ways it compares to prior work:

- Focuses on modeling HHI as free text rather than categorical labels. Most prior work has focused on classifying HHI into a small, fixed set of categories. Using free text enables capturing more diverse interactions.

- Leverages weak supervision from captions via knowledge distillation and abstractive summarization. Many existing datasets for HHI understanding come from video data. This work shows how to extract pseudo-labels for HHI from image captions without manual annotation.

- Presents new metrics for evaluating HHI in images. Since HHI is modeled as free text, new metrics are proposed to measure properties like textual similarity, factual consistency, and verb similarity.

- Demonstrates strong quantitative improvements over baseline captioning models by training on the pseudo-labels. The best model achieves substantial gains in metrics like BLEURT and verb similarity on the proposed benchmarks.

- Releases a new manually annotated test set modeling diverse HHI, unlike most categorically labelled datasets. The variety of interactions is meant to reflect the heavy tail.

Overall, this work pioneers a new paradigm for HHI understanding compared to categorical classification approaches. The weak supervision method allows creating pseudo-labels from abundant image-caption data. The results demonstrate that modelling HHI as free text better captures diverse real-world interactions between people.
