# [Controlled Training Data Generation with Diffusion Models](https://arxiv.org/abs/2403.15309)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper "Controlled Training Data Generation with Diffusion Models":

Problem:
- Real-world test conditions change over time, while training datasets remain static due to high collection costs. This causes a distribution shift between training and test data which hurts model performance.
- Existing open-loop data generation methods lack automatic feedback mechanisms to refine and improve the generated data.
- Simply generating more diverse data without considering the target distribution or model's failure modes can be inefficient.

Proposed Solution:
- Use text-to-image diffusion models to generate synthetic training data. Condition the model on both a textual prompt and a label to produce aligned image-label pairs.
- Introduce two feedback mechanisms:
   1) Model-informed feedback: Find "adversarial prompts" that maximize the loss of a target model, reflecting its failure modes.
   2) Target distribution-informed feedback: Guide prompts to match a target distribution using similarity of CLIP embeddings between generated and target images/descriptions.
- Combine both mechanisms into "Guided Adversarial Prompts" (GAP) to efficiently generate useful training data.

Contributions:
- A closed-loop framework with two feedback mechanisms for controlled data generation.
- Model-informed prompts find diverse and adversarial images to improve a given model.
- Target distribution guidance steers generations towards a specific distribution shift.
- Evaluated on classification, depth estimation tasks over multiple datasets and models. GAP consistently improves data-efficiency over open-loop baselines.
- Demonstrates the benefit of closed-loop, guided data generation over fixed, open-loop approaches.

The key insight is the use of feedback loops during data generation to efficiently produce useful training data for adapting models to new distributions. Both model- and distribution-informed guidance are crucial for sample-efficient adaptation.
