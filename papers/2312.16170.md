# [EmbodiedScan: A Holistic Multi-Modal 3D Perception Suite Towards   Embodied AI](https://arxiv.org/abs/2312.16170)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
There is a gap between the computer vision research on global, scene-level 3D perception tasks and the practical needs of embodied AI agents, which require ego-centric, holistic 3D scene understanding grounded in natural language interactions. Existing datasets and models do not sufficiently support research in this direction.

Proposed Solution: 
The paper introduces EmbodiedScan, a large-scale multi-modal dataset for ego-centric 3D scene perception, containing:
- Over 5k RGB-D video scans with ego-centric views 
- Dense 3D object annotations - 1M boxes over 760 categories 
- Occupancy maps with semantics
- 1M visually-grounded language descriptions 

It also proposes Embodied Perceptron, a multi-modal neural network framework that accepts variable numbers of RGB-D views and language queries as inputs for holistic 3D scene understanding tasks like 3D detection, occupancy prediction and visual grounding.

Main Contributions:
- New benchmark for embodied AI with rich multi-modal indoor scan data
- Scalable model architecture for ego-centric 3D perception 
- Fundamental and language-grounded 3D perception benchmarks on the dataset
- Analysis of model performance showing gaps to be addressed for embodied agents

The work helps connect global 3D perception research to the practical needs of future embodied AI systems through appropriate data, models and analysis. Challenges highlighted can drive further research towards real-world readiness in this area.
