# Difformer: Empowering Diffusion Models on the Embedding Space for Text   Generation

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the key focus of this paper is on studying the challenges of adapting continuous diffusion models to discrete textual data, particularly when generating text in the embedding space. The main hypotheses/questions appear to be:1) How does using learnable embedding spaces affect diffusion model training compared to fixed data spaces like images? Can it lead to issues like collapse of the loss function?2) How does the imbalanced distribution of token frequencies and resulting varied embedding norms affect adding gaussian noise uniformly during diffusion?3) Does the standard gaussian noise schedule provide sufficient training signal across diffusion steps or is the model insufficiently trained on some steps?The paper then proposes solutions to address these challenges, including using an additional anchor loss, embedding normalization, and amplifying noise. Experiments demonstrate these improve performance of embedding diffusion models on text generation tasks like machine translation and summarization.In summary, the key focus seems to be on analyzing the unique challenges of adapting continuous diffusion models to the learnable embedding space for text, and proposing techniques to address them. The central hypothesis appears to be that properly accounting for properties of embeddings will improve text generation performance for diffusion models.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contributions appear to be:- Conducting a thorough study of the challenges of applying continuous diffusion models to textual data in the embedding space. The key challenges identified are the collapse of the denoising objective function, imbalanced embedding scales, and insufficient training due to inadequate noise. - Proposing a model called Difformer to address these challenges. Difformer introduces an anchor loss function to regularize embeddings, a layer normalization module to normalize embedding scales, and a noise factor to increase the amount of noise during training.- Evaluating Difformer on machine translation and text summarization tasks. Results show Difformer outperforms previous diffusion-based models as well as iterative non-autoregressive models.In summary, the main contribution seems to be proposing and evaluating the Difformer model to overcome key challenges that arise when adapting continuous diffusion models to textual data in the embedding space. The paper provides both analysis of the problems and solutions in the form of the Difformer model.
