# [DisWOT: Student Architecture Search for Distillation WithOut Training](https://arxiv.org/abs/2303.15678)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading, the central research question this paper addresses is:How to efficiently search for the optimal student architecture for a given teacher model to maximize knowledge distillation performance, without needing to train candidate models during the search?The key hypothesis is that similarity metrics between randomly initialized teacher and student models, in terms of feature semantics and sample relations, can serve as effective proxies during architecture search to predict eventual distillation performance after training.In summary, this paper proposes a training-free framework called DisWOT to search for the best student architecture to match a given teacher for knowledge distillation. It uses proposed similarity metrics as proxies during the search to avoid costly training of candidates. The goal is to find a student architecture that can maximize knowledge transfer and distillation performance when trained later.


## What is the main contribution of this paper?

 The main contributions of this paper are:1. It proposes DisWOT, a novel training-free framework to search for the best student architecture for a given teacher model in knowledge distillation. 2. It introduces new zero-cost metrics based on similarity of feature semantics and sample relations between teacher and student models. These metrics are used to select the optimal student architecture via an evolutionary algorithm without any training.3. The paper shows that the proposed DisWOT framework significantly improves the model's accuracy and efficiency tradeoff in inference, achieving over 180x training acceleration.4. It achieves state-of-the-art distillation performance on CIFAR, ImageNet and NAS-Bench datasets compared to other methods.5. The paper reveals close connections between knowledge distillation and neural architecture search by proposing and evaluating a series of KD-based zero proxies.In summary, the key contribution is a training-free student architecture search method for knowledge distillation that uses semantic and relational similarity metrics to find optimal student models. This achieves superior accuracy and efficiency without costly training or search.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the paper:The paper proposes a new training-free framework called DisWOT that searches for optimal student architectures to match a given teacher model for knowledge distillation, achieving better performance and acceleration without any training costs.


## How does this paper compare to other research in the same field?

 Here are some key points comparing this paper to other related research:- This paper introduces a new training-free framework called DisWOT for searching optimal student architectures for knowledge distillation. Most prior work on adapting student models for distillation uses training-based methods like architecture search or designing specialized loss functions/distillation techniques. DisWOT offers a more efficient alternative without any training.- DisWOT proposes new zero-cost metrics to evaluate similarity of feature semantics and sample relations between teacher and student models. These metrics aim to select student models that can better match the teacher's knowledge. Other zero-cost NAS methods tend to use more generic metrics related to model capacity. - The paper shows DisWOT can find students that outperform hand-designed models when distilled from the same teacher. This highlights the benefit of tailoring the student architecture specifically for distillation rather than just using off-the-shelf smaller models.- DisWOT is evaluated on diverse datasets (CIFAR, ImageNet) and search spaces. Experiments show it can find new state-of-the-art architectures and significantly improve accuracy compared to knowledge distillation with fixed student models.- The paper makes connections between knowledge distillation objectives and zero-cost NAS proxies. Several distillation loss functions are shown to have good ranking correlation for architecture performance. This helps link the areas of distillation and NAS.Overall, DisWOT offers a novel perspective on optimizing student models for knowledge distillation by framing it as a zero-cost architecture search problem. The training-free approach and specialized similarity metrics help find better student architectures more efficiently compared to prior work. The connections made to NAS also open up new research directions.


## What future research directions do the authors suggest?

 Based on my reading of the paper, some key future research directions suggested by the authors are:- Expand DisWOT for downstream tasks beyond classification, such as object detection and semantic segmentation. The current work focuses on evaluating DisWOT for image classification tasks. Applying it to other computer vision tasks could be an interesting extension.- Investigate the use of DisWOT metrics as distillers and zero-proxies on larger datasets and search spaces. The authors propose using the DisWOT similarity metrics directly as distillation losses and demonstrate their effectiveness as zero-cost performance predictors. More extensive experiments could reveal their potential as general distillers and proxies. - Develop advanced similarity metrics building on the DisWOT framework. The authors propose semantic and relational similarity metrics in DisWOT. There is room to explore other kinds of similarity metrics to better match teacher-student knowledge.- Apply DisWOT to search spaces with larger architectures like transformers. The current experiments mainly deal with CNN search spaces. Evaluating how well DisWOT transfers to transformer-based search spaces could be impactful.- Combine DisWOT with adaptive training strategies. The authors use fixed training configurations in the distillation stage. Jointly optimizing the training process and architecture search could further boost the performance.- Expand the analysis and theoretical understanding of DisWOT. More analysis on how the similarity metrics correlate to distillation performance could provide useful insights. Formalizing the theoretical connections could strengthen the approach.In summary, the key suggestions are to expand DisWOT to more tasks, datasets, search spaces and training configurations, develop improved similarity metrics, and enhance the theoretical understanding. Advancing research along these directions can help realize the full potential of training-free distillation-aware architecture search.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the key points from the paper:This CVPR 2023 paper proposes DisWOT, a novel training-free framework to search for optimal student architectures to match a given teacher model for knowledge distillation. The key idea is to measure semantic and relational similarity between randomly initialized teacher-student pairs using proposed metrics based onGrad-CAM activation maps and sample correlation matrices. An evolution algorithm searches for the student architecture with highest similarity to the teacher without any training. Extensive experiments on CIFAR, ImageNet and NAS datasets demonstrate that DisWOT can find better students to improve accuracy-efficiency tradeoffs in inference through distillation, with 180-1000x less training than typical NAS methods. The proposed similarity metrics are also shown to work well as distillation objectives and zero-cost NAS proxies. Overall, DisWOT effectively matches student models to teachers by architectural search without training, setting a new state-of-the-art for architecture search for knowledge distillation.


## Summarize the paper in two paragraphs.

 Here are two paragraphs summarizing the key points of the paper:This paper proposes DisWOT, a training-free framework for searching optimal student architectures for knowledge distillation. DisWOT is motivated by the observation that large gaps between teacher and student models limit distillation performance. To address this, DisWOT introduces two new zero-cost metrics to measure the similarity of feature semantics and sample relations between randomly initialized teacher-student pairs. These metrics demonstrate strong correlation with final distillation accuracy and are used to guide an evolutionary search to find better matched students for a given teacher, without any training. Extensive experiments validate that DisWOT consistently discovers students that achieve superior accuracy after distillation across diverse datasets and search spaces, significantly outperforming prior zero-shot NAS methods. The authors also extend the similarity metrics in DisWOT to new distillation objectives, further boosting performance. Overall, DisWOT provides an efficient and effective approach to student architecture search for knowledge distillation.In more detail, DisWOT proposes two main techniques: 1) New zero-cost metrics to measure similarity of semantic localization maps and sample relation matrices between teacher and student models, obtained efficiently from random initialization. These are shown to strongly correlate with final distillation accuracy after training. 2) An evolutionary architecture search guided by the similarity metrics to find optimal students matched to a teacher, completely avoiding expensive training. The searched students consistently achieve state-of-the-art accuracy after distillation across multiple datasets and search spaces. DisWOT also expands the similarity metrics into new distillation objectives to further improve performance. Experiments comprehensively demonstrate the superiority of DisWOT over prior NAS methods for distillation-based training.


## Summarize the main method used in the paper in one paragraph.

 Here is a one paragraph summary of the main method used in this paper:The paper proposes a new training-free framework called DisWOT for searching optimal student architectures for knowledge distillation with a given teacher model. DisWOT introduces two novel zero-cost metrics - semantic similarity and relation similarity between teacher and student models, which are measured based on similarities of Grad-CAM activation maps and sample correlation matrices respectively. These metrics are used by an evolutionary search algorithm to select the best student architecture from a search space, without needing to train any candidates. Once the optimal architecture is identified, it is distilled using the teacher model. DisWOT avoids expensive training of candidates during search by exploiting similarities of untrained teacher-student model pairs. The key novelty is using higher-order statistics like Grad-CAM activations rather than just model weights to guide the search process in a completely training-free manner.


## What problem or question is the paper addressing?

 This paper is addressing the problem of reducing the performance gap between large teacher models and small student models in knowledge distillation. Specifically, it points out that existing distillation methods struggle to transfer knowledge effectively when there is a large difference in capacity between the teacher and student models. The key question the paper tries to answer is: how can we automatically search for the best student architecture for a given teacher model in order to maximize the distillation performance?The main contributions in addressing this question are:- Empirically demonstrating the discrepancy between vanilla training accuracy and distillation accuracy, showing that an optimal model under vanilla training is not necessarily the best for distillation.- Proposing novel zero-cost metrics based on feature semantic similarity and sample relation similarity to better predict distillation performance without training.- Introducing a training-free framework called DisWOT that searches for the optimal student architecture using these metrics and an evolutionary algorithm.- Achieving state-of-the-art distillation performance with at least 180x training acceleration by finding better matched students.- Expanding the similarity metrics as new distillers and KD-based zero-proxies to connect distillation with NAS.In summary, the key innovation is using higher-order feature statistics to optimize the student model architecture itself to fit the teacher better, in contrast to previous work on adapting the distillation process. This allows much more efficient architecture search focused directly on distillation performance.
