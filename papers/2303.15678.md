# [DisWOT: Student Architecture Search for Distillation WithOut Training](https://arxiv.org/abs/2303.15678)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central research question this paper addresses is:How to efficiently search for the optimal student architecture for a given teacher model to maximize knowledge distillation performance, without needing to train candidate models during the search?The key hypothesis is that similarity metrics between randomly initialized teacher and student models, in terms of feature semantics and sample relations, can serve as effective proxies during architecture search to predict eventual distillation performance after training.In summary, this paper proposes a training-free framework called DisWOT to search for the best student architecture to match a given teacher for knowledge distillation. It uses proposed similarity metrics as proxies during the search to avoid costly training of candidates. The goal is to find a student architecture that can maximize knowledge transfer and distillation performance when trained later.


## What is the main contribution of this paper?

The main contributions of this paper are:1. It proposes DisWOT, a novel training-free framework to search for the best student architecture for a given teacher model in knowledge distillation. 2. It introduces new zero-cost metrics based on similarity of feature semantics and sample relations between teacher and student models. These metrics are used to select the optimal student architecture via an evolutionary algorithm without any training.3. The paper shows that the proposed DisWOT framework significantly improves the model's accuracy and efficiency tradeoff in inference, achieving over 180x training acceleration.4. It achieves state-of-the-art distillation performance on CIFAR, ImageNet and NAS-Bench datasets compared to other methods.5. The paper reveals close connections between knowledge distillation and neural architecture search by proposing and evaluating a series of KD-based zero proxies.In summary, the key contribution is a training-free student architecture search method for knowledge distillation that uses semantic and relational similarity metrics to find optimal student models. This achieves superior accuracy and efficiency without costly training or search.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper proposes a new training-free framework called DisWOT that searches for optimal student architectures to match a given teacher model for knowledge distillation, achieving better performance and acceleration without any training costs.


## How does this paper compare to other research in the same field?

Here are some key points comparing this paper to other related research:- This paper introduces a new training-free framework called DisWOT for searching optimal student architectures for knowledge distillation. Most prior work on adapting student models for distillation uses training-based methods like architecture search or designing specialized loss functions/distillation techniques. DisWOT offers a more efficient alternative without any training.- DisWOT proposes new zero-cost metrics to evaluate similarity of feature semantics and sample relations between teacher and student models. These metrics aim to select student models that can better match the teacher's knowledge. Other zero-cost NAS methods tend to use more generic metrics related to model capacity. - The paper shows DisWOT can find students that outperform hand-designed models when distilled from the same teacher. This highlights the benefit of tailoring the student architecture specifically for distillation rather than just using off-the-shelf smaller models.- DisWOT is evaluated on diverse datasets (CIFAR, ImageNet) and search spaces. Experiments show it can find new state-of-the-art architectures and significantly improve accuracy compared to knowledge distillation with fixed student models.- The paper makes connections between knowledge distillation objectives and zero-cost NAS proxies. Several distillation loss functions are shown to have good ranking correlation for architecture performance. This helps link the areas of distillation and NAS.Overall, DisWOT offers a novel perspective on optimizing student models for knowledge distillation by framing it as a zero-cost architecture search problem. The training-free approach and specialized similarity metrics help find better student architectures more efficiently compared to prior work. The connections made to NAS also open up new research directions.


## What future research directions do the authors suggest?

Based on my reading of the paper, some key future research directions suggested by the authors are:- Expand DisWOT for downstream tasks beyond classification, such as object detection and semantic segmentation. The current work focuses on evaluating DisWOT for image classification tasks. Applying it to other computer vision tasks could be an interesting extension.- Investigate the use of DisWOT metrics as distillers and zero-proxies on larger datasets and search spaces. The authors propose using the DisWOT similarity metrics directly as distillation losses and demonstrate their effectiveness as zero-cost performance predictors. More extensive experiments could reveal their potential as general distillers and proxies. - Develop advanced similarity metrics building on the DisWOT framework. The authors propose semantic and relational similarity metrics in DisWOT. There is room to explore other kinds of similarity metrics to better match teacher-student knowledge.- Apply DisWOT to search spaces with larger architectures like transformers. The current experiments mainly deal with CNN search spaces. Evaluating how well DisWOT transfers to transformer-based search spaces could be impactful.- Combine DisWOT with adaptive training strategies. The authors use fixed training configurations in the distillation stage. Jointly optimizing the training process and architecture search could further boost the performance.- Expand the analysis and theoretical understanding of DisWOT. More analysis on how the similarity metrics correlate to distillation performance could provide useful insights. Formalizing the theoretical connections could strengthen the approach.In summary, the key suggestions are to expand DisWOT to more tasks, datasets, search spaces and training configurations, develop improved similarity metrics, and enhance the theoretical understanding. Advancing research along these directions can help realize the full potential of training-free distillation-aware architecture search.
