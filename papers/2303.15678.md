# [DisWOT: Student Architecture Search for Distillation WithOut Training](https://arxiv.org/abs/2303.15678)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central research question this paper addresses is:How to efficiently search for the optimal student architecture for a given teacher model to maximize knowledge distillation performance, without needing to train candidate models during the search?The key hypothesis is that similarity metrics between randomly initialized teacher and student models, in terms of feature semantics and sample relations, can serve as effective proxies during architecture search to predict eventual distillation performance after training.In summary, this paper proposes a training-free framework called DisWOT to search for the best student architecture to match a given teacher for knowledge distillation. It uses proposed similarity metrics as proxies during the search to avoid costly training of candidates. The goal is to find a student architecture that can maximize knowledge transfer and distillation performance when trained later.


## What is the main contribution of this paper?

The main contributions of this paper are:1. It proposes DisWOT, a novel training-free framework to search for the best student architecture for a given teacher model in knowledge distillation. 2. It introduces new zero-cost metrics based on similarity of feature semantics and sample relations between teacher and student models. These metrics are used to select the optimal student architecture via an evolutionary algorithm without any training.3. The paper shows that the proposed DisWOT framework significantly improves the model's accuracy and efficiency tradeoff in inference, achieving over 180x training acceleration.4. It achieves state-of-the-art distillation performance on CIFAR, ImageNet and NAS-Bench datasets compared to other methods.5. The paper reveals close connections between knowledge distillation and neural architecture search by proposing and evaluating a series of KD-based zero proxies.In summary, the key contribution is a training-free student architecture search method for knowledge distillation that uses semantic and relational similarity metrics to find optimal student models. This achieves superior accuracy and efficiency without costly training or search.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper proposes a new training-free framework called DisWOT that searches for optimal student architectures to match a given teacher model for knowledge distillation, achieving better performance and acceleration without any training costs.
