# [One Wide Feedforward is All You Need](https://arxiv.org/abs/2309.01826)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central research question this paper seeks to address is: What is the role and importance of the Feed Forward Network (FFN) component in Transformer models for machine translation? The key hypotheses explored are:1) The FFN components contain a high degree of redundancy across layers in the encoder and decoder.2) The FFN components, especially in the decoder, can be substantially reduced or even removed without major impacts on model accuracy.3) The parameters from pruned FFN components can be reallocated to expand the width of the remaining FFNs, allowing the model to regain accuracy while maintaining efficiency.So in summary, the paper investigates the redundancy and importance of FFNs through extensive experimentation with sharing, pruning, and expanding these components. The overarching goal is to understand if Transformer models can be made more parameter-efficient by rethinking the allocation of parameters from FFNs.
