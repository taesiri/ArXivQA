# [One Wide Feedforward is All You Need](https://arxiv.org/abs/2309.01826)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central research question this paper seeks to address is: What is the role and importance of the Feed Forward Network (FFN) component in Transformer models for machine translation? The key hypotheses explored are:1) The FFN components contain a high degree of redundancy across layers in the encoder and decoder.2) The FFN components, especially in the decoder, can be substantially reduced or even removed without major impacts on model accuracy.3) The parameters from pruned FFN components can be reallocated to expand the width of the remaining FFNs, allowing the model to regain accuracy while maintaining efficiency.So in summary, the paper investigates the redundancy and importance of FFNs through extensive experimentation with sharing, pruning, and expanding these components. The overarching goal is to understand if Transformer models can be made more parameter-efficient by rethinking the allocation of parameters from FFNs.


## What is the main contribution of this paper?

The main contribution of this paper is exploring the role of the Feed Forward Network (FFN) in Transformer models for machine translation. The key findings are:- The FFN contains a high degree of redundancy between layers and between the encoder and decoder. This allows substantially reducing the number of parameters by sharing or removing FFNs with minimal impact on accuracy.- Sharing a single FFN across all encoder layers and removing the decoder FFN leads to significant parameter savings and faster inference speed with only a small drop in BLEU score. - Increasing the dimension of the shared encoder FFN while removing the decoder FFN results in a model with similar size but improved accuracy compared to the baseline Transformer Big. This One Wide FFN model achieves gains of up to 0.9 BLEU while also being faster.- Analysis of the internal representations shows the proposed models have high similarity to the baseline Transformer, indicating they capture similar information despite having fewer redundant parameters. In summary, the paper shows the FFN can be substantially reduced or shared across layers with little impact on accuracy. The proposed One Wide FFN model is a very parameter-efficient configuration that outperforms the baseline Transformer Big.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper explores reducing redundant parameters in the Transformer architecture for machine translation by sharing or removing the feedforward network (FFN) across encoder and decoder layers, finding that sharing a single widened FFN in the encoder and removing the decoder FFN maintains accuracy while substantially reducing parameters and latency.


## How does this paper compare to other research in the same field?

Here are a few key ways this paper compares to other related research:- The paper explores reducing redundancy in transformer models, a topic that has been studied from other angles like pruning attention heads. This work provides a new perspective by focusing on sharing/removing feedforward networks (FFNs).- Most prior work on parameter sharing for transformers has involved some form of layer-wise sharing, where subsets of layers share parameters. This paper shows that sharing a single FFN across all encoder layers works just as well, a more extreme form of sharing.- While some previous papers have proposed sharing FFN parameters across encoder layers, this paper takes it further by completely removing the FFN from the decoder. Showing the decoder FFN can be removed with little accuracy drop is a novel finding.- Many works have proposed modifications to the transformer architecture itself (e.g. replacing FFN with sparse networks). This paper sticks to the standard architecture but alters FFN sharing, making the findings broadly applicable.- The analysis of representational similarity between models using techniques like CKA is thorough, going beyond just reporting accuracy. This provides insight into how the internal representations change with different FFN sharing schemes.- The extensive experiments on multiple datasets, languages, and model types (multilingual, low-resource, etc) demonstrate the generality of the core findings.In summary, this paper provides a comprehensive exploration of FFN redundancy in transformers, using both extensive experiments and representation analysis. The specific findings around severe FFN sharing are novel, as is the complete removal of the decoder FFN. The work builds on prior research while carving out a unique perspective on efficient transformers.


## What future research directions do the authors suggest?

The authors suggest a few potential future research directions:- Exploring ways to further reduce redundancy in Transformer models. The paper shows the FFN contains redundancy, but there may be additional opportunities to reduce redundancy in attention mechanisms or other components. Identifying and removing other redundant parts could lead to even more efficient models.- Applying the approach of sharing/dropping FFNs to other sequence modeling tasks beyond machine translation, such as text summarization, question answering, etc. The paper focuses only on MT so it's unknown if the findings generalize. Testing on more tasks would help validate the broad applicability.- Developing more advanced methods for sharing parameters across layers. The paper uses simple tied weights, but more sophisticated sharing approaches like conditional computation could be explored. This might allow models to adaptively determine which layers to share based on the input.- Analyzing the effect of shared/dropped FFNs when scaling up model size and data. The paper tests mainly on base/big architectures, but it's unclear if the trends hold when going to even bigger models trained on more data.- Studying whether redundancy reduction through FFN sharing impacts robustness or generalization. The representations may end up less redundant but it's unknown if that affects susceptibility to adversarial examples or performance on out-of-distribution data.In summary, the main future directions are 1) finding additional avenues for redundancy reduction, 2) testing the approach on more tasks, 3) developing more advanced parameter sharing methods, and 4) understanding how it interacts with scaling and generalization. Overall the paper opens up many interesting research questions around efficient and compact Transformer design.


## Summarize the paper in one paragraph.

Here is a one paragraph summary of the key points in the paper:This paper explores the role of the Feed Forward Network (FFN) in Transformer models for machine translation. The authors find that the FFN contains a high degree of redundancy between layers and between the encoder and decoder. By sharing or removing the FFN across layers, they are able to substantially reduce parameters and increase inference speed with only a minor drop in BLEU score. In particular, they find sharing a single FFN across the encoder and removing the FFN entirely from the decoder performs well. Furthermore, widening this shared encoder FFN recovers the parameters and accuracy lost from sharing and dropping FFNs, and even improves upon the baseline Transformer Big model. The authors conduct similarity analyses showing the internal representations of the reduced models remain highly similar to the baseline Transformer. Overall, the paper demonstrates the FFN's redundancy and that Transformer models can be made much more efficient by appropriately sharing or removing the FFN.
