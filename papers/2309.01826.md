# [One Wide Feedforward is All You Need](https://arxiv.org/abs/2309.01826)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central research question this paper seeks to address is: What is the role and importance of the Feed Forward Network (FFN) component in Transformer models for machine translation? The key hypotheses explored are:1) The FFN components contain a high degree of redundancy across layers in the encoder and decoder.2) The FFN components, especially in the decoder, can be substantially reduced or even removed without major impacts on model accuracy.3) The parameters from pruned FFN components can be reallocated to expand the width of the remaining FFNs, allowing the model to regain accuracy while maintaining efficiency.So in summary, the paper investigates the redundancy and importance of FFNs through extensive experimentation with sharing, pruning, and expanding these components. The overarching goal is to understand if Transformer models can be made more parameter-efficient by rethinking the allocation of parameters from FFNs.


## What is the main contribution of this paper?

The main contribution of this paper is exploring the role of the Feed Forward Network (FFN) in Transformer models for machine translation. The key findings are:- The FFN contains a high degree of redundancy between layers and between the encoder and decoder. This allows substantially reducing the number of parameters by sharing or removing FFNs with minimal impact on accuracy.- Sharing a single FFN across all encoder layers and removing the decoder FFN leads to significant parameter savings and faster inference speed with only a small drop in BLEU score. - Increasing the dimension of the shared encoder FFN while removing the decoder FFN results in a model with similar size but improved accuracy compared to the baseline Transformer Big. This One Wide FFN model achieves gains of up to 0.9 BLEU while also being faster.- Analysis of the internal representations shows the proposed models have high similarity to the baseline Transformer, indicating they capture similar information despite having fewer redundant parameters. In summary, the paper shows the FFN can be substantially reduced or shared across layers with little impact on accuracy. The proposed One Wide FFN model is a very parameter-efficient configuration that outperforms the baseline Transformer Big.
