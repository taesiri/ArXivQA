# [Grammar Prompting for Domain-Specific Language Generation with Large   Language Models](https://arxiv.org/abs/2305.19234)

## What is the central research question or hypothesis that this paper addresses?

This paper explores using grammar prompting as an approach to enable large language models (LLMs) to generate strings from structured, domain-specific languages (DSLs) after being trained on just a handful of examples. The key hypothesis is that augmenting demonstration examples with specialized grammars that are minimally sufficient to generate the example outputs will allow LLMs to better acquire and apply the constraints and semantics of a complex DSL from a small number of examples.The paper tests this hypothesis across a diverse set of DSLs for semantic parsing, AI planning, and molecule generation. The results generally support the hypothesis, showing that grammar prompting can improve upon standard prompting baselines in the few-shot setting across these domains.In summary, the central hypothesis is that grammar prompting, where each example is augmented with a specialized grammar, can enable few-shot learning of complex DSLs with LLMs. The experiments on diverse DSLs provide evidence supporting this hypothesis.
