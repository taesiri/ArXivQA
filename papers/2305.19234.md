# [Grammar Prompting for Domain-Specific Language Generation with Large   Language Models](https://arxiv.org/abs/2305.19234)

## What is the central research question or hypothesis that this paper addresses?

This paper explores using grammar prompting as an approach to enable large language models (LLMs) to generate strings from structured, domain-specific languages (DSLs) after being trained on just a handful of examples. The key hypothesis is that augmenting demonstration examples with specialized grammars that are minimally sufficient to generate the example outputs will allow LLMs to better acquire and apply the constraints and semantics of a complex DSL from a small number of examples.The paper tests this hypothesis across a diverse set of DSLs for semantic parsing, AI planning, and molecule generation. The results generally support the hypothesis, showing that grammar prompting can improve upon standard prompting baselines in the few-shot setting across these domains.In summary, the central hypothesis is that grammar prompting, where each example is augmented with a specialized grammar, can enable few-shot learning of complex DSLs with LLMs. The experiments on diverse DSLs provide evidence supporting this hypothesis.


## What is the main contribution of this paper?

The main contribution of this paper is proposing a method called "grammar prompting" to improve the few-shot learning capabilities of large language models (LLMs) for generating outputs in highly structured domain-specific languages (DSLs). Specifically, the key ideas are:- Augment each demonstration example with a "specialized" BNF grammar that is minimally sufficient to generate that particular output. This grammar is a subset of the full DSL grammar.- During inference, first predict a BNF grammar given the input, and then generate the output according to the predicted grammar.- The specialized grammar focuses the LLM on the necessary symbolic manipulations needed for the given input-output pair.- The use of a formal BNF grammar also allows imposing syntactic constraints during decoding.Experiments show that grammar prompting improves few-shot learning for diverse DSL generation tasks like semantic parsing, PDDL planning, and molecule generation. This demonstrates the potential of harnessing LLMs for structured language generation by combining their pretrained skills with external knowledge expressed through grammars.In summary, the key contribution is proposing and evaluating the grammar prompting method to improve few-shot structured language generation with LLMs. The approach draws on the metalanguage capabilities acquired through pretraining while using grammars to focus reasoning.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper proposes using specialized context-free grammars within prompts to improve few-shot learning of structured language generation tasks like semantic parsing and molecule generation with large language models.
