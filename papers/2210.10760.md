# [Scaling Laws for Reward Model Overoptimization](https://arxiv.org/abs/2210.10760)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the main research question seems to be: How does the relationship between optimizing against a learned proxy reward model and the resulting true reward (as defined by a fixed "gold standard" model) follow certain scaling laws, and how do the coefficients in these laws depend on properties of the models?In particular, the authors investigate how the overoptimization scaling laws differ between reinforcement learning (RL) and best-of-n (BoN) sampling, and how the coefficients in these laws scale with the number of parameters in the proxy reward model, the amount of training data for the proxy model, and the size of the policy model being optimized.The key findings related to this question include:- The relationship between true reward and policy divergence from the initial policy follows different functional forms for RL vs BoN, but the coefficients scale smoothly with model size in both cases.- More proxy model parameters and training data lead to higher true reward and less overoptimization, as measured by the coefficients. - Policy model size does not substantially change the relationship between proxy and true reward.- Adding a KL penalty to RL does not improve the true reward, just causes earlier convergence.So in summary, the central hypothesis is that overoptimization due to proxy reward models follows smooth scaling laws that can be characterized empirically. The results provide support for this hypothesis and characterize the precise relationships.


## What is the main contribution of this paper?

Based on my reading of the paper abstract, the main contributions appear to be:- Empirically deriving functional forms for the relationship between the gold/ground truth reward and the KL divergence from the initial policy for two optimization methods - best-of-n sampling and reinforcement learning. Specifically, the paper finds:Best-of-n: R(d) = d(α - βd) Reinforcement learning: R(d) = d(α - βlogd)Where d is the square root of the KL divergence.- Showing that the coefficients α and β in these functional forms scale smoothly with properties like the number of reward model parameters and size of the training dataset. This allows prediction of the attained gold reward.- Observing that larger policies benefit less from optimizing against a reward model, but do not actually overoptimize faster in terms of KL divergence. - Discussing implications of these empirical findings for theoretical models of Goodhart's law and alignment. The scaling laws provide insight into different kinds of Goodhart effects occurring.So in summary, the key contributions appear to be deriving empirical scaling laws characterizing the overoptimization phenomenon in two settings, and discussing the theoretical implications. The fits to smooth functional forms and their coefficient scaling seem to be the most novel empirical contributions.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper empirically studies how optimizing against an imperfect learned reward model leads to overoptimization, finding this relationship follows different functional forms for best-of-n sampling versus reinforcement learning, with coefficients that scale smoothly with model size; it discusses implications for alignment, existing Goodhart taxonomies, and using KL divergence to compare optimization algorithms.


## How does this paper compare to other research in the same field?

Here are a few key points comparing this paper to other related work in AI safety/alignment and reinforcement learning from human feedback:- This paper takes a very empirical approach, systematically studying overoptimization and developing scaling laws through extensive experiments. Much existing work is more theoretical/conceptual. The empirical focus and large-scale experiments are a distinctive contribution.- The use of a synthetic "gold standard" reward model to simulate human feedback is a clever technique that allows large-scale experiments without expensive human labeling. This enables studying trends and scaling laws that would not be feasible with actual human data.- The functional forms fit to the empirical data, characterizing how ground truth reward decreases with overoptimization of a proxy reward model, are novel. Prior work has not derived such analytical characterizations.- The breakdown using the taxonomy of different kinds of Goodhart effects (regressional, extremal, causal, adversarial) connects the empirical results to conceptual theory in a meaningful way.- The experiments focus narrowly on reward model overoptimization in RLHF, rather than exploring different environments or other facets of AI safety. This focused scope enables methodical experiments and clearer conclusions.- The analysis of scaling with different hyperparameters provides engineering insights for implementing RLHF systems. Most prior alignment theory papers lack this practical grounding.- The work builds directly on prior RLHF papers like Anthropic's, but makes a distinct contribution through comprehensive hyperparameter scaling studies.Overall, the tight integration of empirical findings and conceptual theory within a narrow scope differentiates this work from most prior research and enables it to make clear contributions to both the practical engineering and conceptual understanding of an important AI safety challenge. The scaling laws and insights into different modes of overoptimization are significant additions to the field.


## What future research directions do the authors suggest?

Based on my reading of the paper, some of the main future research directions suggested by the authors include:- Validating these results on other environments and experimental setups beyond just the InstructGPT environment used in this work. The authors note that confirming whether these findings generalize more broadly would be valuable.- Further validating the synthetic data setup used here, since the simulated "gold" reward model may not fully capture dynamics with real human feedback. - Investigating methods for making reward models more robust to optimization, as there is still much work to be done in this area.- Exploring other forms of optimization beyond just best-of-n sampling and PPO, and categorizing differences between optimization algorithms.- Better understanding the functional form of proxy reward model scores, which the authors found more difficult to model than gold reward scores. - Empirically exploring adversarial Goodhart, since the models in this work are not powerful enough to exhibit it.- More carefully exploring scaling trends with policy size, since the authors only tested two policy sizes.- Testing the assumptions made about multi-iteration RLHF and seeing if they hold empirically.In summary, the authors call for more work to confirm the generality of their findings, better understand proxy reward modeling, systematically compare optimization algorithms, account for adversarial attacks, and validate assumptions about iterative training.


## Summarize the paper in one paragraph.

Here is a one paragraph summary of the key points from the paper:The paper studies overoptimization of neural network reward models used in reinforcement learning, where optimizing too much against an imperfect reward model proxy leads to reduced performance on the true reward (as per Goodhart's law). Using a synthetic setup with a fixed "gold standard" reward model in place of human labels, they characterize how the gold reward changes as a function of the divergence between the initial and optimized policies. They find this relationship follows different functional forms for best-of-n sampling versus policy gradient RL, with coefficients that scale smoothly with model size. Key results include: 1) Larger models overoptimize less; 2) More data leads to less overoptimization; 3) The optimization inefficiency of RL versus best-of-n; 4) Ineffectiveness of KL penalties in RL for reducing overoptimization. The implications for alignment, limitations of the synthetic setup, and directions for future work are also discussed. Overall, this provides empirical validation of the functional forms of overoptimization and scaling laws with respect to divergence and model size.


## Summarize the paper in two paragraphs.

Here is a two paragraph summary of the paper:The paper explores how the performance on a "gold standard" reward measure changes as policies are optimized against learned "proxy" reward models, using either reinforcement learning or best-of-n sampling. The authors find that this relationship follows different functional forms for RL versus best-of-n, but the coefficients scale smoothly with properties like the number of reward model parameters. Specifically, for best-of-n sampling the relationship follows R(d) = d(α - βd) and for RL it follows R(d) = d(α - βlog(d)), where d is the KL divergence from the initial policy. The authors discuss implications of these empirical laws. The coefficients can be interpreted in terms of different theoretical categories of Goodhart's law - regressional, causal, extremal and adversarial. The smooth scaling suggests ways to predict overoptimization, which could inform approaches like using fresh human feedback to periodically update the reward model. Limitations include reliance on a synthetic setup and lack of extremely large models. But overall this work makes progress towards empirically-grounded theories of specification gaming and alignment.


## Summarize the main method used in the paper in one paragraph.

The paper presents an empirical study of overoptimization of neural network reward models. The authors use a synthetic setup where a large pretrained "gold" reward model provides the ground truth rewards. Smaller reward models are trained to match the gold model's scores. These proxy reward models are then optimized against using either reinforcement learning or best-of-n sampling. The relationship between the proxy reward and the gold reward is studied, and found to follow different functional forms depending on the optimization method. The coefficients of these forms scale systematically with factors like the number of reward model parameters. The authors find that both methods exhibit overoptimization, where the proxy reward continues increasing while the gold reward decreases. They discuss implications for alignment research. The synthetic setup allows scalable study of overoptimization while avoiding the expense of human labeling. Overall, this work helps characterize the empirical behavior of neural network reward model overoptimization across different methods and model sizes.


## What problem or question is the paper addressing?

Based on my reading, the key points of this paper are:- It studies overoptimization when training language models using reward models trained on human preferences. Overoptimization refers to the phenomenon where optimizing too much against an imperfect reward model proxy leads to worse performance on the true reward.- The main problem addressed is lack of understanding of how overoptimization scales with various factors like reward model size, dataset size, policy size etc. Measuring this requires large amounts of human preference data. - To overcome the expense of human data, the authors use a synthetic setup where the "ground truth" reward is provided by a fixed large "gold" reward model instead of humans.- The main results are empirical scaling laws characterizing how the gold reward varies as the policy is optimized against a proxy reward model, using either reinforcement learning or best-of-n sampling. - These take the form of mathematical functions relating the gold reward to the KL divergence from initial to optimized policy. The coefficients follow smooth trends with factors like reward model size.- The implications for alignment and safe application of reward modeling are discussed. Limitations include use of a synthetic setup and focus on two optimization methods.In summary, the key problem is lack of understanding of reward model overoptimization and how it scales, which this paper aims to shed light on through a detailed empirical study using a synthetic setup.
