# [Deep ReLU Networks Preserve Expected Length](https://arxiv.org/abs/2102.10492v2)

## What is the central research question or hypothesis that this paper addresses?

 This paper addresses the question of how the depth of deep ReLU networks affects their ability to distort lengths and volumes of input manifolds. Specifically, it investigates whether the length/volume distortion of such networks grows exponentially with depth, as had been previously claimed. 

The central hypothesis is that, for ReLU networks with standard random initialization, the expected length and volume distortion does not grow exponentially with depth. Rather, the distortions remain bounded or decrease slightly with depth.

The key contributions are:

1) Proving upper bounds on the expected length distortion that show it does not grow exponentially with depth for standard random initialization. 

2) Generalizing this to prove upper bounds on higher moments of length distortion and on volume distortion.

3) Showing empirically that these bounds accurately capture the architecture dependence, unlike previous looser lower bounds.

4) Providing intuition for why distortion does not grow exponentially by relating it to properties of the input-output Jacobian and products of random matrices.

In summary, the paper challenges the prevailing view that depth leads to exponential length/volume distortion for ReLU nets, giving theory and experiments that show this distortion is well-controlled for standard initialization.


## What is the main contribution of this paper?

 This paper proves that the expected length distortion of deep ReLU networks does not grow exponentially with depth, contrary to some prior claims. The key contributions are:

1. The authors prove tight upper bounds on the expected length distortion that show it does not grow, and in fact shrinks slightly, with network depth. This is in contrast to previous work that only provided loose lower bounds.

2. They generalize the analysis to bound higher moments of length distortion, showing the variance is well-controlled. 

3. They extend the results to analyze how networks distort higher-dimensional volumes, again proving upper bounds. 

4. The theoretical predictions are shown to closely match experimental observations on randomly initialized networks, unlike previous looser bounds. 

5. The results apply to the standard He weight initialization used in practice, unlike some prior work that studied unrealistic weight settings.

So in summary, this paper provides a thorough theoretical and empirical analysis of how length and volume are distorted in realistic deep ReLU nets, proving they do not explode exponentially as depth increases, contrary to some prior belief. The results help explain why deep networks can compute well-behaved functions suitable for learning.
