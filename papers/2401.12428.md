# [CIM-MLC: A Multi-level Compilation Stack for Computing-In-Memory   Accelerators](https://arxiv.org/abs/2401.12428)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem: 
- There is a growing diversity of computing-in-memory (CIM) hardware architectures to accelerate deep neural networks (DNNs), using different memory devices, architecture organizations, and programming interfaces.
- Existing CIM compilation tools lack generality and fail to fully explore optimization opportunities across different hardware abstraction levels. They are often tailored to fixed architectures or only optimize at a single granularity level like matrix-vector multiplications.

Proposed Solution - CIM-MLC:
- Presents a unified hardware abstraction to represent diverse CIM architectures using a 3-tier model (chip/core/crossbar) with associated architecture parameters.
- Defines computing mode abstractions to capture different programming interface granularities - from coarse DNN operators to fine-grained vector operations.
- Proposes a multi-level scheduling approach that hierarchically optimizes DNN computation mapping and scheduling, from coarse operator graphs down to fine-grained vector computations, based on the hardware architecture exposed.
- Optimization techniques used at each level include operator duplication, pipelining, data remapping to improve parallelism and resource utilization.

Key Contributions:
- CIM-MLC is the first general-purpose compilation framework for diverse CIM hardware architectures. The unified abstractions enable automatically compiling to different devices, architectures, interfaces.
- Multi-level scheduling explores a broader optimization search space across abstraction tiers than prior single-level CIM compilers, leading to higher efficiency.
- Evaluated on 3 recent CIM designs, CIM-MLC improves speedup by 2.3-3.7x over prior scheduling methods and reduces power by 75% in one design.
- CIM-MLC reduces compilation effort for CIM hardware designers and provides a common interface for mapping DNN models to different architectures.

In summary, the paper presents a novel multi-level compilation framework to efficiently map DNN computations onto diverse and rapidly evolving CIM hardware architectures in an automated manner. The unified abstractions and hierarchical optimization approach generalizes across devices, organizations, and interfaces.


## Summarize the paper in one sentence.

 Here is a one-sentence summary of the paper:

The paper proposes CIM-MLC, a general multi-level compilation framework for computing-in-memory accelerators that uses hardware and computation abstraction along with multi-granularity scheduling optimization to map neural networks onto diverse CIM architectures for efficient inference.


## What is the main contribution of this paper?

 This paper proposes CIM-MLC, a general multi-level compilation framework for computing-in-memory (CIM) accelerators. The main contributions are:

1) It proposes a hardware abstraction to represent various CIM architectures and computing modes, enabling CIM-MLC to support compiling for different CIM hardware. 

2) It proposes a multi-level DNN scheduling approach that explores optimization opportunities across different architectural tiers of CIM, from coarse-grained computing graphs to fine-grained vector operations. This covers a broader design space compared to prior CIM compilation works.

3) Experimental results show CIM-MLC achieves 3.2x higher inference speed on average compared to a prior CIM-oriented compiler. It also accelerates existing CIM designs by 2.3-3.7x while reducing power consumption by 75% in some cases.

In summary, the key innovation is the multi-level compilation approach combining unified CIM hardware abstraction and cross-tier optimization, enabling efficient deployment of DNNs on diverse CIM accelerators.


## What are the keywords or key terms associated with this paper?

 Here are some of the key terms and keywords highlighted in this paper:

- Computing-in-memory (CIM)
- Hardware abstraction
- Architecture parameters
- Computing mode abstraction 
- Meta-operators
- Multi-level scheduling
- Computational graph grained (CG-grained) optimization
- Matrix-vector multiplication grained (MVM-grained) optimization  
- Vector-vector multiplication grained (VVM-grained) optimization
- Operator duplication
- Pipeline strategies
- Data remapping

The paper proposes CIM-MLC, a multi-level compilation framework for general CIM architectures. It features hardware abstraction to support diverse CIM designs and multi-level scheduling to optimize mapping and scheduling of DNN computations onto different CIM devices. Key aspects include establishing architecture parameters, computing mode abstractions, associated meta-operator sets, and multi-grained scheduling techniques like duplication, pipelining, and remapping to maximize throughput and efficiency.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in this paper:

1. The paper proposes a three-tier hardware abstraction for CIM architectures. How does mapping the abstraction to different computing granularities help explore optimization opportunities during compilation? What are some limitations of only having a single level of abstraction?

2. The paper discusses meta-operators for different computing modes like CM, XBM, and WLM. How do these meta-operators help to represent computations on diverse CIM architectures? What extensions would be needed to support newer types of CIM architectures or computing modes?

3. Explain the CG-grained optimization algorithm in detail. How does it balance utilization of cores and bandwidth while duplicating operators? What metrics could be added to further improve the dynamic programming formulation?  

4. The MVM-grained optimization introduces a pipeline strategy to reduce peak power consumption. Analyze the tradeoffs between higher throughput and power savings for different pipeline configurations. How could the formulation be adapted for other objectives like latency or energy?

5. The data remapping strategy in VVM optimization enables concurrent row computations. Discuss the overhead of remapping data across crossbars and how it could limit speedups for some applications. What criteria should be used to determine when remapping provides benefits?

6. Compare and contrast the optimization spaces explored at different granularities - CG, MVM, and VVM. What are the complexities and limitations faced at each level? How do the joint optimizations span a wider design space?

7. The paper shows speedups on different model architectures. Analyze the results to determine which models benefit more from each level of optimization. What architectural properties lead to such outcomes?

8. Discuss the sensitivity analysis on core numbers, crossbar sizes, etc. How do such hardware constraints impact applicability of the proposed techniques? When would certain optimizations become ineffective?

9. Propose two additional metrics that can be incorporated into the cost functions optimized at each granularity level. How would considering these metrics lead to different scheduling decisions?

10. The paper focuses on ReRAM based CIM architectures. What changes would be required in the abstractions and optimization formulations to handle other devices like SRAM or flash? Identify key differences.
