# [Order-Disorder: Imitation Adversarial Attacks for Black-box Neural   Ranking Models](https://arxiv.org/abs/2209.06506)

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper addresses is: how to conduct adversarial attacks against black-box neural text ranking models? Specifically, the authors propose a novel imitation adversarial attack method against black-box neural ranking models. The key ideas include:1) Training a ranking imitation model to transparentize the target black-box ranking model, by sampling ranking results and training a pairwise BERT model.2) Proposing a Pairwise Anchor-based Trigger (PAT) generation method that utilizes the ranking imitation model's pairwise loss and ranking information to craft adversarial triggers. 3) Employing fluency and semantic consistency constraints during trigger generation to equip the triggers with camouflage. 4) Demonstrating the attack effectiveness and transferability of the triggers from the imitation model to the target victim models.The main hypothesis is that by training a similar ranking imitation model and generating camouflaged triggers based on it, the attack can transfer to black-box neural ranking models effectively. The experiments on passage ranking datasets validate the effectiveness of the proposed attack method.In summary, the paper focuses on investigating a novel imitation-based adversarial attack against black-box text ranking models, which remains underexplored in prior studies. The proposed attack method and extensive evaluations reveal vulnerabilities of neural ranking models.


## What is the main contribution of this paper?

The main contributions of this paper are:1. It proposes a novel imitation adversarial attack method against black-box neural text ranking models. This is the first work to investigate transfer-based attacks on neural text ranking models in a black-box setting.2. It introduces a ranking imitation model that can transparentize the victim black-box ranker via effective pairwise learning, without needing access to the model internals or training data. 3. It proposes a Pairwise Anchor-based Trigger (PAT) generation method that leverages pairwise loss and anchor candidates to craft adversarial triggers with camouflages.4. It conducts extensive experiments on multiple datasets and models to demonstrate the effectiveness of the proposed attack method. The adversarial triggers can successfully manipulate the rankings and transfer across different victim models.5. It provides comprehensive analyses on the attack performance, transferability, imperceptibility, and potential defenses. This sheds light on the vulnerabilities of neural ranking models and motivates further research into this novel security problem.In summary, this is the first work to study imitation adversarial attacks against black-box neural ranking models, proposing innovative methods for transparentizing and attacking the victim model. The findings reveal risks of ranking manipulation and aim to motivate more robust ranking models.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper proposes a novel black-box adversarial attack method against neural text ranking systems by training a ranking imitation model to transparentize the target model and generate camouflaged adversarial triggers that can manipulate the ranking results.


## How does this paper compare to other research in the same field?

This paper presents some interesting new research on adversarial attacks against neural text ranking models. Here is a comparison to related prior work in this field:- Focus on black-box attacks: Most prior work has focused on white-box attacks where the adversary has full knowledge and access to the target model. This paper proposes a new approach for black-box attacks where the adversary has limited knowledge about the target model.- Ranking model imitation: A key contribution is using a ranking model imitation approach to approximate the target black-box model for generating attacks. They train an imitation model using relative ranking judgments sampled from the target model's output. - Novel pairwise loss attack: They propose a new pairwise anchor-based trigger generation method that leverages relative ranking information. This is tailored for attacks on ranking models compared to prior work on classification.- Emphasis on stealthiness: Their method adds fluency and semantic consistency constraints to generate more stealthy and imperceptible triggers compared to prior attacks like collisions.- Experiments on multiple datasets: They test their approach on 3 datasets of different domains (MS MARCO, TREC DL, Natural Questions), showing wide applicability.- Analysis of defenses: The paper analyzes potential defenses like perplexity filtering and shows limitations, providing insights into future work on defending against such attacks.Overall, this paper makes solid contributions over prior work by focusing on more realistic black-box attacks on ranking, proposing a novel attack method using ranking imitation and pairwise loss, and extensive experiments demonstrating effectiveness. The analysis of stealthiness and defenses also offers useful insights into this emerging research area.


## What future research directions do the authors suggest?

Based on my reading of the paper, here are some of the main future research directions suggested by the authors:- Investigate more imperceptible and universal adversarial attacks against text ranking models. The authors suggest exploring ways to make the attacks more stealthy and broadly effective across different models and datasets.- Explore methods to effectively detect and defend against adversarial ranking attacks. The authors propose developing novel techniques to identify manipulated rankings and make models more robust. - Study adversarial attacks in other information retrieval tasks beyond passage ranking, such as document ranking. The authors suggest extending their attack methods to other IR scenarios.- Analyze the time complexity and efficiency of the proposed attack methods. The authors propose evaluating the computational costs of their attacks.- Conduct human evaluations to assess the naturalness and detectability of different attack techniques by real users. The authors suggest human studies could complement automatic evaluations.- Explore the effectiveness of adversarial training as a defense method by fine-tuning models on adversarial examples.- Investigate more flexible ways of injecting triggers into texts and their effects on attack success and stealthiness.- Generalize the attacks to other model architectures beyond BERT-based models. The authors propose evaluating the transferability of their methods to other neural ranking models.In summary, the authors lay out a research agenda focused on making neural ranking models more robust through adversarial attacks and defenses, with emphasis on stealthiness, effectiveness, and human perception.


## Summarize the paper in one paragraph.

Here is a one paragraph summary of the key points from the paper:This paper proposes an imitation adversarial attack method against black-box neural ranking models. The authors first train a ranking imitation model using relative ranking information sampled from the victim model's outputs. This imitation model is able to mimic the victim model's rankings. Using this imitation model, the authors propose a novel Pairwise Anchor-based Trigger (PAT) generation method to create adversarial triggers for each candidate passage. These triggers can manipulate the ranking results when transferred to the victim model. The PAT method introduces a pairwise loss function using an anchor passage to optimize the triggers. It also uses language model fluency and next sentence prediction constraints to improve the triggers. Experiments on passage ranking datasets demonstrate that the proposed attack method can successfully manipulate rankings on various state-of-the-art neural ranking models in a black-box setting. The imitation model achieves high ranking similarity with the victim models, showing the efficacy of the proposed approach.
