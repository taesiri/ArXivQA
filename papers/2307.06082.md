# VELMA: Verbalization Embodiment of LLM Agents for Vision and Language   Navigation in Street View

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is: How can large language models (LLMs) be effectively utilized as the reasoning engine for an embodied agent performing vision and language navigation in a complex, real-world environment like Street View? The key challenges outlined are:- Vision and language navigation requires grounding natural language instructions in visual observations of the environment and reasoning about actions over long trajectories.- It is difficult to connect LLMs with an interactive visual environment since they lack innate visual capabilities.The hypothesis seems to be that verbalizing the agent's observations and actions into text can provide an effective interface for tapping into the reasoning capabilities of LLMs for this visually-grounded navigation task.The paper introduces VELMA, an LLM-based agent that verbalizes its trajectory and visual observations at each step to query the LLM to decide the next action. The research aims to demonstrate that this approach can enable few-shot learning and achieve state-of-the-art performance on urban vision and language navigation in Street View.In summary, the central research question is how to best utilize LLMs for embodied navigation agents by verbalizing visual observations and actions, even though LLMs lack innate visual capabilities.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contribution seems to be introducing VELMA, an embodied large language model (LLM) agent for vision and language navigation in Street View environments. The key ideas of VELMA include:- Using a verbalization of the agent's trajectory and visual observations to provide contextual prompts to the LLM to guide next action predictions. Visual information like landmarks is converted to descriptive text using CLIP embeddings. - Modifying the commonly used Touchdown environment to fix inconsistencies in action sequences required at intersections. This makes the environment more intuitive for few-shot interaction.- Achieving strong few-shot navigation performance by prompting large pretrained language models like GPT-3/GPT-4 with just 2 example trajectories.- Further improving performance by finetuning the LLM on navigation demonstrations and using a response-based learning approach to directly optimize for task completion. - Reporting new state-of-the-art results on the Touchdown and Map2seq datasets by leveraging the reasoning and generalization capabilities of LLMs combined with the proposed verbalization embodiment.In summary, the main contribution seems to be proposing an effective way to leverage LLMs for embodied navigation by verbalizing the visual aspects and sequences of experience, and demonstrating strong few-shot and finetuned results on a challenging urban VLN task.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper introduces VELMA, a large language model-based agent for vision and language navigation in Street View environments, which uses verbalization of the navigation trajectory and visual observations to provide contextual prompts for the agent to predict the next action.


## How does this paper compare to other research in the same field?

Here are a few key points comparing this paper to other research in the field of vision and language navigation:- This paper focuses on navigation in a large-scale urban environment using real Street View imagery. Much prior work has used simulated environments with simpler visuals or smaller scale spaces like indoor settings. Using a complex real-world environment is more challenging.- The paper proposes an agent called VELMA that is based on a large language model (LLM). LLMs have become very popular in AI recently, but their use for embodied navigation agents is still relatively new. Many prior navigation agents use more traditional deep learning architectures like RNNs or Transformers. - VELMA relies on verbalizing the visual observations and full trajectory into text which is fed to the LLM at each step. Other techniques like fusing visual features or using separate vision modules are more common. Verbalization is an interesting way to fully leverage the reasoning skills of LLMs.- The paper shows VELMA can learn to navigate from just a couple of example trajectories, demonstrating strong few-shot generalization. Other work usually requires more training data. VELMA also achieves new state-of-the-art results when fully trained.- A modification is made to the commonly used Touchdown environment to fix inconsistent action sequences at intersections. This change is important to enable intuitive communication for instruction following.Overall, this paper pushes the boundaries of vision-language navigation by using a challenging real-world setting, applying recent advances in LLMs, and proposing an effective embodiment by verbalization approach. The results demonstrate LLMs are becoming a promising new paradigm for building capable embodied agents.
