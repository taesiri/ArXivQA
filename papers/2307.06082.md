# [VELMA: Verbalization Embodiment of LLM Agents for Vision and Language   Navigation in Street View](https://arxiv.org/abs/2307.06082)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is: How can large language models (LLMs) be effectively utilized as the reasoning engine for an embodied agent performing vision and language navigation in a complex, real-world environment like Street View? The key challenges outlined are:- Vision and language navigation requires grounding natural language instructions in visual observations of the environment and reasoning about actions over long trajectories.- It is difficult to connect LLMs with an interactive visual environment since they lack innate visual capabilities.The hypothesis seems to be that verbalizing the agent's observations and actions into text can provide an effective interface for tapping into the reasoning capabilities of LLMs for this visually-grounded navigation task.The paper introduces VELMA, an LLM-based agent that verbalizes its trajectory and visual observations at each step to query the LLM to decide the next action. The research aims to demonstrate that this approach can enable few-shot learning and achieve state-of-the-art performance on urban vision and language navigation in Street View.In summary, the central research question is how to best utilize LLMs for embodied navigation agents by verbalizing visual observations and actions, even though LLMs lack innate visual capabilities.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contribution seems to be introducing VELMA, an embodied large language model (LLM) agent for vision and language navigation in Street View environments. The key ideas of VELMA include:- Using a verbalization of the agent's trajectory and visual observations to provide contextual prompts to the LLM to guide next action predictions. Visual information like landmarks is converted to descriptive text using CLIP embeddings. - Modifying the commonly used Touchdown environment to fix inconsistencies in action sequences required at intersections. This makes the environment more intuitive for few-shot interaction.- Achieving strong few-shot navigation performance by prompting large pretrained language models like GPT-3/GPT-4 with just 2 example trajectories.- Further improving performance by finetuning the LLM on navigation demonstrations and using a response-based learning approach to directly optimize for task completion. - Reporting new state-of-the-art results on the Touchdown and Map2seq datasets by leveraging the reasoning and generalization capabilities of LLMs combined with the proposed verbalization embodiment.In summary, the main contribution seems to be proposing an effective way to leverage LLMs for embodied navigation by verbalizing the visual aspects and sequences of experience, and demonstrating strong few-shot and finetuned results on a challenging urban VLN task.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper introduces VELMA, a large language model-based agent for vision and language navigation in Street View environments, which uses verbalization of the navigation trajectory and visual observations to provide contextual prompts for the agent to predict the next action.


## How does this paper compare to other research in the same field?

Here are a few key points comparing this paper to other research in the field of vision and language navigation:- This paper focuses on navigation in a large-scale urban environment using real Street View imagery. Much prior work has used simulated environments with simpler visuals or smaller scale spaces like indoor settings. Using a complex real-world environment is more challenging.- The paper proposes an agent called VELMA that is based on a large language model (LLM). LLMs have become very popular in AI recently, but their use for embodied navigation agents is still relatively new. Many prior navigation agents use more traditional deep learning architectures like RNNs or Transformers. - VELMA relies on verbalizing the visual observations and full trajectory into text which is fed to the LLM at each step. Other techniques like fusing visual features or using separate vision modules are more common. Verbalization is an interesting way to fully leverage the reasoning skills of LLMs.- The paper shows VELMA can learn to navigate from just a couple of example trajectories, demonstrating strong few-shot generalization. Other work usually requires more training data. VELMA also achieves new state-of-the-art results when fully trained.- A modification is made to the commonly used Touchdown environment to fix inconsistent action sequences at intersections. This change is important to enable intuitive communication for instruction following.Overall, this paper pushes the boundaries of vision-language navigation by using a challenging real-world setting, applying recent advances in LLMs, and proposing an effective embodiment by verbalization approach. The results demonstrate LLMs are becoming a promising new paradigm for building capable embodied agents.


## What future research directions do the authors suggest?

Based on my review of the paper, some of the main future research directions suggested by the authors include:- Testing the proposed VELMA agent in more challenging and diverse environments beyond Street View, such as indoor spaces or landscapes without clear paths. They suggest environments with less structured layouts and more complex visual scenes.- Exploring different ways to represent the visual observations and environment context provided to the agent, beyond just verbal descriptions. For example, providing cropped image patches or abstract sketches of landmark objects and scenes.- Investigating if and how much the performance of VELMA depends on the quality of the landmark extraction and scoring components. Trying alternative methods beyond CLIP for detecting landmark visibility.- Extending the agent with capabilities for clarification interaction and active perception. For example, being able to query which direction a certain landmark is, or explicitly rotating the view direction to gather more observations. - Scaling up through pretraining on large amounts of unlabeled Street View imagery and navigation instructions to better generalize.- Combining the benefits of the proposed verbalization-based embodiment with other perception and reasoning modules, such as computer vision components.- Exploring whether VELMA's learned navigation strategies are interpretable and whether the agent can explain its decisions and rationale.In summary, they suggest enhancements in the diversity and complexity of environments, the perception and context representation, the landmark grounding, active sensing, scalability, interpretability, and combining verbalization with other reasoning approaches.


## Summarize the paper in one paragraph.

Here is a one paragraph summary of the key points from the paper:The paper introduces VELMA, a large language model (LLM) based agent for vision and language navigation in Street View environments. VELMA utilizes an embodiment by verbalization approach where the navigation task, including the agent's trajectory and visual observations, is verbalized into text and used to prompt the LLM to take the next action. To incorporate visual information, landmarks are extracted from instructions and their visibility in panoramas is scored using CLIP. VELMA shows strong few-shot learning capability, completing navigation routes by being prompted with just two example trajectories. When finetuned on thousands of demonstrations, VELMA achieves new state-of-the-art results on two Street View navigation datasets, improving task completion rates by 25-30% over prior methods. The work demonstrates how verbalizing the embodied navigation task enables leveraging LLMs for sequential decision making grounded in visual observations.


## Summarize the paper in two paragraphs.

Here is a two paragraph summary of the paper:The paper introduces VELMA, an embodied agent that uses a large language model (LLM) for vision and language navigation in Street View environments. VELMA verbalizes the navigation task, including the trajectory and visual observations, to create a contextual prompt for the LLM to predict the next action. To incorporate visual information, landmarks are extracted from instructions and scored for visibility in panoramas with CLIP, then appended to the prompt. In few-shot learning, VELMA achieved strong results, accomplishing the navigation task 10-23% of the time with just two examples. When finetuned on the full training set, VELMA achieved 25-30% relative improvement over prior methods, establishing a new state-of-the-art on two navigation datasets. The environment was also improved to align action sequences with instructions. Overall, the work demonstrates how verbalization and embodiment enables LLMs to excel at interactive visual tasks with minimal training.In more detail, the paper proposes modifications to the commonly used Touchdown environment to fix inconsistencies in action sequences at intersections. It also adds a "turn around" action for more natural communication. For the agent, landmarks are extracted from instructions using GPT-3 then scored for visibility with CLIP, thresholded based on a distribution of training panoramas. Observed landmarks and intersections are verbalized and appended to the prompt sequence. The LLM reasons about navigation by next word prediction. In few-shot learning, VELMA already accomplishes the task at rates of 10% and 23% on two datasets. When finetuned on the full training set with a mix of student-forcing and teacher-forcing, VELMA achieves new state-of-the-art results, demonstrating the potential of LLMs for interactive visual tasks when properly embodied through verbalization.


## Summarize the main method used in the paper in one paragraph.

Here is a one paragraph summary of the main method used in the paper:The paper proposes VELMA, an embodied large language model (LLM) agent for vision and language navigation in Street View environments. The key idea is to verbalize the navigation task, including the trajectory and visual observations, into a text prompt that provides context for the LLM to predict the next navigation action through next word prediction. The verbalized visual observations consist of street intersections detected from the navigation graph, as well as landmarks extracted from the instructions text and scored for visibility in panoramas along the route using CLIP embeddings. This allows the LLM, which has no innate visual capabilities, to reason about navigation actions solely based on the verbalized context in a few-shot setting. The approach is evaluated on the Touchdown and Map2seq datasets for navigation in Manhattan Street View. Using finetuning on full training sets, VELMA achieves new state-of-the-art results, demonstrating the effectiveness of embodiment by verbalization for utilizing LLMs in interactive visual environments.


## What problem or question is the paper addressing?

Based on my reading of the paper, it seems the main problem/question being addressed is how to best connect large language models (LLMs) with an interactive visual environment for the task of vision and language navigation (VLN).Specifically, the paper introduces an "embodied LLM agent" called VELMA for VLN in urban street scenes using Google Street View. The key challenges are:- VLN in this realistic street environment requires grounding language instructions in visual observations of the surroundings, and reasoning about navigation actions over long trajectories. - It is difficult to effectively integrate LLMs with this interactive visual task, compared to traditional NLP tasks.So the main question is how to utilize LLMs for VLN in a way that connects their reasoning and language capabilities with real visual perceptions from a navigation environment like Street View.The proposed solution is to "verbally embody" the LLM by representing the visual environment and agent trajectory as text that is concatenated to prompt the model to take the next action. So they are investigating whether verbalization of the visual navigation experience can allow LLMs to succeed at this challenging embodied AI task.
