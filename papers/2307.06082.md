# VELMA: Verbalization Embodiment of LLM Agents for Vision and Language
  Navigation in Street View

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is: How can large language models (LLMs) be effectively utilized as the reasoning engine for an embodied agent performing vision and language navigation in a complex, real-world environment like Street View? The key challenges outlined are:- Vision and language navigation requires grounding natural language instructions in visual observations of the environment and reasoning about actions over long trajectories.- It is difficult to connect LLMs with an interactive visual environment since they lack innate visual capabilities.The hypothesis seems to be that verbalizing the agent's observations and actions into text can provide an effective interface for tapping into the reasoning capabilities of LLMs for this visually-grounded navigation task.The paper introduces VELMA, an LLM-based agent that verbalizes its trajectory and visual observations at each step to query the LLM to decide the next action. The research aims to demonstrate that this approach can enable few-shot learning and achieve state-of-the-art performance on urban vision and language navigation in Street View.In summary, the central research question is how to best utilize LLMs for embodied navigation agents by verbalizing visual observations and actions, even though LLMs lack innate visual capabilities.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contribution seems to be introducing VELMA, an embodied large language model (LLM) agent for vision and language navigation in Street View environments. The key ideas of VELMA include:- Using a verbalization of the agent's trajectory and visual observations to provide contextual prompts to the LLM to guide next action predictions. Visual information like landmarks is converted to descriptive text using CLIP embeddings. - Modifying the commonly used Touchdown environment to fix inconsistencies in action sequences required at intersections. This makes the environment more intuitive for few-shot interaction.- Achieving strong few-shot navigation performance by prompting large pretrained language models like GPT-3/GPT-4 with just 2 example trajectories.- Further improving performance by finetuning the LLM on navigation demonstrations and using a response-based learning approach to directly optimize for task completion. - Reporting new state-of-the-art results on the Touchdown and Map2seq datasets by leveraging the reasoning and generalization capabilities of LLMs combined with the proposed verbalization embodiment.In summary, the main contribution seems to be proposing an effective way to leverage LLMs for embodied navigation by verbalizing the visual aspects and sequences of experience, and demonstrating strong few-shot and finetuned results on a challenging urban VLN task.
