# [Self-Supervised Bot Play for Conversational Recommendation with   Justifications](https://arxiv.org/abs/2112.05197)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question seems to be:

How can we develop an effective conversational recommender system that can justify its suggestions, adapt recommendations based on user feedback, and be trained without requiring large amounts of crowd-sourced dialog data? 

The key hypotheses appear to be:

1) Modeling the recommendation conversation as an expert justifying suggestions and a user critiquing subjective aspects will lead to more natural and effective interactions compared to existing approaches.

2) Self-supervised bot play can be used to train a conversational recommender without crowdsourced dialog transcripts, allowing the approach to be applied to new domains. 

3) Their proposed two-part training framework of pretraining a joint recommendation and justification model followed by bot play fine-tuning will outperform existing conversational recommenders in multi-turn simulated dialogs and with real users.

In summary, the central research aims to develop a conversational recommender that can justify suggestions and adapt interactively without relying on large dialog datasets. The key hypotheses are around using a justification-critiquing dialog structure and a bot play training approach to achieve superior performance.


## What is the main contribution of this paper?

 The main contribution of this paper is presenting a new two-part framework for training conversational recommender systems without requiring large collections of human dialog transcripts. The key ideas are:

1. First training a model to jointly recommend items and generate justifications using sets of subjective aspects extracted from reviews. 

2. Then fine-tuning the model using self-supervised bot play based on simulated conversations using the review data.

The paper shows this framework can be applied to different base recommendation models (BPR and PLRec) across diverse domains. The resulting conversational agents (BPR-Bot and PLRec-Bot) outperform prior state-of-the-art methods in multi-turn conversational recommendation. Experiments and user studies demonstrate the models trained with this framework provide more useful, helpful and knowledgeable recommendations compared to alternatives.

In summary, the main contribution is a new training framework to develop conversational recommender systems without needing expensive human dialog data. The framework is model-agnostic and achieves superior performance to prior methods when evaluated on real-world datasets and with human users.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the key points from the paper:

The paper presents a two-part framework to train conversational recommendation systems without requiring large collections of human dialog transcripts, using bot-play with simulated user critiques based on review data to enable multi-turn critiquing.


## How does this paper compare to other research in the same field?

 Here are a few key ways this paper compares to other research in conversational recommendation systems:

- This paper proposes a new framework for training conversational recommender systems using self-supervised bot play, without requiring large amounts of human-generated dialog data. This makes it more practical to develop conversational recommenders for new domains compared to prior work like RecDial or KBRD that rely on crowdsourced dialog transcripts.

- The paper demonstrates applying the framework to both a matrix factorization model (BPR) and a linear recommender model (PLRec), showing it is model-agnostic. Prior conversational recommenders have typically focused on a single base recommendation model. 

- The framework incorporates justified critiquing, where the system provides explanations for its suggestions using extracted aspects, and allows the user to critique any aspect. This provides more flexible user interactions compared to some prior systems that ask yes/no questions about pre-specified attributes.

- Experiments across 3 datasets show the BPR-Bot and PLRec-Bot models trained with this framework outperform state-of-the-art baselines like CE-VAE and Latent Linear Critiquing models in multi-turn success rate and efficiency.

- The bot-play fine-tuning stage leads to better performance compared to just training the base recommender and justification model, demonstrating the importance of that self-supervised phase.

- The paper also shows the framework can be combined with query refinement strategies to significantly boost success rates by reducing candidate sets based on critiqued aspects.

So in summary, this paper pushes forward conversational recommendation in multiple dimensions compared to prior work, demonstrating a practical training approach, flexibility in base recommender systems, and strong empirical results. The self-supervised bot-play technique seems particularly novel and impactful.


## What future research directions do the authors suggest?

 Based on my reading of the paper, the authors suggest the following future research directions:

- Adapting their framework to natural language critiques (i.e. complete utterances) instead of aspect-level critiques, allowing users more flexibility in how they express feedback.

- Adding noise to the user model during bot-play training to ensure the model encounters more examples of rare aspects being critiqued. This could improve performance in sparse domains like music. 

- Experimenting with different bot-play losses like ranking losses instead of cross-entropy to optimize the fine-tuning process.

- Modifying their framework to support "hard critiquing" where critiqued aspects are completely filtered out of future suggestions. This could involve masking the loss to only adjust scores of non-pruned items, or using a ranking loss to push critiqued items down.

- Applying their bot-play framework to other types of recommender systems beyond matrix factorization and linear models.

- Evaluating their models in other complex domains beyond books, beer, and music to further demonstrate wide applicability.

- Conducting additional human studies to evaluate aspects like user satisfaction, trust, and preference elicitation.

In summary, the main future directions are exploring improvements to the bot-play training process, supporting more flexible natural language interactions, applying the framework to new recommender systems and domains, and conducting further human evaluations. The key goal is improving the flexibility, applicability and user experience of conversational recommendation.


## Summarize the paper in one paragraph.

 The paper presents a two-part framework for training conversational recommender systems that can justify suggestions and incorporate user feedback. In the first part, a recommender model is trained alongside an aspect prediction module to generate personalized justifications. In the second part, this pretrained model is fine-tuned via self-supervised bot play on historical user reviews to mimic iterative critiquing conversations. The resulting models, PLRec-Bot and BPR-Bot, are evaluated on three real-world recommendation datasets. Experiments show the bot-play fine-tuned models achieve higher success rates and faster turns to target compared to state-of-the-art critiquing and conversational methods. A user study also demonstrates the model's ability to provide useful, adaptive recommendations in cold-start settings. The key novelty is the transcript-free bot-play framework, allowing conversational recommenders to be trained on any domain with review data rather than relying on costly human dialog datasets. Overall, this work presents an effective and scalable approach to train multi-turn conversational recommender systems.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the paper:

This paper proposes a new two-part framework for training conversational recommender systems without needing large collections of human dialog transcripts. First, they train an initial model for next-item recommendation that can generate justifications by predicting relevant subjective aspects of items. They then fine-tune this model via self-supervised bot play to incorporate iterative user feedback and critiques. 

The authors apply their framework to two popular recommendation models, PLRec and BPR, to create PLRec-Bot and BPR-Bot. Experiments on three real-world datasets show these conversational agents achieve higher success rates and faster turns to target items compared to prior conversational recommendation and critiquing methods. The models are also evaluated in user studies demonstrating their ability to provide useful recommendations and effectively incorporate critiques in both warm-start and cold-start settings. Overall, the proposed bot play framework enables training multi-turn conversational recommenders from review data alone and results in systems that iteratively refine suggestions through natural critiquing interactions.


## Summarize the main method used in the paper in one paragraph.

 The main method presented in this paper is a two-part framework for training conversational recommender systems that can suggest items to users and provide justifications, while incorporating user feedback to refine suggestions over multiple conversation turns. 

The first part of the framework trains a base recommender model to suggest items and generate aspect-based justifications using extracted aspects from user reviews. The second part uses simulated conversations between a "seeker bot" and the trained recommender model to further fine-tune the model's ability to incorporate critiques and update suggestions. Specifically, for each user-item pair, the seeker bot converses with the model by critiquing inconsistent aspects in the justification until the target item is recommended. The model is fine-tuned to improve its performance in these simulated conversations.

The framework is model-agnostic and can leverage different base recommender systems. The authors demonstrate it with matrix factorization (BPR) and linear recommendation (PLRec) models. Experiments on three datasets show the resulting conversational recommender models outperform baselines in multi-turn critiquing simulations and are preferred by human users. A key advantage is the framework allows training on review data alone, without requiring expensive collection of human-human dialog data.


## What problem or question is the paper addressing?

 The paper is addressing several related problems/questions in conversational recommendation systems:

1. How to better mimic human-like interactions for recommendation, where experts justify suggestions and seekers provide feedback to iteratively find a good item.

2. How to allow flexible critiquing of subjective aspects in a justification, building on ideas from conversational critiquing systems. 

3. How to develop conversational recommendation systems for new domains where large amounts of crowd-sourced dialog data may not be available for training.

The key problems seem to be around making conversational recommender systems more natural, flexible, and explainable like human conversations, while also making them applicable to diverse domains beyond just commonly studied ones like movies. The paper aims to improve conversational recommendation along these dimensions.
