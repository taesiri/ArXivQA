# [Efficient Backpropagation with Variance-Controlled Adaptive Sampling](https://arxiv.org/abs/2402.17227)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper "Efficient Backpropagation with Variance-Controlled Adaptive Sampling":

Problem:
Training neural networks is computationally expensive as it requires processing every data point in the batch and every connection in the network through repeated forward and backpropagation. This results in high time and computational complexity that scales with batch size and model size. There is a need for methods to accelerate neural network training.

Solution:
The paper proposes a variance-controlled adaptive sampling (VCAS) method to accelerate backpropagation. The key ideas are:

1) Construct an unbiased approximated stochastic gradient (ASG) by partially conducting backpropagation using fine-grained importance sampling. This is done by:
- Adaptive sampling on the data dimension when computing activation gradients. Samples data points proportionally to their gradient norm.
- Further sampling on the token dimension when computing weight gradients for linear layers. Samples tokens according to the leverage score.

2) Control the additional variance incurred by the ASG to be a small fraction of the original stochastic gradient variance. This is achieved by an adaptation algorithm that learns the sample ratios jointly during training.

Main Contributions:

- Proposes VCAS method that computes ASG using fine-grained adaptive sampling and controls its variance. VCAS reduces computational cost of backpropagation while maintaining accuracy.

- Achieves up to 73.87% FLOPs reduction in backpropagation and 49.58% overall speedup. VCAS mirrors the original loss trajectory and validation accuracy on various vision and language tasks.

- Adaptively determines sample ratios based on gradient sparsity patterns across layers and time. Eliminates need to manually tune this hyperparameter.

- Establishes robustness of VCAS across tasks compared to prior arts through extensive experiments on model finetuning and pretraining.

In summary, the paper makes significant contributions in developing an efficient and robust sampling-based method to accelerate neural network training by approximating backpropagation in an adaptive way with theoretical guarantees.
