# MMChat: Multi-Modal Chat Dataset on Social Media

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be:How to construct a large-scale, high-quality multi-modal dialogue dataset that exhibits the sparsity phenomenon observed in real-world conversations, where not all utterances are strictly grounded in the associated images?The key points are:- Previous multi-modal dialogue datasets assume every utterance is grounded in the given images. But in real conversations, the topic often drifts so not all utterances directly relate to the images. - To address this, the authors introduce MMChat, a large Chinese multi-modal dialogue dataset exhibiting "sparsity", where dialogues start image-grounded but drift off-topic.- They collect 32.4M raw dialogues from social media and filter it down to 120K high-quality dialogues with associated images.- The dataset construction aims to facilitate research on multi-modal dialogue systems that can handle sparse image grounding, and benchmark models are provided.- A subset of 100K dialogues are manually annotated for image quality and relevance, yielding the MMChat-hf dataset.So in summary, the key research contribution is the construction of a large-scale multi-modal dialogue dataset exhibiting sparsity, to promote research on models that can handle such sparse image grounding in conversations.


## What is the main contribution of this paper?

Based on my reading, the main contributions of this paper are:1. The authors construct a large multi-modal dialogue dataset called MMChat, which contains sparse image-grounded dialogues from real conversations on social media. This dataset addresses the issue of "sparsity" in image-grounded dialogues, where not every utterance is grounded on the image. 2. They provide two versions of the dataset - the full MMChat with 120K dialogues, and a human-filtered subset MMChat-hf with higher quality and stronger image grounding. 3. The authors develop benchmark models for dialogue generation on MMChat to handle the sparsity issue. Their model uses an attention routing mechanism to better incorporate sparse image features.4. Experiments show that incorporating visual features improves dialogue generation performance, and their model can alleviate the sparsity issue more effectively compared to baselines.5. The paper introduces a large multi-modal conversational dataset grounded in real social media conversations, and provides models to handle the practical issue of sparse image grounding in dialogues. This could help develop more engaging dialogue systems that can perceive and converse about visual contexts.In summary, the key contribution is the construction of a unique large-scale multi-modal dialogue dataset that exhibits the phenomenon of sparse image grounding, along with benchmark models that effectively incorporate visual context despite the sparsity.
