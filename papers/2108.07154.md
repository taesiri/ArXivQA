# [MMChat: Multi-Modal Chat Dataset on Social Media](https://arxiv.org/abs/2108.07154)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question seems to be:How to construct a large-scale, high-quality multi-modal dialogue dataset that exhibits the sparsity phenomenon observed in real-world conversations, where not all utterances are strictly grounded in the associated images?The key points are:- Previous multi-modal dialogue datasets assume every utterance is grounded in the given images. But in real conversations, the topic often drifts so not all utterances directly relate to the images. - To address this, the authors introduce MMChat, a large Chinese multi-modal dialogue dataset exhibiting "sparsity", where dialogues start image-grounded but drift off-topic.- They collect 32.4M raw dialogues from social media and filter it down to 120K high-quality dialogues with associated images.- The dataset construction aims to facilitate research on multi-modal dialogue systems that can handle sparse image grounding, and benchmark models are provided.- A subset of 100K dialogues are manually annotated for image quality and relevance, yielding the MMChat-hf dataset.So in summary, the key research contribution is the construction of a large-scale multi-modal dialogue dataset exhibiting sparsity, to promote research on models that can handle such sparse image grounding in conversations.


## What is the main contribution of this paper?

 Based on my reading, the main contributions of this paper are:1. The authors construct a large multi-modal dialogue dataset called MMChat, which contains sparse image-grounded dialogues from real conversations on social media. This dataset addresses the issue of "sparsity" in image-grounded dialogues, where not every utterance is grounded on the image. 2. They provide two versions of the dataset - the full MMChat with 120K dialogues, and a human-filtered subset MMChat-hf with higher quality and stronger image grounding. 3. The authors develop benchmark models for dialogue generation on MMChat to handle the sparsity issue. Their model uses an attention routing mechanism to better incorporate sparse image features.4. Experiments show that incorporating visual features improves dialogue generation performance, and their model can alleviate the sparsity issue more effectively compared to baselines.5. The paper introduces a large multi-modal conversational dataset grounded in real social media conversations, and provides models to handle the practical issue of sparse image grounding in dialogues. This could help develop more engaging dialogue systems that can perceive and converse about visual contexts.In summary, the key contribution is the construction of a unique large-scale multi-modal dialogue dataset that exhibits the phenomenon of sparse image grounding, along with benchmark models that effectively incorporate visual context despite the sparsity.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the key points from the paper:The paper introduces MMChat, a large-scale Chinese multi-modal dialogue dataset containing over 120K dialogues with associated images collected from social media, proposes methods to filter and annotate the data, and builds benchmark models showing incorporating visual context improves dialogue modeling and helps address the issue of image-sparse dialogues where not all utterances directly relate to the image.


## How does this paper compare to other research in the same field?

 Here are some key ways this paper on the MMChat dataset relates to prior work on multi-modal dialogue systems:- Most prior multi-modal dialogue datasets are either crowd-sourced (e.g. ImageChat) or extracted from movies/TV shows (e.g. OpenViDial). In contrast, MMChat is collected from real conversational data on social media.- Existing datasets assume every utterance is grounded in the associated image(s). MMChat highlights the issue of "sparsity" - where dialogues drift to non image-grounded topics over time.- With 32.4M raw dialogues and 120K filtered sessions, MMChat is among the largest multi-modal dialogue datasets. This provides more data to train robust models.- MMChat contains both automatic annotations (object labels, captions) and human annotations assessing image/dialogue quality and relevance. This allows for filtered subsets like MMChat-hf.- The benchmark model incorporated attention routing to handle sparsity by flexibly weighting image regions. Results show this improves over models that don't account for sparsity.- The authors designed the dataset to study real Chinese conversations and noted it could benefit multi-modal pretraining. Overall, it provides a new large-scale resource to advance multi-modal dialogue research.In summary, MMChat offers a uniquely large and spontaneous multi-modal dataset that models the sparsity issue in real dialogues. The dataset and models provide a valuable resource to drive progress in this domain.


## What future research directions do the authors suggest?

 Based on my reading of the paper, here are some of the main future research directions suggested by the authors:- Building larger multi-modal dialogue datasets with more diverse contexts and modalities beyond just text and images. The authors suggest incorporating audio, video, gestures, etc. to develop more engaging and human-like dialogue systems.- Developing more advanced models to handle the image-sparsity issue in multi-modal dialogues, where not every utterance is grounded in the visual context. The authors' proposed attention routing mechanism could be further improved.- Leveraging the large amount of unfiltered raw dialogues they collected for pre-training large multi-modal models. The raw dialogues can help with more robust contextual representations.- Studying the proposed datasets from a social sciences perspective to understand how Chinese multi-modal conversations are conducted in social media.- Developing better evaluation metrics beyond n-gram overlap to assess multi-modal dialogue systems, especially on appropriateness and context-consistency.- Exploring personalization in multi-modal dialogue systems using visual contexts and Artificial Intelligence anchors.- Testing the multi-modal models on downstream tasks like visual question answering using the dialogue history and image context.In summary, the key directions are around scaling up the data, developing better models to handle multi-modality, and exploring research from both AI and social science perspectives. Evaluation and personalization are also important future considerations suggested by the authors.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the paper:The paper introduces MMChat, a large-scale Chinese multi-modal dialogue dataset collected from social media. It contains over 120K dialogues with associated images, addressing the issue of "sparsity" where not all utterances are grounded on the image. The authors propose an elaborate filtering process to construct a high-quality dataset from 32M raw dialogues. They also offer a human-filtered subset MMChat-hf with additional manual verification. To handle sparsity, they build benchmark models using an attention routing mechanism to balance the contribution of image features. Experiments show incorporating images improves dialogue modeling, and their approach alleviates sparsity. The dataset helps develop multi-modal systems and understand Chinese conversations. It also benefits multi-modal pretraining and social science research.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the paper:The paper introduces MMChat, a large-scale Chinese multi-modal dialogue dataset collected from real conversations on social media. The dataset contains over 120,000 filtered dialogue sessions with associated images, addressing the issue of "sparsity" where not all utterances are grounded on the image. Unlike previous datasets that are crowd-sourced or from fictitious sources, MMChat contains real dialogues exhibiting the sparsity phenomenon where the conversation drifts away from the initial image-grounded topic. The authors propose an elaborate data filtering process to construct high-quality dialogues in MMChat. They also provide a human-filtered subset MMChat-hf based on manual annotation of a sample of 100k dialogues. Benchmark models are developed on MMChat to handle the sparsity issue using an attention routing mechanism to selectively utilize image features. Experiments demonstrate the usefulness of incorporating visual information in dialogue modeling, and that explicitly handling sparsity improves performance. The datasets can facilitate research into realistic multi-modal conversations and understanding Chinese social media conversations.


## Summarize the main method used in the paper in one paragraph.

 Here is a one paragraph summary of the main method used in the paper:The paper proposes a multi-modal dialogue generation model based on a sequence-to-sequence architecture. The model has two encoders - one for encoding the textual dialogue context and one for encoding the image context. The textual encoder is a 12-layer Transformer initialized with weights from a pre-trained GPT model. The image encoder is a Faster R-CNN model pre-trained on Visual Genome to extract regional features from images. These encoded representations are merged in the decoder through an attention routing mechanism that computes separate attention scores for the textual context, image context, and previous decoded tokens. This allows flexible control over the image features to handle the sparsity of image-grounding in dialogues. The decoder shares weights with the textual encoder and is trained to generate the next dialogue utterance by attending to the textual, visual and previous token representations.


## What problem or question is the paper addressing?

 Based on my reading, the key points of this paper are:- The paper focuses on building open-domain dialogue systems that can conduct human-like conversations by perceiving multi-modal contexts beyond just text, such as images. These dialogue systems are referred to as Multi-Modal Dialogue Systems (MMDSs).- Existing datasets for training MMDSs have a significant drawback - they assume every utterance in a dialogue is grounded on a given image, but this is often not true in real conversations. The paper terms this issue the "sparsity" phenomenon.- To address this sparsity issue, the paper introduces a new large-scale Chinese multi-modal dialogue dataset called MMChat containing over 120K dialogues. The key feature is that these dialogues exhibit "sparse" image grounding, where only initial utterances are grounded on images.- The paper also provides a human-filtered subset MMChat-hf with higher quality dialogues strongly correlated to images.- Benchmark models are built on MMChat to validate the dataset and specifically handle the sparsity issue using an attention routing mechanism. Results show incorporating images is beneficial but explicitly modeling sparsity further improves performance.In summary, the key problem addressed is the lack of datasets exhibiting real sparse image-grounded dialogues to train better MMDS models, and the paper introduces a large-scale Chinese dataset MMChat to fill this gap. The dataset enables better modeling of real conversations where visual grounding diminishes over time.
