# [The Efficiency Misnomer](https://arxiv.org/abs/2110.12894)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the main research focus seems to be examining the efficiency and computational costs of different neural network architectures. Specifically, the paper discusses various metrics used to measure model efficiency, such as number of parameters, FLOPs, and speed/throughput. The key research questions appear to be:- How well do different efficiency metrics correlate with each other? Can we assume they provide similar views on model efficiency?- Can efficiency metrics be misleading or insufficient if used in isolation to compare models? - How prevalent is incomplete or biased reporting of efficiency metrics in the literature?- What are some concrete examples where different efficiency metrics lead to different conclusions about model comparisons?- How should efficiency metrics be reported to avoid misleading or partial conclusions?The central hypothesis seems to be that no single efficiency metric provides a complete picture, and incomplete reporting of metrics can be misleading. The authors aim to demonstrate cases where efficiency comparisons depend strongly on the choice of metric, and provide suggestions to improve reporting practices.In summary, the key focus is critically analyzing efficiency metrics and their usage in model comparisons, showing how reliance on limited metrics can lead to biased or incomplete conclusions. The authors advocate more holistic reporting of efficiency to avoid these issues.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contributions are:- Highlighting the issues and intrinsic difficulties with measuring the efficiency of deep neural networks using common cost indicators like parameter count, FLOPs, and speed/throughput. - Showing through examples that these cost indicators can often contradict each other or lead to different conclusions about model efficiency. Relying only on one metric can result in an incomplete or misleading picture.- Analyzing the advantages, disadvantages, and trade-offs of different efficiency metrics. Discussing factors that can cause discrepancies between them like parallelism, hardware differences, etc.- Characterizing the problem of making unfair, partial, or incomplete comparisons between models by only reporting metrics favorable to one model - referred to as the "efficiency misnomer."- Providing a set of guidelines and suggestions for better evaluating and reporting efficiency of models, such as: avoiding relying only on one metric, considering both training and inference costs, being aware of how architectural differences affect metrics, and avoiding restricted comparisons that don't give the full picture.In summary, the key contribution is critically looking at common practices for evaluating model efficiency, showing the limitations of relying only on a few standard metrics, and providing recommendations for more accurate and holistic characterization of model efficiency. The paper highlights the non-trivial nature of this issue.
