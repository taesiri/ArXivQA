# [Joint Shapley values: a measure of joint feature importance](https://arxiv.org/abs/2107.11357)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is:How can we extend the axioms underlying the Shapley value from game theory to directly measure the contribution of sets of features (rather than just individual features) to a machine learning model's predictions?The key hypothesis appears to be that extending the original Shapley value axioms to apply to sets of features will lead to a useful "joint Shapley value" that provides insights into groups of correlated features in a model. Specifically, the paper introduces a set of "joint axioms" that extend the original efficiency, symmetry, null player, and linearity properties of the Shapley value to apply to coalitions (sets) rather than just individual players (features). It then proves that there is a unique "joint Shapley value" satisfying these axioms for any given order of explanation k. The joint Shapley value directly extends the intuitions of the original Shapley value to measure the average contribution of a set of features to model predictions. This provides a new tool for understanding feature interactions and relative importance in machine learning models.


## What is the main contribution of this paper?

This paper introduces the concept of joint Shapley values, which directly extend Shapley's original axioms and intuitions to measure the importance of sets of features rather than individual features. The key contributions are:- Formally defining joint Shapley values as a measure of the average marginal contribution of a set of features to a model's predictions. This extends Shapley's original axioms of efficiency, symmetry, null player, etc. to sets rather than just individuals. - Proving the uniqueness of joint Shapley values for any given order of explanation k. The joint Shapley values are shown to be the unique solution to the extended set of axioms.- Illustrating how joint Shapley values provide different and complementary insights to existing interaction indices. Joint Shapley values focus on the overall contribution of a set, while interaction indices look at interactions between specific elements.- Demonstrating how joint Shapley values give intuitive and meaningful results in game theory examples and machine learning attribution problems. The examples show how joint values can reveal effects like negation, enhancement, context dependence, etc. in a model.- Introducing a "presence-adjusted" global Shapley value for binary features that better captures the overall importance of a feature set compared to standard global averages.Overall, the paper formally establishes joint Shapley values as a novel and well-founded approach to understanding the combined importance of groups of features in machine learning models and other applications. The uniqueness results and examples showcase their usefulness in practice.
