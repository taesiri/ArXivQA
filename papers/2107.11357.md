# [Joint Shapley values: a measure of joint feature importance](https://arxiv.org/abs/2107.11357)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is:How can we extend the axioms underlying the Shapley value from game theory to directly measure the contribution of sets of features (rather than just individual features) to a machine learning model's predictions?The key hypothesis appears to be that extending the original Shapley value axioms to apply to sets of features will lead to a useful "joint Shapley value" that provides insights into groups of correlated features in a model. Specifically, the paper introduces a set of "joint axioms" that extend the original efficiency, symmetry, null player, and linearity properties of the Shapley value to apply to coalitions (sets) rather than just individual players (features). It then proves that there is a unique "joint Shapley value" satisfying these axioms for any given order of explanation k. The joint Shapley value directly extends the intuitions of the original Shapley value to measure the average contribution of a set of features to model predictions. This provides a new tool for understanding feature interactions and relative importance in machine learning models.


## What is the main contribution of this paper?

This paper introduces the concept of joint Shapley values, which directly extend Shapley's original axioms and intuitions to measure the importance of sets of features rather than individual features. The key contributions are:- Formally defining joint Shapley values as a measure of the average marginal contribution of a set of features to a model's predictions. This extends Shapley's original axioms of efficiency, symmetry, null player, etc. to sets rather than just individuals. - Proving the uniqueness of joint Shapley values for any given order of explanation k. The joint Shapley values are shown to be the unique solution to the extended set of axioms.- Illustrating how joint Shapley values provide different and complementary insights to existing interaction indices. Joint Shapley values focus on the overall contribution of a set, while interaction indices look at interactions between specific elements.- Demonstrating how joint Shapley values give intuitive and meaningful results in game theory examples and machine learning attribution problems. The examples show how joint values can reveal effects like negation, enhancement, context dependence, etc. in a model.- Introducing a "presence-adjusted" global Shapley value for binary features that better captures the overall importance of a feature set compared to standard global averages.Overall, the paper formally establishes joint Shapley values as a novel and well-founded approach to understanding the combined importance of groups of features in machine learning models and other applications. The uniqueness results and examples showcase their usefulness in practice.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Based on my understanding of the paper, here is a one sentence summary:The paper introduces joint Shapley values as a way to measure the importance of sets of features in machine learning models, extending the standard Shapley value that measures importance of individual features.


## How does this paper compare to other research in the same field?

Without having read the full paper, it is difficult to comprehensively compare it to all other research in explainable AI and feature attribution. However, based on the abstract and introduction, a few key points stand out:- The paper introduces a new attribution method called "joint Shapley values" that directly extends the classic Shapley value to measure the importance of sets of features rather than individual features. This appears novel compared to prior work on set-based attribution methods like interaction indices.- The joint Shapley values aim to capture the average marginal contribution of a set of features to a model's predictions. This is philosophically aligned with the original intuition of Shapley values but adapted to feature sets.- The paper proves theoretical uniqueness results about the joint Shapley values satisfying certain desirable axioms. This provides a principled foundation for the method.- The joint Shapley values do not depend recursively on single feature attributions like some prior set attribution methods. The goal is to assess joint/interactive importance rather than combinations of individual importances.- Experiments show the joint Shapley values can provide complementary insights to existing methods on both synthetic examples and real datasets. This demonstrates the value of the approach in practice.So in summary, the joint Shapley values appear to offer a new perspective on set attribution that stays true to the original Shapley value intuition. The uniqueness theorems and comparisons to other methods help position the technique in the literature. Overall it looks like a solid incremental advance in the field of explainable AI. More detailed comparisons on additional datasets/models would strengthen the empirical evaluation.


## What future research directions do the authors suggest?

Based on my reading of the paper, some of the main future research directions suggested by the authors include:- Developing methods to maintain properties like global efficiency when calculating joint Shapley values. The authors mention the SAGE method from Covert et al. (2020) as an example of a more sophisticated way to maintain efficiency at the global level.- Making the sampling techniques used to estimate joint Shapley values more efficient, such as the approaches proposed by Williamson and Menon (2020) or Mitchell et al. (2021). The authors note the high computational complexity of calculating exact joint Shapley values.- Further exploration of the order of explanation parameter k. The authors suggest k should be chosen to balance insight (higher k) vs computational cost (lower k). More research could help determine optimal or recommended values of k for different applications.- Applying joint Shapley values to additional machine learning model attribution problems beyond the examples given in the paper. The authors present the method as a general approach for determining feature importance in complex models.- Developing specialized techniques for models with certain properties, such as the presence-adjusted global values introduced for binary features. Approaches tailored to common model types could improve interpretability.- Comparative analysis of joint Shapley values and other related methods like interaction indices. The authors suggest the approaches provide complementary information about feature sets.- Investigation of human-centered evaluation approaches to assess the intelligibility of joint Shapley value explanations. The authors note that understanding complex models is labor intensive.So in summary, the main directions are improving computational tractability, exploring the method in a wider range of applications, developing specialized techniques for common situations, and human-centered evaluation of the explanation utility.


## Summarize the paper in one paragraph.

Here is a one paragraph summary of the key points from the paper:The paper introduces the concept of joint Shapley values, which directly extend Shapley's original axioms and intuitions about feature importance to sets of features. Joint Shapley values measure the average contribution of a set of features to a machine learning model's predictions. The authors prove the uniqueness of joint Shapley values for any specified order of explanation k. They show how joint Shapley values provide different insights than existing interaction indices, capturing a set of features' joint contribution rather than effects conditional on other features. Experiments on game theory models and machine learning datasets demonstrate intuitive results, including identifying negation, enhancement, and contextual effects in text data. Overall, joint Shapley values offer a new approach to understanding joint feature importance in machine learning models.


## Summarize the paper in two paragraphs.

Here is a two paragraph summary of the paper:The paper introduces the concept of joint Shapley values, which directly extend the standard Shapley value to measure the contributions of sets of features rather than individual features. The standard Shapley value, based on cooperative game theory, assigns a value to each player that represents their average marginal contribution across all possible coalitions. The joint Shapley value extends this by allowing "players" to arrive and contribute in groups rather than individually. After presenting axioms that uniquely define the joint Shapley values, the paper shows how they can provide complementary insights to existing interaction indices for feature importance. While interaction indices focus on the interplay between specific features, joint Shapley values assess the overall contribution of feature sets. Experiments on simulated data, the Boston housing dataset, and movie reviews demonstrate intuitive attributions from the joint Shapley values. The values capture negation, enhancement, and other relationships between words and feature sets. Overall, joint Shapley values directly extend the desirable properties of the Shapley value to quantify the importance of feature groups.


## Summarize the main method used in the paper in one paragraph.

Here is a one paragraph summary of the main method used in the paper:The paper introduces joint Shapley values, which extend the standard Shapley value to measure the contribution of sets of features to a model's predictions. The key method is an extension of Shapley's original axioms to apply to sets rather than just individual features. In particular, the null axiom is generalized so that a null set of features contributes zero value. The joint efficiency axiom sums values over all sets up to a given size k. Joint anonymity and joint symmetry axioms are also introduced. These axioms uniquely define joint Shapley values for each k. The resulting measure averages a set of features' marginal contribution over possible arrival orders, just as the standard Shapley value does for individual features. Experiments on simulated data, housing data, and movie reviews illustrate how joint Shapley values provide insights into feature interactions.


## What problem or question is the paper addressing?

Based on my reading, the key points of the paper are:- The paper introduces a new concept called "joint Shapley values", which extends the standard Shapley value to measure the importance of sets of features rather than individual features. The standard Shapley value is commonly used in machine learning explainability to assign importance scores to individual input features of a model. - The joint Shapley value measures the average contribution of a set of features to a model's predictions. This allows assessing the joint importance of correlated features, which individual Shapley values can miss.- The paper proposes axioms that uniquely define the joint Shapley values, extending the original axioms of the Shapley value. This includes a "joint null" axiom stating that a set of features that contributes no predictive power should get zero importance.- Experiments on simulated and real-world data demonstrate how joint Shapley values can provide insights into feature interactions and dependencies that are not captured by existing feature importance measures like the Shapley value.- The joint Shapley value directly reflects the original intuition of the Shapley value applied to sets rather than individual features. This differs from prior work on set-based Shapley extensions using different axioms.So in summary, the key problem is extending the Shapley value to sets of features in a principled way to better understand feature interactions and dependencies in complex machine learning models. The joint Shapley value is proposed as a solution based on extending the original Shapley axioms to sets.
