# [Learning or Self-aligning? Rethinking Instruction Fine-tuning](https://arxiv.org/abs/2402.18243)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Instruction fine-tuning (IFT) is a critical step in building large language models (LLMs), but there is limited understanding of its underlying mechanisms. 
- Two potential roles of IFT have been proposed: transferring behavioral norms and injecting additional world knowledge. However, these two factors are coupled together in previous research, making it hard to analyze their individual effects.

Proposed Solution:
- The paper designs a knowledge intervention framework to decouple the two factors in IFT by controlling the consistency between knowledge in IFT data and models' existing parameter knowledge. 
- This enables analyzing the individual impact of behavioral norm transfer vs world knowledge injection.

Key Findings:
- Attempting to inject additional world knowledge through IFT often fails to improve performance and can even be harmful if inconsistent with model's parameter knowledge.
- Maintaining knowledge consistency before and after IFT plays a key role - IFT is more an act of "self-aligning" instruction responses to match models' existing knowledge.
- Optimal IFT balances some inconsistency to expand knowledge while avoiding divergence from original parameter knowledge distribution.

Main Contributions:  
- Framework to decouple and analyze factors affecting IFT.
- Discovering that IFT works more by self-alignment rather than knowledge injection. 
- Identifying knowledge consistency before/after IFT as the key determinant of IFT effectiveness.
