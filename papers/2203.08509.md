# [Differentiable DAG Sampling](https://arxiv.org/abs/2203.08509)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be: How can we develop a fast, differentiable method for probabilistic modeling and sampling of DAGs that guarantees valid DAG outputs during training?The key points are:- The paper proposes a new probabilistic model over DAGs called DP-DAG that allows fast and differentiable DAG sampling. - DP-DAG samples a DAG by sampling a node ordering using Gumbel-Sinkhorn or Gumbel-Top-k, then sampling edges consistent with that ordering using Gumbel-Softmax.- This approach guarantees valid DAG outputs at any point during training, without needing complex augmented Lagrangian optimization schemes. - The paper develops VI-DP-DAG, a variational inference method that combines DP-DAG with neural networks to learn DAGs from observational data.- VI-DP-DAG approximates the posterior over DAG structures given the observed data.- Experiments show VI-DP-DAG learns high-quality DAG structures faster than existing differentiable DAG learning methods.So in summary, the main research question is how to develop a fast, differentiable, and probabilistic approach to DAG modeling and sampling that avoids issues with previous methods and enables high-quality causal learning. DP-DAG and VI-DP-DAG are proposed as solutions.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence TL;DR summary of the paper:The paper proposes a new differentiable probabilistic model over DAGs (DP-DAG) for fast and differentiable DAG sampling, and combines it with variational inference for DAG learning from observational data while guaranteeing valid DAG outputs during training.


## How does this paper compare to other research in the same field?

Here are some thoughts on how this paper compares to other research:- The paper proposes a new differentiable probabilistic model over DAGs called DP-DAG. This appears novel compared to other differentiable DAG learning methods like DAG-GNN, DAG-NF, GraN-DAG, and Masked-DAG which do not define full probability distributions over DAGs. Defining a probabilistic model allows estimating uncertainty over DAG structures.- Most existing differentiable DAG learning methods like DAG-GNN, DAG-NF, GraN-DAG, and Masked-DAG use an augmented Lagrangian optimization which can be computationally expensive. In contrast, DP-DAG enables fast and differentiable sampling of DAGs using Gumbel-Sinkhorn and does not require complex Lagrangian optimization. This seems more efficient.- Many previous differentiable DAG learning methods require non-differentiable post-processing steps like cycle removal to output valid DAGs. DP-DAG guarantees valid DAG outputs during training without needing extra processing steps. This is a nice benefit over methods that require post-processing.- The proposed VI-DP-DAG method combines DP-DAG with variational inference for DAG structure learning. This provides a probabilistic interpretation unlike other methods that just optimize a point estimate DAG. Modeling distribution over structures could be useful for some applications.- The paper shows strong empirical results for DP-DAG and VI-DP-DAG on both synthetic and real-world datasets. The models achieve state-of-the-art or competitive performance for DAG structure learning and estimating causal mechanisms while training significantly faster.Overall, the proposed DP-DAG model and VI-DP-DAG method seem to provide useful innovations over existing differentiable DAG learning techniques, especially in terms of modeling distributions over structures, avoiding complex optimization, and achieving efficient training. The strong experimental results also help demonstrate the effectiveness of the approach.


## What future research directions do the authors suggest?

Based on my reading of the paper, some of the main future research directions suggested by the authors include:- Developing more efficient algorithms and optimizations for learning the structure and parameters of DAG models from data. The authors note that structure learning remains computationally challenging, especially for large graphs. They suggest exploring methods like partitioning the graph, parallel/distributed computing, and approximation algorithms.- Incorporating prior knowledge and constraints into DAG learning. The authors mention incorporating metadata, expert knowledge, and known structural constraints to improve accuracy and efficiency. This can help restrict the search space and guide the discovery process.- Extending DAG models to handle latent variables, cyclic relationships, and feedback loops. The paper focuses on directed acyclic graphs but notes that many real-world systems have cycles or unobserved variables. Developing extensions to model these more complex scenarios is an important direction.- Evaluating and comparing different scoring metrics and criteria for assessing DAGs. The choice of scoring function impacts which structures are preferred. More analysis is needed on the theoretical properties and empirical performance of different scoring metrics. - Developing more robust methods to causal discovery and inference from learned DAGs. They note more work is needed on reliably using DAGs to infer causation and make predictions, especially from observational data.- Scaling up DAG methods to very large datasets with thousands of variables. Developing approximations and distributed algorithms to enable DAG learning on massive datasets is key for real-world applications.- Validating DAG methods on real-world data where ground truth is unknown. Most current evaluation uses simulations where the true DAG is known. Testing on real-world datasets with domain experts is important future work.In summary, the authors highlight improving computational scalability, incorporating more background knowledge, extending modeling capabilities, and demonstrating performance on large real-world applications as key directions for advancing DAG-based learning.


## What is the main contribution of this paper?

Based on the abstract and introduction, the main contributions of this paper appear to be:1. Proposing a new differentiable probabilistic model over DAGs called \ours{} (\oursacro{}) that allows fast and differentiable DAG sampling suited for continuous optimization. \oursacro{} samples a DAG by sampling a node ordering and then sampling edges consistent with that ordering. It uses Gumbel-Sinkhorn, Gumbel-Top-k and Gumbel-Softmax to parametrize differentiable sampling. 2. Proposing VI-\oursacro{}, a new method for learning DAGs from observational data. VI-\oursacro{} combines \oursacro{} with variational inference to approximate the posterior distribution over DAG edges. It guarantees valid DAG outputs during training without needing complex augmented Lagrangian optimization.3. Showing experimentally that VI-\oursacro{} outperforms other differentiable DAG learning methods on both synthetic and real datasets. It achieves better DAG structure learning and learns better causal mechanisms while training significantly faster than other methods.In summary, the main contribution appears to be proposing a fast and simple way to do differentiable DAG sampling that enables a new variational inference method for learning high-quality DAGs from observational data. The method does not rely on complex constrained optimization schemes like other differentiable DAG learning approaches.
