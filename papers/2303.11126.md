# [Robustifying Token Attention for Vision Transformers](https://arxiv.org/abs/2303.11126)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question seems to be: 

How can we make the attention mechanism in vision transformers (ViTs) more robust against common image corruptions and perturbations?

The authors observe that standard ViTs tend to rely too heavily on a few "important" tokens when computing self-attention. However, these tokens are not actually robust and can change entirely when the input image is corrupted with noise, blur, etc. 

To address this, the authors propose two main techniques:

1) Token-aware Average Pooling (TAP): Allows each token to adaptively aggregate information from its local neighborhood, making the model less reliant on just a few individual tokens.

2) Attention Diversification Loss (ADL): Explicitly encourages the model to distribute attention across diverse input tokens rather than focusing too heavily on the same "important" tokens.

The central hypothesis is that alleviating the "token overfocusing" phenomenon via these techniques will substantially improve the robustness of ViT models against common image corruptions while maintaining accuracy on clean images. The experiments aim to validate this hypothesis across different model architectures and tasks.

In summary, the core research question is how to make ViT attention more robust, with the central hypothesis being that reducing token overfocusing will achieve this goal. The TAP and ADL methods are proposed to address the token overfocusing problem.
