# [Agglomerative Transformer for Human-Object Interaction Detection](https://arxiv.org/abs/2308.08370)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central research question this paper tries to address is: How can we enable Transformer-based human-object interaction (HOI) detectors to flexibly exploit extra instance-level cues (e.g. pose, gaze) in an efficient, single-stage manner? The key hypotheses appear to be:1) Instance-level cues like human pose and gaze are important for HOI detection, especially for recognizing subtle differences between highly similar interactions. 2) Prior Transformer-based HOI detectors struggle to leverage these cues due to misalignment between local patch tokens and instance-level information.3) By proposing "instance tokens" that dynamically cluster patches into integral instance representations, the authors' method AGER allows efficient incorporation of extra cues in a single-stage Transformer pipeline.In summary, the paper aims to improve HOI detection in Transformers by better aligning representations with full instance context, enabling flexible utilization of extra cues without compromising efficiency or pipeline complexity. The key novelty is the dynamically learned instance tokens aligned to entire object/human extent.


## What is the main contribution of this paper?

The main contributions of this paper are:1. It proposes AGER, a new framework for human-object interaction (HOI) detection that enables Transformer-based detectors to flexibly exploit extra instance-level cues in a single-stage and end-to-end manner. 2. It introduces the concept of instance tokens, which are generated by dynamically clustering patch tokens using a text-guided mechanism. Instance tokens encourage the integrality of instance representations, allowing the flexible extraction of cues like human pose, spatial locations, and object categories.3. AGER achieves new state-of-the-art performance on the HICO-Det dataset, demonstrating the benefits of exploiting extra cues with integral instance tokens in a single-stage pipeline. It reduces GFLOPs by 8.5% and improves FPS by 36% compared to prior Transformer-based methods.4. The proposed text-guided clustering mechanism enables end-to-end training of the instance encoder in AGER, eliminating the need for additional object detectors or instance decoders. 5. AGER provides the first demonstration of incorporating clustering into Transformers for HOI detection, opening up new possibilities for efficient utilization of extra cues in this domain.In summary, the main contribution is the proposal of AGER, a new single-stage Transformer-based framework for HOI detection that leverages integral instance tokens to flexibly exploit extra cues while achieving efficiency gains. The instance tokens are generated via a novel text-guided clustering approach.
