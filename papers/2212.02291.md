# [I2MVFormer: Large Language Model Generated Multi-View Document   Supervision for Zero-Shot Image Classification](https://arxiv.org/abs/2212.02291)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading, the main research question this paper addresses is:

Can large language models generate useful auxiliary information in the form of text descriptions (views) for zero-shot image classification models, and can a model be designed to effectively leverage multiple complementary views? 

The key hypotheses seem to be:

1) Large language models can generate high-quality, discriminative text views for unseen classes when conditioned with just a few examples, reducing the need for expensive human annotations.

2) Multiple views from a language model can provide complementary information about a class compared to relying on just a single text source like Wikipedia.

3) A model specifically designed for multi-view learning, like the proposed I2MVFormer architecture, will be more effective at exploiting multiple text views than simply concatenating them.

4) Learning to summarize the most discriminative information from each view before aligning with visual features will improve efficiency and performance compared to exhaustive attention between all text tokens and image patches.

The experiments aim to validate these hypotheses by showing performance improvements from using LLM-generated multi-view supervision and the benefits of the proposed I2MVFormer model over strong baselines. The overall goal is to advance zero-shot learning by generating better auxiliary information with less human effort and designing models to make optimal use of it.


## What is the main contribution of this paper?

 The main contributions of this paper are:

1. It provides the first study into using a Large Language Model (LLM) to generate auxiliary text information for zero-shot image classification. The authors propose a prompting strategy to extract multiple text descriptions ("views") for each class from the LLM, showing that the different views provide complementary information.

2. The paper proposes a new model called I2MVFormer for zero-shot image classification using the multi-view LLM-generated text supervision. The model has two key components:

- SVSummary module that extracts a discriminative summary from each view. 

- MVSummary module that combines the per-view summaries into a multi-view class-level summary. 

Together these allow the model to align the multi-view text information with global and local visual features to learn an effective zero-shot classifier.

3. The I2MVFormer model with LLM-generated multi-view supervision establishes new state-of-the-art results on three benchmark datasets for zero-shot image classification using unsupervised text. The model significantly outperforms previous methods that use a single text source like Wikipedia.

4. The results show that the multi-view text from the LLM provides complementary information to Wikipedia text, and that I2MVFormer is better at consuming multi-view knowledge compared to prior work. The LLM text alone can outperform prior work with Wikipedia text.

In summary, the key innovation is using an LLM to generate multi-view class descriptions for zero-shot learning, and designing a model (I2MVFormer) to effectively learn from this multi-view supervision. This allows reaching new state-of-the-art results.
