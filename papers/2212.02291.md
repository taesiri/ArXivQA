# [I2MVFormer: Large Language Model Generated Multi-View Document   Supervision for Zero-Shot Image Classification](https://arxiv.org/abs/2212.02291)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading, the main research question this paper addresses is:

Can large language models generate useful auxiliary information in the form of text descriptions (views) for zero-shot image classification models, and can a model be designed to effectively leverage multiple complementary views? 

The key hypotheses seem to be:

1) Large language models can generate high-quality, discriminative text views for unseen classes when conditioned with just a few examples, reducing the need for expensive human annotations.

2) Multiple views from a language model can provide complementary information about a class compared to relying on just a single text source like Wikipedia.

3) A model specifically designed for multi-view learning, like the proposed I2MVFormer architecture, will be more effective at exploiting multiple text views than simply concatenating them.

4) Learning to summarize the most discriminative information from each view before aligning with visual features will improve efficiency and performance compared to exhaustive attention between all text tokens and image patches.

The experiments aim to validate these hypotheses by showing performance improvements from using LLM-generated multi-view supervision and the benefits of the proposed I2MVFormer model over strong baselines. The overall goal is to advance zero-shot learning by generating better auxiliary information with less human effort and designing models to make optimal use of it.
