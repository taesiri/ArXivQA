# [Creativity Inspired Zero-Shot Learning](https://arxiv.org/abs/1904.01109)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the key research question this paper addresses is: How to improve zero-shot learning by explicitly modeling the capability to deviate generations of unseen classes from seen classes, inspired by concepts of creativity?The main hypothesis is that encouraging the model to carefully deviate from seen classes will improve its ability to recognize unseen classes in zero-shot learning scenarios. The authors propose a novel approach called Creativity Inspired Zero-Shot Learning (CIZSL) that incorporates this idea.Specifically, some key aspects the paper investigates:- Modeling a "creativity inspired" loss that encourages the generator to produce features for unseen classes that are distinct from seen classes, while still remaining realistic. This is inspired by concepts like the Wundt curve from psychology of human creativity.- Defining and experimenting with different parameterized entropy losses to help learn appropriate levels of deviation from seen classes.- Showing their proposed CIZSL approach improves performance across multiple zero-shot learning tasks like recognition, retrieval, and generalized zero-shot learning on benchmarks using both Wikipedia articles and attributes for class descriptions. - Demonstrating their creativity-inspired loss gives consistent gains when added to existing generative zero-shot learning models.So in summary, the central hypothesis is that explicitly modeling deviation from seen classes will help generalization to unseen classes in zero-shot learning, and they design and evaluate a method to incorporate this idea.


## What is the main contribution of this paper?

Based on my understanding, the main contributions of this paper are:1. It proposes a novel zero-shot learning approach called Creativity Inspired Zero-shot Learning (CIZSL) that explicitly models the capability to generate discriminative visual features for unseen classes by learning to deviate from seen classes. This is inspired by the psychology of human creativity and the notion of the Wundt curve. 2. The proposed CIZSL loss is unsupervised and orthogonal to existing generative ZSL methods. It can be easily integrated with any generative ZSL model like GAZSL, FeatGen, and cycle-WGAN to improve their performance without adding extra parameters or requiring more labels.3. Extensive experiments on 7 benchmarks using both Wikipedia articles and attributes as semantic representations demonstrate consistent improvements in zero-shot recognition, generalized zero-shot learning, and zero-shot retrieval over state-of-the-art methods.In summary, the key novelty is modeling the deviation from seen classes for generating discriminative unseen class features, inspired by human creativity. The simplicity of integrating the proposed CIZSL loss with existing models and the consistent gains across different tasks and datasets are also notable contributions.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper proposes a creativity-inspired approach for zero-shot learning that carefully deviates visual feature generations of unseen classes from seen classes to improve discrimination, while using an entropy measure to maintain realism and enable knowledge transfer from seen classes.


## How does this paper compare to other research in the same field?

This paper presents a creative approach to zero-shot learning, which aims to recognize objects from classes not seen during training. It makes the following key contributions:- Proposes a novel creativity-inspired loss function for generative adversarial networks (GANs) to encourage the model to deviate from seen classes when generating examples of unseen classes. This is inspired by theories of creativity from psychology and helps improve discrimination between seen and unseen classes. - Integrates the proposed creativity loss in a simple and modular way into existing GAN-based zero-shot learning methods like GAZSL and FeatGen. Shows consistent improvements over state-of-the-art on multiple benchmark datasets.- Comprehensive evaluation on multiple tasks - zero-shot recognition, generalized zero-shot learning, and zero-shot retrieval using both Wikipedia articles and attribute representations.The key difference from prior work is the explicit modeling of a creativity-inspired loss to encourage discrimination between seen and unseen classes. Most prior GAN-based zero-shot learning works like GAZSL and FeatGen do not have an explicit incentive to deviate from seen classes. Some prior works on computational creativity for art/fashion generation use losses to encourage deviation but are not designed for cross-modal synthesis and zero-shot learning.The simplicity of the proposed approach and its consistency in improving existing models is a strength. The connections drawn to psychology/creativity literature and the comprehensive evaluation are also distinguishing aspects. Overall, it presents a novel perspective to generative zero-shot learning through the lens of computational creativity.


## What future research directions do the authors suggest?

The authors of the paper suggest a few future research directions in zero-shot learning:1. Combining generative and embedding-based methods: The authors mention that generative and embedding-based methods have complementary strengths, so combining them could lead to further improvements. They suggest ideas like using synthesized examples to train embedding models.2. Exploring different semantic representations: Most existing work uses attributes or word vectors as the semantic representation. The authors suggest exploring other representations like language models or knowledge graphs. This could provide richer semantic information to help with generalization.3. Studying the effect of synthesized examples: The authors generate fake examples as training data, but how these synthetic examples affect model training is still not well understood. Analyzing the effect of synthetic data and developing ways to optimize the generator to produce better examples are interesting directions. 4. Applying to broader domains: Much ZSL research focuses on datasets of objects, animals, etc. Applying ZSL to complex real-world scenarios like robotics and improving the robustness is an important direction.5. Theoretical analysis: There is little theoretical analysis of ZSL methods. Analyzing the generalization guarantees, sample complexity, effect of different semantic spaces etc. theoretically can provide better understanding.6. Interactive/active learning: Allowing models to query information as needed like attributes of unseen classes can make ZSL more practical. Studying interactive learning settings is suggested.7. Social awareness and contextual knowledge: Since human ZSL relies heavily on contextual knowledge and social interactions, incorporating external knowledge and social learning signals like dialog could make models more powerful.In summary, some key directions are combining existing approaches, exploring new representations and domains, theoretical analysis, studying the effect of synthesized data, and incorporating external knowledge or interactions. Advancing ZSL along these directions could enable deploying ZSL in real-world applications and make progress towards human-like learning.


## Summarize the paper in one paragraph.

Here is a one paragraph summary of the paper:The paper proposes a generative adversarial network (GAN) approach for zero-shot learning. Zero-shot learning aims to recognize objects from classes not seen during training, by leveraging side information like text descriptions or attributes. The key idea is to train a GAN model to generate visual features for unseen classes conditioned only on their text descriptions, converting zero-shot learning into a conventional classification task. The model has a generator that takes noisy text as input and generates fake image features, and a discriminator that tries to distinguish real from fake features and predict class labels. A key contribution is a "creativity inspired" loss that encourages the generator to deviate from seen classes when generating for unseen classes, while preserving realism. This improves discrimination between seen and unseen classes. Extensive experiments on benchmark datasets demonstrate state-of-the-art performance on zero-shot recognition, retrieval, and generalized zero-shot learning using both Wikipedia articles and attributes as side information. The simple yet effective approach consistently outperforms prior work.


## Summarize the paper in two paragraphs.

Here is a two paragraph summary of the paper "Creative Adversarial Networks Generating "Art" by Learning About Styles and Deviating from Style Norms":The paper proposes a novel method called Creative Adversarial Networks (CANs) to generate art images that are creative and novel compared to existing artworks. The key idea is to build on top of Generative Adversarial Networks (GANs) but add an additional loss function that encourages the generated images to deviate from the styles seen during training. Specifically, the authors first train a Deep Convolutional GAN on a dataset of existing artworks in a particular style (e.g. landscapes). This allows the GAN to generate realistic images mimicking the training data style. To make the GAN generate novel and creative images, an additional multi-class entropy loss is added to the training process. This loss maximizes the entropy of the training style classification distribution predicted by the GAN discriminator. By maximizing entropy, the GAN is encouraged to generate images that do not clearly match any one style seen during training. Experiments show the proposed CAN model can generate abstract artworks that are considered creative by humans while still resembling the overall training domain like landscapes.In summary, the paper demonstrates how GANs can be adapted to generate creative and novel content by adding an entropy loss that promotes deviation from training data distributions. The proposed CAN model opens up interesting future work at the intersection of AI creativity and generative modeling.


## Summarize the main method used in the paper in one paragraph.

Here is a one paragraph summary of the main method used in the paper:The paper proposes a generative adversarial network (GAN) based approach for zero-shot learning. The key idea is to use a conditional GAN to generate visual features for novel classes given only their semantic descriptions such as text or attributes. Specifically, the generator takes as input a semantic description concatenated with random noise and produces a synthesized visual feature. The discriminator is tasked with distinguishing real visual features of seen classes from the fake generated features, as well as predicting the class label. A visual pivot regularization term is added to the GAN loss to encourage the distribution of fake features for each class to be centered around the average feature of real samples from that class. This allows transforming the zero-shot learning problem into a conventional classification task, where a classifier can be trained on the generated pseudo-data of unseen classes. The model is trained in an adversarial fashion with alternating optimization of the generator and discriminator. Extensive experiments demonstrate superior performance on benchmark datasets compared to previous state-of-the-art methods.In summary, the key aspects are: 1) Using a conditional GAN framework to generate visual features from semantic descriptions of novel classes. 2) Adding a visual pivot regularization to match distributions of real and fake features. 3) Converting zero-shot learning to classification on generated pseudo-data. 4) Adversarial training of generator and discriminator networks. 5) Demonstrating improved performance on benchmarks.
