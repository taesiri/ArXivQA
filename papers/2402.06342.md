# [Promoting Target Data in Context-aware Neural Machine Translation](https://arxiv.org/abs/2402.06342)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Standard neural machine translation (NMT) models translate sentences in isolation and fail to adequately handle discourse-level phenomena like lexical cohesion, deixis, gender coherence, etc. that require contextual information. 
- Context-aware NMT models typically rely on parallel source-target context sentences, but most discourse phenomena rely on target-side information or both source and target context.

Proposed Solution:
- Promote target language context by prepending target context sentences to the source sentence in a concatenation-based approach, either alone (tgt-n-to-n) or combined with source context (src+tgt-n-to-n).  
- Intuition is that target context will be exploited by the decoder while source context will be copied or associated with source information by the encoder.

Experiments:
- Evaluated on English-Russian and Basque-Spanish over parallel and contrastive test sets covering target-only and source+target phenomena.
- Proposed tgt-n-to-n and src+tgt-n-to-n models outperform n-to-n baseline that uses equal source+target context.
- src+tgt-n-to-n achieved best overall results, significantly improving on target phenomena and achieving parity/slight gains on other phenomena.  

Contributions:
- Showed promoting target context leads to significant gains, especially on target-side phenomena, with no architecture changes.
- Combining source+target context provides best balance, outperforming strong n-to-n baselines across different phenomena.  
- Proposed approach robust even with back-translated target data instead of parallel corpora.
- Simple yet effective way of exploiting target context in standard NMT architectures.

In summary, the paper demonstrates the efficacy of promoting target context data in context-aware NMT via simple concatenation-based approaches without any architectural changes. The src+tgt-n-to-n model provides a robust approach to improve performance across different discourse phenomena.
