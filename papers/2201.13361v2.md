# [Signing the Supermask: Keep, Hide, Invert](https://arxiv.org/abs/2201.13361v2)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading, the key points of this paper are:

- It introduces the concept of "signed Supermasks", which extends prior work on Supermasks by allowing weights to be masked or sign-inverted during training. This provides more flexibility compared to regular Supermasks that only mask weights.

- Signed Supermasks are shown to identify very sparse subnetworks (0.5-4% of original weights) that match or exceed the performance of baseline models across various architectures and datasets. This demonstrates that very few weights may be sufficient to solve a task.

- The proposed ELUS weight initialization method accounts for masking and the ELU activation, leading to better performance than standard initializations like He or Xavier.

- Analysis of the learned masks provides insights into which weights matter most and shows the similarity of identified subnetworks across runs, suggesting the masks capture meaningful network structure. 

- Adjusting hyperparameters can indirectly control the pruning rate, allowing extremely sparse networks (<1% weights) that still perform decently. This reveals how few weights are really needed.

- Overall, signed Supermasks provide a simple yet powerful approach to uncover highly sparse, performant subnetworks. This facilitates efficiency and potentially improved interpretability while maintaining accuracy.

In summary, the central hypothesis is that allowing both masking and sign-inverting during training can identify extremely sparse neural network substructures that perform as well or better than the original models. The experiments across various models validate this hypothesis and analyze the properties of the resulting signed Supermasks.
