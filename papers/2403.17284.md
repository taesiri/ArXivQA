# [Common Ground Tracking in Multimodal Dialogue](https://arxiv.org/abs/2403.17284)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
The paper addresses the challenge of tracking the common ground (shared beliefs and knowledge) between participants in a collaborative, multimodal dialogue. Specifically, it focuses on task-oriented dialogues where participants communicate through multiple modalities like speech, gesture, and physical actions to achieve a shared goal. Tracking the common ground is important for dialogue systems to understand user needs and beliefs. 

Prior work has looked at dialogue state tracking to model user needs, but less attention has been paid to tracking common ground. The paper argues that in collaborative tasks, tracking common ground is as important as tracking individual user states.

Proposed Solution:
The paper presents a computational model and automated pipeline to track the evolution of common ground in a multimodal dialogue. The core of their solution is representing common ground as a "Common Ground Structure" (CGS) consisting of:

- QBank: Questions under discussion that need to be resolved 
- EBank: Propositions that have some evidence of truth  
- FBank: Propositions accepted as true facts

They collect a multimodal dataset of groups collaborating on a physical task. This is annotated with speech, gestures, actions and common ground moves like statements, accepts, doubts. 

They train a neural sequence model to classify dialogue moves. Extracted propositions are then fed to a set of formal closure rules that update the CGS by promoting facts from questions to evidence, or evidence to accepted facts.

Contributions:

1. A new challenging task of tracking common ground from multimodal conversational evidence

2. An integration of a formal model of common ground within an automated pipeline that can process raw multimodal signals

3. Release of a novel dataset with detailed semantic annotations enabling the study of common ground dynamics

The model outputs at each turn an updated estimate of the group's common ground. This can be used by intelligent agents to better understand collaborative tasks and facilitate productive group work. The authors empirically evaluate different feature types and establish performance benchmarks on this novel task.


## Summarize the paper in one sentence.

 Here is a one-sentence summary of the paper:

The paper presents a method for tracking the evolution of common ground (the set of shared beliefs) in a multimodal dialogue by detecting dialogue moves expressed through multiple modalities like speech, gesture, and action, extracting the propositions conveyed, and using a formal model of belief updated via public announcements to incrementally construct common ground structures reflecting the questions under discussion, evidence, and facts agreed upon by the group over the course of a collaborative, situated task.


## What is the main contribution of this paper?

 According to the paper, the main contributions are:

1. A challenging new task: multimodal common ground tracking, with a formal model of common ground in a shared, situated task.

2. A novel incorporation of the formal model into an automated pipeline that tracks the evolution of group common ground over time. 

3. An augmentation of the Weights Task Dataset with gesture, action, and common ground annotations, to enable the operationalization of the formal model.

In summary, the key contribution is introducing the novel task of tracking common ground in multimodal dialogues, proposing a formal model for it, and implementing an automated pipeline to perform this tracking on a multimodal dataset. The formal model captures shared beliefs, evidence, and open questions, and the pipeline uses a classifier and rules to update these components based on observable actions and utterances.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and concepts related to this work include:

- Common ground tracking (CGT) - The main task focused on in the paper, identifying the shared beliefs and "questions under discussion" (QUDs) held by participants in a dialogue.

- Multimodality - The paper studies multimodal interactions, including speech, gestures, actions, etc. as different modes of communication to update common ground.

- Formal model - The paper presents a formal model of common ground, evidence-based belief, and update rules for incorporating new information. 

- Dialogue Game Board (DGB) - The Common Ground Structure consisting of QBank, EBank, and FBank is based on the notion of a DGB.

- Weights Task Dataset (WTD) - The dataset of collaborative problem solving used and augmented with additional gesture, action, and common ground annotations.  

- Propositional content - The factual information and beliefs extracted from utterances and other behaviors to populate the Common Ground Structure.

- Cognitive dialogue states - Categories like STATEMENT, ACCEPT, DOUBT that are annotated and predicted to determine impacts on common ground.

- SÃ¸rensen-Dice coefficient - The evaluation metric used to quantify overlap between predicted and true common ground contents.

- Multimodal features - Combinations of textual, acoustic, collaborative, gesture, and action features used by models to predict dialogue states and propositions.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in this paper:

1. How does the common ground structure (CGS) consisting of the QBank, EBank, and FBank relate to the formal definitions and axioms of evidence-based belief from dynamic epistemic logic? What theoretical guarantees or limitations does this connection provide?

2. The updates to the CGS defined use public announcement logic operators. What implicit assumptions about the dialogue situation and participants' access to information are embedded in this choice? Could a different set of update rules better capture situated, multimodal dialogues? 

3. How was the move classifier model architecture designed and optimized? What alternatives were considered and how did they compare in performance? What are the limitations of the current model?

4. What were the most difficult challenges in extracting propositions from the dense paraphrases using only text and a language model? How robust is this approach to complex utterances involving multiple objects or indirect speech? 

5. How do the closure rules handle compound propositions involving multiple blocks or relations like inequalities? Are they flexible enough to generalize to more complex tasks and goals?

6. The results show differences across groups in how various modalities contribute to common ground tracking. What hypotheses can be made about group communication strategies to explain these differences?

7. How sensitive is the pipeline to misclassifications of move types by the classifier? What cascading effects could these errors have on downstream components like the closure rules? 

8. What assumptions are made about participant knowledge, access to information, and goals that may limit applying this method to other collaborative tasks? How could the formal model and updates handle conflicting beliefs between participants?

9. From an HCI perspective, what additional modalities could augment the common ground tracking? How might features like emotion, engagement, or personality traits further inform the dialogue model?

10. If deployed as part of an AI assistant in a collaborative learning environment, what interventions could the CGS model enable to improve group outcomes and facilitate productive interactions?
