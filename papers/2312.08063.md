# [Estimation of Concept Explanations Should be Uncertainty Aware](https://arxiv.org/abs/2312.08063)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a high-quality paragraph summarizing the key points of the paper:

This paper proposes a new method called Uncertainty-Aware Concept Explanations (U-ACE) for generating more reliable concept-based explanations of machine learning models. Concept explanations aim to interpret a model by estimating the importance of human-understandable concepts. The authors observe that existing methods for estimating concept explanations can be unstable due to high variance, especially when concepts are missing or irrelevant. To address this, U-ACE incorporates Bayesian uncertainty modeling into the concept importance estimation. Specifically, U-ACE models aleatoric uncertainty arising from noisy concept activations as well as epistemic uncertainty arising from missing or hard-to-recognize concepts. Through theoretical analysis, the authors show U-ACE is more robust to over-complete and under-complete concept sets compared to standard methods. Empirically, U-ACE generates higher quality explanations on both synthetic and real-world image datasets based on distance from a simple linear explanation method using ground truth annotations. The improved reliability and label-efficiency of U-ACE concept explanations could make them valuable for tasks like model debugging and scientific discovery.
