# [Tasks That Language Models Don't Learn](https://arxiv.org/abs/2402.11349)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- The paper argues that large language models (LLMs) like ChatGPT, despite their impressive capabilities, lack a comprehensive understanding of certain aspects of language due to their text-only training. Specifically, they miss out on the rich multimodal nature of human language acquisition involving visual, auditory, and other sensory inputs.

- This sensory deprivation means LLMs may have "blind spots" in processing stylistic, orthographic, visual, and auditory features of language. Their understanding of the supradiegetic aspects of language (how it looks and sounds) is limited compared to the diegetic, semantic content. 

- Drawing on philosophical thought experiments like Mary's Room, the paper contends there are experiential aspects of language LLMs cannot learn from text alone, posing challenges for developing models that truly understand language like humans.

Proposed Solution:
- The paper introduces a benchmark called the H-Test comprising 10 classification tasks testing LLMs' ability to infer visual and auditory properties of language they have never directly experienced.

- Tasks include identifying words with one letter uppercase, words starting with vowels, rhyming words etc. Intuitive for humans, but require sensory grounding absent in LLM training.

Main Contributions:
- Empirically demonstrates through H-Test that leading LLMs like Claude, Anthropic, LLaMA, GPT-3.5 etc. stay near 50% accuracy, close to random chance, highlighting linguistic "blind spots."

- Shows common LLM improvements like more data, model scale, computational power, multi-step reasoning do not easily solve H-Test, strengthening the claim that sensory experience is critical.

- Performance gap between GPT-3.5 and GPT-4 (80% accuracy) shows multimodality may be imperative for comprehensively learning language. But smaller multimodal models don't automatically do better.

- Overall makes a case for key perceptual differences between human and LLM language processing, raising philosophical questions about the depth of language understanding current text-trained models can achieve.

Let me know if you would like me to clarify or expand on any part of this summary further. I aimed to capture the core essence as concisely yet completely as possible.
