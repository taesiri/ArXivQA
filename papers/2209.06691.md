# [Certified Robustness to Word Substitution Ranking Attack for Neural   Ranking Models](https://arxiv.org/abs/2209.06691)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question appears to be: How can we develop a certified defense method to enhance the robustness of neural ranking models against word substitution ranking attacks?The key points are:- Neural ranking models have shown promising results but are vulnerable to adversarial attacks like word substitution ranking attacks. This raises concerns when deploying them in real applications.- Existing defense methods like adversarial training offer no theoretical guarantee on models' robustness and can eventually be defeated by other attacks. - To escape this arms race, the authors propose developing rigorous and provable certified defense methods for neural ranking models. - The paper introduces a notion of Certified Top-K Robustness for ranking models, indicating the model can prevent documents outside top K from entering top K under any attack.- It proposes CertDR, a certified defense method, to achieve certified top-K robustness by constructing a smoothed ranking model and deriving a certification criterion based on ranking and statistical properties.- Experiments demonstrate CertDR can significantly improve defense ability and certified robustness compared to state-of-the-art empirical defenses.In summary, the key research question is how to develop a theoretically grounded certified defense approach to enhance the robustness of neural ranking models against adversarial attacks like word substitution ranking attacks. The proposed CertDR method aims to address this question.


## What is the main contribution of this paper?

This paper proposes a certified defense method called CertDR for defending neural ranking models against word substitution attacks. The main contributions are:1. It defines a new notion of "Certified Top-K Robustness" for ranking models, which means the model can provably prevent documents outside the top K from being attacked into the top K. This is more suitable for ranking scenarios where people care about top results. 2. It develops a certification procedure based on randomized smoothing to verify the certified top-K robustness without exhaustive search. It provides theoretical analysis on how to leverage the ranking property and statistical property of randomized smoothing for efficient certification.3. It proposes a practical certified defense algorithm. It uses noise data augmentation to train a smoothed ranker and conducts statistical tests to certify the top-K robustness. 4. It evaluates CertDR on two web search datasets and shows it can significantly improve the certified robustness over state-of-the-art empirical defenses.In summary, the main contribution is proposing the first certified defense method for neural ranking models to provably improve robustness against word substitution attacks. It provides a new notion of robustness suitable for ranking and develops both theoretical and algorithmic solutions.


## How does this paper compare to other research in the same field?

Here are some thoughts on how this paper compares to other research in the field of adversarial attacks and defenses for neural ranking models:- This paper introduces a new concept of "Certified Top-K Robustness" for ranking models, which is tailored to the unique requirements of information retrieval compared to classification tasks. Defining and achieving certified robustness specifically for ranking is a novel contribution not explored in prior work on adversarial attacks in IR.- The proposed defense method CertDR is the first certified defense approach designed for neural ranking models. It leverages randomized smoothing, which has been used for certified defenses in image and text classification, but the authors adapt it innovatively for the ranking task. - Most prior defense methods for adversarial attacks are empirical defenses, meaning they improve robustness in practice but don't provide theoretical guarantees. This work provides the first method to certifiably verify the robustness of ranking models. - Compared to empirical defenses like adversarial training or data augmentation, CertDR demonstrates superior defense performance in the experiments. However, there is still substantial room for improvement in achieving high certified robustness.- The paper compares different types of ranking models - classical probabilistic models like BM25 and neural models like BERT. The analysis shows even powerful pretrained models like BERT have limited certified robustness, highlighting the difficulty of this problem.- The notion of certified robustness and the CertDR defense are only considered for word substitution attacks. Extending the notions and techniques to other kinds of attacks could be an interesting direction for future work.Overall, this paper makes an important first contribution in defining and achieving certified robustness for neural ranking models. The results demonstrate limitations of existing models and defenses, and point to many interesting open problems in developing provably robust ranking methods. The approach has parallels with certified defenses in classification, but is adapted creatively for information retrieval.


## What future research directions do the authors suggest?

Based on my reading of the paper, here are some of the main future research directions suggested by the authors:- Develop techniques to strengthen the notion of Certified Top-K Robustness to guarantee that the order of top-K ranking results remains unchanged under attacks. The current definition only prevents documents outside the top K from entering the top K. - Explore more empirical defense methods for neural ranking models, in addition to just augmenting the training set with adversarial examples. The authors note that more adequate empirical defenses need to be developed specifically for information retrieval.- Achieve a better trade-off between clean accuracy and robust accuracy for the randomized smoothed rankers. The experiments showed a moderate drop in ranking performance for the smoothed ranker compared to the original ranker.- Leverage pre-training techniques like BERT to further enhance the robustness of neural ranking models. The experiments showed BERT had higher certified robustness than other models.- Extend the framework to defend against query attacks, not just document attacks. The current method focuses on document perturbations.- Develop certified defenses that provide guarantees about keeping the top-K results in the same order, which is more demanding than just keeping documents outside top-K out.- Evaluate the approach on other datasets and tasks beyond web search ranking.- Analyze the certified robustness of different ranking models in more depth to understand what factors affect robustness.In summary, the main future directions are around developing more advanced empirical and certified defenses tailored to ranking, achieving better accuracy-robustness trade-offs, leveraging pre-training, extending the guarantees to keep top-K order intact, evaluating on more tasks, and analyzing what makes models more robust.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence TL;DR summary of the paper:The paper proposes a certified defense method called CertDR that can provably enhance the robustness of neural ranking models against word substitution attacks by constructing a smoothed ranking model and certification bounds based on the statistical properties of random ensembles.
