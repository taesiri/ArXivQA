# [TaleCrafter: Interactive Story Visualization with Multiple Characters](https://arxiv.org/abs/2305.18247)

## What is the central research question or hypothesis that this paper addresses?

This paper proposes an interactive story visualization system for generating videos from plain text stories. The key research questions and focus areas are:- How to build a generic story visualization system that can handle novel characters and scenes beyond a specific training dataset? - How to achieve identity consistency when generating multiple characters in the story visualization?- How to enable control over the layout and local object structure in the generated images, beyond implicit determination solely by the text?The main hypotheses are:- Leveraging large pre-trained language models and text-to-image models can enable generalization to new characters and scenes.- Learning personalized weights for each character can help maintain identity consistency across generated images.- Incorporating layout and sketch as additional inputs can allow control over object locations and local structures.To summarize, this paper focuses on developing an interactive story visualization system that can adapt to novel characters and scenes while preserving identity consistency and supporting user control over layout and local object structure. The core research questions revolve around utilizing large pre-trained models and multi-modal inputs to achieve these capabilities.


## What is the main contribution of this paper?

The main contribution of this paper is proposing an interactive story visualization system that can handle multiple novel characters and scenes while supporting interactive editing of layout and local structure. Specifically, the paper makes the following key contributions:1. It proposes a complete pipeline for interactive story visualization, consisting of four main components:- Story-to-prompt (S2P) generation: Converts a story into detailed prompts suitable for text-to-image models using a large language model.- Text-to-layout (T2L) generation: Generates layouts depicting the main subjects in the prompts. Allows interactive refinement of the layout.  - Controllable text-to-image (C-T2I) generation: Renders images based on prompts, layouts, sketches, and character images, while preserving character identities. Enables editing of layout and local structure.- Image-to-video (I2V) animation: Animates the generated images to create vivid videos.2. It develops a novel controllable text-to-image generation module (C-T2I) that serves as the core of the system. This module can handle multiple modalities such as text, layout, and sketch while preserving identities of multiple characters. It supports interactive editing of layout and local structure.3. The proposed system can adapt to new characters and scenes by leveraging large pre-trained language and image models. It does not require training on specific datasets. This enables zero-shot generalization.4. Extensive experiments demonstrate the ability of the system to generate consistent and high-quality visual story sequences involving multiple characters and scenes. Both automatic metrics and human evaluations validate the effectiveness of the approach.In summary, the key innovation is a versatile story visualization system built on top of large pre-trained models, with a focus on identity consistency, structure controllability, and generalization ability. The controllable text-to-image module is the main technical contribution that makes this possible.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper proposes an interactive story visualization system with four components - generating prompts from stories, generating layouts from prompts, generating images from prompts, layouts and sketches while preserving character identities, and animating the images into videos - that enables creating vivid visualizations for new scenes and characters through controllable image generation and editing.
