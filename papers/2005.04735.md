# [Categorical Stochastic Processes and Likelihood](https://arxiv.org/abs/2005.04735)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be: How can categorical probability theory and neural networks be integrated to build neural network models that have an explicit representation of probabilistic structure?Specifically, the paper aims to develop a categorical framework for stochastic processes that is compatible with neural network function composition. This allows constructing neural network models where layers are stochastic processes, and the probabilistic dependencies between layers are made explicit through categorical composition. The key ideas and contributions towards this research question appear to be:- Defining a co-Kleisli category of stochastic processes to make stochastic process composition compatible with neural net function composition.- Introducing a parameterization construction to make stochastic process composition associative and functorial. - Extending parameterized stochastic processes to statistical models that include a likelihood function.- Using the likelihood function and maximum likelihood estimation to derive a loss function and gradient update rules, defining a backpropagation functor.In summary, the paper develops categorical tools to integrate probabilistic and neural network representations, enabling neural network models that have explicit probabilistic semantics. The central hypothesis is that this integration will lead to more powerful and interpretable neural network models.


## What is the main contribution of this paper?

Based on my reading of the paper abstract, it seems the main contribution is developing a categorical framework to represent stochastic processes and derive likelihood functions in a compositional way. Specifically, the paper:- Defines two extensions of function composition to stochastic process subordination, one based on a co-Kleisli category and one based on Lawvere theory parameterization. These allow composing stochastic processes while maintaining independence assumptions.- Extends stochastic processes to parametric statistical models that include a layer of parameterization. This allows representing models like linear regression categorically. - Shows how the Radon-Nikodym derivative acts as a functor from parametric statistical models to likelihood functions.- Defines subcategories of parametric statistical models where the maximum likelihood procedure can be used to derive an error function and learning algorithm, formalizing a link between probabilistic modeling and gradient-based optimization.Overall, it provides a principled categorical framework to compositionally build statistical models, incorporate uncertainty, and derive learning algorithms in a functorial way. The main novelty seems to be developing categorical stochastic process representations that interface cleanly with likelihood-based learning.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper presents a category-theoretic framework for modeling stochastic processes and deriving likelihood-based optimization objectives, with the goal of capturing aleatoric uncertainty in a composable way.


## How does this paper compare to other research in the same field?

This paper presents a category theoretic perspective on modeling stochastic processes and deriving optimization objectives. Some key similarities and differences to other related work:Similarities:- Like Cho et al. and Culbertson et al., this paper aims to provide a compositional framework for probabilistic modeling. It represents uncertainty using categorical constructions.- The conditional likelihood functions defined in this paper are similar in spirit to the "stochastic maps" used by Fritz. Both aim to represent conditional probability distributions categorically.Differences:  - Cho et al. and Culbertson et al. focus on Bayesian modeling whereas this paper takes a frequentist perspective. The models do not represent uncertainty over parameters.- Fritz represents independence of random variables in terms of tensor products. This paper instead uses an expanded probability space for composition.- Whereas Fritz aims to construct a universal Markov category, this paper defines a category motivated specifically by parameter estimation objectives.Overall, a key contribution of this paper is providing a frequentist categorical framework tailored for likelihood-based optimization objectives. The composition uses an expanded sample space rather than tensor products to ensure conditional independence. The likelihood functions provide a bridge to gradient-based learning algorithms.


## What future research directions do the authors suggest?

The paper suggests several potential future research directions:1. Extending the category of deterministic, frequentist models ($\df$) to handle generative algorithms that model uncertainty in the input vector and Bayesian algorithms that model uncertainty in the parameter vector. Currently, $\df$ contains only discriminative, frequentist models where the parameters and inputs are fixed. Extending to generative and Bayesian models would allow capturing more complex interactions between different sources of uncertainty.2. Relaxing the definition of Marginal Likelihood Factorization Categories, which may currently be overly restrictive. For example, each category is defined by a single marginal error function $er$, making it hard to compose categories with different $er$. Relaxing these restrictions or proving they are necessary could lead to a more general theory. 3. Exploring in more depth the relationships between the categories introduced, such as $\df$, $\peuc$, and $\ceucmeas$, and other existing categorical probabilistic models like the Kleisli category of the Giry monad. There may be further useful connections and translations between these frameworks.4. Expanding the practical demonstration of likelihood-based learning. While a simple prototyping example is shown, applying the proposed techniques to real-world probabilistic modeling tasks could better reveal their utility.5. Investigating other possible applications of the categorical compositional structure defined over stochastic processes, outside of the learning setting explored in the paper. The compositional reasoning enabled could potentially be useful in other domains as well.In summary, the main suggestions are to expand the scope of models covered, generalize the theoretical framework, further explore relationships to other categories, demonstrate real-world application, and investigate other applications of the compositional tools introduced.


## Summarize the paper in one paragraph.

Here is a one paragraph summary of the paper:The paper presents a category-theoretic perspective on modeling randomness and uncertainty in machine learning systems. It defines two extensions of function composition for composing stochastic processes: one based on co-Kleisli categories and one based on Lawvere parameterization. These are related to the category of Markov kernels through a pushforward procedure. The paper then extends stochastic processes to parametric statistical models and defines a way to compose their likelihood functions. It shows how maximum likelihood estimation defines a family of functors from categories of statistical models to the category of supervised learning algorithms. The main contributions are developing compositional frameworks for stochastic processes compatible with subordination and parameterization, relating these to Markov kernels, extending them to statistical models, and using likelihood functions to define optimization objectives and derive learning algorithms categorically.


## Summarize the paper in two paragraphs.

Here is a two paragraph summary of the key points from the paper:The paper proposes a categorical framework for modeling stochastic processes and deriving likelihood functions. It introduces two main categorical constructions for composing stochastic processes in a way that is compatible with both stochastic process subordination and parametric function composition. The first construction is based on the co-Kleisli category of the product comonad, which allows composing stochastic processes over a shared probability space. The second uses Lawvere parameterization to compose processes over independent probability spaces like in the category of Markov kernels. The paper then extends these categories to incorporate parametric statistical models, which have an added layer of parameterization. It shows how the Radon-Nikodym derivative acts as a functor into a category of likelihood functions. Finally, it defines subcategories of parametric statistical models where the likelihood has a specific factored form. Over these subcategories, the maximum likelihood procedure induces an identity-on-objects functor into the category of supervised learning algorithms, providing a principled way to derive optimization objectives and learning algorithms from stochastic process models.


## Summarize the main method used in the paper in one paragraph.

Here is a one paragraph summary of the main method used in the paper:The paper proposes a category-theoretic framework for modeling and composing stochastic processes. It defines two extensions of function composition to stochastic process subordination: one based on a co-Kleisli category and one based on the parameterization of a category with a Lawvere theory. These extensions relate to the category of Markov kernels through a pushforward procedure. The paper extends stochastic processes to parametric statistical models and defines a way to compose their likelihood functions. It shows how the maximum likelihood estimation procedure defines a family of identity-on-objects functors from categories of statistical models to the category of supervised learning algorithms. The key idea is to replace the domain of the Backpropagation functor with a probabilistically motivated category, so that error functions and optimization objectives can be derived through the maximum likelihood framework. This allows capturing the compositional structure of stochastic systems and propagating uncertainty through model training.


## What problem or question is the paper addressing?

Based on my reading of the abstract, introduction and conclusions, this paper seems to be addressing the following main problems/questions:1) How to compose stochastic processes in a way that is compatible with both subordination (where one stochastic process acts as the time scale for another) and parametric function composition (as used in neural networks). 2) How to represent the aleatoric/inherent uncertainty in physical systems compositionally, in contrast to standard neural network models which can only capture epistemic/data-driven uncertainty.3) How to extend categorical models of optimization like backpropagation to probabilistic models, defining composition of likelihood functions.4) How learning algorithms like maximum likelihood estimation can be framed categorically.Specifically, the paper introduces two categorical constructions for composing stochastic processes - one based on co-Kleisli categories and one based on Lawvere theories. It shows how these relate to the standard category of stochastic processes. It then extends these categories to "parametric statistical models" which add a layer of parametric inputs, allowing the definition of likelihood functions. The maximum likelihood estimation procedure is used to define error/loss functions and optimization objectives.Finally, the maximum likelihood estimators are used to construct a family of "backpropagation" functors that map categories of statistical models to categories of learning algorithms, capturing the process of optimizing parameters to maximize likelihood.Overall, the paper provides a detailed categorical perspective on probabilistic modeling and likelihood-based learning, connecting ideas from probability theory, category theory, and machine learning. The constructions allow representing and propagating uncertainty in structured and compositional ways.
