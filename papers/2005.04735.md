# [Categorical Stochastic Processes and Likelihood](https://arxiv.org/abs/2005.04735)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be: How can categorical probability theory and neural networks be integrated to build neural network models that have an explicit representation of probabilistic structure?Specifically, the paper aims to develop a categorical framework for stochastic processes that is compatible with neural network function composition. This allows constructing neural network models where layers are stochastic processes, and the probabilistic dependencies between layers are made explicit through categorical composition. The key ideas and contributions towards this research question appear to be:- Defining a co-Kleisli category of stochastic processes to make stochastic process composition compatible with neural net function composition.- Introducing a parameterization construction to make stochastic process composition associative and functorial. - Extending parameterized stochastic processes to statistical models that include a likelihood function.- Using the likelihood function and maximum likelihood estimation to derive a loss function and gradient update rules, defining a backpropagation functor.In summary, the paper develops categorical tools to integrate probabilistic and neural network representations, enabling neural network models that have explicit probabilistic semantics. The central hypothesis is that this integration will lead to more powerful and interpretable neural network models.


## What is the main contribution of this paper?

Based on my reading of the paper abstract, it seems the main contribution is developing a categorical framework to represent stochastic processes and derive likelihood functions in a compositional way. Specifically, the paper:- Defines two extensions of function composition to stochastic process subordination, one based on a co-Kleisli category and one based on Lawvere theory parameterization. These allow composing stochastic processes while maintaining independence assumptions.- Extends stochastic processes to parametric statistical models that include a layer of parameterization. This allows representing models like linear regression categorically. - Shows how the Radon-Nikodym derivative acts as a functor from parametric statistical models to likelihood functions.- Defines subcategories of parametric statistical models where the maximum likelihood procedure can be used to derive an error function and learning algorithm, formalizing a link between probabilistic modeling and gradient-based optimization.Overall, it provides a principled categorical framework to compositionally build statistical models, incorporate uncertainty, and derive learning algorithms in a functorial way. The main novelty seems to be developing categorical stochastic process representations that interface cleanly with likelihood-based learning.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper presents a category-theoretic framework for modeling stochastic processes and deriving likelihood-based optimization objectives, with the goal of capturing aleatoric uncertainty in a composable way.
