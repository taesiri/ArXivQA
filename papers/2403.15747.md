# [CodeShell Technical Report](https://arxiv.org/abs/2403.15747)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Developing high-performance code language models requires massive datasets, yet collecting quality data is challenging. Existing models use vast low-quality code data, risking the generation of erroneous code. 

Solution - CodeShell Model:  
- Proposes CodeShell, a 7B parameter code foundation model, incorporating architectural innovations like Grouped-Query Attention and Rotary Positional Embedding into GPT-2 for stronger structural modeling.  

- Constructs pipeline to filter 100B high-quality tokens from GitHub, Stack Overflow etc. using techniques like perplexity filtering, rule-based analysis and learning-based scoring.

- Pretrains CodeShell on this 100B token data over 5 epochs. Experiments smaller 1B parameter version, showing far better performance with filtered data.


Contributions:
- Releases CodeShell-7B model showing competitive performance to models trained on much larger 250B+ datasets across tasks in Python, Java, JavaScript etc.

- Demonstrates efficiency of data filtering pipeline requiring only 500B tokens to exceed bigger models, compared to the usual trillion token requirement. Reduces costs.

- Shows increasing context length from 2K to 8K tokens enhances capacity for longer code with no performance drop on shorter tasks. Better handles complexity.

In summary, the paper presents CodeShell, an efficient 7B parameter code foundation model trained on a novel filtered 100B token dataset. It demonstrates competitive multilingual performance despite 5x smaller data size, highlighting the value of quality data filtering.
