# [SqueezeAttention: 2D Management of KV-Cache in LLM Inference via   Layer-wise Optimal Budget](https://arxiv.org/abs/2404.04793)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Large language models (LLMs) have massive inference costs, dominiated by the key-value (KV) cache memory which grows linearly with number of layers, context length and batch size. 
- Existing works optimize the KV cache only from the context length perspective by dropping unimportant tokens. The optimization opportunities from the layer dimension is unexplored.

Key Idea: 
- Layers have different importance levels regarding the output embedding, which can be quantified by the cosine similarity of embeddings before and after the self-attention block.
- Layers can be grouped by importance, allowing more budgets for important groups and less for unimportant groups.

Proposed Solution - SqueezeAttention:
- A 2D KV cache compressionframework that jointly optimizes cache allocation across both context and layer dimensions.  
- Measures layer importance on-the-fly using cosine similarity of embeddings.
- Clusters layers into groups by importance, assigns higher cache budgets to more important groups.
- Works orthogonally with existing 1D cache compression methods.

Key Results:
- 30-70% cache memory reduction compared to competitive baselines while maintaining accuracy. 
- Up to 2.2x inference throughput improvement over full cache.
- Generalizable to diverse LM architectures and dataset complexities.

Main Contributions:
- First work to optimize KV cache management from a layer perspective.
- Simple but effective metric to quantify and leverage layer importance.
- Demonstrated optimization opportunities orthogonal to existing sequence-based methods.

In summary, the paper proposes a novel 2D perspective to optimize KV cache for LLM inference, achieving significant memory and throughput gains. The layer-importance awareness allows more targeted cache allocation across both dimensions.
