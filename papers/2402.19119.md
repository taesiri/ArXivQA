# [VIXEN: Visual Text Comparison Network for Image Difference Captioning](https://arxiv.org/abs/2402.19119)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Image manipulation is often used to create fake news and spread misinformation. Tools are needed to help users quickly assess image provenance and detect manipulations. 
- Existing image difference captioning methods have limitations in terms of training data volume and diversity, as well as generalization to diverse image contents and manipulation types.

Proposed Solution:
- The paper proposes VIXEN, a novel cross-modal architecture to generate textual summaries explaining the differences between two images. 
- VIXEN uses a 2-branch design, encoding image pairs with CLIP into an embedding space that is fed into the GPT-J language model to generate captions.
- VIXEN is trained on synthetic image pairs from the InstructPix2Pix dataset, which uses AI-guided prompting to create diverse edits. The captions are augmented with edit summaries from GPT-3.

Main Contributions:
- Cross-modal image differencing method to produce manipulation-aware image captions 
- Synthetic training framework using prompt-based image editing datasets with augmented edit summaries
- State-of-the-art performance on InstructPix2Pix and good generalization to other datasets
- Introduces augmented version of InstructPix2Pix with GPT-3 generated difference summaries

In summary, the paper presents VIXEN, a novel deep learning method to automatically generate natural language descriptions of differences between two images. It is trained on a large-scale synthetic dataset of diverse image manipulations and shows improved performance over prior work. The approach could help counter misinformation and fake imagery.
