# [Distributional Dataset Distillation with Subtask Decomposition](https://arxiv.org/abs/2403.00999)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper "Distributional Dataset Distillation with Subtask Decomposition":

Problem:
- Existing prototype-based dataset distillation methods incur high storage costs and long training times due to storing soft labels, augmentation parameters, etc. in addition to the prototypes. This is not captured well by the commonly used Images Per Class (IPC) metric.
- There is a need for more comprehensive evaluation metrics that account for total storage cost and downstream training cost.

Proposed Solution: 
- Propose Distributional Dataset Distillation (D3) which represents the dataset as a latent distribution paired with a decoder, allowing more compact storage and control over the tradeoff between storage, training cost and accuracy.
- Impose a Gaussian latent structure with learnable per-class mean and variance parameters. Use multiple latent codes per class to scale up distillation quality. 
- Propose a federated distillation scheme to decompose the problem into distilling subsets of classes in parallel using subtask experts, which are then aggregated. Show this achieves good generalization to the full task.

Main Contributions:
- Propose more comprehensive metrics to evaluate dataset distillation methods on storage cost, training cost and accuracy. Show state-of-the-art methods perform worse on these.
- First framework to distill datasets into latent distributions rather than explicit prototypes. Allows more efficient compression and sampling.
- Simple yet effective federated distillation scheme to scale up to large datasets like ImageNet by decomposing into subtasks.
- Achieve state-of-the-art accuracy on TinyImageNet and ImageNet under small memory budgets, significantly outperforming prior prototype-based methods. 

In summary, the paper addresses limitations of existing dataset distillation methods by representing the dataset as a distribution paired with a decoder, and proposes a federated scheme to distill large datasets efficiently while achieving better accuracy under small memory budgets. The new evaluation metrics also highlight deficiencies of prior work.
