# [Data-driven Prior Learning for Bayesian Optimisation](https://arxiv.org/abs/2311.14653)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a high-quality paragraph summarizing the key points of the paper:

This paper presents an improved method for transfer learning in Bayesian optimization called Prior Learning for Bayesian Optimization (PLeBO). The key idea is to learn priors over the hyperparameters of the Gaussian process surrogate model from previous tuning tasks, instead of assuming the optima are shared between tasks. This "prior transfer" approach only requires that the optimization landscapes have similar shapes. PLeBO uses Markov chain Monte Carlo in a preprocessing step to learn the hyperparameter priors from tuning data. It then samples new candidates from this distribution to help optimize new tasks. Experiments on synthetic and real-world air pollution data show that PLeBO and other prior transfer methods find good solutions faster than no transfer or direct transfer approaches. The inferred priors are shown to reasonably approximate the true values. While computationally more expensive, PLeBO outperforms standard Bayesian optimization and direct transfer methods when few evaluations are available. This demonstrates the promise of learned hyperparameter priors for optimization problems where the tasks have related but not identical optima.


## Summarize the paper in one sentence.

 Here is a one sentence summary of the paper:

The paper proposes a Bayesian optimization method called Prior Learning for Bayesian Optimization (PLeBO) that transfers knowledge between tasks by learning priors over the Gaussian process hyperparameters from previous tasks and using them to inform the surrogate model for new tasks, making fewer function evaluations necessary.


## What is the main contribution of this paper?

 The main contribution of this paper is the development and analysis of a new Bayesian optimization transfer learning method called Prior Learning for Bayesian Optimization (PLeBO). Specifically:

- PLeBO learns priors over the Gaussian process hyperparameters from previous tuning tasks. This allows it to better approximate the optimization landscape when few evaluations are available on a new test task.

- It relies on the weaker assumption that the shapes of the optimization landscapes are similar rather than the optima being similar. This makes it applicable to a wider range of problems compared to methods relying on shared optima.

- The authors introduce a new implementation based on NumPyro which is more stable and modular than their previous version.

- They analyze the quality of the inferred priors and show that PLeBO outperforms various baseline methods with no transfer or direct transfer on synthetic and real-world air pollution tasks. This demonstrates the advantage of prior transfer when the tasks have similar shapes but different optima.

In summary, the main contribution is the development and validation of the PLeBO method for transfer learning in Bayesian optimization under the assumption of similar optimization landscape shapes rather than similar optima.


## What are the keywords or key terms associated with this paper?

 Based on scanning the paper, some of the key terms and keywords associated with this paper include:

- Bayesian optimisation
- Transfer learning
- Priors
- Markov chain Monte Carlo
- Gaussian processes
- Surrogate models
- Acquisition functions
- Expected improvement
- Air pollution monitoring

The paper presents a method called "Prior Learning for Bayesian Optimisation" (PLeBO) for transfer learning in Bayesian optimisation. It focuses on learning priors on the hyperparameters of Gaussian process surrogate models in order to better approximate the objective function and find good solutions faster, especially with limited function evaluations. The method is demonstrated on an air pollution monitoring application in addition to synthetic benchmarks.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in this paper:

1. The paper proposes a new transfer learning method called PLeBO that learns priors over the hyperparameters of Gaussian processes from previous tuning tasks. How does the hierarchical model used by PLeBO differ from other hierarchical Bayesian optimization methods like MTBO?

2. The paper argues that PLeBO makes a weaker assumption than direct transfer approaches, only requiring similar landscape shapes rather than similar optima. What are some real-world scenarios where this assumption would hold but direct transfer assumptions would not?

3. The preprocessing step uses MCMC to approximate the posterior distribution over the hyperparameters. Why is the posterior useful here rather than just point estimates like maximum likelihood? How sensitive are the results to the quality of the posterior approximation?

4. The candidate samples are used to calculate a weighted acquisition function during optimization. Why is it beneficial to use multiple candidates rather than just the mean or maximum a posteriori hyperparameters from the tuning tasks?

5. The implementation uses importance weighting to fit the candidate hyperparameters to new observations. What are the advantages of this online updating approach compared to fixing the hyperparameters after the preprocessing?

6. The results show that PLeBO outperforms EI by a small margin on the test problems. What modifications could be made to PLeBO to better exploit the correct hyperparameter values to get larger improvements?

7. How does the computational cost of PLeBO, particularly in the optimization step, compare to standard Bayesian optimization? In what scenarios would the added cost be justified?

8. The air pollution tasks use log-transformed, standardized data as inputs to the Gaussian processes. How might this preprocessing affect the ability to transfer useful priors between tasks?

9. For the selection benchmark, why does the shared hyperparameters approach outperform PLeBO when the tuning and test tasks are less similar? How could PLeBO be made more robust?

10. The method currently uses 2 hyperparameters - lengthscales and signal variance. How could PLeBO be extended to handle a larger number of hyperparameters, where posterior approximation becomes more difficult?
