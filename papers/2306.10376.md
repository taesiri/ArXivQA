# CLARA: Classifying and Disambiguating User Commands for Reliable   Interactive Robotic Agents

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be: How can we enhance the reliability of interactive robot agents that utilize large language models (LLMs) by properly handling ambiguous or infeasible user commands?Specifically, the paper proposes an approach called CLARA (CLAssifying and disAambiguating user commands for reliable Robotic Agents) that aims to:1. Classify user commands as clear, ambiguous, or infeasible based on estimating uncertainty from the LLM.2. Disambiguate ambiguous commands by interacting with the user via question generation. 3. Incorporate situational awareness when classifying commands as ambiguous vs. infeasible.The key hypothesis appears to be that explicitly modeling and handling uncertainty in LLM predictions for robot agents will allow for more reliable human-robot interaction by avoiding undesired actions from ambiguous or infeasible commands. The proposed CLARA method incorporates uncertainty estimation, situational awareness, and disambiguation to address this problem.In summary, the central research question is how to make interactive robots powered by LLMs more reliable by properly interpreting uncertain user commands. The hypothesis is that modeling uncertainty and disambiguating commands will enhance reliability. CLARA is presented as a method to achieve this.


## What is the main contribution of this paper?

Based on my reading, the main contributions of this paper appear to be:1. Proposing a method to classify user commands as clear, ambiguous, or infeasible in interactive robotic agents that use large language models (LLMs). This involves estimating uncertainty in the LLM to recognize ambiguous/infeasible commands, and using situational awareness to further classify uncertain commands.2. Introducing an approach to disambiguate ambiguous commands by interacting with users via question generation using the LLM in a zero-shot manner. 3. Presenting a dataset called "Situational Awareness for Goal Classification in Robotic Tasks (SaGC)" which contains pairs of commands, scene descriptions, and uncertainty labels to evaluate situational awareness.4. Validating the proposed methods on the SaGC dataset, in pick-and-place simulations, and through real-world human-robot interaction experiments for handing over objects.The key ideas seem to be using uncertainty estimation and situational awareness with LLMs to classify unclear user commands, and leveraging the LLMs in a zero-shot way for disambiguation via natural interaction. The dataset and experiments in both simulation and real-world settings demonstrate the viability of the approach.In summary, the main contribution appears to be developing and evaluating methods to handle uncertain/ambiguous commands for more reliable human-robot interaction using LLMs. The proposed techniques aim to classify commands and interactively disambiguate them when needed.
