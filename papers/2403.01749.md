# [Differentially Private Synthetic Data via Foundation Model APIs 2: Text](https://arxiv.org/abs/2403.01749)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key aspects of the paper:

The paper proposes a new method called Augmented Private Evolution (Aug-PE) for generating differentially private synthetic text data. The goal is to create privacy-preserving text datasets that can be safely shared and used for training machine learning models, without revealing sensitive information about individuals in the original private data. 

The key problem is that current state-of-the-art methods require fine-tuning large language models on private data using differential privacy mechanisms. However, this approach is not feasible for proprietary models like Claude and for the latest models like GPT-4, which are only accessible via APIs. Even for open-source models, differentially private fine-tuning demands considerable compute resources.

To address this, Aug-PE leverages foundation model APIs to generate synthetic text without any model training. It builds on the Private Evolution (PE) framework, which iteratively improves random samples by selecting similar synthetic samples guided by private data and generating more samples. Aug-PE adapts PE specifically for text with new generation and selection techniques.

Key technical contributions include:
(1) Customized APIs for diverse text generation and variations. 
(2) Adaptive text lengths matching real data distribution.
(3) Enhanced selection for high-quality and diverse samples.

Experiments on multiple datasets and models demonstrate Aug-PE generates high utility DP synthetic text, achieving comparable performance to state-of-the-art DP fine-tuning baselines when using the same base model. It also effectively leverages more powerful models like GPT-3.5 where fine-tuning is infeasible.

In summary, the key impact is enabling high-quality DP text synthesis without model training, facilitating privacy-preserving applications of proprietary and large foundation models.
