# [Whispers that Shake Foundations: Analyzing and Mitigating False Premise   Hallucinations in Large Language Models](https://arxiv.org/abs/2402.19103)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Large language models (LLMs) suffer from the issue of hallucination, generating plausible but factually incorrect texts. One important type is false premise hallucination - when LLMs generate hallucinated texts when asked questions containing false premises.  
- False premise questions are common, with users likely to ask them when interacting with LLMs. But LLMs tend to respond directly without verifying plausibility, even when factual knowledge is present in parameters. 
- Analyzing false premise hallucination is challenging as models still hallucinate despite having pertinent factual knowledge stored. 

Methodology:
- The paper conducts comprehensive analysis from model surface to internals to reveal mechanism behind false premise hallucination.
- Proposes automatic pipeline to construct datasets containing false premise questions to evaluate hallucination. Authors create two datasets based on this.
- Observes models exhibit more uncertainty when generating hallucinated texts, validating hypothesis.
- Analysis of information flow shows factual knowledge about subject is disturbed in shallow layers around false object in question. 
- Further analysis into self-attention layers reveals presence of "false premise heads" - small set of heads disturbing knowledge extraction process and contributing to hallucinations.

Proposed Solution - FAITH:
- Method to mitigate false premise hallucination by constraining identified false premise heads during inference.
- Localizes false premise heads for false premise questions which are then constrained around false objects in questions.

Main Contributions:
- Comprehensive analysis revealing mechanism behind false premise hallucination using automatically constructed evaluation datasets.
- Identification of false premise attention heads disturbing knowledge extraction.
- FAITH method leveraging analysis to mitigate false premise hallucination by constraining small subset of heads.
- Extensive experiments show accuracy increases of ~20% by constraining only ~1% heads.
