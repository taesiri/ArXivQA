# [Making Vision Transformers Efficient from A Token Sparsification View](https://arxiv.org/abs/2303.08685)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central research question this paper aims to address is: How can we design an efficient vision transformer (ViT) architecture that achieves high accuracy while reducing computation cost?Specifically, the paper proposes a novel method called Semantic Token ViT (STViT) to construct efficient ViTs by reducing the number of tokens processed in the self-attention layers. The key ideas are:- ViTs do not need to maintain a full-size feature map like CNNs. A few discrete tokens with high-level semantic representations are sufficient to achieve good performance.- The redundant image tokens can be replaced by a small number of "semantic tokens" that represent cluster centers and capture semantic information. - The semantic tokens are generated by applying standard self-attention layers to cluster and aggregate image tokens. This allows adapting STViT to both global and local ViTs.- A spatial initialization and incorporation of global context help the semantic tokens capture both fine-grained local semantics and high-level global semantics.- For downstream tasks, a recovery module can be added to periodically restore the full spatial resolution while still using semantic tokens in intermediate layers.The central hypothesis is that maintaining a full set of image tokens is unnecessary for ViTs, and semantic tokens can effectively represent the image while greatly reducing computation cost. Experiments on image classification, video recognition, detection and segmentation validate the effectiveness and efficiency of the proposed STViT method.In summary, the key research question is how to design a token sparsification algorithm to build efficient ViT models, and the central hypothesis is that a small number of semantic tokens can replace redundant image tokens without sacrificing accuracy.
