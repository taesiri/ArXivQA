# [Making Vision Transformers Efficient from A Token Sparsification View](https://arxiv.org/abs/2303.08685)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central research question this paper aims to address is: How can we design an efficient vision transformer (ViT) architecture that achieves high accuracy while reducing computation cost?Specifically, the paper proposes a novel method called Semantic Token ViT (STViT) to construct efficient ViTs by reducing the number of tokens processed in the self-attention layers. The key ideas are:- ViTs do not need to maintain a full-size feature map like CNNs. A few discrete tokens with high-level semantic representations are sufficient to achieve good performance.- The redundant image tokens can be replaced by a small number of "semantic tokens" that represent cluster centers and capture semantic information. - The semantic tokens are generated by applying standard self-attention layers to cluster and aggregate image tokens. This allows adapting STViT to both global and local ViTs.- A spatial initialization and incorporation of global context help the semantic tokens capture both fine-grained local semantics and high-level global semantics.- For downstream tasks, a recovery module can be added to periodically restore the full spatial resolution while still using semantic tokens in intermediate layers.The central hypothesis is that maintaining a full set of image tokens is unnecessary for ViTs, and semantic tokens can effectively represent the image while greatly reducing computation cost. Experiments on image classification, video recognition, detection and segmentation validate the effectiveness and efficiency of the proposed STViT method.In summary, the key research question is how to design a token sparsification algorithm to build efficient ViT models, and the central hypothesis is that a small number of semantic tokens can replace redundant image tokens without sacrificing accuracy.


## What is the main contribution of this paper?

The main contribution of this paper is proposing a novel method called Semantic Token ViT (STViT) to construct efficient and high-performance vision transformers (ViTs) by reducing the number of tokens. The key ideas are:- Replace the majority of image/video tokens with a small number of "semantic tokens" that represent cluster centers and capture high-level semantic information. This significantly reduces computational cost. - Use standard self-attention layers to generate the semantic tokens through clustering, without introducing any additional networks.- Apply spatial and global initialization for the semantic tokens to incorporate both local and global semantics.- Extend the method to both global (e.g. DeiT) and local (e.g. Swin) vision transformers for image classification.- Design a STViT-Recover model to restore the spatial details from semantic tokens, enabling the use of STViT for downstream tasks like object detection and segmentation. - Demonstrate state-of-the-art efficiency improvements on various ViT models with negligible or sometimes even improved accuracy, for both image and video classification.- Show strong performance of STViT-Recover on downstream tasks compared to the original ViT models, with over 30% FLOPs reduction.In summary, the key contribution is presenting a simple yet effective general framework to sparsify tokens in vision transformers to improve efficiency, without sacrificing performance. The method can be broadly applied to both global and local ViTs for image, video and downstream tasks.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper proposes Semantic Token Vision Transformer (STViT), a new efficient vision transformer architecture that uses a small number of semantic tokens generated via self-attention clustering to replace the majority of image tokens while maintaining accuracy, enabling significant reductions in computational complexity for both global and local vision transformers as well as their use in downstream tasks like object detection.


## How does this paper compare to other research in the same field?

Here are my thoughts on how this paper compares to other research in the field of vision transformers:- The main focus of this paper is on making vision transformers more efficient by reducing the number of tokens. This is an important line of work, as the quadratic complexity of self-attention in transformers is a major limitation. - The paper proposes a novel method called Semantic Token Vision Transformer (STViT) to generate a small number of "semantic tokens" that capture high-level semantic information about the image. This allows replacing the large number of image tokens with a much smaller set of semantic tokens.- Other recent works like TokenLearner, DynamicViT, EVo-ViT etc. have also aimed to reduce tokens in vision transformers. However, this paper claims better performance in terms of preserving accuracy while reducing FLOPs. The core ideas around clustering tokens and using self-attention for this seem quite effective.- A key distinction claimed is that their method works well for both global and local vision transformers like Swin Transformers. Most prior works have focused only on global transformers like ViT. Showing strong results on Swin is impressive.- The idea of a recovery module to restore spatial resolution is novel, allowing the use of STViT as a backbone for downstream tasks like detection. Other papers on token pruning have not explored this.- Thorough experiments on image classification, video, detection and segmentation validate the efficacy of their approach and analysis provides useful insights.In summary, this paper pushes state-of-the-art in efficient vision transformers by careful token reduction, with innovations like semantic token generation, application to local transformers and recovery modules. The thorough empirical study is a strength. If the claims hold up, this could become an influential work in this rapidly evolving field.


## What future research directions do the authors suggest?

Based on my reading of the paper, some key future research directions the authors suggest are:- Improving efficiency of vision transformers through additional techniques like distillation, quantization, and lightweight architectures. The authors mention that their method provides a strong and simple baseline architecture, which can serve as a starting point for exploring these other directions to further enhance efficiency.- Extending the semantic token approach to other vision transformer models and tasks beyond classification. The authors demonstrate it on Swin Transformers for video recognition and object detection/segmentation, but suggest exploring its effectiveness in other models (e.g. convnext, vit, etc.) and tasks like detection, segmentation, video analysis. - Developing better positional encoding strategies for semantic tokens. The authors find standard positional encodings don't help much for semantic tokens, so suggest exploring improved encoding methods tailored for semantic tokens.- Exploring the cluster assumptions and properties of semantic tokens. The authors provide some analysis justifying semantic tokens as cluster centers, but suggest more investigation into the clustering behaviors and semantic representations learned.- Applying semantic tokens in other modalities like natural language processing. The authors suggest the concept of clustering tokens into semantic representations may have broader applications beyond vision.- Developing more advanced recovery modules and strategies. The authors provide a basic recovery module to enable downstream tasks, but suggest exploring more sophisticated approaches to restore spatial details.In summary, the main directions are improving efficiency further, extending to more models and tasks, better understanding clustering properties, and developing more advanced recovery mechanisms for detailed spatial information. The semantic token concept seems promising, but still requires more research to fully explore and improve it.


## Summarize the paper in one paragraph.

Here is a one paragraph summary of the key points from the paper:This paper proposes Semantic Token Vision Transformer (STViT), a novel and efficient vision transformer architecture that uses a small number of semantic tokens to replace most of the original image tokens. STViT utilizes the clustering properties of self-attention to generate semantic tokens representing cluster centers. These tokens incorporate both local and global semantic information and can effectively represent the image using much fewer tokens, significantly reducing computational complexity. The semantic tokens are generated by a Semantic Token Generation Module (STGM) that pools the image tokens and recovers cluster centers through attention layers. STViT achieves strong performance on image classification benchmarks, reducing computation by over 50% on DeiT models and around 20% on Swin Transformer models with minimal accuracy drop. The authors further propose STViT-R which adds a recovery module to restore spatial details, enabling use for downstream tasks like detection and segmentation. Experiments show STViT-R can serve as an efficient backbone, reducing FLOPs by over 30% while maintaining strong performance. Key advantages of STViT include its simplicity, effectiveness on both global and local transformers, and ability to work for both image classification and downstream tasks.
