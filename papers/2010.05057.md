# [Fairness-aware Agnostic Federated Learning](https://arxiv.org/abs/2010.05057)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is: How to achieve fairness in federated learning when the testing data distribution is unknown or different from the training data distribution?The key points are:- In federated learning, the training data is distributed across multiple devices/clients. This can lead to differences in data distributions between clients (non-IID data).- Most prior work on federated learning focuses on privacy protection and communication efficiency, but achieving fairness is less explored.- Fairness constraints on the centralized model using just the training data cannot guarantee fairness on unknown testing data. - The paper proposes a fairness-aware agnostic federated learning framework (AgnosticFair) to deal with unknown testing distributions. The main hypothesis is that by using kernel reweighing functions in the loss function and fairness constraints, the model trained with AgnosticFair can achieve high accuracy and fairness guarantees even when the testing distribution is unknown.In summary, the key research question is how to achieve fairness in federated learning under unknown testing data distributions. The proposed approach is AgnosticFair which uses kernel reweighing to make the model robust to unknown distributions.


## What is the main contribution of this paper?

The main contribution of this paper is proposing a framework for fairness-aware federated learning that can handle unknown testing data distributions. Specifically:- It proposes an agnostic federated learning approach that uses kernel reweighting functions to make the loss function and fairness constraints robust to unknown shifts between training and testing distributions. - It formulates the problem as a two-player adversarial minimax game between a learner that minimizes the loss and an adversary that tries to maximize the loss by generating the worst-case testing distribution.- It develops an efficient optimization method where clients optimize model parameters and the server optimizes reweighting coefficients. This allows training a global model with fairness guarantees without exposing raw client data.- It conducts experiments on real datasets that demonstrate the approach can achieve higher accuracy and fairness under distribution shift compared to standard federated learning and other baselines.In summary, the key novelty is developing a federated learning framework that can provide fairness guarantees even when the testing distribution is unknown or different from the client training distributions. This is an important contribution as prior federated learning research has not addressed fairness under such distribution shift.
