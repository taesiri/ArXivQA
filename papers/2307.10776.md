# [Urban Radiance Field Representation with Deformable Neural Mesh   Primitives](https://arxiv.org/abs/2307.10776)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central hypothesis of this paper is that a neural radiance field representation based on deformable neural mesh primitives can achieve high quality view synthesis for large outdoor urban scenes in an efficient manner. Some key points:- The paper proposes representing complex outdoor scenes using a collection of deformable neural mesh primitives (DNMPs). Each DNMP captures the geometry and radiance of a local region.- The geometry of each DNMP is controlled by a low-dimensional latent code, which is optimized to deform the mesh and capture the local 3D structure. This allows efficiently optimizing the geometry.- The radiance is captured by features associated with each mesh vertex, which can be rendered efficiently via rasterization. - The scene is hierarchically represented with DNMPs at multiple scales to handle noise and missing regions in the input 3D reconstruction.- Experiments show the method achieves state-of-the-art view synthesis results on urban datasets much more efficiently than vanilla NeRF and other baselines.So in summary, the central hypothesis is that the proposed DNMP representation can achieve high quality radiance field modeling and rendering for large outdoor scenes in an efficient manner compared to prior work. The key ideas are using a primitive-based representation, learning geometry via mesh deformation, and leveraging rasterization for efficiency.


## What is the main contribution of this paper?

Based on my reading, the main contributions of this paper are:1. It proposes a novel neural scene representation called Deformable Neural Mesh Primitive (DNMP) for efficient rendering of large-scale scenes like urban environments. 2. DNMP combines the efficiency of classic mesh representation with the powerful neural representation capability. Each DNMP models the geometry and radiance information of a local 3D region.3. To represent an entire scene, it voxelizes the scene based on 3D reconstruction and assigns a DNMP to each voxel to parameterize its local geometry and appearance. 4. To enable robust geometry optimization, the mesh vertex positions of each DNMP are decoded from a compact latent space learned by an autoencoder on various local 3D structures.5. It proposes a hierarchical DNMP representation to handle incomplete reconstructions. The scene is voxelized at multiple resolutions and blended together during rendering.6. For rendering, rasterization is used to interpolate vertex features and sample surface points. An MLP predicts radiance and opacity per sample which are blended to produce pixel colors.7. Experiments show the method achieves state-of-the-art novel view synthesis on urban datasets with faster speed and lower memory than NeRF variants. Scene editing is also enabled.In summary, the key innovation is the DNMP representation that combines meshes and neural features for efficient large-scale scene modeling and rendering. The compact latent shape space and hierarchical representation also improve robustness.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key point from the paper:The paper proposes a novel representation of urban radiance fields using deformable neural mesh primitives, which enables efficient and high-quality rendering of large-scale outdoor scenes.
