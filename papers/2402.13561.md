# [Cognitive Visual-Language Mapper: Advancing Multimodal Comprehension   with Enhanced Visual Knowledge Alignment](https://arxiv.org/abs/2402.13561)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Large multimodal models (LMMs) can perform well on visual question answering (VQA) but still struggle with knowledge-based VQA which requires external knowledge to answer questions about images. 
- Current LMMs focus on aligning image-text descriptions and overlook aligning images with relevant knowledge (visual-knowledge alignment). Visual knowledge is important for analyzing, inferring and interpreting information from images.

Proposed Solution: 
- Present a Cognitive Visual-Language Mapper (CVLM) with two components to improve LMMs:
   1) Visual Knowledge Aligner (VKA): Generates relevant knowledge for images and projects it into the language space of LMMs. It is pretrained on image-knowledge pairs and fine-tuned using query tokens and a linear layer.
   2) Fine-Grained Knowledge Adapter (FKA): Distills and injects fine-grained visual knowledge from image regions into each layer of the LMM.
- VKA connects images to knowledge. FKA provides detailed knowledge to improve reasoning abilities of LMMs.

Main Contributions:
- Explore visual-language knowledge alignment in LMMs by associating images with knowledge (VKA) and distilling detailed knowledge (FKA). 
- VKA is pretrained on image-knowledge pairs from Wikipedia and fine-tuned to project knowledge into LMMs.
- FKA adapts and injects fine-grained knowledge from image regions into the LMM.
- Experiments show CVLM outperforms previous LMMs on knowledge-based VQA datasets by 4-5%. Ablations validate effectiveness of VKA and FKA.

In summary, the paper introduces techniques to align images with visual knowledge and inject it into LMMs to enhance performance on knowledge-based VQA tasks. The VKA and FKA components are shown to be effective through quantitative results and ablations.
