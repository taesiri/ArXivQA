# [SpiRit-LM: Interleaved Spoken and Written Language Model](https://arxiv.org/abs/2402.05755)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Text only large language models (LLMs) like GPT-3 and LLaMA have shown strong performance in natural language processing tasks through pretraining on massive amounts of text data. However, they lack the ability to model speech data and generate expressive speech. 

- On the other hand, recent speech LMs trained only on speech fail to reach the level of semantic understanding of text LMs.

- Existing pipelines that convert speech to text, generate using a text LM, and synthesize using TTS, also fail to properly model speech expressivity.

Proposed Solution: 
- Introduce Spirit-LM (SpirLM), a single multimodal foundation model that can generate both text and expressive speech through continuous pretraining of LLaMA-2 on text, speech, and aligned speech+text data.

- Text is encoded with BPE tokens. Speech is encoded with semantic units from HuBERT (for Spirit-LM-Base), combined with pitch and style units that model expressivity (for Spirit-LM-Expressive).

- Key contribution is a word-level "interleaving" scheme to align and mix speech and text sequences during training. Enables cross-modal inference.

Main Results:
- Strong performance on speech & text comprehension tasks, with ability for few-shot learning on downstream tasks like ASR, TTS, speech classification through prompting.

- Introduce Speech-Text Sentiment Preservation (STSP) benchmark to evaluate expressive abilities. Spirit-LM-Expressive matches human performance in preserving sentiment across modalities.

- First single model capable of generating speech or text while preserving meaning and expressivity of the input, enabling applications like controllable dialog systems.

Limitations:
- Speech generation quality not on par with text generation. Architectural and scaling improvements needed.

- Limited evaluation on non-English languages.

- Lacks fine-tuning to ensure model alignments with user preferences and safety.

Broader Impact:
- Enables new multimodal experiences but shares risks like harmful applications seen in text LLMs. Mitigation strategies are proposed.

- Provides a foundation for future work on aligning models to user needs through fine-tuning with preference learning.
