# [Emergent Equivariance in Deep Ensembles](https://arxiv.org/abs/2403.03103)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem: 
- Equivariance is important for many machine learning problems to encode symmetries and improve model performance, but developing manifestly equivariant architectures can be difficult. Data augmentation is an alternative but typically equivariance only emerges late in training and on the data manifold.

- Deep ensembles provide uncertainty estimates and are widely used, but their relation to equivariance has not been explored.

Solution:
- The paper proves theoretically using neural tangent kernel theory that deep ensembles become exactly equivariant under data augmentation in the infinite width limit. 

- This equivariance emerges for the ensemble predictions but not necessarily for individual members and holds throughout training, off the data manifold, without constraints on the architecture.

Main Contributions:
- Paper proves that infinitely wide deep ensembles are secretly equivariant models due to data augmentation, with equivariance emerging from their ensemble prediction.

- Equivariance emerges early in training, off-manifold, for any architecture, with no need for individual members to be equivariant.

- Provides analysis of deviations from exact equivariance due to finite width, ensemble size, continuous groups. 

- Validates emergent equivariance empirically on an Ising model, image classification, and medical histology data.

In summary, the key insight is that deep ensembles can act as equivariant models via their ensemble prediction as a consequence of data augmentation and without explicitly enforcing equivariance. The practical benefit is an easy way to improve model uncertainty and equivariance simultaneously.
