# [Pre-trained Language Model based Ranking in Baidu Search](https://arxiv.org/abs/2105.11108)

## What is the central research question or hypothesis that this paper addresses?

 The central research question this paper addresses is how to effectively deploy pre-trained language models (PLMs) like ERNIE for ranking in a large-scale online search engine system. Specifically, the paper focuses on overcoming three key challenges:1. The inefficiency of PLMs for ranking long text documents due to the quadratic computation complexity. 2. The discrepancy between existing pre-training objectives and the demands of relevance modeling in ranking.3. The need for compatibility of the fine-tuned PLM ranker with other components in a multi-modal search system.To address these challenges, the paper proposes and evaluates several techniques:1. A content-aware Pyramid-ERNIE architecture to efficiently summarize and contextualize long documents.2. A relevance-oriented pre-training strategy using calibrated user click data.3. A human-anchored fine-tuning approach to stabilize ranking signals.The central hypothesis is that by tackling these challenges through the proposed techniques, PLMs can be effectively deployed in a large-scale online search engine to significantly improve ranking performance. The paper conducts extensive offline and online experiments to evaluate this hypothesis.


## What is the main contribution of this paper?

 The main contribution of this paper is presenting solutions for effectively applying pre-trained language models (PLMs) like ERNIE to large-scale online ranking systems. The key techniques proposed include:1) A content-aware Pyramid-ERNIE architecture to efficiently summarize long documents and model relevance between queries, titles, and summaries. This balances efficiency and effectiveness for ranking.2) A relevance-oriented pre-training strategy to leverage large-scale user click data. A tree-based model calibrates the noisy clicks to align with human relevance judgments.  3) A human-anchored fine-tuning approach to stabilize ranking scores and improve compatibility with other ranking components in the search system.4) Extensive offline and online experiments that validate the effectiveness of the proposed techniques in boosting search engine performance. In summary, the main contribution is providing practical solutions and insights for deploying state-of-the-art PLMs in a large real-world search engine system, which faces challenges like long documents, noisy user clicks, and compatibility with existing components. The techniques cover optimizing model architecture, pre-training, and fine-tuning strategies tailored for web search ranking.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 The paper introduces techniques to efficiently deploy the pre-trained language model ERNIE for ranking in Baidu's search engine. The main ideas are: 1) Use a pyramid architecture and query-dependent summary extraction to balance efficiency and effectiveness when ranking long documents. 2) Pre-train on large-scale user clicks calibrated by a tree model to capture relevance. 3) Use human-anchored fine-tuning to stabilize ranking signals.In one sentence: The paper presents techniques to efficiently deploy the pre-trained language model ERNIE for ranking in Baidu's search engine by using a pyramid architecture, relevance-oriented pre-training, and human-anchored fine-tuning.


## How does this paper compare to other research in the same field?

 Here are a few thoughts on how this paper compares to other research in pre-trained language models for ranking:- The paper focuses on deploying pre-trained language models, specifically ERNIE, to large-scale web search ranking systems. This is an important practical contribution as most prior work on PLMs for ranking has been theoretical or evaluated only in offline settings.- To handle long web documents, the paper proposes a query-dependent summarization method and a Pyramid-ERNIE architecture that summarizes documents then interacts query and summary representations. This is a novel and useful approach for improving efficiency and effectiveness tradeoffs compared to prior PLM ranking work that didn't address long texts well.- For pre-training, the paper introduces a strategy to refine noisy click data using a tree-based model to align clicks with human relevance judgments. This helps tailor pre-training to ranking relevance, unlike previous pre-training objectives. The idea of calibrating user clicks for ranking is quite innovative.- The human-anchored fine-tuning technique is also unique for stabilizing ranking scores and aligning with human judgments. Most prior PLM fine-tuning uses standard approaches without accounting for human relevance preferences.- Overall, the techniques for document summarization, calibrated pre-training, and human-anchored fine-tuning provide novel contributions over prior PLM ranking research. The large-scale offline and online evaluation demonstrates these methods' usefulness for real-world systems.In summary, the paper makes useful practical contributions for deploying PLMs in large search systems, with innovations in web document modeling, pre-training strategies, and human-aligned fine-tuning. The techniques and thorough evaluation differentiate this paper from previous theoretical PLM ranking works.


## What future research directions do the authors suggest?

 Based on my review of the paper, here are some of the future research directions suggested by the authors:- Exploring other efficient architectures and pre-training objectives for deploying PLMs in large-scale ranking systems. The authors suggest investigating other modular structures beyond Pyramid-ERNIE or new pre-training tasks that better capture query-document relevance.- Leveraging other sources of user behavior data for relevance-oriented pre-training. The authors used click data but suggest exploring user dwell time, skip rates, reformulations etc. as additional signals. - Testing the proposed methods on other types of search verticals beyond web search, such as news, video, image search. The techniques may be applicable to other domains.- Conducting further analysis to understand the performance differences between offline evaluation and online metrics. The authors suggest more research on reasons behind disparities.- Considering other human judgments beyond relevance, such as credibility, novelty, understandability to better align with user needs.- Exploring different options for combining pointwise and pairwise loss in human-anchored fine-tuning. The ratio could be further optimized.- Studying how to efficiently update pre-trained models as user interests and language evolve over time.In summary, the main future directions are around improvements to architectures, pre-training tasks, data sources, model analysis, and human preference alignment for deploying PLMs in large-scale ranking systems.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the paper:This paper presents techniques for deploying pre-trained language models (PLMs) like ERNIE for ranking in large-scale search engines. The key challenges are the efficiency of PLMs for long documents, tailoring PLMs for relevance ranking, and ensuring compatibility with other ranking components. To address efficiency, they propose a Pyramid-ERNIE architecture that summarizes documents and then matches query-title and summary representations. For relevance ranking, they calibrate user clicks with a decision tree and use the refined clicks to pre-train. To improve compatibility, they use a mix of pointwise and pairwise loss when fine-tuning to stabilize scores. Extensive offline and online experiments on Baidu search validate the techniques, showing significant improvements in metrics like PNR, DCG and user preference. The work provides insights into deploying PLMs for industrial ranking.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the paper:The paper presents techniques to effectively deploy pre-trained language models (PLMs) like ERNIE for ranking in large-scale search engines. The first challenge is that PLMs are inefficient at processing long web documents for context-aware ranking. To address this, the authors propose a Pyramid-ERNIE architecture that first summarizes the document efficiently using a query-weighted extraction algorithm and then captures query-document relevance through contextual interactions. The second challenge is the discrepancy between existing pre-training objectives and relevance modeling needed for ranking. The authors tackle this by using a tree-based model to calibrate noisy user clicks to human relevance judgments, and then pre-train Pyramid-ERNIE on the refined click data to learn query-document relevance. Finally, to make the scores compatible with other search components, a human-anchored fine-tuning approach is introduced. Extensive offline and online experiments on Baidu search engine demonstrate the effectiveness of the proposed techniques.


## Summarize the main method used in the paper in one paragraph.

 The paper presents techniques for deploying pre-trained language models (PLMs) for ranking in Baidu's search engine. The main method is as follows:To enable content-aware ranking while maintaining efficiency, they propose a Pyramid-ERNIE model. This extracts a query-dependent summary for each document using an efficient algorithm, then passes the query, title, and summary through separate BERT encoders before combining their representations for final scoring. For pre-training, they use a tree-based model to calibrate large amounts of user click data to better reflect relevance. The calibrated data is used to pre-train Pyramid-ERNIE with a ranking-based objective.For fine-tuning, they use a mixture of pairwise and pointwise losses. The pointwise loss anchors the scores to human relevance judgments, improving stability and compatibility with other ranking signals.In summary, the key techniques are an efficient Pyramid-ERNIE architecture, relevance-oriented pre-training using calibrated user clicks, and human-anchored fine-tuning for stability and compatibility. Extensive experiments demonstrate improved search quality.


## What problem or question is the paper addressing?

 Based on my reading, the key points of this paper are:- The paper focuses on deploying pre-trained language models (PLMs) like ERNIE for web search ranking in a large-scale online system. Specifically, it aims to address three main challenges:1) Efficiency of modeling long documents: PLMs like BERT have quadratic complexity in input length, making it inefficient to encode full web documents directly. 2) Mismatch between existing pre-training objectives and ranking: Tasks like masked language modeling used in pre-training do not explicitly model relevance between queries and documents needed for ranking.3) Compatibility with existing ranking components: Scores from fine-tuned PLMs need to be compatible with scores from other existing rankers in a real search system.- To address efficiency, the paper proposes a Pyramid-ERNIE architecture that summarizes documents and then models interactions between query, title, and summary. - For better pre-training, they use a tree-based model to calibrate raw user clicks to model relevance and use that to pre-train ERNIE.- For compatibility, they propose a human-anchored fine-tuning approach that adds pointwise loss to stabilize scores.- Experiments on Baidu search engine show effectiveness of the proposed techniques in improving ranking quality online.In summary, the key focus is on practical techniques to enable deploying PLMs in industrial web search by addressing efficiency, pre-training, and compatibility issues. The solutions are shown to improve real search systems.
