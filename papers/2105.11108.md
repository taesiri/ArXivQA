# [Pre-trained Language Model based Ranking in Baidu Search](https://arxiv.org/abs/2105.11108)

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper addresses is how to effectively deploy pre-trained language models (PLMs) like ERNIE for ranking in a large-scale online search engine system. Specifically, the paper focuses on overcoming three key challenges:1. The inefficiency of PLMs for ranking long text documents due to the quadratic computation complexity. 2. The discrepancy between existing pre-training objectives and the demands of relevance modeling in ranking.3. The need for compatibility of the fine-tuned PLM ranker with other components in a multi-modal search system.To address these challenges, the paper proposes and evaluates several techniques:1. A content-aware Pyramid-ERNIE architecture to efficiently summarize and contextualize long documents.2. A relevance-oriented pre-training strategy using calibrated user click data.3. A human-anchored fine-tuning approach to stabilize ranking signals.The central hypothesis is that by tackling these challenges through the proposed techniques, PLMs can be effectively deployed in a large-scale online search engine to significantly improve ranking performance. The paper conducts extensive offline and online experiments to evaluate this hypothesis.


## What is the main contribution of this paper?

The main contribution of this paper is presenting solutions for effectively applying pre-trained language models (PLMs) like ERNIE to large-scale online ranking systems. The key techniques proposed include:1) A content-aware Pyramid-ERNIE architecture to efficiently summarize long documents and model relevance between queries, titles, and summaries. This balances efficiency and effectiveness for ranking.2) A relevance-oriented pre-training strategy to leverage large-scale user click data. A tree-based model calibrates the noisy clicks to align with human relevance judgments.  3) A human-anchored fine-tuning approach to stabilize ranking scores and improve compatibility with other ranking components in the search system.4) Extensive offline and online experiments that validate the effectiveness of the proposed techniques in boosting search engine performance. In summary, the main contribution is providing practical solutions and insights for deploying state-of-the-art PLMs in a large real-world search engine system, which faces challenges like long documents, noisy user clicks, and compatibility with existing components. The techniques cover optimizing model architecture, pre-training, and fine-tuning strategies tailored for web search ranking.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

The paper introduces techniques to efficiently deploy the pre-trained language model ERNIE for ranking in Baidu's search engine. The main ideas are: 1) Use a pyramid architecture and query-dependent summary extraction to balance efficiency and effectiveness when ranking long documents. 2) Pre-train on large-scale user clicks calibrated by a tree model to capture relevance. 3) Use human-anchored fine-tuning to stabilize ranking signals.In one sentence: The paper presents techniques to efficiently deploy the pre-trained language model ERNIE for ranking in Baidu's search engine by using a pyramid architecture, relevance-oriented pre-training, and human-anchored fine-tuning.


## How does this paper compare to other research in the same field?

Here are a few thoughts on how this paper compares to other research in pre-trained language models for ranking:- The paper focuses on deploying pre-trained language models, specifically ERNIE, to large-scale web search ranking systems. This is an important practical contribution as most prior work on PLMs for ranking has been theoretical or evaluated only in offline settings.- To handle long web documents, the paper proposes a query-dependent summarization method and a Pyramid-ERNIE architecture that summarizes documents then interacts query and summary representations. This is a novel and useful approach for improving efficiency and effectiveness tradeoffs compared to prior PLM ranking work that didn't address long texts well.- For pre-training, the paper introduces a strategy to refine noisy click data using a tree-based model to align clicks with human relevance judgments. This helps tailor pre-training to ranking relevance, unlike previous pre-training objectives. The idea of calibrating user clicks for ranking is quite innovative.- The human-anchored fine-tuning technique is also unique for stabilizing ranking scores and aligning with human judgments. Most prior PLM fine-tuning uses standard approaches without accounting for human relevance preferences.- Overall, the techniques for document summarization, calibrated pre-training, and human-anchored fine-tuning provide novel contributions over prior PLM ranking research. The large-scale offline and online evaluation demonstrates these methods' usefulness for real-world systems.In summary, the paper makes useful practical contributions for deploying PLMs in large search systems, with innovations in web document modeling, pre-training strategies, and human-aligned fine-tuning. The techniques and thorough evaluation differentiate this paper from previous theoretical PLM ranking works.


## What future research directions do the authors suggest?

Based on my review of the paper, here are some of the future research directions suggested by the authors:- Exploring other efficient architectures and pre-training objectives for deploying PLMs in large-scale ranking systems. The authors suggest investigating other modular structures beyond Pyramid-ERNIE or new pre-training tasks that better capture query-document relevance.- Leveraging other sources of user behavior data for relevance-oriented pre-training. The authors used click data but suggest exploring user dwell time, skip rates, reformulations etc. as additional signals. - Testing the proposed methods on other types of search verticals beyond web search, such as news, video, image search. The techniques may be applicable to other domains.- Conducting further analysis to understand the performance differences between offline evaluation and online metrics. The authors suggest more research on reasons behind disparities.- Considering other human judgments beyond relevance, such as credibility, novelty, understandability to better align with user needs.- Exploring different options for combining pointwise and pairwise loss in human-anchored fine-tuning. The ratio could be further optimized.- Studying how to efficiently update pre-trained models as user interests and language evolve over time.In summary, the main future directions are around improvements to architectures, pre-training tasks, data sources, model analysis, and human preference alignment for deploying PLMs in large-scale ranking systems.


## Summarize the paper in one paragraph.

Here is a one paragraph summary of the paper:This paper presents techniques for deploying pre-trained language models (PLMs) like ERNIE for ranking in large-scale search engines. The key challenges are the efficiency of PLMs for long documents, tailoring PLMs for relevance ranking, and ensuring compatibility with other ranking components. To address efficiency, they propose a Pyramid-ERNIE architecture that summarizes documents and then matches query-title and summary representations. For relevance ranking, they calibrate user clicks with a decision tree and use the refined clicks to pre-train. To improve compatibility, they use a mix of pointwise and pairwise loss when fine-tuning to stabilize scores. Extensive offline and online experiments on Baidu search validate the techniques, showing significant improvements in metrics like PNR, DCG and user preference. The work provides insights into deploying PLMs for industrial ranking.
