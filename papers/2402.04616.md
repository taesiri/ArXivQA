# [TinyLLM: Learning a Small Student from Multiple Large Language Models](https://arxiv.org/abs/2402.04616)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
Existing knowledge distillation methods for transferring capabilities from large language models (LLMs) to smaller ones have two key limitations: 
1) Limited knowledge diversity - Most methods use a single teacher model, restricting the student model to the biases and weaknesses of that one teacher.  
2) Lack of rich contextual information - Methods focus on answer labels rather than the reasoning behind the answers.

Proposed Solution:
The paper proposes TinyLLM, a novel distillation paradigm to learn a small student LLM by transferring reasoning skills from multiple large teacher LLMs. Key ideas:

1) Uses multiple teacher models to provide diverse perspectives and mitigate limitation 1. 
2) Asks teachers to provide rationales to support their answers, ensuring the student understands the reasoning behind the answers. This helps address limitation 2.
3) An in-context example generator and teacher-forcing Chain-of-Thought strategy further ground the rationales in appropriate context.
4) A multi-task framework trains the student to not only predict answers but also generate rationales like the teachers.

Main Contributions:
1) Identifies key limitations of existing knowledge distillation methods for LLMs
2) Proposes TinyLLM, a new distillation paradigm using multiple teacher rationales and strategies to ground them.
3) Shows through extensive experiments that TinyLLM outperforms individual teacher models and baseline distillation methods by 12.57% on average across tasks.
4) Demonstrates TinyLLM can match or exceed teacher performance with only 1.1-26% of their parameters.

In summary, the paper makes significant contributions in improving knowledge distillation for LLMs by using diverse teacher rationales grounded in context to overcome limitations of existing approaches. The proposed TinyLLM method convincingly outperforms strong baselines.
