# [SwiftFormer: Efficient Additive Attention for Transformer-based   Real-time Mobile Vision Applications](https://arxiv.org/abs/2303.15446)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is:How can we design an efficient self-attention mechanism that achieves better speed-accuracy trade-offs compared to previous methods for real-time mobile vision applications?The key hypothesis is that an additive attention mechanism with simplified query-key interactions can effectively model global context while eliminating the need for expensive matrix multiplications inherent in standard self-attention. This can lead to faster and more accurate models suitable for mobile deployment.Specifically, the paper proposes "efficient additive attention" that:- Replaces matrix multiplications with element-wise operations to reduce computational complexity. - Eliminates explicit modeling of key-value interactions and instead uses a simple linear layer, while still encoding effective global context from query-key interactions.- Has linear complexity with respect to number of tokens, enabling usage across all stages for consistent global context modeling.To validate this hypothesis, the paper introduces "SwiftFormer" models utilizing the proposed attention mechanism and demonstrates state-of-the-art accuracy and speed trade-offs on ImageNet classification as well as COCO object detection and ADE20K semantic segmentation.In summary, the core research question is how to design a fast yet accurate self-attention variant to boost vision model performance on mobile devices, which the proposed efficient additive attention aims to address.
