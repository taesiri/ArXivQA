# [Attentional Pooling for Action Recognition](https://arxiv.org/abs/1711.01467)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a summary paragraph of the key points from the paper:

This paper proposes an attention-based pooling module that can be easily incorporated into standard CNN architectures for action recognition. The key idea is to formulate attention as a low-rank approximation of second-order pooling, which allows efficient computation. Specifically, the module computes a bottom-up saliency map as well as a top-down attention map modulated by the class labels. These maps are combined and used to weight the feature map for selective pooling. This requires minimal additional parameters while providing significant performance gains. The method is evaluated on image and video action recognition datasets including MPII, HICO, and HMDB51. Without any bounding box or pose supervision, it achieves state-of-the-art results on MPII and HMDB51 RGB streams, and competitive performance on HICO. Additional experiments analyze various modeling choices and show that explicit attention is most beneficial when the base features have sufficiently high spatial resolution. The connections to second-order pooling also suggest that action recognition can be viewed as a fine-grained recognition problem. Overall, the proposed attention module provides an effective and easy-to-implement way of improving feature pooling in CNNs for action recognition.


## Summarize the paper in one sentence.

 Here is a one-sentence summary of the paper:

The paper proposes an attention module that weights spatial average pooling in CNNs as a low-rank approximation of bilinear pooling, achieving state-of-the-art performance for action recognition in images and videos with minimal computational overhead.


## What is the main contribution of this paper?

 According to the paper, the main contributions are:

(1) An easy-to-use extension of state-of-the-art base architectures that incorporates attention to give significant improvement in action recognition performance with virtually no increase in computation cost.

(2) Extensive analysis of its performance on three action recognition datasets across still images and videos, obtaining state-of-the-art results on MPII and HMDB-51 (for RGB, single-frame models) and competitive performance on HICO. 

(3) Analysis of different base architectures for the applicability of the proposed attention module.

(4) Mathematical analysis showing the equivalence of the proposed attentional pooling to a rank-1 approximation of second order or bilinear pooling, suggesting a characterization of action recognition as a fine-grained recognition problem.

So in summary, the main contribution is an attention module that can be easily incorporated into standard CNN architectures to improve action recognition performance by focusing computation on relevant parts of the input. This is analyzed extensively across different datasets and base architectures, and also linked to bilinear models for fine-grained recognition.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and concepts associated with this paper include:

- Action recognition
- Attention models
- Attentional pooling
- Bottom-up and top-down attention
- Low-rank approximations
- Second-order pooling 
- Bilinear pooling
- Fine-grained recognition
- Human pose regularization
- Image and video understanding

The paper introduces an attentional pooling module that can be inserted into CNN architectures to learn to focus computational resources on important regions in images and videos for the task of action recognition. This is formulated as a low-rank approximation of second-order pooling, drawing connections between attention and bilinear models commonly used for fine-grained recognition. The model combines bottom-up saliency and top-down task-specific attention. Additional supervision is provided through human pose labels to encourage focusing on human-object interactions. The approach is analyzed and demonstrated to achieve state-of-the-art or competitive performance on standard action recognition datasets including MPII, HICO, and HMDB51.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in the paper:

1) The paper proposes attentional pooling as a low-rank approximation of second-order pooling. Can you explain this connection and derivation in more detail? How does it lead to the proposed attention formulation?

2) The paper combines top-down and bottom-up attention in the proposed model. What is the motivation behind this? How are the two attention maps generated and combined?

3) What are the differences between the two network architectures explored in the paper (Fig 2b)? What are the trade-offs and why does pose-regularization lead to better performance on videos?

4) How exactly does pose-regularization help guide the attention model? What is the intuition behind using pose to regularize the attention maps?

5) The paper analyzes the effect of using different base networks like ResNet and Inception for the proposed attention model. What differences did they find and why? What does this tell us about architecture design choices?

6) How does the proposed attentional pooling module compare against using full second-order pooling? What approximations did they have to make and what were the tradeoffs?

7) The paper shows attention helps more for ResNet than Inception architectures. Can you analyze the architectures and explain possible reasons behind this observation?

8) Can you extend the analysis to explore per-class attention maps? What are the challenges there and how does performance compare to a class-agnostic attention map?

9) The paper briefly explores higher rank-P approximations. How does performance vary with P and what does this tell you about the proposed rank-1 approximation?

10) The paper claims the proposed model does not need instance-level supervision. Can you think of modifications to make it work at an instance-level? What supervision would it need?
