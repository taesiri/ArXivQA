# [Multimodality Helps Unimodality: Cross-Modal Few-Shot Learning with   Multimodal Models](https://arxiv.org/abs/2301.06267)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be: Can multimodal models be adapted for few-shot learning in a more effective way by leveraging cross-modal information directly during training, rather than just using it to initialize a unimodal model?The key hypothesis appears to be that including examples from different modalities (e.g. images, text, audio) together in the training set will lead to better few-shot learning performance compared to only training on examples from a single modality. The authors argue that human learning utilizes cross-modal information and representations, but most prior work on adapting pretrained models like CLIP for downstream tasks uses a unimodal training setup. So this paper proposes and evaluates a simple but effective method of cross-modal adaptation, where examples from different modalities are incorporated in the training set and trained jointly.The main contributions seem to be:- Proposing cross-modal adaptation as a new paradigm for effectively finetuning multimodal models like CLIP in the few-shot regime.- Achieving SOTA few-shot classification results with simple linear classifiers, outperforming more complex finetuning techniques.- Demonstrating that cross-modal learning benefits not just linear probing but also other methods like prompting and adapter-based finetuning.- Extending the approach to audio with AudioCLIP and constructing an audio-visual few-shot benchmark.- Providing analysis and intuition on why cross-modal training helps, such as reducing ambiguity and producing complementary cues.In summary, the paper presents strong empirical evidence that directly incorporating cross-modal information during training is an effective strategy for adapting multimodal models for few-shot learning. The simple methodology also provides a strong baseline for future work.


## What is the main contribution of this paper?

The main contribution of this paper is proposing a cross-modal adaptation approach for few-shot learning with multimodal models like CLIP. The key ideas are:- Leveraging additional modalities (e.g. text, audio) as extra training examples to augment the few visual examples. This helps resolve the ambiguity and underspecification of few-shot learning.- Simply treating examples from different modalities as additional training samples, since multimodal models like CLIP embed them in the same space. This allows training a classifier on multimodal data.- Achieving state-of-the-art results on few-shot classification by using class name texts as extra examples. A simple linear classifier outperforms prior finetuning methods.- Demonstrating the approach also works for audio with AudioCLIP, constructing an audiovisual few-shot benchmark. Listening to barks improves visual dog classification.- Showing cross-modal training benefits prior methods like prompting and adapters, since it provides complementary information to finetuning visual features.In summary, the key contribution is a simple but effective cross-modal adaptation approach to leverage multimodal representations for few-shot learning. The results demonstrate this is a promising direction to improve generalization from few examples.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Based on my reading, the main takeaway of this paper is that multimodal learning using vision and language modalities can significantly improve performance on few-shot image classification tasks. The key idea is to leverage pre-trained multimodal models like CLIP that map different modalities like images and text to the same semantic representation space. By treating class name texts as additional training examples, the authors propose a simple yet effective cross-modal adaptation approach to few-shot learning that achieves state-of-the-art results with just a linear classifier. The method is shown to be beneficial even when combined with prior uni-modal adaptation techniques like prompting and adapter-based tuning. Experiments also demonstrate that audio modalities can likewise improve vision tasks, suggesting the broad applicability of cross-modal learning.In summary, the paper shows that cross-modal training is a promising paradigm for adapting multimodal foundation models, as the complementary information from different modalities acts as a strong inductive bias to learn from limited data.
