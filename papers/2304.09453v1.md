# [Network Pruning Spaces](https://arxiv.org/abs/2304.09453v1)

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper addresses is: What are the general principles that can help us understand and refine neural network pruning algorithms, going beyond producing a single best pruning recipe?To explore this, the paper introduces the concept of "network pruning spaces" which parameterize populations of subnetwork architectures. By analyzing distributions of subnetworks sampled from these spaces, the authors identify some key observations:1) There exists an optimal FLOPs-to-parameter ratio for a given pruning regime. 2) Subnetworks achieving this optimal ratio tend to perform the best.3) The optimal ratio and best possible performance seem predictable based on the original network design. Based on these observations, the authors propose refinements to the pruning space search process to find high-performing subnetworks more efficiently. Overall, this moves beyond optimizing one pruning recipe to extracting general principles about the relationships between network architecture, efficiency, and performance under pruning.


## What is the main contribution of this paper?

This paper introduces the concept of "network pruning spaces" for exploring general principles and patterns in neural network pruning. The key contributions are:1. Presenting network pruning spaces that parameterize populations of subnetwork architectures pruned from an original network. This allows analyzing distributions of subnetworks rather than just producing a single pruned network. 2. Based on empirical studies of pruning spaces, making conjectures about the existence of an optimal FLOPs-to-parameters ratio in a pruning regime, and that networks matching this ratio tend to perform best.3. Using the conjectures to refine the pruning space with constraints on the FLOPs-to-parameters ratio, reducing the search cost for finding good pruned subnetworks.4. Showing analytically that the performance limitation of pruned networks in a regime can be predicted by the optimal FLOPs-to-parameters ratio. This provides a tool for reasoning about efficiency-accuracy tradeoffs in pruning.In summary, the key novelty is introducing pruning spaces to uncover general principles, instead of just producing one pruned network. The conjectures and analysis help explain why certain network structures perform well when pruned. This provides guidance for developing better pruning algorithms.
