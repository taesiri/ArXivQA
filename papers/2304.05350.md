# [Astroformer: More Data Might not be all you need for Classification](https://arxiv.org/abs/2304.05350)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question appears to be: Can a hybrid transformer-convolutional neural network architecture paired with careful data augmentation and regularization techniques achieve state-of-the-art performance in classifying galaxy morphologies from images, even when trained from scratch on a relatively small dataset?The key points are:- The authors propose a hybrid architecture called Astroformer that combines transformers and convolutional neural networks, inspired by models like CoAtNet. - They aim to show this architecture can learn effectively from small datasets when trained from scratch, without requiring pretraining on much larger datasets like ImageNet.- The authors hypothesize that combining the global modeling of transformers with the local inductive biases of CNNs will improve sample efficiency and generalization.- They also emphasize the importance of choosing suitable data augmentation and regularization strategies for good performance in the low-data regime.- The main experiment tests Astroformer on classifying galaxy morphologies on the Galaxy10 DECals dataset containing only ~17k images. They achieve new state-of-the-art results, beating prior methods by a large margin.So in summary, the central hypothesis is that the proposed Astroformer architecture along with proper training techniques can achieve excellent performance on galaxy morphology classification from small datasets, surpassing previous approaches. Demonstrating strong capability in the low-data regime is the key focus.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contributions appear to be:- Proposing a hybrid transformer-convolutional neural network architecture called Astroformer for galaxy morphology classification. The architecture combines convolutions and self-attention in a novel way.- Achieving state-of-the-art results on the Galaxy10 DECals dataset for galaxy morphology classification, improving accuracy by 4.62% over previous methods. This is the first use of a hybrid transformer-convolutional model for this task.- Demonstrating that the proposed Astroformer model can achieve strong performance on low-data image classification benchmarks like CIFAR-100 and Tiny ImageNet without relying on pre-training. This suggests the model can learn effective representations from small datasets. - Analyzing different model architectures and training techniques to determine what works best in the low-data regime when training from scratch. In particular, the paper finds that using 3 convolutional stages followed by 1 transformer stage works better than other arrangements.- Providing insights into model design choices like data augmentation, regularization, and architectural modifications that allow transformers and hybrid models to generalize well when trained on small datasets from scratch.In summary, the main contribution is proposing Astroformer, a hybrid transformer-convolutional model tailored to the low-data regime, and showing it can surpass prior state-of-the-art on galaxy morphology classification while also performing well on other small image datasets. The analysis around training such models on limited data is also a key contribution.
