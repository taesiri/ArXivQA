# [Astroformer: More Data Might not be all you need for Classification](https://arxiv.org/abs/2304.05350)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question appears to be: Can a hybrid transformer-convolutional neural network architecture paired with careful data augmentation and regularization techniques achieve state-of-the-art performance in classifying galaxy morphologies from images, even when trained from scratch on a relatively small dataset?The key points are:- The authors propose a hybrid architecture called Astroformer that combines transformers and convolutional neural networks, inspired by models like CoAtNet. - They aim to show this architecture can learn effectively from small datasets when trained from scratch, without requiring pretraining on much larger datasets like ImageNet.- The authors hypothesize that combining the global modeling of transformers with the local inductive biases of CNNs will improve sample efficiency and generalization.- They also emphasize the importance of choosing suitable data augmentation and regularization strategies for good performance in the low-data regime.- The main experiment tests Astroformer on classifying galaxy morphologies on the Galaxy10 DECals dataset containing only ~17k images. They achieve new state-of-the-art results, beating prior methods by a large margin.So in summary, the central hypothesis is that the proposed Astroformer architecture along with proper training techniques can achieve excellent performance on galaxy morphology classification from small datasets, surpassing previous approaches. Demonstrating strong capability in the low-data regime is the key focus.
