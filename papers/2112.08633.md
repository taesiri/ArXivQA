# [Learning To Retrieve Prompts for In-Context Learning](https://arxiv.org/abs/2112.08633)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the abstract, the key research question this paper addresses is:How can we develop an efficient method for retrieving good prompts (training examples) for in-context learning in large language models?The main hypothesis seems to be:Using language models themselves to label which training examples will be good prompts, and training a prompt retriever from this signal, will result in better performance compared to prior prompt retrieval methods.In summary, the paper proposes a method called EPR (Efficient Prompt Retrieval) that uses a scoring LM to label training examples as good or bad prompts. It then trains a prompt retriever using this labeled data. The authors hypothesize and show that this approach substantially outperforms prior prompt retrieval methods across several language understanding tasks. The main research contribution is developing an effective technique to retrieve high-quality prompts to guide in-context learning in large LMs.


## What is the main contribution of this paper?

Based on my reading, the main contribution of this paper is proposing an efficient method for retrieving good prompts (training examples) for in-context learning in large language models. Specifically:- The paper proposes using the language model itself to label which training examples will be useful prompts. For each training input-output pair (x,y), they estimate the probability of y given x and candidate prompt examples, and label high probability examples as positive and low probability ones as negative. - They then train a dense retriever using these labeled prompts with a contrastive loss. The retriever learns a similarity function between a test input and candidate prompts. - At test time, for a given input, the retriever efficiently retrieves the most similar prompts from the training set, which are then provided along with the input to the language model for in-context prediction.- They show this approach substantially outperforms various unsupervised and supervised baselines on semantic parsing tasks using GPT-Neo. It also works well as a proxy method when using larger LMs like GPT-3 for inference.In summary, the key contribution is using the language model itself to train an efficient prompt retriever, which helps improve in-context learning performance across models. The method provides an effective way to interface with and harness large language models.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper proposes an efficient method to retrieve good training examples (prompts) for in-context learning in large language models, where the language models themselves are used to label which training examples will be useful.


## How does this paper compare to other research in the same field?

Here are a few key points on how this paper compares to other related work:- The paper proposes using language models themselves to label training data for prompt retrieval, rather than relying on surface-level heuristic similarity metrics. This is a novel way to leverage the knowledge within large pretrained LMs to train prompt retrievers. Other work has focused more on unsupervised similarity or rule-based supervision.- For training the prompt retriever, the paper employs a contrastive learning approach using hard negatives mined from candidates ranked by the scoring LM. This follows recent trends in using contrastive methods to train dense retrievers, but tailors the negative sampling strategy to the prompt retrieval setting.- The paper shows substantial gains from their proposed Efficient Prompt Retrieval (EPR) method across three semantic parsing datasets, outperforming both unsupervised and supervised baselines. The improvements are demonstrated using GPT-Neo and as a proxy model for larger LMs like GPT-3. This provides strong empirical evidence for the effectiveness of their approach.- EPR is model-agnostic and can work without access to model parameters, making it suitable even when using LMs in a blackbox manner. This contrasts with some other prompt engineering methods that require model fine-tuning or access. The authors position EPR as an approach to efficiently interface with ever-larger LMs.- While focused on in-context learning for semantic parsing, the idea of using the LM itself for prompt retrieval training supervision could potentially extend more broadly. However, the paper does not explore wider applications.In summary, the key novelty of this paper is using LMs for prompt retrieval supervision, combined with contrastive learning. This achieves substantial gains over baselines, providing a generalizable way to improve in-context learning through efficient retrieval. The model-agnostic nature also suits growing LM scales.
