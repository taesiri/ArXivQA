# [Self-Evaluation Improves Selective Generation in Large Language Models](https://arxiv.org/abs/2312.09300)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Large language models (LLMs) can generate text that may be untrustworthy or low quality. Having a reliable way to assess the quality of LLM generations is important for safe deployment.  
- Sequence-level likelihood scores from LLMs do not reliably indicate generation quality. But LLMs show good calibration for selecting correct answers or evaluating true/false statements.

Proposed Solution:
- Reformulate open-ended generation as a token-level prediction task by having the LLM self-evaluate its answers. This leverages the LLM's superior token-level calibration.
- Employ either a multi-way comparison ("Sample and Select") or point-wise evaluation ("Sample and Eval") approach. Can also include a "None of the Above" option.  
- Propose a variety of scoring methods based on self-evaluation. Also propose a hybrid method that combines multi-way comparison for answer selection and point-wise evaluation for scoring.

Main Contributions:  
- Show self-evaluation based scores improve accuracy and correlate better with output quality compared to sequence likelihood scores.
- Demonstrate the proposed methods significantly enhance quality calibration, enabling better selective generation on TruthfulQA and TL;DR benchmarks using PaLM and GPT-3.
- Establish token-level self-evaluation as an effective way to assess open-ended generations from LLMs by leveraging their existing calibration at the token level.
