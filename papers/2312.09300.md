# [Self-Evaluation Improves Selective Generation in Large Language Models](https://arxiv.org/abs/2312.09300)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Large language models (LLMs) can generate text that may be untrustworthy or low quality. Having a reliable way to assess the quality of LLM generations is important for safe deployment.  
- Sequence-level likelihood scores from LLMs do not reliably indicate generation quality. But LLMs show good calibration for selecting correct answers or evaluating true/false statements.

Proposed Solution:
- Reformulate open-ended generation as a token-level prediction task by having the LLM self-evaluate its answers. This leverages the LLM's superior token-level calibration.
- Employ either a multi-way comparison ("Sample and Select") or point-wise evaluation ("Sample and Eval") approach. Can also include a "None of the Above" option.  
- Propose a variety of scoring methods based on self-evaluation. Also propose a hybrid method that combines multi-way comparison for answer selection and point-wise evaluation for scoring.

Main Contributions:  
- Show self-evaluation based scores improve accuracy and correlate better with output quality compared to sequence likelihood scores.
- Demonstrate the proposed methods significantly enhance quality calibration, enabling better selective generation on TruthfulQA and TL;DR benchmarks using PaLM and GPT-3.
- Establish token-level self-evaluation as an effective way to assess open-ended generations from LLMs by leveraging their existing calibration at the token level.


## Summarize the paper in one sentence.

 This paper proposes using language models' ability to self-evaluate their own generations in order to improve selective generation.


## What is the main contribution of this paper?

 The main contribution of this paper is proposing methods to improve the quality calibration of large language models on open-ended generative tasks, in order to better estimate model confidence for selective generation. Specifically:

- They reformulate open-ended generation as token-level prediction tasks, leveraging LLMs' good calibration at token level - either with multi-way comparison ("Sample and Select") or point-wise evaluation ("Sample and Eval"). This allows defining various scores based on model's self-evaluation.

- They systematically compare different scoring approaches on selective generation benchmarks, using metrics like Calibration AUC and Selective AUC. 

- Through experiments on TruthfulQA and TLDR datasets using PaLM and GPT-3, they demonstrate that the proposed self-evaluation scores correlate better with output quality and enable better selective generation compared to likelihood-based scores.

- They show the self-evaluation scores complement other quality improvement techniques like self-critiquing and revising. The key insight is reducing free-form gen to token-level scoring.

In summary, the main contribution is developing better calibrated confidence scores for selective generation in LLMs, by designing prompts that convert free-form gen to token-level tasks that leverage LLMs' stronger calibration. The self-evaluation paradigm helps abstain poor quality outputs.


## What are the keywords or key terms associated with this paper?

 Based on my review, some of the key terms and concepts discussed in this paper include:

- Large language models (LLMs): The paper focuses on evaluating and improving the quality and calibration of large language models like PaLM and GPT.

- Selective generation: Using confidence scores to selectively generate high-quality outputs from LLMs while abstaining from low-quality outputs. 

- Quality calibration: Assessing how well a model's confidence scores correlate with the actual quality of the generated outputs.

- Self-evaluation: Getting the LLM to evaluate and score its own generated outputs at the token level via prompts.

- Sample and select: Generating multiple candidate answers, formatting them as a multiple choice question, and having the LLM select the best answer.

- Sample and eval: Generating a candidate answer, asking the LLM to evaluate if it is a good answer in a binary true/false format.  

- Hybrid method: Using sample and select to choose the best answer, then scoring that answer with sample and eval.

- Calibration AUC: Area under the ROC curve using model confidence and output quality.

- Selective AUC: Area under the selective generation curve sweeping over different abstention rates.

In summary, the key focus is on using self-evaluation prompts for an LLM to assess the quality of its own open-ended textual generations, in order to improve selective generation.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in this paper:

1. How does reducing sequence-level scoring to token-level scoring help improve quality calibration for large language models? Does it leverage certain capabilities that token-level scoring has over sequence-level?

2. Why is directly using the softmax probability from the Sample and Select method not an ideal confidence score? What issues like position bias and probability dispersion does it have?

3. What are the key differences between the Sample and Select and Sample and Eval methods proposed in the paper? What are the relative advantages and disadvantages of each? 

4. Explain the hybrid "Sample and Select and Eval" method in detail. How does it aim to combine the strengths of both Sample and Select and Sample and Eval approaches?

5. How exactly is the final confidence score computed in the hybrid approach, specifically when the "None of the Above" option is included? Walk through the steps involved.

6. What metrics beyond accuracy, like Calibration AUC and Selective AUC, are used to evaluate the quality calibration ability of different scoring methods? Why are they better suited for this purpose?

7. How complementary is self-evaluation to other answer quality improvement techniques like self-critique and revise? Does order matter?

8. What assumptions does the reduction of sequence-level scoring to token-level scoring make regarding language models capabilities? Are they reasonably valid? 

9. For practical deployment, what are the inference time and computational costs to consider with self-evaluation versus a purely sequence-level scoring approach?

10. How might the self-evaluation approach be modified or improved to work better for very long, multi-paragraph generative tasks? Would all components transfer over effectively?
