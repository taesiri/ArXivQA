# [Pretext-Contrastive Learning: Toward Good Practices in Self-supervised   Video Representation Leaning](https://arxiv.org/abs/2010.15464)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be:How can we combine pretext tasks and contrastive learning in a general framework to boost performance on self-supervised video representation learning?The authors propose combining pretext tasks (which focus on learning representations within a sample) and contrastive learning (which aims to distinguish between different instances) into a joint optimization framework called Pretext-Contrastive Learning (PCL). Their goal is to show that this framework can facilitate the advantages of both pretext tasks and contrastive learning to achieve state-of-the-art performance on downstream tasks like video retrieval and recognition. They test their PCL framework on three different pretext task baselines using different network backbones. The results demonstrate substantial improvements over the individual baselines and the generality of their framework across tasks and models. Through extensive experiments and ablation studies, they aim to show that each component of their proposal (pretext task, contrastive learning, residual clips, data augmentation) contributes positively to performance.In summary, the central hypothesis is that combining pretext tasks and contrastive learning in an effective general framework can boost performance on self-supervised video representation learning across different models and tasks. The experiments and results are meant to demonstrate and validate this hypothesis.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contributions seem to be:1. Proposing a joint optimization framework called Pretext-Contrastive Learning (PCL) that combines pretext tasks and contrastive learning to take advantage of both approaches for self-supervised video representation learning. 2. Showing that with proper data processing strategies like using residual clips and strong data augmentations, the PCL framework can greatly boost the performance of existing pretext task baselines as well as contrastive learning baselines.3. Demonstrating the effectiveness and generality of the PCL framework by testing it with different pretext task baselines, network backbones, and evaluation tasks/datasets. The results show consistent and significant improvements.4. Achieving new state-of-the-art results on standard benchmarks with PCL, while using much less pre-training data than prior methods.5. Providing extensive ablation studies and analysis to validate the impact of different components of their framework like residual clips, augmentations, loss balancing etc.6. Setting up an easily adaptable PCL framework that can serve as a standard training strategy to be applied to many existing self-supervised video representation learning methods to improve their performance.In summary, the key novelty seems to be in systematically exploring and pushing the limits of combining pretext tasks and contrastive learning under a joint optimization framework augmented with data processing strategies to set new state-of-the-art benchmarks for self-supervised video representation learning.
