# [Learning User Embeddings from Human Gaze for Personalised Saliency   Prediction](https://arxiv.org/abs/2403.13653)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem: 
Most saliency prediction models focus on predicting universal saliency that aggregates gaze data from multiple observers, disregarding individual differences in viewing behavior. However, there are significant variations in how humans deploy visual attention due to factors like scene complexity, expertise level, age, or personality traits. Personalized saliency prediction is important for applications like assistive systems, cognitive state estimation, and human-computer interaction. Prior personalized saliency methods require collecting explicit user characteristics which is tedious and raises privacy concerns.

Method:
The paper proposes a novel method to extract user embeddings that capture individual gaze behavior differences from pairs of natural images and corresponding personal saliency maps. A Siamese convolutional neural network is trained using a triplet margin loss to differentiate gaze behavior of different users on the same images. The model takes multiple image-saliency map pairs of a user and outputs a user embedding vector. 

The extracted user embeddings are integrated into a convolutional neural network for personalized saliency prediction. The embedding is used to predict convolution filters that are applied on the image-saliency features to incorporate user-specific information. The model is trained to predict a discrepancy map between the personal and universal saliency which is added to the universal saliency to get the final personalized saliency prediction.

Contributions:
- A new way to extract user embeddings from implicit gaze behavior that captures individual viewing differences without needing explicit user characteristics
- Demonstrated the discriminative power of embeddings and their ability to cluster gaze behavior of different users
- Significantly improved personalized saliency prediction over baselines on two public datasets and showed good generalization to unseen users and images
- The user embeddings could benefit other applications needing to leverage individual user models

The method opens promising directions for learning reusable gaze-based user representations. Potential negative societal impacts like impersonation and security risks with widespread gaze behavior modeling are discussed.


## Summarize the paper in one sentence.

 Here is a one-sentence summary of the paper:

The paper proposes a novel method to learn reusable embeddings that capture individual user characteristics from human gaze patterns on images, and shows how these embeddings can be integrated into convolutional neural networks to effectively predict personalized image saliency.


## What is the main contribution of this paper?

 The main contribution of this paper is a novel method to extract user embeddings from pairs of natural images and corresponding user-specific saliency maps. Specifically:

- They propose a Siamese convolutional neural network that takes as input multiple images along with personal saliency maps of a particular user. It learns to extract a user embedding vector that captures distinctive characteristics of that user's gaze patterns.

- They demonstrate that these learned embeddings have high discriminative power to differentiate between individuals based on their gaze behavior. 

- They show these embeddings can be effectively incorporated into a neural network for personalised saliency prediction. By conditioning the model on the user embedding, it can refine a universal saliency map to tailor it to an individual user.

- Their method outperforms baselines in personalising saliency predictions for both seen and unseen users/images, without requiring any explicit user traits or preferences as input.

In summary, the key contribution is a way to extract reusable neural embeddings from implicit gaze data that capture user-specific viewing behavior. These embeddings help to improve performance on the downstream task of personalised saliency prediction.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, the main keywords or key terms associated with this work are:

gaze, eye-tracking, saliency, personal saliency, user embeddings, user model, deep learning

In particular, the paper focuses on:

- Learning user embeddings to capture individual differences in visual attention and gaze behavior
- Using these embeddings to refine universal saliency maps into personalized saliency maps 
- Demonstrating the effectiveness of the embeddings on the task of personalized saliency prediction
- Proposing a novel method involving a Siamese CNN architecture to extract embeddings from pairs of images and personal saliency maps

So the core themes relate to personalized modeling of visual attention, learning representations of user behavior from eye tracking data, and using these to improve saliency prediction performance. The method leverages deep learning, specifically convolutional neural networks, to accomplish this.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in this paper:

1. The paper proposes learning user embeddings from human gaze data to capture individual differences. What are some of the key advantages and limitations of using gaze data compared to other potential sources of user behaviour data for learning embeddings?

2. The Siamese convolutional neural network architecture is used to learn the user embeddings. What are some alternative neural network architectures that could be explored? What modifications could be made to the proposed architecture?

3. The user embeddings are averaged over multiple input samples. What are some potential issues with this approach and how could the aggregation of embeddings over multiple samples be improved? 

4. The paper demonstrates refined saliency prediction as a downstream task using the learned user embeddings. What are some other potential downstream tasks or applications where these embeddings could be beneficial?

5. The user embeddings are evaluated based on their ability to discriminate between different users. What other evaluation metrics could be used to analyze the quality and usefulness of the learned embeddings?

6. The paper combines data from multiple datasets with different users and image stimuli. What are some of the additional challenges and considerations when learning user embeddings from such heterogeneous data sources?

7. The paper argues their method does not require explicit user input. However, the gaze data collection still requires user involvement. How could the approach be extended to learn embeddings in a purely unsupervised manner from raw gaze data?

8. What kinds of biases could be present in the learned user embeddings and how might they impact downstream usage? How could potential biases be reduced?

9. The paper demonstrates refined saliency prediction for seen and unseen users. How well could the approach personalize predictions for entirely new user populations not observed during training?

10. The method learns embeddings to differentiate gaze patterns between users. Could a similar approach be used to capture changes in viewing behaviour within users over time rather than just between users?
