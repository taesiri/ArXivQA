# [Exploring Discontinuity for Video Frame Interpolation](https://arxiv.org/abs/2202.07291)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question is how to make video frame interpolation networks robust to discontinuous motions like logos, user interfaces, subtitles, etc., which violate the assumption of continuous motion between frames. 

The key ideas proposed to address this question are:

1) A new data augmentation strategy called Figure-Text Mixing (FTM) that mixes synthetic discontinuous elements like static figures and moving text into the training data. This allows models to learn to handle discontinuous motion without needing extra training datasets.

2) A Discontinuity Map (D-map) module that predicts a 1-channel map indicating discontinuity between frames. The D-map allows switching between continuous motion interpolation and simply copying pixels from the previous frame for discontinuous areas.

3) Objective functions to provide supervision for the D-map prediction when FTM augmentation is used, since the augmented elements provide ground truth discontinuity.

The central hypothesis is that by augmenting training data and explicitly modeling discontinuity, they can make existing video frame interpolation networks perform better on videos containing discontinuous motion elements without degrading performance on regular continuous motion videos.


## What is the main contribution of this paper?

 The main contribution of this paper is proposing methods to make existing deep learning-based video frame interpolation (VFI) models robust to discontinuous motions in videos. The key ideas are:

- A new data augmentation strategy called Figure-Text Mixing (FTM) that mixes figures and text into training videos to teach models about discontinuous motions without needing extra datasets. 

- A lightweight module to predict a discontinuity map ($D$-map) that distinguishes between areas of continuous and discontinuous motion in a video. Pixels in discontinuous areas are directly copied from input frames instead of interpolated.

- Loss functions to supervise the discontinuity map prediction when the ground truth is known from FTM augmentation.

- Evaluation of these ideas applied to recent state-of-the-art VFI models like AdaCoF, CAIN, and VFIT on a new test set of videos with discontinuous motions. The proposed methods significantly improve interpolation quality compared to the original models, while maintaining competitive performance on standard benchmarks of continuous motions.

In summary, the key contribution is making deep VFI models, which normally assume only continuous motion, robust to discontinuous motions as well through novel training strategies and model architecture changes. This expands the applicability of deep VFI to more real-world videos with graphics, text, etc.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the key points from the paper:

The paper proposes three techniques - a novel data augmentation strategy called figure-text mixing (FTM), a lightweight module to predict a discontinuity map ($D$-map) that distinguishes continuous and discontinuous motion areas, and loss functions to supervise the discontinuous motion areas - to make existing deep learning video frame interpolation (VFI) architectures robust to discontinuous motions like logos, user interfaces, and subtitles, and shows these techniques significantly improve interpolation quality on both videos with discontinuous motions and standard benchmarks.


## How does this paper compare to other research in the same field?

 Here are some key observations on how this paper compares to other research in the field of video frame interpolation:

- This paper focuses specifically on handling discontinuous motions in videos, such as logos, user interfaces, subtitles, etc. Many prior works in video frame interpolation focus only on continuous motions and do not address handling discontinuous motions well.

- The paper proposes three main techniques to make video frame interpolation networks robust to discontinuous motions: (1) A new data augmentation strategy called Figure-Text Mixing (FTM) to help networks learn discontinuous motions during training. (2) A lightweight Discontinuity Map (D-map) module that predicts areas of discontinuous motion. (3) Loss functions to provide supervision for the D-map prediction.

- The paper demonstrates the general applicability of the proposed techniques by integrating them into several recent state-of-the-art video frame interpolation networks including warping-based, direct prediction-based, and transformer-based architectures.

- The paper constructs a new benchmark dataset called the Graphical Discontinuous Motion (GDM) dataset containing videos with abundant discontinuous motions to evaluate performance. Most prior benchmarks focus on continuous motions.

- Experiments show the proposed techniques significantly improve interpolation performance on the GDM dataset across different base network architectures. The techniques also improve or maintain performance on standard benchmarks with primarily continuous motions.

- Compared to some prior works that focus on handling discontinuous motions in specific video domains like cartoons, this work aims for more general applicability across different types of videos with discontinuous elements.

- Limitations of this work are that the problem formulation and solutions for discontinuous motions are still somewhat simplistic. The paper suggests further research on more fundamental understanding of discontinuous motion could lead to more optimal solutions.

In summary, a key contribution of this paper is extending the capabilities of video frame interpolation to handle discontinuous motions better, which has been lacking in most prior art. The proposed techniques demonstrate promising generalizability across network architectures and video domains.
