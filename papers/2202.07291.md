# [Exploring Discontinuity for Video Frame Interpolation](https://arxiv.org/abs/2202.07291)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question is how to make video frame interpolation networks robust to discontinuous motions like logos, user interfaces, subtitles, etc., which violate the assumption of continuous motion between frames. 

The key ideas proposed to address this question are:

1) A new data augmentation strategy called Figure-Text Mixing (FTM) that mixes synthetic discontinuous elements like static figures and moving text into the training data. This allows models to learn to handle discontinuous motion without needing extra training datasets.

2) A Discontinuity Map (D-map) module that predicts a 1-channel map indicating discontinuity between frames. The D-map allows switching between continuous motion interpolation and simply copying pixels from the previous frame for discontinuous areas.

3) Objective functions to provide supervision for the D-map prediction when FTM augmentation is used, since the augmented elements provide ground truth discontinuity.

The central hypothesis is that by augmenting training data and explicitly modeling discontinuity, they can make existing video frame interpolation networks perform better on videos containing discontinuous motion elements without degrading performance on regular continuous motion videos.


## What is the main contribution of this paper?

 The main contribution of this paper is proposing methods to make existing deep learning-based video frame interpolation (VFI) models robust to discontinuous motions in videos. The key ideas are:

- A new data augmentation strategy called Figure-Text Mixing (FTM) that mixes figures and text into training videos to teach models about discontinuous motions without needing extra datasets. 

- A lightweight module to predict a discontinuity map ($D$-map) that distinguishes between areas of continuous and discontinuous motion in a video. Pixels in discontinuous areas are directly copied from input frames instead of interpolated.

- Loss functions to supervise the discontinuity map prediction when the ground truth is known from FTM augmentation.

- Evaluation of these ideas applied to recent state-of-the-art VFI models like AdaCoF, CAIN, and VFIT on a new test set of videos with discontinuous motions. The proposed methods significantly improve interpolation quality compared to the original models, while maintaining competitive performance on standard benchmarks of continuous motions.

In summary, the key contribution is making deep VFI models, which normally assume only continuous motion, robust to discontinuous motions as well through novel training strategies and model architecture changes. This expands the applicability of deep VFI to more real-world videos with graphics, text, etc.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the key points from the paper:

The paper proposes three techniques - a novel data augmentation strategy called figure-text mixing (FTM), a lightweight module to predict a discontinuity map ($D$-map) that distinguishes continuous and discontinuous motion areas, and loss functions to supervise the discontinuous motion areas - to make existing deep learning video frame interpolation (VFI) architectures robust to discontinuous motions like logos, user interfaces, and subtitles, and shows these techniques significantly improve interpolation quality on both videos with discontinuous motions and standard benchmarks.
