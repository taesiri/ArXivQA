# [Exploring Discontinuity for Video Frame Interpolation](https://arxiv.org/abs/2202.07291)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question is how to make video frame interpolation networks robust to discontinuous motions like logos, user interfaces, subtitles, etc., which violate the assumption of continuous motion between frames. 

The key ideas proposed to address this question are:

1) A new data augmentation strategy called Figure-Text Mixing (FTM) that mixes synthetic discontinuous elements like static figures and moving text into the training data. This allows models to learn to handle discontinuous motion without needing extra training datasets.

2) A Discontinuity Map (D-map) module that predicts a 1-channel map indicating discontinuity between frames. The D-map allows switching between continuous motion interpolation and simply copying pixels from the previous frame for discontinuous areas.

3) Objective functions to provide supervision for the D-map prediction when FTM augmentation is used, since the augmented elements provide ground truth discontinuity.

The central hypothesis is that by augmenting training data and explicitly modeling discontinuity, they can make existing video frame interpolation networks perform better on videos containing discontinuous motion elements without degrading performance on regular continuous motion videos.


## What is the main contribution of this paper?

 The main contribution of this paper is proposing methods to make existing deep learning-based video frame interpolation (VFI) models robust to discontinuous motions in videos. The key ideas are:

- A new data augmentation strategy called Figure-Text Mixing (FTM) that mixes figures and text into training videos to teach models about discontinuous motions without needing extra datasets. 

- A lightweight module to predict a discontinuity map ($D$-map) that distinguishes between areas of continuous and discontinuous motion in a video. Pixels in discontinuous areas are directly copied from input frames instead of interpolated.

- Loss functions to supervise the discontinuity map prediction when the ground truth is known from FTM augmentation.

- Evaluation of these ideas applied to recent state-of-the-art VFI models like AdaCoF, CAIN, and VFIT on a new test set of videos with discontinuous motions. The proposed methods significantly improve interpolation quality compared to the original models, while maintaining competitive performance on standard benchmarks of continuous motions.

In summary, the key contribution is making deep VFI models, which normally assume only continuous motion, robust to discontinuous motions as well through novel training strategies and model architecture changes. This expands the applicability of deep VFI to more real-world videos with graphics, text, etc.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the key points from the paper:

The paper proposes three techniques - a novel data augmentation strategy called figure-text mixing (FTM), a lightweight module to predict a discontinuity map ($D$-map) that distinguishes continuous and discontinuous motion areas, and loss functions to supervise the discontinuous motion areas - to make existing deep learning video frame interpolation (VFI) architectures robust to discontinuous motions like logos, user interfaces, and subtitles, and shows these techniques significantly improve interpolation quality on both videos with discontinuous motions and standard benchmarks.


## How does this paper compare to other research in the same field?

 Here are some key observations on how this paper compares to other research in the field of video frame interpolation:

- This paper focuses specifically on handling discontinuous motions in videos, such as logos, user interfaces, subtitles, etc. Many prior works in video frame interpolation focus only on continuous motions and do not address handling discontinuous motions well.

- The paper proposes three main techniques to make video frame interpolation networks robust to discontinuous motions: (1) A new data augmentation strategy called Figure-Text Mixing (FTM) to help networks learn discontinuous motions during training. (2) A lightweight Discontinuity Map (D-map) module that predicts areas of discontinuous motion. (3) Loss functions to provide supervision for the D-map prediction.

- The paper demonstrates the general applicability of the proposed techniques by integrating them into several recent state-of-the-art video frame interpolation networks including warping-based, direct prediction-based, and transformer-based architectures.

- The paper constructs a new benchmark dataset called the Graphical Discontinuous Motion (GDM) dataset containing videos with abundant discontinuous motions to evaluate performance. Most prior benchmarks focus on continuous motions.

- Experiments show the proposed techniques significantly improve interpolation performance on the GDM dataset across different base network architectures. The techniques also improve or maintain performance on standard benchmarks with primarily continuous motions.

- Compared to some prior works that focus on handling discontinuous motions in specific video domains like cartoons, this work aims for more general applicability across different types of videos with discontinuous elements.

- Limitations of this work are that the problem formulation and solutions for discontinuous motions are still somewhat simplistic. The paper suggests further research on more fundamental understanding of discontinuous motion could lead to more optimal solutions.

In summary, a key contribution of this paper is extending the capabilities of video frame interpolation to handle discontinuous motions better, which has been lacking in most prior art. The proposed techniques demonstrate promising generalizability across network architectures and video domains.


## What future research directions do the authors suggest?

 Based on my reading of the paper, here are some of the main future research directions suggested by the authors:

- Developing more optimal solutions for handling discontinuous motions in video frame interpolation. The authors acknowledge limitations of their approaches, noting they are somewhat naive definitions of discontinuous motion. They suggest further research could focus on more fundamental natures of discontinuous motion and propose more optimal solutions.

- Expanding the capabilities of video frame interpolation networks to handle more complex motions beyond continuous and discontinuous motions. For example, the authors mention quadratic motion interpolation as an area for further work.

- Applying the proposed data augmentation techniques like Figure-Text Mixing (FTM) to other video processing tasks beyond just frame interpolation. The authors show FTM can help models learn about discontinuous motions without needing extra datasets. This idea could be extended to other tasks.

- Exploring how discontinuity maps and the copying mechanism can be incorporated into end-to-end learned models rather than as a separate module. This could potentially improve efficiency and performance.

- Evaluating the proposed methods on a wider diversity of video types and benchmarks to further demonstrate their generalizability. The authors introduce a new GDM dataset focused on discontinuous motions, but more evaluation is needed.

- Investigating how the ideas proposed could be adapted for higher resolution videos, as the experiments focused on relatively low resolutions around 256x256 pixels.

In summary, the authors point to several worthwhile directions to build upon their techniques for handling discontinuous motions in video frame interpolation. Advancing the robustness, efficiency, and generalizability of the approaches through further research on the nature of discontinuous motion and applications to other tasks is encouraged.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the key points from the paper:

This paper proposes three techniques to make existing deep learning-based video frame interpolation (VFI) methods robust to discontinuous motions like logos, subtitles, and user interfaces. First, they propose a data augmentation strategy called figure-text mixing (FTM) that mixes random figures and text into training videos to teach models about discontinuous motion. Second, they propose a lightweight module to predict a discontinuity map (D-map) that distinguishes continuous vs discontinuous motion areas. Pixels in discontinuous areas are copied from input frames rather than interpolated. Third, they propose loss functions to supervise the D-map prediction when using FTM augmentation. They test their methods by adding them to state-of-the-art VFI networks. The augmented networks achieve significantly improved performance on a new discontinuous motion dataset they collected, as well as standard continuous motion benchmarks like Vimeo90K. The proposed techniques make VFI networks robust to both continuous and discontinuous motions.
