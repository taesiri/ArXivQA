# [KnowTuning: Knowledge-aware Fine-tuning for Large Language Models](https://arxiv.org/abs/2402.11176)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Large language models (LLMs) have shown success on many NLP tasks but still struggle with effectively leveraging knowledge for knowledge-intensive question answering. 
- They are prone to generating incomplete, non-factual, or illogical answers, stemming from inadequate knowledge awareness during vanilla fine-tuning.

Proposed Solution - KnowTuning:  
- A novel knowledge-aware fine-tuning method to explicitly and implicitly improve knowledge awareness of LLMs.
- Includes explicit knowledge generation stage to train LLMs to identify knowledge triples in answers.
- Implements implicit knowledge comparison across 3 aspects - completeness, factuality, logicality. Constructs comparison sets by deleting/revising/shuffling triples to create worse answers in those aspects.  

Main Contributions:
- Proposes KnowTuning for knowledge-aware fine-tuning to enhance knowledge awareness of LLMs.
- Demonstrates effectiveness on general and medical QA datasets through automatic and human evaluation.
- Shows KnowTuning improvements generalize to unseen QA datasets.
- Provides ablation studies analyzing impact of different components of KnowTuning.

In summary, this paper identifies inadequate knowledge awareness as a key limitation of LLMs on knowledge-intensive QA and introduces a novel fine-tuning approach KnowTuning that explicitly and implicitly trains knowledge awareness across multiple facets. Extensive experiments validate the effectiveness of KnowTuning in enhancing LLM performance on such knowledge-intensive QA tasks.
