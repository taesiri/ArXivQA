# [Learning to Answer Visual Questions from Web Videos](https://arxiv.org/abs/2205.05019)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the main research questions/hypotheses of this paper are:1) Can VideoQA models be trained effectively without large amounts of manually annotated visual data by instead leveraging large amounts of readily available narrated video data? 2) Can strong zero-shot VideoQA performance be achieved by pretraining a model on automatically generated VideoQA data and then evaluating it on unseen target datasets without any finetuning on those target datasets?3) How does pretraining a VideoQA model on automatically generated data compare to pretraining on other large video-text datasets without QA annotations or transferring from existing manually annotated VQA datasets? 4) Does the proposed automatic VideoQA dataset generation approach generalize to different sources of web video data beyond just narrated instructional videos?5) Does the introduction of a new manually annotated VideoQA benchmark with multiple answers per question and reduced language bias provide additional insights for VideoQA research?In summary, the key goals seem to be 1) proposing a scalable approach to VideoQA training without manual annotation, 2) demonstrating strong zero-shot transfer, 3) benchmarking against other pretraining strategies, 4) showing generalization to different web video data, and 5) introducing a new challenging VideoQA benchmark.


## What is the main contribution of this paper?

The main contributions of this paper are:1. It proposes an approach to automatically generate a large-scale video question answering (VideoQA) dataset called HowToVQA69M from narrated instructional videos. The method uses transformers trained on text data to generate question-answer pairs from video narrations. 2. It introduces a training procedure for VideoQA models using contrastive learning between a multi-modal video-question transformer model and an answer transformer model. This allows handling the large vocabulary of possible answers in the generated dataset.3. It demonstrates strong zero-shot performance of the model trained on HowToVQA69M on existing VideoQA datasets. It also proposes a VideoQA feature probe evaluation setting.4. The pretrained model achieves competitive results when finetuned on several VideoQA benchmarks like MSRVTT-QA, ActivityNet-QA, MSVD-QA and How2QA.5. It shows the approach can be applied to other sources of web video data by generating the WebVidVQA3M dataset from alt-text video descriptions.6. It presents the iVQA benchmark for evaluating video question answering, which is manually annotated and avoids language biases.In summary, the main contribution is an automated approach to generate large-scale training data for VideoQA models from web videos with readily available narrations or descriptions. The pretraining enables competitive performance on existing benchmarks.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the main points from the paper:The paper proposes an approach to automatically generate large-scale training data for video question answering by leveraging language models and cross-modal supervision from readily available narrated videos or videos with alt-text, trains a multi-modal videoQA model on this data via contrastive learning, and demonstrates strong zero-shot generalization as well as competitive results when finetuned on existing VQA datasets.
