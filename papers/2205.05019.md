# [Learning to Answer Visual Questions from Web Videos](https://arxiv.org/abs/2205.05019)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the main research questions/hypotheses of this paper are:1) Can VideoQA models be trained effectively without large amounts of manually annotated visual data by instead leveraging large amounts of readily available narrated video data? 2) Can strong zero-shot VideoQA performance be achieved by pretraining a model on automatically generated VideoQA data and then evaluating it on unseen target datasets without any finetuning on those target datasets?3) How does pretraining a VideoQA model on automatically generated data compare to pretraining on other large video-text datasets without QA annotations or transferring from existing manually annotated VQA datasets? 4) Does the proposed automatic VideoQA dataset generation approach generalize to different sources of web video data beyond just narrated instructional videos?5) Does the introduction of a new manually annotated VideoQA benchmark with multiple answers per question and reduced language bias provide additional insights for VideoQA research?In summary, the key goals seem to be 1) proposing a scalable approach to VideoQA training without manual annotation, 2) demonstrating strong zero-shot transfer, 3) benchmarking against other pretraining strategies, 4) showing generalization to different web video data, and 5) introducing a new challenging VideoQA benchmark.


## What is the main contribution of this paper?

The main contributions of this paper are:1. It proposes an approach to automatically generate a large-scale video question answering (VideoQA) dataset called HowToVQA69M from narrated instructional videos. The method uses transformers trained on text data to generate question-answer pairs from video narrations. 2. It introduces a training procedure for VideoQA models using contrastive learning between a multi-modal video-question transformer model and an answer transformer model. This allows handling the large vocabulary of possible answers in the generated dataset.3. It demonstrates strong zero-shot performance of the model trained on HowToVQA69M on existing VideoQA datasets. It also proposes a VideoQA feature probe evaluation setting.4. The pretrained model achieves competitive results when finetuned on several VideoQA benchmarks like MSRVTT-QA, ActivityNet-QA, MSVD-QA and How2QA.5. It shows the approach can be applied to other sources of web video data by generating the WebVidVQA3M dataset from alt-text video descriptions.6. It presents the iVQA benchmark for evaluating video question answering, which is manually annotated and avoids language biases.In summary, the main contribution is an automated approach to generate large-scale training data for VideoQA models from web videos with readily available narrations or descriptions. The pretraining enables competitive performance on existing benchmarks.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the main points from the paper:The paper proposes an approach to automatically generate large-scale training data for video question answering by leveraging language models and cross-modal supervision from readily available narrated videos or videos with alt-text, trains a multi-modal videoQA model on this data via contrastive learning, and demonstrates strong zero-shot generalization as well as competitive results when finetuned on existing VQA datasets.


## How does this paper compare to other research in the same field?

Here are a few key ways this paper compares to other research in video question answering:- Dataset scale: This paper introduces two large-scale automatically generated datasets - HowToVQA69M with 69M video-question-answer triplets, and WebVidVQA3M with 3.5M triplets. These are orders of magnitude larger than previous VideoQA datasets, which have typically been limited to hundreds of thousands of samples due to the expense of manual annotation. The large scale of these datasets allows the authors' model to be pretrained in a task-specific manner.- Learning framework: The paper proposes a contrastive learning framework to directly model open-ended answers instead of casting VideoQA as a classification problem over a fixed set of possible answers. This allows their model to handle the full diversity of free-form answers in the large-scale datasets. Other recent works have typically reduced VideoQA to a classification problem with a limited output space.- Generalization: The paper demonstrates strong generalization through zero-shot evaluation to unseen datasets and a VideoQA feature probe evaluation setting. Most prior works have focused on supervised finetuning and evaluation on existing benchmarks. The zero-shot and feature probe settings better evaluate how well the models generalize.- Data source: The datasets are generated automatically from web videos with readily available narrations and alt-text, rather than expensive manual annotations. Using alternative sources of inexpensive supervision for pretraining is an important direction for scaling up VideoQA.- Model architecture: The paper uses a dual transformer architecture with joint video-question and answer encoders. Other recent works have explored different architectures like recurrent networks or graph neural networks. The transformer allows implicitly modeling temporal structure.Overall, the large-scale datasets, contrastive learning framework, generalization evaluation, and use of weak supervision point to promising research directions for advancing VideoQA. The model architecture is also representative of the trend toward transformers for multimodal tasks.
