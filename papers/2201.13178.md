# [Few-Shot Backdoor Attacks on Visual Object Tracking](https://arxiv.org/abs/2201.13178)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is: Can visual object tracking (VOT) models be vulnerable to backdoor attacks through data poisoning during training?The key hypothesis is that an adversary can implant hidden backdoors into VOT models by injecting a trigger pattern into a small subset of training samples. When the trigger appears at test time, it will cause the backdoored model to fail in tracking the target object.To summarize, the main research goals of this paper are:1) Reveal the backdoor threat in visual object tracking models caused by potential data poisoning attacks during training. 2) Design an effective backdoor attack that can degrade the tracking performance of VOT models by adding a trigger pattern to just a few frames.3) Empirically evaluate the attack under different settings and test if it can bypass potential defenses.Overall, this is the first work that studies backdoor attacks against visual object tracking models, an important type of model for many real-world applications. The results highlight the vulnerability of outsourced training and using third-party VOT models.


## What is the main contribution of this paper?

This paper proposes a few-shot backdoor attack (FSBA) against visual object tracking (VOT) models. The main contributions are:1. It reveals the vulnerability of VOT models to backdoor attacks caused by outsourced training or using third-party pre-trained models. 2. It proposes a simple yet effective few-shot untargeted backdoor attack that can significantly degrade the tracking performance by just attaching a trigger pattern to the target object in one or a few frames. 3. It treats the attack task as multi-task learning, which alternately maximizes a feature loss in the hidden space while preserving the tracking performance. This allows both attack effectiveness and stealthiness.4. It empirically shows that the proposed attack is effective in both digital and physical worlds, and resistant to potential defenses.5. To the best of the authors' knowledge, this is the first backdoor attack against VOT models and video-based computer vision tasks. The attack methodology provides a new perspective for designing backdoor attacks.In summary, the key contribution is proposing the first few-shot untargeted backdoor attack against visual object tracking models, revealing their vulnerability to training-time threats. The attack is designed to be effective yet stealthy and works well in both digital and physical worlds.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper proposes a few-shot backdoor attack against siamese network based visual object tracking models, which can effectively degrade the tracking performance by attaching an adversarial trigger pattern to the target object in only one or a few frames.


## How does this paper compare to other research in the same field?

Here are some key comparisons between this paper and other related research in the field of backdoor attacks on visual object tracking:- This is the first paper to propose a backdoor attack specifically designed for visual object tracking (VOT). Most prior work has focused on backdoor attacks for image classification. The authors highlight that directly applying classification-based attacks to VOT models is often ineffective. - The attack is untargeted, meaning the goal is to simply cause tracking failure rather than misclassify to a specific target label. This is better suited to VOT since the tracked objects are often not known ahead of time.- The attack uses a novel "feature loss" to cause separation in the feature space between clean and poisoned frames. This helps ensure the attack will work even if the trigger only appears briefly. Prior attacks like BadNets rely solely on modifying the classification loss.- The attack achieves strong one-shot or few-shot effectiveness on multiple SOTA VOT models like SiamRPN++. Most prior attacks require the trigger to be present more extensively.- The attack is evaluated in both digital and physical attack settings. Many prior works only show digital effectiveness. Physical validity is important for understanding real-world implications.- The attack is shown to be resistant to potential defenses like fine-tuning and input preprocessing. This demonstrates it is a strong attack baseline future defenses should aim to mitigate.Overall, this paper makes significant contributions by proposing the first backdoor attack specialized for VOT models. The attack methodology and evaluation establish a strong new baseline for this threat model going forward.
