# [Few-Shot Backdoor Attacks on Visual Object Tracking](https://arxiv.org/abs/2201.13178)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is: Can visual object tracking (VOT) models be vulnerable to backdoor attacks through data poisoning during training?The key hypothesis is that an adversary can implant hidden backdoors into VOT models by injecting a trigger pattern into a small subset of training samples. When the trigger appears at test time, it will cause the backdoored model to fail in tracking the target object.To summarize, the main research goals of this paper are:1) Reveal the backdoor threat in visual object tracking models caused by potential data poisoning attacks during training. 2) Design an effective backdoor attack that can degrade the tracking performance of VOT models by adding a trigger pattern to just a few frames.3) Empirically evaluate the attack under different settings and test if it can bypass potential defenses.Overall, this is the first work that studies backdoor attacks against visual object tracking models, an important type of model for many real-world applications. The results highlight the vulnerability of outsourced training and using third-party VOT models.


## What is the main contribution of this paper?

This paper proposes a few-shot backdoor attack (FSBA) against visual object tracking (VOT) models. The main contributions are:1. It reveals the vulnerability of VOT models to backdoor attacks caused by outsourced training or using third-party pre-trained models. 2. It proposes a simple yet effective few-shot untargeted backdoor attack that can significantly degrade the tracking performance by just attaching a trigger pattern to the target object in one or a few frames. 3. It treats the attack task as multi-task learning, which alternately maximizes a feature loss in the hidden space while preserving the tracking performance. This allows both attack effectiveness and stealthiness.4. It empirically shows that the proposed attack is effective in both digital and physical worlds, and resistant to potential defenses.5. To the best of the authors' knowledge, this is the first backdoor attack against VOT models and video-based computer vision tasks. The attack methodology provides a new perspective for designing backdoor attacks.In summary, the key contribution is proposing the first few-shot untargeted backdoor attack against visual object tracking models, revealing their vulnerability to training-time threats. The attack is designed to be effective yet stealthy and works well in both digital and physical worlds.
