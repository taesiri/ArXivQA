# [APIServe: Efficient API Support for Large-Language Model Inferencing](https://arxiv.org/abs/2402.01869)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Large language models (LLMs) are being augmented with external APIs to extend their capabilities, such as arithmetic calculation, question answering, virtual environment interaction, etc.  
- Existing LLM inference systems are not designed to handle these API calls efficiently. They treat an API call as terminating the current request, discarding context, and later resuming as a new request when the API returns. This causes unnecessary recomputation of context and wastes GPU memory and resources.

Proposed Solution:
- The paper proposes \trick, the first LLM inference framework designed for API-augmented LLMs. \trick aims to minimize GPU memory waste caused by API calls so the saved GPU memory can be used to serve more requests.

Key Contributions:

1. Formulates equations to quantify the GPU memory waste of existing techniques like discarding context (\discard) and preserving context (\preserve) during API calls.

2. Proposes improved techniques to reduce waste:
   - Swap pipelining and chunking: Swaps context to CPU in a pipelined manner and splits swapping across iterations to hide swapping latency. Eliminates nearly all GPU waste of swapping.
   - Recomputation chunking: Splits discarded context into small chunks and mixes their recomputation with normal token decoding to improve GPU utilization. Reduces recomputation waste by over 60\%.

3. At run-time, dynamically chooses between preserve, improved swap and improved discard for each API-paused request to minimize total memory waste across all requests, while ensuring fairness.

Results:
- Implemented on top of state-of-the-art vLLM system. 
- Improves throughput by 1.6x for 6B model and 1.8x for 13B model over vLLM baseline, while reducing latency.
- Achieves over 2x more completed requests per second compared to baselines.

In summary, the paper proposes novel techniques in \trick to efficiently handle API calls in LLMs by minimizing GPU memory waste, and demonstrates significant improvements over existing LLM inference systems.
