# [Adversarial Training with OCR Modality Perturbation for Scene-Text   Visual Question Answering](https://arxiv.org/abs/2403.09288)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Scene Text Visual Question Answering (ST-VQA) aims to understand scene text in images and answer related questions. 
- Existing methods rely heavily on optical character recognition (OCR) which can have errors due to issues like blurring, distortion, etc. in scene images.  
- Aggressive fine-tuning on limited spatial information and erroneous OCR texts leads to overfitting.

Proposed Solution:
- Proposes an adversarial training architecture called ATS tailored for ST-VQA.  
- Introduces an Adversarial OCR Enhancement (AOE) module that uses adversarial training in OCR embedding space to make the representations fault-tolerant to OCR errors.
- Incorporates a Spatial-Aware Self-Attention (SASA) mechanism in AOE to help capture spatial relationships between OCR tokens.
- Uses adversarial training as regularization to improve generalization and reduce overfitting.

Main Contributions:
- First to design adversarial training architectures for ST-VQA to learn fault-tolerant OCR representations.
- Uses relative position embeddings instead of absolute embeddings in SASA to provide broader perspective for spatial modeling.   
- Achieves new state-of-the-art results on ST-VQA and TextVQA benchmarks, demonstrating effectiveness.
- Provides a novel paradigm for multimodal adversarial training.

In summary, the paper proposes a novel adversarial training framework with spatial modeling capabilities tailored for ST-VQA that achieves superior performance over existing methods. The main ideas are enhancing fault tolerance using adversarial OCR perturbations and spatial-aware self-attention.
