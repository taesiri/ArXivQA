# [Pre-training for Ad-hoc Retrieval: Hyperlink is Also You Need](https://arxiv.org/abs/2108.09346)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central research question this paper tries to address is: How to design effective pre-training objectives tailored for ad-hoc retrieval by leveraging the supervised signals from hyperlinks and anchor texts?The key hypothesis is that:Leveraging hyperlinks and anchor texts to construct pre-training objectives can provide more accurate and reliable supervised signals compared to existing methods, and better resemble the downstream ad-hoc retrieval task. This can help the pre-trained language model learn better query-document matching ability and further improve the ranking performance when fine-tuned on downstream retrieval tasks.Specifically, the paper proposes four pre-training tasks based on hyperlinks to capture different views of anchor-document relevance:1) Representative query prediction 2) Query disambiguation modeling3) Representative document prediction4) Anchor co-occurrence modelingBy pre-training the Transformer model to predict pair-wise preferences on the constructed query-document pairs, the model can learn query-document matching from the anchor-document correlations. This provides a better initialization for fine-tuning on downstream ranking tasks compared to pre-training objectives not tailored for IR.The central hypothesis is that introducing supervised signals from hyperlinks into pre-training objectives can boost the ranking performance when fine-tuned on ad-hoc retrieval tasks. Experiments on two datasets validate this hypothesis and show significant improvements over strong baselines.


## What is the main contribution of this paper?

The main contribution of this paper is proposing a novel pre-training framework called HARP (Pre-training objectives for ad-hoc Retrieval with Anchor texts and Hyperlinks) tailored for ad-hoc retrieval. Specifically, the key ideas and contributions are:- Proposing to leverage the supervised signals brought by hyperlinks and anchor texts to design pre-training objectives for ad-hoc retrieval. This provides more accurate and reliable training data compared to existing methods like query likelihood sampling.- Designing four pre-training tasks based on hyperlinks to capture anchor-document relevance from different perspectives:  1) Representative query prediction  2) Query disambiguation    3) Representative document prediction  4) Anchor co-occurrence modeling- Pre-training the Transformer model on large-scale pseudo query-document pairs constructed by the four pre-training tasks jointly with masked language modeling. This allows the model to learn query-document matching during pre-training.- Conducting experiments on MS MARCO and TREC DL datasets. Results show that HARP significantly outperforms state-of-the-art methods including BERT and previous pre-training objectives tailored for IR. This demonstrates the effectiveness of introducing hyperlinks for pre-training in improving ad-hoc retrieval performance.In summary, the main contribution is proposing a novel pre-training framework that utilizes hyperlinks to generate high-quality training data and designs IR-oriented pre-training objectives to learn query-document matching, which leads to improved performance on ad-hoc retrieval tasks.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper proposes a novel pre-training framework called HARP that leverages supervised signals from hyperlinks and anchor texts to design four pre-training tasks to learn query-document relevance matching, then fine-tunes the pre-trained model on downstream ad-hoc retrieval tasks and demonstrates improved performance.


## How does this paper compare to other research in the same field?

Here are my thoughts on how this paper compares to other research in the field of pre-training for information retrieval (IR):- The main novelty of this paper is using hyperlinks and anchor texts for pre-training tailored to IR tasks. Most prior work on pre-training for IR uses objectives like inverse cloze task, predicting masked words, or sampling word sets as pseudo-queries. Leveraging anchor texts as a source of supervision is a unique approach.- The idea of using anchor text as queries and target pages as relevant documents has been explored before in IR, but not for pre-training objectives. This paper is the first to design pre-training tasks around anchor-document pairs.- The proposed pre-training approach HARP uses four novel objectives: representative query prediction, query disambiguation, representative document prediction, and anchor co-occurrence modeling. These capture different useful signals compared to prior pre-training tasks.- Existing pre-training objectives like masked language modeling are more general. HARP focuses specifically on retrieving documents for queries, making it better suited for ad-hoc retrieval.- The most related prior work is PROP, which also aims to pre-train for ad-hoc IR. But PROP samples pseudo-queries based on document words. HARP uses human-generated anchor text as queries instead.- Experiments show HARP achieves new state-of-the-art results on MS MARCO and TREC DL, outperforming PROP and other baselines. This demonstrates the effectiveness of leveraging anchor text for pre-training.Overall, the use of anchor text provides a novel and effective source of supervision for pre-training tailored to IR. The proposed objectives are well-motivated to simulate query-document matching. This paper presents a unique pre-training approach with strong empirical results.
