# [PIRLNav: Pretraining with Imitation and RL Finetuning for ObjectNav](https://arxiv.org/abs/2301.07302)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading, the central research question this paper tries to address is: How can we effectively combine imitation learning (IL) from human demonstrations and reinforcement learning (RL) to develop policies for object goal navigation that efficiently learn to explore and generalize to novel environments?

Specifically, the key research questions seem to be:

- Can we use IL on human demonstrations to pretrain policies, and then use RL to finetune them for better generalization compared to just IL? 

- Do human demonstrations capture critical navigation behaviors and exploration strategies compared to other "free" sources of demonstrations like shortest paths or frontier exploration?

- How does the performance after RL finetuning scale with the amount of human demonstrations used for IL pretraining? Can we identify the point of diminishing returns to avoid excessive data collection?

- What are the main failure modes of policies trained with this IL-RL approach, and how can we further improve them?

The central hypothesis seems to be that combining IL and RL in a principled manner can lead to policies that explore intelligently like humans, while also being more scalable than pure IL, leading to state-of-the-art performance on object goal navigation. The experiments aim to substantiate this hypothesis.


## What is the main contribution of this paper?

 The main contribution of this paper is presenting PIRLNav, a two-stage learning scheme for ObjectGoal navigation (ObjNav) that involves behavior cloning (BC) pretraining on human demonstrations followed by reinforcement learning (RL) finetuning. 

The key highlights are:

- PIRLNav achieves state-of-the-art results on ObjNav, improving success rate by 5% over prior work. 

- It proposes an effective methodology for combining imitation learning (IL) and RL that involves a critic-only learning phase and gradual transition to training both actor and critic. This overcomes challenges like destructive policy updates that arise when naively finetuning a BC pretrained policy with RL.

- It provides an extensive empirical analysis on the impact of different demonstration datasets (human, shortest paths, frontier exploration) for IL pretraining on downstream RL finetuning performance. The results show that human demonstrations enable better generalization compared to other demonstration sources.

- It studies the scaling behavior with increasing BC dataset size and shows diminishing returns from RL finetuning as BC performance saturates. This suggests possibilities for achieving good performance without large-scale human demonstration collection.

- It analyzes failure modes to provide insights into further improving ObjNav performance through better annotations, navigation, and recognition capabilities.

In summary, the key contribution is the proposed PIRLNav approach that combines IL and RL effectively to achieve state-of-the-art on a challenging embodied navigation task like ObjNav. The empirical analysis also provides useful insights on how to productively utilize human and automatic demonstrations.
