# [PIRLNav: Pretraining with Imitation and RL Finetuning for ObjectNav](https://arxiv.org/abs/2301.07302)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading, the central research question this paper tries to address is: How can we effectively combine imitation learning (IL) from human demonstrations and reinforcement learning (RL) to develop policies for object goal navigation that efficiently learn to explore and generalize to novel environments?

Specifically, the key research questions seem to be:

- Can we use IL on human demonstrations to pretrain policies, and then use RL to finetune them for better generalization compared to just IL? 

- Do human demonstrations capture critical navigation behaviors and exploration strategies compared to other "free" sources of demonstrations like shortest paths or frontier exploration?

- How does the performance after RL finetuning scale with the amount of human demonstrations used for IL pretraining? Can we identify the point of diminishing returns to avoid excessive data collection?

- What are the main failure modes of policies trained with this IL-RL approach, and how can we further improve them?

The central hypothesis seems to be that combining IL and RL in a principled manner can lead to policies that explore intelligently like humans, while also being more scalable than pure IL, leading to state-of-the-art performance on object goal navigation. The experiments aim to substantiate this hypothesis.
