# [PDFTriage: Question Answering over Long, Structured Documents](https://arxiv.org/abs/2309.08872)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be: How can language models be improved to better handle question answering over long, structured documents where the full document exceeds the model's context length limit?The authors identify a key issue that large language models like GPT-3 struggle with document QA when the full document cannot fit within the model's context window. They state:"Large Language Models (LLMs) have issues with document question answering (QA) in situations where the document is unable to fit in the small context length of an LLM."To address this issue, the main contribution of the paper seems to be proposing an approach called "PDFTriage" to enable models to selectively retrieve relevant context from structured documents based on content and document structure. The key hypothesis behind PDFTriage seems to be that representing documents as plain text is incongruous with a user's mental model of structured documents like PDFs, webpages, etc. By giving the model access to metadata about document structure and functions to query structure, the authors hypothesize this will improve performance on certain question types compared to plain text retrieval baselines.The paper introduces the PDFTriage technique and conducts experiments on a new dataset of 900+ human-generated questions to evaluate if PDFTriage improves performance over baseline methods on various question categories.So in summary, the central research question is how to improve document QA for LLMs when the full document exceeds the model's context length, with a hypothesis that leveraging document structure metadata and querying will help for certain question types. The PDFTriage technique is proposed and evaluated to test this hypothesis.


## What is the main contribution of this paper?

Based on my review of the paper, the main contributions appear to be:- Proposing the PDFTriage approach for question answering over long, structured documents. PDFTriage converts PDF documents into structured metadata and allows models to retrieve relevant context using functions like fetch_pages() and fetch_table(). This allows the model to leverage document structure for QA.- Releasing a new benchmark dataset for document QA consisting of 900+ human-generated questions over 80 structured documents across 10 categories. This facilitates further research on document QA. - Demonstrating through experiments that PDFTriage-augmented models can reliably answer several classes of questions that plain retrieval-augmented LLMs fail on. The experiments show PDFTriage improves answer quality, accuracy, readability and informativeness compared to page and chunk retrieval baselines.In summary, the key contribution seems to be presenting the PDFTriage technique to enable models to leverage document structure for more effective question answering over long, structured documents. The new dataset and experiments help demonstrate the value of this approach.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Based on a brief skim of the paper, here is a one sentence summary:This paper provides instructions and guidelines for authors to format their accepted papers for AAAI conference proceedings and technical reports using LaTeX, including details on style files, formatting requirements, copyright forms, and tips for ensuring papers meet submission standards.


## How does this paper compare to other research in the same field?

Here are a few thoughts on how this paper compares to other research in its field:- It builds on previous work in structured document question answering. The authors cite relevant prior work like DocVQA, DUDE, and QASPER that have also created datasets and models for document QA. This paper aims to expand on those efforts by handling a broader range of question types that require reasoning across document structure.- The focus on leveraging document structure makes it distinct from a lot of QA research that uses plain text documents. Many datasets like SQuAD and Natural Questions do not have complex document structure for models to reason over. So this paper's approach stands out for explicitly handling things like tables, figures, and sections.- ThePrompt+Retrieve approach is common in recent QA systems, but this paper uses retrieval in a more specialized way. Instead of generic passage retrieval, it retrieves specific structural elements like pages, tables, figures etc. This allows more precise context selection.- The model performances are strong but not state-of-the-art. The goal seems to be demonstrating a capability more than maximizing performance. There is room to build on this work by training more optimized models.- The human evaluation component is valuable for QA. Many papers rely only on automated metrics, but this paper includes human judgments and preferences which give a better sense of quality.- The authors are releasing the dataset which is important for advancing work in this area. Previous datasets have limitations, so new datasets enable progress.Overall, this paper makes a solid contribution to an important niche problem in QA. It demonstrates some novel ideas for handling complex documents that should inspire more research on this challenging task. The techniques still need refinement to maximize performance, but the foundations seem promising.


## What future research directions do the authors suggest?

Based on my reading of the paper, here are some of the key future research directions suggested by the authors:- Developing multi-modal approaches that incorporate table and figure information into GPT-4 question-answering for documents. The authors note that their current approach focuses primarily on textual content, but incorporating non-textual elements like tables and figures could further improve performance on certain types of questions.- Incorporating question type in the PDFTriage approach to improve efficiency and efficacy. The authors suggest tailoring the retrieval and prompting strategies based on the type of question being asked, rather than using a one-size-fits-all approach. This could help optimize performance for different question categories.- Exploring few-shot prompt-tuning to better align generative LLMs with human preferences in evaluation tasks. The authors find that off-the-shelf GPT evaluation does not correlate well with human judgments, indicating room for improvement via techniques like prompt tuning.- Developing more robust multi-stage reasoning capabilities to handle complex multi-hop questions. While PDFTriage shows promise on certain multi-step questions, there is still room to expand the capabilities to handle more complex reasoning.- Expanding the diversity of document types beyond born-digital PDFs, such as scanned documents, web pages, presentations, etc. Testing the approach on a wider range of document formats would demonstrate more robust generalization.- Incorporating explainability to provide users transparency into the reasoning process. The authors do not focus on explainability in this work, but surfacing the intermediate retrieval steps could improve trustworthiness.Overall, the authors lay out a promising research agenda focused on expanding multi-modal reasoning, optimization based on question types, leveraging human preferences, and testing on more complex questions and diverse documents. Advancing these areas could significantly improve document QA abilities.
