# [Outlier-Aware Training for Low-Bit Quantization of Structural   Re-Parameterized Networks](https://arxiv.org/abs/2402.07200)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Structural re-parameterization networks like RepVGG have high accuracy but suffer from poor quantization performance due to the merging process which introduces outliers into the weights. 
- These outliers make the weight distribution distinct from conventional CNNs, heightening difficulties for quantization, especially at lower bitwidths (<8 bits).

Proposed Solution:
- Propose an Outlier Aware Batch Normalization (OABN) operator to suppress outliers during training by clipping the γ parameter in BN based on the running variance σ2. This makes RepVGG compatible with common PTQ methods.

- Also propose a ClusterQAT framework which incorporates clustering into quantization-aware training to dynamically adjust quantization intervals and preserve distribution patterns in weights for lower bit quantization.

Main Contributions:
- Identify cause of poor quantization in RepVGG originating from the identity BN branches that introduce outliers. Propose OABN to remove these.

- Show OABN enables high 8-bit PTQ performance on RepVGG improving upon prior work. Also enables feasibility of lower bit quantization down to 3-bits using proposed ClusterQAT technique.

- OABN has lower compute cost than prior QARepVGG technique while enabling better accuracy under constrained quantization scenarios with fewer weight states.

- Joint OABN and ClusterQAT framework provides an efficient training pipeline to address difficulties in low-bit quantization of structural re-parameterization networks like RepVGG.

In summary, the key novelty is identifying the source of outliers in RepVGG and proposing both algorithmic (OABN) and quantization (ClusterQAT) solutions to enable high accuracy low-bit quantization while lowering training cost.
