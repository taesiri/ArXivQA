# [In-Context Example Ordering Guided by Label Distributions](https://arxiv.org/abs/2402.11447)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem Statement
- In-context learning (ICL) with large language models shows promise for few-shot learning, but is sensitive to the choice and order of in-context examples. Using the same examples but different orderings can result in performance varying from near state-of-the-art to random guessing. 

- Prior work on optimizing example orderings requires additional labeled data beyond the in-context examples. 

- This paper examines how to select good orderings using only the in-context examples themselves and model probability outputs.

Proposed Solution
- Formulate in-context ordering as an optimization problem under 3 settings: 
   1) Only in-context examples
   2) Additional unlabeled examples 
   3) Additional unlabeled & prior label distribution

- Propose 2 principles for ordering guided by model probability predictions:
   1) Ordering should have minimum bias (KL divergence to uniform distribution) on null input  
   2) Observed label distribution on unlabeled data should match informative prior

- Instantiate principles into a scoring function and sampling-based selection method called Probability Distribution Ordering (PDO)

- Apply PDO with both direct probability and PMI scoring methods

Contributions 
- Show PDO improves accuracy and reduces variance across 13 classification datasets and 9 LLMs
- PDO reduces model miscalibration, improving reliability
- PDO matches performance of prior methods needing labeled data, using only model probability outputs 
- Analysis shows PDO can also improve example selection for a task over random baselines

Main conclusion is that optimizing example orderings based solely on model probability outputs is an effective approach to enhance ICL, without needing additional labeled data.
