# [MISA: Unveiling the Vulnerabilities in Split Federated Learning](https://arxiv.org/abs/2312.11026)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Federated learning (FL) and split learning (SL) are popular distributed machine learning paradigms but have limitations. FL requires high compute resources at edge devices while SL has low efficiency.
- Split federated learning (SFL) combines FL and SL to address these issues. Prior work claims SFL is robust to poisoning attacks. 

Proposed Solution:
- The paper proposes a novel poisoning attack called MISA that attacks both the top and bottom models in SFL to cause a misalignment and accuracy drop.

- For the top model, malicious smashed data is crafted using the mean and variance of benign data to introduce bias. 

- For the bottom model, the attack is formalized as an optimization problem. Thompson sampling is used to dynamically select the best perturbation direction. A new method called SnL combines coarse search and fine localization to find the optimal perturbation magnitude.

Key Contributions:
- MISA demonstrates SFL's vulnerability to poisoning attacks, challenging prior beliefs of its robustness.

- MISA jointly poisons top and bottom models to induce misalignment, unlike prior work that only poisons the bottom model.

- Extensive experiments show MISA causes higher accuracy drops than state-of-the-art attacks under various configurations. An ablation study shows joint top and bottom model poisoning causes even higher degradation than separate attacks.

In summary, the key insight is poisoning both top and bottom models in SFL can lead to significant accuracy drops. The proposed MISA attack challenges assumptions about SFL's robustness and underscores the need for further defense research.


## Summarize the paper in one sentence.

 This paper proposes a novel poisoning attack called MISA that effectively attacks both the top and bottom models in split federated learning, causing misalignment in the global model and leading to significant accuracy drops.


## What is the main contribution of this paper?

 The main contribution of this paper is proposing a novel poisoning attack against split federated learning (SFL) called MISA. Specifically:

- MISA is the first attack that simultaneously poisons both the top model and bottom model in SFL, causing a misalignment that leads to a significant drop in model accuracy. This challenges the belief that SFL is robust to poisoning attacks.

- MISA introduces new techniques for poisoning the top model via malicious smashed data and poisoning the bottom model by formulating it as an optimization problem and using Thompson sampling and a "search and locate" method to solve it.

- Extensive experiments show that MISA causes much higher accuracy drops compared to state-of-the-art attacks across different aggregation algorithms, model split positions, attacker ratios, and data distributions. This demonstrates the superiority and practicality of MISA.

In summary, the key contribution is unveiling vulnerabilities in SFL to poisoning attacks via the proposed MISA attack, which is highly effective in practice. This calls attention to the need for securing SFL systems.


## What are the keywords or key terms associated with this paper?

 Based on reviewing the paper, I identify the following key terms and keywords associated with it:

Keywords:
- Split Federated Learning
- Poisoning Attack  
- Robustness

Key Terms:
- Federated Learning (FL)
- Split Learning (SL)  
- Split Federated Learning (SFL)
- Main server
- Fed server
- Bottom model  
- Top model
- Cut layer
- Smashed data
- MISA (poisoning attack proposed in paper)
- Availability 
- Min-Sum optimization
- Thompson Sampling
- SnL (Search and Locate)

The paper proposes a new poisoning attack called MISA against split federated learning systems. The key focus areas are the vulnerability and robustness of split federated learning, and analyzing the impact of this attack.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in the paper:

1. The paper proposes a novel poisoning attack called MISA against split federated learning (SFL). What is the key insight behind this attack that makes it more effective than existing attacks?

2. MISA poisons both the top model and bottom model in SFL. How does it indirectly poison the top model which is inaccessible to attackers? What is the rationale behind the smashed data crafting method?

3. MISA formalizes the bottom model poisoning as a Min-Sum optimization problem. Can you explain the objective formulation and its rationale in detail? 

4. The paper utilizes Thompson Sampling to dynamically select the perturbation direction when attacking the bottom model. Why is adaptively selecting the direction important? And how does Thompson Sampling model this as a Multi-Armed Bandit problem?

5. For finding the optimal perturbation magnitude, MISA proposes a SnL (Search and Locate) method. Can you explain how the coarse-grained searching phase works and its connection with the fine-grained locating phase?

6. The experiments showcase that MISA causes higher accuracy drop compared to state-of-the-art attacks. What is a key observation from the ablation study that explains why joint top and bottom model poisoning is more effective?

7. How does the position where the model is split impact the effectiveness of MISA and other attacks? What trend was observed as the split position becomes deeper?

8. When the attacker ratio is varied, how does the accuracy drop trend differ between data poisoning attacks like LF and model poisoning attacks like MISA/AGRT?

9. What impact does the heterogeneity of client data distributions have on the performance of MISA and other baseline attacks? How does MISA fare under extreme data distribution cases?

10. The threat model makes some assumptions like 20% attackers and attacker knowledge constraints. How can these assumptions be relaxed or tightened to evaluate the attack performance under different real-world scenarios?
