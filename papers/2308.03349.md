# SciGraphQA: A Large-Scale Synthetic Multi-Turn Question-Answering   Dataset for Scientific Graphs

## What is the central research question or hypothesis that this paper addresses?

Based on the abstract, this paper seems to present a synthetic multi-turn question-answering dataset related to academic graphs, called SciGraphQA. The key aspects of this dataset include:- It is derived from over 290,000 Computer Science and Machine Learning papers published on ArXiv between 2010-2020. - It contains 295K samples of open-vocabulary, multi-turn question-answering dialogues about the graphs in these papers. - The questions and answers were generated using a commercial large language model called Palm-2.- Comprehensive textual context was provided to Palm-2, including the paper title, abstract, paragraph mentioning the figure, and rich text from the graph itself. - The resulting dialogues have an average of 2.23 question-answer turns per graph.- When evaluated by GPT-4, the question-answer turns were rated 8.7/10 on average for relevance.So in summary, the central focus of this paper seems to be introducing SciGraphQA as a new large-scale dataset to support research into multi-modal large language models for scientific graph question answering. The authors evaluate performance of current MLLMs on SciGraphQA and propose it as a valuable benchmark for future work.


## What is the main contribution of this paper?

The main contribution of this paper is presenting SciGraphQA, a large-scale synthetic multi-turn question-answering dataset for scientific graphs. The key aspects are:- The dataset contains 295K samples of multi-turn dialogues related to academic graphs derived from over 290,000 Arxiv papers. This makes it the largest open-sourced chart VQA dataset with real-world non-synthetic graphs. - The questions and answers were generated using a commercial large language model (Palm-2) conditioned on comprehensive textual context like paper title, abstract, caption, and paragraph mentioning the graph. This results in more natural and diverse dialogues compared to using fixed question templates.- The quality of the question-answer pairs was validated using GPT-4, obtaining an average rating of 8.7/10 on a 3K test set.- The paper evaluates various state-of-the-art multimodal large language models on this dataset in both zero-shot and fine-tuning settings. Prompt augmentation using extracted data tables is shown to improve zero-shot performance.- A baseline model fine-tuned on SciGraphQA called SciGraphQA-baseline is presented, outperforming prior models with a CIDEr score of 0.26.In summary, the key contribution is creating a large-scale dataset to advance research on question-answering for scientific graphs, providing a valuable benchmark and training resource. The empirical results demonstrate current challenges and opportunities for improving multimodal reasoning on this complex domain.


## How does this paper compare to other research in the same field?

Based on my review of the paper, here is how I would compare it to other research in the same field:- This paper presents a new dataset called SciGraphQA for question answering on scientific graphs. Other datasets like FigureQA, DVQA, PlotQA, and ChartQA have also focused on visual question answering for charts and graphs, but SciGraphQA is unique in its use of real scientific graphs and its large size of 300K question-answer pairs. This makes it the largest open-sourced dataset for non-synthetic chart VQA.- The questions and answers in SciGraphQA were generated using a large language model conditioned on rich context about the graph, including the paper's title, abstract, caption, and a mentioning paragraph. This results in more natural, conversational dialogues compared to other datasets that use fixed question templates or restricted vocabularies. The authors validated the quality of the dialogues using GPT-4.- For model evaluation, the paper tests several recent multimodal large language models like LLaMa, BLIP, and mPLUG in zero-shot and fine-tuning settings. The best performance was achieved by fine-tuning LLaMa on SciGraphQA, demonstrating the value of this dataset for instruction tuning. Other papers have evaluated simpler VQA models like VisualBERT or ViLT on different chart QA datasets.- The paper also explores the use of an expert model called DePlot to extract data tables from charts and enhance the textual prompts. This idea of leveraging external expert systems is similar to prior work using OCR, but DePlot provides richer structural information. The benefits of augmenting prompts with expert knowledge is a novel contribution.- Overall, SciGraphQA pushes forward chart VQA research through its scale, real-world scientific data, and conversational nature. The empirical analyses around state-of-the-art MLLMs and prompt engineering also provide useful insights to the community. This paper represents an important step towards more capable and robust QA systems for scientific graphs.In summary, while there has been much prior work on chart VQA, SciGraphQA stands out for its unique characteristics and rigorous experiments with modern multimodal LLMs. The researchers have created a valuable resource and benchmark for continued progress in this domain.
