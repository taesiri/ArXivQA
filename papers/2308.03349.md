# SciGraphQA: A Large-Scale Synthetic Multi-Turn Question-Answering   Dataset for Scientific Graphs

## What is the central research question or hypothesis that this paper addresses?

Based on the abstract, this paper seems to present a synthetic multi-turn question-answering dataset related to academic graphs, called SciGraphQA. The key aspects of this dataset include:- It is derived from over 290,000 Computer Science and Machine Learning papers published on ArXiv between 2010-2020. - It contains 295K samples of open-vocabulary, multi-turn question-answering dialogues about the graphs in these papers. - The questions and answers were generated using a commercial large language model called Palm-2.- Comprehensive textual context was provided to Palm-2, including the paper title, abstract, paragraph mentioning the figure, and rich text from the graph itself. - The resulting dialogues have an average of 2.23 question-answer turns per graph.- When evaluated by GPT-4, the question-answer turns were rated 8.7/10 on average for relevance.So in summary, the central focus of this paper seems to be introducing SciGraphQA as a new large-scale dataset to support research into multi-modal large language models for scientific graph question answering. The authors evaluate performance of current MLLMs on SciGraphQA and propose it as a valuable benchmark for future work.


## What is the main contribution of this paper?

The main contribution of this paper is presenting SciGraphQA, a large-scale synthetic multi-turn question-answering dataset for scientific graphs. The key aspects are:- The dataset contains 295K samples of multi-turn dialogues related to academic graphs derived from over 290,000 Arxiv papers. This makes it the largest open-sourced chart VQA dataset with real-world non-synthetic graphs. - The questions and answers were generated using a commercial large language model (Palm-2) conditioned on comprehensive textual context like paper title, abstract, caption, and paragraph mentioning the graph. This results in more natural and diverse dialogues compared to using fixed question templates.- The quality of the question-answer pairs was validated using GPT-4, obtaining an average rating of 8.7/10 on a 3K test set.- The paper evaluates various state-of-the-art multimodal large language models on this dataset in both zero-shot and fine-tuning settings. Prompt augmentation using extracted data tables is shown to improve zero-shot performance.- A baseline model fine-tuned on SciGraphQA called SciGraphQA-baseline is presented, outperforming prior models with a CIDEr score of 0.26.In summary, the key contribution is creating a large-scale dataset to advance research on question-answering for scientific graphs, providing a valuable benchmark and training resource. The empirical results demonstrate current challenges and opportunities for improving multimodal reasoning on this complex domain.
