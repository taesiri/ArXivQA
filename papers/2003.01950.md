# [AlignTTS: Efficient Feed-Forward Text-to-Speech System without Explicit   Alignment](https://arxiv.org/abs/2003.01950)

## What is the central research question or hypothesis that this paper addresses?

 The central research question this paper addresses is how to improve the speed and performance of text-to-speech (TTS) synthesis systems using a non-autoregressive feed-forward architecture. 

Specifically, the key points are:

- Proposing AlignTTS, a feed-forward network to generate mel-spectrograms in parallel for fast TTS.

- Using a duration predictor to align text to mel-spectrograms instead of attention mechanism. 

- Introducing an alignment loss to precisely learn text-to-mel alignments during training.

- Achieving state-of-the-art performance while being over 50x faster than real-time TTS on LJSpeech dataset.

In summary, the main hypothesis is that using a feed-forward Transformer combined with explicit alignment modeling can achieve both high-quality and very fast parallel TTS compared to autoregressive methods like Tacotron. The experiments aim to validate this hypothesis.


## What is the main contribution of this paper?

 The main contributions of this paper are:

1. Proposing AlignTTS, a feed-forward text-to-speech model that can generate mel-spectrograms in parallel for fast speech synthesis. 

2. Presenting an alignment loss to train AlignTTS to learn the alignment between text and mel-spectrograms. This produces more precise alignments compared to attention mechanisms in autoregressive models like Tacotron and Transformer TTS.

3. Achieving state-of-the-art speech synthesis quality that outperforms Transformer TTS on MOS scores, while being over 50x faster than real-time synthesis speed.

In summary, the key innovation is the alignment loss for directly learning text-to-mel alignments in a non-autoregressive feed-forward model like AlignTTS. This allows parallel mel-spectrogram generation for fast synthesis while still achieving high quality speech.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the paper:

The paper proposes AlignTTS, a fast and high-quality text-to-speech model based on a feed-forward transformer that learns alignments between text and mel-spectrograms using an alignment loss inspired by the Baum-Welch algorithm.


## How does this paper compare to other research in the same field?

 Here are a few key points comparing this AlignTTS paper to other recent TTS research:

- This paper tackles the problem of low inference efficiency in autoregressive TTS models like Tacotron 2 and Transformer TTS. The proposed AlignTTS model generates mel spectrograms in parallel for much faster inference speed.

- To enable parallel mel spectrogram generation, AlignTTS predicts durations for each input character using a duration predictor module. This is similar to FastSpeech but AlignTTS trains the duration predictor itself rather than requiring alignments from a pre-trained autoregressive model. 

- A key contribution is the proposed alignment loss to train the mix density network to learn alignments between text and mel spectrograms. This avoids needing a separate autoregressive alignment model. The alignment loss sums over possible alignments like the Baum-Welch algorithm for HMMs.

- Experiments show AlignTTS matches or exceeds the audio quality of Tacotron 2/Transformer TTS while being over 50x faster. This helps address the speed limitations of previous autoregressive models.

- Other recent work has also tackled fast parallel TTS with similar high-level approaches but differences in architecture details, alignment modeling, and training procedures. Examples are FastSpeech 2, AlignTTS, ParaNet, and Flow-TTS.

- Overall, this paper makes nice contributions in alignment modeling and training for parallel TTS to improve on earlier work like FastSpeech. The results demonstrate very fast TTS generation without sacrificing quality. AlignTTS appears competitive with other state-of-the-art parallel TTS methods.
