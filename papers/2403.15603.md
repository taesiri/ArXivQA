# [Forward Learning for Gradient-based Black-box Saliency Map Generation](https://arxiv.org/abs/2403.15603)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
Gradient-based saliency maps are widely used to explain decisions of deep neural networks. However, as models become more complex and black-box, such as proprietary APIs or large models like ChatGPT, computing gradients to generate saliency maps becomes very challenging or even infeasible. This hinders the applicability of conventional explanation methods that rely on gradient access. 

Proposed Solution:
This paper proposes a novel framework to estimate gradients and generate saliency maps to interpret black-box model decisions, without needing internal gradient access. The key ideas are:

1) Use the likelihood ratio method to estimate gradients by adding small random noise to inputs, forwarding them through the model, and computing a proxy gradient based on the output and noise density. 

2) Propose a blockwise computation technique to enhance accuracy by only adding noise to random input blocks at a time. This significantly reduces variance.

3) Integrate estimated gradients into conventional saliency map generation methods like Integrated Gradients. This enables explaining any differentiable black-box model.

4) Propose techniques to select targets for gradient estimation when only hard label decisions or text outputs are available from the black-box model.

Main Contributions:
- First framework to generate gradient-based saliency maps for black-box neural networks using likelihood ratio method
- Blockwise computation with variance reduction for accurate gradient estimates on high-dimensional inputs 
- Enables conventional saliency map methods like Integrated Gradients to be used on black-box models
- Target selection techniques for non-numeric outputs (e.g. hard labels, text)
- Extensive experiments validating the approach, like deletion/insertion metrics and adversarial attacks 
- Demonstration of scalability by explaining decisions of large proprietary vision-language model GPT-Vision

The proposed techniques open up gradient-based explanation methods to black-box scenarios, with experiments showing accurate and useful saliency maps even for very large closed-source models. This facilitates model understanding and trust in opaque AI systems.
