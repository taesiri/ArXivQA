# [Dozerformer: Sequence Adaptive Sparse Transformer for Multivariate Time   Series Forecasting](https://arxiv.org/abs/2312.06874)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Transformers have shown remarkable performance for multivariate time series (MTS) forecasting due to their ability to capture long-term dependencies. However, the standard attention mechanism has two key limitations:
  1) Quadratic time complexity that limits sequence length
  2) Generates future values from the entire historical sequence, ignoring the different importance of historical time steps for different forecast horizons.

Proposed Solution:
- The paper proposes a Dozer Attention mechanism with three sparse components:
  1) Local: Each query attends exclusively to keys within a localized window of neighboring time steps, capturing locality in MTS data.  
  2) Stride: Each query selectively attends to keys at predefined intervals, capturing seasonality.
  3) Vary: Query attention adapts based on forecast horizon, using more historical context for longer horizons.
  
- These components aim to capture essential attributes of MTS data while eliminating less relevant queries/keys.

- The Dozerformer framework incorporates Dozer Attention in a transformer encoder-decoder architecture to model seasonal components. A linear layer handles trend components.

Main Contributions:  
- Dozer Attention mechanism with Local, Stride and Vary components that enable sequence-adaptive sparsity based on MTS characteristics and forecast horizon.
- Dozerformer framework integrating Dozer Attention for modeling seasonality via transformers, with a linear layer for trends.
- Superior accuracy and efficiency over state-of-the-art methods on 9 benchmark datasets. Dozerformer reduces MSE by 40.6% over best baseline.

In summary, the paper introduces an innovative sparse attention mechanism and framework tailored to the characteristics and forecasting requirements of MTS data, achieving new state-of-the-art performance.
