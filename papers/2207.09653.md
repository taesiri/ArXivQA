# [FedDM: Iterative Distribution Matching for Communication-Efficient   Federated Learning](https://arxiv.org/abs/2207.09653)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question is how to design a communication-efficient federated learning algorithm that can handle non-i.i.d. data partitioning across clients. 

The key hypothesis is that by having each client learn a local surrogate function to approximate its local objective, and sending these surrogate functions to the server, the server can obtain a more global view of the loss landscape compared to just receiving local model updates like gradients. This will allow the server to make more progress per communication round and achieve higher efficiency.

Specifically, the paper proposes FedDM, which has each client learn a synthetic dataset via distribution matching to build a local surrogate loss function. The server then aggregates these surrogate functions to optimize the global model. The use of synthetic data allows implicit access to a more balanced global dataset, while requiring less communication compared to transmitting large model parameters or gradients.

So in summary, the central hypothesis is that surrogate functions based on synthesized data can enable more communication-efficient and effective federated learning compared to prior algorithms like FedAvg that average local model updates, especially in the non-i.i.d. setting. FedDM is proposed to validate this hypothesis.


## What is the main contribution of this paper?

 This paper proposes FedDM, a new federated learning algorithm based on iterative distribution matching. The key ideas and contributions are:

- It introduces an iterative surrogate minimization framework for federated learning. Rather than simply aggregate model updates from clients like previous algorithms, it builds a global surrogate function on the server from local surrogate functions constructed by each client. 

- To build the local surrogate, each client learns a small synthesized dataset to match the loss distribution of its real data through distribution matching techniques like MMD. This allows the server to gain a more global view of the loss landscape.

- By sending small synthesized datasets instead of model updates, FedDM achieves better communication efficiency compared to prior federated learning algorithms.

- Experiments on image classification tasks demonstrate FedDM requires much fewer rounds of communication to reach good accuracy compared to baselines. It also shows better performance on non-IID and unbalanced datasets.

- The paper also adapts FedDM to provide differential privacy guarantees, and shows it can train more accurate models than other private federated learning algorithms under the same privacy budget.

In summary, the key contribution is proposing distribution matching to build surrogate functions for efficient and accurate federated learning, with theoretical privacy guarantees. This is a novel way to tackle the communication challenge in federated learning.
