# [FedDM: Iterative Distribution Matching for Communication-Efficient   Federated Learning](https://arxiv.org/abs/2207.09653)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question is how to design a communication-efficient federated learning algorithm that can handle non-i.i.d. data partitioning across clients. 

The key hypothesis is that by having each client learn a local surrogate function to approximate its local objective, and sending these surrogate functions to the server, the server can obtain a more global view of the loss landscape compared to just receiving local model updates like gradients. This will allow the server to make more progress per communication round and achieve higher efficiency.

Specifically, the paper proposes FedDM, which has each client learn a synthetic dataset via distribution matching to build a local surrogate loss function. The server then aggregates these surrogate functions to optimize the global model. The use of synthetic data allows implicit access to a more balanced global dataset, while requiring less communication compared to transmitting large model parameters or gradients.

So in summary, the central hypothesis is that surrogate functions based on synthesized data can enable more communication-efficient and effective federated learning compared to prior algorithms like FedAvg that average local model updates, especially in the non-i.i.d. setting. FedDM is proposed to validate this hypothesis.


## What is the main contribution of this paper?

 This paper proposes FedDM, a new federated learning algorithm based on iterative distribution matching. The key ideas and contributions are:

- It introduces an iterative surrogate minimization framework for federated learning. Rather than simply aggregate model updates from clients like previous algorithms, it builds a global surrogate function on the server from local surrogate functions constructed by each client. 

- To build the local surrogate, each client learns a small synthesized dataset to match the loss distribution of its real data through distribution matching techniques like MMD. This allows the server to gain a more global view of the loss landscape.

- By sending small synthesized datasets instead of model updates, FedDM achieves better communication efficiency compared to prior federated learning algorithms.

- Experiments on image classification tasks demonstrate FedDM requires much fewer rounds of communication to reach good accuracy compared to baselines. It also shows better performance on non-IID and unbalanced datasets.

- The paper also adapts FedDM to provide differential privacy guarantees, and shows it can train more accurate models than other private federated learning algorithms under the same privacy budget.

In summary, the key contribution is proposing distribution matching to build surrogate functions for efficient and accurate federated learning, with theoretical privacy guarantees. This is a novel way to tackle the communication challenge in federated learning.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the key points from the paper: 

The paper proposes FedDM, a federated learning method that improves communication efficiency and effectiveness by having clients learn a surrogate function locally through distribution matching on synthesized data and sending the compact synthesized datasets to the server for aggregated training.


## How does this paper compare to other research in the same field?

 Based on my reading of the paper, here is a summary of how it compares to other related research:

- The paper proposes a new federated learning algorithm called FedDM (Federated Learning with Distribution Matching) that aims to improve communication efficiency. This aligns with a major focus in federated learning research which is reducing the number of communication rounds required for training.

- Most existing federated learning algorithms like FedAvg, FedProx, etc. involve iterative model averaging, where clients send model updates like weights or gradients to the server for aggregation. FedDM takes a different approach based on building local surrogate functions for the loss using synthetic data, which provides more information to the server about the loss landscape.

- Using synthetic/condensed datasets for efficiency has been explored in other works on dataset distillation and data condensation like DAWNBench, but this paper adapts the idea for federated learning specifically. Their use of distribution matching to learn the synthetic dataset is also novel.

- For handling non-IID data in federated learning, methods like FedProx and SCAFFOLD have been proposed. FedDM handles non-IID data through better approximating the global loss by aggregating informative local surrogates. 

- The paper shows FedDM converges much faster than FedAvg, FedProx etc. in terms of communication rounds and outperforms them in terms of accuracy under non-IID settings. This demonstrates the effectiveness of their approach.

- They also demonstrate FedDM can preserve differential privacy, which relates to work on privacy-preserving federated learning. Overall, the proposed method and experimental results advance the state-of-the-art in communication-efficient federated learning.

In summary, FedDM introduces a novel surrogate loss-based approach for federated learning that achieves better efficiency and performance compared to existing algorithms, advancing research in this domain. The adaptation of dataset distillation ideas to the federated setting is also novel.
