# [FedDM: Iterative Distribution Matching for Communication-Efficient   Federated Learning](https://arxiv.org/abs/2207.09653)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question is how to design a communication-efficient federated learning algorithm that can handle non-i.i.d. data partitioning across clients. 

The key hypothesis is that by having each client learn a local surrogate function to approximate its local objective, and sending these surrogate functions to the server, the server can obtain a more global view of the loss landscape compared to just receiving local model updates like gradients. This will allow the server to make more progress per communication round and achieve higher efficiency.

Specifically, the paper proposes FedDM, which has each client learn a synthetic dataset via distribution matching to build a local surrogate loss function. The server then aggregates these surrogate functions to optimize the global model. The use of synthetic data allows implicit access to a more balanced global dataset, while requiring less communication compared to transmitting large model parameters or gradients.

So in summary, the central hypothesis is that surrogate functions based on synthesized data can enable more communication-efficient and effective federated learning compared to prior algorithms like FedAvg that average local model updates, especially in the non-i.i.d. setting. FedDM is proposed to validate this hypothesis.
