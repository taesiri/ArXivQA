# [Modular Visual Question Answering via Code Generation](https://arxiv.org/abs/2306.05392)

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper addresses is whether generating modular programs via code synthesis can improve performance on few-shot visual question answering, compared to more monolithic approaches based solely on large language models. Specifically, the authors introduce CodeVQA, a method that uses a code-generating language model to produce modular Python programs for answering visual questions. These programs make use of predefined visual primitives and control flow to reason about the question and image(s). The key hypothesis is that this more compositional approach will generalize better in the few-shot setting compared to just using a large LM directly on the question and image captions.The authors evaluate CodeVQA on the GQA, COVR, and NLVR2 datasets in the few-shot setting against strong LM baselines. The results show CodeVQA outperforming baselines by 2-3%, supporting the hypothesis that modular code synthesis provides benefits for few-shot VQA beyond just using a capable LM. The analysis also reveals the biggest gains on spatial reasoning and conditional questions where CodeVQA can break the complex question into simpler modular pieces.In summary, the central hypothesis is that code synthesis for modular program execution can enhance few-shot VQA performance over monolithic LM approaches, which the experiments and analysis generally validate. The modular approach provides more systematic compositional reasoning.
