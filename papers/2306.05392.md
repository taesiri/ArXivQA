# [Modular Visual Question Answering via Code Generation](https://arxiv.org/abs/2306.05392)

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper addresses is whether generating modular programs via code synthesis can improve performance on few-shot visual question answering, compared to more monolithic approaches based solely on large language models. Specifically, the authors introduce CodeVQA, a method that uses a code-generating language model to produce modular Python programs for answering visual questions. These programs make use of predefined visual primitives and control flow to reason about the question and image(s). The key hypothesis is that this more compositional approach will generalize better in the few-shot setting compared to just using a large LM directly on the question and image captions.The authors evaluate CodeVQA on the GQA, COVR, and NLVR2 datasets in the few-shot setting against strong LM baselines. The results show CodeVQA outperforming baselines by 2-3%, supporting the hypothesis that modular code synthesis provides benefits for few-shot VQA beyond just using a capable LM. The analysis also reveals the biggest gains on spatial reasoning and conditional questions where CodeVQA can break the complex question into simpler modular pieces.In summary, the central hypothesis is that code synthesis for modular program execution can enhance few-shot VQA performance over monolithic LM approaches, which the experiments and analysis generally validate. The modular approach provides more systematic compositional reasoning.


## What is the main contribution of this paper?

Based on my reading, the main contributions of this paper are:1. They propose CodeVQA, a framework for visual question answering (VQA) that formulates VQA as modular code generation using large language models. 2. CodeVQA generates Python code to compose predefined visual primitives based on the input question. The visual primitives are wrappers around pre-trained vision-language models and probe the image to extract relevant information.3. CodeVQA does not require retraining modules or parsers like prior modular VQA methods. It can adapt to new VQA distributions in a few-shot manner via prompting.4. They evaluate CodeVQA on few-shot VQA and show it outperforms prior few-shot VQA methods, especially on datasets requiring reasoning over multiple images like COVR.5. They demonstrate CodeVQA can achieve strong performance on GQA using only models pre-trained on image-text data, without relying on supervised VQA models.6. Their analysis shows the benefits of CodeVQA's modularity and program synthesis, compared to a competitive few-shot baseline using the same in-context learning and models.In summary, the main contribution is proposing and evaluating a new framework CodeVQA that achieves strong few-shot VQA performance by formulating VQA as modular code generation using pre-trained language and vision models. The modularity and program synthesis aspects are key.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper presents a framework for visual question answering that uses a language model to generate Python programs invoking pre-trained visual modules, improving accuracy in the few-shot setting without additional model training.
