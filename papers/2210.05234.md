# [It Takes Two: Masked Appearance-Motion Modeling for Self-supervised   Video Transformer Pre-training](https://arxiv.org/abs/2210.05234)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading, the central research question this paper addresses is: 

How can we better leverage motion/temporal information in videos for self-supervised video transformer pre-training?

The key points are:

- Existing methods like VideoMAE and ST-MAE use a mask-and-predict framework for self-supervised pre-training, but they mainly focus on reconstructing spatial information and do not effectively utilize temporal/motion cues in videos. 

- This paper proposes to use two separate reconstruction targets - one for appearance and one for motion. The goal is to force the encoder to learn better spatio-temporal representations by predicting both appearance and motion.

- They introduce a motion stream to explicitly reconstruct motion-related targets like optical flow or RGB differences between frames. The appearance stream uses VQGAN codes as the target.

- Having two disentangled tasksforces the shared encoder to capture both spatial and temporal statistics. It also speeds up convergence during pre-training.

- They adopt a regressor between encoder and decoders to avoid entanglement and help the encoder focus on feature extraction.

- Experiments show their method (MAM^2) outperforms VideoMAE and other methods on multiple downstream tasks, while using fewer pre-training epochs.

In summary, the main hypothesis is that explicitly reconstructing motion signals alongside appearance can help transformers learn more transferable video representations in a self-supervised manner. The dual prediction targets better cover the spatio-temporal essence of videos.


## What is the main contribution of this paper?

 Based on my reading of the paper, the main contributions appear to be:

- Proposing a Masked Appearance-Motion Modeling (MAM^2) framework for self-supervised video representation learning. This framework uses disentangled decoders to reconstruct visual appearance and motion targets simultaneously.

- Adopting a regressor and alignment module to separate feature encoding from pretext task completion. This allows the encoder to focus on extracting robust spatio-temporal features. 

- Demonstrating that simple RGB difference frames are an effective motion prediction target. The motion decoder reconstructs this while the appearance decoder uses VQGAN tokens.

- Achieving state-of-the-art performance on multiple video action recognition benchmarks (Kinetics-400, Something-Something V2, UCF101, HMDB51) with fewer pre-training epochs than prior methods. For example, matching VideoMAE accuracy on Kinetics-400 with only 50% of the pre-training epochs.

- Showing the benefits of decoupled appearance and motion modeling, including more efficient convergence during pre-training. The dual decoder approach better handles spatio-temporal redundancy.

In summary, the key contribution appears to be proposing the MAM^2 framework with disentangled modeling of appearance and motion targets to enable more efficient and effective self-supervised pre-training of video transformers. The results demonstrate improved downstream task performance.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the key points in the paper:

The paper proposes a pre-training framework called Masked Appearance-Motion Modeling (MAM2) that reconstructs appearance and motion targets separately using dual disentangled decoders to guide the encoder to learn robust spatio-temporal video representations, achieving strong performance on downstream tasks with fewer pre-training epochs than prior work.


## How does this paper compare to other research in the same field?

 Here is my analysis of how this paper compares to other research in the field of self-supervised video representation learning:

The main contribution of this paper is proposing a dual-stream framework for pre-training video transformers, with separate decoders for reconstructing masked appearance and motion targets. This is a novel approach compared to prior work on masked video modeling. 

Previous methods like VideoMAE and ST-MAE extended the MAE image modeling approach to videos by reconstructing raw pixels in a space-time agnostic manner. They did not explicitly model motion or leverage motion-specific pretext tasks. 

This paper shows that incorporating an explicit motion modeling stream during pre-training improves downstream task performance and convergence speed. Using RGB differences as the motion target is a simple and effective choice.

The idea of dual-stream modeling has similarities to two-stream CNN architectures that integrate RGB and optical flow streams. The key difference here is the two streams operate on learned latent representations rather than raw signals.

The regressor and alignment components are adapted from prior work like Context Autoencoders. This helps separate feature encoding from pretext task completion.

Overall, this paper makes a novel contribution in showing the benefits of dual appearance and motion modeling for self-supervised video transformer pre-training. The results demonstrate improved accuracy and faster convergence over methods that reconstruct appearance only. The design choices leverage strengths of prior work on masked modeling and two-stream networks.
