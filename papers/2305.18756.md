# VSTAR: A Video-grounded Dialogue Dataset for Situated Semantic   Understanding with Scene and Topic Transitions

## What is the central research question or hypothesis that this paper addresses?

The central research question addressed in this paper is how to develop an open-domain dialogue system that can comprehend complex multimodal signals and generate coherent responses grounded in both visual and dialogue context. Specifically, the authors aim to enable the dialogue system to:1. Identify scene and topic transitions in both video and dialogue modalities.2. Leverage the scene and topic structure to generate responses that are consistent with the contextual information. To achieve this, the authors collected and annotated a new video-grounded dialogue dataset called VSTAR with scene and topic boundary labels. Based on this dataset, they proposed and benchmarked models for video scene segmentation, dialogue topic segmentation, and video-grounded response generation. The key hypothesis is that explicitly modeling scene and topic transitions will lead to better video understanding and more coherent dialogue responses compared to prior work that treats the video and dialogues independently. The experiments validate the importance of multimodal scene and topic information for situated dialogue modeling.In summary, the paper introduces a new multimodal dialogue benchmark focused on high-level semantic perception, defined by scene and topic transitions. This is in contrast to prior work that relies more heavily on object-level vision-language links. The central hypothesis is that structured segmentation will enable more human-like dialogue agents that comprehend real-world situational dynamics.


## What is the main contribution of this paper?

This paper introduces VSTAR, a new large-scale video-grounded dialogue dataset for situated semantic understanding. The main contributions are:1. VSTAR contains 395 TV shows with 185K 90-second video clips paired with dialogues. It has annotations for video scene boundaries and dialogue topic boundaries to capture scene and topic transitions. 2. The paper proposes three challenging tasks using VSTAR: video-grounded dialogue scene segmentation, topic segmentation, and response generation. These require modeling scene and topic transitions for coherent understanding.3. Transformer-based models are proposed and benchmarked on VSTAR for the three tasks. The models leverage multi-modal context with scene and topic segment embeddings. Experiments demonstrate the value of modeling transitions.4. VSTAR is the first large-scale dataset combining multi-modal information to find semantic transitions in video and dialogue. The benchmarks require reasoning over high-level contextual information beyond object-level links between modalities.In summary, the main contribution is introducing the new VSTAR dataset and associated tasks to promote video-grounded dialogue systems that can understand realistic multimodal signals with scene and topic changes. This is an important step towards conversational agents for complicated situations.
