# [C-ICL: Contrastive In-context Learning for Information Extraction](https://arxiv.org/abs/2402.11254)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
Recent works have explored using few-shot in-context learning with large language models (LLMs) for information extraction (IE) tasks like named entity recognition (NER) and relation extraction (RE). However, existing methods tend to use only correct/positive examples in the demonstrations, neglecting the potential value of incorrect/negative examples in enhancing the learning process.  

Proposed Solution: 
The paper proposes a novel contrastive in-context learning approach called c-ICL that utilizes both correct/positive and incorrect/negative sample constructions to create demonstrations for the LLM. The key ideas are:

(1) Incorporate not just the positive samples but also the reasoning behind them as prompts to allow the LLM to identify and correct potential errors. 

(2) Select hard negative samples using semantic similarity-aware self-consistency ranking. Hard negatives possess valuable information about the types of errors made by the LLM.

(3) Retrieve positive samples most similar to the test sentence to provide better context.

(4) Design prompts for both natural language LLMs and code LLMs that leverage the contrastive samples.

Main Contributions:

(1) Develops a contrastive in-context learning approach using both positive and negative examples to enhance IE capabilities of LLMs.

(2) Proposes effective strategies to select high-quality hard negative samples and semantically similar positive samples for demonstrations.

(3) Comprehensive experiments on multiple NER and RE benchmarks demonstrate state-of-the-art performance, proving the versatility of the approach.

(4) Careful analyses provide insights into the benefits of contrastive demonstrations for in-context learning.

In summary, the key novelty is using contrastive positive and negative prompts in a principled way to teach the LLM to avoid errors and perform better few-shot IE.
