# Improving performance of deep learning models with axiomatic attribution   priors and expected gradients

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research questions/hypotheses appear to be:1) Can imposing constraints on a deep learning model's feature attributions during training lead to models with more desirable properties, such as being more interpretable or robust? 2) Can new types of constraints on attributions, beyond just setting attributions of some features to specific target values, be formulated? Specifically, can constraints be imposed that encode "high-level" priors about relationships between features, rather than just priors about individual features?3) Can a new attribution method be developed that is more axiomatically justified, efficient to compute, and well-suited to being incorporated into the training process through an attribution prior?The authors introduce the concept of "attribution priors", which constrain a model's feature attributions during training to have certain properties. They hypothesize that novel attribution priors can be formulated to encode things like smoothness, sparsity, and similarity between related features. They also hypothesize that their new attribution method, expected gradients, will enable more effective training with attribution priors due to its theoretical guarantees and computational efficiency. The central goals are to demonstrate:- New types of attribution priors beyond previous work- That these novel priors improve model interpretability and performance- That the expected gradients attribution method outperforms previous methods when incorporated into training through attribution priors.The key hypotheses are that attribution priors are a flexible framework for encoding human domain knowledge into models, that the specific new priors introduced will be beneficial, and that expected gradients will be both axiomatically and empirically superior as the attribution method for imposing such priors.


## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central hypothesis of this paper is that using axiomatically correct feature attributions (such as expected gradients) to encode high-level priors into deep learning models can improve their performance and interpretability on real-world tasks. Specifically, the paper proposes using differentiable functions of a model's axiomatic feature attributions as regularization terms during training. This allows encoding flexible priors that capture relationships between features, rather than just encouraging attributions towards predetermined target values. The main hypotheses tested are:1) Attribution priors based on axiomatic attributions like expected gradients will outperform previous attribution priors based on simple gradients.2) Novel high-level priors encoded via expected gradients attributions can improve model performance in image classification, drug response prediction, and healthcare tasks. 3) The proposed expected gradients attribution method will better identify important features on benchmark tasks compared to previous attribution methods.In summary, the central hypothesis is that axiomatically correct attributions enable more effective high-level priors that in turn improve model performance and interpretability compared to previous attribution-based regularization approaches. Experiments across computer vision, biology, and healthcare domains aim to test this hypothesis.


## What is the main contribution of this paper?

Based on my reading, the main contributions of this paper are:1. It introduces a flexible framework called "attribution priors" for incorporating domain knowledge into deep learning models by penalizing their feature attribution explanations during training. This allows encoding priors that features should be smooth, sparse, etc without needing to know exact importance values.2. It develops a new axiomatic attribution method called "expected gradients" that enables efficiently training attribution priors by reformulating attributions as expectations that can be estimated with sampling. This also satisfies desirable properties like implementation invariance. 3. It demonstrates the effectiveness of training with attribution priors and expected gradients on several real-world tasks:- Using a smoothness prior on images improves robustness and interpretability- Using a biological graph prior on gene expression data improves prediction and captures relevant pathways- Using a sparsity prior on healthcare data improves prediction with limited dataOverall, the key ideas are developing more flexible attribution priors beyond previous work, enabling efficient training with axiomatic attributions, and showing performance gains across diverse domains. The proposed methods improve interpretability and robustness without sacrificing accuracy.


## What is the main contribution of this paper?

Based on my reading, the main contribution of this paper is introducing a new framework for incorporating feature attribution methods into the training of deep learning models, called "attribution priors". The key ideas are:- Attribution priors allow encoding high-level domain knowledge about how models should treat input features into the training process. This is done by adding a penalty term to the loss function during training that depends on the feature attributions (e.g. integrated gradients) of the model.- The paper demonstrates several new types of flexible attribution priors that improve model performance and interpretability without needing to pre-specify target attribution values for individual features. These include priors for smoothness, sparsity, and using biological graph information.- The paper introduces a new attribution method called "expected gradients" that is efficient to calculate and integrate into training while satisfying important axiomatic properties like implementation invariance. - Across image, biological, and healthcare experiments, the authors show that using expected gradients attributions to calculate priors (rather than simpler gradients) substantially improves performance.In summary, the key contribution is presenting attribution priors as a general framework for encoding domain knowledge via feature attributions, demonstrating new flexible priors, and showing that using axiom-satisfying attributions like expected gradients enables significiant gains. This provides tools to induce desirable behaviors in deep learning models aligned with human intuition.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence TL;DR summary of the paper:The paper introduces a method to use axiomatic feature attributions, which explain model predictions, to encourage desirable behaviors in deep learning models during training.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

The TL;DR version of the paper is: This paper proposes training deep learning models to have more interpretable explanations by regularizing their gradient-based feature attributions using attribution priors.In one sentence, I would summarize it as: The paper introduces attribution priors, which are regularizers that encourage deep learning models to have certain desired properties in their feature attributions during training.


## How does this paper compare to other research in the same field?

Here is a summary and comparison of the key points of this paper to other related work:The paper introduces a method for using axiomatic feature attributions (explanations of model predictions based on input features) to encourage desirable behaviors in deep neural network models during training. The key contributions are:- Presenting attribution priors as a general framework for incorporating differentiable functions of a model's feature attributions into the training loss. This allows encoding high-level domain knowledge as regularization terms. - Demonstrating new types of flexible attribution priors beyond just encouraging attributions to match specified targets. These include smoothness over images, biological graphs, and sparsity.- Introducing expected gradients, a novel attribution method that satisfies key axioms and enables efficient computation of attributions for regularization.This relates to prior work as follows:- Attribution priors build on the idea of "Right for the Right Reasons" regularization introduced by Ross et al. (2017), but are more general and flexible. They also outperform gradient-based regularization.- Using attributions in training was also explored in recent works like Liu et al. (2019) and Rieger et al. (2020) but with more limited attribution methods and less sophisticated priors. Expected gradients enable more efficient training.- New attribution priors go beyond just matching target values, allowing more flexible shaping of model behavior based on domain knowledge.- Expected gradients provides a fast, axiomatically sound attribution method. It outperforms integrated gradients on benchmarks.So in summary, this paper significantly expands the framework and demonstrates the value of encoding domain knowledge via attribution priors. The new methods and ideas could have broad impact on applied deep learning.


## How does this paper compare to other research in the same field?

Based on my reading of the paper, here is how it compares and relates to other research in the field:- The paper introduces a new method called "attribution priors" for incorporating human domain knowledge into deep learning models using feature attribution methods. This builds on prior work like "Right for the Right Reasons" by Ross et al. that also trained models to have correct attributions. The key advances are using more sophisticated attribution methods that satisfy important axioms and introducing flexible priors that encode relationships between features rather than just target values.- The flexible priors like smoothness over images or graphs are novel ways to encode human intuition that have not been explored before. Prior work focused on getting attributions close to pre-set targets. Encoding relationships between features is a more natural way for humans to express preferences.- Using an attribution method like expected gradients that satisfies key axioms like implementation invariance sets this work apart from just using gradients, which can fail for common architectures. The efficiency of expected gradients also enables training complex priors in a reasonable time.- Recent work like CDEP also incorporates explanations into training but is limited to certain architectures and optimizing explanations for groups rather than individual features. The flexibility of attribution priors to use any differentiable model and any feature-wise penalty is an advantage.- The empirical benefits like increased robustness and biological interpretability demonstrate the practical value of this approach over just training for accuracy. This relates to growing work on how explanations can improve models rather than just diagnose them.- Overall, attribution priors advance the state-of-the-art in encoding human domain knowledge into deep learning models by introducing flexible, high-level priors and efficient, axiomatically-sound attributions. The gains across diverse tasks validate the benefits of this approach.In summary, this work moves beyond previous limitations in training with explanations and enables rich human knowledge to be incorporated through intuitive priors that improve model performance and interpretability. The results substantiate the advantages of this framework for many real-world applications.
