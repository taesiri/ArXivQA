# [Controlling Text-to-Image Diffusion by Orthogonal Finetuning](https://arxiv.org/abs/2306.07280)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be: How can we effectively guide or control large text-to-image diffusion models to perform different downstream tasks without losing their impressive pretraining generative capabilities?The authors tackle this question by proposing a new finetuning approach called Orthogonal Finetuning (OFT) that adaptively transforms the internal representations of a pretrained text-to-image diffusion model for downstream tasks while preserving key structural properties. Specifically, the key hypothesis seems to be:Preserving the hyperspherical energy, which characterizes the pairwise neuron relationships, is crucial for maintaining the semantic generation capabilities of pretrained text-to-image diffusion models during finetuning.The authors test this hypothesis by showing that their proposed OFT method, which provably preserves hyperspherical energy, outperforms other finetuning techniques in generating high-fidelity and controllable images for tasks like subject-driven generation and controllable generation.In summary, the paper introduces OFT as a way to effectively guide powerful pretrained text-to-image diffusion models to new tasks while maintaining their strong generative performance, with the core hypothesis that preserving hyperspherical energy is key to preserving semantics.
