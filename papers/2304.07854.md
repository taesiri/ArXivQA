# [Towards Better Instruction Following Language Models for Chinese:   Investigating the Impact of Training Data and Evaluation](https://arxiv.org/abs/2304.07854)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question seems to be:How do different training data factors, such as quantity, quality, and linguistic distribution, impact the performance of instruction-following language models for Chinese?The key hypothesis appears to be that these training data factors significantly influence the performance of models on real-world Chinese instruction-following tasks. Specifically, the authors investigate:- The impact of extending LLaMA's vocabulary and pretraining on Chinese data.- The influence of training data quality by comparing models trained on GPT-3.5 vs GPT-4 generated data. - The effect of linguistic distribution by training models on Chinese vs English data.- The importance of training data quantity by incrementally increasing the dataset size.To test these hypotheses, the authors evaluate various trained models on a manually reviewed test set of 1,000 Chinese instruction samples across diverse real-world scenarios. The goal is to supplement human evaluation with quantitative analysis to provide insights into improving open-source conversational models, especially for Chinese language use cases.


## What is the main contribution of this paper?

 Based on my reading of the paper, the main contributions appear to be:1. Evaluating the impact of different training data factors like quantity, quality, and linguistic distribution on instruction-following model performance. The analysis is grounded in several publicly available high-quality instruction datasets and the authors' own Chinese multi-turn conversations. 2. Assessing various models on a manually reviewed evaluation set of 1,000 Chinese instruction samples covering 9 real-world scenarios. This provides quantitative analysis to complement manual evaluation and offer insights into advancing open-source conversational models.3. Extending the vocabulary of LLaMA and conducting secondary pretraining on 3.4B Chinese words to improve its efficiency in processing Chinese data. This reduced training/inference time by 60% without compromising performance.4. Making their model, data, and code publicly available to contribute to the development of more accessible and efficient open-source conversational models, especially for Chinese.In summary, the key contribution is a comprehensive investigation and comparison of how training data factors impact model performance on real-world Chinese instruction following, along with optimizations to improve LLaMA's Chinese language capabilities. The analysis provides valuable insights while the public resources aim to advance open-source conversational AI.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence TL;DR summary of the paper:The paper investigates the impact of training data factors like quantity, quality, and linguistic distribution on the performance of instruction following language models for Chinese, using publicly available datasets and a new 1,000 sample evaluation set across 9 scenarios.


## How does this paper compare to other research in the same field?

 Here is a summary of how this paper compares to other research in the field of instruction-following language models:- Focus: This paper focuses specifically on evaluating Chinese instruction-following models in a comprehensive and in-depth way. Many other papers have evaluated large language models more generally on a wider range of tasks. - Data: The paper utilizes publicly available high-quality instruction datasets like Alpaca and ShareGPT, as well as proprietary Chinese conversation data. Other papers often use proprietary datasets or generate their own data.- Evaluation: The evaluation uses a set of 1,000 diverse real-world instruction samples across 9 scenarios. Many other evaluations use common NLP datasets or human evaluations on a smaller set of examples.- Analysis: The paper provides an in-depth analysis of how training data factors like quality, quantity, and linguistic distribution impact model performance. Other works have not investigated these factors as thoroughly.- Model: The paper enhances LLaMA's vocabulary and pretraining for Chinese. Most other works start from a standard pretrained model without this language-specific optimization.- Openness: The model, data, and code are publicly released to facilitate reproducibility. Many other proprietary models only report overall performance metrics.Overall, this paper provides useful insights into training Chinese instruction-following models by conducting a more comprehensive quantitative evaluation grounded in real-world use cases. The focus on analyzing training data factors and optimizing for Chinese also distinguishes this work from much of the related research. The commitment to openness is a notable contribution to the field.


## What future research directions do the authors suggest?

 Based on my reading of the paper, some of the main future research directions suggested by the authors include:- Continuing to build more comprehensive and high-quality evaluation datasets. The authors point out limitations in their current evaluation set and argue that developing better evaluation data should be a priority. They suggest this data should cover diverse usage scenarios and have a balanced difficulty distribution.- Expanding the vocabulary and adapting models like LLaMA to better handle Chinese text. The authors show their extended vocabulary and continued pre-training on Chinese text improves efficiency for Chinese scenarios. More work could be done to optimize models for Chinese specifically.- Investigating the impacts of different training data factors more thoroughly. The authors explore effects of data quantity, quality, and linguistic distribution, but more analysis could be done on other factors like topic diversity.- Developing better content filtering and data cleaning methods to improve training data quality at scale. The authors manually cleaned some data, but automated methods are needed for larger datasets.- Continuing to close the performance gap between open-source models and proprietary models like ChatGPT. The authors' model still lags significantly behind ChatGPT, so more model architecture tweaks and training is needed.- Testing different parameter-efficient tuning approaches beyond full finetuning. The authors use full finetuning, but methods like adapter tuning may enable using more data.- Exploring multitask training to improve robustness and coverage of models. The authors' evaluation is focused on single turn instructions, so multitask training could help for multi-turn conversations.In summary, the main suggestions are to keep improving evaluation, optimize for Chinese, scale up high-quality training data, and continue advancing model architectures and training techniques to close the gap to state-of-the-art proprietary models. Robust evaluation and diverse training data seem to be the most critical next steps emphasized.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the key points from the paper:This paper investigates the influence of various training data factors on the performance of instruction-following conversational models, with a focus on Chinese scenarios. The authors evaluate different models trained on publicly available high-quality instruction datasets as well as proprietary Chinese multi-turn dialogues. They assess the models on a manually reviewed evaluation set of 1000 Chinese instruction samples covering 9 real-world use cases. Key factors analyzed include training data quantity, quality, and linguistic distribution. The authors find that higher quality and larger quantity of Chinese training data leads to better performance on their Chinese test set. They also extend the vocabulary of the open-source LLaMA model and pre-train it on Chinese text to improve its efficiency in processing Chinese. Overall, this study provides insights into data and modeling factors that influence instruction-following for conversational agents, with a useful focus on Chinese language scenarios. The authors contribute their model, data and code publicly to benefit the community.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the key points from the paper:The paper investigates the influence of various training data factors on the performance of instruction-following conversational models. The analysis uses several publicly available high-quality instruction datasets and the authors' own Chinese multi-turn conversations. The evaluation consists of 1,000 manually reviewed samples covering 9 real-world scenarios. The goal is to supplement qualitative analysis with quantitative results to provide insights into improving open-source chat models. The study examines factors like training data quantity, quality, and linguistic distribution. It extends the vocabulary and conducts secondary pretraining of LLaMA on Chinese text to boost efficiency in the Chinese domain, reducing training and inference time by 60%. The considerable gap between the models and ChatGPT motivates continued efforts to improve open-source conversational models. The challenges of building a comprehensive evaluation set are also discussed. The model, data, and code are publicly released to contribute to accessible and efficient conversational models for Chinese.


## Summarize the main method used in the paper in one paragraph.

 Here is a one paragraph summary of the main method used in the paper:The paper investigates the influence of various training data factors on the performance of instruction-following models. The authors evaluate different models trained on publicly available datasets as well as their own Chinese multi-turn conversations. The main method is to train variants of the LLaMA model using different combinations of the training datasets, including varying the language (Chinese vs English), data quantity, and data quality. The models are evaluated on a set of 1,000 Chinese instruction samples across 9 real-world scenarios. In addition, the authors extend LLaMA's vocabulary and conduct secondary pretraining on Chinese text to improve its efficiency in processing Chinese data. The results demonstrate the importance of high-quality and large-scale training data for achieving strong instruction-following performance. The paper contributes reproducible insights to guide the advancement of open-source conversational models.


## What problem or question is the paper addressing?

 Based on my reading, the key points of this paper are:1. The paper aims to investigate the influence of various training data factors on the performance of instruction following language models, with a focus on Chinese scenarios. 2. The factors examined include training data quantity, quality, and linguistic distribution. The models are evaluated on a manually reviewed test set of 1000 Chinese instruction samples across 9 real-world usage scenarios.3. The goal is to supplement manual evaluations with quantitative analysis to provide insights into improving open-source conversational models, especially in the Chinese domain. 4. The paper also extends the vocabulary of LLaMA and conducts secondary pretraining on Chinese data to enhance its efficiency in processing Chinese instructions.5. The overall research question seems to be: How do factors like training data quantity, quality and linguistic distribution impact the performance of instruction following models on real-world Chinese instruction tasks? The paper aims to investigate this question through empirical experiments and evaluation.In summary, the key focus is on systematically evaluating how different training data factors affect instruction following models for Chinese, with the goal of advancing open-source conversational models. The paper supplements human evaluation with quantitative results on a diverse test set.
