# [AtP*: An efficient and scalable method for localizing LLM behaviour to   components](https://arxiv.org/abs/2403.00745)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem Statement
- Identifying which internal components (e.g. attention heads, neurons) are causally responsible for a model's behavior is important for interpretability, but exhaustively evaluating contributions across all components is prohibitively expensive for large models.

- "Activation Patching" measures a component's contribution by replacing its activation with a reference activation taken from a corrupted input, and measuring the change in model output. But it still requires separate forward passes for each component. 

- The paper aims to find a much faster approximation that can reliably estimate contributions for all components simultaneously.

Proposed Solution - Attribution Patching (AtP)
- AtP uses first-order Taylor expansion of the model output w.r.t. a component's activation. This linear approximation can be computed for all components in just 2 forward passes and 1 backward pass.

- However, AtP has failure modes leading to false negatives: 
  - Attention saturation: AtP's linear approx fails when attention distribution changes drastically between clean and reference input.
  - Cancellation: Positive and negative effects can cancel out across prompt pairs.
  
- To address this, the paper proposes "AtP*":
  - Fixes attention saturation issue by explicitly recomputing attention probabilities when patching queries/keys.
  - Reduces cancellation issue by modifying backpropagation to "drop" some downstream gradients.
  
- The paper also proposes diagnostic methods to statistically bound the effect size of any remaining false negatives.

Contributions
- First systematic evaluation of AtP showing it significantly outperforms alternatives like subset sampling or blocking.

- Identification and mitigation of AtP's failure modes to create more reliable AtP*.

- Exploration of AtP for edge attribution, and analysis of computational tradeoffs.

- Guidance on best attribution method to use based on use case constraints.

In summary, the paper demonstrates that AtP and AtP* enable much faster yet still reliable estimation of model component contributions compared to traditional activation patching, enabling more scalable interpretability.
