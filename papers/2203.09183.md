# [RoMe: A Robust Metric for Evaluating Natural Language Generation](https://arxiv.org/abs/2203.09183)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the main research question of this paper is: How can we develop an automatic evaluation metric for natural language generation (NLG) systems that robustly captures semantic similarity, syntactic structure, and grammatical correctness?The key points are:- The authors argue that existing NLG evaluation metrics like BLEU, METEOR, and ROUGE focus mainly on word overlap and are not robust enough to syntactic variations (e.g. passive voice) or grammatical issues. - They propose a new metric called RoMe that incorporates three main components:    1) Earth Mover's Distance (EMD) to measure semantic similarity, handling issues like repetitive words.    2) Semantically enhanced Tree Edit Distance (TED-SE) to measure syntactic similarity.    3) A grammatical acceptability classifier.- RoMe combines these components using a neural network to generate a robust automated evaluation score.- Through experiments on various NLG datasets, the authors demonstrate that RoMe correlates better with human judgments compared to prior metrics.- They also perform extensive robustness testing using text perturbations to show RoMe is more robust than other metrics.In summary, the main research question is how to develop a robust automated NLG evaluation metric combining semantic, syntactic, and grammatical assessments. RoMe is proposed as a solution to this question.


## What is the main contribution of this paper?

The main contribution of this paper is proposing RoMe, a robust automatic evaluation metric for natural language generation (NLG) systems. The key aspects of RoMe are:- It uses Earth Mover's Distance (EMD) to compute semantic similarity between the reference and hypothesis sentences. To handle issues like repetitive words and passive voice, it incorporates techniques like hard word alignment and soft penalization into the EMD computation. - It uses a semantically enhanced Tree Edit Distance (TED) to compute syntactic similarity between dependency parse trees of the reference and hypothesis. This allows capturing differences in syntactic structures.- It incorporates a classifier that evaluates the grammatical acceptability of the generated hypothesis. - It combines the scores from EMD, enhanced TED, and grammatical acceptability into a feature vector that is fed into a neural network to produce the final quality score.- It is more robust compared to previous metrics based on extensive testing on perturbed/adversarial examples.So in summary, the main contribution is proposing a more robust automated metric that captures semantic, syntactic, and grammatical aspects effectively to correlate better with human judgments across diverse NLG tasks and datasets. The robustness testing and improvements to handle issues like repetitive words etc. are also key contributions.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper proposes RoMe, a new automatic evaluation metric for natural language generation systems that incorporates semantic similarity, syntactic similarity, and grammatical acceptability features to generate a robust overall quality score that correlates well with human judgments.


## How does this paper compare to other research in the same field?

Here are my thoughts on how this paper compares to other research in evaluating natural language generation systems:- The paper proposes a new automatic evaluation metric called RoMe that aims to address some limitations of existing metrics like BLEU, METEOR, etc. It incorporates semantic similarity, syntactic similarity, and grammatical acceptability into a single metric. This makes it more comprehensive than many existing metrics that focus only on n-gram overlap or semantic similarity. - A key contribution is the modifications made to earth mover's distance to better handle issues like repetitive words and passive voice. Many existing metrics struggle to appropriately score these types of variants. The soft-penalization and hard word alignment techniques seem to improve handling of these issues.- Using a neural network classifier on top of the metric features is a common technique now for learning evaluation metrics. Other recent papers have done this as well, like MoverScore and BERTScore. However, RoMe incorporates some unique features into the classifier like the enhanced EMD and TED scores.- The robustness analysis is a nice addition and addresses an important concern - how well do these metrics hold up to various perturbations and attacks on the text? The analysis helps validate that RoMe is more robust than metrics like BLEU.- For datasets like BAGEL and WebNLG, RoMe achieves state-of-the-art correlation with human judgments, outperforming other existing metrics. But on some datasets like WebNLG, the difference is small compared to MoverScore.- The code and annotation tools will be released which is an important contribution to the community. This will support further research and validation of results.Overall the paper makes solid contributions in advancing evaluation metrics for NLG. The modifications for EMD and use of syntactic features differentiate RoMe from some other existing methods and seem to improve performance. However, the gains are incremental in some cases. The robustness analysis also adds useful validation.


## What future research directions do the authors suggest?

Based on my reading of the paper, some of the main future research directions suggested by the authors are:- Extending RoMe to support more languages beyond just English. The current version of RoMe focuses on evaluating English sentences, but the authors mention wanting to expand it to handle multiple languages in the future. - Improving the fluency assessment capabilities of RoMe. The robustness analysis showed there is still room for improvement in how well RoMe handles evaluating the fluency of perturbed/adversarial texts. The authors suggest better handling of keywords like entities may lead to improved correlation with human judgments.- Incorporating more syntactic features into the evaluation. The authors note that RoMe currently uses a semantically enhanced tree edit distance to compare syntax trees. But they suggest exploring the inclusion of other syntactic features like constituent labels, head-modifier dependencies, etc. could further improve the syntactic analysis.- Exploring supervised approaches and fine-tuning contextualized models. The current RoMe uses a simple self-supervised model, but the authors suggest investigating supervised approaches and fine-tuning large pre-trained language models may further improve performance. - Conducting more human annotation studies. The authors recognize the need for expanded human annotation studies on diverse NLG tasks and datasets to further analyze how automated metrics correlate with human judgments in different scenarios.In summary, the main future directions are developing RoMe into a more robust, syntactically-aware, and multilingual metric, using more sophisticated modeling approaches and expanded human annotated datasets. The authors lay out a research roadmap for improving automatic NLG evaluation.


## Summarize the paper in one paragraph.

Here is a one paragraph summary of the paper:The paper proposes RoMe, a new automatic evaluation metric for assessing the quality of natural language generation (NLG) systems. RoMe combines several features related to semantic similarity, syntactic similarity, and grammatical acceptability to produce an overall quality score. First, it uses an enhanced version of Earth Mover's Distance (EMD) to compute semantic similarity between the reference and hypothesis sentences. This enhanced EMD incorporates techniques like hard word alignment and soft penalization to handle issues like repetitive words and passive voice. Second, it computes syntactic similarity using a semantically enhanced Tree Edit Distance (TED) algorithm on dependency parse trees. Third, it predicts grammatical acceptability using a binary classifier trained on linguistics data. Finally, it trains a simple neural network in a self-supervised manner to combine the EMD, TED, and grammar scores into an overall quality score. Experiments on NLG datasets from various domains show RoMe correlates better with human judgments than metrics like BLEU, METEOR, BERTScore, and MoverScore. The authors also conduct robustness tests using adversarial techniques which demonstrate RoMe's capabilities in handling diverse linguistic phenomena.


## Summarize the paper in two paragraphs.

Here is a two paragraph summary of the key points from the paper:The paper proposes an automatic evaluation metric called RoMe for assessing the quality of natural language generation (NLG) systems. RoMe uses several features to evaluate an NLG system's output: 1) Earth Mover's Distance (EMD) to measure semantic similarity between the generated text and reference text, incorporating techniques to handle word repetition and passive voice. 2) Semantically enhanced Tree Edit Distance (TED) to compare syntactic structures of reference and hypothesis sentences. 3) A classifier that predicts the grammatical acceptability of the generated text. 4) A neural network that combines the EMD, TED, and grammatical acceptability scores to produce a final quality score for the generated text.Experiments were conducted on varied NLG datasets spanning knowledge graphs, dialog systems, and structured data-to-text generation. RoMe demonstrated stronger correlation with human judgments compared to metrics like BLEU, METEOR, BERTscore, and MoverScore across these datasets. Robustness analysis using text perturbations also showed RoMe handles diverse linguistic variations better than other metrics. The modular design and multiple linguistic features allow RoMe to effectively evaluate key aspects of NLG systems. Future work includes extending RoMe to multiple languages beyond English. Overall, RoMe provides an automatic, robust metric correlated with human assessment of NLG quality across tasks.


## Summarize the main method used in the paper in one paragraph.

Here is a one paragraph summary of the main method used in the paper:The paper proposes RoMe, a robust automated evaluation metric for natural language generation systems. RoMe computes three main features - semantic similarity using Earth Mover's Distance (EMD), syntactic similarity using a semantically enhanced Tree Edit Distance (TED-SE), and grammatical acceptability using a binary classifier. For EMD, the paper incorporates hard word alignment and soft penalization techniques to handle issues like repetitive words and passive voice. TED-SE compares dependency parse trees using cosine similarity rather than exact match to allow handling of synonyms. These three scores are fed into a simple feedforward neural network classifier to generate the final RoMe score, which represents the overall quality of a hypothesis sentence. The network is trained in a self-supervised manner using artificially generated samples. RoMe aims to provide a robust automated metric that captures semantic, syntactic and grammatical qualities of generated text across diverse NLG tasks.
