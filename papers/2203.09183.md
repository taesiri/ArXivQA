# RoMe: A Robust Metric for Evaluating Natural Language Generation

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the main research question of this paper is: How can we develop an automatic evaluation metric for natural language generation (NLG) systems that robustly captures semantic similarity, syntactic structure, and grammatical correctness?The key points are:- The authors argue that existing NLG evaluation metrics like BLEU, METEOR, and ROUGE focus mainly on word overlap and are not robust enough to syntactic variations (e.g. passive voice) or grammatical issues. - They propose a new metric called RoMe that incorporates three main components:    1) Earth Mover's Distance (EMD) to measure semantic similarity, handling issues like repetitive words.    2) Semantically enhanced Tree Edit Distance (TED-SE) to measure syntactic similarity.    3) A grammatical acceptability classifier.- RoMe combines these components using a neural network to generate a robust automated evaluation score.- Through experiments on various NLG datasets, the authors demonstrate that RoMe correlates better with human judgments compared to prior metrics.- They also perform extensive robustness testing using text perturbations to show RoMe is more robust than other metrics.In summary, the main research question is how to develop a robust automated NLG evaluation metric combining semantic, syntactic, and grammatical assessments. RoMe is proposed as a solution to this question.


## What is the main contribution of this paper?

The main contribution of this paper is proposing RoMe, a robust automatic evaluation metric for natural language generation (NLG) systems. The key aspects of RoMe are:- It uses Earth Mover's Distance (EMD) to compute semantic similarity between the reference and hypothesis sentences. To handle issues like repetitive words and passive voice, it incorporates techniques like hard word alignment and soft penalization into the EMD computation. - It uses a semantically enhanced Tree Edit Distance (TED) to compute syntactic similarity between dependency parse trees of the reference and hypothesis. This allows capturing differences in syntactic structures.- It incorporates a classifier that evaluates the grammatical acceptability of the generated hypothesis. - It combines the scores from EMD, enhanced TED, and grammatical acceptability into a feature vector that is fed into a neural network to produce the final quality score.- It is more robust compared to previous metrics based on extensive testing on perturbed/adversarial examples.So in summary, the main contribution is proposing a more robust automated metric that captures semantic, syntactic, and grammatical aspects effectively to correlate better with human judgments across diverse NLG tasks and datasets. The robustness testing and improvements to handle issues like repetitive words etc. are also key contributions.
