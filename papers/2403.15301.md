# [Planning with a Learned Policy Basis to Optimally Solve Complex Tasks](https://arxiv.org/abs/2403.15301)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
Reinforcement learning (RL) methods can successfully solve a variety of sequential decision problems. However, learning policies that can generalize predictably across multiple tasks with non-Markovian reward specifications remains challenging. The paper aims to develop a method that can leverage previously learned knowledge to efficiently solve new tasks specified via finite state automata (FSAs), while still ensuring global optimality.

Proposed Solution: 
The paper proposes to use successor features (SFs) to learn a policy basis, where each policy solves a well-defined sub-problem or sub-goal. For a new task that involves the same sub-problems but specified via an FSA, these policies can be combined through planning to generate an optimal solution without requiring additional learning. 

Key Ideas:
- Learn a set of policies along with their SFs that constitute a convex coverage set (CCS). This CCS forms an expressive policy basis for planning.
- Associate SF dimensions with terminal/exit states in the environment. This reduces the planning to only consider exit states rather than all states.  
- Perform value iteration style planning over the exit states based on the FSA specification to retrieve optimal weights for combining policies in the CCS. This results in the optimal policy for the task.

Contributions:
- Propose using SFs to learn a policy basis suitable for planning in stochastic domains.
- Develop a planning framework that leverages such policy bases for zero-shot generalization to complex FSA-specified tasks.
- Prove that if the CCS policies are optimal, the framework produces globally optimal solutions even in stochastic environments.

The experiments demonstrate the approach can efficiently compose optimal behavior for new unseen tasks, outperforming prior hierarchical and multi-task RL techniques. A key benefit is predictable generalization without retraining.
