# [InstructPix2Pix: Learning to Follow Image Editing Instructions](https://arxiv.org/abs/2211.09800)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is: How can we train a generative model to edit images according to textual instructions, without needing labels that directly pair images and editing instructions?The key points are:- Image editing from textual instructions is an important and useful capability, but lacks paired training data of images and editing instructions. - The authors propose generating a large paired dataset by combining two pretrained generative models - GPT-3 for text and Stable Diffusion for images. - GPT-3 is finetuned to convert image captions to editing instructions and edited captions. Stable Diffusion converts caption pairs to image pairs.- This generated dataset is used to train InstructPix2Pix, a conditional diffusion model for image editing.- At inference time, InstructPix2Pix can edit real images according to user-provided instructions, despite being trained only on synthetic data.So in summary, the main hypothesis is that generating a large paired dataset with other generative models can enable training an instruction-following image editor without real human-annotated data. The key contribution is the proposed training data generation and model training process.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contributions appear to be:- Proposing a method to generate a large paired dataset for instruction-based image editing by combining two pretrained models: a language model (GPT-3) and a text-to-image model (Stable Diffusion). The language model is finetuned to generate editing instructions and modified captions, while the text-to-image model converts caption pairs to image pairs. - Training a conditional diffusion model called InstructPix2Pix on the generated paired dataset. This model takes as input an image and a textual instruction describing how to edit that image. It can perform a diverse range of edits including replacing objects, changing artistic style/medium, modifying attributes, etc.- Demonstrating that despite being trained on synthetic data, InstructPix2Pix generalizes to editing real images from human-written instructions at test time. The model directly performs the edit in the forward pass without needing per-example fine-tuning or inversion.- Analyzing the model quantitatively and qualitatively, comparing it to baseline editing approaches, and performing ablations. The results show it can perform a wide variety of edits not possible with other text-based editing methods.In summary, the key contribution appears to be proposing a novel approach to generate training data for instruction-based image editing, and using this data to train a model that can generalize to editing real images from natural language instructions. The method sidesteps the lack of human-annotated training data for this task.
