# [Findings of the First Workshop on Simulating Conversational Intelligence   in Chat](https://arxiv.org/abs/2402.06420)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Evaluating open-domain conversational AI systems remains challenging due to the lack of reliable automatic metrics and issues with human evaluation in past attempts. There is a need for effective methods to assess these systems' ability to simulate intelligent conversation over multiple turns while positing arguments. 

Proposed Solution:
- This paper introduces a shared task focused on simulating conversational intelligence. Participating systems are interactively evaluated in a live human assessment adapted from the Direct Assessment method successful in machine translation.

Shared Task:  
- Uses fine-tuned DialoGPT-Medium on Freakonomics podcasts as a strong baseline.
- Allows any model architecture and training data. 
- Evaluated by human judges on assigned topics using Likert scale ratings based on performance criteria.
- Aims to provide insights into models' human-like conversation abilities.

Key Contributions:
- Organizes a shared task to assess open-domain dialogue models via reliable human evaluation. 
- Adapts the Direct Assessment method to enable continuous quality measurement and model improvement tracking.
- Compiles and releases conversation data to facilitate research into better metrics and evaluations.
- Overall, advances the state-of-the-art in evaluating conversational intelligence through a rigorous human assessment framework.

The paper concludes by pointing to a site that will provide full results and analysis once the human evaluation completes. The shared task data will also be made public to further research in this direction.
