# [TESSERACT: Eliminating Experimental Bias in Malware Classification   across Space and Time](https://arxiv.org/abs/1807.07838)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Android malware classification has reported very high performance (F1 scores up to 0.99), but these results are likely biased due to unrealistic assumptions in experimental settings.
- Two key sources of bias are identified: 
  (1) Spatial bias: Unrealistic malware-to-goodware ratios in training and testing
  (2) Temporal bias: Temporally inconsistent splits of training/testing data
- These biases cause performance inflation of up to 50% compared to a realistic setting.

Solution:
- Formally define spatio-temporal constraints for unbiased evaluation:
  C1) Temporal training consistency 
  C2) Temporal goodware/malware windows consistency
  C3) Realistic malware percentage in testing
- Introduce a new time-aware performance metric, AUT, to capture classifier robustness to time decay 
- Propose a tuning algorithm to optimize malware F1/Precision/Recall subject to a maximum tolerated error
- Implement an open-source framework, TESSERACT, to evaluate Android malware classifiers without spatial/temporal biases

Contributions:  
- Identify and quantify sources of experimental bias in Android malware classification
- Formally define constraints and metrics for unbiased, comparable evaluations
- Algorithm to tune classifier performance for the minority malware class  
- TESSERACT framework to facilitate further research towards more robust malware classifiers
- Case study with TESSERACT revealing counter-intuitive performance of DREBIN, MaMaDroid and DL classifiers after debiasing  

Overall, the paper makes a strong case that current evaluation practices in Android malware classification are positively biased. By identifying specific sources of bias, proposing constraints to enable unbiased evaluation, and releasing an open-source framework, the authors aim to encourage more rigorous evaluation standards and progress towards more robust malware classifiers.
