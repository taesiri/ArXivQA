# [Weakly Convex Regularisers for Inverse Problems: Convergence of Critical   Points and Primal-Dual Optimisation](https://arxiv.org/abs/2402.01052)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Solving ill-posed inverse problems using variational regularization methods requires choosing a good regularizer. Traditional hand-crafted regularizers have limitations in capturing complex image properties.
- Recently, learned regularizers have shown improved performance but lack theoretical guarantees on behavior and optimization convergence. 
- Existing convergence results focus on global minima, but in practice algorithms converge to critical points. Hence convergence guarantees are needed for critical points.

Proposed Solution:
- Introduce a class of weakly convex regularizers that achieve well-posedness and prove convergence guarantees for critical points under reasonable conditions.
- Show that the primal-dual hybrid gradient method converges for solving the variational problem under weak convexity. An O(log(k)/k) ergodic rate is proven under a Kurdyka-Łojasiewicz condition.

- Propose Input Weakly Convex Neural Networks (IWCNNs) for learning weakly convex regularizers in an adversarial framework. IWCNNs have universal approximation capabilities. 

- Define Adversarial Weakly Convex Regularizers (AWCRs) using IWCNNs and show improved performance over prior learned regularizers in CT reconstruction tasks.

Main Contributions:
- Generalized notion of convergent regularization for critical points using weak convexity
- Convergence guarantees for primal-dual methods under weak convexity
- Introduction of IWCNN architecture and proof of universal approximation 
- Definition of AWCR framework and empirically showed improved reconstruction performance compared to state-of-the-art learned regularizers

The key insight is that weak convexity provides a middle ground between convexity and non-convexity to derive theoretical guarantees while retaining the benefits of non-convex learning.


## Summarize the paper in one sentence.

 Here is a one sentence summary of the key points from the paper:

The paper proves convergence guarantees for weakly convex regularizers in variational regularization of inverse problems via stability analysis and primal-dual optimization rates and shows that input weakly convex neural networks can learn such regularizers in a weakly supervised fashion, with improved CT reconstruction performance compared to related adversarial methods.


## What is the main contribution of this paper?

 This paper makes several contributions related to weakly convex regularisers for solving inverse problems. Some of the main contributions are:

1) Proves existence, stability, and convergence guarantees (in the sense of critical points rather than global minima) for variational regularisation using weakly convex regularisers. 

2) Shows that a class of weakly convex regularisers achieves convergent regularisation in terms of critical points, generalizing previous results that focused only on global minima.

3) Analyzes the convergence of the primal-dual hybrid gradient method for solving the variational problem under weak convexity assumptions, and proves an ergodic O(log(k)/k) rate under a Kurdyka-Łojasiewicz condition.

4) Introduces input weakly convex neural networks (IWCNNs) for learning weakly convex regularisers, proves they have universal approximation capabilities, and defines an adversarial weak convex regulariser (AWCR) using IWCNNs.

5) Validates the theoretical results empirically by training an AWCR for CT image reconstruction, showing improved performance over prior adversarial regularisation methods.

In summary, the main contribution is a comprehensive theoretical and empirical analysis of weakly convex regularisers for inverse problems, including optimization, approximation, and statistical guarantees. The introduced AWCR also advances the state-of-the-art in learned regularisation.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and keywords associated with it include:

- Image reconstruction
- Inverse problems
- Learned regularisation
- Weak convexity 
- Deep learning
- Computed tomography (CT)
- Convergent regularisation
- Primal-dual optimisation
- Input weakly convex neural networks (IWCNN)
- Adversarial regularisation 
- Adversarial weak convex regulariser (AWCR)
- Kurdyka-Łojasiewicz (KL) inequality

The paper proves theoretical guarantees for using weakly convex regularisers in the context of inverse problems and image reconstruction. Key contributions include showing convergence of critical points for certain weakly convex regularisers, proving convergence rates for primal-dual algorithms under weak convexity assumptions, and introducing the IWCNN architecture that allows learning of weakly convex neural network regularisers. The theory is applied to CT image reconstruction using a proposed adversarial weak convex regulariser (AWCR).


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 potential in-depth questions about the method proposed in the paper:

1) The paper proposes an adversarial weak convex regulariser (AWCR) for learned regularisation in inverse problems. How does the AWCR architecture balance expressivity and provable guarantees compared to previous adversarial regularisers like the adversarial convex regulariser (ACR)?

2) Theorem 3 proves a universal approximation result for input weakly convex neural networks (IWCNNs). What role does this result play in justifying the use of IWCNNs within the AWCR framework? How does it overcome limitations of input convex neural networks?  

3) The paper aims to prove guarantees not just for convergence of global minima but for critical points. Why is this an important distinction? What challenges arise in proving convergence guarantees for critical points?

4) How do the stability and convergence guarantees for the inverse problem in Theorems 1 and 2 differ from previous results? What new assumptions are made to handle the weakly convex setting?

5) Primal-dual methods play an important role in the optimisation analysis. What are the key challenges in analysing primal-dual schemes under weak convexity? How does the analysis here differ from previous primal-dual convergence results?  

6) What role does the Kurdyka-Łojasiewicz inequality play in the analysis of ergodic convergence rates for the primal-dual method? When can we expect this property to hold for learned regularisers?

7) The AWCR is interpreted as approximating a distance function to the data manifold. How valid is this interpretation and when might it break down? How could the approximation be further improved?

8) How do the theoretical guarantees translate into the empirical CT reconstruction results? What explains the difference in performance between AWCR and AWCR-PD in the limited data case?

9) The AWCR converges to critical points which are shown to be stable. However, when non-convexity is present there may exist other unstable critical points. Do the results provide any insight into avoiding such points?

10) Weak convexity provides a middle ground between convex and non-convex optimisation. What other applications might benefit from a weakly convex formulation and analysis? What extensions of this work could be meaningful to explore?
