# [Towards Seamless Adaptation of Pre-trained Models for Visual Place   Recognition](https://arxiv.org/abs/2402.14505)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
Visual place recognition (VPR) aims to estimate the location of a query image by retrieving its best match from a database of geo-tagged images. VPR has applications in robot localization and augmented reality. Key challenges in VPR include changes in illumination, weather, viewpoint, and the presence of dynamic objects. Recent vision foundation models show great generalization ability but have not been well explored for VPR. Directly finetuning them for VPR may damage their transferability. Besides, the image representation they produce may focus on irrelevant regions while ignoring discriminative landmarks critical for distinguishing places. Moreover, existing two-stage VPR methods require spatial verification in re-ranking using local features, which is time-consuming.

Proposed Solution:
This paper proposes SelaVPR, a seamless adaptation method to unleash the capability of pre-trained vision models for efficient and effective VPR. Specifically, SelaVPR realizes hybrid global-local adaptation by adding lightweight adapters to the frozen foundation model backbone:

1) Global adaptation adapters after MHA and MLP layers guide the model to focus on discriminative landmarks while ignoring dynamic regions, producing compact global features for candidate retrieval.  

2) Local adaptation up-convolutional layers provide proper dense local features that can directly match across images without spatial verification to re-rank candidates.

A novel mutual nearest neighbor loss is also introduced to optimize local feature similarity.

Main Contributions:

1) A hybrid global-local adaptation method to seamlessly adapt pre-trained models for producing both global and local features tailored for efficient VPR.

2) A mutual nearest neighbor loss guiding effective local adaptation to enable direct feature matching without spatial verification for ultra-fast re-ranking.

3) State-of-the-art VPR performance on benchmarks with faster runtime, using less training data and time. The method ranks 1st on the MSLS challenge leaderboard.

In summary, the paper provides a promising direction to unlock the capability of vision foundation models for practical large-scale VPR applications via sensible adaptation techniques.
