# Copy-and-Paste Networks for Deep Video Inpainting

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is: How can we develop an efficient deep learning framework for video inpainting that produces high quality results with temporal consistency? The key points are:- Video inpainting is more challenging than image inpainting due to the extra temporal dimension and need for coherence. Prior deep learning methods have limitations. - The authors propose a novel deep neural network framework called Copy-and-Paste Networks that exploits information from multiple frames to fill in missing regions in the target frame.- The framework consists of three main components:1) An alignment network to register frames using affine transforms. This allows using more distant frames unlike optical flow approaches.2) A copy network with a context matching module to identify valuable pixels to copy from reference frames.3) A paste network to decode features and fill in the holes.- The model is trained end-to-end with various losses to ensure spatio-temporal coherence.- Results show the method is faster and qualitatively better than prior optimization and learning methods for video inpainting.In summary, the main hypothesis is that a deep learning framework leveraging information across frames with proper alignment and context matching can achieve high quality and efficient video inpainting. The experiments seem to validate this hypothesis.


## What is the main contribution of this paper?

The main contributions of this paper are:- Proposing a novel DNN-based framework called Copy-and-Paste Networks for video inpainting. The network learns to copy corresponding content from reference frames and paste them to fill holes in the target frame.- An alignment network is proposed to compute affine transformations between frames, allowing the network to utilize information from more distant frames. - A context matching module is proposed to determine which pixels from the aligned reference frames are valuable for copying.- The method produces visually pleasing and temporally coherent results while running faster than prior optimization-based methods.- The framework is extended for enhancing over/under exposed frames in videos, which is shown to significantly improve performance on a lane detection task.In summary, the key novelty is the copy-and-paste framework that leverages information from multiple reference frames to fill holes in the target frame after alignment. This allows better leveraging of temporal information compared to prior video inpainting methods.
