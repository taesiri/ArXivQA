# GODEL: Large-Scale Pre-Training for Goal-Directed Dialog

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be:How can we develop a large pre-trained dialog model that is effective for adapting to a wide range of downstream dialog tasks requiring external knowledge/information?The key points are:- Existing pre-trained dialog models like DialoGPT are limited in their ability to leverage external knowledge and be adapted to knowledge-grounded dialog tasks. - The authors propose a new model called GODEL (Grounded Open Dialogue Language Model) that is pre-trained in multiple phases, including a grounded dialog pre-training phase using datasets that require responses conditioned on external knowledge.- They evaluate GODEL on a variety of goal-directed dialog tasks like task-oriented dialog, conversational QA, and open-domain grounded dialog. - The paper shows GODEL outperforms existing pre-trained dialog models like DialoGPT in few-shot fine-tuning setups on these downstream tasks in terms of both human and automatic evaluation.- The paper also introduces a notion of utility for evaluating dialog models, assessing the usefulness of responses beyond just communicative quality. This utility-driven evaluation is shown to have better inter-annotator agreement and correlation with automated metrics compared to intrinsic evaluation alone.So in summary, the main research question is how to develop a pre-trained dialog model that can effectively leverage external knowledge/information for improved performance on downstream goal-directed dialog tasks through a multi-phase pre-training approach.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contributions are:1. Introduction of GODEL (Grounded Open Dialogue Language Model), a large pre-trained language model for dialog that is designed for general-domain conversation and fully open-sourced. 2. GODEL is pre-trained in three phases, incorporating data from web text, publicly-available dialog, and existing corpora that support grounded dialog tasks. The grounded pre-training is designed to enable adapting GODEL to downstream dialog tasks that require external knowledge.3. Evaluation of GODEL on a suite of benchmarks spanning task-oriented dialog, conversational QA, and open-domain dialog. The results show GODEL outperforms prior pre-trained dialog models like DialoGPT in few-shot fine-tuning setups. 4. Introduction of a notion of utility for dialog systems that focuses on the usefulness of responses, rather than just intrinsic qualities like fluency. This utility is shown to offer better inter-annotator agreement and correlation with automated metrics.5. Release of GODEL models, data, and code to serve as strong baselines for future research in goal-directed dialog. The models released range from 220M to 770M parameters, as well as a 175B parameter version based on GPT-J.In summary, the main contributions are the GODEL model itself, the utility-focused evaluation methodology, and the public release of models/data to facilitate research in this area. The results demonstrate the effectiveness of GODEL for goal-directed dialog tasks.
