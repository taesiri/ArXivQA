# [Augmented Sliced Wasserstein Distances](https://arxiv.org/abs/2006.08812)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the key research question seems to be:How to develop a flexible, data-adaptive distance metric between probability distributions that improves upon limitations of existing methods like the sliced Wasserstein distance? The paper proposes a new distance called the "augmented sliced Wasserstein distance" (ASWD) to address this question. The key ideas are:- Map samples to higher-dimensional hypersurfaces parameterized by neural networks before computing the sliced Wasserstein distance. This enables more flexible nonlinear projections compared to linear projections used in existing sliced Wasserstein methods. - Optimize the hypersurfaces used for projection by maximizing the resulting distance between distributions. This makes the metric data-adaptive.- Use an injective architecture for the neural networks to guarantee that the ASWD satisfies the properties of a mathematical metric. - Demonstrate superior performance over existing sliced Wasserstein variants on synthetic data and generative modeling tasks.So in summary, the main hypothesis is that developing a flexible, optimized, nonlinear version of sliced Wasserstein distances can improve performance and overcome limitations of existing approaches. The ASWD method is proposed to test this hypothesis.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contributions appear to be:1. Proposing a new family of distance metrics called augmented sliced Wasserstein distances (ASWDs). The key idea is to first map samples to higher-dimensional hypersurfaces parameterized by neural networks before taking random linear projections and computing the sliced Wasserstein distance. This allows for more flexible nonlinear projections to better capture complex structures of data distributions.2. Providing theoretical analysis to show the conditions under which the ASWD is a valid metric distance. In particular, it is proven that the ASWD is a metric if the mapping to the higher-dimensional space is injective. 3. Presenting an approach to optimize the hypersurfaces using gradient ascent, so that the hypersurfaces can be learned in a data-driven way rather than manually designed. 4. Demonstrating superior performance of the ASWD over other Wasserstein distance variants on both synthetic data and real-world image datasets. Experiments show the ASWD requires fewer projections to achieve strong results.In summary, the main contribution appears to be proposing the ASWD as a new sliced Wasserstein distance metric that employs flexible neural network mappings, can be optimized in an end-to-end manner, and achieves state-of-the-art performance on generative modeling tasks involving distributions in high dimensions.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points in the paper:The paper proposes a new family of distance metrics called augmented sliced Wasserstein distances (ASWDs) which are constructed by first mapping samples to higher-dimensional hypersurfaces parameterized by neural networks - this enables more flexible nonlinear projections that can better capture complex structures of data distributions compared to prior sliced Wasserstein metrics.


## How does this paper compare to other research in the same field?

Based on my reading of the paper, here is a summary of how it compares to other research in the same field:The paper presents a new variant of the sliced Wasserstein distance called the augmented sliced Wasserstein distance (ASWD). The ASWD is compared primarily against existing sliced Wasserstein metrics like the sliced Wasserstein distance (SWD), generalized sliced Wasserstein distance (GSWD), and distributional sliced Wasserstein distance (DSWD).Some key differences between the ASWD and prior work:- The ASWD maps samples to hypersurfaces parameterized by neural networks before computing the sliced Wasserstein distance. This allows more flexible nonlinear projections compared to the linear projections used in the SWD.- The hypersurfaces used in the ASWD can be optimized via gradient ascent to find projections that maximize the distance between distributions. In contrast, the GSWD relies on pre-defined projection functions that cannot be optimized.- The ASWD adaptively learns the projection space from data, while prior methods like GSWD and DSWD require manually specifying the form of the projection function.- The ASWD guarantees a valid metric by using an injective neural network architecture. The GSWD-NN method loses metric properties by using a standard neural network.- Compared to DSWD which optimizes the distribution of projection directions, ASWD optimizes the hypersurfaces themselves.In experiments, the ASWD outperforms prior sliced Wasserstein metrics on tasks like generative modeling, image color transfer, and finding Wasserstein barycenters. The flexibility to learn nonlinear projections seems to give ASWD an advantage.In summary, the key novelty of ASWD is the idea of optimizing hypersurfaces for projection rather than using predefined or randomized projections. This makes ASWD more flexible and adaptive compared to previous sliced Wasserstein metrics. The results demonstrate the value of this idea across several experiments.
