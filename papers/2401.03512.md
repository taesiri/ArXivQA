# [Token-free LLMs Can Generate Chinese Classical Poetry with More Accurate   Format](https://arxiv.org/abs/2401.03512)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Chinese classical poetry has strict format requirements on number of lines and characters per line. 
- Existing token-based LLMs often generate poems with improper format, with excess or insufficient characters.
- This is likely due to the difficulty of "token planning", as models have limited knowledge of the token-character relationships in Chinese.

Proposed Solution:
- Convert token-based LLM to a token-free model by removing all long tokens from vocab, keeping only character and byte-level tokens.  
- Tailor the inputs and outputs of the model to operate only at character-level.
- Finetune the token-free LLM on a poetry generation dataset with format requirements.

Main Contributions:
- Confirm token-based LLMs have deficient knowledge of token-character relationships through a "spelling bee" probing task. 
- Demonstrate an effective method to convert a token-based LLM (Qwen-chat-7B) into a token-free model.
- The tailored token-free model achieves 96% accuracy on poetry format requirements, much higher than token-based counterpart (84%) and GPT-4 (38%).
- Show token-free model can inherit strong language generation abilities from the original LLM after conversion.
- Release an AI Poet agent with the ability to generate Chinese classical poetry with proper format following complex instructions.

In summary, the paper clearly identifies limitations of token-based LLMs for format-sensitive generation tasks, and provides an effective token-free solution resulting in greatly improved accuracy. The tailored token-free LLM also retains strong language abilities inherited from the original model.


## Summarize the paper in one sentence.

 This paper proposes a token-free LLM tailored from Qwen-chat-7B to improve Chinese classical poetry generation by overcoming the token planning problem faced by token-based models, achieving higher format accuracy.


## What is the main contribution of this paper?

 The main contributions of this paper are:

1. It probes existing token-based large language models (LLMs) like Qwen-chat and finds that their knowledge of the token-character relationship in Chinese is limited. This lack of knowledge makes it difficult for token-based models to do "token planning" for character-sensitive tasks like generating Chinese poetry with proper formatting.

2. It shows a method to tailor a token-based LLM into a token-free model by removing long tokens from the vocabulary and adding a mask to the output logits. This transforms the model to operate at the character and byte level for Chinese text.

3. It trains the tailored token-free model, based on Qwen-chat-7B, on a poetry generation instruction dataset. This finetuned model achieves much higher format accuracy of 0.96 compared to token-based Qwen-chat-7B (0.84) and GPT-4 (0.38). It also outperforms the specially designed poetry generation model Jiuge-GPT-2 (0.91).

In summary, the main contribution is demonstrating a token-free modeling approach that takes advantage of existing pretrained LLMs and achieves state-of-the-art performance on generating Chinese classical poetry with proper formatting. The paper verifies the limitations of token-based models and provides an effective solution.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and keywords associated with this paper include:

- Token-free LLMs - The paper proposes using token-free large language models, rather than token-based models, for Chinese text generation. This removes dependency on tokenization algorithms like BPE or WordPiece.

- Chinese classical poetry generation - The paper focuses on improving accuracy in generating Chinese poetry with proper formatting and character count requirements.

- Format requirements - The strict formatting rules for lines and stanzas in classical Chinese poetry styles like Shi and Ci. The paper aims to improve adherence to these rules.

- Token planning - The paper hypothesizes that the difficulty of planning out tokens to meet precise character counts causes formatting errors in poetry generation.  

- Spelling bee probing - A probing method used to test language models' knowledge of the relationship between tokens and the characters that make them up.

- Tailoring/Finetuning LLMs - The approach of tailoring or finetuning existing large language models into token-free models, rather than training from scratch.

- Qwen-chat - The paper tailors and finetunes the Qwen-chat LLM into a token-free model for improved poetry generation.

Does this summary cover the key terms and topics associated with this paper? Let me know if you need any clarification or have additional questions.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 potential in-depth questions about the method proposed in this paper:

1. The paper proposes that the token planning issue stems from insufficient knowledge of token-character relationships in language models. How exactly did the authors measure and quantify this knowledge gap? What specific probing tasks and metrics were used?

2. Tailoring an existing token-based model into a token-free model seems non-trivial. What are some key challenges in removing certain tokens from the vocabulary while retaining much of the model's capabilities? How did the authors address potential degradation in quality?

3. The authors compare the tailored token-free model to both the original token-based model and other poetry generation models. What are the relative pros and cons between these approaches? Under what circumstances might the token-free model perform worse?

4. What are some examples of complex poetry generation tasks that token-based models struggle with but the tailored token-free model handles well? How does the model architecture enable this?

5. How large was the dataset used to fine-tune the token-free model? What was the diversity of poetry forms and content themes represented? How might limitations in the dataset impact overall quality?  

6. Besides accuracy of format/structure, how does the semantic relevance and coherence of the generated poems compare between models? What automatic and human evaluation was done?

7. The authors focus specifically on classical Chinese poetry forms in this work. How well might the approach transfer to generating modern free-form poetry? What adjustments would be required?

8. What opportunities are there to further enhance the tailored token-free model's capabilities? For example, could retained multi-character tokens be re-introduced in a more strategic manner?

9. How efficiently can the tailored token-free model generate poetry, compared to alternatives? How does tailoring impact computational overhead for inference?

10. Looking ahead, how well might this token manipulation approach work for other language generation tasks where precision character/word count matters? What are some promising application areas?
