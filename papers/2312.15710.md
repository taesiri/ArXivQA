# [Alleviating Hallucinations of Large Language Models through Induced   Hallucinations](https://arxiv.org/abs/2312.15710)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Large language models (LLMs) like ChatGPT can generate impressive text, but sometimes include inaccurate or fabricated information, referred to as "hallucinations". These hallucinations limit the practical applicability of LLMs. 
- Prior work has tried methods like strategic data selection, reinforcement learning from feedback, and knowledge retrieval to reduce hallucinations. However, these methods have limitations around feasibility and effectiveness.

Proposed Solution: 
- The paper proposes a simple yet effective "induce-then-contrast decoding" (ICD) strategy to mitigate LLM hallucinations.  
- First, they construct a "factually weak" LLM by inducing hallucinations from the original LLM using fine-tuning or prompting.  
- Then during decoding, they penalize the predictions from this weak LLM to guide the original LLM to generate more factual text. 

Key Contributions:
- Demonstrate hallucinations can be readily induced from LLMs through slight fine-tuning or zero-shot prompting.
- Propose to treat such induced hallucinations as a penalty term to compel LLMs to generate factual text.
- Show ICD boosts performance of LLMs like Llama, Mistral, Baichuan on hallucination benchmarks TruthfulQA and FActScore.
- ICD enables smaller LLMs to match performance of much larger models like ChatGPT and GPT-4 on reducing hallucinations.
- Additional analyses provide insights into data size, task formats, model sizes, etc. for effective application of ICD.

In summary, the paper presents a lightweight yet effective ICD strategy to mitigate hallucinations of LLMs by first inducing and then penalizing fabricated information.
