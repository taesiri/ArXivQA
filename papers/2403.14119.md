# [C-TPT: Calibrated Test-Time Prompt Tuning for Vision-Language Models via   Text Feature Dispersion](https://arxiv.org/abs/2403.14119)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Test-time prompt tuning methods like TPT improve the accuracy of vision-language models like CLIP without needing labeled data. However, they reduce prediction entropy, worsening the calibration (alignment between predicted and true probabilities). 
- Traditional calibration techniques require substantial labeled data, making them inapplicable for test-time prompt tuning.

Key Observations:
- The choice of prompt significantly impacts the calibration error even at similar accuracy levels. Some prompts offer better calibration.
- Well-calibrated prompts exhibit wider dispersion of class-embedded text features compared to poorly-calibrated prompts. There is a strong negative correlation between text feature dispersion and calibration error.

Proposed Solution - Calibrated Test-Time Prompt Tuning (C-TPT):
- Defines Average Text Feature Dispersion (ATFD) to quantify dispersion of text features. Establishes quantitatively the strong negative correlation between ATFD and Expected Calibration Error (ECE).
- Jointly optimizes prompt during test-time to maximize accuracy (using TPT) and calibration (by maximizing ATFD). Enables calibration improvements without needing labeled data.

Key Results:
- When combined with TPT, C-TPT reduces the calibration error by 47-56% for CLIP-RN50 and 52-53% for CLIP-ViT-B/16 on fine-grained classification datasets, without compromising accuracy gains.
- Effective calibration improvements also demonstrated under natural distribution shifts.
- Shows the applicability of C-TPT for training-time prompt tuning methods.

Main Contributions:
- First comprehensive study revealing impact of prompts on calibration in CLIP prompt tuning.
- Proposes tailored technique, C-TPT, to enable calibration during test-time prompt tuning without needing labeled data by exploiting inherent CLIP properties.
- Extensive experiments validate effectiveness of C-TPT in enhancing calibration across datasets and CLIP architectures.
