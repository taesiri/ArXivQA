# [Where does In-context Translation Happen in Large Language Models](https://arxiv.org/abs/2403.04510)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem Statement:
- In-context learning (ICL) allows large language models (LLMs) like GPT-3 to perform various tasks by conditioning on example inputs and desired outputs in the context, without explicit training. However, little is known about where in the model's layers the actual task recognition and execution happens. 

- This paper studies where machine translation (MT) emerges in GPT models via ICL. MT is used as the task for analysis since there is little ambiguity in evaluating if the model translates to a different language or not.

Methodology: 
- The authors introduce "layer-from context masking", which masks attention to the context tokens (prompt examples and instructions) from a certain layer onwards during inference. 

- Several causal masking experiments are conducted on GPTNeo 2.7B, Bloom 3B, Llama 7B and Llama 7B-chat on English <=> French MT.

Key Findings:
- Models do not need to attend to context across all layers to perform MT via ICL. There is a "task recognition point" where attention over context is no longer needed.

- There are critical middle layers responsible for locating the translation task. Masking them significantly harms performance. Subsequent layers are more redundant.

- The task recognition point results in potential computational savings during inference when prompt is longer than test input.

- Lightweight fine-tuning (LoRA layers) is most impactful in earlier critical layers compared to later redundant ones.

- Around 10% of heads can be removed in ICL MT models unlike >90% heads in supervised MT transformers.

To summarize, the key contributions are introducing layer-from context masking to identify a task recognition point and critical layers for ICL, analysis of redundancy, and comparative fine-tuning of layers.
