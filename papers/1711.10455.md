# [Backprop as Functor: A compositional perspective on supervised learning](https://arxiv.org/abs/1711.10455)

## What is the central research question or hypothesis that this paper addresses?

This paper addresses the question of how to understand and generalize the backpropagation algorithm for supervised learning from a categorical perspective. The key ideas and contributions are:- Identifying the concept of a "request function" as crucial for compositionality of learning algorithms. The request function allows an algorithm to request updated training data from upstream components.- Defining a symmetric monoidal category called Lrn where morphisms are learners (parameter space, implementation function, update function, request function). This provides an algebraic framework to compose learning algorithms.- Showing that for a fixed error function and step size, gradient descent and backpropagation give a functor from parametrized functions to Lrn. This makes precise the sense in which backpropagation is compositional. - Relating neural nets to parametrized functions via a functor, allowing backpropagation on nets to be understood categorically. Bimonoid structure explains nets.- Demonstrating on an example network how the functoriality gives a way to factor and distribute backpropagation computations.- Discussing implications such as broader notions of layered networks, modular combination of learning algorithms beyond backpropagation, relationships to lenses and open games, and potential for non-euclidean data.In summary, the central hypothesis is that a categorical perspective can help explain, generalize, and suggest applications of backpropagation. The request function and category Lrn are key concepts introduced to formalize the compositionality of learning.


## What is the main contribution of this paper?

This paper presents a framework for understanding backpropagation and gradient descent from a categorical perspective. The key contributions are:1. It defines a category Lrn of "learners" where morphisms are learning algorithms consisting of a parameter space, implementation function, update function, and request function. Composition in this category captures how to chain learning algorithms together.2. It shows that for a fixed step size and error function, gradient descent defines a functor from a category Para of parametrized functions to Lrn. This functoriality expresses that gradient descent is compatible with composition of functions. 3. It relates neural networks to parametrized functions via a functor NNet -> Para. Composing with the gradient descent functor gives a way to interpret neural nets as learning algorithms.4. It shows the category Lrn has additional "bimonoids" structure that allows splitting/merging of wires. This expresses how neural nets can be built compositionally from basic units.5. It provides an explicit example of composing two neural net layers into a full net and training it via backpropagation, showing the functoriality in action.In summary, the paper gives a functorial/compositional perspective on supervised learning and backpropagation. This provides a way to understand and generalize neural net architectures and learning in terms of categorical concepts. The compositional view also relates learning algorithms to other structures like lenses and open games.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper defines a category of learning algorithms where gradient descent and backpropagation give a functor from parametrized functions to learning algorithms, providing a compositional perspective on supervised learning that links it to structures from bidirectional programming and open games.


## How does this paper compare to other research in the same field?

Here are a few thoughts on how this paper compares to other research on compositional perspectives on machine learning:- The key novelty of this paper is in defining a symmetric monoidal category of "learners" to model compositionality of learning algorithms. This provides a formal algebraic framework for studying how different learning components can be combined and composed into larger systems. Other work has studied compositionality in machine learning, but not from this categorical perspective.- The paper shows that gradient descent and backpropagation can be understood functorially between categories of parametrized functions and learners. This provides an elegant perspective on backpropagation as a way of "translating" gradients across compositions of functions. Other work has studied backpropagation from an algebraic lens, but the categorical functorial view is unique.- By modeling learning algorithms categorically, the paper connects machine learning to other compositional frameworks like lenses and open games. Making these high-level connections helps integrate machine learning into the broader landscape of compositional systems. This kind of cross-pollination of ideas is valuable for the field.- The emphasis on compositionality aligns with a general trend in machine learning towards more modular, reusable, and interpretable systems. The categorical perspective here formalizes intuitions about composing learning components in a rigorous way. This fits into a larger effort to develop more structured approaches to machine learning.- The treatment of neural networks from a categorical/algebraic view is similar to other categorical perspectives, like work modeling NNs in terms of algebraic structures like monoids. The focus on neural networks here demonstrates the utility of the categorical learning framework.Overall the paper offers a novel algebraic formalism for compositional learning systems. It makes connections to related compositional paradigms and provides conceptual clarity about backpropagation and learning. The categorical perspective opens up new directions for thinking about compositional machine learning.


## What future research directions do the authors suggest?

The paper suggests several promising directions for future research on functorial backpropagation:- Generalizing the error functions that can be used. The main theorem requires the error function's partial derivatives to be invertible. But commonly used error functions like cross-entropy do not satisfy this. The authors discuss how their framework could likely be extended to handle such cases.- Exploring more general types of learners beyond those arising from neural nets and gradient descent. The category of learners is very broad, but so far they have mostly considered the subcategory generated by neural nets. New primitives and languages for specifying learners could be developed.- Incorporating notions of "well-behavedness" for learners, akin to the well-behaved lenses and open games the category of learners resembles. This could lead to ways of ensuring learners converge properly during training.- Developing the bicategorical structure, where 2-morphisms relate learners with different architectures. This could provide a way to formally relate hyperparameters like network depth and width.- Comparing in more detail to bidirectional languages like lenses and open games. This could reveal new structures and methods for machine learning.- Finding more applications of the compositional perspective. The authors describe how it clarifies ideas like weight tying and factoring gradient computations. Additional ways the framework could elucidate machine learning should be explored.In summary, the paper proposes exploring more generalized notions of learners, integrating ideas from related compositional structures, and further developing the applications of the categorical perspective introduced here. Overall it aims to build on the functorial view of backpropagation to create more powerful and broadly applicable learning algorithms.


## Summarize the paper in one paragraph.

Here is a one paragraph summary of the paper:The paper presents a categorical framework for understanding and generalizing backpropagation in neural networks. It introduces the symmetric monoidal category of "learners" where morphisms are learning algorithms consisting of a parameter space, implementation function, update function, and request function. The key insight is that the request function allows compositionality of learning algorithms. The main result is that gradient descent and backpropagation give a functor from parametrized neural networks to learners, showing that backpropagation is functorial/compositional. This allows backpropagation and gradient descent to be applied much more broadly than standard neural networks. The framework also relates learners to bidirectional programming languages ("lenses") and game theory ("open games"). Overall, the categorical perspective provides a formal, compositional understanding of backpropagation and its generalizations for machine learning.


## Summarize the paper in two paragraphs.

Here is a two paragraph summary of the paper "Backprop as Functor: A Compositional Perspective on Supervised Learning":This paper develops a categorical framework for understanding supervised learning algorithms and their compositionality. The key insight is that in order to compose supervised learning algorithms, we need not just parameter spaces and implementation functions, but also update functions for changing the parameters based on training data, and request functions for requesting updated training data from upstream learners in a composite architecture. The authors define a symmetric monoidal category called Lrn where the morphisms are learning algorithms consisting of these four components. They show that for a fixed choice of error function and step size, gradient descent and backpropagation define a functor from a category of parametrized functions to Lrn. This functoriality encodes the fact that backpropagation allows gradient descent to be performed locally, by message passing between components. The framework provides a broad conceptualization of learning algorithms, relating neural networks, bidirectional programming languages, and mathematical optimization. It offers new perspectives on composing and structuring machine learning systems.


## Summarize the main method used in the paper in one paragraph.

Here is a one paragraph summary of the main method used in the paper:The paper develops a categorical framework to show that backpropagation for neural networks arises as a result of the functoriality of gradient descent. Specifically, the authors define a category Lrn where morphisms are learning algorithms consisting of a parameter space, an implementation function, an update function, and a request function. They then show that for a fixed choice of error function and step size, gradient descent defines a functor from a category Para of parametrized functions to Lrn. This functor sends parametrized functions to learning algorithms where the update function does gradient descent and the request function computes the backpropagation values. The functoriality encodes the fact that gradient descent and backpropagation are compositional, allowing efficient computation by breaking down networks into subparts. Overall, the categorical perspective provides a formal understanding of how gradient descent and backpropagation arise systematically in neural network learning.


## What problem or question is the paper addressing?

This paper is addressing the problem of understanding backpropagation and supervised learning from a compositional, algebraic perspective. Some key aspects:- Backpropagation for neural networks is an effective method for training neural networks, but can seem somewhat mysterious. The goal is to better understand how and why it works through the lens of category theory.- Supervised learning algorithms search over a parameter space to find functions approximating some ideal function. The paper wants to formalize how these algorithms can be composed.- The key insight is that to compose learning algorithms, you need not just a parameter space and implementation function, but also an update rule and a "request function". The request function allows passing information backwards to construct training data for upstream learners.- The paper shows learning algorithms with these components form a symmetric monoidal category called Lrn. This provides an algebraic framework to understand learning algorithm composition.- Gradient descent and backpropagation for a fixed step size and error function that satisfies certain properties is shown to give a functor from parametrized functions to Lrn. This makes the backpropagation algorithm itself compositional. - This perspective links learning algorithms to structures from bidirectional programming languages and microeconomics. It also provides a broader understanding of backpropagation beyond just neural networks.In summary, the goal is to develop a compositional, algebraic framework for understanding supervised learning and backpropagation, providing new tools and insight into these methods. Category theory and the symmetric monoidal category Lrn are the main conceptual tools used.
