# [Backprop as Functor: A compositional perspective on supervised learning](https://arxiv.org/abs/1711.10455)

## What is the central research question or hypothesis that this paper addresses?

This paper addresses the question of how to understand and generalize the backpropagation algorithm for supervised learning from a categorical perspective. The key ideas and contributions are:- Identifying the concept of a "request function" as crucial for compositionality of learning algorithms. The request function allows an algorithm to request updated training data from upstream components.- Defining a symmetric monoidal category called Lrn where morphisms are learners (parameter space, implementation function, update function, request function). This provides an algebraic framework to compose learning algorithms.- Showing that for a fixed error function and step size, gradient descent and backpropagation give a functor from parametrized functions to Lrn. This makes precise the sense in which backpropagation is compositional. - Relating neural nets to parametrized functions via a functor, allowing backpropagation on nets to be understood categorically. Bimonoid structure explains nets.- Demonstrating on an example network how the functoriality gives a way to factor and distribute backpropagation computations.- Discussing implications such as broader notions of layered networks, modular combination of learning algorithms beyond backpropagation, relationships to lenses and open games, and potential for non-euclidean data.In summary, the central hypothesis is that a categorical perspective can help explain, generalize, and suggest applications of backpropagation. The request function and category Lrn are key concepts introduced to formalize the compositionality of learning.


## What is the main contribution of this paper?

This paper presents a framework for understanding backpropagation and gradient descent from a categorical perspective. The key contributions are:1. It defines a category Lrn of "learners" where morphisms are learning algorithms consisting of a parameter space, implementation function, update function, and request function. Composition in this category captures how to chain learning algorithms together.2. It shows that for a fixed step size and error function, gradient descent defines a functor from a category Para of parametrized functions to Lrn. This functoriality expresses that gradient descent is compatible with composition of functions. 3. It relates neural networks to parametrized functions via a functor NNet -> Para. Composing with the gradient descent functor gives a way to interpret neural nets as learning algorithms.4. It shows the category Lrn has additional "bimonoids" structure that allows splitting/merging of wires. This expresses how neural nets can be built compositionally from basic units.5. It provides an explicit example of composing two neural net layers into a full net and training it via backpropagation, showing the functoriality in action.In summary, the paper gives a functorial/compositional perspective on supervised learning and backpropagation. This provides a way to understand and generalize neural net architectures and learning in terms of categorical concepts. The compositional view also relates learning algorithms to other structures like lenses and open games.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper defines a category of learning algorithms where gradient descent and backpropagation give a functor from parametrized functions to learning algorithms, providing a compositional perspective on supervised learning that links it to structures from bidirectional programming and open games.


## How does this paper compare to other research in the same field?

Here are a few thoughts on how this paper compares to other research on compositional perspectives on machine learning:- The key novelty of this paper is in defining a symmetric monoidal category of "learners" to model compositionality of learning algorithms. This provides a formal algebraic framework for studying how different learning components can be combined and composed into larger systems. Other work has studied compositionality in machine learning, but not from this categorical perspective.- The paper shows that gradient descent and backpropagation can be understood functorially between categories of parametrized functions and learners. This provides an elegant perspective on backpropagation as a way of "translating" gradients across compositions of functions. Other work has studied backpropagation from an algebraic lens, but the categorical functorial view is unique.- By modeling learning algorithms categorically, the paper connects machine learning to other compositional frameworks like lenses and open games. Making these high-level connections helps integrate machine learning into the broader landscape of compositional systems. This kind of cross-pollination of ideas is valuable for the field.- The emphasis on compositionality aligns with a general trend in machine learning towards more modular, reusable, and interpretable systems. The categorical perspective here formalizes intuitions about composing learning components in a rigorous way. This fits into a larger effort to develop more structured approaches to machine learning.- The treatment of neural networks from a categorical/algebraic view is similar to other categorical perspectives, like work modeling NNs in terms of algebraic structures like monoids. The focus on neural networks here demonstrates the utility of the categorical learning framework.Overall the paper offers a novel algebraic formalism for compositional learning systems. It makes connections to related compositional paradigms and provides conceptual clarity about backpropagation and learning. The categorical perspective opens up new directions for thinking about compositional machine learning.
