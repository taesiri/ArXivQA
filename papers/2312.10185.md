# [Student as an Inherent Denoiser of Noisy Teacher](https://arxiv.org/abs/2312.10185)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem Statement
- Knowledge distillation (KD) is widely used to transfer knowledge from a large language model (teacher) to a smaller specialized model (student) in low-data regimes. This is done by using the teacher to generate pseudo labels on unlabeled data which the student learns through label matching.  

- However, pseudo labels from the teacher are often noisy, which can negatively impact student performance. Despite many works using KD with noisy teachers, there is little investigation into optimizing learning from noisy pseudo labels.

Key Finding
- The paper provides an analysis on the learning process during vanilla KD with noisy teachers, both theoretically and empirically. A key finding is that the student model exhibits an inherent ability to "denoise" the noisy teacher labels, generating superior predictions compared to the noisy labels it's trained on.

- This indicates that vanilla KD is suboptimal as the student is forced to learn inferior labels to its own predictions. The denoising occurs because the student tends to converge faster to cleaner teacher labels than noisier ones.

Proposed Solution  
- Motivated by the above finding, the paper proposes Peer-Advised KD (PA-KD) to improve learning from noisy teachers. 

- PA-KD utilizes the highly-converged subset of teacher labels to train a peer student model. This peer model then re-labels the remaining data to get improved pseudo labels for training the ultimate student model.

Main Results
- Experiments on grammar induction datasets demonstrate PA-KD can outperform strong baselines by ~5% F1 with only 50 labeled examples, being competitive with supervised finetuning using 15x more labeled data.

- Additionally, just selectively using highly-converged teacher labels for vanilla KD also achieves strong performance, confirming that learning full noisy labels is suboptimal.

In summary, the key contribution is providing new insights into KD with noisy teachers and proposing an effective PA-KD method to address limitations of vanilla KD. The results also validate the importance of accounting for noise when learning from neural language models.
