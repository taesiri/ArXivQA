# [An Attentive Inductive Bias for Sequential Recommendation Beyond the   Self-Attention](https://arxiv.org/abs/2312.10325)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Transformer-based sequential recommendation (SR) models have two key limitations: 1) Insufficient inductive bias to capture fine-grained sequential patterns. 2) The self-attention mechanism acts as a low-pass filter, causing oversmoothing and failing to capture crucial temporal dynamics. 

- The paper proves that self-attention inherently acts as a low-pass filter, continuously erasing high-frequency signals. This results in oversmoothing where representations become similar, losing ability to capture detailed user preferences.

Proposed Solution:
- Proposes BSARec, a novel SR model using Fourier transform to inject inductive bias and mitigate oversmoothing.

- Leverages Fourier transform's frequency information to gain access to sequential patterns overlooked by self-attention. This enhances inductive bias.

- Introduces a frequency rescaler to extract high and low frequencies. This captures both long-term interests (low freq) and short-term trends (high freq) in user behavior.  

- Trades off between the verified inductive bias and trainable self-attention. Self-attention focuses on capturing non-obvious attentions.

Main Contributions:
- First work to reveal the low-pass filter nature of self-attention in SR models, causing oversmoothing.

- Pioneering use of Fourier transform in SR to inject inductive bias via frequency information. Novel frequency rescaler design for high-pass filters to mitigate oversmoothing.

- BSARec outperforms 7 baselines on 6 benchmark datasets, significantly advancing SR capabilities. Demonstrates effectiveness in tackling limitations of self-attention.

In summary, the paper identifies key limitations of self-attention for SR regarding inductive bias and oversmoothing. It makes seminal contributions through BSARec leveraging Fourier transform to enhance sequential modeling while mitigating oversmoothing via high-pass filters. Extensive experiments validate BSARec's state-of-the-art performance.
