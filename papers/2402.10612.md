# [Retrieve Only When It Needs: Adaptive Retrieval Augmentation for   Hallucination Mitigation in Large Language Models](https://arxiv.org/abs/2402.10612)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Large language models (LLMs) like ChatGPT sometimes produce factually incorrect or nonsensical text, referred to as "hallucinations". This poses risks for real-world deployment.
- Two main types of hallucinations can occur:
  - Internal hallucinations: Due to limited knowledge of LLMs, they may fail to reason accurately. 
  - External hallucinations: When using retrieval augmentation to fill knowledge gaps, irrelevant information can get incorporated, accumulating errors.

Proposed Solution:
- The paper proposes "Rowen", a novel method to enhance LLMs with selective retrieval augmentation tailored to address hallucinated outputs. 
- It uses a multilingual semantic-aware detection module to evaluate consistency of perturbed responses across languages for the same queries. Inconsistencies indicate potential hallucinations.
- Upon detecting inconsistencies, Rowen activates retrieval of external information to help the LLM rectify its reasoning and correct inaccuracies. If responses remain consistent, the initial LLM-generated answer is retained.

Key Contributions:
- Introduces an innovative multilingual hallucination detection module that perturbs questions and checks cross-lingual semantic consistency of responses.
- Activates selective retrieval augmentation only when hallucinations are detected to minimize incorporating erroneous information.
- Effectively balances utilization of parametric knowledge within LLM and external knowledge sources.
- Experiments on TruthfulQA and StrategyQA datasets show Rowen substantially outperforms prior state-of-the-art in detecting and mitigating hallucinated LLM outputs.

In summary, Rowen adeptly harmonizes internal LLM reasoning and external evidence to mitigate hallucinations through a dynamic switcher mechanism that retrieves knowledge only when needed.
