# [Mirror Descent Policy Optimization](https://arxiv.org/abs/2005.09814v5)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading, the main research questions/hypotheses addressed in this paper are:

1. Can we derive efficient and practical reinforcement learning algorithms based on the principles of mirror descent, which has strong theoretical foundations in convex optimization? 

2. Can mirror descent provide a unified framework to understand, analyze, and improve upon state-of-the-art RL algorithms like TRPO, PPO and SAC?

3. How do the algorithms derived from mirror descent principles, referred to as MDPO, perform compared to TRPO, PPO and SAC on standard continuous control benchmarks?

Specifically, the authors are motivated by recent theoretical works that established connections between mirror descent and trust region policy optimization methods in RL. However, there remains a gap between the theoretically analyzed tabular algorithms and practical deep RL methods. 

The paper proposes to bridge this gap by deriving on-policy and off-policy MDPO algorithms that approximate the mirror descent updates by taking multiple SGD steps. It then highlights theoretical connections of on-policy MDPO to TRPO and PPO, and shows how SAC can be derived from off-policy MDPO.

The main hypothesis is that algorithms derived from mirror descent principles can match or outperform current state-of-the-art methods, while being simpler, more efficient, and providing a unifying view. The results on standard MuJoCo benchmarks generally validate this hypothesis.

In summary, the core research questions are around deriving practical RL algorithms based on mirror descent, understanding connections to existing methods, and evaluating their performance against current state-of-the-art. The MDPO algorithms and analyses serve to address these questions.


## What is the main contribution of this paper?

 The main contribution of the paper "Mirror Descent Policy Optimization" is proposing a new reinforcement learning algorithm called MDPO (mirror descent policy optimization). 

The key ideas are:

- MDPO is inspired by the theory of mirror descent, a well-known optimization method, and aims to bridge the gap between theoretically analyzed RL algorithms and practical state-of-the-art algorithms like TRPO and PPO.

- MDPO iteratively updates the policy by approximately solving a trust region optimization problem. This is done by taking multiple gradient steps on an objective function with two terms: one for optimizing the RL reward, and one proximity term to keep consecutive policies close.

- The authors derive on-policy and off-policy variants of MDPO. The on-policy version is related to TRPO and PPO, while the off-policy version is connected to SAC.

- MDPO does not require explicitly enforcing a hard trust region constraint like TRPO. It approximately satisfies the trust region requirement through the multiple gradient steps.

- Experiments across several MuJoCo continuous control tasks show MDPO achieves better or comparable performance to TRPO, PPO and SAC.

In summary, the main contribution is proposing MDPO as a new RL algorithm derived from mirror descent principles. It offers a unified view to understand several popular RL algorithms, while achieving strong empirical performance. MDPO is also simple to implement, not needing constraints or line searches.
