# [FSD-Inference: Fully Serverless Distributed Inference with Scalable   Cloud Communication](https://arxiv.org/abs/2403.15195)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem: 
Serverless computing offers great scalability, elasticity, and cost-effectiveness but constraints on memory, CPU, and function runtime have hindered its adoption for data-intensive applications and machine learning (ML) workloads. Traditional server-based platforms enable distributed computation via fast networks and inter-process communication (IPC) mechanisms like MPI and shared memory. The lack of such solutions in serverless makes parallel computation with significant IPC challenging.

Proposed Solution:
The paper proposes FSD-Inference, the first fully serverless and highly scalable system for distributed ML inference. It introduces novel fully serverless communication schemes using publish-subscribe/queuing and object storage services to enable the required data-intensive function-to-function interactions for large neural network inference. 

The overall architecture runs inference over a tree of FaaS instances. A hierarchical function launch mechanism minimizes startup delays. The inference algorithm performs distributed matrix-vector product across layers, using the introduced IPC channels to gather partial results. Two variants are presented:

1. FSD-Inf-Queue: Uses publish-subscribe and queues for communication. Messages are published to topics that fan-out to target queues. Workers poll queues to receive messages.

2. FSD-Inf-Object: Uses object storage. Workers write/read intermediate results as objects. 

Both variants overlap communication with computation to amortize overheads. The paper also formalizes a cost model for serverless inference and provides design recommendations based on model size, parallelism, and query patterns.
  
Main Contributions:

- First fully serverless and scalable system for distributed ML inference that mitigates FaaS constraints

- Novel serverless point-to-point communication schemes using publish-subscribe/queueing and object storage  

- Hierarchical function launch mechanism to minimize startup delays

- Formalized and validated cost model for serverless inference 

- Design recommendations for serverless ML inference systems

- Experimental analysis showing the system is significantly more cost-effective and scalable than server-based alternatives while achieving competitive performance against optimized HPC solutions
