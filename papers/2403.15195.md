# [FSD-Inference: Fully Serverless Distributed Inference with Scalable   Cloud Communication](https://arxiv.org/abs/2403.15195)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem: 
Serverless computing offers great scalability, elasticity, and cost-effectiveness but constraints on memory, CPU, and function runtime have hindered its adoption for data-intensive applications and machine learning (ML) workloads. Traditional server-based platforms enable distributed computation via fast networks and inter-process communication (IPC) mechanisms like MPI and shared memory. The lack of such solutions in serverless makes parallel computation with significant IPC challenging.

Proposed Solution:
The paper proposes FSD-Inference, the first fully serverless and highly scalable system for distributed ML inference. It introduces novel fully serverless communication schemes using publish-subscribe/queuing and object storage services to enable the required data-intensive function-to-function interactions for large neural network inference. 

The overall architecture runs inference over a tree of FaaS instances. A hierarchical function launch mechanism minimizes startup delays. The inference algorithm performs distributed matrix-vector product across layers, using the introduced IPC channels to gather partial results. Two variants are presented:

1. FSD-Inf-Queue: Uses publish-subscribe and queues for communication. Messages are published to topics that fan-out to target queues. Workers poll queues to receive messages.

2. FSD-Inf-Object: Uses object storage. Workers write/read intermediate results as objects. 

Both variants overlap communication with computation to amortize overheads. The paper also formalizes a cost model for serverless inference and provides design recommendations based on model size, parallelism, and query patterns.
  
Main Contributions:

- First fully serverless and scalable system for distributed ML inference that mitigates FaaS constraints

- Novel serverless point-to-point communication schemes using publish-subscribe/queueing and object storage  

- Hierarchical function launch mechanism to minimize startup delays

- Formalized and validated cost model for serverless inference 

- Design recommendations for serverless ML inference systems

- Experimental analysis showing the system is significantly more cost-effective and scalable than server-based alternatives while achieving competitive performance against optimized HPC solutions


## Summarize the paper in one sentence.

 Here is a one-sentence summary of the paper:

FSD-Inference introduces the first fully serverless system for distributed machine learning inference that achieves high scalability and performance at low cost through novel serverless communication schemes and optimizations such as hypergraph partitioning.


## What is the main contribution of this paper?

 The main contribution of this paper is FSD-Inference, which is presented as the first fully serverless and highly scalable system for distributed machine learning inference. Specifically, the paper:

- Introduces FSD-Inference, a novel system for distributed ML inference that is designed to work fully within a serverless platform like AWS Lambda, overcoming constraints like limited memory and lack of direct inter-function communication.

- Presents two novel fully serverless point-to-point communication schemes for FSD-Inference, using publish-subscribe/queueing services and object storage services respectively. 

- Provides formalized cost models for the different FSD-Inference variants, and offers recommendations on designing serverless ML inference solutions optimized for cost and performance based on model size and workload patterns.

- Demonstrates through experiments that FSD-Inference achieves significant scalability and cost-effectiveness compared to server-based and even HPC-based alternatives, with competitive performance.

- Shows that the publish-subscribe/queueing communication channel has a more favorable cost profile compared to object storage as parallelism increases, making it a viable fully serverless alternative.

In summary, the main contribution is the design, implementation and evaluation of FSD-Inference, the first highly scalable and fully serverless system for distributed machine learning inference in the cloud.


## What are the keywords or key terms associated with this paper?

 Based on my review of the paper's content, some of the key terms and keywords associated with this paper include:

- Fully serverless distributed inference
- Function-as-a-Service (FaaS)
- Intra-layer model parallelism
- Hypergraph partitioning
- Point-to-point communication schemes
- Publish-subscribe/queueing communication channel
- Object storage communication channel
- Cost model
- Sporadic inference workloads
- AWS Lambda
- Message passing interface (MPI)
- Sparse deep neural networks

The paper presents a system called FSD-Inference for distributed machine learning inference using FaaS offerings like AWS Lambda. It introduces novel serverless communication schemes like publish-subscribe/queueing and object storage to enable scalable inference. Key innovations include intra-layer model parallelism across Lambda functions achieved via hypergraph partitioning. A cost model is also formalized. Performance is evaluated on sparse DNN benchmarks, showing the system can handle sporadic workloads and exploit sparsity effectively compared to alternatives.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 potential in-depth questions about the method proposed in this paper:

1. The paper proposes two novel fully serverless communication schemes for distributed machine learning - one using publish-subscribe/queuing and the other using object storage. What are the key differences in how these schemes work and what are the tradeoffs between them in terms of cost, performance, and scalability?

2. The paper introduces a hierarchical function launch mechanism to minimize startup delays and enable FaaS instances to automatically determine their position in the execution tree. How does this mechanism work and how does it help improve performance compared to alternative launch approaches? 

3. The paper adapts hypergraph partitioning techniques for distributing models across FaaS instances. How is this an improvement over simpler partitioning schemes like random partitioning? What specifically does hypergraph partitioning optimize for in this context?

4. The paper provides several optimizations like overlapping communication with computation, distributing communication requests across multiple resources, and multi-threading to improve performance. Can you explain one of these optimizations in more detail and quantify its impact?

5. The cost model provides recommendations for when to use the serial, queue-based, and object storage-based versions of FSD-Inference. What are the key factors and workloads characteristics that determine which approach is most suitable?

6. The experiments show FSD-Inference can outperform optimized HPC solutions in some cases. What allows a serverless solution to achieve this? Is it sustainable as model and workload sizes continue to grow?

7. The paper claims FSD-Inference is the first system to exploit intra-layer model parallelism in FaaS settings. What specifically does this mean and why is it important? How does it differ from prior approaches?

8. For handling sporadic workloads, what are the key advantages of FSD-Inference over a serverful always-on approach or job-scoped servers? How does cost and performance compare?

9. The paper shows publish-subscribe/queuing costs grow slower than object storage costs as parallelism increases. Why is this the case based on the pricing models of these services? Are there cases where object storage still excels?

10. What enhancement could be made to the FSD-Inference system to further improve its performance or cost-efficiency based on the experiments and analysis provided in the paper?
