# [Unsupervised Dense Information Retrieval with Contrastive Learning](https://arxiv.org/abs/2112.09118)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the main research question seems to be:Can contrastive learning be used to train high-performing dense retrievers in an unsupervised manner, matching or exceeding the performance of traditional unsupervised sparse methods like BM25?The key hypothesis appears to be that by using contrastive learning techniques like MoCo, dense neural retrievers can be trained without any labeled data and achieve strong retrieval performance across a variety of settings. Specifically, the paper investigates:- How well contrastive learning can work for unsupervised retriever training, compared to BM25 and other unsupervised baselines- Using contrastive pre-training to improve supervised dense retriever fine-tuning with few training examples - Leveraging contrastive pre-training for multilingual retrieval where labeled data is scarce- Performing in-depth ablations to validate design choices like using cropping over inverse cloze task for generating positive pairsThe main aim seems to be pushing contrastive learning to its limits for unsupervised dense retriever training across diverse settings, challenging the notion that large labeled datasets are required to train such models. Matching or exceeding BM25 would demonstrate the viability of this unsupervised learning approach.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contributions are:1. Showing that contrastive learning can be used to train unsupervised dense retrievers that are competitive with BM25 on many datasets/metrics. On the BEIR benchmark, their unsupervised model outperforms BM25 on 11 out of 15 datasets for Recall@100. 2. Demonstrating that their contrastively trained model benefits from few training examples when fine-tuned on in-domain data, outperforming transfer from large datasets like MS MARCO.3. Using their model as pre-training before fine-tuning on MS MARCO leads to SOTA results on the BEIR benchmark. The pre-trained model helps compared to training just on MS MARCO.4. Applying their approach to multilingual retrieval, where they show the model can achieve strong performance even without annotated data in languages other than English. It also enables cross-lingual retrieval between languages with different scripts.In summary, the key contribution is showing the effectiveness of contrastive learning for unsupervised training of dense retrievers, which can match or exceed BM25 without requiring annotated training data. The pre-trained models also transfer well to new domains/languages.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence TL;DR summary of the paper:The paper proposes using contrastive learning to train dense text retrievers without supervision, showing this unsupervised approach can match or exceed the performance of BM25 on several benchmarks while also enabling effective transfer and cross-lingual retrieval.


## How does this paper compare to other research in the same field?

Here are some key ways this paper compares to other research in the field of information retrieval:- This paper focuses on training dense neural network based retrievers in an unsupervised way, using contrastive learning. Much prior work on neural retrievers requires large amounts of supervised training data. This paper shows competitive results can be achieved without supervision.- The paper shows that pre-training with contrastive learning improves performance when fine-tuning the retriever on supervised data like MS MARCO. This demonstrates the value of pre-training for this task. Other work has explored pre-training techniques like the inverse cloze task, but this paper shows contrastive learning works better.- For multilingual retrieval, the paper shows that pre-training multilingual models with contrastive learning and then fine-tuning on English data leads to strong cross-lingual transfer. This is a novel way of doing multilingual retrieval that does not rely on aligned data.- The paper provides an extensive empirical evaluation on benchmarks like BEIR. It shows the unsupervised model is competitive and establishes new state-of-the-art results when combined with supervised fine-tuning. This benchmarking on standard datasets allows direct comparison to a wide range of other methods.- The ablations in the paper help justify the design choices like using cropping over the inverse cloze task. This level of analysis and justification is not always present in similar papers that introduce new techniques.Overall, the paper makes contributions in pushing unsupervised and multilingual learning for retrieval, with solid benchmarking. The techniques seem quite general and could likely be applied in other information retrieval domains beyond standard text search.
