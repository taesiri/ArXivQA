# [Do Llamas Work in English? On the Latent Language of Multilingual   Transformers](https://arxiv.org/abs/2402.10588)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Modern large language models (LLMs) like GPT-3 are mostly trained on English text, yet work well even for non-English languages. It is unclear whether they use English as an implicit "pivot language" internally.
- This question is important to understand how LLMs function and whether they have intrinsic linguistic biases. However, it is difficult to study as LLMs operate on abstract vector embeddings after the input layer.

Methods:  
- The paper focuses on the Llama family of transformer LMs and applies the "logit lens" method to decode probable next tokens from intermediate layers when prompted in non-English languages.
- Carefully constructed prompts with an unambiguous correct next token are designed, allowing attribution of decoded tokens to languages. 
- The evolution of probabilities and latent embeddings is analyzed across layers when prompted to translate or repeat words.

Key Findings:
- Logit lens reveals probability first goes to English versions of correct tokens in middle layers before shifting to input language in later layers. 
- Tracking embeddings geometrically shows three phases - no decoding, semantics in "concept space" biased toward English tokens, then input language decoding.
- This suggests the abstract "concept space" uses representations closer to English, which may have consequences for downstream biases.

Main Contributions:
- Provides first empirical evidence that multilingual LLMs use English-biased concept representations internally as a "lingua franca".
- Proposes interpretable framework of "input space" -> English-biased "concept space" -> "output space" to model this behaviour.
- Showcases the promise of combining carefully designed prompts and model interpretation tools like logit lens to unpack the black-box of large neural language models.

The paper makes key strides toward understanding internal biases in LLMs. Future work should explore the effects of these biases and how alternate training could mitigate them.


## Summarize the paper in one sentence.

 Here is a one-sentence summary of the paper:

The paper investigates whether multilingual language models like Llama use English as an internal "pivot language" when processing non-English inputs, finding evidence that while abstract latent representations may be closer to English, the model does not explicitly translate inputs to English before outputting other languages.


## What is the main contribution of this paper?

 Based on my understanding, the main contribution of this paper is empirically investigating whether large language models like LLama use English as an internal "pivot language" when processing non-English inputs. To study this, the authors carefully design prompts with unambiguous language attribution, apply the "logit lens" method to decode latent representations in different layers, and analyze the evolution of language probabilities across layers. Their key findings suggest that while intermediate representations do temporarily move closer to English, the model does not simply translate inputs to English and back. Rather, there appears to be an abstract English-biased "concept space" that representations move through before reaching output tokens. This provides new evidence about the internal workings of multilingual models and how biases manifest in their latent spaces. The conceptual model proposing input space -> concept space -> output space is a notable theoretical contribution for understanding these models. Overall, the work empirically analyzes the latent linguistics of large language models in a novel way to shed light on an important question.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and concepts include:

- Multilingual language models - The paper focuses on analyzing multilingual models like LLama that are trained on data from multiple languages.

- Internal pivot language - A main question explored is whether models use English as an implicit "pivot" language internally when processing non-English inputs.

- Logit lens - A technique used to decode next token probabilities from intermediate layers to analyze the internal state of models. 

- Language attribution - Creating prompts with unambiguous language attribution for the correct next token.

- Phases - The analysis reveals three distinct phases as latents are transformed layer-by-layer: (1) away from tokens, (2) biased toward English tokens, (3) moving to input language tokens.  

- Concept space - A proposed abstract "concept space" that is closer to English token embeddings, leading to the English bias observed.

- Token energy - A metric introduced to measure how much of a latent vector's energy translates into next-token logits.

- Mechanistic interpretability - The paper draws on techniques from this emerging field to "reverse engineer" and understand neural models.

- Multilingual biases - The paper discusses implications such as how an English bias could potentially lead to linguistic or cultural biases.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in this paper:

1. The paper argues that studying whether LLMs use English as an internal pivot language can give insights into how they function and the origins of bias. Can you expand on what specific insights we could gain and how it could uncover origins of bias?

2. The method relies on carefully constructed prompts with an unambiguous correct next token that reveals which language the model is operating in. What are some limitations of only studying single token predictions and how could the methodology be expanded to study longer text generations?  

3. The paper introduces the idea of a high-dimensional "concept space" that is biased toward English token embeddings. What do you think this concept space represents and how does the bias manifest itself?

4. The results show three distinct phases as the model transforms an input embedding - operating in "input space", "concept space", and "output space". Can you explain what computations you think are happening in each phase? 

5. The paper argues that the internal lingua franca is concepts rather than English. Do you think this interpretation fully explains the English bias observed? What alternative explanations could there be?

6. The methodology seems to reveal the working of the model quite transparently. Do you think studying simpler models like this can really give us insights into the inner workings of much larger LLMs?

7. The paper focuses only on the Llama family of models. Do you think these results could generalize to other LLMs and multilingual models? What evidence supports or contradicts the generalization?

8. The prompts are carefully constructed to force unambiguous outputs. Do you think this could limit insights into the model's typical behavior? How could more naturalistic prompts provide additional insights?

9. The results show probability shifts from English to the target language in later layers. What does this reveal about the role of depth in these models when processing different languages?

10. The paper argues the results have implications for bias and trustworthiness. Can you expand on the potential negative consequences if concepts are intrinsically biased toward English patterns of thought?
