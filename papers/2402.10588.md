# [Do Llamas Work in English? On the Latent Language of Multilingual   Transformers](https://arxiv.org/abs/2402.10588)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Modern large language models (LLMs) like GPT-3 are mostly trained on English text, yet work well even for non-English languages. It is unclear whether they use English as an implicit "pivot language" internally.
- This question is important to understand how LLMs function and whether they have intrinsic linguistic biases. However, it is difficult to study as LLMs operate on abstract vector embeddings after the input layer.

Methods:  
- The paper focuses on the Llama family of transformer LMs and applies the "logit lens" method to decode probable next tokens from intermediate layers when prompted in non-English languages.
- Carefully constructed prompts with an unambiguous correct next token are designed, allowing attribution of decoded tokens to languages. 
- The evolution of probabilities and latent embeddings is analyzed across layers when prompted to translate or repeat words.

Key Findings:
- Logit lens reveals probability first goes to English versions of correct tokens in middle layers before shifting to input language in later layers. 
- Tracking embeddings geometrically shows three phases - no decoding, semantics in "concept space" biased toward English tokens, then input language decoding.
- This suggests the abstract "concept space" uses representations closer to English, which may have consequences for downstream biases.

Main Contributions:
- Provides first empirical evidence that multilingual LLMs use English-biased concept representations internally as a "lingua franca".
- Proposes interpretable framework of "input space" -> English-biased "concept space" -> "output space" to model this behaviour.
- Showcases the promise of combining carefully designed prompts and model interpretation tools like logit lens to unpack the black-box of large neural language models.

The paper makes key strides toward understanding internal biases in LLMs. Future work should explore the effects of these biases and how alternate training could mitigate them.
