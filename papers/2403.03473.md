# [Inverse-Free Fast Natural Gradient Descent Method for Deep Learning](https://arxiv.org/abs/2403.03473)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
Second-order optimization methods like natural gradient descent (NGD) can converge faster than first-order methods like SGD by incorporating curvature information of the loss landscape. However, computing the inverse of the Fisher information matrix is computationally expensive for deep neural networks, making these methods impractical. 

Proposed Solution:
The paper proposes a fast natural gradient descent (FNGD) method that only requires computing the inverse during the first epoch. It reformulates the preconditioning formula in NGD as a weighted sum of per-sample gradients using the Sherman-Morrison-Woodbury (SMW) formula. The weights depend only on the sample correlation matrix which is assumed to be constant across epochs. By sharing the weights across epochs, FNGD approximates NGD preconditioning as a fixed-weight sum akin to SGD.

Main Contributions:
- Establishes connection between NGD and SGD by reformulating NGD preconditioning as weighted per-sample gradient sum using SMW formula
- Reduces preconditioning complexity from O(N^2) to O(N) by rearranging computation order 
- Proposes weight sharing technique across epochs based on assumption of constant sample correlation  
- Achieves 2.05x speedup over KFAC with comparable performance on CIFAR classification
- Outperforms AdamW on machine translation while requiring almost equal training time

In summary, the paper makes NGD computationally efficient by approximating it as a fixed-weight per-sample gradient sum with shared weights across epochs. This reduces the complexity and eliminates the need to compute inverses after the first epoch. Experiments show superior efficiency over state-of-the-art second-order methods.
