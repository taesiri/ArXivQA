# [GIVL: Improving Geographical Inclusivity of Vision-Language Models with   Pre-Training Methods](https://arxiv.org/abs/2301.01893)

## What is the central research question or hypothesis that this paper addresses?

This paper proposes GIVL, a geographically inclusive vision-language pre-trained model, to address the issue of geographical bias in existing vision-language pre-trained models (VLPs). The key hypothesis is that by pre-training VLPs on a geographically diverse image dataset along with novel pre-training objectives, the models can learn to better understand visual concepts and knowledge from different regions of the world. This should lead to improved performance and more balanced accuracy across images from Western vs non-Western regions on downstream geo-diverse vision-language tasks.The main research questions addressed are:1) How can we construct a geographically diverse pre-training corpus to provide images depicting concepts from diverse regions? 2) What novel pre-training objectives can encourage the model to learn alignments between geo-diverse images and corresponding textual knowledge during pre-training?3) Will the proposed model, GIVL, achieve better performance on geo-diverse vision-language tasks compared to prior VLPs? Can it obtain more balanced accuracy on Western vs non-Western test data?4) Can GIVL still achieve competitive performance on common vision-language tasks, demonstrating its generalizability?To summarize, the central hypothesis is that with a geographically diverse pre-training corpus and objectives aimed at aligning images and knowledge, GIVL can learn improved representations to understand concepts from different world regions. This should lead to state-of-the-art and more equitable performance on downstream geo-diverse vision-language tasks.


## What is the main contribution of this paper?

The main contribution of this paper is proposing GIVL, a geographically inclusive vision-language pre-trained model, to improve the performance of vision-language models on data from diverse regions and cultures. Specifically, the key contributions are:1. Identifying two key attributes of geo-diverse visual concepts that can help learn geo-diverse knowledge: - Concepts in similar categories have unique knowledge and visual characteristics.- Concepts with similar visual features may be completely different categories.2. Proposing two new pre-training objectives based on the attributes:- Image-Knowledge Matching (IKM) to align images with corresponding knowledge and identify category similarities. - Image Edit Checking (IEC) to differentiate visually similar concepts from different categories.3. Achieving state-of-the-art and more balanced performance on geo-diverse vision-language tasks like MaRVL, GD-VCR, and WIT compared to similar-sized models trained on similar data.4. Obtaining competitive results on common vision-language benchmarks like GQA and COCO captioning compared to models trained with more data.In summary, the key contribution is developing a new pre-training approach to improve geographical inclusivity and balance the performance of vision-language models across diverse data sources and cultures. The new objectives help the model learn nuanced geo-diverse knowledge.
