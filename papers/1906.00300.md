# [Latent Retrieval for Weakly Supervised Open Domain Question Answering](https://arxiv.org/abs/1906.00300)

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper addresses is whether it is possible to jointly learn a retriever and reader for open domain question answering using only question-answer string pairs as supervision, without relying on any pre-existing information retrieval system. The key hypotheses are:1) Pre-training the retriever with an Inverse Cloze Task can provide a strong enough initialization such that the full model can be trained end-to-end.2) Learning to retrieve is crucial for question answering when the questions reflect a genuine information need (i.e. the question asker does not already know the answer).3) Traditional IR systems like BM25 are sufficient when the question asker already knows the answer, but learned retrieval is needed when the question asker is genuinely seeking information.In summary, the central research question is whether end-to-end joint learning of retrieval and reading is possible with weak supervision, and the key hypothesis is that this is made feasible through pre-training the retriever with Inverse Cloze Task. The authors argue learned retrieval is beneficial specifically for answer-seeking questions.


## What is the main contribution of this paper?

The main contribution of this paper is presenting ORQA, an open domain question answering system where the retriever and reader components are jointly learned end-to-end using only question-answer pairs as supervision. Key points:- ORQA does not rely on any external IR system for retrieving evidence candidates. Instead, evidence retrieval is treated as a latent variable and learned directly from the QA data.- A key challenge is that learning retrieval from scratch is impractical. ORQA addresses this via pre-training the retriever with an Inverse Cloze Task (ICT). - ICT treats a sentence as a pseudo-question and its surrounding context as pseudo-evidence. This provides a useful initialization for evidence retrieval.- Experiments show ORQA outperforms traditional IR systems like BM25 on datasets where question askers genuinely need information. On datasets where question askers already know the answer, BM25 is sufficient.- ORQA demonstrates the value of learning to retrieve for open domain QA, instead of relying on a fixed IR system. The joint learning approach allows the retriever to be optimized directly for answering questions.In summary, the main contribution is presenting the first open domain QA system with end-to-end learned retrieval, enabled by pre-training the retriever with ICT. This shows the promise of learning to retrieve instead of using an off-the-shelf IR system.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper proposes an open-domain question answering system called ORQA that jointly learns to retrieve evidence and read it to answer questions, using only question-answer pairs as supervision and pre-training the retriever with an Inverse Cloze Task.


## How does this paper compare to other research in the same field?

Here are some key points in comparing this paper to other research in open domain question answering:- This paper proposes the first end-to-end learned retriever and reader for open domain QA. Previous work relied on blackbox IR systems to retrieve evidence candidates. The authors argue that QA retrieval is fundamentally different from IR retrieval.- The paper introduces a new pre-training technique called Inverse Cloze Task (ICT) to learn dense retriever representations without any QA data. This allows joint training of the retriever and reader using only question-answer string pairs.- Experiments show that learned retrieval substantially outperforms traditional IR methods like BM25 when question writers genuinely do not know the answer. On datasets where question writers know the answer, BM25 is sufficient.- The approach is evaluated on open versions of 5 QA datasets - Natural Questions, WebQuestions, CuratedTrec, TriviaQA, and SQuAD. Performance gains are shown on 3 of the datasets where questions reflect real information needs.- Compared to prior work on improving retrieval like R3 and REALM, this approach learns retrieval over the full open corpus rather than reranking a candidate set. The limitations are the compressed dense representations.- Overall, this paper makes an important contribution in showing the viability of end-to-end open retrieval QA without any traditional IR system. The ICT pre-training method is an interesting technique to learn retrievers without any QA data.


## What future research directions do the authors suggest?

Based on my reading of the paper, here are some of the main future research directions suggested by the authors:- Integrating their approach with methods that re-rank candidate evidence passages from a closed set. The authors mention recent work like R^3 and BERTserini that could complement their open retrieval approach.- Exploring hybrid retrieval approaches that combine the benefits of dense vector representations with traditional sparse, high-precision representations like BM25. The authors note there are still challenges precisely representing very specific concepts with 128-dimensional vectors.- Applying their approach to other information-seeking domains beyond open-domain QA, such as semantic parsing, information extraction, and dialogue systems. The authors draw connections to weakly supervised learning in these areas.- Scaling up the model capacity, for example by using larger BERT encoders, to support retrieval over even larger evidence corpora. The authors use a relatively small BERT model to enable training on a single machine.- Addressing the biases in existing QA datasets, for example by collecting new datasets reflecting genuine information needs. The authors analyze how biases like knowledge of the answer impacts learned retrieval.- Integrating retrieved evidence directly into language model architectures like GPT-3, instead of just using it to supervise a separate reader model. The authors frame ORQA in terms of standalone retriever and reader components.In summary, the main future directions are improving recall, handling specificity, applying the approach to new domains, scaling up capacity, addressing dataset biases, and end-to-end integration with large language models.


## Summarize the paper in one paragraph.

Here is a one paragraph summary of the paper:This paper proposes an end-to-end open domain question answering system called ORQA, where both the retriever and reader modules are trained jointly using only question-answer string pairs as supervision. This is done by pre-training the retriever module with an Inverse Cloze Task (ICT) to provide a strong initialization for retrieving relevant evidence passages from a corpus like Wikipedia. ICT treats a sentence as a pseudo-question and its surrounding context as pseudo-evidence. The model is trained to identify the true context for a sentence among negative candidates. After pre-training with ICT, the full ORQA model can be trained end-to-end on question-answer pairs to jointly update the retriever and reader modules, without needing any traditional IR system or annotation of supporting evidence. Experiments on open versions of five QA datasets show ORQA significantly outperforms BM25 retrieval when questions reflect a genuine information need. The limitations are reduced performance on dataset biases where question askers already know the answer.
