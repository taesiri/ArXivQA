# [Latent Retrieval for Weakly Supervised Open Domain Question Answering](https://arxiv.org/abs/1906.00300)

## What is the central research question or hypothesis that this paper addresses?

The central research question this paper addresses is whether it is possible to jointly learn a retriever and reader for open domain question answering using only question-answer string pairs as supervision, without relying on any pre-existing information retrieval system. The key hypotheses are:1) Pre-training the retriever with an Inverse Cloze Task can provide a strong enough initialization such that the full model can be trained end-to-end.2) Learning to retrieve is crucial for question answering when the questions reflect a genuine information need (i.e. the question asker does not already know the answer).3) Traditional IR systems like BM25 are sufficient when the question asker already knows the answer, but learned retrieval is needed when the question asker is genuinely seeking information.In summary, the central research question is whether end-to-end joint learning of retrieval and reading is possible with weak supervision, and the key hypothesis is that this is made feasible through pre-training the retriever with Inverse Cloze Task. The authors argue learned retrieval is beneficial specifically for answer-seeking questions.


## What is the main contribution of this paper?

The main contribution of this paper is presenting ORQA, an open domain question answering system where the retriever and reader components are jointly learned end-to-end using only question-answer pairs as supervision. Key points:- ORQA does not rely on any external IR system for retrieving evidence candidates. Instead, evidence retrieval is treated as a latent variable and learned directly from the QA data.- A key challenge is that learning retrieval from scratch is impractical. ORQA addresses this via pre-training the retriever with an Inverse Cloze Task (ICT). - ICT treats a sentence as a pseudo-question and its surrounding context as pseudo-evidence. This provides a useful initialization for evidence retrieval.- Experiments show ORQA outperforms traditional IR systems like BM25 on datasets where question askers genuinely need information. On datasets where question askers already know the answer, BM25 is sufficient.- ORQA demonstrates the value of learning to retrieve for open domain QA, instead of relying on a fixed IR system. The joint learning approach allows the retriever to be optimized directly for answering questions.In summary, the main contribution is presenting the first open domain QA system with end-to-end learned retrieval, enabled by pre-training the retriever with ICT. This shows the promise of learning to retrieve instead of using an off-the-shelf IR system.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper proposes an open-domain question answering system called ORQA that jointly learns to retrieve evidence and read it to answer questions, using only question-answer pairs as supervision and pre-training the retriever with an Inverse Cloze Task.


## How does this paper compare to other research in the same field?

Here are some key points in comparing this paper to other research in open domain question answering:- This paper proposes the first end-to-end learned retriever and reader for open domain QA. Previous work relied on blackbox IR systems to retrieve evidence candidates. The authors argue that QA retrieval is fundamentally different from IR retrieval.- The paper introduces a new pre-training technique called Inverse Cloze Task (ICT) to learn dense retriever representations without any QA data. This allows joint training of the retriever and reader using only question-answer string pairs.- Experiments show that learned retrieval substantially outperforms traditional IR methods like BM25 when question writers genuinely do not know the answer. On datasets where question writers know the answer, BM25 is sufficient.- The approach is evaluated on open versions of 5 QA datasets - Natural Questions, WebQuestions, CuratedTrec, TriviaQA, and SQuAD. Performance gains are shown on 3 of the datasets where questions reflect real information needs.- Compared to prior work on improving retrieval like R3 and REALM, this approach learns retrieval over the full open corpus rather than reranking a candidate set. The limitations are the compressed dense representations.- Overall, this paper makes an important contribution in showing the viability of end-to-end open retrieval QA without any traditional IR system. The ICT pre-training method is an interesting technique to learn retrievers without any QA data.
