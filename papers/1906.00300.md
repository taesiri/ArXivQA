# [Latent Retrieval for Weakly Supervised Open Domain Question Answering](https://arxiv.org/abs/1906.00300)

## What is the central research question or hypothesis that this paper addresses?

 The central research question this paper addresses is whether it is possible to jointly learn a retriever and reader for open domain question answering using only question-answer string pairs as supervision, without relying on any pre-existing information retrieval system. 

The key hypotheses are:

1) Pre-training the retriever with an Inverse Cloze Task can provide a strong enough initialization such that the full model can be trained end-to-end.

2) Learning to retrieve is crucial for question answering when the questions reflect a genuine information need (i.e. the question asker does not already know the answer).

3) Traditional IR systems like BM25 are sufficient when the question asker already knows the answer, but learned retrieval is needed when the question asker is genuinely seeking information.

In summary, the central research question is whether end-to-end joint learning of retrieval and reading is possible with weak supervision, and the key hypothesis is that this is made feasible through pre-training the retriever with Inverse Cloze Task. The authors argue learned retrieval is beneficial specifically for answer-seeking questions.


## What is the main contribution of this paper?

 The main contribution of this paper is presenting ORQA, an open domain question answering system where the retriever and reader components are jointly learned end-to-end using only question-answer pairs as supervision. Key points:

- ORQA does not rely on any external IR system for retrieving evidence candidates. Instead, evidence retrieval is treated as a latent variable and learned directly from the QA data.

- A key challenge is that learning retrieval from scratch is impractical. ORQA addresses this via pre-training the retriever with an Inverse Cloze Task (ICT). 

- ICT treats a sentence as a pseudo-question and its surrounding context as pseudo-evidence. This provides a useful initialization for evidence retrieval.

- Experiments show ORQA outperforms traditional IR systems like BM25 on datasets where question askers genuinely need information. On datasets where question askers already know the answer, BM25 is sufficient.

- ORQA demonstrates the value of learning to retrieve for open domain QA, instead of relying on a fixed IR system. The joint learning approach allows the retriever to be optimized directly for answering questions.

In summary, the main contribution is presenting the first open domain QA system with end-to-end learned retrieval, enabled by pre-training the retriever with ICT. This shows the promise of learning to retrieve instead of using an off-the-shelf IR system.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the paper:

The paper proposes an open-domain question answering system called ORQA that jointly learns to retrieve evidence and read it to answer questions, using only question-answer pairs as supervision and pre-training the retriever with an Inverse Cloze Task.


## How does this paper compare to other research in the same field?

 Here are some key points in comparing this paper to other research in open domain question answering:

- This paper proposes the first end-to-end learned retriever and reader for open domain QA. Previous work relied on blackbox IR systems to retrieve evidence candidates. The authors argue that QA retrieval is fundamentally different from IR retrieval.

- The paper introduces a new pre-training technique called Inverse Cloze Task (ICT) to learn dense retriever representations without any QA data. This allows joint training of the retriever and reader using only question-answer string pairs.

- Experiments show that learned retrieval substantially outperforms traditional IR methods like BM25 when question writers genuinely do not know the answer. On datasets where question writers know the answer, BM25 is sufficient.

- The approach is evaluated on open versions of 5 QA datasets - Natural Questions, WebQuestions, CuratedTrec, TriviaQA, and SQuAD. Performance gains are shown on 3 of the datasets where questions reflect real information needs.

- Compared to prior work on improving retrieval like R3 and REALM, this approach learns retrieval over the full open corpus rather than reranking a candidate set. The limitations are the compressed dense representations.

- Overall, this paper makes an important contribution in showing the viability of end-to-end open retrieval QA without any traditional IR system. The ICT pre-training method is an interesting technique to learn retrievers without any QA data.


## What future research directions do the authors suggest?

 Based on my reading of the paper, here are some of the main future research directions suggested by the authors:

- Integrating their approach with methods that re-rank candidate evidence passages from a closed set. The authors mention recent work like R^3 and BERTserini that could complement their open retrieval approach.

- Exploring hybrid retrieval approaches that combine the benefits of dense vector representations with traditional sparse, high-precision representations like BM25. The authors note there are still challenges precisely representing very specific concepts with 128-dimensional vectors.

- Applying their approach to other information-seeking domains beyond open-domain QA, such as semantic parsing, information extraction, and dialogue systems. The authors draw connections to weakly supervised learning in these areas.

- Scaling up the model capacity, for example by using larger BERT encoders, to support retrieval over even larger evidence corpora. The authors use a relatively small BERT model to enable training on a single machine.

- Addressing the biases in existing QA datasets, for example by collecting new datasets reflecting genuine information needs. The authors analyze how biases like knowledge of the answer impacts learned retrieval.

- Integrating retrieved evidence directly into language model architectures like GPT-3, instead of just using it to supervise a separate reader model. The authors frame ORQA in terms of standalone retriever and reader components.

In summary, the main future directions are improving recall, handling specificity, applying the approach to new domains, scaling up capacity, addressing dataset biases, and end-to-end integration with large language models.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the paper:

This paper proposes an end-to-end open domain question answering system called ORQA, where both the retriever and reader modules are trained jointly using only question-answer string pairs as supervision. This is done by pre-training the retriever module with an Inverse Cloze Task (ICT) to provide a strong initialization for retrieving relevant evidence passages from a corpus like Wikipedia. ICT treats a sentence as a pseudo-question and its surrounding context as pseudo-evidence. The model is trained to identify the true context for a sentence among negative candidates. After pre-training with ICT, the full ORQA model can be trained end-to-end on question-answer pairs to jointly update the retriever and reader modules, without needing any traditional IR system or annotation of supporting evidence. Experiments on open versions of five QA datasets show ORQA significantly outperforms BM25 retrieval when questions reflect a genuine information need. The limitations are reduced performance on dataset biases where question askers already know the answer.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the paper:

This paper proposes a new open-domain question answering system called ORQA that jointly learns to retrieve evidence and read it to generate answers. Unlike previous approaches that rely on off-the-shelf IR systems, ORQA learns its retriever end-to-end from question-answer pairs, treating evidence retrieval as a latent variable. To enable this joint training, the authors pretrain the retriever with an Inverse Cloze Task (ICT) to provide a useful initialization before fine-tuning on question-answering data. ICT treats a sentence as a pseudo-question and its surrounding context as pseudo-evidence, requiring the model to retrieve the correct context for a sentence from candidates. 

The authors evaluate ORQA on open versions of five QA datasets. On datasets where questions reflect a genuine information need, ORQA outperforms strong BM25 retrieval by 6-19 EM points. However, on datasets where question askers already know the answer (TriviaQA, SQuAD), BM25 remains superior since QA retrieval differs from IR. Overall, the results demonstrate that directly learning to retrieve evidence for QA is beneficial, and that models can learn to go beyond lexical matching by pretraining with ICT. The end-to-end approach also avoids the limitations of pipelined QA systems relying on fixed external IR components.


## Summarize the main method used in the paper in one paragraph.

 Here is a one paragraph summary of the main method used in the paper:

The paper proposes an open-retrieval question answering (ORQA) system where the retriever and reader are jointly learned end-to-end using only question-answer string pairs, without requiring any traditional IR system or labeled supporting evidence. This is made possible through an Inverse Cloze Task (ICT) pre-training approach. ICT treats sentences as pseudo-questions and their surrounding context as pseudo-evidence, and trains the retriever to select the correct context for a sentence from candidates in the batch. This provides a strong initialization for the retriever that enables end-to-end fine-tuning on question-answer pairs by treating retrieval as a latent variable. During fine-tuning, the reader scores all possible answer spans from the top retrieved evidence blocks. The model is optimized to maximize the marginal likelihood of the correct answer string. This approach allows jointly learning to retrieve and read directly from question-answer supervision. Experiments on open versions of several QA datasets show significant gains over traditional IR systems when questions reflect an authentic information need.


## What problem or question is the paper addressing?

 The paper is addressing the problem of open domain question answering, where the evidence/context must be retrieved from a large open corpus rather than being provided as input. The key issues it aims to tackle are:

1) Current approaches rely on a blackbox IR system for retrieval, which cannot be fine-tuned on the downstream QA task. The authors argue that QA retrieval is fundamentally different from standard IR.

2) Existing approaches assume either strong supervision with gold evidence/context, or weak supervision where IR results are treated as noisy labels. The authors aim to remove this dependence on gold evidence. 

3) Jointly learning the retriever and reader components in an end-to-end manner is challenging due to the large search space and lack of gold evidence.

The main question the paper tries to answer is: Can we learn to retrieve evidence from an open corpus in a completely end-to-end manner using only question-answer string pairs as supervision?


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, here are some key terms and concepts:

- Open-Retrieval Question Answering (ORQA) - The main model proposed in the paper for end-to-end question answering with learned retrieval.

- Inverse Cloze Task (ICT) - The unsupervised pre-training method used to initialize the retriever component of ORQA. Treats sentences as pseudo-questions and surrounding context as pseudo-evidence. 

- Learned retrieval - Using neural models to retrieve evidence, rather than traditional IR systems like BM25. Shows large improvements on datasets where question askers don't know the answer.

- Jointly learned retriever and reader - Previous work uses blackbox IR systems. ORQA shows you can learn both components end-to-end.

- Question-answer supervision - ORQA is trained only on QA string pairs, without any pre-selected evidence documents.

- Latent variable learning - Treating evidence retrieval as a latent variable during training, since annotating the evidence is expensive/unavailable.

- Biases in QA datasets - Many datasets have biases towards lexical overlap or knowledge of the answer that make traditional IR sufficient.

- Information-need vs known-answer questions - Learned retrieval excels on real user questions expressing an information need.


## Based on the paper, what questions should we ask to create a comprehensive summary of it? Please list at least 10 questions.

 Here are 10 potential questions to summarize the key points of this paper:

1. What is the problem that the paper aims to solve?

2. What are the limitations of existing approaches for open domain question answering? 

3. What is the key idea proposed in this paper to address the limitations?

4. What is the Inverse Cloze Task and how is it used for pre-training the retriever model?

5. How does the proposed ORQA model work for joint training of the retriever and reader components? 

6. What datasets were used for evaluating ORQA and what were the main results?

7. How does ORQA compare to baselines like BM25 and other latent retrieval methods? 

8. What is the impact of masking rate during pre-training using the Inverse Cloze Task?

9. What are some examples comparing ORQA predictions to the baselines?

10. What are the limitations of ORQA and potential ideas for future work to address them?


## Design 10 in-depth questions about method proposed in this paper and avoid simple question.

 Here are 10 potential in-depth questions about the method proposed in this paper:

1. The Inverse Cloze Task (ICT) is proposed to pre-train the retriever. How does ICT differ from traditional cloze task objectives, and why is it better suited for pre-training a retriever? What kinds of knowledge does ICT learn that are crucial for downstream evidence retrieval?

2. The paper argues that ICT requires learning more than just word matching features. What examples or analysis support this claim? How could the model leverage context to go beyond word matching during ICT pre-training?

3. The retriever and reader are jointly trained in an end-to-end manner, which is enabled by the ICT pre-training. What are the advantages of joint training over a pipelined approach? How does joint training allow the model to improve over standard IR systems?

4. During inference, how does the model handle the challenge of searching over the enormous open corpus? How does ICT pre-training facilitate efficient inference?

5. The learning method relies on finding all possible correct answer spans within the retrieved evidence, including spurious ones. How does the model avoid getting distracted by spurious ambiguities? What properties of ICT help address this challenge?  

6. What types of biases are present in existing QA datasets, and how do they impact evaluation of open retrieval QA systems? Why is it important to test on datasets with different biases?

7. The model underperforms on datasets where question writers know the answer (e.g. TriviaQA). What are the limitations of the learned dense retriever that cause this gap compared to traditional sparse retrieval methods?  

8. How does the model architecture build on recent advances in transfer learning and contextual representations? What motivates the choice of BERT for both the retriever and reader?

9. The work argues that QA is fundamentally different from traditional IR. What supports this claim? What capabilities are needed for QA retrieval that go beyond standard IR systems?

10. How does this work relate to weakly supervised semantic parsing literature? What parallels can be drawn regarding the inference, learning, and utilization of inductive biases?


## Summarize the paper in one sentence.

 The paper presents Open-Retrieval Question Answering (ORQA), a model for end-to-end open domain question answering that jointly learns to retrieve evidence and read from question-answer pairs without any traditional IR system.


## Summarize the paper in one paragraphs.

 Here is a one paragraph summary of the paper:

This paper proposes an open-retrieval question answering (ORQA) system that jointly learns to retrieve and read evidence from a large corpus in an end-to-end manner using only question-answer string pairs as supervision. A key contribution is pre-training the evidence retriever with an Inverse Cloze Task (ICT) to provide a strong initialization for end-to-end training. ICT treats a sentence as a pseudo-question and its surrounding context as pseudo-evidence, requiring the model to infer context from an underspecified query. Experiments on open versions of 5 QA datasets show learned retrieval substantially outperforms BM25 on datasets with genuine information-seeking questions like Natural Questions and WebQuestions. However, BM25 remains competitive on datasets like TriviaQA and SQuAD where questioners already know the answer, reducing the task to traditional IR-style keyword matching. This demonstrates the value of learned retrieval for open QA when questions reflect real user information needs.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 potential in-depth questions about the method proposed in this paper:

1. The Inverse Cloze Task (ICT) is proposed to pre-train the retriever model. Why is ICT helpful for this task compared to other unsupervised objectives? How does it encourage learning useful retrieval features?

2. During ICT pre-training, the sentence is masked from its context 90% of the time. What is the motivation behind this masking rate? How would the results differ if you masked more or less often?

3. The paper argues that question answering is fundamentally different from information retrieval. What specific differences make traditional IR methods like BM25 suboptimal for open domain QA? How does the proposed joint retriever and reader model address these issues?

4. The joint retriever and reader model is trained end-to-end using only question-answer pairs. Why is this challenging compared to having access to supporting evidence during training? How does the ICT pre-training enable feasible end-to-end learning?

5. How does the inference process balance tractability and expressivity during training and evaluation? What techniques like pre-encoding evidence blocks make inference feasible at scale?

6. What biases are present in existing QA datasets like SQuAD and TriviaQA? Why do these biases make them problematic for evaluating open domain QA systems with learned retrieval?

7. How do the authors analyze the strengths and weaknesses of their model compared to the BM25 baseline? What kinds of questions or evidence does each model handle better?

8. How does the proposed model compare to previous approaches like DrQA in a strongly supervised setting? What accounts for differences in performance?

9. The performance on SQuAD drops significantly from dev to test. What causes this performance gap and why does it suggest SQuAD has problematic biases?

10. What are some promising future directions for improving upon this approach? How could the strengths of learned dense retrieval be combined with traditional sparse methods?


## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a high-quality summary paragraph of the paper:

The paper proposes a new approach for open domain question answering (QA) called Open-Retrieval QA (ORQA). Unlike most prior work which relies on a fixed external information retrieval (IR) system to retrieve evidence candidates, ORQA jointly learns a retriever and reader end-to-end using only question-answer string pairs as supervision. This end-to-end latent variable learning is enabled through a new pre-training approach called Inverse Cloze Task (ICT). In ICT, sentences are treated as pseudo-questions and context paragraphs as pseudo-evidence. The retriever must select the correct context paragraph for a sentence from negative samples. After ICT pre-training, the authors show the retriever provides a strong enough signal to bootstrap joint learning on QA datasets. Experiments on open versions of 5 QA datasets show when questions reflect a real information need (Natural Questions, WebQuestions, CuratedTrec), the learned retriever substantially outperforms BM25 retrieval. However, for QA datasets where questions are contrived assuming knowledge of the answer (TriviaQA, SQuAD), BM25 remains superior. This demonstrates the value of learned retrieval for genuine open domain QA. The work provides the first demonstration of jointly learning to retrieve and read without any IR system or annotation of supporting evidence.
