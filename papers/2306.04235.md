# [MobileNMT: Enabling Translation in 15MB and 30ms](https://arxiv.org/abs/2306.04235)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central research question or hypothesis addressed in this paper is: How to design a neural machine translation system that is highly efficient in terms of model size, computation, memory usage, and decoding speed, so it can be easily deployed on mobile or embedded devices with limited resources.Specifically, the paper investigates:- Principles for designing parameter-efficient NMT architectures to make efficient use of computation and memory. This includes strategies like reducing vocabulary size, model width reduction, and using a deep encoder but shallow decoder.- Training techniques like knowledge distillation and adjustments to hyperparameters to achieve high accuracy with the compact model architectures. - Quantization and integer-only inference to reduce model size and enable faster decoding.- Optimizations to the inference engine like efficient GEMM and memory usage to maximize decoding speed.The overall goal is to develop MobileNMT - an NMT system that can deliver high-quality translation in a very small model footprint (e.g. 10-20MB) and with low latency (e.g. 30ms), suitable for deployment on mobile devices. The paper presents techniques across model architecture, training, quantization, and optimization of the inference engine to achieve this goal.
