# [MobileNMT: Enabling Translation in 15MB and 30ms](https://arxiv.org/abs/2306.04235)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central research question or hypothesis addressed in this paper is: How to design a neural machine translation system that is highly efficient in terms of model size, computation, memory usage, and decoding speed, so it can be easily deployed on mobile or embedded devices with limited resources.Specifically, the paper investigates:- Principles for designing parameter-efficient NMT architectures to make efficient use of computation and memory. This includes strategies like reducing vocabulary size, model width reduction, and using a deep encoder but shallow decoder.- Training techniques like knowledge distillation and adjustments to hyperparameters to achieve high accuracy with the compact model architectures. - Quantization and integer-only inference to reduce model size and enable faster decoding.- Optimizations to the inference engine like efficient GEMM and memory usage to maximize decoding speed.The overall goal is to develop MobileNMT - an NMT system that can deliver high-quality translation in a very small model footprint (e.g. 10-20MB) and with low latency (e.g. 30ms), suitable for deployment on mobile devices. The paper presents techniques across model architecture, training, quantization, and optimization of the inference engine to achieve this goal.


## What is the main contribution of this paper?

The main contributions of this paper are:1. The paper proposes MobileNMT, a Transformer-based neural machine translation system that is designed to be deployment-friendly for mobile devices. The system achieves compression ratios of 43.6-87.2x and speedups of 27.7-47.0x compared to Transformer-big while maintaining high translation quality.2. The paper proposes three principles for designing parameter-efficient NMT models: (a) reducing vocabulary size is more effective than embedding factorization for embedding compression, (b) reducing model width is more efficient than cross-layer parameter sharing for encoder/decoder compression, and (c) encoder depth is very important for model performance.3. The paper adjusts training strategies like removing dropout, using larger learning rates, and adding L2 regularization to achieve higher accuracy on the compressed MobileNMT models. 4. The paper implements optimizations like 8-bit quantization, GEMM optimization, and memory optimization in a custom inference engine tailored for MobileNMT.In summary, the main contribution is the proposal of MobileNMT, a complete machine translation system co-designed for deployment on mobile devices in terms of model architecture, training strategies, and inference engine implementation. The system achieves good compression and speedup while maintaining high translation accuracy.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper proposes MobileNMT, a machine translation system optimized for deployment on mobile devices through co-design of model architecture, training strategies, and inference engine, enabling translation in just 15MB model size and 30ms latency with minimal accuracy loss compared to larger models.
