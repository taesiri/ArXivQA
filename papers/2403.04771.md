# [QASE Enhanced PLMs: Improved Control in Text Generation for MRC](https://arxiv.org/abs/2403.04771)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Generative models underperform in machine reading comprehension (MRC) tasks due to out-of-control text generation, leading to two main issues - (1) ill-formed answers with incomplete or redundant phrases and (2) factual inconsistency between generated answers and the context. 

Proposed Solution: 
- The authors introduce a lightweight Question-Attended Span Extraction (QASE) module that is integrated during fine-tuning of pre-trained language models (PLMs) to guide answer generation in MRC tasks.

- QASE focuses model attention on potential answer spans in the context through a question-guided multi-head attention mechanism. This matches answer generation to relevant context.

- PLMs are fine-tuned on language modeling and span extraction losses simultaneously using multi-task learning. This enhances context grounding of answers.

Main Contributions:
- Developed QASE module to improve quality and factual consistency of fine-tuned PLMs for MRC, allowing them to match state-of-the-art extractive methods and outperform models like GPT-4.

- Showed QASE significantly boosts performance of PLMs without requiring major increase in computational resources, benefiting researchers with constraints.

- Evaluated on SQuAD, MultiSpanQA and Quoref datasets. QASE-enhanced models achieve best published results for generative methods on these datasets based on exact match and F1 metrics.

In summary, the paper introduces an effective lightweight QASE module to guide text generation for MRC that sets new benchmarks in generative QA while being computationally affordable.
