# [Jumping through Local Minima: Quantization in the Loss Landscape of   Vision Transformers](https://arxiv.org/abs/2308.10814)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the main research question it addresses is:

How can we effectively quantize vision transformer (ViT) models to very low bitwidths (e.g. 3-4 bits) while maintaining high accuracy?

The key hypotheses behind their proposed method, Evol-Q, seem to be:

- Small perturbations in quantization scales can lead to significant improvements in quantized ViT accuracy.

- Quantized ViTs have an extremely non-smooth loss landscape with respect to these perturbations, making gradient-based optimization ineffective. 

- Evolutionary search can effectively traverse this non-smooth landscape to find improved quantization scales.

- Using an infoNCE loss helps smooth the landscape and prevents overfitting during the evolutionary search.

So in summary, the central research question is how to effectively quantize ViTs to very low bitwidths. The key hypotheses are that evolutionary search with an infoNCE loss can handle the non-smooth landscape and lead to optimized quantization scales and accuracy.


## What is the main contribution of this paper?

 The main contribution of this paper is proposing a new method called Evol-Q for post-training quantization of vision transformers (ViTs). The key ideas are:

- Small perturbations in quantization scales can lead to significant improvements in accuracy for low-bit quantized ViTs (e.g. 4-bit). 

- ViTs have a highly non-smooth loss landscape with respect to quantization scales, making gradient-based methods ineffective.

- Evolutionary search is used to effectively traverse the non-smooth landscape and find good quantization scales.

- An infoNCE loss is used to evaluate quantization scales during search. This helps prevent overfitting to the small calibration set and makes the loss landscape smoother.

- Experiments show Evol-Q improves accuracy significantly over prior methods on a variety of ViT models for extreme quantization schemes like 4-bit and 3-bit weights.

In summary, the main contribution is proposing Evol-Q, an evolutionary search based method to optimize quantization scales for low-bit ViTs by leveraging the observations around the non-smooth loss landscape and effects of small perturbations.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the paper:

The paper proposes a new method called Evol-Q that uses evolutionary search and an infoNCE loss to optimize the quantization scales of vision transformers in a post-training quantization setting, achieving state-of-the-art accuracy by effectively traversing the highly non-smooth loss landscape induced by quantization.


## How does this paper compare to other research in the same field?

 Here are my thoughts on how this paper compares to other research in the field of quantizing vision transformers:

- The key innovation in this paper is using evolutionary search and an infoNCE loss to optimize the quantization scales in a block-wise manner for vision transformers (ViTs). Prior work has focused more on gradient-based optimization methods or quantization-aware training. Using evolutionary search is a novel approach to handling the highly non-smooth loss landscape of quantized ViTs.

- The paper demonstrates state-of-the-art results for post-training quantization of ViTs, outperforming recent methods like PSAQ-ViT, PTQ4ViT, and FQ-ViT in many cases. The improvements are especially significant in more extreme quantization scenarios like 3-4 bit weights.

- The use of infoNCE loss seems to be unique in this application. Other papers have combined quantization and contrastive losses for regularization during training, but using infoNCE specifically to smooth the quantization loss landscape and prevent overfitting to the calibration set is novel.

- The method generalizes well across diverse model architectures like DeiT, ViT, Swin, LeViT, and even CNNs. Showing strong quantization results across many model families demonstrates the robustness of the approach.

- The paper provides useful analysis and visualizations of the non-smooth loss landscape, comparing ViTs to CNNs. This helps motivate the need for an approach like evolutionary search rather than gradient-based methods.

- The method does require some tuning of hyperparameters like population size, number of cycles, etc. More analysis could help explain how to set these parameters optimally.

Overall, I think this paper makes excellent contributions by tackling ViT quantization through a creative co-design of evolutionary search and contrastive losses. The results and analysis convincingly demonstrate the effectiveness of this approach compared to prior art.
