# [DynamoNet: Dynamic Action and Motion Network](https://arxiv.org/abs/1904.11407)

## What is the central research question or hypothesis that this paper addresses?

This paper proposes a novel approach for human action recognition in videos. The key ideas/contributions are:- The paper aims to improve action recognition by learning better video-specific motion representations. This is in contrast to prior works that use standard spatio-temporal filters to model motion. - It proposes to use dynamic convolutional filters to learn video-specific motion features for improved action classification. The filters are conditioned on the input video and can capture internal motion patterns.- It jointly optimizes action classification along with future frame prediction as a proxy task to learn good motion features. By predicting future frames, the model is forced to learn more informative motion patterns that aid classification. - The overall model called DynamoNet has two branches - one for classification and one for future frame prediction. The future frame prediction generates dynamic motion filters to predict the next frame given previous ones. - These learned dynamic motion filters provide a compact motion representation for the whole video that captures important motion cues. This motion representation is combined with standard 3D ConvNet features for final classification.- Evaluations on UCF101, HMDB51 and Kinetics datasets show state-of-the-art results by combining the learned dynamic motion features with 3D ConvNets like ResNext and STCNet.In summary, the main hypothesis is that learning dynamic motion filters by predicting future frames can capture more informative video-specific motion patterns that lead to improved action recognition when combined with 3D ConvNets. The results validate this hypothesis and show the efficacy of the proposed DynamoNet model.
