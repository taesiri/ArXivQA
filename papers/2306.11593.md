# Improving Image Captioning Descriptiveness by Ranking and LLM-based   Fusion

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research questions addressed are:1) RQ1: Is it possible to get more detailed captions by fusing the captions generated by the most advanced state-of-the-art captioning models? 2) RQ2: Would the generated captions be more representative of the image content according to human judgement?3) RQ3: Can current image caption models generate richer and more descriptive captions if they are trained using ground-truth data that exhibits these characteristics?The key hypothesis seems to be that by combining and fusing captions from multiple state-of-the-art image captioning models, it is possible to generate more detailed and descriptive captions that better align with human judgments, compared to relying on any single captioning model alone. The authors propose a novel training-free captioning model that leverages existing models to generate candidate captions, ranks them using an image-text matching metric, and then fuses the top two captions using a large language model. Experiments are conducted on the MS-COCO dataset to evaluate caption quality, diversity, and human appeal. Results suggest the proposed model can produce captions that are more diverse, semantically richer, and preferred by humans, while still achieving competitive scores on standard image captioning metrics. The overall goal is generating more informative image descriptions that could improve training data for vision-language models.


## What is the main contribution of this paper?

Based on my reading, the main contributions of this paper are:1. Proposes a new image captioning model that can generate more detailed and descriptive captions compared to existing state-of-the-art models. 2. The model works by generating captions from multiple SoTA models, ranking them using an image-text matching metric, and then fusing the top 2 ranked captions using a large language model. This allows incorporating complementary information from different models.3. Demonstrates through quantitative and qualitative analysis that the captions generated by the proposed model are more semantically aligned with the image content, exhibit greater lexical diversity, and are preferred by human evaluators. 4. Shows the potential of using the proposed model to automatically generate richer ground-truth captions to train better image captioning models. 5. The model does not require additional training of neural networks, making it efficient to implement. It cleverly combines existing SoTA models through ranking and fusion.6. Proposes a new application of large language models for fusing and enhancing image captions through natural language prompting.In summary, the key contribution is a novel training-free approach to generate more detailed and human-like image captions by effectively combining existing SoTA models, which could help create better vision-language datasets and models. The method is analyzed extensively through both automatic metrics and human evaluation.
