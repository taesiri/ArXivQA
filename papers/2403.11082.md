# [RobustSentEmbed: Robust Sentence Embeddings Using Adversarial   Self-Supervised Contrastive Learning](https://arxiv.org/abs/2403.11082)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key aspects of the paper:

Problem:
- Pre-trained language models (PLMs) have shown outstanding performance on various NLP tasks but their sentence representations lack robustness against adversarial attacks and generalization across tasks. 
- Existing PLM-based sentence embeddings are vulnerable to small perturbations in input text which can easily fool the model.

Proposed Solution: 
- The paper proposes RobustSentEmbed, a self-supervised sentence embedding framework to improve generalization and robustness.

- It generates token and sentence level adversarial perturbations using FGSM and PGD attacks. These perturbations confuse the encoder model to treat them as different instances.

- A contrastive learning objective is used alongside a replaced token detection task. This maximizes similarity between original sentence embedding and its positive pair as well as adversarial embedding to make the model robust.

Main Contributions:
- Comprehensive experiments show RobustSentEmbed significantly reduces attack success rates across various adversarial attacks. For e.g. BERTAttack success reduced from 75% to 39%.

- It also achieves state-of-the-art performance on semantic textual similarity tasks and transfer learning tasks, outperforming prior methods.

- Thus, RobustSentEmbed generates high quality sentence embeddings with improved generalization as well as robustness against adversarial attacks, addressing limitations of prior PLM-based methods.

In summary, the paper makes notable contributions through RobustSentEmbed framework which can produce robust and effective sentence representations for enhanced performance on various NLP tasks.
