# Composable Text Controls in Latent Space with ODEs

## What is the central research question or hypothesis that this paper addresses?

This paper develops an approach called LatentOps for performing composable text control operations in the latent space of text, rather than directly in the complex text sequence space. The central hypothesis is that operating in the compact and continuous latent space will allow for more efficient and robust sampling of target text sequences with desired attributes, compared to prior approaches that rely on search or optimization in the high-dimensional discrete text space. Specifically, the key research questions addressed are:1. How to formulate latent space energy-based models (EBMs) that enable flexible composition of arbitrary text control operators (e.g. attribute classifiers) while matching the structure of a pretrained variational autoencoder (VAE) text model?2. How to efficiently and robustly sample the resulting latent EBM distribution to obtain vectors adhering to desired text attributes? The paper proposes using ordinary differential equation (ODE) solvers for this sampling.3. How to connect the sampled latent vectors to a pretrained language model (LM) like GPT-2 to decode the vectors back into fluent target text sequences? The paper adapts the LM efficiently without full finetuning.By addressing these questions to develop the LatentOps approach, the central hypothesis is that text generation and editing tasks with complex attribute requirements can be performed more efficiently and flexibly compared to prior work, while leveraging powerful pretrained LMs. The experiments aim to validate these advantages.


## What is the main contribution of this paper?

Based on the abstract and introduction, the main contributions of this paper appear to be:1. Proposing a new approach called LatentOps for performing composable text control operations in the compact latent space of text, rather than the complex sequence space as done in previous methods. This allows more efficient and robust sampling using ODEs.2. Developing an energy-based model (EBM) formulation in the latent space that permits plugging in arbitrary operators (e.g. attribute classifiers) to define a distribution for sampling text with desired attributes. This enables flexible composition of different text control operations. 3. Connecting pretrained language models (e.g. GPT-2) to the latent space through efficient adaptation without full finetuning, in order to decode the latent samples into coherent text.4. Showcasing the effectiveness of LatentOps on compositional tasks like sequential/simultaneous text editing across attributes and text generation with attribute combinations, demonstrating improved quality and efficiency over previous baselines.In summary, the key ideas are performing composable text operations in a compact latent space rather than complex sequence space, using efficient ODE sampling, plug-and-play formulation with EBMs, and connecting to pretrained LMs through light adaptation. The experiments then demonstrate the benefits of this approach.
