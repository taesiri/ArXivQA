# [Composable Text Controls in Latent Space with ODEs](https://arxiv.org/abs/2208.00638)

## What is the central research question or hypothesis that this paper addresses?

 This paper develops an approach called LatentOps for performing composable text control operations in the latent space of text, rather than directly in the complex text sequence space. The central hypothesis is that operating in the compact and continuous latent space will allow for more efficient and robust sampling of target text sequences with desired attributes, compared to prior approaches that rely on search or optimization in the high-dimensional discrete text space. 

Specifically, the key research questions addressed are:

1. How to formulate latent space energy-based models (EBMs) that enable flexible composition of arbitrary text control operators (e.g. attribute classifiers) while matching the structure of a pretrained variational autoencoder (VAE) text model?

2. How to efficiently and robustly sample the resulting latent EBM distribution to obtain vectors adhering to desired text attributes? The paper proposes using ordinary differential equation (ODE) solvers for this sampling.

3. How to connect the sampled latent vectors to a pretrained language model (LM) like GPT-2 to decode the vectors back into fluent target text sequences? The paper adapts the LM efficiently without full finetuning.

By addressing these questions to develop the LatentOps approach, the central hypothesis is that text generation and editing tasks with complex attribute requirements can be performed more efficiently and flexibly compared to prior work, while leveraging powerful pretrained LMs. The experiments aim to validate these advantages.


## What is the main contribution of this paper?

 Based on the abstract and introduction, the main contributions of this paper appear to be:

1. Proposing a new approach called LatentOps for performing composable text control operations in the compact latent space of text, rather than the complex sequence space as done in previous methods. This allows more efficient and robust sampling using ODEs.

2. Developing an energy-based model (EBM) formulation in the latent space that permits plugging in arbitrary operators (e.g. attribute classifiers) to define a distribution for sampling text with desired attributes. This enables flexible composition of different text control operations. 

3. Connecting pretrained language models (e.g. GPT-2) to the latent space through efficient adaptation without full finetuning, in order to decode the latent samples into coherent text.

4. Showcasing the effectiveness of LatentOps on compositional tasks like sequential/simultaneous text editing across attributes and text generation with attribute combinations, demonstrating improved quality and efficiency over previous baselines.

In summary, the key ideas are performing composable text operations in a compact latent space rather than complex sequence space, using efficient ODE sampling, plug-and-play formulation with EBMs, and connecting to pretrained LMs through light adaptation. The experiments then demonstrate the benefits of this approach.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the paper:

The paper proposes a new approach called LatentOps that performs composable text control operations efficiently in the compact latent space of text by leveraging energy-based models and ordinary differential equation sampling.

\section{Introduction}

Real-world text applications often involve composing diverse text control operations, such as editing attributes, manipulating keywords, generating new text with desired properties, etc. This paper proposes an efficient approach called LatentOps that enables composable text operations in the compact latent space of text. The key ideas are:

- Connect pretrained language models to a continuous latent space through efficient adaptation as a VAE decoder. 

- Define arbitrary text operators (e.g. classifiers) in the latent space to form a latent EBM distribution.

- Develop an ODE sampler for efficient and robust sampling from the distribution.

Experiments on compositional attribute control and text editing demonstrate improved quality and efficiency over previous sequence-space methods.

\section{Summary}

The key points of the paper are:

- Motivation: Real-world text tasks require composition of diverse control operations like editing attributes, keywords, generating new text, etc. Prior works are inefficient as they operate directly in complex sequence space.

- Approach: 
  - Connect pretrained LMs to continuous latent space via efficient VAE adaptation (only update small subset of parameters).

  - Formulate latent EBM by plugging in arbitrary operators (e.g. classifiers) in latent space.

  - Develop ODE sampler for efficient and robust sampling from distribution.

- Benefits: 
  - Avoid difficult optimization/sampling in high-dim sequence space.

  - Compatible with powerful pretrained LMs via light adaptation. 

  - Flexible composition of any operators.

  - Improved quality and efficiency over baselines on compositional control tasks.

- Limitations: 
  - Harder to control at token level due to sentence-level latent vector.

  - Difficult to generate very long sentences.

  - Inherits some limitations of autoregressive LMs.

In summary, the paper proposes an approach called LatentOps that performs composable text control operations in a latent space, demonstrating improved quality and efficiency over previous sequence-space methods. The key ideas are efficient LM adaptation, latent EBM formulation, and ODE sampling.


## How does this paper compare to other research in the same field?

 Here are a few key ways this paper compares to other recent research on composable text generation and editing in the latent space:

- Most prior work has focused on single attribute control or small pretrained models. This paper enables compositional control with multiple attributes using large pretrained LMs like GPT-2 by adapting them to a latent space. 

- Many recent methods perform optimization or sampling directly in the complex text sequence space. This paper formulates the operations in a compact continuous latent space which is more efficient.

- For latent space methods, previous work often uses gradient-based editing or separate pretrained autoencoders/VAEs. This paper connects the latent space directly to pretrained LMs through efficient adaptation within a VAE framework.

- This paper develops an ODE-based sampler for the latent EBM that is more robust and efficient than Langevin Dynamics used in some prior latent space text EBM work.

- The proposed approach is flexible and plug-and-play, allowing easy combination of arbitrary new operations by simply training lightweight classifiers in the fixed latent space. Most prior work lacks such compositionality.

- Experiments demonstrate strong performance in compositional generation and editing tasks by combining various attributes, significantly outperforming previous plug-and-play approaches.

In summary, the key novelty of this paper is in formulating the composable text operations directly in a continuous latent space compatible with large pretrained LMs, and developing an efficient ODE-based sampler, which together enable flexible and high-quality text control. The experiments demonstrate the effectiveness of latent space operations compared to modifying complex text sequences directly.


## What future research directions do the authors suggest?

 Here are some key future research directions suggested by the authors:

- Exploring more complex text generation tasks beyond the sentence level, such as generating paragraphs or documents with coherent structure and discourse relations. The current approach focuses on sentence-level generation.

- Extending the approach to enable more fine-grained control, such as generating text that conforms to specific syntactic templates or textual patterns. The current approach performs attribute-level control. 

- Incorporating external knowledge into the framework, such as factual knowledge or commonsense knowledge, to improve the factual correctness and logical consistency of generated text.

- Developing more advanced evaluation metrics and benchmarks to better assess the quality of controlled text generation along various linguistic dimensions.

- Exploring different conditional density estimations beyond the EBM formulation to model the joint distribution over text and attributes, which may lead to better sampling efficiency.

- Investigating the societal impacts of controllable text generation and developing techniques to detect and mitigate potential harms.

In summary, the main future directions are to scale up the approach to more complex text generation tasks, enable finer-grained control, incorporate external knowledge, improve evaluation, explore alternative conditional modeling and sampling techniques, and investigate societal impacts. Advancing research along these dimensions can help develop more capable and robust controllable text generation systems.

\section{Introduction}
% Recent large pretrained language models (LMs) such as GPTs \hzt{cite GPT-2/3} are capable of generating well-formed coherent text. 
\begin{figure}[t]
    \centering
    \includegraphics[width=0.45\textwidth,page=1]{fig/intro.pdf}
    \vspace{-8pt}
    \caption{Examples of different composition of text operations, such as editing a text in terms of different attributes sequentially (top) or at the same time (middle), or generating a new text of target properties (bottom). The proposed \textsc{LatentOps} enables a single LM (e.g., an adapted GPT-2) to perform arbitrary text operation composition in the latent space.}
    \label{fig:intro}
    \vspace{-22pt}
\end{figure}

Many text problems involve a diverse set of text control operations, such as editing different attributes (e.g., sentiment, formality) of the text, inserting or changing the keywords, generating new text of diverse properties, and so forth. In particular, different \emph{composition} of those operations are often required in various real-world applications (Figure~\ref{fig:intro}).  

Conventional approaches typically build a conditional model (e.g., by finetuning pretrained language models) for each specific combination of operations~\cite{hu2017toward,keskarCTRL2019,DBLP:journals/corr/abs-1909-08593}, which is unscalable given the combinatorially many possible compositions and the lack of supervised data. Most recent research thus has started to explore plug-and-play solutions. Given a pretrained language model (LM), those approaches plug in arbitrary constraints to guide the production of desired text sequences \cite{Dathathri2020Plug,DBLP:journals/corr/abs-2104-05218, DBLP:conf/nips/KumarMST21, DBLP:conf/emnlp/KrauseGMKJSR21,DBLP:journals/corr/abs-2203-13299,QinCOLD}. 
The approaches, however, typically rely on search or optimization in the complex text \emph{sequence space}. The discrete nature of text makes the search/optimization extremely difficult. Though some recent work introduces continuous approximations to the discrete tokens \citep{qin2020back,QinCOLD,DBLP:conf/nips/KumarMST21}, the high dimensionality and complexity of the sequence space still renders it inefficient to find the accurate high-quality text.
% (i.e., sequences of discrete tokens, or continuous token presentations or logits) that makes it difficult and inefficient to find the accurate high-quality text.

In this paper, we develop \textsc{LatentOps}, a new efficient approach that performs composable control operations in the compact and continuous \emph{latent space} of text. \textsc{LatentOps} permits plugging in arbitrary operators (e.g., attribute classifiers) applied on text latent vectors, to form an energy-based distribution on the low-dimensional latent space. We then develop  
an efficient sampler based on ordinary differential equations (ODEs) \cite{DBLP:conf/iclr/0011SKKEP21,nie2021controllable,Vahdat_LSGM} to draw latent vector samples that bear the desired attributes. 
%that effectively samples from the distribution guided by gradients, and yields the target vectors adhering to the combined operators. 

A key challenge after getting the latent vector is to decode it into the target text sequence. To this end, we connect the latent space to pretrained LM decoders (e.g., GPT-2) by efficiently adapting a small subset of the LM parameters in a variational auto-encoding (VAE) manner~\cite{DBLP:journals/corr/KingmaW13,DBLP:conf/conll/BowmanVVDJB16}. 

Previous attempts of editing text in latent space have often been limited to single attribute and small-scale models, due to the incompatibility of the latent space with the existing transformer-based pretrained LMs \cite{DBLP:conf/nips/WangH019,DBLP:conf/aaai/LiuFZPL20, DBLP:conf/icml/ShenMBJ20,DBLP:conf/acl/DuanXPHL20,DBLP:conf/emnlp/MaiPMSH20}. \textsc{LatentOps} overcomes the difficulties and enables a single large LM to perform arbitrary composable text controls.

We conduct experiments on three challenging settings, including sequential editing of text \emph{w.r.t.} a series of attributes, editing compositional attributes simultaneously, and generating new text given various attributes. Results show that composing operators within our method manages to generate or edit high-quality text, substantially improving over respective baselines in terms of quality and efficiency.

% Pretrained language models (PLMs), e.g., GPT-2~\cite{gpt2} and BERT~\cite{bert}, have been verified the powerful ability to generate fluent and reasonable sentences in a variety of tasks in Natural Language Process (NLP). 
% However, fluency is only one basic goal of text generation. In real application, how to generate sentences that satisfy various constraints is still a challenging problem. 
% For example in Fig.~\ref{fig:intro}, for attribute-guided generation~\cite{DBLP:conf/aaai/LiuFZPL20,DBLP:conf/nips/WangH019,DBLP:conf/icml/MuellerGJ17}, e.g., sentiment or tense, we expect the generated sentences conforming to the desired attribute. For keyword-guided generation~\cite{DBLP:journals/corr/abs-2202-11705}, we wish to guarantee that the specified keywords are included in the generated output. 
% Further, how to combine several constraints effectively, e.g., as the last example in Fig.~\ref{fig:intro}, generate a sentence that is negative sentiment and past tense, and contains keyword \textit{shop}, is still a great challenge to current methods.

% Training a conditional model~\cite{DBLP:journals/corr/abs-1911-11161, DBLP:conf/acl/DaiLQH19, DBLP:conf/naacl/LiuNW21} is a general way to control the attributes, that is the conditional information is specified during training to guide the generation. The weakness of this approach is non-negligible: once the model is trained, it's not easy to adapt it to new attributes, i.e., it requires re-train the whole model if new attributes are introduced, which is expensive and time-consumed. An alternative approach is to train an unconditional model first, and then introduce conditional model into it with a small cost~\cite{DBLP:conf/aaai/LiuFZPL20,DBLP:conf/nips/WangH019, DBLP:conf/icml/ShenMBJ20}. Auto-encoder and variational auto-encoder are usually chosen as the generative model, and use some gradient-based method to edit the latent vector to satisfy the desired attributes. Broadly speaking, these methods are hard to conduct compositional generation, and are restricted in small-scale architecture like LSTM~\cite{hochreiter1997long}, restricting the generation capacity. 

% Another way~\cite{Dathathri2020Plug,DBLP:journals/corr/abs-2104-05218,QinCOLD} is to modify the hidden states or logits based on a pretrained language models, which usually train some attribute classifiers and add some constraints. But the drawbacks are that it's slow and inefficient, because the operations are conducted in the token-level hidden spaces.
% % and a pre-defined prefix is needed \hzt{why is prefix needed?}
% % , which will also lead to low diversity \hzt{why low-diversity? Diversity is not our key advantage. Do not emphasize the non-critical thing.}.

% % In order to overcome the aforementioned shortcomings, we'd like to explore a workflow that is efficient and flexible, takes full advantage of the capacity of pretrained language models and possesses strong ability of compositionality.
% Some recent works~\cite{nie2021controllable, DBLP:journals/corr/abs-2004-06030,DBLP:conf/iclr/GrathwohlWJD0S20} made a successful attempt to compose multiple attributes in image generation. \citet{nie2021controllable} modify the input latent vector of a pretrained image GAN to control the attributes. However, the connection between the GAN latent space and data space is unidirectional, requiring paired latent-image data for training, which is hard to obtain.
% % Inspired by some works~\cite{nie2021controllable, DBLP:journals/corr/abs-2004-06030,DBLP:conf/iclr/GrathwohlWJD0S20} of image generation area, energy-based models (EBMs)~\cite{hopfield1982neural,DBLP:journals/neco/Hinton02,lecun2006tutorial} are a workable solution to the compositionality problem. The reason is that we can combine multiple energy functions representing different attributes to constitute the text generator. Each factor is represented by an individual energy function, and if the factor exists in the text, a low energy will be given.
% % \hzt{A few works already highlight EBMs for composable controlled text generation, like COLD decoding. Our highlight/key differentiator is not EBMs, but instead operations in latent space. Note that in the Abstract I even do not mention EBMs in order to differentiate from COLD etc.}
% In text area, \citet{li-etal-2020-optimus} provide a novel view to utilize PLMs to construct large VAE, which possesses the strong generation ability and diversity, and a structural continuous low-dimensional latent space, which is efficient and flexible to sample. But it requires fully finetunes the PLMs, comsuming large time and resources. 

% In this work, we design a new approach for composable text operations in the compact latent space of text. Each operation is based on a operator (attribute classifier) in latent space and we can combine arbitrary plug-in operators to generate desired text. 
% To guarantee the quality of generated texts, we connect the PLMs (i.e., GPT-2) to a latent space through efficient adaption, which then decodes the sampled vectors into the desired text sequences. Dispensing with fully finetuning, we only require a small part of weights to train. 
% Further, the low-dimensionality and differentiablity of the text vector permit us to develop an efficient and robust sampler based on the ordinary different equations (ODEs)~\cite{DBLP:conf/iclr/0011SKKEP21}. 
% Our method can be viewed as various text control operations (\textit{TextOps}) in latent space with ODEs. 



% we take advantage of the compositionality of EBMs and the strong generative capacity of pretrained language model, e.g., GPT-2, to accomplish high-quality controllable and compositional text generation \hzt{And this sentence suddenly is not relevant to latent space at all, disconnected from the last sentence.}. And we don't require to finetune the whole GPT-2, but only train an extra layer. Our method can be viewed as various text control operations (\textit{TextOps}) in latent space \hzt{only until the very end of introduction, we start mentioning our key idea: "text control operations in latent space", and the key methodological contribution, i.e., ODE sampler}, which is efficient and reliable. Further, we leverage the robustness and efficiency of ordinary different equation (ODE) solvers~\cite{DBLP:conf/iclr/0011SKKEP21,nie2021controllable} to sample from latent space. 

Here are some thoughts on potential future research directions:

- Exploring more complex text generation tasks: The current approach focuses on sentence-level generation. Scaling up to generate longer coherent paragraphs/documents would be an interesting challenge. This would likely require incorporating discourse-level structure and coherence constraints.

- Enabling finer-grained control: The attribute-level control could be extended to allow more granular editing of content, such as enforcing specific syntactic templates or textual patterns. This may require incorporating syntactic parsers/analyzers within the framework.

- Incorporating external knowledge: Integrating external knowledge bases (facts, commonsense etc.) could help improve factual correctness and logical consistency of generated text. Useful directions include conditioned decoding and retrieval-augmented generation.

- Developing better evaluation: More advanced automatic and human evaluation metrics are needed to properly assess controlled generation quality across linguistic dimensions. Useful directions include learning better content similarity metrics, designing more diagnostic test suites, and conducting user studies.

- Exploring alternative density estimations: The current EBM formulation could be replaced by other conditional density models that enable more efficient sampling. Useful directions include flow-based models, diffusion probabilistic models, and normalizing flows.

- Studying societal impacts: As controllable text generation matures, it is crucial to investigate potential misuse cases and develop techniques to detect and mitigate harms. Useful directions include auditing biases in systems and datasets, developing countermeasures against malicious use, and encouraging responsible research practices.

In summary, the high-level goals are to scale up the capabilities, refine control, increase grounding, improve evaluation, innovate on conditional modeling/sampling, and ensure societal safety. Advancing along these dimensions can lead to more robust and socially responsible controllable text generation.

The authors propose some good directions for future work on controllable text generation. Here are a few additional ideas:

- Multi-task learning: Explore multi-task learning frameworks to jointly model multiple attributes in a single model, avoiding retraining for new attributes. This could improve efficiency and leverage inter-dependencies between attributes.

- Long form generation: Extend beyond sentence-level generation to model global coherence and discourse structure for generating long-form text like essays, reports, stories etc. Hierarchical latent variable modeling may help here.

- Low/few-shot learning: Develop techniques like prompt-learning or meta-learning to enable controlling new attributes with very limited labeled data. This can avoid costly data annotation. 

- Semantic control: Move beyond surface form attributes like sentiment to enable control over semantic aspects like meaning, entailment, factual accuracy etc. Useful techniques may include grounding in knowledge bases.

- Creative control: Design control schemes that go beyond mimicking human text to enabling more creative expression, like through style transfer, sentence fusion, or guided imagination.

- Safety and ethics: Conduct rigorous testing to characterize failure modes and enable safe deployment. Develop techniques like watermarking and provenance tracking to prevent misuse. Study social impacts through surveys and simulation.

- User interfaces: Create intuitive user interfaces that allow easy specification of control parameters without needing expertise in ML. Democratizing control to end users can enable many beneficial applications.

- Theoretical analysis: Conduct rigorous theoretical analysis to elucidate modes of failure, characterize control bounds, and provide optimality guarantees. This can lead to more robust approaches.

Overall, advancing capabilities for complex tasks, extreme data efficiency, semantic grounding, creative expression, safe deployment, easy use by non-experts, and rigorous understanding would help unlock the full potential of controllable text generation.

Here are a few additional future research directions to consider for controllable text generation:

- Multi-modal control: Extend control frameworks to allow conditioning on non-text inputs like images, audio, video etc. This can enable multi-modal applications like image/video captioning with fine-grained control.

- Discrete control: Allow control through discrete operations like word/phrase insertion, deletion, replacement etc. This is more intuitive than continuous latent space editing for some use cases.

- Hierarchical control: Develop hierarchical control schemes operating at different linguistic levels (words, sentences, paragraphs) for more structured editing. Useful techniques may include hierarchical VAEs.

- Weakly supervised control: Minimize reliance on labeled data through weakly/un-supervised techniques like adversarial learning, self-supervision, and leveraging unlabeled corpora.

- Personalized control: Learn to automatically customize control schemes to individual users based on interaction history, writing style, persona etc. This can improve ease of use.

- Theoretical foundations: Strengthen theoretical understanding of controllability for text through analyses grounded in linguistics, information theory, causality, game theory etc. 

- Interpretability: Increase interpretability of how models enact control through attention visualizations, saliency maps, example-based explanations etc. This is important for trust and transparency.

- Efficiency: Improve computational and sample efficiency through techniques like discrete latent variables, normalizing flows, and better gradient estimators.

Overall, broadening the scope and scale of control, reducing supervision, personalizing to users, strengthening theoretical grounding, and boosting interpretability and efficiency would be valuable research directions to explore. This can help drive advances in controllable text generation.

Some other potential research directions for controllable text generation:

- Low-resource language support: Adapt techniques to work well even with limited training data, enabling control capabilities for low-resource languages. Useful directions include cross-lingual transfer learning and leveraging multilingual pretrained models.

- Non-categorical control: Explore controlling non-categorical aspects like style


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the paper:

This paper proposes LatentOps, a new approach for performing composable text control operations in the compact latent space of text. It adapts pretrained language models like GPT-2 to a latent space using variational autoencoders. Arbitrary operators like attribute classifiers can then be plugged into this latent space to form energy-based distributions. Efficient sampling from these distributions is performed using ordinary differential equation solvers, resulting in latent vectors that can be decoded into text sequences with desired attributes. This approach avoids difficult optimization in complex text sequence spaces. Experiments show it can effectively generate and edit text by composing various control operations, outperforming previous methods in quality and efficiency. The key advantage is performing textual control in a low-dimensional continuous latent space rather than directly in the discrete output space.

\section{Conclusions}
% \vspace{-5pt}
 We have developed a new efficient approach that performs composable control operations in the compact latent space of text, named \textsc{LatentOps}. 
The proposed method permits combining arbitrary operators applied on a latent vector, resulting in an energy-based distribution on the low-dimensional continuous latent space.
We develop an efficient and robust sampler based on ODEs that effectively samples from the distribution guided by gradients.
We connect the latent space to popular pretrained LM by efficient adaptation without finetuning the whole model.
We showcase its compositionality, flexibility and firm performance on several distinct tasks.
In future work, we can explore the control of more complicated texts.


% \clearpage
\section{Limitations}
Here are some potential limitations of our proposed LatentOps approach:

- The operations are based on sentence-level latent vectors, which makes it difficult to control text at a more fine-grained token level or generate under specific syntactic constraints. 

- The compact latent space may not easily support generating very long text sequences.

- We rely on pretrained auto-regressive language models like GPT-2 as the base text generation module, which have certain inherent limitations around repetition, factual consistency, etc.

- The approach requires first establishing a fitted latent space using VAE for any new text domain, which takes some upfront effort.

- The quality of control relies on the classifiers plugged in, which need to be trained with sufficient labeled data.

- There could be entanglement across different attributes that need to be disentangled, which is still an open challenge.

- Evaluating controllable text generation itself remains an active area of research, and better metrics are needed beyond current proxy measures.

In summary, the approach makes significant advances in flexible text control but still has limitations around granularity of control, scale, reliance on base LMs, setup effort, and evaluation. Addressing these limitations could further advance controllable text generation capabilities.

\section{Ethical Considerations}
Here are some important ethical considerations around controllable text generation models like LatentOps:

- \textbf{Bias}: The models could inherit and amplify biases present in the training data for the base LMs and classifiers. Proactive steps are needed to detect and mitigate any unfair biases.

- \textbf{Misuse}: Like other text generation models, LatentOps could potentially be misused to produce harmful, biased, or misleading content. Monitoring systems and publication norms should be established by researchers.

- \textbf{Transparency}: The controllability of models should be clearly communicated to avoid any misconceptions that the outputs are human-written.

- \textbf{Truthfulness}: The generated text should not be claimed or perceived as factual without verification. Warnings could be provided not to assume factual accuracy.

- \textbf{Consent}: User data should only be utilized to develop such models with appropriate consent and privacy protections in place.

- \textbf{Directed harm}: Safeguards are needed to prevent deliberately malicious use to generate offensive, threatening, or illegal text content.  

- \textbf{Legal compliance}: Researchers and practitioners should ensure compliance with all applicable laws and regulations regarding AI development and usage.

- \textbf{Oversight}: Regular reviews by ethics advisory boards could help assess the potential benefits and risks, especially as capabilities continue to evolve.

Researchers and practitioners have an ethical responsibility to consider these factors and adopt appropriate mitigations when building, testing, and deploying controllable NLG models. The generation of synthetic text comes with risks, and care must be taken to address them responsibly.

\section{Future Work}
Here are some potential directions for future work to build upon LatentOps:

- Support more fine-grained control at the token level, like specifying syntax or structured outputs. The current approach operates at the sentence level.

- Explore alternate latent variable models beyond VAEs, like normalizing flows or auto-regressive models, that can represent more complex distributions.

- Develop hierarchical latent variable models to handle long-form, multi-sentence text generation in a coherent fashion.

- Enhance the ODE solver with adaptive step sizes or neural network approximations for greater efficiency and scalability.

- Study semi-supervised or few-shot approaches to acquire the attribute classifiers with less labeled data.

- Design disentanglement techniques to separate entangled attributes like semantics and syntax more effectively.

- Experiment with ensembles of diverse classifiers to improve robustness and uncertainty estimation.

- Incorporate commonsense knowledge and reasoning abilities for more factual and logical controllable generation.

- Evaluate the controllability, safety, and ethics of models with more rigorous methods beyond just proxy metrics.

- Demonstrate benefits on real-world applications like data augmentation, creative writing, educational content generation, etc.

- Commercialize the technology for responsible real-world deployment after sufficient testing and risk assessment.

There remain open research problems around scaling, disentanglement, evaluation and practical usage where continued progress will enable safer and more capable controllable text generation.

\section{Acknowledgements}
We thank...

\bibliography{anthology}
\bibliographystyle{acl_natbib}

\end{document}



\begin{abstract}
Real-world text applications often involve \emph{composing} a  wide range of text control operations, such as editing the text \emph{w.r.t.} an attribute, manipulating keywords and structure, and generating new text of desired properties. Prior work typically learns/finetunes a language model (LM) to perform individual or specific subsets of operations. Recent research has studied combining operations in a plug-and-play manner, often with costly search or optimization in the complex sequence space. This paper proposes a new efficient approach for composable text operations in the compact \emph{latent} space of text. The low-dimensionality and differentiability of the text latent vector allow us to develop an efficient sampler based on ordinary differential equations (ODEs) given arbitrary plug-in operators (e.g., attribute classifiers). 
By connecting pretrained LMs (e.g., GPT-2) to the latent space through efficient adaption, we then decode the sampled vectors into desired text sequences. 
The flexible approach permits diverse control operators (sentiment, tense, formality, keywords, etc.) acquired using any relevant data from different domains. Experiments show that composing those operators within our approach manages to generate or edit high-quality text, substantially improving over previous methods in terms of generation quality and efficiency.\footnote{Code: \url{https://github.com/guangyliu/LatentOps}}
\end{abstract}


% Real-world text applications often involve composing a  wide range of text control operations, such as editing the text w.r.t. an attribute, manipulating keywords and structure, and generating new text of desired properties. Prior work typically learns/finetunes a language model (LM) to perform individual or specific subsets of operations. Recent research has studied combining operations in a plug-and-play manner, often with costly search or optimization in the complex sequence space. This paper proposes a new efficient approach for composable text operations in the compact latent space of text. The low-dimensionality and differentiability of the text latent vector allow us to develop an efficient sampler based on ordinary differential equations (ODEs) given arbitrary plug-in operators (e.g., attribute classifiers). We connect pretrained LMs (e.g., GPT-2) to the latent space through efficient adaption, which then decodes the sampled vectors into the desired text sequences. The flexible approach permits diverse control operators, such as sentiment, tense, formality, and keywords, learned with any relevant data from different domains. Experiments show that composing those operators with our approach manages to generate or edit high-quality text, substantially improving over respective baselines in terms of quality and efficiency.
% \section{Appendix}
% \section{Finetuning }
\onecolumn
\section{Derivation of ODE Formulation}
\label{app:derivation_ode}
%%%%%%%%%%%%%
\newcommand{\td}{\text d}
\newcommand{\gxt}{\bm{G}(\bm{x},t)}
\newcommand{\bmx}{\bm x}
\newcommand{\fxt}{\bm f(\bm x,t)}
\newcommand{\barw}{\Bar{\bm w}}
%%%%%%%%%%%%%
\subsection{General Form}
Let's consider the general diffusion process defined by SDEs in the following form (see more details in Appendix A and D.1 of \citet{DBLP:conf/iclr/0011SKKEP21}):
\begin{equation}
\label{eq:general_forward_sde}
    \text{d}\bm{x} = \bm f(\bm x,t)\text{d}t +\bm{G}(\bm x, t)d\bm w,
\end{equation}
where $\bm f(\cdot,t):\mathbb{R}^d\rightarrow\mathbb{R}^d$ and $\bm{G}(\cdot,t):\mathbb{R}^d\rightarrow\mathbb{R}^{d\times d}$. The corresponding reverse-time SDE is derived by~\citet{anderson1982reverse}:
\begin{equation}
\label{eq:general_reverse_sde}
    \td\bmx=\left\{ \fxt - \nabla_{\bmx} \cdot[\gxt\gxt^{\text T}] - \gxt\gxt^{\text T}\nabla_{\bmx} \log p_t(\bmx) \right\}\td t+\gxt\td\barw,
\end{equation}
where we refer $\nabla_{\bmx}\cdot \bm F(\bmx) := [\nabla_{\bmx}\cdot \bm f^1(\bmx),...,\nabla_{\bmx}\cdot \bm f^d(\bmx)]^{\text T}$ for a matrix-valued function $\bm F(\bm x):=[\bm f^1(\bmx),...,\bm f^d(\bmx)]^{\text T}$, and $\nabla_{\bmx}\cdot \bm f^i(\bmx)$ is the Jacobian matrix of $f^i(\bmx)$.
Then the ODE corresponding to Eq.~\ref{eq:general_forward_sde} has the following form:
\begin{equation}
\label{eq:general_ode}
    \td\bmx=\left\{ \fxt - \frac{1}{2}\nabla_{\bmx}\cdot[\gxt\gxt^{\text T}] -\frac{1}{2}\gxt\gxt^{\text T}\nabla_{\bmx}\log p_t(\bmx) \right\}\td t.
\end{equation}
\subsection{Derivation of Our ODE}
In this work, we adopt the Variance Preserving (VP) SDE~\cite{DBLP:conf/iclr/0011SKKEP21} to define the forward diffusion process:
\begin{equation}
    \text{d} \bm x = -\frac{1}{2}\beta(t)\bm x \text{d}t+\sqrt{\beta(t)}\text{d} \bm w,
\end{equation}
where the coefficient functions of Eq.~\ref{eq:general_forward_sde} are $\fxt=-\frac{1}{2}\beta(t)\bmx\in\mathbb{R}^d$ and $\gxt=\bm G(t)=\sqrt{\beta(t)}\bm I_d\in\mathbb{R}^{d\times d}$, independent of $\bm x$. Following Eq.~\ref{eq:general_reverse_sde}, the corresponding reverse-time SDE is derived as:
\begin{equation}
\begin{split}
    \td\bmx &= \left[ -\frac{1}{2}\beta(t)\bmx - \beta(t)\nabla_{\bmx}\cdot\bm I_d - \beta(t)\bm I_d\nabla_{\bmx}\log p_t(\bm x)  \right]\td t + \sqrt{\beta(t)}\bm I_d\td \barw\\
    & = \left[ -\frac{1}{2}\beta(t)\bmx - \beta(t)\nabla_{\bmx}\log p_t(\bm x)  \right]\td t + \sqrt{\beta(t)}\td \barw\\
    &= -\frac{1}{2}\beta(t)\left[\bmx + 2\nabla_{\bmx}\log p_t(\bm x) \right]\td t + \sqrt{\beta(t)}\td \barw,
\end{split}
\end{equation}
which infers to the Eq.~\ref{eq:sde}. Then, we derive the deterministic process (ODE) on the basis of Eq.~\ref{eq:general_ode}:
\begin{equation}
    \begin{split}
        \td\bmx &= \left[ -\frac{1}{2}\beta(t)\bmx -\frac{1}{2}\beta(t)\nabla_{\bmx}\cdot\bm I_d-\frac{1}{2}\beta(t)\bm I_d\nabla_{\bmx}\log p_t(\bmx) \right]\td t\\
        &=\left[ -\frac{1}{2}\beta(t)\bmx -\frac{1}{2}\beta(t)\nabla_{\bmx}\log p_t(\bmx) \right]\td t\\
        &=-\frac{1}{2}\beta(t)\left[\bmx +\nabla_{\bmx}\log p_t(\bmx) \right]\td t,
    \end{split}
\end{equation}
which gives the derivation of Eq.~\ref{eq:ode_x}.
\newpage
\section{Evaluation of Sample Selection Strategy}
\label{app:sample_selection}
As we stated in \S\ref{sec:implement}, we adopt a sample selection strategy for content-related generation tasks (text editing and generation with keywords).
Previous works also have similar strategies to improve the generation quality (i.e., PPLM~\cite{Dathathri2020Plug} and FUDGE~\cite{DBLP:journals/corr/abs-2104-05218}).

Since our latent model is trained by VAE objective, a sample $\bm x \in\mathcal{X}$ corresponds to a distribution $\mathcal{N}(\bm\mu,\bm\sigma^2)$ in $\mathcal{Z}$. Thus, we can search for better output by expanding the search space through sampling $\bm z_n\sim\mathcal{N}(\bm\mu,\bm\sigma^2)$, where $n=1,..., N$, and pick the best. 
Specifically, from ODE sampling, $\bm z(0)$ acts as the mean, and the variance $\bm \sigma^2$ is predefined. We generate $\bm z_n$ by sampling $\bm\epsilon_n$ from standard Gaussian:
\begin{equation}
    \bm z_n = \bm z(0) + \bm\sigma\odot\bm\epsilon_n, \quad \bm \epsilon_n\sim\mathcal N(\bm 0, \bm I).
\end{equation}

We decode each $\bm z_n$ and pick the best one according to the criterion of the task. We prefer the output that conforms to the desired attribute and achieves a high BLEU score with the source text for the text editing task. We want the output that contains the desired keyword or its variants for the generation with keywords. 

In our experiments (text editing and generation with keywords), we set $N=20$ as the default. To better demonstrate the strategy's improvement, we provide the quantitative and qualitative results towards different $N$.

We follow the same setting of text editing with single attribute on Yelp (\S\ref{app:example_text_edit_single}). The automatic evaluation results are shown in Table~\ref{tab:sample_selection}. As $N$ increases, all the metrics get improved. To reflect the trend of change in accuracy and content preservation, we plot Figure~\ref{fig:sample_selection}, which indicates that large $N$ gives better accuracy and better input-BLEU.


\begin{figure}[ht]
\begin{minipage}[ht]{0.5\textwidth}
\centering
\small
\begin{tabular}{cccccc}
\toprule
% $N$&Acc&iBL&rBL&CTC&PPL\\\midrule

\multirow{2}{*}{$N$}&Accuracy$\uparrow$&\multicolumn{3}{c}{Content$\uparrow$}&Fluency$\downarrow$\\\cmidrule(r){2-2}\cmidrule(r){3-5}\cmidrule(r){6-6}
& Sentiment & iBL & rBL & CTC & PPL\\\midrule
2&0.75&51.1&21.4&0.4737&26.3\\
4&0.82&50.6&22.0&0.4729&26.7\\
6&0.89&49.6&22.3&0.4729&26.2\\
8&0.9&50.5&22.2&0.4732&25.9\\
10&0.92&50.8&23.1&0.4730&26.2\\
12&0.93&51.4&23.2&0.4733&26.1\\
14&\underline{0.94}&51.4&23.0&0.4732&26.9\\
16&\underline{0.94}&52.4&23.4


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the paper:

This paper proposes an approach called LatentOps for performng composable text control operations in the latent space of text. The key ideas are: 1) Equip pretrained language models like GPT-2 with a compact continuous latent space using parameter-efficient adaptation within a VAE framework. This allows mapping latent vectors to text. 2) Define energy-based models (EBMs) in this latent space by plugging in arbitrary operators like attribute classifiers. This gives a latent distribution with desired properties. 3) Develop an efficient ODE-based sampler that can draw samples from this distribution by following gradients.

Compared to prior work that operates directly in the sequence space, LatentOps avoids expensive search or optimization in high-dimensional discrete text space. By working in the compact latent space, it permits flexible composition of diverse control operators acquired using any small attribute datasets. Experiments show it can effectively generate or edit text with complex combinations of attributes, dramatically improving previous approaches in quality and efficiency. Overall, it provides an interpretable and efficient way to perform compositional operations on top of pretrained LMs.

\section{Conclusions}
% \vspace{-5pt}
 We have developed a new efficient approach that performs composable control operations in the compact latent space of text, named \textsc{LatentOps}. 
The proposed method permits combining arbitrary operators applied on a latent vector, resulting in an energy-based distribution on the low-dimensional continuous latent space.
We develop an efficient and robust sampler based on ODEs that effectively samples from the distribution guided by gradients.
We connect the latent space to popular pretrained LM by efficient adaptation without finetuning the whole model.
We showcase its compositionality, flexibility and firm performance on several distinct tasks.
In future work, we can explore the control of more complicated texts.


\bibliography{anthology}
\bibliographystyle{acl_natbib}


\clearpage
\appendix

% \section{Appendix}
% \section{Finetuning }
\onecolumn
\section{Derivation of ODE Formulation}
\label{app:derivation_ode}
%%%%%%%%%%%%%
\newcommand{\td}{\text d}
\newcommand{\gxt}{\bm{G}(\bm{x},t)}
\newcommand{\bmx}{\bm x}
\newcommand{\fxt}{\bm f(\bm x,t)}
\newcommand{\barw}{\Bar{\bm w}}
%%%%%%%%%%%%%
\subsection{General Form}
Let's consider the general diffusion process defined by SDEs in the following form (see more details in Appendix A and D.1 of \citet{DBLP:conf/iclr/0011SKKEP21}):
\begin{equation}
\label{eq:general_forward_sde}
    \text{d}\bm{x} = \bm f(\bm x,t)\text{d}t +\bm{G}(\bm x, t)d\bm w,
\end{equation}
where $\bm f(\cdot,t):\mathbb{R}^d\rightarrow\mathbb{R}^d$ and $\bm{G}(\cdot,t):\mathbb{R}^d\rightarrow\mathbb{R}^{d\times d}$. The corresponding reverse-time SDE is derived by~\citet{anderson1982reverse}:
\begin{equation}
\label{eq:general_reverse_sde}
    \td\bmx=\left\{ \fxt - \nabla_{\bmx} \cdot[\gxt\gxt^{\text T}] - \gxt\gxt^{\text T}\nabla_{\bmx} \log p_t(\bmx) \right\}\td t+\gxt\td\barw,
\end{equation}
where we refer $\nabla_{\bmx}\cdot \bm F(\bmx) := [\nabla_{\bmx}\cdot \bm f^1(\bmx),...,\nabla_{\bmx}\cdot \bm f^d(\bmx)]^{\text T}$ for a matrix-valued function $\bm F(\bm x):=[\bm f^1(\bmx),...,\bm f^d(\bmx)]^{\text T}$, and $\nabla_{\bmx}\cdot \bm f^i(\bmx)$ is the Jacobian matrix of $f^i(\bmx)$.
Then the ODE corresponding to Eq.~\ref{eq:general_forward_sde} has the following form:
\begin{equation}
\label{eq:general_ode}
    \td\bmx=\left\{ \fxt - \frac{1}{2}\nabla_{\bmx}\cdot[\gxt\gxt^{\text T}] -\frac{1}{2}\gxt\gxt^{\text T}\nabla_{\bmx}\log p_t(\bmx) \right\}\td t.
\end{equation}
\subsection{Derivation of Our ODE}
In this work, we adopt the Variance Preserving (VP) SDE~\cite{DBLP:conf/iclr/0011SKKEP21} to define the forward diffusion process:
\begin{equation}
    \text{d} \bm x = -\frac{1}{2}\beta(t)\bm x \text{d}t+\sqrt{\beta(t)}\text{d} \bm w,
\end{equation}
where the coefficient functions of Eq.~\ref{eq:general_forward_sde} are $\fxt=-\frac{1}{2}\beta(t)\bmx\in\mathbb{R}^d$ and $\gxt=\bm G(t)=\sqrt{\beta(t)}\bm I_d\in\mathbb{R}^{d\times d}$, independent of $\bm x$. Following Eq.~\ref{eq:general_reverse_sde}, the corresponding reverse-time SDE is derived as:
\begin{equation}
\begin{split}
    \td\bmx &= \left[ -\frac{1}{2}\beta(t)\bmx - \beta(t)\nabla_{\bmx}\cdot\bm I_d - \beta(t)\bm I_d\nabla_{\bmx}\log p_t(\bm x)  \right]\td t + \sqrt{\beta(t)}\bm I_d\td \barw\\
    & = \left[ -\frac{1}{2}\beta(t)\bmx - \beta(t)\nabla_{\bmx}\log p_t(\bm x)  \right]\td t + \sqrt{\beta(t)}\td \barw\\
    &= -\frac{1}{2}\beta(t)\left[\bmx + 2\nabla_{\bmx}\log p_t(\bm x) \right]\td t + \sqrt{\beta(t)}\td \barw,
\end{split}
\end{equation}
which infers to the Eq.~\ref{eq:sde}. Then, we derive the deterministic process (ODE) on the basis of Eq.~\ref{eq:general_ode}:
\begin{equation}
    \begin{split}
        \td\bmx &= \left[ -\frac{1}{2}\beta(t)\bmx -\frac{1}{2}\beta(t)\nabla_{\bmx}\cdot\bm I_d-\frac{1}{2}\beta(t)\bm I_d\nabla_{\bmx}\log p_t(\bmx) \right]\td t\\
        &=\left[ -\frac{1}{2}\beta(t)\bmx -\frac{1}{2}\beta(t)\nabla_{\bmx}\log p_t(\bmx) \right]\td t\\
        &=-\frac{1}{2}\beta(t)\left[\bmx +\nabla_{\bmx}\log p_t(\bmx) \right]\td t,
    \end{split}
\end{equation}
which gives the derivation of Eq.~\ref{eq:ode_x}.
\newpage
\section{Evaluation of Sample Selection Strategy}
\label{app:sample_selection}
As we stated in \S\ref{sec:implement}, we adopt a sample selection strategy for content-related generation tasks (text editing and generation with keywords).
Previous works also have similar strategies to improve the generation quality (i.e., PPLM~\cite{Dathathri2020Plug} and FUDGE~\cite{DBLP:journals/corr/abs-2104-05218}).

Since our latent model is trained by VAE objective, a sample $\bm x \in\mathcal{X}$ corresponds to a distribution $\mathcal{N}(\bm\mu,\bm\sigma^2)$ in $\mathcal{Z}$. Thus, we can search for better output by expanding the search space through sampling $\bm z_n\sim\mathcal{N}(\bm\mu,\bm\sigma^2)$, where $n=1,..., N$, and pick the best. 
Specifically, from ODE sampling, $\bm z(0)$ acts as the mean, and the variance $\bm \sigma^2$ is predefined. We generate $\bm z_n$ by sampling $\bm\epsilon_n$ from standard Gaussian:
\begin{equation}
    \bm z_n = \bm z(0) + \bm\sigma\odot\bm\epsilon_n, \quad \bm \epsilon_n\sim\mathcal N(\bm 0, \bm I).
\end{equation}

We decode each $\bm z_n$ and pick the best one according to the criterion of the task. We prefer the output that conforms to the desired attribute and achieves a high BLEU score with the source text for the text editing task. We want the output that contains the desired keyword or its variants for the generation with keywords. 

In our experiments (text editing and generation with keywords), we set $N=20$ as the default. To better demonstrate the strategy's improvement, we provide the quantitative and qualitative results towards different $N$.

We follow the same setting of text editing with single attribute on Yelp (\S\ref{app:example_text_edit_single}). The automatic evaluation results are shown in Table~\ref{tab:sample_selection}. As $N$ increases, all the metrics get improved. To reflect the trend of change in accuracy and content preservation, we plot Figure~\ref{fig:sample_selection}, which indicates that large $N$ gives better accuracy and better input-BLEU.


\begin{figure}[ht]
\begin{minipage}[ht]{0.5\textwidth}
\centering
\small
\begin{tabular}{cccccc}
\toprule
% $N$&Acc&iBL&rBL&CTC&PPL\\\midrule

\multirow{2}{*}{$N$}&Accuracy$\uparrow$&\multicolumn{3}{c}{Content$\uparrow$}&Fluency$\downarrow$\\\cmidrule(r){2-2}\cmidrule(r){3-5}\cmidrule(r){6-6}
& Sentiment & iBL & rBL & CTC & PPL\\\midrule
2&0.75&51.1&21.4&0.4737&26.3\\
4&0.82&50.6&22.0&0.4729&26.7\\
6&0.89&49.6&22.3&0.4729&26.2\\
8&0.9&50.5&22.2&0.4732&25.9\\
10&0.92&50.8&23.1&0.4730&26.2\\
12&0.93&51.4&23.2&0.4733&26.1\\
14&\underline{0.94}&51.4&23.0&0.4732&26.9\\
16&\underline{0.94}&52.4&23.4&0.4737&\underline{25.9}\\
18&\textbf{0.95}&\underline{52.6}&\underline{23.6}&\underline{0.4739}&\textbf{25.8}\\
20&\textbf{0.95}&\textbf{54.0}&2\textbf{4.2}&\textbf{0.4743}&\underline{25.9}\\
\bottomrule
\end{tabular}
  \captionof{table}{Automatic evaluation results towards to different $N$ on Yelp review dataset. 
  We mark the best \textbf{bold} and the second best \underline{underline}.}
  \label{tab:sample_selection}
\end{minipage}
\hfill
\begin{minipage}[ht]{0.48\textwidth}
\centering
\includegraphics[width=0.9\textwidth,page=1]{fig/sample_selection_fig.pdf}
\captionof{figure}{The trend of change of accuracy and input-BLEU as $N$ increases. The digit below each data point represents the corresponding $N$.}
\label{fig:sample_selection}
\end{minipage}
\end{figure}
We also provide some examples in Table~\ref{tab:exmaple_sample_selection}. One observation is that all the outputs from the same source sequence describe similar scenarios but slightly differ in expression.
Thus, we can select the most suitable expression based on predefined rules. 

% Therefore, we can improve the automatic evaluation results through selecting the most proper one.
% This explains why the strategy could improve the performance of automatic results.
\begin{table}[ht]
% \renewcommand{\arraystretch}{0.99}
    \centering
    \footnotesize
    \vspace{-19pt}
    \begin{tabular}{l|l}
    \toprule
Source&there is definitely not enough room in that part of the venue .\\
Target&there is so much room in that part of the venue\\\midrule
&there is definitely plenty of room in that perfect location .\\
&there is definitely no room enough in that venue to be the best part .\\
&there is definitely plenty of room right in that venue .\\
&there is definitely plenty of room right in the venue that needs .\\
&there is definitely plenty of room right in that venue .\\
&there is definitely enough room that can be right in the venue .\\
&there is definitely nothing better in room for that type of venue .\\
&there is definitely plenty of room in the right venue for that level .\\
&there is definitely nothing better in that room style of place .\\
&there is definitely a good room inside that best of all need in space .\\
&there is definitely plenty of room in the right level that is appropriate .\\
&there is definitely enough room in that right part of the venue .\\
&there is definitely plenty of room right in the deck that is needed .\\
&there is definitely enough room in that good atmosphere .\\
&there is definitely plenty of room in the right area , which is comfortable .\\
&there is definitely plenty of room in that perfect state of the place .\\
&there is definitely plenty of room that ideal in the location .\\
&there is definitely enough room in that perfect venue to all .\\
&there is definitely plenty of room in the right venue as well .\\
&there is definitely plenty of room available in the overall venue , too .\\
\midrule[1pt]
Source&it is n't terrible , but it is n't very good either .\\
Target&it is n't perfect , but it is very good .\\\midrule
&it is n't terrible , but it is very good also !\\
&it is very good , but it does n't even look great !\\
&it is n't terrible , but it is very good and definitely is good !\\
&it is n't great , but it is definitely very good !\\
&it is n't terrible , it is good and the menu is definitely great !\\
&it is n't terrible , but it is n't very good either .\\
&it is n't terrible , but it is very good also .\\
&it is n't terrible , but it is very good also !\\
&it is n't terrible , but it is definitely very good !\\
&it is very good , and it is n't terrible either .\\
&it is n't terrible , but it is very good and well made !\\
&it is very good , but it 's not really great either .\\
&it is n't terrible , but it is very good and well worth it .\\
&it is n't terrible , but it is definitely very good and good !\\
&it is n't terrible , but it is very good also !\\
&it is n't terrible , but it is very good and definitely is great !\\
&it is n't terrible , but it is very good also .\\
&it is n't terrible , but it is n't very good either .\\
&it is n't terrible , but it is very good also .\\
&it is n't terrible , but it is very good and always great !\\
\midrule[1pt]
Source&the food was pretty bad , i would not go there again .\\
Target&the food was great, i would go there again.\\\midrule
&he food was pretty good , i would go there again .\\
&the food was pretty good , i would def go there again !\\
&the food was pretty good , i would go again !\\
&the food was pretty good , i would go there again !\\
&the food was pretty good , i would definitely go there again .\\
&the food was pretty good , i would go back there again .\\
&the food was pretty good , i would definitely go back again .\\
&the food was pretty good , i would definitely go there again !\\
&the food was pretty good , i would definitely go there again .\\
&the food was pretty good , i would always go there again .\\
&the food was pretty good , i would go there again .\\
&the food was pretty good , i would not go there again .\\
&the food was pretty good , i would go there again .\\
&the food was pretty good , i would go back there again .\\
&the food was pretty good , i would go there again .\\
&the food was pretty good , i would definitely go there again !\\
&the food was pretty good , i would not go there again .


## Summarize the main method used in the paper in one paragraph.

 Here is a one paragraph summary of the main method used in the paper:

The paper proposes a new approach called LatentOps for performing composable text control operations in the compact latent space of text. It first connects pretrained language models like GPT-2 to a continuous latent space using a VAE framework with minimal parameter tuning. It then defines energy functions in this latent space using arbitrary attribute classifiers/operators. These energy functions characterize a latent space EBM distribution that enables sampling of latent vectors with desired attributes via an efficient ODE solver. The key benefits are performing text operations in a low-dimensional differentiable space compared to complex discrete text space, and the flexibility to compose arbitrary operators. The sampled latent vectors are decoded to target text sequences using the adapted pretrained LM decoder.


## What problem or question is the paper addressing?

 This paper is proposing a new method for composable text operations in the latent space of text. Here are the key points:

- The goal is to perform diverse text control operations like editing attributes of text, manipulating keywords, generating new text with desired properties, etc. Prior methods typically require training task-specific models or optimizing in complex sequence space, which is inefficient. 

- The key idea is to perform composable operations in the compact latent space of text instead of sequence space. This is enabled by two components:

1) A VAE to connect pretrained LMs like GPT-2 to a continuous latent space through efficient adaptation.

2) Energy-based models (EBMs) defined on the latent space to incorporate arbitrary operators (e.g. attribute classifiers) and sample desired vectors efficiently using ODEs.

- This allows plugging in any number of attribute classifiers to control text generation on the fly. Sampling is more efficient in low-dimensional latent space compared to complex sequence space.

- The approach is flexible to use any pretrained LMs and acquire classifiers from different domains. Experiments show it improves over baselines in compositional generation and editing tasks.

In summary, the key novelty is performing composable text operations in a compact latent space rather than sequence space, enabled by adapting LMs and latent-space EBMs with efficient ODE sampling. This makes diverse text controls more efficient while retaining generation quality.

\section{Conclusion}
% \vspace{-5pt}
 We have developed a new efficient approach that performs composable control operations in the compact latent space of text, named \textsc{LatentOps}. 
The proposed method permits combining arbitrary operators applied on a latent vector, resulting in an energy-based distribution on the low-dimensional continuous latent space.
We develop an efficient and robust sampler based on ODEs that effectively samples from the distribution guided by gradients.
We connect the latent space to popular pretrained LM by efficient adaptation without finetuning the whole model.
We showcase its compositionality, flexibility and firm performance on several distinct tasks.
In future work, we can explore the control of more complicated texts.

\section{Limitations}

Here are some potential limitations of our proposed approach:

- Since the operations are based on sentence-level latent vectors, it may be more difficult to control text at a more fine-grained token level, such as generating text under specific syntactic constraints.

- Due to the low dimensionality of the latent space, generating very long text sequences can be challenging. The model may struggle to encode all the necessary information to generate coherent long texts.

- Our approach relies on auto-regressive language models like GPT-2 which have some inherent limitations, such as exposure bias during generation. So any issues with the foundation LM may propagate.

- The attribute classifiers used to define operations are trained on limited labeled data. Their imperfections could affect the output quality, especially for sparse attributes.

- There could be entanglement across attributes that make disentangled control difficult, e.g., formality and sentiment are often correlated.

- Evaluating the controllability and compositionality of generated text remains an open challenge. We rely on automatic metrics which may not sufficiently capture text quality.

In summary, operating in the compact latent space can enable more efficient composition than sequence space, but comes with challenges in fine-grained control, long text generation, and evaluation. Addressing these limitations could be interesting future work.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key keywords and terms include:

- Composable text operations
- Latent space 
- Energy-based models (EBMs)
- Ordinary differential equations (ODEs)
- Variational auto-encoder (VAE) 
- Pretrained language models (LMs)
- Attribute classifiers
- Text editing
- Text generation

To summarize, the key ideas of this paper involve performing composable text control operations in the latent space of text using energy-based models and efficient ODE-based sampling. It connects pretrained LMs to the latent space through adaptation and allows plugging in arbitrary attribute classifiers/operators to enable flexible text editing and generation while improving over previous approaches. The main advantages highlighted are the efficiency and quality gained by operating in the compact latent space compared to the complex text sequence space.

\section{Conclusions}
% \vspace{-5pt}
 We have developed a new efficient approach that performs composable control operations in the compact latent space of text, named \textsc{LatentOps}. 
The proposed method permits combining arbitrary operators applied on a latent vector, resulting in an energy-based distribution on the low-dimensional continuous latent space.
We develop an efficient and robust sampler based on ODEs that effectively samples from the distribution guided by gradients.
We connect the latent space to popular pretrained LM by efficient adaptation without finetuning the whole model.
We showcase its compositionality, flexibility and firm performance on several distinct tasks.
In future work, we can explore the control of more complicated texts.


% \clearpage
\section{Limitations}
Some potential limitations of the proposed \textsc{LatentOps} approach include:

- Operating on sentence-level latent vectors makes fine-grained token-level control more difficult (e.g., generating text with specific syntax).

- The low dimensionality of the latent space may make it challenging to generate very long, coherent texts.

- Limitations inherent in auto-regressive language models used for decoding could propagate (e.g., repetition, lack of long-term coherence).

- Relying on attribute classifiers makes the approach susceptible to errors or limitations of those classifiers (e.g., biased classifiers could propagate harmful biases).

- Need for some labeled data to train classifiers, though approach aims to reduce the amount needed.

- Finetuning a VAE on top of a pretrained LM still requires some labeled data and compute resources.

- Difficult to control aspects of text not captured well in the latent space from finetuning.

- Balancing fluency and adequacy during editing operations remains challenging.

- Hard to evaluate how well semantic meaning is preserved when manipulating latent vectors.

So in summary, the key limitations stem from operating in a reduced latent space and relying on imperfect pretrained components, which can make very fine-grained control and guaranteeing meaning preservation difficult. Scaling to very long texts and reducing reliance on labeled data are also challenges.

\section{Background}
% In this section, we first review the background of several key concepts used in our method.

\subsection{Energy-based Models and ODE Sampling}
\label{sec:bg_ebms}
Given an arbitrary energy function $E(\bm x)\in \mathbb{R}$, energy-based models (EBMs) define a Boltzmann distribution:
\begin{equation}
\small
\setlength\abovedisplayskip{0.1cm}
\setlength\belowdisplayskip{0.2cm}
    % p_\theta( \bm x) \propto e^{-E_{\theta}( \bm x)},
    \label{eq:def_ebm}
    p(\bm x) = e^{-E(\bm x)} / Z, %\propto e^{-E(\bm x)},
\end{equation}
where $Z=\sum_{\bm{x} \in \mathcal{X}} e^{-E(\bm x)}$ is the normalization term (the summation is replaced by integration if $\bm x  \in \mathcal{X}$ is a continuous variable). 
% EBMs represent data $\bm x$ by learning an unnormlaized probability distribution. Given an energy function $E(x)\in \mathbb{R}$, the EBM is defined as a Boltzmann distribution:
% \begin{equation}
%     % p_\theta( \bm x) \propto e^{-E_{\theta}( \bm x)},
%     \label{eq:def_ebm}
%     p(\bm x) = \frac{e^{-E(\bm x)}}{Z} \propto e^{-E(\bm x)}
% \end{equation}
% where $E( \bm x)$ (the energy) is an nonlinear regression function, usually parameterized by a neural network, and $Z$ is the normalization term as follow:
% \begin{equation}
%     \label{eq:normal_z}
%     Z= \int e^{-E(\bm x)}\text{d}\bm x,
% \end{equation}
% which is a constant towards to $\bm x$. 
EBMs are flexible to incorporate any functions or constraints into the energy function $E(\bm x)$. Recent work has explored text-based EBMs (where $\bm x$ is a text sequence) for controllable text generation \cite{DBLP:conf/nips/HuYSQLDX18,DBLP:conf/iclr/DengBOSR20,DBLP:conf/iclr/KhalifaED21,DBLP:journals/corr/abs-2203-13299,QinCOLD}.
% ,yulatent
Despite the flexibility, sampling from EBMs is rather challenging due to the intractable $Z$. The text-based EBMs
%, similar to other plug-and-play approaches mentioned in \S\ref{sec:intro}, 
face with even more difficult sampling due to the extremely large and complex (discrete or soft) text space.

% Due to the discrete nature of text, previous studies \citep{DBLP:journals/corr/abs-2203-13299}\hzt{more refs?} typically use the discrete Markov chain Monte Carlo (MCMC) sampling. The recent work \citep{QinCOLD} instead applies continuous approximations to the text and resorts to Langevin Dynamics (LD) \cite{DBLP:conf/icml/WellingT11,DBLP:journals/corr/abs-1811-08413,DBLP:journals/corr/abs-2004-06030}, a gradient-based MCMC, for more efficient sampling. On the other hand, due to several hyperparameters (e.g., step size, number of steps, noise scale), LD tends to be sensitive to the configurations \citep{nie2021controllable}\hzt{more classical refs?}. 
% More crucially, the extremely large and complex text sequence space $\mathcal{X}$ in the above text-based EBMs, either discrete or with continuous approximations, makes the sampling very challenging.

Langevin dynamics \cite[LD,][]{DBLP:conf/icml/WellingT11,DBLP:journals/corr/abs-1811-08413} is a gradient-based Markov chain Monte Carlo (MCMC) approach often used for sampling from EBMs \citep{DBLP:conf/nips/DuM19,DBLP:conf/nips/SongE19,DBLP:journals/corr/abs-2004-06030,QinCOLD}. It is considered as a more efficient way compared to other gradient-free alternatives (e.g., Gibbs sampling \cite{bishop2006pattern}). However, due to several critical hyperparameters (e.g., step size, number of steps, noise scale), LD tends to be sensitive and unrobust in practice \citep{nie2021controllable,DBLP:journals/corr/abs-1903-08689,DBLP:conf/iclr/GrathwohlWJD0S20}. 

On the other hand, stochastic/ordinary differential equations (SDEs/ODEs) \cite{anderson1982reverse} offer another sampling technique recently applied in image generation \citep{DBLP:conf/iclr/0011SKKEP21,nie2021controllable}. An SDE characterizes a \emph{diffusion process} that maps real data to random noise in continuous time $t\in[0, T]$. Specifically, let $\bm{x}(t)$ be the value of the process following $\bm{x}(t)\sim p_t(\bm{x})$, indexed by time $t$. At start time $t=0$, $\bm{x}(0)\sim p_0(\bm{x})$ which is the data distribution, and at the end $t=T$, $\bm{x}(T)\sim p_T(\bm{x})$ which is the noise distribution (e.g., standard Gaussian). The \emph{reverse} SDE instead generates a real sample from the noise by working backwards in time (from $t=T$ to $t=0$). More formally, consider a {\it variance-preserving} SDE \citep{DBLP:conf/iclr/0011SKKEP21} whose reverse is written as
\begin{equation}
\small
\setlength\abovedisplayskip{0.2cm}
\setlength\belowdisplayskip{0.2cm}
\label{eq:sde}
    \text{d}\bm x=-\frac{1}{2}\beta(t)[\bm x+2\nabla_{\bm x}\log p_t(\bm x)]\text{d}t + \sqrt{\beta(t)}\text{d}\Bar{\bm w},
\end{equation}
where d$t$ is an infinitesimal negative time step; $\Bar{\bm w}$ is a standard Wiener process when time flows backwards from $T$ to $0$; and the scalar $\beta(t):=\beta_0 + (\beta_T - \beta_0)t$ is a time-variant coefficient linear \emph{w.r.t.} time $t$. Given a noise $\bm{x}(T)\sim p_T(\bm{x})$, solving the above reverse SDE returns a $\bm{x}(0)$ that is a sample from the desired distribution $p_0(\bm{x})$. One could use different numerical solvers to this end.
%Drawing a data sample $\bm{x}(0)$ then amounts to solving the reverse SDE, for which we can use different numerical solvers~
\cite{burrage2000numerical, higham2001algorithmic,rossler2009second}. 
The SDE sampler sometimes need to combine with an additional corrector to improve the sample quality \citep{DBLP:conf/iclr/0011SKKEP21}.

Further, as shown in \citep{DBLP:conf/iclr/0011SKKEP21,DBLP:journals/entropy/MaoutsaRO20}, each (reverse) SDE has a corresponding ODE, solving which leads to samples following the same distribution. The ODE is written as (see Appendix~\ref{app:derivation_ode} for the derivations):
\begin{equation}
\small
\setlength\abovedisplayskip{0.2cm}
\setlength\belowdisplayskip{0.2cm}
    \label{eq:ode_x}
    \text{d}\bm x=-\frac{1}{2}\beta(t)[\bm x+\nabla_x\log p_t(\bm x)]\text{d}t.
\end{equation}
Solving the ODE with relevant numerical methods~\cite{euler1824institutionum,calvo1990fifth,engstler1997mur8} corresponds to an sampling approach that is more efficient and robust \citep{DBLP:conf/iclr/0011SKKEP21,nie2021controllable}.

In this work, we adapt the ODE sampling for our approach. Crucially, we overcome the text control and sampling difficulties in the aforementioned sequence-space methods, by defining the text control operations in a compact latent space, handled by a latent-space EBMs with the ODE solver for efficient sampling.



% EBMs have strong flexibility to incorporate with kinds of functions into the $E(\bm x)$. Some recent work~\cite{DBLP:conf/nips/HuYSQLDX18,DBLP:conf/iclr/DengBOSR20,yulatent, QinCOLD,DBLP:conf/iclr/KhalifaED21} has explored text-based EBMs for various tasks, but the EBMs are mainly adopted on decoding procedure. We instead formulate EBMs in low-dimensional continuous latent space, that enables off-the-shelf pretrained generative models to perform arbitrary compositional and controllable generation efficiently, without further finetuning.
% % To train an EBM on a data distribution $p_{D}$ with maximum likelihood estimation (MLE), contrastive divergence~\cite{DBLP:journals/neco/Hinton02} is a good choice.  \citet{DBLP:journals/corr/abs-1903-08689} proposed to minimize the Monte Carlo estimate of maximum likelihood by the gradient of the data log-likelihood $L(\theta) = \mathbb{E}_{ \bm x \sim p_{D}}[\log p_\theta[\nabla_{\theta}E_{\theta}( \bm x)]]$ as follow:
% % \begin{equation}
% %     \nabla L(\theta) = \mathbb{E}_{ \bm x^+ \sim p_D}\nabla E_\theta(\bm x^+) - \mathbb{E}_{ \bm x^-\sim p_\theta}\nabla_\theta E_\theta( \bm x^-).
% % \end{equation}

% % \lgy{SOLVED}\hzt{Our work does not use Langevin Dynamics, so do not spend space to disucss LD here. Instead, introduce ODE/SDE.}
% % To sample $\bm x^-$ for both training and generation, MCMC based off Langevin Dynamics~\cite{DBLP:conf/icml/WellingT11} is the common approach. Samples are initialized from uniform random noise and are iteratively refined as follow:
% % \begin{equation}
% %     x_{k+1} = x_k - \frac{\lambda}{2}\nabla_{\bm x}E_\theta (x_{k-1}) + \omega_{k},
% % \end{equation}
% % where $k$ is the $k^{th}$ iteration step, $x_0\sim p_0(\bm x)$ is the initial distribution, and $\omega_k \sim \mathcal{N}(0,\lambda I)$ is Gaussian noise. This form of sampling allows us to use the gradient of the combined distribution to generate samples from distributions composed of $p_\theta$ and the other distributions.  $x_k$ will be guaranteed to follow the $p_\theta$ under some regularity conditions~\cite{DBLP:journals/corr/abs-2101-03288}, when both $1/\lambda$ and $t$ approach to infinity.

% In spite of flexibility, while, sampling from EBMs is rather difficult due to the computation of $Z$. MCMC based off Langevin Dynamicss (LD)~\cite{DBLP:conf/icml/WellingT11} is the common sampling method~\cite{DBLP:journals/corr/abs-1811-08413,DBLP:journals/corr/abs-2004-06030,QinCOLD}. However, arbitrary energy models can cause sharp changes in gradients, so LD is hyperparameter-sensitive, and requires well-designed techniques to improve the speed and stability of convergence~\cite{DBLP:journals/corr/abs-1903-08689,DBLP:conf/iclr/GrathwohlWJD0S20}.
%%
% Sampling by solving solving discretized reverse-time Stochastic Differential Equations (SDEs)~\cite{anderson1982reverse} is another technique, and \citet{DBLP:conf/iclr/0011SKKEP21} recently successfully applied it in generative models.
% % \citet{DBLP:conf/iclr/0011SKKEP21} proposed a new sampling algorithm by solving discretized reverse-time Stochastic Differential Equations (SDEs). 
% Specifically, from time $t=0$ to $t=T$, the forward SDE maps real data $\bm x(0)\sim p_0(\bm x)$ to a simple noise distribution $\bm x(T)\sim p_T(\bm x)$ (e.g., standard Gaussian).
% And the reverse SDE aims to generate samples from the simple noise distribution. For example, we could sample from $p_0(\bm x)$ by solving the reverse SDE as follow:
% \begin{equation}
% \label{eq:sde}
%     \text{d}\bm x=-\frac{1}{2}\beta(t)[\bm x+2\nabla_{\bm x}\log p_t(\bm x)]\text{d}t + \sqrt{\beta(t)}\text{d}\Bar{\bm w},
% \end{equation}
% where d$t$ is an infinitesimal negative time-step, $\Bar{w}$ is a standard Wiener process when time flows backwards from $T$ to $0$, and $\beta(t)$ is a time-variant coefficient and is linear \emph{w.r.t.} time $t$. 
% We can direct solve the reverse SDE by numerical solvers. To improve the quality of samples, \citet{DBLP:conf/iclr/0011SKKEP21} propose Predictor-Corrector (PC) sampler, which adopts LD to further correct the samples from SDE solvers.
% % \citet{DBLP:conf/iclr/0011SKKEP21} proposed a reverse diffusion solver for solving reverse-time SDEs.

% Also, because there always exists a corresponding deterministic process (ODE) whose trajectories share the same marginal $\{p_t(\bm x)\}_{t=0}^{T}$ as the SDE~\cite{DBLP:journals/entropy/MaoutsaRO20,DBLP:conf/iclr/0011SKKEP21}, numerical method can be applied to solve the ODE:
% \begin{equation}
%     \label{eq:ode_x


## Based on the paper, what questions should we ask to create a comprehensive summary of it? Please list at least 10 questions.

 Here are 10 potential summary questions for the paper:

1. What is the key problem the authors are trying to solve with this work? 

2. What are the main limitations of prior methods for controllable text generation that the authors identify?

3. What is the high-level approach proposed in this paper? What are the two key components?

4. How does the proposed method formulate composable operations in the latent space? How is the EBM defined?

5. How does the proposed method sample latent vectors that satisfy desired attributes? What sampler is used and why?

6. How does the method connect the latent space to pretrained language models for decoding? How much retraining is needed?

7. What are the main benefits of operating in the latent space compared to sequence space according to the authors?

8. What tasks does the paper evaluate the method on? What are the key results?

9. What are the limitations of the proposed approach identified by the authors?

10. What are potential future directions based on this work?


## Design 10 in-depth questions about method proposed in this paper and avoid simple question.

 Here are 10 potential in-depth questions about the method proposed in this paper:

1. The paper proposes performing text operations in the latent space rather than the sequence space. Why is operating in the latent space more effective for composable text generation and editing? What are the key benefits?

2. The method adapts a pretrained language model like GPT-2 into a variational autoencoder framework. Why is it advantageous to build the latent space using a pretrained LM versus training an encoder-decoder from scratch? 

3. The energy-based model formulation includes a Gaussian prior over the latent space matched to the VAE's prior. How does this prior help ensure the decoded texts are fluent and high-quality?

4. The method acquires attribute classifiers by training on small specialized datasets separate from the target domain corpus. How does this design choice improve flexibility and composability? What are the tradeoffs?

5. The paper proposes an ODE-based sampler for efficient and robust sampling from the latent EBM. How does the ODE sampler compare to alternatives like Langevin dynamics or SDE/corrector samplers in terms of performance and sample quality?

6. For text editing, the method initializes the ODE solver with the latent vector of the source text. How does this initialization help preserve relevant information from the original text during editing?

7. The approach adds Gaussian noise to sampled latent vectors and selects the best variant. When is this strategy helpful and how does it improve results? What are limitations?

8. How does the method balance tradeoffs between accuracy, fluency, and diversity in generated and edited text? How could the balancing be further improved?

9. The approach requires minimal adaptation of a pretrained LM's parameters for the latent space. How does this differ from prior work like Optimus? What efficiency benefits does it provide?

10. The paper focuses on categorical attributes, but mentions the method could be extended to continuous attributes. How would the energy functions and overall approach need to be modified to handle continuous attributes?


## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a high-quality paragraph summarizing the key points of the paper:

This paper proposes a new approach called LatentOps for performing composable text control operations in the latent space of language models. It adapts pretrained language models like GPT-2 to a latent space using a variational autoencoder framework, allowing only a small subset of parameters to be tuned. Arbitrary text control operators like classifiers for attributes like sentiment or keywords can be plugged into this latent space, defining a latent energy-based model. Efficient sampling for desired attributes is performed using ODE solvers, avoiding difficult optimization in the complex discrete text space. Experiments demonstrate this approach manages to generate high quality text with control over diverse attributes like sentiment, tense, and keywords both singly and in composition, outperforming previous plug-and-play methods that operate directly in the text sequence space. The approach provides an efficient way to perform controllable text generation and editing by operating in the compact continuous latent space.


## Summarize the paper in one sentence.

 The paper proposes LatentOps, an approach that efficiently enables composable text control operations by defining control objectives in the compact latent space of pretrained language models and using an ODE-based sampler for robust sampling.


## Summarize the paper in one paragraphs.

 Here is a one paragraph summary of the key points from the paper:

This paper proposes a new efficient approach called LatentOps for performing composable text control operations in the compact latent space of language models. It adapts pretrained language models like GPT-2 into a variational autoencoder framework to connect them with a continuous latent space using only a small amount of additional parameters. Arbitrary attribute classifiers can then be plugged into this latent space to define energy-based models for controlling text generation. Efficient sampling from these models is enabled through ordinary differential equation solvers that are more robust than alternatives like Langevin dynamics. Experiments demonstrate that LatentOps can flexibly compose different text control operations like editing attributes and inserting keywords while generating more coherent and higher quality text compared to previous methods that operate directly in the complex discrete sequence space. The compact latent space representation makes the approach very efficient.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in the paper:

1. The paper proposes an energy-based model (EBM) defined on the latent space of text. How is the EBM formulated to allow flexible incorporation of different attributes and classifiers? What are the key components and formulations?

2. The paper uses ordinary differential equations (ODEs) to efficiently sample from the EBM distribution defined on the latent space. How is the ODE derived and adapted for the text latent sampling task? Walk through the key derivations.  

3. The method connects the text latent space to pretrained language models (LMs) like GPT-2 using a variational autoencoder (VAE) framework. How does the VAE training procedure differ from previous work on text VAEs? What makes it efficient for adapting pretrained LMs?

4. What are the advantages of operating in the latent space compared to previous approaches that apply text controls directly in the sequence space? Why is latent space more suitable for the flexible composition of text operations?

5. The method acquires attribute classifiers on the latent space using separate datasets. How does this contribute to its flexibility? Why is the latent space beneficial for efficiently training independent classifiers this way?

6. When editing existing text, how is the ODE sampling initialized to allow preserving relevant information from the original text? Why is this a suitable initialization strategy?

7. For generating new text, the paper samples the ODE initial point from a learned GAN distribution rather than the VAE prior. What is the motivation behind this? How does the GAN help?

8. The paper mentions a sample selection strategy to pick the best sample after ODE solving. How does this work and why is it helpful for generating high-quality outputs?

9. How does the proposed approach differ from recent latent-space editing methods like LDEBM? What are the key innovations that make this method more flexible and compatible with large pretrained LMs?

10. The method is evaluated on compositional tasks like sequential editing and simultaneous editing of multiple attributes. How does it compare, both quantitatively and qualitatively, to previous state-of-the-art baselines on these challenging tasks?
