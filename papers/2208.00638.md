# Composable Text Controls in Latent Space with ODEs

## What is the central research question or hypothesis that this paper addresses?

This paper develops an approach called LatentOps for performing composable text control operations in the latent space of text, rather than directly in the complex text sequence space. The central hypothesis is that operating in the compact and continuous latent space will allow for more efficient and robust sampling of target text sequences with desired attributes, compared to prior approaches that rely on search or optimization in the high-dimensional discrete text space. Specifically, the key research questions addressed are:1. How to formulate latent space energy-based models (EBMs) that enable flexible composition of arbitrary text control operators (e.g. attribute classifiers) while matching the structure of a pretrained variational autoencoder (VAE) text model?2. How to efficiently and robustly sample the resulting latent EBM distribution to obtain vectors adhering to desired text attributes? The paper proposes using ordinary differential equation (ODE) solvers for this sampling.3. How to connect the sampled latent vectors to a pretrained language model (LM) like GPT-2 to decode the vectors back into fluent target text sequences? The paper adapts the LM efficiently without full finetuning.By addressing these questions to develop the LatentOps approach, the central hypothesis is that text generation and editing tasks with complex attribute requirements can be performed more efficiently and flexibly compared to prior work, while leveraging powerful pretrained LMs. The experiments aim to validate these advantages.


## What is the main contribution of this paper?

Based on the abstract and introduction, the main contributions of this paper appear to be:1. Proposing a new approach called LatentOps for performing composable text control operations in the compact latent space of text, rather than the complex sequence space as done in previous methods. This allows more efficient and robust sampling using ODEs.2. Developing an energy-based model (EBM) formulation in the latent space that permits plugging in arbitrary operators (e.g. attribute classifiers) to define a distribution for sampling text with desired attributes. This enables flexible composition of different text control operations. 3. Connecting pretrained language models (e.g. GPT-2) to the latent space through efficient adaptation without full finetuning, in order to decode the latent samples into coherent text.4. Showcasing the effectiveness of LatentOps on compositional tasks like sequential/simultaneous text editing across attributes and text generation with attribute combinations, demonstrating improved quality and efficiency over previous baselines.In summary, the key ideas are performing composable text operations in a compact latent space rather than complex sequence space, using efficient ODE sampling, plug-and-play formulation with EBMs, and connecting to pretrained LMs through light adaptation. The experiments then demonstrate the benefits of this approach.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper proposes a new approach called LatentOps that performs composable text control operations efficiently in the compact latent space of text by leveraging energy-based models and ordinary differential equation sampling.\section{Introduction}Real-world text applications often involve composing diverse text control operations, such as editing attributes, manipulating keywords, generating new text with desired properties, etc. This paper proposes an efficient approach called LatentOps that enables composable text operations in the compact latent space of text. The key ideas are:- Connect pretrained language models to a continuous latent space through efficient adaptation as a VAE decoder. - Define arbitrary text operators (e.g. classifiers) in the latent space to form a latent EBM distribution.- Develop an ODE sampler for efficient and robust sampling from the distribution.Experiments on compositional attribute control and text editing demonstrate improved quality and efficiency over previous sequence-space methods.\section{Summary}The key points of the paper are:- Motivation: Real-world text tasks require composition of diverse control operations like editing attributes, keywords, generating new text, etc. Prior works are inefficient as they operate directly in complex sequence space.- Approach:   - Connect pretrained LMs to continuous latent space via efficient VAE adaptation (only update small subset of parameters).  - Formulate latent EBM by plugging in arbitrary operators (e.g. classifiers) in latent space.  - Develop ODE sampler for efficient and robust sampling from distribution.- Benefits:   - Avoid difficult optimization/sampling in high-dim sequence space.  - Compatible with powerful pretrained LMs via light adaptation.   - Flexible composition of any operators.  - Improved quality and efficiency over baselines on compositional control tasks.- Limitations:   - Harder to control at token level due to sentence-level latent vector.  - Difficult to generate very long sentences.  - Inherits some limitations of autoregressive LMs.In summary, the paper proposes an approach called LatentOps that performs composable text control operations in a latent space, demonstrating improved quality and efficiency over previous sequence-space methods. The key ideas are efficient LM adaptation, latent EBM formulation, and ODE sampling.


## How does this paper compare to other research in the same field?

Here are a few key ways this paper compares to other recent research on composable text generation and editing in the latent space:- Most prior work has focused on single attribute control or small pretrained models. This paper enables compositional control with multiple attributes using large pretrained LMs like GPT-2 by adapting them to a latent space. - Many recent methods perform optimization or sampling directly in the complex text sequence space. This paper formulates the operations in a compact continuous latent space which is more efficient.- For latent space methods, previous work often uses gradient-based editing or separate pretrained autoencoders/VAEs. This paper connects the latent space directly to pretrained LMs through efficient adaptation within a VAE framework.- This paper develops an ODE-based sampler for the latent EBM that is more robust and efficient than Langevin Dynamics used in some prior latent space text EBM work.- The proposed approach is flexible and plug-and-play, allowing easy combination of arbitrary new operations by simply training lightweight classifiers in the fixed latent space. Most prior work lacks such compositionality.- Experiments demonstrate strong performance in compositional generation and editing tasks by combining various attributes, significantly outperforming previous plug-and-play approaches.In summary, the key novelty of this paper is in formulating the composable text operations directly in a continuous latent space compatible with large pretrained LMs, and developing an efficient ODE-based sampler, which together enable flexible and high-quality text control. The experiments demonstrate the effectiveness of latent space operations compared to modifying complex text sequences directly.


## What future research directions do the authors suggest?

Here are some key future research directions suggested by the authors:- Exploring more complex text generation tasks beyond the sentence level, such as generating paragraphs or documents with coherent structure and discourse relations. The current approach focuses on sentence-level generation.- Extending the approach to enable more fine-grained control, such as generating text that conforms to specific syntactic templates or textual patterns. The current approach performs attribute-level control. - Incorporating external knowledge into the framework, such as factual knowledge or commonsense knowledge, to improve the factual correctness and logical consistency of generated text.- Developing more advanced evaluation metrics and benchmarks to better assess the quality of controlled text generation along various linguistic dimensions.- Exploring different conditional density estimations beyond the EBM formulation to model the joint distribution over text and attributes, which may lead to better sampling efficiency.- Investigating the societal impacts of controllable text generation and developing techniques to detect and mitigate potential harms.In summary, the main future directions are to scale up the approach to more complex text generation tasks, enable finer-grained control, incorporate external knowledge, improve evaluation, explore alternative conditional modeling and sampling techniques, and investigate societal impacts. Advancing research along these dimensions can help develop more capable and robust controllable text generation systems.\section{Introduction}% Recent large pretrained language models (LMs) such as GPTs \hzt{cite GPT-2/3} are capable of generating well-formed coherent text. \begin{figure}[t]    \centering    \includegraphics[width=0.45\textwidth,page=1]{fig/intro.pdf}    \vspace{-8pt}    \caption{Examples of different composition of text operations, such as editing a text in terms of different attributes sequentially (top) or at the same time (middle), or generating a new text of target properties (bottom). The proposed \textsc{LatentOps} enables a single LM (e.g., an adapted GPT-2) to perform arbitrary text operation composition in the latent space.}    \label{fig:intro}    \vspace{-22pt}\end{figure}Many text problems involve a diverse set of text control operations, such as editing different attributes (e.g., sentiment, formality) of the text, inserting or changing the keywords, generating new text of diverse properties, and so forth. In particular, different \emph{composition} of those operations are often required in various real-world applications (Figure~\ref{fig:intro}).  Conventional approaches typically build a conditional model (e.g., by finetuning pretrained language models) for each specific combination of operations~\cite{hu2017toward,keskarCTRL2019,DBLP:journals/corr/abs-1909-08593}, which is unscalable given the combinatorially many possible compositions and the lack of supervised data. Most recent research thus has started to explore plug-and-play solutions. Given a pretrained language model (LM), those approaches plug in arbitrary constraints to guide the production of desired text sequences \cite{Dathathri2020Plug,DBLP:journals/corr/abs-2104-05218, DBLP:conf/nips/KumarMST21, DBLP:conf/emnlp/KrauseGMKJSR21,DBLP:journals/corr/abs-2203-13299,QinCOLD}. The approaches, however, typically rely on search or optimization in the complex text \emph{sequence space}. The discrete nature of text makes the search/optimization extremely difficult. Though some recent work introduces continuous approximations to the discrete tokens \citep{qin2020back,QinCOLD,DBLP:conf/nips/KumarMST21}, the high dimensionality and complexity of the sequence space still renders it inefficient to find the accurate high-quality text.% (i.e., sequences of discrete tokens, or continuous token presentations or logits) that makes it difficult and inefficient to find the accurate high-quality text.In this paper, we develop \textsc{LatentOps}, a new efficient approach that performs composable control operations in the compact and continuous \emph{latent space} of text. \textsc{LatentOps} permits plugging in arbitrary operators (e.g., attribute classifiers) applied on text latent vectors, to form an energy-based distribution on the low-dimensional latent space. We then develop  an efficient sampler based on ordinary differential equations (ODEs) \cite{DBLP:conf/iclr/0011SKKEP21,nie2021controllable,Vahdat_LSGM} to draw latent vector samples that bear the desired attributes. %that effectively samples from the distribution guided by gradients, and yields the target vectors adhering to the combined operators. A key challenge after getting the latent vector is to decode it into the target text sequence. To this end, we connect the latent space to pretrained LM decoders (e.g., GPT-2) by efficiently adapting a small subset of the LM parameters in a variational auto-encoding (VAE) manner~\cite{DBLP:journals/corr/KingmaW13,DBLP:conf/conll/BowmanVVDJB16}. Previous attempts of editing text in latent space have often been limited to single attribute and small-scale models, due to the incompatibility of the latent space with the existing transformer-based pretrained LMs \cite{DBLP:conf/nips/WangH019,DBLP:conf/aaai/LiuFZPL20, DBLP:conf/icml/ShenMBJ20,DBLP:conf/acl/DuanXPHL20,DBLP:conf/emnlp/MaiPMSH20}. \textsc{LatentOps} overcomes the difficulties and enables a single large LM to perform arbitrary composable text controls.We conduct experiments on three challenging settings, including sequential editing of text \emph{w.r.t.} a series of attributes, editing compositional attributes simultaneously, and generating new text given various attributes. Results show that composing operators within our method manages to generate or edit high-quality text, substantially improving over respective baselines in terms of quality and efficiency.% Pretrained language models (PLMs), e.g., GPT-2~\cite{gpt2} and BERT~\cite{bert}, have been verified the powerful ability to generate fluent and reasonable sentences in a variety of tasks in Natural Language Process (NLP). % However, fluency is only one basic goal of text generation. In real application, how to generate sentences that satisfy various constraints is still a challenging problem. % For example in Fig.~\ref{fig:intro}, for attribute-guided generation~\cite{DBLP:conf/aaai/LiuFZPL20,DBLP:conf/nips/WangH019,DBLP:conf/icml/MuellerGJ17}, e.g., sentiment or tense, we expect the generated sentences conforming to the desired attribute. For keyword-guided generation~\cite{DBLP:journals/corr/abs-2202-11705}, we wish to guarantee that the specified keywords are included in the generated output. % Further, how to combine several constraints effectively, e.g., as the last example in Fig.~\ref{fig:intro}, generate a sentence that is negative sentiment and past tense, and contains keyword \textit{shop}, is still a great challenge to current methods.% Training a conditional model~\cite{DBLP:journals/corr/abs-1911-11161, DBLP:conf/acl/DaiLQH19, DBLP:conf/naacl/LiuNW21} is a general way to control the attributes, that is the conditional information is specified during training to guide the generation. The weakness of this approach is non-negligible: once the model is trained, it's not easy to adapt it to new attributes, i.e., it requires re-train the whole model if new attributes are introduced, which is expensive and time-consumed. An alternative approach is to train an unconditional model first, and then introduce conditional model into it with a small cost~\cite{DBLP:conf/aaai/LiuFZPL20,DBLP:conf/nips/WangH019, DBLP:conf/icml/ShenMBJ20}. Auto-encoder and variational auto-encoder are usually chosen as the generative model, and use some gradient-based method to edit the latent vector to satisfy the desired attributes. Broadly speaking, these methods are hard to conduct compositional generation, and are restricted in small-scale architecture like LSTM~\cite{hochreiter1997long}, restricting the generation capacity. % Another way~\cite{Dathathri2020Plug,DBLP:journals/corr/abs-2104-05218,QinCOLD} is to modify the hidden states or logits based on a pretrained language models, which usually train some attribute classifiers and add some constraints. But the drawbacks are that it's slow and inefficient, because the operations are conducted in the token-level hidden spaces.% % and a pre-defined prefix is needed \hzt{why is prefix needed?}% % , which will also lead to low diversity \hzt{why low-diversity? Diversity is not our key advantage. Do not emphasize the non-critical thing.}.% % In order to overcome the aforementioned shortcomings, we'd like to explore a workflow that is efficient and flexible, takes full advantage of the capacity of pretrained language models and possesses strong ability of compositionality.% Some recent works~\cite{nie2021controllable, DBLP:journals/corr/abs-2004-06030,DBLP:conf/iclr/GrathwohlWJD0S20} made a successful attempt to compose multiple attributes in image generation. \citet{nie2021controllable} modify the input latent vector of a pretrained image GAN to control the attributes. However, the connection between the GAN latent space and data space is unidirectional, requiring paired latent-image data for training, which is hard to obtain.% % Inspired by some works~\cite{nie2021controllable, DBLP:journals/corr/abs-2004-06030,DBLP:conf/iclr/GrathwohlWJD0S20} of image generation area, energy-based models (EBMs)~\cite{hopfield1982neural,DBLP:journals/neco/Hinton02,lecun2006tutorial} are a workable solution to the compositionality problem. The reason is that we can combine multiple energy functions representing different attributes to constitute the text generator. Each factor is represented by an individual energy function, and if the factor exists in the text, a low energy will be given.% % \hzt{A few works already highlight EBMs for composable controlled text generation, like COLD decoding. Our highlight/key differentiator is not EBMs, but instead operations in latent space. Note that in the Abstract I even do not mention EBMs in order to differentiate from COLD etc.}% In text area, \citet{li-etal-2020-optimus} provide a novel view to utilize PLMs to construct large VAE, which possesses the strong generation ability and diversity, and a structural continuous low-dimensional latent space, which is efficient and flexible to sample. But it requires fully finetunes the PLMs, comsuming large time and resources. % In this work, we design a new approach for composable text operations in the compact latent space of text. Each operation is based on a operator (attribute classifier) in latent space and we can combine arbitrary plug-in operators to generate desired text. % To guarantee the quality of generated texts, we connect the PLMs (i.e., GPT-2) to a latent space through efficient adaption, which then decodes the sampled vectors into the desired text sequences. Dispensing with fully finetuning, we only require a small part of weights to train. % Further, the low-dimensionality and differentiablity of the text vector permit us to develop an efficient and robust sampler based on the ordinary different equations (ODEs)~\cite{DBLP:conf/iclr/0011SKKEP21}. % Our method can be viewed as various text control operations (\textit{TextOps}) in latent space with ODEs. % we take advantage of the compositionality of EBMs and the strong generative capacity of pretrained language model, e.g., GPT-2, to accomplish high-quality controllable and compositional text generation \hzt{And this sentence suddenly is not relevant to latent space at all, disconnected from the last sentence.}. And we don't require to finetune the whole GPT-2, but only train an extra layer. Our method can be viewed as various text control operations (\textit{TextOps}) in latent space \hzt{only until the very end of introduction, we start mentioning our key idea: "text control operations in latent space", and the key methodological contribution, i.e., ODE sampler}, which is efficient and reliable. Further, we leverage the robustness and efficiency of ordinary different equation (ODE) solvers~\cite{DBLP:conf/iclr/0011SKKEP21,nie2021controllable} to sample from latent space. Here are some thoughts on potential future research directions:- Exploring more complex text generation tasks: The current approach focuses on sentence-level generation. Scaling up to generate longer coherent paragraphs/documents would be an interesting challenge. This would likely require incorporating discourse-level structure and coherence constraints.- Enabling finer-grained control: The attribute-level control could be extended to allow more granular editing of content, such as enforcing specific syntactic templates or textual patterns. This may require incorporating syntactic parsers/analyzers within the framework.- Incorporating external knowledge: Integrating external knowledge bases (facts, commonsense etc.) could help improve factual correctness and logical consistency of generated text. Useful directions include conditioned decoding and retrieval-augmented generation.- Developing better evaluation: More advanced automatic and human evaluation metrics are needed to properly assess controlled generation quality across linguistic dimensions. Useful directions include learning better content similarity metrics, designing more diagnostic test suites, and conducting user studies.- Exploring alternative density estimations: The current EBM formulation could be replaced by other conditional density models that enable more efficient sampling. Useful directions include flow-based models, diffusion probabilistic models, and normalizing flows.- Studying societal impacts: As controllable text generation matures, it is crucial to investigate potential misuse cases and develop techniques to detect and mitigate harms. Useful directions include auditing biases in systems and datasets, developing countermeasures against malicious use, and encouraging responsible research practices.In summary, the high-level goals are to scale up the capabilities, refine control, increase grounding, improve evaluation, innovate on conditional modeling/sampling, and ensure societal safety. Advancing along these dimensions can lead to more robust and socially responsible controllable text generation.The authors propose some good directions for future work on controllable text generation. Here are a few additional ideas:- Multi-task learning: Explore multi-task learning frameworks to jointly model multiple attributes in a single model, avoiding retraining for new attributes. This could improve efficiency and leverage inter-dependencies between attributes.- Long form generation: Extend beyond sentence-level generation to model global coherence and discourse structure for generating long-form text like essays, reports, stories etc. Hierarchical latent variable modeling may help here.- Low/few-shot learning: Develop techniques like prompt-learning or meta-learning to enable controlling new attributes with very limited labeled data. This can avoid costly data annotation. - Semantic control: Move beyond surface form attributes like sentiment to enable control over semantic aspects like meaning, entailment, factual accuracy etc. Useful techniques may include grounding in knowledge bases.- Creative control: Design control schemes that go beyond mimicking human text to enabling more creative expression, like through style transfer, sentence fusion, or guided imagination.- Safety and ethics: Conduct rigorous testing to characterize failure modes and enable safe deployment. Develop techniques like watermarking and provenance tracking to prevent misuse. Study social impacts through surveys and simulation.- User interfaces: Create intuitive user interfaces that allow easy specification of control parameters without needing expertise in ML. Democratizing control to end users can enable many beneficial applications.- Theoretical analysis: Conduct rigorous theoretical analysis to elucidate modes of failure, characterize control bounds, and provide optimality guarantees. This can lead to more robust approaches.Overall, advancing capabilities for complex tasks, extreme data efficiency, semantic grounding, creative expression, safe deployment, easy use by non-experts, and rigorous understanding would help unlock the full potential of controllable text generation.Here are a few additional future research directions to consider for controllable text generation:- Multi-modal control: Extend control frameworks to allow conditioning on non-text inputs like images, audio, video etc. This can enable multi-modal applications like image/video captioning with fine-grained control.- Discrete control: Allow control through discrete operations like word/phrase insertion, deletion, replacement etc. This is more intuitive than continuous latent space editing for some use cases.- Hierarchical control: Develop hierarchical control schemes operating at different linguistic levels (words, sentences, paragraphs) for more structured editing. Useful techniques may include hierarchical VAEs.- Weakly supervised control: Minimize reliance on labeled data through weakly/un-supervised techniques like adversarial learning, self-supervision, and leveraging unlabeled corpora.- Personalized control: Learn to automatically customize control schemes to individual users based on interaction history, writing style, persona etc. This can improve ease of use.- Theoretical foundations: Strengthen theoretical understanding of controllability for text through analyses grounded in linguistics, information theory, causality, game theory etc. - Interpretability: Increase interpretability of how models enact control through attention visualizations, saliency maps, example-based explanations etc. This is important for trust and transparency.- Efficiency: Improve computational and sample efficiency through techniques like discrete latent variables, normalizing flows, and better gradient estimators.Overall, broadening the scope and scale of control, reducing supervision, personalizing to users, strengthening theoretical grounding, and boosting interpretability and efficiency would be valuable research directions to explore. This can help drive advances in controllable text generation.Some other potential research directions for controllable text generation:- Low-resource language support: Adapt techniques to work well even with limited training data, enabling control capabilities for low-resource languages. Useful directions include cross-lingual transfer learning and leveraging multilingual pretrained models.- Non-categorical control: Explore controlling non-categorical aspects like style
