# Automatic Prompt Augmentation and Selection with Chain-of-Thought from   Labeled Data

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is: How can chain-of-thought prompting be automated and adapted to downstream tasks without requiring manual engineering of rationales?The key points are:- Chain-of-thought (CoT) prompting advances reasoning abilities of LLMs but relies on carefully designed human-annotated rationales. This is laborious and poses challenges for applying CoT to new tasks.- The paper proposes Automate-CoT, a method to automatically augment and select CoT prompts from labeled data without human annotation of rationale chains. - It has 3 main steps:   1) Augment: Use LM to generate pseudo-chains for questions.   2) Prune: Remove incorrect chains based on answer consistency.   3) Select: Optimize selection of chains using policy gradient.- This automates finding good CoT prompts for a task using just its labeled data. It adapts prompts better than human design by mitigating order/style sensitivity and finding an optimal complexity/diversity tradeoff.- Experiments show state-of-the-art results on arithmetic, commonsense, symbolic reasoning (+2-3\%) and non-reasoning tasks (+2.5\%), demonstrating wide applicability.In summary, the key hypothesis is that chain-of-thought prompting can be effectively automated for any task by augmenting, pruning bad chains, and optimizing selection, without manual annotation of rationales. The results validate this hypothesis.


## What is the main contribution of this paper?

The main contribution of this paper is proposing a new method called Automate-CoT that can automatically augment and select rationale chains for chain-of-thought prompting, without requiring manual engineering of prompts. Specifically, the key ideas and contributions are:- Proposing a pipeline to automatically augment reasoning paths from a small labeled dataset, prune low-quality chains, and select an optimal combination of chains to construct the prompts. This bypasses the need for manually designing prompts for each new dataset.- Applying a variance-reduced policy gradient strategy to optimize the selection of rationale chains. This helps choose the most helpful combinations of chains while mitigating sensitivity issues like order and style. - Demonstrating state-of-the-art results by using Automate-CoT on a diverse set of reasoning and non-reasoning tasks. It improves over manual CoT prompting and other baselines on arithmetic, commonsense, symbolic reasoning, QA, NLI, and sentiment analysis datasets.- Providing comprehensive analysis on the effects of chain complexity, diversity, pool size, etc. and comparisons to fine-tuning that validates the design choices.In summary, the key contribution is developing a prompt augmentation and selection framework that can automatically adapt chain-of-thought prompting to new datasets without human annotation of rationale chains. This advances the applicability of CoT to broader tasks.
