# [Multimodal Transformer With a Low-Computational-Cost Guarantee](https://arxiv.org/abs/2402.15096)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Transformer models have advanced multimodal understanding but suffer from high computational complexity in multi-head attention, which scales quadratically with sequence length. This issue worsens as the number of modalities increases.
- Prior efficient Transformers reduce cost using sparse attention patterns or matrix approximations, but have limitations when applied to multimodal inputs due to the diversity of fusion mechanisms used.

Proposed Solution:
- The authors introduce Low-Cost Multimodal Transformer (LoCoMT), a novel attention mechanism for efficient multimodal fusion. 
- Each attention head in LoCoMT uses one of two patterns: self-attention or cross-attention between modalities. This reduces computations compared to standard multimodal attention.
- LoCoMT allows flexible control over multimodal signals by assigning different patterns to heads and layers. It ensures lower cost than even late fusion Transformers.

Main Contributions:
- Theoretical analysis showing LoCoMT's attention cost scales with the square of differences between modality sequence lengths, making it efficient for varied lengths.
- Empirical evaluation on AudioSet and MedVidCL showing LoCoMT matches or improves on state-of-the-art efficiency and performance.
- Analysis of impact of attention patterns across layers, finding self-attention benefits low layers and cross-attention higher ones.
- Demonstration of configurable performance-efficiency trade-off, with LoCoMT reducing GFLOPs by 81% and 43% over baselines with minimal performance loss.

In summary, the paper introduces LoCoMT, a novel and configurable Transformer attention mechanism for efficient multimodal understanding, which provides strong efficiency and performance.
