# [CI w/o TN: Context Injection without Task Name for Procedure Planning](https://arxiv.org/abs/2402.15579)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem: 
The paper explores the challenge of procedure planning in instructional videos. This involves generating goal-directed action plans to transform a start state to a goal state based on visual observations. Prior methods use varying supervision like visual observations, language instructions or task labels during training. However, recent large language models (LLMs) can produce detailed plans even with just the task name as prompt. This motivates exploring a weaker setting without even the task name.

Proposed Solution:
The paper hypothesizes that previous intermediate supervisions provide context information to aid planning. To obtain cheaper context, they propose using captions of start and goal visual observations as supervision, generated easily via pre-trained vision-language models. Specifically, they use BLIP to generate captions and train a context feature on them using contrastive loss. This context feature is then fed along with noise vectors to a transformer decoder to generate plan actions. Multiple losses including adversarial loss are used for training the full model.

Main Contributions:
- Explores a novel weaker supervision setup for procedure planning without intermediate supervisions like visual states, language instructions or even task names which LLMs currently struggle with.
- Proposes using automatically generated image captions as cheaper contextual supervision to aid planning.
- Achieves comparable performance to prior works on two datasets using the proposed approach, validating the efficacy of cheap caption context over more expensive supervisions.

In summary, the paper makes planning without strong supervision possible by utilizing image captions as contextual information, with experiments confirming the viability of this direction.
