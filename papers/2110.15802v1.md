# [BERMo: What can BERT learn from ELMo?](https://arxiv.org/abs/2110.15802v1)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the key research questions and hypotheses appear to be:

- Can modifying the BERT architecture by linearly combining features from different layers (inspired by ELMo) improve performance on semantic NLP tasks? The hypothesis is that combining features from multiple layers will give a richer feature representation and improve performance, especially on semantic tasks.

- Will adding these skip connections make BERT more robust to compression techniques like pruning? The hypothesis is that improving gradient flow will make the model more stable during pruning. 

- Can this modified architecture enable higher compression rates or better parameter efficiency compared to baseline BERT? The hypothesis is that the improved gradient flow will allow more aggressive pruning while maintaining accuracy.

In summary, the main research questions revolve around whether combining features from multiple layers can improve BERT's performance and robustness, especially for semantic tasks and in compressed settings. The key hypotheses are that this architectural change will enable better feature representations, improve gradient flow, and allow higher levels of pruning without hurting accuracy. The experiments aim to test these hypotheses.
