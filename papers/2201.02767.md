# [QuadTree Attention for Vision Transformers](https://arxiv.org/abs/2201.02767)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the key research questions/hypotheses addressed in this paper are:1. Can building token pyramids and computing attention in a coarse-to-fine manner reduce the quadratic computational complexity of standard vision transformers to linear complexity?2. Can this proposed QuadTree Attention mechanism capture both fine image details and long-range dependencies effectively? 3. Can QuadTree Attention achieve state-of-the-art performance in vision tasks requiring dense predictions like object detection, feature matching, stereo matching etc. while being efficient?4. How does QuadTree Attention compare to other efficient transformer architectures like linear transformers, PVT, Swin Transformer etc. in terms of computational complexity, memory usage and performance in tasks like image classification, object detection, feature matching etc.?In summary, the central hypothesis is that the proposed QuadTree Attention can reduce the quadratic complexity of standard transformers to linear, while retaining the capability to capture both local fine details and global context. This can enable efficient application of transformers to high-resolution vision tasks requiring dense predictions. The paper presents experiments on tasks like image classification, object detection, feature matching and stereo matching to demonstrate the efficiency and effectiveness of QuadTree Attention compared to prior efficient transformer architectures.
