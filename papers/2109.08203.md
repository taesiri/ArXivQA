# [Torch.manual_seed(3407) is all you need: On the influence of random   seeds in deep learning architectures for computer vision](https://arxiv.org/abs/2109.08203)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research questions it addresses are:1. What is the distribution of scores (accuracy) with respect to the choice of random seed in deep learning models for computer vision? 2. Are there "black swan" seeds that produce radically different (better or worse) results compared to most other seeds? 3. Does pretraining on larger datasets mitigate the variability in results induced by the choice of random seed?The overall goal seems to be to analyze the effect of random seed selection on model accuracy across different computer vision tasks and model architectures. The author scans a large number of random seeds (up to 10,000) to test whether seed selection can lead to significant variability in accuracy, which could potentially invalidate or exaggerate results. The study aims to quantify this variability and determine if factors like pretraining mitigate it.In summary, the central hypothesis is that random seed selection can substantially impact measured model accuracy across different computer vision setups, and the author systematically tests this hypothesis through extensive experiments.


## What is the main contribution of this paper?

The main contribution of this paper seems to be an investigation into the effect of random seed selection on the accuracy of deep learning models for computer vision tasks. The key points are:- The author tested a large number of random seeds (up to 10,000) with various deep learning architectures on the CIFAR-10 and ImageNet datasets. - Even though the variance across different seeds was not very large, it was easy to find "outlier" seeds that performed much better or worse than average. - On CIFAR-10, a difference of ~2% accuracy was observed just by selecting different random seeds. This is considered a significant difference in the computer vision community.- On ImageNet, a ~0.5% accuracy difference was seen across seeds even when using the same pretrained model. This is still viewed as significant for ImageNet.- The author argues that many recent computer vision papers likely report results that are partly due to lucky seed selection, rather than just the merits of the method itself. - The paper recommends that authors perform more rigorous analyses by reporting performance over multiple random seeds/dataset splits to determine significance.In summary, the main contribution is demonstrating that performance differences attributed to algorithmic changes in computer vision papers may often be partly or fully explained by just the choice of random seed. The author advocates for more rigorous experimental practices.
