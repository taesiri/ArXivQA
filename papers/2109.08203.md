# [Torch.manual_seed(3407) is all you need: On the influence of random   seeds in deep learning architectures for computer vision](https://arxiv.org/abs/2109.08203)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research questions it addresses are:1. What is the distribution of scores (accuracy) with respect to the choice of random seed in deep learning models for computer vision? 2. Are there "black swan" seeds that produce radically different (better or worse) results compared to most other seeds? 3. Does pretraining on larger datasets mitigate the variability in results induced by the choice of random seed?The overall goal seems to be to analyze the effect of random seed selection on model accuracy across different computer vision tasks and model architectures. The author scans a large number of random seeds (up to 10,000) to test whether seed selection can lead to significant variability in accuracy, which could potentially invalidate or exaggerate results. The study aims to quantify this variability and determine if factors like pretraining mitigate it.In summary, the central hypothesis is that random seed selection can substantially impact measured model accuracy across different computer vision setups, and the author systematically tests this hypothesis through extensive experiments.


## What is the main contribution of this paper?

The main contribution of this paper seems to be an investigation into the effect of random seed selection on the accuracy of deep learning models for computer vision tasks. The key points are:- The author tested a large number of random seeds (up to 10,000) with various deep learning architectures on the CIFAR-10 and ImageNet datasets. - Even though the variance across different seeds was not very large, it was easy to find "outlier" seeds that performed much better or worse than average. - On CIFAR-10, a difference of ~2% accuracy was observed just by selecting different random seeds. This is considered a significant difference in the computer vision community.- On ImageNet, a ~0.5% accuracy difference was seen across seeds even when using the same pretrained model. This is still viewed as significant for ImageNet.- The author argues that many recent computer vision papers likely report results that are partly due to lucky seed selection, rather than just the merits of the method itself. - The paper recommends that authors perform more rigorous analyses by reporting performance over multiple random seeds/dataset splits to determine significance.In summary, the main contribution is demonstrating that performance differences attributed to algorithmic changes in computer vision papers may often be partly or fully explained by just the choice of random seed. The author advocates for more rigorous experimental practices.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

The paper investigates the effect of random seed selection on the performance of deep learning models for computer vision tasks. The main finding is that even small variations in the random seed can produce significant differences in model accuracy, suggesting current practices may overstate performance due to "lucky" seed selection.


## How does this paper compare to other research in the same field?

Here are a few thoughts on how this paper compares to other research in computer vision and deep learning:- The focus on analyzing the impact of random seeds is quite novel. Most CV papers report results from only 1-2 runs and don't investigate seed variability in depth. This paper takes an empirical approach to quantifying seed influence.- The scale of the experiments, with 10,000 seeds tested on CIFAR-10, is impressive. Many seed analysis papers use smaller datasets like MNIST or fewer runs. The ImageNet experiments are more limited but still cover 50 seeds.- The models and accuracies are reasonably strong but not state-of-the-art. As the author acknowledges, testing seeds on the best possible models would give a more definitive picture. But the experiments are solid for the compute budget.- The analysis of seed influence even after convergence is an interesting angle. Many assume training for longer removes variability, but this paper shows stable variation after convergence.- Finding "black swan" high and low performing seeds highlights the risk of reporting only best scores, common practice in many papers. But the distributions are relatively stable, alleviating some concerns.- Using self-supervised pretraining on ImageNet is a relevant experimental setup given the popularity of SSL techniques recently. The observed seed variance is worryingly high for a fixed pretrained model.Overall, while limited in scale, I think the paper makes a useful contribution to the understanding of randomness in vision models. The analysis is generally rigorous and the conclusions, while preliminary, highlight the need for further investigation into reproducibility. It connects well to the broader discussion on rigor in ML research.


## What future research directions do the authors suggest?

Based on my reading of the paper, some key future research directions suggested by the authors are:- Performing more robust studies on the influence of random seeds, scanning a very large number of seeds (10^4 to 10^5) with large state-of-the-art models trained from scratch. This would provide more empirical data on what should be considered statistically significant results versus just noise.- The community adopting more rigorous experimental practices, including performing randomness studies by varying seeds and dataset splits. Authors should report average, standard deviation, minimum and maximum scores over multiple runs. - Exploring the distribution of scores more thoroughly, including computing higher order moments beyond just mean and standard deviation. This could reveal more insights into the shape of the distribution.- Testing whether the variance in results due to random seeds decreases as models achieve higher accuracy, or if the variability persists. More experiments at greater scales are needed.- Evaluating whether certain seeds are intrinsically "better" than others, even after convergence. This could have implications for deliberately selecting good seeds.- Expanding the study to other domains beyond computer vision that rely heavily on deep learning. Assessing the effect of random seeds in areas like NLP.- Developing techniques to mitigate the sensitivity to random seeds, making models more robust and results more consistent across seeds. This could include new regularization methods or training procedures.In summary, the authors call for more rigorous analysis of the influence of random factors like seeds on model performance across computer vision and deep learning. They suggest this is needed to separate true progress from variability caused by randomness.


## Summarize the paper in one paragraph.

The paper investigates the effects of random seed selection on the accuracy of deep learning models for computer vision. It examines the distribution of model accuracy over different random seeds, finding that the distribution is fairly concentrated around the mean, but that it is possible to find outlier seeds that produce significantly higher or lower accuracy. Experiments on CIFAR-10 with 10,000 seeds found a gap of 1.82% between best and worst accuracy. Experiments on ImageNet with pretrained models and 50 seeds still found a 0.5% accuracy gap, which is considered significant. The paper argues that many computer vision results may be overstated due to implicit "lucky" seed selection during model tuning. It recommends authors rigorously study the effects of randomness by varying seeds and dataset splits. Overall, the paper shows random seed selection can significantly impact accuracy of vision models, and more rigor is needed when reporting results.
