# [WeatherProof: Leveraging Language Guidance for Semantic Segmentation in   Adverse Weather](https://arxiv.org/abs/2403.14874)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
Semantic segmentation models achieve high performance on clear weather images but suffer significant drops when tested on images with adverse weather conditions like rain, fog, or snow. This is partly due to the complex visual degradation effects induced by weather, which can be a combination of multiple effects (e.g. both rain and fog). Existing datasets also lack accurately paired clear and degraded images to analyze this gap.

Proposed Solution: 
The authors introduce the WeatherProof dataset - the first semantic segmentation dataset with accurately paired clear and real degraded images, enabling controlled analysis of weather effects. They analyze performance of recent segmentation models on this and find drops of 5-15% mIoU on degraded images. 

To improve robustness, they propose injecting textual guidance on weather composition using CLIP encodings. Specifically, they learn a weighted combination of CLIP encodings of weather texts like "rain" or "fog" that represents the blend of conditions in an image. This vector is concatenated with the image encoding and injected into the model via cross-attention.

Main Contributions:

- WeatherProof, a large-scale semantic segmentation dataset with 174K accurately paired clear and real degraded images 

- Analysis showing models are sensitive to complex blends of weather effects

- A CLIP injection module to provide textual guidance on weather conditions, improving performance by up to 10.2% on WeatherProof and 8.4% on ACDC over standard training

- State-of-the-art results on ACDC semantic segmentation in rain, fog and snow conditions

The proposed method leverages language/vision models like CLIP to make segmentation networks more robust to adverse weather in real-world conditions. The textual guidance acts as an inductive bias to focus learning on weather degradations.
