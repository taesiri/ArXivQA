# [Supervised Pretraining Can Learn In-Context Reinforcement Learning](https://arxiv.org/abs/2306.14892)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central research question this paper addresses is: can large transformer models be pretrained via supervised learning to make effective reinforcement learning (RL) decisions in-context, adapting their behavior in an online or offline manner without any parameter updates at test time? Specifically, the authors introduce a new model called Decision-Pretrained Transformer (DPT) which is pretrained via supervised learning to predict optimal actions in bandit and MDP environments, given a context dataset of state-action-reward tuples. The key research questions are:1) Can this simple pretraining objective give rise to emergent decision-making abilities like exploration, exploitation, and handling uncertainty, without being explicitly trained to do these behaviors? 2) How does DPT generalize to new tasks and datasets at test time? Can it improve upon the strategies in the pretraining data?3) Does DPT implement some form of efficient posterior sampling or Bayesian reinforcement learning under the hood? Can connections be made between in-context learning and posterior sampling?The authors address these questions through empirical evaluations of DPT on bandit and MDP tasks, comparing it to specialized algorithms like Thompson sampling and PPO. They also provide some theoretical analysis to link DPT to posterior sampling and establish regret bounds. The overarching hypothesis is that supervised pretraining alone can instill strong in-context decision-making abilities in transformers without needing complex objectives.
