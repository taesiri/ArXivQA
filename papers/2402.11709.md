# [GNNavi: Navigating the Information Flow in Large Language Models by   Graph Neural Network](https://arxiv.org/abs/2402.11709)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Fine-tuning large language models (LLMs) is crucial to enhance their adaptability, but full parameter fine-tuning has high computational demands. 
- Prompt-based fine-tuning is effective for LLMs in low-data settings, but still computationally expensive.
- Existing parameter-efficient methods like adapter modules are not tailored for prompt-based fine-tuning of LLMs.

Proposed Solution:
- The paper proposes GNNavi, a novel parameter-efficient fine-tuning (PEFT) method designed for prompt-based learning of LLMs.
- It is inspired by recent insights into in-context learning (ICL) - label words act as anchors that aggregate and distribute information.
- GNNavi integrates a graph neural network (GNN) layer into the LLM to precisely guide information flow during prompt processing.
- The input text is treated as a graph with tokens as nodes. Nodes are connected based on paths of information flow learned from ICL studies.
- The GNN layer aggregates information from neighboring nodes to update node representations.

Main Contributions:
- Proposes GNNavi, a specialized PEFT method for LLMs using a GNN layer to navigate information flow when processing prompts.
- Achieves state-of-the-art performance on text classification with only 0.2-0.5% trainable parameters, outperforming full fine-tuning.
- Analyzes GNNavi's information flow and shows more stable aggregation compared to full fine-tuning.
- Introduces novel application of GNNs in NLP to enhance LLMs based on information flow perspective.
- Sheds light on handling LLMs from a graph perspective.

In summary, the paper makes significant contributions in designing a specialized, efficient prompt-based fine-tuning approach for LLMs using graph neural networks to precisely control information flow.
