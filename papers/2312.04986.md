# [Out of Context: How important is Local Context in Neural Program Repair?](https://arxiv.org/abs/2312.04986)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a summary paragraph of the key points from the paper:

This paper systematically studies the effect of local context on neural program repair (NPR) success. Through experiments on over half a million bugs across three datasets and two programming languages, the authors find that increasing symmetric context from 2 to 56 lines yields substantial improvements in repair accuracy, from 16-29\% relative. Benefits continue beyond typical context sizes in NPR, with 7-12% relative gains from 10 to 56 lines. However, context effects vary based on bug type and complexity: identifier-related bugs benefit more than others, while bugs needing many changes improve little. Regarding context position, a window centered on the bug performs best, but extreme positions complement each other in ensembles. Finally, larger models and more candidates do not disproportionately benefit from more context. The authors discuss implications and opportunities around larger contexts, including further potential, robustness issues, and non-local limits. They emphasize the need for clear documentation of context choices and sufficient context in NPR datasets. Overall, this thorough study demonstrates local context is an important driver of NPR success, though many open questions remain about optimal usage.
