# [Reinforcement Learning from Human Feedback with Active Queries](https://arxiv.org/abs/2402.09401)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Aligning large language models (LLMs) with human preferences is important for building useful generative models. This is typically done via reinforcement learning from human feedback (RLHF).
- However, current RLHF methods require large amounts of human-labeled preference data, which is expensive and inefficient to collect. 

Proposed Solution:
- Formulate the problem as a contextual dueling bandit and design query-efficient RLHF algorithms based on ideas from active learning. 
- Propose an Active Proximal Policy Optimization (APPO) algorithm for the linear contextual dueling bandit setting. APPO selectively queries human preference based on an uncertainty criterion and achieves Õ(d2/Δ) regret with Õ(d2/Δ2) queries.
- Propose Active Direct Preference Optimization (ADPO), a practical active learning algorithm for RLHF. ADPO adapts APPO to the direct preference optimization (DPO) framework. It queries preferences when model uncertainty is high and uses model predictions as pseudo-labels when uncertainty is low.

Main Contributions:
- Formalize RLHF as a contextual dueling bandit problem and design the first query-efficient algorithm (APPO) for it with regret and query complexity bounds. The bounds do not depend on the action space size.
- Propose ADPO, the first active learning algorithm for RLHF. Empirically show it matches DPO performance with 50% fewer human queries when finetuning LLMs.
- Demonstrate both theoretically and empirically the promise of active learning for improving query efficiency of RLHF.

In summary, this paper makes both theoretical and practical contributions towards designing query-efficient reinforcement learning algorithms for aligning LLMs to human preferences. The key idea is to leverage active learning to selectively query the most useful human feedback.
