# [Efficient Memory Management for Large Language Model Serving with   PagedAttention](https://arxiv.org/abs/2309.06180)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading, the central research question this paper addresses is:

How can we improve the throughput of large language model (LLM) serving systems by efficiently managing the memory required to store the key-value (KV) cache tensors?

The key hypotheses appear to be:

1) Existing LLM serving systems like Orca and FasterTransformer manage KV cache memory inefficiently, leading to fragmentation and underutilization. This limits the number of requests that can be batched together, hurting throughput.

2) Storing the KV cache in non-contiguous blocks, inspired by virtual memory and paging in operating systems, can alleviate fragmentation and enable more flexible memory sharing across requests. This allows more requests to be batched, improving throughput.

3) Implementing these ideas in a system called vLLM built on a new attention algorithm called PagedAttention can substantially improve LLM serving throughput over existing systems.

So in summary, the main research question is how to enhance LLM serving throughput via better KV cache memory management, with the central hypotheses being that OS-inspired paged memory management and a system called vLLM can achieve this goal. Let me know if you would like me to clarify or expand on any part of the summary!


## What is the main contribution of this paper?

 The main contribution of this paper is proposing PagedAttention, a new attention algorithm that allows the key-value (KV) cache to be stored in non-contiguous paged memory, and building vLLM, a high-throughput large language model (LLM) serving system with efficient memory management enabled by PagedAttention. 

Specifically, the key ideas and contributions are:

- Identifying the challenges of memory allocation inefficiency in serving large language models, including fragmentation and inability to share memory across requests/sequences. 

- Proposing PagedAttention, which divides the KV cache into fixed-size blocks that can be flexibly mapped to physical memory, inspired by virtual memory and paging in operating systems.

- Designing and implementing vLLM, a distributed LLM serving system built on PagedAttention, which achieves near-zero waste in KV cache memory via block-level management and request scheduling co-designed with PagedAttention.

- Supporting various decoding methods like parallel sampling, beam search, and shared prefixes in vLLM using techniques like copy-on-write borrowed from OS.

- Evaluating vLLM on different models and workloads, demonstrating 2-4x throughput improvements compared to previous state-of-the-art systems like FasterTransformer and Orca.

In summary, the key contribution is introducing virtual memory and paging techniques to efficiently manage the dynamically growing KV cache in LLM serving, and building an end-to-end serving system vLLM that substantially improves throughput by reducing memory waste and increasing sharing.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the paper:

The paper proposes PagedAttention, a new attention algorithm that stores attention keys and values in non-contiguous paged memory, and builds vLLM, a high-throughput large language model serving system with efficient memory management enabled by PagedAttention.


## How does this paper compare to other research in the same field?

 Here are some key points comparing this paper to other related work in efficient large language model serving:

- This paper introduces a novel attention algorithm called PagedAttention, which allows storing key/value cache in non-contiguous blocks inspired by virtual memory and paging techniques in operating systems. This is a fundamentally new approach compared to prior work like Orca and FasterTransformer which rely on contiguous memory allocation. 

- The system vLLM built on PagedAttention demonstrates substantial improvements in memory utilization and throughput compared to state-of-the-art systems. For example, it improves throughput by 2-4x over Orca and 22x over FasterTransformer in experiments. This is a significant advance in serving performance.

- The idea of applying OS concepts like paging and virtual memory to GPU memory management is novel. Prior work has not explored this direction for model serving. The block-level memory management enables more flexible and efficient sharing compared to existing systems.

- vLLM supports a wide range of decoding algorithms like parallel sampling, beam search, and shared prefixes through its flexible memory management. Handling diverse decoding methods efficiently is difficult in prior serving systems.

- The distributed execution design allows vLLM to scale to large models exceeding single GPU memory capacity. Distributed memory management for model parallel serving is still an open challenge tackled by this work.

- While iteration-level scheduling techniques like Orca aim to increase utilization via better request scheduling, vLLM focuses more on boosting memory utilization to allow larger batch sizes. The approaches are complementary.

Overall, the paper makes significant contributions through the PagedAttention algorithm, vLLM system design, and comprehensive evaluation of benefits over state-of-the-art. The virtual memory inspired approach is particularly novel for this domain.


## What future research directions do the authors suggest?

 Based on my reading of the paper, here are some of the key future research directions suggested by the authors:

- Applying the virtual memory and paging technique to other GPU workloads that have similar properties as LLM serving (e.g. dynamic memory allocation, memory bound performance). The authors suggest it may not be suitable for workloads like DNN training where memory allocation can be optimized ahead of time.

- Exploring LLM-specific optimizations when applying virtual memory and paging ideas, such as smarter eviction policies, leveraging recomputation opportunities, and fusing GPU kernels.

- Designing schedulers and memory managers tailored for different decoding algorithms and models, building on top of the PagedAttention technique.

- Generalizing PagedAttention to work for other attention mechanisms beyond self-attention, such as various sparse attention methods.

- Developing techniques to reduce the overhead of memory indirection and non-contiguous memory access introduced by the paging approach.

- Applying and evaluating PagedAttention on very large models that require model parallelism across multiple GPUs/servers.

- Exploring how to combine PagedAttention with other complementary optimization techniques like those in Orca.

- Developing analytical models to enable resources provisioning and auto-scaling in LLM serving systems using PagedAttention.

In summary, the key directions are applying the paging ideas to other workloads, co-designing paging with LLM optimizations, reducing paging overheads, scaling it to very large models, and combining it with complementary techniques. The paper lays a solid foundation for many exciting avenues of future work in this domain.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the paper:

This paper proposes PagedAttention, a new attention algorithm for large language models (LLMs) that allows the key-value (KV) cache to be stored in non-contiguous, paged memory blocks. This is inspired by virtual memory and paging techniques in operating systems. Based on PagedAttention, the authors build vLLM, an LLM serving system that achieves near-zero waste in KV cache memory through block-level memory management. vLLM supports flexible sharing of KV cache within and across requests to further reduce memory usage. Evaluations on large LLMs like GPT-3 show that vLLM improves serving throughput by 2-4x compared to state-of-the-art systems like FasterTransformer and Orca, without affecting model accuracy. The improvements are more significant for longer sequences, larger models, and more complex decoding algorithms. The key ideas are managing KV cache in small fixed-size blocks that can be dynamically allocated like pages in virtual memory, and allowing the blocks to be shared across sequences. This resolves the fragmentation and redundancy issues in existing contiguous memory allocation schemes for KV cache.


## Summarize the paper in two paragraphs.

 Here is a 2 paragraph summary of the key points from the paper:

The paper proposes PagedAttention, a new attention algorithm that divides the key-value (KV) cache into fixed-size blocks that can be stored in non-contiguous memory, inspired by virtual memory and paging techniques in operating systems. This allows more flexible management of the dynamically growing and shrinking KV cache memory in large language model (LLM) serving, eliminating fragmentation waste and enabling sharing. Based on PagedAttention, the authors build vLLM, a distributed LLM serving engine. vLLM's memory manager maps logical KV cache blocks to physical GPU memory blocks on demand, avoiding pre-allocation of maximum sequence lengths. It supports complex decoding algorithms via block-level copy-on-write and enables memory sharing across sequences. When memory is scarce, full sequence KV caches can be swapped to CPU or recomputed later after eviction. Evaluations on various LLMs and workloads show vLLM improves throughput by 2-4x over previous systems like Orca and FasterTransformer, by more efficient batching. The gains are higher for longer sequences, larger models, and more advanced decoding methods.

In summary, the key ideas are:

1) PagedAttention allows dividing KV cache into fixed blocks that don't need contiguous storage, enabling flexible paging of the cache.

2) vLLM builds on top of PagedAttention, managing KV cache in blocks dynamically mapped to GPU memory like virtual memory pages. This increases sharing and reduces waste, improving throughput. More gains are seen for larger models, longer sequences, and advanced decoding methods.


## Summarize the main method used in the paper in one paragraph.

 Here is a one paragraph summary of the main method used in the paper:

The paper proposes PagedAttention, a new attention algorithm for large language models (LLMs) that allows the key-value (KV) cache to be stored in non-contiguous, paged memory blocks instead of requiring contiguous storage like existing systems. This is inspired by virtual memory and paging techniques in operating systems. The KV cache for each request is divided into fixed-size blocks that can be mapped to physical memory blocks in a flexible manner, avoiding fragmentation and enabling sharing across requests. Based on PagedAttention, the paper presents vLLM, a high-throughput LLM serving system. vLLM uses a block manager to map logical KV cache blocks to physical GPU memory blocks on the fly during decoding. This allows efficient batching of more concurrent requests by reducing waste in KV cache memory. vLLM also supports techniques like copy-on-write and swapping to handle complex decoding algorithms and variable length requests. Experiments show vLLM improves serving throughput over previous systems by 2-4x for various models and workloads.


## What problem or question is the paper addressing?

 The paper is addressing the problem of inefficient memory management for serving large language models (LLMs). Specifically, it identifies two key challenges:

1. Existing systems like Orca and FasterTransformer store the key-value cache (KV cache) for each request in contiguous GPU memory. This leads to fragmentation (both internal and external) as well as inability to share KV cache across requests. 

2. Complex decoding algorithms like beam search and parallel sampling require flexible sharing of KV cache across sequences, which is not supported by existing systems.

To solve these problems, the paper proposes PagedAttention, a novel attention algorithm inspired by virtual memory and paging in operating systems. PagedAttention allows storing the KV cache in non-contiguous blocks, enabling more efficient memory allocation and sharing. The paper then builds vLLM, a high-throughput LLM serving system utilizing PagedAttention and optimized memory management techniques. Experiments show vLLM improves throughput by 2-4x over state-of-the-art systems.

In summary, the key contribution is applying OS concepts like paging to efficiently manage the unique properties of LLM serving workloads, such as dynamic KV cache sizes and complex sharing patterns. This allows higher batch sizes and throughput while retaining low latency.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and concepts are:

- Large language models (LLMs): The paper focuses on serving and optimizing large Transformer-based language models like GPT and OPT.

- Key-value (KV) cache: The key and value tensors generated during the autoregressive decoding process that represent the contextual information needed for generating future tokens. Efficiently managing the KV cache memory is a core challenge. 

- Memory fragmentation: Existing systems suffer from internal and external fragmentation when allocating contiguous memory for the KV cache. This limits the batch size and throughput.

- PagedAttention: The proposed attention algorithm that allows storing KV cache in non-contiguous blocks inspired by paging in operating systems. Enables more flexible memory management.

- vLLM: The LLM serving system built on PagedAttention. Uses techniques like block-level management, dynamic allocation, and copy-on-write to efficiently handle KV cache memory.

- Throughput: The key performance metric that vLLM aims to optimize by allowing more requests to be batched and processed in parallel.

- Decoding algorithms: vLLM supports various decoding methods like greedy decoding, beam search, parallel sampling through flexible memory management.

In summary, the key focus is developing memory management techniques like PagedAttention and vLLM to improve the throughput of serving large language models by efficiently handling the dynamically growing KV cache memory.
