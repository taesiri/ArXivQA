# [A Theoretical Explanation for Perplexing Behaviors of   Backpropagation-based Visualizations](https://arxiv.org/abs/1805.07039)

## What is the central research question or hypothesis that this paper addresses?

 This paper appears to address the following central research question:

What is the theoretical explanation for why certain backpropagation-based visualizations like Guided Backpropagation (GBP) and Deconvolutional Networks (DeconvNet) generate more visually pleasing but less class-sensitive visualizations compared to plain saliency maps?

The key points seem to be:

- The paper provides a theoretical analysis to explain the behaviors of different backpropagation-based visualizations like saliency maps, GBP, and DeconvNet. 

- It reveals that GBP and DeconvNet are essentially doing partial image recovery rather than highlighting class-relevant pixels or visualizing learned weights. This means they are not directly related to the actual decision-making in neural networks.

- The analysis shows that the backward ReLU introduced in GBP/DeconvNet along with the local connections in CNNs are the main reasons why GBP/DeconvNet generate crisper visualizations compared to saliency maps. 

- But this comes at the cost of reduced class-sensitivity. GBP/DeconvNet visualizations remain largely the same irrespective of the class, while saliency maps change more noticeably.

- The theory and analysis are supported through extensive experiments on CNNs with different architectures and weights (random vs trained).

In summary, the paper provides a theoretical lens to understand and explain the behaviors of popular backpropagation-based visualizations, revealing their limitations in terms of reflecting actual model decisions while explaining their visually appealing nature.


## What is the main contribution of this paper?

 This paper provides a theoretical analysis of backpropagation-based visualization methods for convolutional neural networks (CNNs), with a focus on explaining why methods like Guided Backpropagation (GBP) and Deconvolutional Networks (DeconvNet) produce visually appealing but class-insensitive visualizations compared to simpler methods like saliency maps. The key contributions are:

- They formally analyze GBP, DeconvNet and saliency maps in a simplified 3-layer CNN model with random weights, showing that GBP approximates the input image while saliency maps and DeconvNet behave like random noise. This demonstrates the importance of the backward pass ReLU for image recovery in GBP.

- They extend the analysis to CNNs with max-pooling layers, showing that the pooling operation allows DeconvNet to also approximate image recovery like GBP, explaining its improved visualizations.

- For trained CNNs, they argue GBP and DeconvNet approximate "partial image recovery", where trained weights determine which image patches form an "active path". This explains why they highlight interpretable features like edges consistently across images. 

- Their analysis reveals the backward pass ReLU and local connectivity are key for the visually appealing GBP/DeconvNet visualizations, and that these methods do not highlight class-specific features unlike saliency maps.

- They support the theoretical results with experiments on simple CNNs and VGG networks, showing GBP quality improves with more filters and DeconvNet benefits from max-pooling. Adversarial examples also confirm GBP/DeconvNet are insensitive to model decisions.

Overall, the paper provides a theoretical lens to understand and explain the behaviors of these visualization techniques, highlighting their limitations in showing meaningful class-specific explanations.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence TL;DR of the paper:

The paper provides a theoretical explanation that guided backpropagation (GBP) and deconvolutional networks (DeconvNet), two popular visualization techniques for convolutional neural networks, are actually doing partial image recovery rather than highlighting class-relevant pixels, which means they are essentially unrelated to the network's decision making process.

In short, the paper shows through theory and experiments that GBP and DeconvNet produce visually appealing results by reconstructing parts of the input image, not by revealing insights about the model.


## How does this paper compare to other research in the same field?

 This paper provides a theoretical explanation for why certain backpropagation-based visualizations like Guided Backpropagation (GBP) and Deconvolutional Networks (DeconvNet) generate visually appealing but class-insensitive visualizations, while other methods like saliency maps are noisier but more class-sensitive. The key contributions are:

- Mathematically showing that GBP and DeconvNet rely on backward ReLUs and local connections in CNNs to perform partial image recovery, rather than highlighting class-relevant features. This explains their good visual quality but lack of class-sensitivity.

- Proving that with enough filters/units, GBP can recover the input image in random shallow CNNs regardless of class, contradicting the common "visualizing learned weights" view based on linear models.

- Showing DeconvNet relies on max-pooling to enable image recovery, explaining why it recovers less perfectly than GBP.

- Analyzing how trained weights bias GBP/DeconvNet to recover "active" patches while filtering out others, losing class-relevance. 

This provides a more rigorous nonlinear model-based understanding compared to prior intuitions based on linear approximations. The analysis also concretely pinpoints architectural properties like local connectivity and backward ReLUs as causes of such behaviors.

Other related works have empirically observed the lack of class-sensitivity in GBP/DeconvNet compared to saliency maps. But this paper offers a theoretical justification rooted in the method formulations and network architectures. The insights on image recovery being unrelated to model decisions are also novel and insightful. Overall, this provides a solid theoretical foundation to interpret behaviors of these widely used visualization methods.

\section{Conclusions}
\label{others}

In this paper, we proposed a theoretical explanation for backpropagation-based visualizations, where we started from a random three-layer CNN and later generalized it to more realistic cases. 
% Different from the approximate linear model explanation,
We showed that unlike saliency map, both GBP and DeconvNet are essentially doing (partial) image recovery, which verified their class-insensitive properties. We revealed that it is the backward ReLU, used by both GBP and DeconvNet, along with the local connections in CNNs that is responsible for human-interpretable visualizations. We also explained how DeconvNet also relies on the max-pooling to recover the input. Our analysis was supported by extensive experiments. 
Finally, we hope our analysis can 
provide useful insights into developing better visualization methods for deep neural networks. A future direction is to understand how the GBP visualizations in the trained CNNs filter out image patches layer by layer.

% In this paper, we gave a theoretical explanation for backpropagation-based visualizations in deep neural networks. We revealed that it is the local connections in the CNNs that result in human interpretable visualizations no matter the network has been trained or not. Our results suggested that backpropagation-based visualization methods convey little information about the value of weights and the importance of pixels to a specific class. Finally, our experiments verified theoretical analysis.



% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
% \nocite{langley00}


\section*{Acknowledgements}

Thanks to the anonymous reviewers for useful comments. WN, YZ and AB were supported by IARPA via DoI/IBC contract D16PC00003. Also, we thank Leon Sixt for pointing out an error in the proof of Theorem 1. 


\bibliography{GBP_draft}
\bibliographystyle{icml2018}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% DELETE THIS PART. DO NOT PLACE CONTENT AFTER THE REFERENCES!
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\newpage

\section*{\Large Appendix}

\vspace{3mm}

\appendix 
% \renewcommand\thesection{\Alph{section}}
% \def\thesection{\Alph{section}}

\vspace{-2mm}
\section{Proof of Lemma 1}

\emph{Proof:} Saliency map includes only forward ReLUs without backward ReLUs, whereas DeconvNet includes only backward ReLUs without forward ReLUs. GBP has both types of ReLUs. 
Also, the norm of all the visualization results will be normalized to be in the range of $[0,1]$.
Thus, by taking the (modified) derivative of $f_k(x)$ in Eq. (3) with respect to $x$ and applying the proper normalization, these backpropagation-based visualizations for the $k$-th logit can be unified as
\begin{align} \label{der_sk}
\begin{split}
& s_k(x) \mathop  = \limits^{\left( a \right)} \frac{1}{Z_k} \sum_{i=1}^{N}\sum_{j=1}^{J}{h({V_{q_{ij},k}})\frac{\partial}{\partial x}{g({w^{(i)T}} y^{(j)})}} \\
	&\mathop  = \limits^{\left( b \right)} \frac{1}{Z_k} \sum_{j=1}^{J} {D_j}^T \sum_{i=1}^{N} {h({V_{q_{ij},k}}) \frac{\partial}{\partial y^{(j)}} g({w^{(i)T}} y^{(j)})} \\
	&\mathop = \limits^{(c)} \frac{1}{Z_k} \sum_{j=1}^{J} {D_j}^T \sum_{i=1}^{N} {h({V_{q_{ij},k}}) \tilde{w}^{(i,j)}}
\end{split}
\end{align}
where $Z_k$ is the normalization coefficient to ensure $\|s_k(x)\| \in [0,1]$, $(a)$ follows from the formal definitions of backpropagation-based visualization for a ReLU activation in Eq. (1) with $h(\cdot), g(\cdot)$ being given by Eq. (2), $(b)$ is from applying $y^{(j)} = D_j x$ and swapping the two sums, 
and $(c)$ is from taking the derivative of $g(\cdot)$ in the three cases with
\begin{align*} 
    \tilde{w}^{(i,j)} = \begin{cases}
    w^{(i)} & \text{for DeconvNet}\\
    w^{(i)}\mathbb{I}\left({w^{(i)T}} y^{(j)}\right) & \text{for saliency map and GBP} 
\end{cases}
\end{align*}
as required.
% For a random neural network where every entry of both $V$ and $W$ is assumed to be independently Gaussian distributed with zero mean and variance $c^2$, i.e. 
% $V_{q_{ij},k}, W_{t,i} \sim \mathcal{N}(0, c^2) \text{ }\forall i \in [N], j \in [J], t \in [p]$. Assuming the number of filters $N$ is sufficiently large (e.g. VGG-16 net usually has $N=256$), then we have 
% \begin{align*} %\label{estmate_sk}
% \begin{split}
% 	s_k(x) & \mathop  \approx \limits^{\left( a \right)} \frac{1}{Z_k} \sum_{j=1}^{J} {D_j}^T \mathbb{E}\left[ {h({V_{q_{ij},k}}) \tilde{w}^{(i,j)}} \right] \\
% 	& \mathop  = \limits^{\left( b \right)} \frac{1}{Z_k} \sum_{j=1}^{J} {D_j}^T \mathbb{E}\left[ h({V_{q_{ij},k}}) \right] \mathbb{E} \left[ { \tilde{w}^{(i,j)}} \right]
% \end{split}
% \end{align*}
% where $(a)$ follows from the asymptotic approximation of sample mean to the expectation and $(b)$ follows from the fact that $V_{q_{ij},k}$ and $w^{(i)}$ are independent. 
\hfill $\square$






\vspace{-2mm}
\section{Proof of Theorem 1}


\emph{Proof:}
In a random neural network where every entry of both $V$ and $W$ is assumed to be independently Gaussian distributed with a zero mean and variance $c^2$, we have 
$V_{q_{ij},k} \sim \mathcal{N}(0, c^2)$ and $w^{(i)} \sim \mathcal{N}(0, c^2 I)$ $\forall i \in \{1, \cdots, N\}, j \in \{1,\cdots,J\}$. 
For GBP, in order to ensure $\|s_k(x)\| \in [0,1]$ we first set $Z_k = \tilde{Z}_k N$.
Assuming the number of filters $N$ is sufficiently large (e.g. VGG-16 net usually has $N=256$), then from Eq. (\ref{der_sk}) we have 
\begin{align*} %\label{estmate_sk}
\begin{split}
	s_k(x) & = \frac{1}{\tilde{Z}_k} \sum_{j=1}^{J} {D_j}^T \frac{1}{N} \sum_{i=1}^{N} {h({V_{q_{ij},k}}) \tilde{w}^{(i,j)}} \\
	& \mathop  \approx \limits^{\left( a \right)} \frac{1}{\tilde{Z}_k} \sum_{j=1}^{J} {D_j}^T \mathbb{E}\left[ {h({V_{q_{ij},k}}) \tilde{w}^{(i,j)}} \right] \\
	& \mathop  = \limits^{\left( b \right)} \frac{1}{\tilde{Z}_k} \sum_{j=1}^{J} {D_j}^T \mathbb{E}\left[ h({V_{q_{ij},k}}) \right] \mathbb{E} \left[ { \tilde{w}^{(i,j)}} \right]
\end{split}
\end{align*}
where $(a)$ follows from the asymptotic approximation of sample mean to the expectation and $(b)$ follows from the fact that $V_{q_{ij},k}$ and $w^{(i)}$ are independent. 

For GBP, we have $h({V_{q_{ij},k}}) = \sigma({V_{q_{ij},k}})$. Since we know ${V_{q_{ij},k}} \sim \mathcal{N} (0, c^2)$, then $h({V_{q_{ij},k}})$ follows one-dimensional rectified Gaussian distribution, and by its definition we can easily get $\mathbb{E}\left[ h({V_{q_{ij},k}}) \right] = \sqrt{\frac{1}{2\pi}}c$. Also, from the definition of $\tilde{w}^{(i,j)}$ for GBP, we know $\tilde{w}^{(i,j)}$ follows a $p$-dimensional rectified Gaussian distribution and its p.d.f. is 
% \begin{align} \label{w_pdf}
%     p(w) = \begin{cases}
%     \frac{1}{{(2\pi c^2)}^{\frac{p}{2}}} e^{-\frac{w^T w}{2c^2}} + \frac{1}{2} \delta({y^{(j)T}} w), & \text{if} \;\; {y^{(j)T}} w \geq 0 \\
%     0, & \text{otherwise}
%     \end{cases}
% \end{align}
\begin{align} \label{w_pdf}
   p(w) = \frac{1}{{(2\pi c^2)}^{\frac{p}{2}}} e^{-\frac{w^T w}{2c^2}} u({y^{(j)T}} w) + \frac{1}{2} p(w_A) \delta_A(w)
\end{align}
where the manifold $A \triangleq \{ w | {y^{(j)T}} w = 0 \}$, $p(w_A)$ is a $(p-1)$-dimensional Gaussian distribution projected from the $p$-dimensional Gaussian distribution onto the manifold $A$, with $w_A \in \mathbb{R}^{p-1}$ being the corresponding projected vector of $w$, and $u(t)$ and $\delta_A(w)$ are the unit step function and dirac delta function, respectively, i.e., 
\begin{align*}
    u(t) = \begin{cases} 1, & t > 0 \\ 0, & t \leq 0 \end{cases} \quad \text{and} \quad 
    \delta_A(w) = \begin{cases} +\infty, & w \in A \\ 0, & w \notin A \end{cases}
\end{align*}
By definition, we have 
\begin{align*}
    \int p(w_A) \delta_A(w) dw \triangleq \int p(w_A) dw_A = 1
\end{align*}
such that it satisfies $\int p(w) dw = 1$.
Accordingly, its expectation is given by
\begin{align} \label{w_exp}
\begin{split}
& \mathbb{E} \left[ { \tilde{w}^{(i,j)}} \right] \\
&= \int_{{y^{(j)T}} w > 0} \frac{w}{{(2\pi c^2)}^{\frac{p}{2}}} e^{-\frac{w^T w}{2c^2}} dw  + \int \frac{w}{2} p(w_A) \delta_A(w) dw \\
& \mathop = \limits^{(a)}  \int_{\phi_p > 0} U \phi \cdot \frac{1}{{(2\pi c^2)}^{\frac{p}{2}}} e^{-\frac{\phi^T \phi}{2c^2}} |U| d \phi \\
& \qquad\qquad + \int \frac{1}{2} U\phi p(\phi_{A_p}) \delta_{A_p}(\phi) |U| d \phi\\
& \mathop = \limits^{(b)}  U \int_{\phi_p > 0} \phi \cdot \frac{1}{{(2\pi c^2)}^{\frac{p}{2}}} e^{-\frac{\phi^T \phi}{2c^2}} d \phi \\
& \qquad\qquad + \frac{1}{2} U \int \phi p(\phi_{A_p}) \delta_{A_p}(\phi) d \phi \\
% & \mathop = \limits^{(b)} 
%\sqrt{\frac{2}{\pi}}c \cdot U^T e_n \\
%& \mathop = \limits^{(c)} 
% \sqrt{\frac{2}{\pi}}c \cdot \frac{y^{(j)}}{\| y^{(j)}\|_2}
\end{split}
\end{align}
where $(a)$ follows from the change of variables $w= U \phi$ and $U$ is an unitary matrix satisfying the condition that $U^T \cdot \frac{y^{(j)}}{\| y^{(j)}\|_2} = e^{(p)}$ and $e^{(p)}$ is an unit vector with only the $p$-th entry being 1. That is, $\frac{y^{(j)}}{\| y^{(j)}\|_2}$ is the $p$-th column of $U$. Thus, ${y^{(j)T}} w = {y^{(j)T}} U \phi = e^{(p)T} \phi {\| y^{(j)}\|_2} = \phi_p {\| y^{(j)}\|_2}$ with $\phi_p$ being the $p$-th entry of $\phi$, which means ${y^{(j)T}} w > 0$ is equivalent to $\phi_p > 0$. Also, by the change of variables in the integral, we have $dw = |U| d \phi$ where $|\cdot|$ denotes the determinant of a matrix. Accordingly, we define a new manifold $A_p = \{\phi | \phi_p = 0\}$.
$(b)$ follows from $|U| = 1$ by the definition of an unitary matrix, and the swap between matrix multiplication and the integral. 
% Also, the second term with the dirac delta function vanishes as 

As $\phi$ is a $p$-dimensional vector, the first integral above can be evaluated at each entry, denoted by $\phi_m$, of $\phi$ separately. For $m \neq p$, we have
\begin{align*}
    \begin{split}
        & \int_{\phi_p > 0} \phi_m \cdot \frac{1}{{(2\pi c^2)}^{\frac{p}{2}}} e^{-\frac{\phi^T \phi}{2c^2}} d \phi \\
        & \mathop = \limits^{(a)} \underbrace{\int_{-\infty}^{\infty} \frac{\phi_m}{{(2\pi c^2)}^{\frac{1}{2}}} e^{-\frac{\phi_m^2}{2c^2}} d \phi_m}_{0}  \cdot \int_{0}^{\infty}  \frac{1}{{(2\pi c^2)}^{\frac{1}{2}}} e^{-\frac{\phi_p^2}{2c^2}} d \phi_p \\
        &= 0
    \end{split}
\end{align*}
where $(a)$ follows from the expansion of the multiple integral, and all of the other $p-2$ integrals over $\phi_k$ for $k \notin \{p,m\}$ are 1. For $m=p$, we have
\begin{align*}
    \begin{split}
       


## What future research directions do the authors suggest?

 Based on my reading of the paper, here are some potential future research directions the authors suggest:

- Developing visualization methods that can better highlight class-relevant pixels and convey information about network decisions, rather than just recovering the input image. The authors show limitations with current methods like GBP and DeconvNet in this regard.

- Further analyzing and explaining how trained convolutional layers in CNNs create "dead zones" to filter out certain image patches in visualizations like GBP. The authors provide some initial analysis but say further work is needed to fully understand this process.

- Extending the theoretical analysis to other network architectures beyond CNNs, such as ResNet where the authors observe some deterioration in visualization quality possibly due to skip connections.

- Using the insights from this theoretical analysis to help develop new, improved visualization methods that avoid the pitfalls identified. The authors' framework provides guidance on properties to aim for.

- Exploring whether the standards and analysis framework proposed in this work could be applicable for analyzing and developing other interpretability methods beyond just backpropagation-based visualizations.

- Conducting human studies to empirically evaluate how well different visualization methods help people understand and interpret model decisions. The authors focus on theoretical analysis but suggest human studies could complement this.

- Applying the theoretical framework to analyze visualizations for other tasks beyond image classification, such as visualizing neural networks for language tasks.

So in summary, the authors lay a theoretical foundation and identify limitations of current methods, while suggesting this could guide developing new improved visualizations and extending the analysis to other areas as interesting future work.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the key points from the paper:

The paper proposes a theoretical explanation for why guided backpropagation (GBP) and deconvolutional networks (DeconvNet) generate more visually compelling but less class-sensitive visualizations compared to saliency maps. Through mathematical analysis on random and trained convolutional neural networks (CNNs), the authors reveal that GBP and DeconvNet rely on the backward ReLU activation and local connections in CNNs to perform partial image recovery, which is unrelated to the network's actual decision making. In contrast, saliency maps use the true gradient and thus highlight class-relevant pixels. Experiments on shallow and deep CNNs support the theoretical results, showing GBP and DeconvNet visualizations are insensitive to different class labels. Overall, the work provides new theoretical insights into popular backpropagation-based visualization techniques, explaining their limitations in faithfully representing model decisions despite producing seemingly interpretable visualizations.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the key points from the paper:

The paper proposes a theoretical explanation for why certain backpropagation-based visualizations like Guided Backpropagation (GBP) and Deconvolutional Networks (DeconvNet) generate visually appealing but class-insensitive visualizations compared to simpler methods like saliency maps. The key insights are:

First, the backward ReLU operation introduced in GBP and DeconvNet, combined with the local connections in CNNs, enables these methods to approximately recover parts of the input image rather than highlighting class-relevant pixels. This explains why they produce sharper visualizations but lose sensitivity to the class. In contrast, saliency maps directly visualize the gradient and remain class-sensitive. 

Second, DeconvNet relies on max-pooling layers to recover more accurate visualizations, while this is not needed for GBP. The paper proves these claims theoretically for shallow CNNs, and extends the analysis to deeper networks. Overall, it reveals that GBP and DeconvNet are essentially doing partial image recovery unrelated to model decisions, despite their visually compelling results. Experiments support the theory and highlight other properties like DeconvNet's texture-like artifacts.


## Summarize the main method used in the paper in one paragraph.

 Here is a summary of the main method used in the paper in one paragraph:

The paper proposes a new approach for visualizing and interpreting decisions from convolutional neural networks (CNNs). The key idea is to propagate the class-specific gradient signal back to the input pixel space through all the layers in the CNN. Specifically, the gradient of the class score with respect to the input image is computed using standard backpropagation. This gradient signal is then projected back onto the input pixel space by sequentially applying the transpose of the convolutional and pooling operators. To reduce noise and improve visual quality, the authors employ several techniques including guided backpropagation which masks out negative entries during backpropagation by thresholding non-positive activations to zero. The resulting saliency map highlights the pixels in the input image that have the most influence on the CNN's output for the predicted class. By visualizing these class-discriminative pixels, the method aims to provide insights into how CNNs reach their decisions.

In summary, the paper introduces a backpropagation-based approach to generate saliency maps that highlight class-relevant pixels in the input image. The core idea is to backpropagate the class-specific gradient signal to the input space for visualization. Additional techniques like guided backpropagation are used to reduce noise and improve visual quality. The method helps shed light on which parts of the input are most important for CNN's predictions.

\section{Experiments}
\label{experim}

% \subsection{CNNs with Random Weights}

To verify our theoretical analysis, we conduct a series of experiments on a three-layer CNN, a three-layer fully-connected network (FCN) and a VGG-16 net. For a random network, their weights are all sampled from the truncated Gaussians with a zero-mean and standard deviation $0.1$. Unless stated otherwise, the input is the image ``tabby'' from the ImageNet dataset \cite{deng2009imagenet} with size $224 \times 224 \times 3$. See the supplementary materials for more  results on other images and other neural network such as ResNet \cite{he2016deep}. In the three-layer CNN, the filter size is $7 \times 7 \times 3$, the number of filters is $N=256$, and the stride is $2$. In the three-layer FCN, the hidden layer size is set to $N_h=4096$. By default, the backpropagation-based visualizations are calculated with respect to the maximum class logit. 


\begin{figure}[!t]
	\centering
% 		\begin{subfigure}[b]{0.15\textwidth}
% 			\centering
% 			\includegraphics[width=\textwidth]{tabby.png}
% 			\caption{\footnotesize tabby}
% 		\end{subfigure}
		\centering
		\begin{subfigure}[b]{0.11\textwidth}
			\centering
			\includegraphics[width=\textwidth]{GuidedBackprop_maxlogit_tabby_convnet_filter7by7.pdf}
			\caption*{\footnotesize GBP-CNN}
		\end{subfigure}
		\begin{subfigure}[b]{0.11\textwidth}
			\centering
			\includegraphics[width=\textwidth]{Deconv_maxlogit_tabby.pdf}
			\caption*{\footnotesize Deconv-CNN}
		\end{subfigure}
		\begin{subfigure}[b]{0.11\textwidth}
			\centering
			\includegraphics[width=\textwidth]{PlainSaliency_maxlogit_tabby_convnet_filter7by7.pdf}
			\caption*{\footnotesize Sal-CNN}
		\end{subfigure}
		
		\centering
		\begin{subfigure}[b]{0.11\textwidth}
			\centering
			\includegraphics[width=\textwidth]{GuidedBackprop_maxlogit_tabby_fc_4096.pdf}
			\caption*{\footnotesize GBP-FCN}
		\end{subfigure}
		\begin{subfigure}[b]{0.11\textwidth}
			\centering
			\includegraphics[width=\textwidth]{DeconvNet_maxlogit_tabby_layers1.pdf}
			\caption*{\footnotesize Deconv-FCN}
		\end{subfigure}
		\begin{subfigure}[b]{0.11\textwidth}
			\centering
			\includegraphics[width=\textwidth]{PlainSaliency_maxlogit_tabby_fc_4096.pdf}
			\caption*{\footnotesize Sal-FCN}
		\end{subfigure}
	\caption{Backpropagation-based visualizations in  a random three-layer CNN (top row) and a random three-layer FCN (bottom row) given the input image ``tabby''. From left to right, each column represents GBP, DeconvNet and saliency map, respectively. Only GBP visualization in the CNN is human-interpretable.}\label{verify_cnn_fcn}
\end{figure}

\subsection{Impact of Local Connections}

Figure \ref{verify_cnn_fcn} shows the backpropagation-based visualizations on a random three-layer CNN and a random three-layer FCN, respectively. We can see only GBP in the CNN can produce a human-interpretable visualization, while DeconvNet and saliency map in the CNN get random noise, which verifies our theoretical analysis in the section \ref{three_layer_cnn}. In contrast, as local connections do not exist in the FCN and the input size (e.g. $224\times 224\times3$) is extremely large, all the backpropagation-based methods (including GBP) in the FCN generate random noise. Particularly for GBP, the number of hidden neurons $N_h=4096$ is still not large enough to 
% even nosily 
recover the image.

\begin{figure}[!t]
	\centering
	\begin{subfigure}[b]{0.11\textwidth}
		\centering
		\includegraphics[width=\textwidth]{GuidedBackprop_maxlogit_tabby_convnet_N8.pdf}
		\caption*{\footnotesize $N=8$}
	\end{subfigure}
	\centering
	\begin{subfigure}[b]{0.11\textwidth}
		\centering
		\includegraphics[width=\textwidth]{GuidedBackprop_maxlogit_tabby_convnet_N16.pdf}
		\caption*{\footnotesize $N=16$}
	\end{subfigure}
	\centering
	\begin{subfigure}[b]{0.11\textwidth}
		\centering
		\includegraphics[width=\textwidth]{GuidedBackprop_maxlogit_tabby_convnet_N32.pdf}
		\caption*{\footnotesize $N=32$}
	\end{subfigure}
	\begin{subfigure}[b]{0.11\textwidth}
		\centering
		\includegraphics[width=\textwidth]{GuidedBackprop_maxlogit_tabby_convnet_N64.pdf}
		\caption*{\footnotesize $N=64$}
	\end{subfigure}
% 	\begin{subfigure}[b]{0.11\textwidth}
% 		\centering
% 		\includegraphics[width=\textwidth]{GuidedBackprop_maxlogit_tabby_convnet_N128.png}
% 		\caption*{\footnotesize $N=128$}
% 	\end{subfigure}

    \begin{subfigure}[b]{0.11\textwidth}
		\centering
		\includegraphics[width=\textwidth]{GuidedBackprop_maxlogit_tabby_fc_64input_5000.pdf}
		\caption*{\footnotesize $N_h=5000$}
	\end{subfigure}
	\centering
	\begin{subfigure}[b]{0.11\textwidth}
		\centering
		\includegraphics[width=\textwidth]{GuidedBackprop_maxlogit_tabby_fc_64input_10000.pdf}
		\caption*{\footnotesize $N_h=10000$}
	\end{subfigure}
	\centering
	\begin{subfigure}[b]{0.11\textwidth}
		\centering
		\includegraphics[width=\textwidth]{GuidedBackprop_maxlogit_tabby_fc_64input_40000.pdf}
		\caption*{\footnotesize $N_h=40000$}
	\end{subfigure}
	\begin{subfigure}[b]{0.11\textwidth}
		\centering
		\includegraphics[width=\textwidth]{GuidedBackprop_maxlogit_tabby_fc_64input_70000.pdf}
		\caption*{\footnotesize $N_h=70000$}
	\end{subfigure}
	\caption{GBP visualizations given the input image ``tabby'' in a three-layer CNN (top row) by varying the number of filters $N$ and in a three-layer FCN (bottom row) by varying the number of hidden neurons $N_h$. }\label{gbp_fi}
\end{figure}


To further highlight the impact of local connections in the visual quality of GBP, we vary the number of filters $N$ in the CNN and the number of hidden neurons $N_h$ in the FCN, respectively, while keep other parameters fixed. The results are given in Figure \ref{gbp_fi}. Note that in the FCN, we have downsampled the input image to be of size $64 \times 64 \times 3$ due to computational limitations. We can see that as the number of filters $N$ increases (resp. the hidden layer size $N_h$), the visual quality of GBP in the CNN (resp. in the FCN) becomes better. Interestingly, even by setting $N_h=70000$, which is definitely unrealistic, the FCN cannot achieve a comparable performance to the CNN with $N=64$. Therefore, it confirms that the local connections in the CNN really contribute to the good visual quality of GBP. 

% Besides, we conduct two more experiments for GBP visualizations as follows: In the CNN, we change the number of filters $N$ while keeping other parameters fixed. The GBP visualizations are shown in Figure \ref{gbp_fi} (a).  In the FCN, we downsample the input image to be of size $64 \times 64 \times 3$ and increase the hidden layer size $N_h$. The GBP visualizations are shown in Figure \ref{gbp_fi} (b). We observe that, as the number of filters $N$ (resp. the hidden layer size $N_h$), the GBP visualizations for the CNN (resp. the FCN) become cleaner. These phenomena can be well explained by our theoretical analysis, which states that the either $N$ or $N_h$ should be large enough to get a clean GBP. Note that even by setting $N_h=70000$, which is definitely unrealistic, the FCN cannot achieve a comparable performance to the CNN with $N=16$. Therefore, it is the architectural property of CNNs - \textit{local connections} to be more precise - that really contributes to human interpretable visualizations via GBP or DeconvNet. 

\subsection{Impact of Max-Pooling and Network Depth}

To show the impact of the max-pooling in backpropagation-based visualizations, we then add a max-pooling layer in the above random three-layer CNN while keeping other parameters fixed, and the results are given in Figure \ref{fig_max_pool} (top row). As compared with the visualizations in Figure \ref{verify_cnn_fcn} (top row), neither GBP or saliency map is impacted by the max-pooling, whereas the DeconvNet visualization has now become human interpretable instead of being the random noise as before. It confirms that the max-pooling is critical in helping DeconvNet produce human-interpretable visualizations via image recovery, as predicted by our theoretical analysis in the section \ref{theory_maxpool}. 
% \yang{maybe add a reference pointing back to the exact position in section 3}







% \subsubsection{Random VGG}


% \begin{figure}[!t]
%     \centering
%     \begin{subfigure}[b]{0.11\textwidth}
% 		\centering
% 		\includegraphics[width=\textwidth]{GuidedBackprop_maxlogit_tabby_untrained.png}
% 		\caption*{\footnotesize GBP}
% 	\end{subfigure}
%     \begin{subfigure}[b]{0.11\textwidth}
% 		\centering
% 		\includegraphics[width=\textwidth]{figures/deconv/random/tabby_fc3_0.png}
% 		\caption*{\footnotesize DeconvNet}
% 	\end{subfigure}
%     \begin{subfigure}[b]{0.11\textwidth}
% 		\centering
% 		\includegraphics[width=\textwidth]{figures/saliency_regular_norm/_24.png}
% 		\caption*{\footnotesize Saliency map}
% 	\end{subfigure}
	
% 	\caption{Backpropagation-based visualizations for the untrained VGG-16 net given an input ``tabby''. From the top row to the last row, it is saliency map, DeconvNet and GBP respectively, where ``max'' refers to computing the (modified) gradient for the maximum class logit and the number, say ``731'', refers to computing the (modified) gradient for the $731$-th logit.  These numbers are randomly chosen for generality. Best viewed in the electronic version.}\label{fig_random}
% \end{figure}

To show the impact of network depth, we also apply backpropagation-based visualizations in a random VGG-16 net, which also includes the max-pooling but is much deeper than the three-layer CNN. Figure \ref{fig_max_pool} (bottom row) shows that only saliency map generates random noise while both GBP and DeconvNet could produce human-interpretable visualizations. Though there are subtle visual differences between the top row and bottom row of Figure \ref{fig_max_pool}, the behaviors of backpropagation-based methods are basically unchanged after increasing the network depth. In addition, both GBP and DeconvNet reconstruct every fine-grained detail of the input image in the random VGG , which is different from the trained VGG in Figure \ref{pre_vgg_vis} where only those ``active'' image patches are preserved. 

% Before that, we also describe another interesting experimental observation and provide more intuitions for the following theoretical explanations. When we apply these backpropagation-based methods to an untrained VGG-16 net with weights being randomly initialized from the truncated normal distribution, only saliency map produces random noise while both GBP and DeconvNet could still generate good-looking images, as shown in Figure \ref{fig_random}.
% Different from the well-trained case, both GBP and DeconvNet applied in the untrained case are trying to reconstruct every fine-grained detail of the input image, i.e. image recovery. As predicted by the linear approximation explanation, however, backpropagation-based methods are visualizing the weight vector in the untrained VGG-16 net, which thus should be random noise as the weights here are all randomly sampled from the Gaussian distribution. Therefore, the analysis based on linear models is not able to explain the these behaviors of GBP and DeconvNet, and might be misleading in some cases.




\begin{figure}[!t]
	\centering
	\begin{subfigure}[b]{0.11\textwidth}
		\centering
		\includegraphics[width=\textwidth]{figures/maxpool/tabby_GuidedBackprop_True_0.pdf}
		\caption*{\footnotesize GBP-pool}
	\end{subfigure}
	\begin{subfigure}[b]{0.11\textwidth}
		\centering
		\includegraphics[width=\textwidth]{figures/maxpool/tabby_Deconv_True_0.pdf}
		\caption*{\footnotesize Deconv-pool}
	\end{subfigure}
	\begin{subfigure}[b]{0.11\textwidth}
		\centering
		\includegraphics[width=\textwidth]{figures/maxpool/tabby_PlainSaliency_True_0.pdf}
		\caption*{\footnotesize Sal-pool}
	\end{subfigure}
	
	\begin{subfigure}[b]{0.11\textwidth}
		\centering
		\includegraphics[width=\textwidth]{figures/GuidedBackprop_maxlogit_tabby_untrained.pdf}
		\caption*{\footnotesize GBP-VGG}
	\end{subfigure}
    \begin{subfigure}[b]{0.11\textwidth}
		\centering
		\includegraphics[width=\textwidth]{figures/deconv/random/tabby_fc3_0.pdf}
		\caption*{\footnotesize Deconv-VGG}
	\end{subfigure}
    \begin{subfigure}[b]{0.11\textwidth}
		\centering
		\includegraphics[width=\textwidth]{figures/saliency_regular_norm/_24.pdf}
		\caption*{\footnotesize Sal-VGG}
	\end{subfigure}

	\caption{Backpropagation-based visualizations given the input image ``tabby'' in a random three layer CNN with the max-pooling (top row) and in a random VGG-16 net (bottom row). Now DeconvNet visualization also becomes human-interpretable.}\label{fig_max_pool}
\end{figure}











% \begin{figure}[!t]
% 	\centering


## What problem or question is the paper addressing?

 This paper is addressing the perplexing behaviors of backpropagation-based visualizations for interpreting convolutional neural networks (CNNs). Specifically, it aims to explain why methods like Guided Backpropagation (GBP) and Deconvolutional Networks (DeconvNet) generate cleaner and more visually interpretable results compared to plain gradient saliency maps, but are also less sensitive to the predicted class. 

The key questions the paper tries to address are:

1. Why do GBP and DeconvNet produce such visually compelling results compared to saliency maps, even though they essentially use the same backpropagation algorithm? 

2. What is the core difference in how GBP and DeconvNet treat the ReLU nonlinearity compared to saliency maps?

3. Why are GBP and DeconvNet not sensitive to the predicted class, even though the goal of these visualizations is to highlight pixels relevant for a specific class?

4. What information do GBP and DeconvNet visualizations actually convey, if not about class-specific importance? 

5. What causes the visually sharp results - is it the backward ReLU, local connections, or something else about CNN architectures?

So in summary, the paper tries to provide a theoretical framework to explain the unexpected behaviors of these visualization techniques, especially their inability to highlight class-relevant features despite producing seemingly interpretable visuals.


## What are the keywords or key terms associated with this paper?

 Based on my review, some of the key terms associated with this paper include:

- Backpropagation-based visualizations: The paper focuses on analyzing and explaining the behaviors of several backpropagation-based visualization techniques like saliency maps, guided backpropagation (GBP), and deconvolutional networks (DeconvNet). These methods aim to highlight important pixels in an input image by propagating signals back through a neural network.

- Class sensitivity: The paper finds that saliency maps tend to highlight pixels relevant to a specific class, while GBP and DeconvNet generate cleaner but less class-sensitive visualizations. 

- Image recovery: A key finding is that GBP and DeconvNet are doing partial image recovery rather than highlighting class-relevant pixels. Their visualizations are more driven by recovering the input image.

- Backward ReLU: The paper shows that introducing a backward ReLU in GBP and DeconvNet is a major cause of their improved visualizations compared to saliency maps. It modifies the true gradient.

- Local connections: CNN architectures have local connections between neurons in contrast to fully-connected networks. This is another architectural factor enabling GBP and DeconvNet to produce crisp visualizations.

- Max pooling: The paper explains how max pooling also contributes to DeconvNet's ability to recover visually plausible images.

So in summary, some key terms are backpropagation-based visualizations, class sensitivity, image recovery, backward ReLU, local connections, and max pooling. The interplay of these factors is analyzed to explain how and why different visualization methods behave as they do.


## Based on the paper, what questions should we ask to create a comprehensive summary of it? Please list at least 10 questions.

 Here are 10 questions I would ask to create a comprehensive summary of this paper:

1. What is the main research question or problem addressed in this paper? What gap in existing knowledge does it aim to fill?

2. What is the key hypothesis or claim made by the authors? 

3. What methodology did the authors use to test their hypothesis - e.g. experiments, simulations, theoretical analysis, etc.?

4. What were the main results or findings reported in the paper? Were the authors' hypotheses supported or refuted?

5. Did the paper introduce any new concepts, models, methods or techniques? If so, what are they and what is novel about them?

6. How do the results fit into the existing body of knowledge on this topic? Do they confirm, contradict or extend previous findings? 

7. What are the limitations of the study? Are there important caveats or shortcomings in the methodology, analysis or interpretation that need to be highlighted?

8. What conclusions did the authors draw based on their results? Are the conclusions justified given the study's limitations?

9. What are the key implications or significance of the results for theory, practice or policy in this field? 

10. Did the authors suggest any directions or ideas for future research to build on this study? What open questions remain?

By asking these types of questions, I would be able to analyze the key components of the paper - the background context, hypotheses, methods, results, conclusions and implications. My summary would assess how the study contributes to the field and highlights important insights as well as limitations. This approach helps create a concise yet comprehensive overview of the paper's findings and significance.


## Design 10 in-depth questions about method proposed in this paper and avoid simple question.

 Here are 10 potential in-depth questions about the method proposed in the paper:

1. The paper argues that GBP and DeconvNet are doing partial image recovery rather than highlighting class-relevant pixels. But couldn't some of the recovered image patches still be class-relevant? What evidence supports the claim that the recovered patches are not class-relevant?

2. The analysis shows GBP and DeconvNet rely on backward ReLU and local connections to produce good visualizations. But are backward ReLU and local connections sufficient to explain the visualizations, or are there other factors at play? 

3. How sensitive is the analysis to the assumption that weights are i.i.d. Gaussian? Would the conclusions still hold for other weight distributions?

4. The paper claims GBP and DeconvNet visualizations are unrelated to network decisions. But don't the recovered patches at least provide some information about which input patterns the network is sensitive to?

5. How well does the theoretical analysis match up with empirical results across a range of network architectures and datasets? Are there cases where the theory breaks down?

6. Could the partial image recovery be adapted in some way to make GBP and DeconvNet more class-relevant, or is the image recovery an inherent limitation?

7. The paper focuses on GBP and DeconvNet visualizations. To what extent does the analysis apply to other related methods like DeepLIFT and PatternNet? 

8. How does the filter size $p$ interact with the number of filters $N$ in controlling the image recovery? Is there an optimal ratio between them?

9. For DeconvNet, the paper claims the texture-like artifacts are due to imperfect recovery caused by max pooling. Is there any way to quantify or further analyze these artifacts? 

10. The paper claims saliency maps are more class-relevant because they use the true gradient. But are there ways to improve the class-relevance of GBP and DeconvNet while retaining their visual quality?


## Summarize the paper in one sentence.

 The paper provides a theoretical explanation for why guided backpropagation and deconvolutional networks generate more human-interpretable but less class-sensitive visualizations than saliency maps for convolutional neural networks.


## Summarize the paper in one paragraphs.

 Here is a one paragraph summary of the paper:

This paper provides a theoretical explanation for why guided backpropagation (GBP) and deconvolutional networks (DeconvNets) generate more visually appealing but less class-sensitive visualizations compared to saliency maps. The key findings are that GBP and DeconvNets rely on backward ReLUs to perform partial image recovery rather than highlighting pixels relevant for a specific class, making their visualizations unrelated to the network's decisions. The analysis starts with a random 3-layer CNN, showing GBP approximates the input image while saliency maps and DeconvNets yield noise. Extending to CNNs with max-pooling reveals it enables DeconvNets to reconstruct inputs like GBP. In trained CNNs, GBP and DeconvNet preserve "active" patches due to trained filters and combine them via fully-connected layers, recovering a filtered version of the input. Overall, the theory and experiments demonstrate GBP and DeconvNets focus on visually sharper image recovery over class-specific relevance.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 potential in-depth questions about the method proposed in this paper:

1. How does the proposed method build upon or differ from previous approaches for interpreting CNNs like saliency maps or deconvolution networks? What are the key novelties?

2. The paper claims the proposed method reveals that GBP and DeconvNet are doing partial image recovery. What evidence supports this claim? How does the theoretical analysis show these methods are unrelated to the actual network decisions?

3. One of the key factors identified is the use of a backward ReLU in GBP and DeconvNet. How does the backward ReLU specifically enable partial image recovery in the proposed framework? Why does this differ from a forward ReLU?

4. The paper identifies local connections in CNNs as another key factor in the visualizations produced by GBP and DeconvNet. How do local connections interact with the backward ReLU to produce the observed visualizations? 

5. How does the analysis extend from a simple 3-layer CNN to more complex CNN architectures like those with max-pooling or increased depth? What changes or stays the same in these extended analyses?

6. How well does the theoretical analysis align with the experimental results presented? Are there any cases where the theory does not fully explain what is observed?

7. For trained CNNs, the paper notes GBP and DeconvNet filter based on "active paths" determined by the trained weights. What causes these active paths to form and how do they shape the visualizations?

8. Are there any limitations or assumptions made in the theoretical derivations that could be addressed in future work? How could the analysis be expanded or improved?

9. Does this analysis provide any guidance on how to develop better methods for interpreting or visualizing CNNs? What next steps would you propose based on these findings?

10. How well does the overall framework of formal definitions, theoretical analysis, and experimental validation work for rigorously understanding how CNN visualization methods behave? Could this be a useful framework to apply to other interpretability techniques?


## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a high-quality paragraph summarizing the key points of the paper:

This paper provides a theoretical explanation for the perplexing behaviors of popular backpropagation-based visualizations like guided backpropagation (GBP) and deconvolutional networks (DeconvNet). While these methods generate visually compelling results, the authors show they are essentially doing partial image recovery unrelated to the network's actual decisions. Specifically, the paper reveals that the backward ReLU operation introduced in GBP and DeconvNet, along with the local connectivity inherent in CNNs, leads to crisp visualizations that lack class-sensitivity. In contrast, saliency maps based solely on forward propagation remain noisy but highlight class-relevant pixels. Through mathematical analysis on simplified CNN models and extensive experiments on VGG nets, the authors demonstrate GBP and DeconvNet recover the original input image regardless of the output class. Their theory neatly explains why GBP/DeconvNet appear more interpretable but are not indicative of the network's internal logic. Overall, this rigorous study advances our theoretical understanding of popular visualization techniques and their failure modes.
