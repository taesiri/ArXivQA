# A Theoretical Explanation for Perplexing Behaviors of   Backpropagation-based Visualizations

## What is the central research question or hypothesis that this paper addresses?

This paper appears to address the following central research question:What is the theoretical explanation for why certain backpropagation-based visualizations like Guided Backpropagation (GBP) and Deconvolutional Networks (DeconvNet) generate more visually pleasing but less class-sensitive visualizations compared to plain saliency maps?The key points seem to be:- The paper provides a theoretical analysis to explain the behaviors of different backpropagation-based visualizations like saliency maps, GBP, and DeconvNet. - It reveals that GBP and DeconvNet are essentially doing partial image recovery rather than highlighting class-relevant pixels or visualizing learned weights. This means they are not directly related to the actual decision-making in neural networks.- The analysis shows that the backward ReLU introduced in GBP/DeconvNet along with the local connections in CNNs are the main reasons why GBP/DeconvNet generate crisper visualizations compared to saliency maps. - But this comes at the cost of reduced class-sensitivity. GBP/DeconvNet visualizations remain largely the same irrespective of the class, while saliency maps change more noticeably.- The theory and analysis are supported through extensive experiments on CNNs with different architectures and weights (random vs trained).In summary, the paper provides a theoretical lens to understand and explain the behaviors of popular backpropagation-based visualizations, revealing their limitations in terms of reflecting actual model decisions while explaining their visually appealing nature.


## What is the main contribution of this paper?

This paper provides a theoretical analysis of backpropagation-based visualization methods for convolutional neural networks (CNNs), with a focus on explaining why methods like Guided Backpropagation (GBP) and Deconvolutional Networks (DeconvNet) produce visually appealing but class-insensitive visualizations compared to simpler methods like saliency maps. The key contributions are:- They formally analyze GBP, DeconvNet and saliency maps in a simplified 3-layer CNN model with random weights, showing that GBP approximates the input image while saliency maps and DeconvNet behave like random noise. This demonstrates the importance of the backward pass ReLU for image recovery in GBP.- They extend the analysis to CNNs with max-pooling layers, showing that the pooling operation allows DeconvNet to also approximate image recovery like GBP, explaining its improved visualizations.- For trained CNNs, they argue GBP and DeconvNet approximate "partial image recovery", where trained weights determine which image patches form an "active path". This explains why they highlight interpretable features like edges consistently across images. - Their analysis reveals the backward pass ReLU and local connectivity are key for the visually appealing GBP/DeconvNet visualizations, and that these methods do not highlight class-specific features unlike saliency maps.- They support the theoretical results with experiments on simple CNNs and VGG networks, showing GBP quality improves with more filters and DeconvNet benefits from max-pooling. Adversarial examples also confirm GBP/DeconvNet are insensitive to model decisions.Overall, the paper provides a theoretical lens to understand and explain the behaviors of these visualization techniques, highlighting their limitations in showing meaningful class-specific explanations.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence TL;DR of the paper:The paper provides a theoretical explanation that guided backpropagation (GBP) and deconvolutional networks (DeconvNet), two popular visualization techniques for convolutional neural networks, are actually doing partial image recovery rather than highlighting class-relevant pixels, which means they are essentially unrelated to the network's decision making process.In short, the paper shows through theory and experiments that GBP and DeconvNet produce visually appealing results by reconstructing parts of the input image, not by revealing insights about the model.
