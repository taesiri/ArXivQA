# [Universally Robust Graph Neural Networks by Preserving Neighbor   Similarity](https://arxiv.org/abs/2401.09754)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
Graph neural networks (GNNs) have shown great success in learning from graph-structured data. However, GNNs are vulnerable to adversarial attacks, especially graph structure attacks like adding/deleting edges. Prior work has studied this problem for GNNs on homophilic graphs, where connected nodes tend to be similar. However, GNN robustness on heterophilic graphs, where connected nodes tend to be dissimilar, is not well understood.  

Key Idea:
This paper provides both theoretical and empirical analysis showing that on heterophilic graphs, attackers tend to connect dissimilar nodes not based on the node features but based on the aggregated neighbor features. Leveraging this insight, the authors propose a new GNN model called Neighbor Similarity Preserving GNN (NSPGNN) that preserves similarity of aggregated neighbor features to defend against attacks.

Technical Contributions:
- Theoretically prove the attack loss is negatively correlated with similarity of powered aggregated neighbor features, explaining why attackers connect dissimilar nodes based on neighbor features
- Propose NSPGNN which constructs dual kNN graphs based on neighbor similarity and propagates features to preserve similarity
- Positive kNN graphs use low-pass filter to smooth features of high similarity nodes 
- Negative kNN graphs use high-pass filter to discriminate features of low similarity nodes
- Extensive experiments show NSPGNN outperforms state-of-the-art GNNs in robustness on both heterophilic and homophilic graphs under different attack methods

Key Benefits:
- Provides fundamental understanding of GNN vulnerability on heterophilic graphs 
- Novel neighbor similarity preservation approach for robust GNNs, effective on both heterophilic and homophilic graphs
- Significantly outperforms prior arts in adversarial robustness

The paper makes important theoretical and technical contributions towards robust graph representation learning under heterogeneity. The proposed NSPGNN model offers universal effectiveness and robustness.


## Summarize the paper in one sentence.

 Here is a one-sentence summary of the paper:

The paper proposes a novel graph neural network framework called NSPGNN that enhances robustness on both homophilic and heterophilic graphs by preserving neighbor feature similarity through contrastive learning on dual kNN graphs.


## What is the main contribution of this paper?

 According to the paper, the main contributions are threefold:

1. The paper theoretically and empirically illustrates that preserving the aggregated neighbors' similarity can detect potential malicious effects and universally enhance the adversarial robustness of GNNs, both on homophilic and heterophilic graphs. 

2. The paper proposes a universal robust GNN framework called NSPGNN that preserves neighbor similarity by introducing a dual-kNN graph pipeline to contrastively supervise neighbor-similarity-guided propagation.

3. NSPGNN competitively outperforms other baselines on both homophilic and heterophilic graphs under different adversarial environments. As a byproduct, preserving neighbor similarity also serves as a data augmentation technique to refine the structure of clean graphs for better semi-supervised node classification performance.

In summary, the key contribution is proposing the NSPGNN framework that can universally and significantly improve the robustness of GNNs on both homophilic and heterophilic graphs under adversarial attacks. This is achieved by preserving neighbor similarity to detect malicious effects in poisoned graphs.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and concepts are:

- Graph neural networks (GNNs)
- Adversarial attacks on graphs 
- Robustness of GNNs
- Homophilic vs heterophilic graphs
- Neighbor similarity preservation
- Dual kNN graph construction
- Low-pass and high-pass graph filters
- Semi-supervised node classification

The paper proposes a robust GNN model called NSPGNN that aims to preserve neighbor similarity in order to defend against adversarial attacks. It introduces concepts like dual kNN graphs with positive and negative components that guide a neighbor-similarity propagation process using low-pass and high-pass filters. Experiments demonstrate improved defense against attacks on both homophilic and heterophilic graphs, as well as gains on semi-supervised node classification tasks. So the key ideas have to do with adversarial robustness, graph structure learning, and representation learning.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 potential in-depth questions about the method proposed in this paper:

1. The paper proposes constructing positive and negative kNN graphs based on the similarity of aggregated neighbor features. Why is this approach better than using ego-feature similarity as in previous methods? What are the limitations?

2. The paper claims the method works well on both homophilic and heterophilic graphs. What is the intuition behind why preserving neighbor similarity helps for both cases? Are there any differences in how the method performs? 

3. What are the computational complexities involved in constructing the positive and negative kNN graphs? How do the dual graphs impact the overall complexity compared to baseline GNN methods?

4. How exactly does propagating features along the positive vs negative graphs help preserve neighbor similarities? Explain the mechanisms of the low-pass and high-pass filters in this context.  

5. The sensitivity analysis shows the method is more sensitive to k1 than k2. Intuitively explain why this is the case. How should one choose the values of k1 and k2?

6. Can you think of any other techniques, beyond kNN graphs, to encode high and low similarity neighbor information? What are the potential pros and cons?

7. Theoretically analyze how susceptible the proposed method would be to other forms of attack not explored in the paper, such as feature-based attacks. Identify any potential limitations.  

8. How does the performance of the method vary across different levels of sparsity in the graph structure? Are there any limitations in highly sparse graphs?

9. The method uses a simple MLP architecture to obtain learnable weights. Can you think of any graph neural network architectures that could potentially learn these weights in a more meaningful topology-aware manner?

10. For real-world dynamic graphs, reconstructing the kNN graphs from scratch may be inefficient. Can you propose any incremental approaches to efficiently update the graphs and weights?
