# Multimodal Procedural Planning via Dual Text-Image Prompting

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is: How can we develop models that can generate coherent and useful multimodal (text and image) procedural plans towards achieving a high-level goal?The key hypothesis seems to be:By leveraging the reasoning abilities of large language models and the image generation capabilities of diffusion models through a dual prompting approach, we can generate multimodal plans that provide informative, temporally coherent, and accurate guidance for completing tasks.Specifically, the dual prompting framework called Text-Image Prompting (TIP) uses a Text-to-Image Bridge and Image-to-Text Bridge to ground the text plans in the visual context and vice versa. This allows the model to generate plans that are well-aligned across modalities.The paper aims to demonstrate that this dual prompting approach enables zero-shot multimodal planning that is superior to unimodal or separately generated multimodal plans in terms of informativeness, coherence, and accuracy. The hypothesis is evaluated on two new datasets for this task.In summary, the key question is how to elicit strong multimodal planning abilities by combining reasoning from language models and image generation, which is addressed through the proposed TIP framework and evaluated on new multimodal planning datasets.
