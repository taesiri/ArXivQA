# Multimodal Procedural Planning via Dual Text-Image Prompting

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is: How can we develop models that can generate coherent and useful multimodal (text and image) procedural plans towards achieving a high-level goal?The key hypothesis seems to be:By leveraging the reasoning abilities of large language models and the image generation capabilities of diffusion models through a dual prompting approach, we can generate multimodal plans that provide informative, temporally coherent, and accurate guidance for completing tasks.Specifically, the dual prompting framework called Text-Image Prompting (TIP) uses a Text-to-Image Bridge and Image-to-Text Bridge to ground the text plans in the visual context and vice versa. This allows the model to generate plans that are well-aligned across modalities.The paper aims to demonstrate that this dual prompting approach enables zero-shot multimodal planning that is superior to unimodal or separately generated multimodal plans in terms of informativeness, coherence, and accuracy. The hypothesis is evaluated on two new datasets for this task.In summary, the key question is how to elicit strong multimodal planning abilities by combining reasoning from language models and image generation, which is addressed through the proposed TIP framework and evaluated on new multimodal planning datasets.


## What is the main contribution of this paper?

Based on my reading, the main contributions of this paper are:1. Introducing the multimodal procedural planning (MPP) task, where models are given a high-level goal and generate paired sequences of text and image steps to provide guidance for completing the goal. The key challenges are ensuring informativeness, temporal coherence, and accuracy across modalities.2. Proposing Text-Image Prompting (TIP), a dual-modality prompting method that combines the capabilities of large language models (LLMs) and diffusion text-to-image models. TIP uses two "bridges":- Text-to-Image Bridge: Uses LLMs to generate explicit scene descriptions that assist the text-to-image model in generating informative, grounded image plans. - Image-to-Text Bridge: Verbalizes the image plans using captions and injects them back into the LLM prompt to improve text plan informativeness and grounding.3. Collecting two new datasets, WikiPlan and RecipePlan, as testbeds for evaluating multimodal procedural planning.4. Demonstrating through human and automatic evaluations that TIP substantially improves over unimodal and basic multimodal baselines on informativeness, coherence, and accuracy for the MPP task on the collected datasets.In summary, the key contribution is proposing the multimodal procedural planning task and the dual text-image prompting method TIP, along with collecting relevant datasets and showing strong performance improvements over baselines. The dual grounding of text and images is a novel aspect enabled by TIP.
