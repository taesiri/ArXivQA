# [Mixture-of-Supernets: Improving Weight-Sharing Supernet Training with   Architecture-Routed Mixture-of-Experts](https://arxiv.org/abs/2306.04845)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be: How can we improve weight-sharing supernet training to minimize the performance gap between subnets extracted from the supernet vs training those subnets from scratch?The key hypotheses proposed in the paper are:1) Adopting a mixture-of-experts (MoE) approach can enhance the expressive power of the supernet model to better customize weights for different subnets. 2) Having separate expert weight matrices that subnets can route to can increase the overall capacity allocated to subnets beyond the capacity of a single DNN.3) Specializing weights at the level of individual neurons (neuron-wise MoS) provides more fine-grained control than layer-wise MoE.The overall goal is to improve supernet training so the performance of extracted subnets approaches their standalone potential, minimizing the need for additional retraining. This improves the efficiency of neural architecture search pipelines that rely on weight-sharing supernet training.
