# [Mixture-of-Supernets: Improving Weight-Sharing Supernet Training with   Architecture-Routed Mixture-of-Experts](https://arxiv.org/abs/2306.04845)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be: How can we improve weight-sharing supernet training to minimize the performance gap between subnets extracted from the supernet vs training those subnets from scratch?The key hypotheses proposed in the paper are:1) Adopting a mixture-of-experts (MoE) approach can enhance the expressive power of the supernet model to better customize weights for different subnets. 2) Having separate expert weight matrices that subnets can route to can increase the overall capacity allocated to subnets beyond the capacity of a single DNN.3) Specializing weights at the level of individual neurons (neuron-wise MoS) provides more fine-grained control than layer-wise MoE.The overall goal is to improve supernet training so the performance of extracted subnets approaches their standalone potential, minimizing the need for additional retraining. This improves the efficiency of neural architecture search pipelines that rely on weight-sharing supernet training.


## What is the main contribution of this paper?

Based on my reading of the paper abstract, the main contributions appear to be:1. Proposing Mixture-of-Supernets (MoS), a formulation to improve supernet training by enhancing its expressive power using a Mixture-of-Experts (MoE) approach. 2. Adopting MoE to dynamically generate flexible weights for different subnetworks in the supernet based on their architecture. This allows customization of weights for each subnetwork architecture.3. Demonstrating the effectiveness of the proposed method for neural architecture search (NAS) to build efficient models for machine translation and task-agnostic BERT. Key results:- For machine translation, MoS reduces the supernet vs standalone model performance gap, yields better latency-BLEU tradeoffs, and reduces additional training time compared to prior state-of-the-art HAT.- For BERT, MoS closes the supernet vs standalone gap, achieves state-of-the-art NAS performance for multiple model sizes, and reduces/eliminates additional training.4. The proposed MoS framework minimizes the need for retraining subnetworks after NAS by improving the supernet training itself. This greatly improves the efficiency of NAS.In summary, the main contribution appears to be proposing the MoS formulation to enhance supernet training for NAS, and demonstrating its effectiveness for building efficient NLP models with improved NAS efficiency.


## How does this paper compare to other research in the same field?

Here are some points comparing this paper to other related research:- This paper proposes a new method called Mixture-of-Supernets (MoS) to improve weight-sharing supernet training for neural architecture search (NAS). Other recent work on improving supernet training includes methods like sandwich training, progressive shrinking, and few-shot NAS. - A key distinction of the MoS method is using a mixture-of-experts (MoE) approach to allow more flexible weight sharing between subnets, instead of direct weight sharing. This helps address issues like gradient conflict and limited model capacity in standard supernets.- For NAS for machine translation (MT), the paper shows MoS can find architectures with better latency-BLEU tradeoffs compared to the prior state-of-the-art HAT method. MoS also reduces the supernet-standalone gap more than HAT.- For NAS for efficient BERT models, MoS achieves comparable or better performance than leading methods like NAS-BERT and AutoDistil for various model sizes, without expensive retraining.- Overall, a key contribution seems to be improving supernet training itself to enable more accurate architecture evaluation and reduce the need for retraining subnets later. This improves NAS efficiency.- Compared to some other NAS methods that still require full retraining of subnets, the ability of MoS to extract performant weights directly from the supernet is a useful advantage.- The experiments focus on MT and BERT tasks. Testing MoS more broadly on other NLP NAS tasks could be interesting future work to understand generalizability.In summary, the paper introduces a novel MoE-based supernet training approach that advances NAS efficiency and performance compared to other recent methods on major NLP tasks. Reducing supernet-standalone gap and retraining time are noteworthy improvements.


## What future research directions do the authors suggest?

Based on my reading of the paper, some of the key future research directions suggested by the authors are:- Applying the proposed Mixture-of-Supernets (MoS) framework to build efficient autoregressive decoder-only language models like GPT. The authors state that given the growing interest in models like GPT-4, using MoS to design efficient GPT architectures would be an impactful direction for future work.- Investigating the full potential of MoS by combining larger training budgets (≥200K steps) and more expert weights (≥16). The authors used a relatively small number of expert weights (m=2) and standard training budgets in their experiments for fair comparison. Scaling up both dimensions could reveal the capabilities of larger MoS models.- Studying the effect of applying MoS to other components of the Transformer architecture besides the feedforward layers, such as the self-attention projection layers and LayerNorm. The authors currently only replace the feedforward network in Transformer with MoS.- Applying MoS to more NLP tasks beyond machine translation and BERT pretraining. The authors focused their experiments on these two popular NLP benchmarks, but suggest testing the proposed methods on other tasks as well.- Exploring the reasons behind the performance trends observed, such as why the neuron-wise MoS did not exhibit training instability despite increased flexibility. The authors suggest further analysis could provide insights into these phenomena.In summary, the key directions are scaling up MoS models, applying MoS more broadly across Transformer components and NLP tasks, and further analysis to explain observed experimental results and inform future research.


## Summarize the paper in one paragraph.

Here is a one paragraph summary of the paper:The paper proposes Mixture-of-Supernets (MoS), a generalized supernet formulation where mixture-of-experts (MoE) is adopted to enhance the expressive power of the supernet model for neural architecture search (NAS). Typically, weight-sharing supernets have limited capacity to extract customized weights for different architectures. To address this, MoS maintains multiple sets of expert weights and constructs architecture-specific weights through an architecture-based routing mechanism. This allows flexible weight sharing and increased capacity. Two variants are presented: layer-wise MoS combines expert weights at the granularity of layers, while neuron-wise MoS does so at the granularity of neurons. Experiments on NAS for machine translation and BERT show that MoS can minimize the supernet vs. standalone performance gap, achieve state-of-the-art results, and reduce the time for retraining subnetworks. The method provides an effective way to improve weight-sharing supernet training and NAS efficiency.


## Summarize the paper in two paragraphs.

Here is a 2 paragraph summary of the paper:The paper proposes Mixture-of-Supernets (MoS), a method to improve weight sharing supernet training for neural architecture search (NAS). Supernets allow fast evaluation of candidate architectures by sharing weights between models in the search space. However, standard weight sharing has limitations in capacity and conflicts between architectures. To address this, MoS uses a mixture-of-experts approach to generate flexible weights for each architecture based on an architecture encoding. There are two variants: layer-wise MoS which mixes expert weight matrices, and neuron-wise MoS which mixes expert neurons. This allows specialized weights for each architecture. Experiments on NAS for machine translation and BERT show MoS reduces the supernet-scratch gap and finds better architectures vs prior NAS methods. For machine translation, MoS yields 20-22% lower search time and 5% higher BLEU than the state-of-the-art HAT method. For BERT, MoS achieves similar performance to NAS-BERT and AutoDistil at various model sizes without additional training.
