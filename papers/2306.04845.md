# [Mixture-of-Supernets: Improving Weight-Sharing Supernet Training with   Architecture-Routed Mixture-of-Experts](https://arxiv.org/abs/2306.04845)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be: How can we improve weight-sharing supernet training to minimize the performance gap between subnets extracted from the supernet vs training those subnets from scratch?The key hypotheses proposed in the paper are:1) Adopting a mixture-of-experts (MoE) approach can enhance the expressive power of the supernet model to better customize weights for different subnets. 2) Having separate expert weight matrices that subnets can route to can increase the overall capacity allocated to subnets beyond the capacity of a single DNN.3) Specializing weights at the level of individual neurons (neuron-wise MoS) provides more fine-grained control than layer-wise MoE.The overall goal is to improve supernet training so the performance of extracted subnets approaches their standalone potential, minimizing the need for additional retraining. This improves the efficiency of neural architecture search pipelines that rely on weight-sharing supernet training.


## What is the main contribution of this paper?

Based on my reading of the paper abstract, the main contributions appear to be:1. Proposing Mixture-of-Supernets (MoS), a formulation to improve supernet training by enhancing its expressive power using a Mixture-of-Experts (MoE) approach. 2. Adopting MoE to dynamically generate flexible weights for different subnetworks in the supernet based on their architecture. This allows customization of weights for each subnetwork architecture.3. Demonstrating the effectiveness of the proposed method for neural architecture search (NAS) to build efficient models for machine translation and task-agnostic BERT. Key results:- For machine translation, MoS reduces the supernet vs standalone model performance gap, yields better latency-BLEU tradeoffs, and reduces additional training time compared to prior state-of-the-art HAT.- For BERT, MoS closes the supernet vs standalone gap, achieves state-of-the-art NAS performance for multiple model sizes, and reduces/eliminates additional training.4. The proposed MoS framework minimizes the need for retraining subnetworks after NAS by improving the supernet training itself. This greatly improves the efficiency of NAS.In summary, the main contribution appears to be proposing the MoS formulation to enhance supernet training for NAS, and demonstrating its effectiveness for building efficient NLP models with improved NAS efficiency.
