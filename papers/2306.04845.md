# [Mixture-of-Supernets: Improving Weight-Sharing Supernet Training with   Architecture-Routed Mixture-of-Experts](https://arxiv.org/abs/2306.04845)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be: How can we improve weight-sharing supernet training to minimize the performance gap between subnets extracted from the supernet vs training those subnets from scratch?The key hypotheses proposed in the paper are:1) Adopting a mixture-of-experts (MoE) approach can enhance the expressive power of the supernet model to better customize weights for different subnets. 2) Having separate expert weight matrices that subnets can route to can increase the overall capacity allocated to subnets beyond the capacity of a single DNN.3) Specializing weights at the level of individual neurons (neuron-wise MoS) provides more fine-grained control than layer-wise MoE.The overall goal is to improve supernet training so the performance of extracted subnets approaches their standalone potential, minimizing the need for additional retraining. This improves the efficiency of neural architecture search pipelines that rely on weight-sharing supernet training.


## What is the main contribution of this paper?

Based on my reading of the paper abstract, the main contributions appear to be:1. Proposing Mixture-of-Supernets (MoS), a formulation to improve supernet training by enhancing its expressive power using a Mixture-of-Experts (MoE) approach. 2. Adopting MoE to dynamically generate flexible weights for different subnetworks in the supernet based on their architecture. This allows customization of weights for each subnetwork architecture.3. Demonstrating the effectiveness of the proposed method for neural architecture search (NAS) to build efficient models for machine translation and task-agnostic BERT. Key results:- For machine translation, MoS reduces the supernet vs standalone model performance gap, yields better latency-BLEU tradeoffs, and reduces additional training time compared to prior state-of-the-art HAT.- For BERT, MoS closes the supernet vs standalone gap, achieves state-of-the-art NAS performance for multiple model sizes, and reduces/eliminates additional training.4. The proposed MoS framework minimizes the need for retraining subnetworks after NAS by improving the supernet training itself. This greatly improves the efficiency of NAS.In summary, the main contribution appears to be proposing the MoS formulation to enhance supernet training for NAS, and demonstrating its effectiveness for building efficient NLP models with improved NAS efficiency.


## How does this paper compare to other research in the same field?

Here are some points comparing this paper to other related research:- This paper proposes a new method called Mixture-of-Supernets (MoS) to improve weight-sharing supernet training for neural architecture search (NAS). Other recent work on improving supernet training includes methods like sandwich training, progressive shrinking, and few-shot NAS. - A key distinction of the MoS method is using a mixture-of-experts (MoE) approach to allow more flexible weight sharing between subnets, instead of direct weight sharing. This helps address issues like gradient conflict and limited model capacity in standard supernets.- For NAS for machine translation (MT), the paper shows MoS can find architectures with better latency-BLEU tradeoffs compared to the prior state-of-the-art HAT method. MoS also reduces the supernet-standalone gap more than HAT.- For NAS for efficient BERT models, MoS achieves comparable or better performance than leading methods like NAS-BERT and AutoDistil for various model sizes, without expensive retraining.- Overall, a key contribution seems to be improving supernet training itself to enable more accurate architecture evaluation and reduce the need for retraining subnets later. This improves NAS efficiency.- Compared to some other NAS methods that still require full retraining of subnets, the ability of MoS to extract performant weights directly from the supernet is a useful advantage.- The experiments focus on MT and BERT tasks. Testing MoS more broadly on other NLP NAS tasks could be interesting future work to understand generalizability.In summary, the paper introduces a novel MoE-based supernet training approach that advances NAS efficiency and performance compared to other recent methods on major NLP tasks. Reducing supernet-standalone gap and retraining time are noteworthy improvements.
