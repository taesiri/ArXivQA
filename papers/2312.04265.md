# [Stronger, Fewer, &amp; Superior: Harnessing Vision Foundation Models for   Domain Generalized Semantic Segmentation](https://arxiv.org/abs/2312.04265)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a high-quality one-paragraph summary of the key points from this paper:

This paper first assesses various Vision Foundation Models (VFMs) including CLIP, MAE, SAM, EVA02, and DINOv2 in the context of Domain Generalized Semantic Segmentation (DGSS) by employing them as backbone architectures. The experiments show VFMs achieve significantly better performance compared to previous DGSS methods without any specialized design, demonstrating their powerful generalization capabilities. However, fine-tuning VFMs with all their parameters on smaller datasets results in overfitting, limiting generalizability. Thus the authors propose "Rein", a robust fine-tuning strategy to efficiently harness VFMs for DGSS using fewer trainable parameters. Rein employs random tokens linked to instances to enable instance-level feature refinement through an attention mechanism within each backbone layer. Additional techniques like shared MLP weights and low-rank token matrices further reduce parameters. Extensive experiments on diverse DGSS benchmarks show Rein outperforms state-of-the-art with fewer trainable parameters. For example, Rein achieves 68.1% mIoU on Cityscapes using only 1% extra trainable parameters in DINOv2 backbone, without accessing any real urban datasets during training. The consistent performance gains validate Rein's effectiveness in employing fewer parameters to leverage stronger VFMs for superior generalizability.
