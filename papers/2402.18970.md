# [PrivatEyes: Appearance-based Gaze Estimation Using Federated Secure   Multi-Party Computation](https://arxiv.org/abs/2402.18970)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Appearance-based gaze estimation methods require large-scale eye image datasets for training, but collecting and sharing such sensitive data poses privacy risks.
- Prior work used federated learning (FL) for privacy-preserving gaze estimation training, but FL leaks private information through the shared model updates.

Proposed Solution:
- The paper proposes "PrivatEyes", a novel privacy-enhancing training approach that combines FL and secure multi-party computation (MPC).

- In PrivatEyes, clients train models locally on their data and secret-share the model updates among multiple servers. The servers then securely aggregate the updates using MPC.

- MPC guarantees that servers do not learn the individual model updates or private data, even if all-but-one server is malicious.

Main Contributions:

- First approach to combine FL and MPC for privacy-preserving gaze estimation training with strong security guarantees.

- New "DualView" attack that reconstructs user appearance and gaze distribution to evaluate privacy leakage.

- Evaluations on MPIIGaze, MPIIFaceGaze, etc. show PrivatEyes matches performance of non-secure methods with negligible overhead.

- Experiments demonstrate significantly better privacy for PrivatEyes over centralized FL, e.g. DualView accurately reconstructs 15/15 people for FL but 0/15 people for PrivatEyes.

In summary, PrivatEyes enables collaborative training of accurate gaze estimators with formal privacy guarantees and shown resilience against data leakage attacks.


## Summarize the paper in one sentence.

 Here is a one-sentence summary of the paper:

The paper proposes PrivatEyes, the first privacy-preserving training approach for appearance-based gaze estimation that combines federated learning and secure multi-party computation to train gaze estimators across multiple private datasets while keeping individual gaze data private even from malicious aggregating servers.


## What is the main contribution of this paper?

 The main contribution of this paper is proposing PrivatEyes, the first privacy-preserving training approach for appearance-based gaze estimation that combines federated learning (FL) and secure multi-party computation (MPC). Key aspects of the contribution include:

1) PrivatEyes enables collaborative training of gaze estimators across multiple clients' private datasets, while guaranteeing that individual gaze data remains private even if a majority of the aggregating servers is malicious.

2) The paper introduces a new data leakage attack called DualView that shows PrivatEyes limits the leakage of private training data more effectively than previous approaches like centralized federated learning.

3) Evaluations on several benchmark gaze estimation datasets demonstrate that PrivatEyes maintains on-par gaze estimation accuracy and computational costs compared to non-secure counterparts, with improved privacy guarantees.

In summary, PrivatEyes advances privacy-preserving machine learning for gaze estimation via a novel combination of federated learning and MPC, empirically evaluated to have strong security with limited impact on utility. The key innovation is enabling collaborative learning on distributed gaze datasets with formal privacy assurances against malicious adversaries.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and concepts associated with this paper include:

- Gaze estimation - The task of estimating where a person is looking based on images of their eyes/face. This is the core application domain being studied.

- Privacy - A major focus of the paper is on training gaze estimation models while preserving privacy of the training data.

- Federated learning (FL) - A distributed machine learning approach that trains models across multiple clients without directly sharing private training data. This forms one component of their approach.

- Secure multi-party computation (MPC) - Cryptographic techniques to allow multiple parties to jointly compute on sensitive data while keeping the data private. This is combined with FL in their method.

- Individual updates (IU) - The locally trained model parameters from each client in federated learning before aggregation. Protecting these is key for privacy.

- Output model (OM) - The aggregated model after federated learning rounds. This still contains some private information.

- DualView attack - The novel attack method proposed to evaluate privacy leakage from gaze estimation models. It targets both facial appearance and gaze direction.

- Re-identification, pixel similarity, gaze error, gaze distribution similarity - Metrics used to quantify performance of the DualView attack and privacy preservation of different training methods.

In summary, the key focus is on privacy-preserving gaze estimation via a combination of federated learning and secure multi-party computation. The DualView attack then allows evaluation of information leakage under different training schemes.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 potential in-depth questions about the method proposed in this paper:

1. How does PrivatEyes combine federated learning and secure multi-party computation to achieve privacy-preserving gaze estimation training? What are the key advantages of this approach over using federated learning alone?

2. Explain the threat model assumed in the DualView attack. What knowledge does the adversary have access to and what are the goals of the attack? 

3. The DualView attack uses a GAN to reconstruct training images. What is the architecture of this GAN and what loss functions does it aim to optimize? How is auxiliary knowledge incorporated?

4. What security guarantees does PrivatEyes provide as long as one server remains honest? Explain the privacy properties formally ensured. 

5. How does the amount of information leakage in PrivatEyes compare to adaptive federated learning and generic MPC? Analyze the tradeoffs between privacy, efficiency and gaze estimation accuracy.  

6. What explains the difference in performance between reconstructing the appearance versus the gaze distribution of training images in the DualView attack? 

7. How do the results demonstrate that model convergence during federated learning training also affects privacy? Explain the role of prior model updates as auxiliary knowledge.

8. Analyze the limitations of generic MPC that prevent it from being applied efficiently to real-world gaze estimation problems. How does PrivatEyes overcome these?

9. Discuss the assumptions regarding trust in servers made in PrivatEyes. How can these be aligned with users' attitudes towards sharing sensitive eye data based on survey insights? 

10. Suggest areas of future work to address current limitations of privacy-preserving gaze estimation, such as hyperparameter optimization and semi-supervised learning.
