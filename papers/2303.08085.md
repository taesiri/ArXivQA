# [Alias-Free Convnets: Fractional Shift Invariance via Polynomial   Activations](https://arxiv.org/abs/2303.08085)

## What is the central research question or hypothesis that this paper addresses?

 This paper proposes a new convolutional neural network (CNN) architecture called Alias-Free ConvNet (AFC) that aims to achieve true shift invariance and shift equivariance. The central hypothesis is that by preventing aliasing effects that stem from downsampling layers and non-linear activations, it is possible to construct a CNN that is provably invariant/equivariant to input image translations, even fractional (sub-pixel) shifts. 

Specifically, the paper addresses the following key questions:

- How can aliasing effects in CNNs, caused by downsampling and non-linearities, be eliminated to achieve true shift invariance/equivariance?

- Can polynomial activations be used instead of standard activations like ReLU to limit bandwidth expansion and prevent aliasing, while still achieving competitive accuracy on large-scale tasks like ImageNet? 

- Does preventing aliasing lead to CNNs that are more robust to adversarial attacks based on small input image translations?

- Can fractional shift invariance/equivariance be formally proven and demonstrated empirically?

The central hypothesis is that by using alias-free downsampling layers and polynomial activations in an end-to-end manner, it is possible to construct a CNN that is provably invariant/equivariant even to fractional input shifts. The paper proposes specific techniques and provides both theoretical analysis and empirical evaluations to test this hypothesis.
