# [Alias-Free Convnets: Fractional Shift Invariance via Polynomial   Activations](https://arxiv.org/abs/2303.08085)

## What is the central research question or hypothesis that this paper addresses?

 This paper proposes a new convolutional neural network (CNN) architecture called Alias-Free ConvNet (AFC) that aims to achieve true shift invariance and shift equivariance. The central hypothesis is that by preventing aliasing effects that stem from downsampling layers and non-linear activations, it is possible to construct a CNN that is provably invariant/equivariant to input image translations, even fractional (sub-pixel) shifts. 

Specifically, the paper addresses the following key questions:

- How can aliasing effects in CNNs, caused by downsampling and non-linearities, be eliminated to achieve true shift invariance/equivariance?

- Can polynomial activations be used instead of standard activations like ReLU to limit bandwidth expansion and prevent aliasing, while still achieving competitive accuracy on large-scale tasks like ImageNet? 

- Does preventing aliasing lead to CNNs that are more robust to adversarial attacks based on small input image translations?

- Can fractional shift invariance/equivariance be formally proven and demonstrated empirically?

The central hypothesis is that by using alias-free downsampling layers and polynomial activations in an end-to-end manner, it is possible to construct a CNN that is provably invariant/equivariant even to fractional input shifts. The paper proposes specific techniques and provides both theoretical analysis and empirical evaluations to test this hypothesis.


## What is the main contribution of this paper?

 The main contribution of this paper is proposing an alias-free convolutional neural network (CNN) architecture that is provably invariant to translations, including fractional (sub-pixel) translations. The key ideas are:

- Using polynomial activations instead of standard activations like ReLU. Polynomials have limited frequency support, so with proper upsampling/downsampling they do not cause aliasing.

- Modifying downsampling operators like strided convolution to use anti-aliasing low-pass filters (BlurPool).

- Modifying normalization to be shift-equivariant. 

- Theoretical analysis showing these modifications make the network provably invariant to any translation of the input, even fractional shifts.

- Empirical evaluation showing the alias-free CNN has superior robustness to translation attacks compared to standard CNNs and prior methods for invariance.

So in summary, it presents the first CNN architecture that is provably invariant to all translations by comprehensively addressing aliasing in activations, downsampling and normalization. This is shown to improve robustness. The use of polynomial activations is key to handle aliasing from nonlinearities.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the key point from the paper:

The paper proposes a new aliasing-free convolutional neural network architecture that uses polynomial activations to achieve guaranteed shift invariance and shift equivariance, even for fractional pixel translations.


## How does this paper compare to other research in the same field?

 Here are a few thoughts on how this paper compares to other research on constructing shift-invariant convolutional neural networks (CNNs):

- The paper proposes a new method for creating CNNs that are provably shift-invariant, even to sub-pixel translations. This guarantees robustness to adversarial attacks based on image translations. Prior work like that of Azulay and Weiss (2019) showed CNNs are not inherently shift-invariant, while others like Zhang (2019) and Chaman et al. (2020) proposed methods to improve shift-invariance but could not achieve perfect invariance.

- The key innovation is using polynomial activations in an alias-free framework. Most prior work focused only on downsampling layers as a source of aliasing. The authors recognize non-linearities also introduce aliasing, and show polynomials have bounded frequency range to prevent this. 

- Using polynomials for activations is novel, as most prior work uses ReLUs or smooth activations like ELU/GeLU. The authors demonstrate polynomials can work well with proper initialization and normalization. This is an interesting finding even apart from the shift-invariance benefits.

- The method guarantees both shift-invariant outputs and shift-equivariant internal representations. Some prior work like Chaman et al. focused only on invariant outputs. Equivariance is useful for tasks like segmentation.

- The certified robustness is for circular shifts, which is limited. But the method still demonstrates improved robustness on other shifts like crop-based translations. And circular shifts are relevant in some application domains.

- There is an accuracy vs robustness tradeoff, with a 1% drop compared to baseline ConvNeXt. But the model outperforms others under adversarial shifts. Exploring ways to recover the accuracy loss could be interesting future work.

Overall, the alias-free framework with polynomial activations seems like an important advance over prior art in provable shift-invariance for CNNs. The method is novel and has nice theoretical properties. Demonstrating effectiveness on large-scale tasks like ImageNet classification is also significant.
