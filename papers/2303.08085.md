# [Alias-Free Convnets: Fractional Shift Invariance via Polynomial   Activations](https://arxiv.org/abs/2303.08085)

## What is the central research question or hypothesis that this paper addresses?

 This paper proposes a new convolutional neural network (CNN) architecture called Alias-Free ConvNet (AFC) that aims to achieve true shift invariance and shift equivariance. The central hypothesis is that by preventing aliasing effects that stem from downsampling layers and non-linear activations, it is possible to construct a CNN that is provably invariant/equivariant to input image translations, even fractional (sub-pixel) shifts. 

Specifically, the paper addresses the following key questions:

- How can aliasing effects in CNNs, caused by downsampling and non-linearities, be eliminated to achieve true shift invariance/equivariance?

- Can polynomial activations be used instead of standard activations like ReLU to limit bandwidth expansion and prevent aliasing, while still achieving competitive accuracy on large-scale tasks like ImageNet? 

- Does preventing aliasing lead to CNNs that are more robust to adversarial attacks based on small input image translations?

- Can fractional shift invariance/equivariance be formally proven and demonstrated empirically?

The central hypothesis is that by using alias-free downsampling layers and polynomial activations in an end-to-end manner, it is possible to construct a CNN that is provably invariant/equivariant even to fractional input shifts. The paper proposes specific techniques and provides both theoretical analysis and empirical evaluations to test this hypothesis.


## What is the main contribution of this paper?

 The main contribution of this paper is proposing an alias-free convolutional neural network (CNN) architecture that is provably invariant to translations, including fractional (sub-pixel) translations. The key ideas are:

- Using polynomial activations instead of standard activations like ReLU. Polynomials have limited frequency support, so with proper upsampling/downsampling they do not cause aliasing.

- Modifying downsampling operators like strided convolution to use anti-aliasing low-pass filters (BlurPool).

- Modifying normalization to be shift-equivariant. 

- Theoretical analysis showing these modifications make the network provably invariant to any translation of the input, even fractional shifts.

- Empirical evaluation showing the alias-free CNN has superior robustness to translation attacks compared to standard CNNs and prior methods for invariance.

So in summary, it presents the first CNN architecture that is provably invariant to all translations by comprehensively addressing aliasing in activations, downsampling and normalization. This is shown to improve robustness. The use of polynomial activations is key to handle aliasing from nonlinearities.
