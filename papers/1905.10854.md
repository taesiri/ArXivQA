# [Let's Agree to Agree: Neural Networks Share Classification Order on Real   Datasets](https://arxiv.org/abs/1905.10854)

## What is the central research question or hypothesis that this paper addresses?

 This paper studies the learning dynamics of neural networks and investigates whether neural networks with the same architecture learn examples in the same order. The central hypothesis is that neural networks exhibit similar learning dynamics, in the sense that they learn examples in the same order despite differences in initialization, optimization, hyperparameters, etc. The key research questions are:1) Do neural networks with the same architecture learn the training and test examples in a consistent order across different random initializations, minibatch orderings, and other sources of randomness? 2) Is this consistent learning order preserved even when training the networks on different subsets of the data sampled from the same distribution?3) Do neural networks with different architectures learn the data in similar orders, or is the learning order more architecture-specific? 4) Is the consistent learning order an artifact of the neural network training process, or does it reflect underlying structure in the data?To summarize, the central hypothesis is that neural networks exhibit consistent learning dynamics and order examples similarly during training. The key questions aim to characterize the scope of this consistency across networks and data as well as understand the underlying causes.


## What is the main contribution of this paper?

 Based on my reading, the main contributions of this paper are:1. Proposing a new way to compare neural network models called True-Positive (TP) agreement. This measures the agreement between models on which examples they classify correctly. 2. Empirically showing that neural networks with the same architecture tend to learn examples in the same order, as evidenced by similar TP-agreement throughout training. This order is robust across different hyperparameters, optimizations, and random initializations.3. Demonstrating that while neural networks with different architectures may learn at different paces, they still tend to agree on the order in which examples are learned. More powerful models first learn the examples already learned by weaker models before continuing to learn new examples.4. Arguing based on experiments with synthetic datasets and non-neural classifiers that the consistent learning order of neural networks on real datasets is not just an artifact of SGD optimization or dataset complexity. Rather, it reflects how neural networks discover structure in these datasets.5. Proposing that the consistent order in which examples are learned by neural networks on a given dataset reflects an intrinsic hierarchy of complexity or learnability of those examples. In summary, the paper provides both a new way to compare neural network models based on their learning dynamics, and evidence that the learning order is a robust property of how neural networks interact with the structure of real-world datasets.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the key points from the paper:The paper empirically shows that deep neural networks trained on the same datasets learn to correctly classify examples in a similar order, regardless of architecture or hyperparameters, reflecting shared structure in the data.


## How does this paper compare to other research in the same field?

 This paper presents a robust empirical observation that deep neural networks learn examples in real-world datasets in a similar order, regardless of architecture or training details. The key findings are:- Neural networks with the same architecture learn the training and test sets in a very similar order, as evidenced by the bi-modal distribution of the proposed TP-agreement score over examples. This order is robust to initialization, optimization details, hyperparameters, etc.- The learning order also correlates well across different architectures trained on the same dataset. More powerful architectures first learn the examples already learned by weaker architectures before continuing to learn more. - On synthetic datasets like Gabor patches or Gaussian data, neural networks lose this consistent learning order, indicating it emerges from the coupling of architecture and real-world data.- Non-neural methods like AdaBoost do not learn the same examples early on real datasets, suggesting the learning order is unique to neural networks.These findings relate to several active areas of deep learning research:- Understanding generalization - The robust learning order may reflect how neural nets discover structure in data. It echoes results claiming NNs "prefer" simple hypotheses fitting the data.- Comparing neural nets - Most prior work compares nets via representational similarity, while this directly compares predictions. The order correlation could be a useful similarity measure.- Curriculum learning - The learning order provides a measure of example difficulty. But experiments show curriculum learning based on this order does not improve performance.- Role of architecture - Results on synthetic data show architecture matters for consistent order. This connects to work analyzing how architecture affects learning.Overall the paper provides a new characterization of how neural nets learn, with potential implications for understanding generalization, similarities, and the interplay between architecture and data. The consistent order on real-world data is the main novel finding.


## What future research directions do the authors suggest?

 The authors suggest several promising directions for future research:1. Further analyze why certain examples are consistently easier for neural networks to learn across different architectures and datasets. They hypothesize this may relate to discovering structure in natural datasets, but more investigation is needed. 2. Use the similarity in learning dynamics across neural network instances as a tool for tasks like novelty detection. For example, when test images elicit very different classifications from an ensemble of models, this may indicate out-of-distribution samples.3. Explore whether the learning dynamics and orderings found in vision extend to other modalities like audio, video, and text.4. Study the relationship between learning dynamics and curriculum/active learning, where examples are presented in a meaningful order. Can learning dynamics inform the sequencing of training data?5. Investigate theoretically why neural networks exhibit this consistent learning behavior, while other models like SVMs do not. What properties lead neural nets to learn in such a robust order?6. Develop synthetic datasets where neural networks demonstrate more diversity in learning dynamics, to better understand the interplay between architectures, optimization, and datasets.7. Compare neural learning dynamics under different training schemes like meta-learning, continual learning, and multi-task learning. Do consistencies still emerge?Overall, the robust similarities in learning dynamics appear fundamental to how neural networks operate, and many open questions remain about the causes and potential applications. The authors have empirically characterized an intriguing phenomenon that warrants much further study across domains, architectures, and learning settings.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the paper:This paper empirically demonstrates that neural networks tend to learn the examples in training and test sets in a similar order, independent of architecture, optimization, initialization, and hyperparameters. Specifically, the authors show that neural networks exhibit a bi-modal distribution in their true positive agreement (TP-agreement) scores over examples throughout training. This indicates that most examples start out being classified incorrectly by all models, then rapidly shift to being classified correctly by all models at some point during training. The order in which examples shift from incorrect to correct classification is highly correlated across neural networks, even those with different architectures trained on different subsets of the data. This phenomenon holds across various image and text classification tasks. The learning order only breaks down on synthetic datasets, suggesting it may reflect how neural networks discover structure in real-world data. Overall, the robust similarity in learning order indicates neural network models are more alike than their different weights and architectures suggest.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the paper:This paper empirically demonstrates that deep neural networks learn examples from datasets in a similar order, regardless of differences in architecture, hyperparameters, or random initialization. The authors train multiple instances of various neural network architectures on datasets like ImageNet, CIFAR, and MNIST. They propose a metric called "TP-agreement" to measure the fraction of networks that correctly classify an example at a given training epoch. Plotting the distribution of TP-agreement over examples reveals a bi-modal pattern, indicating the networks consistently learn some examples earlier than others. This holds even for different architectures applied to the same dataset. For example, the easier examples learned first by AlexNet on ImageNet are also learned first by ResNet-50. The learning order also correlates across different random initializations and hyperparameters of the same architecture. However, the bi-modal pattern disappears on synthetic datasets like randomly labeled images, suggesting it emerges from structure in real-world data. Overall, the results indicate neural networks tend to learn the underlying structure of datasets in a similar order, reflecting consistencies in how different models fit the data.In summary, this paper shows neural networks learn real datasets in a similar order, demonstrating consistencies in how neural networks discover structure in data. The robust learning order holds across different architectures, initializations, and hyperparameters, but does not occur on synthetic datasets. The authors propose metrics like TP-agreement to quantify the learning order, revealing it is not an artifact of SGD but rather reflects properties of the datasets and modeling approach. The results provide insight into neural network generalization.


## Summarize the main method used in the paper in one paragraph.

 Here is a one paragraph summary of the main method used in the paper:The paper proposes a way to measure the similarity between different neural network models by analyzing their classification predictions per example during training. They train multiple instances of the same architecture on a dataset, each with different random initializations and minibatch orders. At every epoch during training, they compare the classification vectors (correct/incorrect per example) of the model instances to analyze their true positive agreement (TP-agreement). They find that the distribution of TP-agreement scores is bi-modal, indicating the models learn the examples in a similar order - some examples have high TP-agreement while others have low TP-agreement. They also show this pattern holds when training different architectures on the same dataset, indicating a robust learning order. Based on the TP-agreement scores, they define an accessibility measure to quantify how quickly examples are learned. They argue the learning order reflects how neural networks discover structure in natural datasets.


## What problem or question is the paper addressing?

 Based on my reading, the key points of this paper are:- The paper proposes a new way to compare different neural network models by looking at their predictions on individual examples, rather than just overall accuracy. Specifically, they define a "True Positive agreement" (TP-agreement) score that measures how consistent multiple models are in classifying each example correctly. - Through extensive experiments, the paper shows that neural networks with the same architecture learn the train and test examples in a very consistent order, even with different random initializations and optimization. The TP-agreement distribution is bimodal, indicating the examples are either learned early on by all models or not learned at all. - This consistent learning order occurs not just within a single architecture, but also across different architectures. For example, AlexNet first learns the examples ResNet-50 learns, then continues on. The correlation of learning order is very high between models.- On synthetic datasets, neural networks do not show this consistent learning order, suggesting it emerges from the interplay between architectures and real-world data. Other learning algorithms like AdaBoost also do not learn natural datasets in the same order as neural networks.- The key hypothesis is that the consistent learning order reflects inherent structure in the data that neural networks are able to discover. The order may reveal which examples are fundamentally easier or harder for a neural network architecture.In summary, the paper provides robust empirical evidence that neural networks learn the examples in real-world datasets in a consistent order, both within and across architectures. This order likely reflects an inherent structure in the data that neural networks are able to discover.
