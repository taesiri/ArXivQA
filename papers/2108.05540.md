# [Unsupervised Corpus Aware Language Model Pre-training for Dense Passage   Retrieval](https://arxiv.org/abs/2108.05540)

## What is the central research question or hypothesis that this paper addresses?

The central hypothesis of this paper is that heavy data engineering and large batch training, as used in state-of-the-art dense retriever fine-tuning techniques like RocketQA, can be avoided by instead pre-training a language model to have two key properties:1) Noise resistance, so it is robust to mislabeling and other noise in the training data.2) A well-structured global embedding space, so passages are embedded close to related passages and far from unrelated ones. To achieve this, the authors propose a two-step pre-training method called coCondenser:1) Condenser pre-training, which uses a modified BERT architecture to produce an information-rich CLS representation robust to noise. 2) Corpus-aware contrastive pre-training, which adds an unsupervised contrastive loss to structure the passage embedding space in a corpus-aware way.The central hypothesis is that a model pre-trained this way will not need heavy data engineering or large batches when fine-tuned for dense retrieval, since it already has noise resistance and a useful embedding space. Experiments on passage ranking datasets seem to confirm this hypothesis.


## What is the main contribution of this paper?

Based on my reading, the main contributions of this paper are:1. It identifies two key challenges with training dense passage retrievers: fragility to training data noise and requiring large batches for learning a robust embedding space. 2. It proposes a new pre-training method called coCondenser to address these challenges. coCondenser combines the Condenser architecture with a corpus-level contrastive loss. 3. Condenser architecture learns to condense information into a dense vector through language model pre-training, making it more robust to noise. 4. The contrastive loss trains an embedding space over the corpus in an unsupervised way, removing the need for large batches during fine-tuning.5. Experiments on MS MARCO, Natural Questions, and TriviaQA show coCondenser achieves comparable or better performance than state-of-the-art systems like RocketQA, while using simple fine-tuning without data augmentation or large batches.In summary, the key contribution is a new pre-training method coCondenser that makes dense retrievers much easier and more robust to train by handling noise and embedding space issues upfront. This removes the need for heavy data engineering during fine-tuning.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper proposes an unsupervised corpus-aware language model pre-training method called coCondenser that allows effective small batch fine-tuning of dense retrievers without relying on data engineering techniques like denoising, augmentation, or large batches.


## How does this paper compare to other research in the same field?

Here are a few key points on how this paper compares to other research in dense retrieval:- This paper focuses on pre-training language models specifically for dense retrieval, which has been relatively unexplored until recently. Most prior work has focused on designing retrieval-specific fine-tuning techniques like RocketQA. A concurrent work, DPR-PAQ, also looks at pre-training but uses a large set of synthetic QA pairs rather than the unsupervised corpus-level pre-training proposed here.- The proposed coCondenser method aims to address two issues with typical language models: sensitivity to noise/mislabeling and difficulty learning an effective passage embedding space. It does this by incorporating the Condenser architecture and adding an unsupervised contrastive loss over the target corpus during pre-training.- Experiments show coCondenser achieves performance on par or better than RocketQA and other state-of-the-art methods on passage ranking benchmarks, while using much simpler fine-tuning (no data augmentation, filtering, etc). This highlights the effectiveness of the pre-training approach.- Comparisons to DPR-PAQ show coCondenser is competitive when using BERT base, despite DPR-PAQ's large semi-supervised pre-training data. However, DPR-PAQ shows the best results by combining its pre-training with RoBERTa large. This suggests supervised pre-training may still be better for maximizing performance when resources allow.- For real-world application, coCondenser provides an efficient way to pre-train once on a corpus then fine-tune easily for different tasks/queries. This removes much of the engineering effort required in prior work to realize the full potential of dense retrievers.So in summary, coCondenser demonstrates the promise of unsupervised pre-training for dense retrieval as an alternative to complex fine-tuning methods or large semi-supervised pre-training. The results and comparisons provide useful insights on the current state-of-the-art in this area.
