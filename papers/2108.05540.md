# [Unsupervised Corpus Aware Language Model Pre-training for Dense Passage   Retrieval](https://arxiv.org/abs/2108.05540)

## What is the central research question or hypothesis that this paper addresses?

 The central hypothesis of this paper is that heavy data engineering and large batch training, as used in state-of-the-art dense retriever fine-tuning techniques like RocketQA, can be avoided by instead pre-training a language model to have two key properties:1) Noise resistance, so it is robust to mislabeling and other noise in the training data.2) A well-structured global embedding space, so passages are embedded close to related passages and far from unrelated ones. To achieve this, the authors propose a two-step pre-training method called coCondenser:1) Condenser pre-training, which uses a modified BERT architecture to produce an information-rich CLS representation robust to noise. 2) Corpus-aware contrastive pre-training, which adds an unsupervised contrastive loss to structure the passage embedding space in a corpus-aware way.The central hypothesis is that a model pre-trained this way will not need heavy data engineering or large batches when fine-tuned for dense retrieval, since it already has noise resistance and a useful embedding space. Experiments on passage ranking datasets seem to confirm this hypothesis.


## What is the main contribution of this paper?

 Based on my reading, the main contributions of this paper are:1. It identifies two key challenges with training dense passage retrievers: fragility to training data noise and requiring large batches for learning a robust embedding space. 2. It proposes a new pre-training method called coCondenser to address these challenges. coCondenser combines the Condenser architecture with a corpus-level contrastive loss. 3. Condenser architecture learns to condense information into a dense vector through language model pre-training, making it more robust to noise. 4. The contrastive loss trains an embedding space over the corpus in an unsupervised way, removing the need for large batches during fine-tuning.5. Experiments on MS MARCO, Natural Questions, and TriviaQA show coCondenser achieves comparable or better performance than state-of-the-art systems like RocketQA, while using simple fine-tuning without data augmentation or large batches.In summary, the key contribution is a new pre-training method coCondenser that makes dense retrievers much easier and more robust to train by handling noise and embedding space issues upfront. This removes the need for heavy data engineering during fine-tuning.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the paper:The paper proposes an unsupervised corpus-aware language model pre-training method called coCondenser that allows effective small batch fine-tuning of dense retrievers without relying on data engineering techniques like denoising, augmentation, or large batches.


## How does this paper compare to other research in the same field?

 Here are a few key points on how this paper compares to other research in dense retrieval:- This paper focuses on pre-training language models specifically for dense retrieval, which has been relatively unexplored until recently. Most prior work has focused on designing retrieval-specific fine-tuning techniques like RocketQA. A concurrent work, DPR-PAQ, also looks at pre-training but uses a large set of synthetic QA pairs rather than the unsupervised corpus-level pre-training proposed here.- The proposed coCondenser method aims to address two issues with typical language models: sensitivity to noise/mislabeling and difficulty learning an effective passage embedding space. It does this by incorporating the Condenser architecture and adding an unsupervised contrastive loss over the target corpus during pre-training.- Experiments show coCondenser achieves performance on par or better than RocketQA and other state-of-the-art methods on passage ranking benchmarks, while using much simpler fine-tuning (no data augmentation, filtering, etc). This highlights the effectiveness of the pre-training approach.- Comparisons to DPR-PAQ show coCondenser is competitive when using BERT base, despite DPR-PAQ's large semi-supervised pre-training data. However, DPR-PAQ shows the best results by combining its pre-training with RoBERTa large. This suggests supervised pre-training may still be better for maximizing performance when resources allow.- For real-world application, coCondenser provides an efficient way to pre-train once on a corpus then fine-tune easily for different tasks/queries. This removes much of the engineering effort required in prior work to realize the full potential of dense retrievers.So in summary, coCondenser demonstrates the promise of unsupervised pre-training for dense retrieval as an alternative to complex fine-tuning methods or large semi-supervised pre-training. The results and comparisons provide useful insights on the current state-of-the-art in this area.


## What future research directions do the authors suggest?

 Based on my reading of the paper, some potential future research directions the authors suggest are:- Exploring smarter augmentation techniques during pre-training, beyond just random span sampling, to better learn representations. The authors note they leave this for future work.- Integrating additional pre-training or fine-tuning techniques on top of coCondenser to further improve performance. The authors mention this could be an area for future work to investigate.- Applying coCondenser pre-training to other corpora beyond Wikipedia and MS-MARCO. The popularity and effectiveness of coCondenser for these two corpora suggests it may be worthwhile for other corpora as well.- Adapting coCondenser for cross-lingual retrieval by pre-training on corpora in multiple languages. The current work focuses on English but cross-lingual retrieval is an important area.- Exploring whether the ideas from coCondenser could transfer to other dense retrieval models besides BERT, like RoBERTa. - Analyzing in more detail the tradeoffs between unsupervised pre-training with coCondenser versus semi-supervised pre-training like in DPR-PAQ.- Testing coCondenser on a broader range of information retrieval tasks beyond passage ranking.So in summary, some key directions mentioned are exploring other pre-training techniques, applying it to new corpora/languages/models, more comparison to semi-supervised approaches, and evaluation on more tasks. The authors lay out a promising initial approach but suggest quite a few interesting ways it could be extended.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the key points from the paper:The paper proposes a new method called coCondenser for pre-training language models to make them more effective for dense passage retrieval tasks. The coCondenser method has two main components. First, it uses the Condenser architecture which conditions language model pre-training on learning good CLS representations that summarize the input text. Second, it adds an unsupervised corpus-level contrastive loss that brings passages from the same document closer together in the embedding space. Experiments on passage ranking for MS MARCO, Natural Questions, and TriviaQA show that models pre-trained with coCondenser achieve similar or better performance compared to state-of-the-art systems like RocketQA, with greatly reduced computational requirements during fine-tuning. The key advantages of coCondenser are that it makes models more robust to noise and learns a better structured embedding space in a completely unsupervised way, avoiding complex fine-tuning pipelines. Overall, the work demonstrates the effectiveness of designing specialized pre-training methods for improving dense retrievers.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the paper:The paper proposes a method to pre-train language models to make them more effective for dense passage retrieval. The authors identify two challenges when fine-tuning language models for dense retrieval: fragility to training data noise and the need for large batches to learn a robust embedding space. To address these issues, they utilize a recently proposed Condenser pre-training architecture, which learns to condense information into a dense vector through language model pre-training. This makes the model more robust to noise. They then propose coCondenser, which adds an unsupervised corpus-level contrastive loss during pre-training to warm up the passage embedding space. Experiments on passage retrieval datasets show that models pre-trained with coCondenser can be effectively fine-tuned with simple techniques like hard negative mining, small batches, and no data augmentation or filtering. Without heavy data engineering, coCondenser achieves comparable performance to state-of-the-art systems like RocketQA. The authors argue that coCondenser provides an easy way to pre-train language models for dense retrieval by making models noise resistant and providing a pre-structured embedding space. This removes the need for extensive data engineering during fine-tuning.


## Summarize the main method used in the paper in one paragraph.

 Here is a one paragraph summary of the main method used in the paper:The paper proposes a two-stage pre-training method called coCondenser for training dense passage retrievers. In the first stage, they use a model architecture called Condenser that learns to condense information into a dense vector through language model pre-training. Condenser produces an information-rich vector that is robust to noise. In the second stage, they build on top of Condenser and add an unsupervised corpus-level contrastive loss during pre-training. This contrastive loss trains the model so that vector representations of text spans from the same document are close together while spans from different documents are farther apart in the embedding space. This results in a corpus-aware language model pre-trained for dense retrieval that can be effectively fine-tuned on downstream tasks using small batches without needing heavy data engineering or model tuning. Experiments show coCondenser achieves strong performance on passage ranking datasets like MS MARCO, Natural Questions, and TriviaQA.


## What problem or question is the paper addressing?

 Based on my reading, the key points of this paper are:1. Fine-tuning dense retrievers (models that encode text into dense vectors for retrieval) to realize their full potential requires carefully designed fine-tuning techniques like denoising and large batch training. However, these techniques require significant computational resources that may not be feasible for many researchers. 2. The authors hypothesize that typical language models (LMs) are sensitive to mislabeling/noise in training data, and their CLS vectors are not trained explicitly to form good passage embeddings. The optimized training in recent work like RocketQA addresses these issues through denoising and large batches.3. Instead of directly using these expensive training techniques, the authors propose pre-training an LM called coCondenser to make it noise resistant and learn a structured embedding space. This is done through two modifications:(a) The Condenser architecture that focuses on conditioning LM prediction on the CLS vector to make it information rich. (b) Adding a corpus-level contrastive loss that brings embeddings of spans from the same document closer and those from different documents farther apart.4. By pre-training once on a target corpus (like Wikipedia), coCondenser can then be easily fine-tuned on downstream tasks using small batches, without needing data engineering. Experiments on passage ranking datasets show coCondenser matches or exceeds state-of-the-art performance.In summary, the key idea is to move insights from expensive dense retriever training into one-time pre-training for easier fine-tuning later. The pre-trained model is noise resistant and has a better passage embedding space.
