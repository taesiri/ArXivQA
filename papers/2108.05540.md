# [Unsupervised Corpus Aware Language Model Pre-training for Dense Passage   Retrieval](https://arxiv.org/abs/2108.05540)

## What is the central research question or hypothesis that this paper addresses?

The central hypothesis of this paper is that heavy data engineering and large batch training, as used in state-of-the-art dense retriever fine-tuning techniques like RocketQA, can be avoided by instead pre-training a language model to have two key properties:1) Noise resistance, so it is robust to mislabeling and other noise in the training data.2) A well-structured global embedding space, so passages are embedded close to related passages and far from unrelated ones. To achieve this, the authors propose a two-step pre-training method called coCondenser:1) Condenser pre-training, which uses a modified BERT architecture to produce an information-rich CLS representation robust to noise. 2) Corpus-aware contrastive pre-training, which adds an unsupervised contrastive loss to structure the passage embedding space in a corpus-aware way.The central hypothesis is that a model pre-trained this way will not need heavy data engineering or large batches when fine-tuned for dense retrieval, since it already has noise resistance and a useful embedding space. Experiments on passage ranking datasets seem to confirm this hypothesis.


## What is the main contribution of this paper?

Based on my reading, the main contributions of this paper are:1. It identifies two key challenges with training dense passage retrievers: fragility to training data noise and requiring large batches for learning a robust embedding space. 2. It proposes a new pre-training method called coCondenser to address these challenges. coCondenser combines the Condenser architecture with a corpus-level contrastive loss. 3. Condenser architecture learns to condense information into a dense vector through language model pre-training, making it more robust to noise. 4. The contrastive loss trains an embedding space over the corpus in an unsupervised way, removing the need for large batches during fine-tuning.5. Experiments on MS MARCO, Natural Questions, and TriviaQA show coCondenser achieves comparable or better performance than state-of-the-art systems like RocketQA, while using simple fine-tuning without data augmentation or large batches.In summary, the key contribution is a new pre-training method coCondenser that makes dense retrievers much easier and more robust to train by handling noise and embedding space issues upfront. This removes the need for heavy data engineering during fine-tuning.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper proposes an unsupervised corpus-aware language model pre-training method called coCondenser that allows effective small batch fine-tuning of dense retrievers without relying on data engineering techniques like denoising, augmentation, or large batches.


## How does this paper compare to other research in the same field?

Here are a few key points on how this paper compares to other research in dense retrieval:- This paper focuses on pre-training language models specifically for dense retrieval, which has been relatively unexplored until recently. Most prior work has focused on designing retrieval-specific fine-tuning techniques like RocketQA. A concurrent work, DPR-PAQ, also looks at pre-training but uses a large set of synthetic QA pairs rather than the unsupervised corpus-level pre-training proposed here.- The proposed coCondenser method aims to address two issues with typical language models: sensitivity to noise/mislabeling and difficulty learning an effective passage embedding space. It does this by incorporating the Condenser architecture and adding an unsupervised contrastive loss over the target corpus during pre-training.- Experiments show coCondenser achieves performance on par or better than RocketQA and other state-of-the-art methods on passage ranking benchmarks, while using much simpler fine-tuning (no data augmentation, filtering, etc). This highlights the effectiveness of the pre-training approach.- Comparisons to DPR-PAQ show coCondenser is competitive when using BERT base, despite DPR-PAQ's large semi-supervised pre-training data. However, DPR-PAQ shows the best results by combining its pre-training with RoBERTa large. This suggests supervised pre-training may still be better for maximizing performance when resources allow.- For real-world application, coCondenser provides an efficient way to pre-train once on a corpus then fine-tune easily for different tasks/queries. This removes much of the engineering effort required in prior work to realize the full potential of dense retrievers.So in summary, coCondenser demonstrates the promise of unsupervised pre-training for dense retrieval as an alternative to complex fine-tuning methods or large semi-supervised pre-training. The results and comparisons provide useful insights on the current state-of-the-art in this area.


## What future research directions do the authors suggest?

Based on my reading of the paper, some potential future research directions the authors suggest are:- Exploring smarter augmentation techniques during pre-training, beyond just random span sampling, to better learn representations. The authors note they leave this for future work.- Integrating additional pre-training or fine-tuning techniques on top of coCondenser to further improve performance. The authors mention this could be an area for future work to investigate.- Applying coCondenser pre-training to other corpora beyond Wikipedia and MS-MARCO. The popularity and effectiveness of coCondenser for these two corpora suggests it may be worthwhile for other corpora as well.- Adapting coCondenser for cross-lingual retrieval by pre-training on corpora in multiple languages. The current work focuses on English but cross-lingual retrieval is an important area.- Exploring whether the ideas from coCondenser could transfer to other dense retrieval models besides BERT, like RoBERTa. - Analyzing in more detail the tradeoffs between unsupervised pre-training with coCondenser versus semi-supervised pre-training like in DPR-PAQ.- Testing coCondenser on a broader range of information retrieval tasks beyond passage ranking.So in summary, some key directions mentioned are exploring other pre-training techniques, applying it to new corpora/languages/models, more comparison to semi-supervised approaches, and evaluation on more tasks. The authors lay out a promising initial approach but suggest quite a few interesting ways it could be extended.


## Summarize the paper in one paragraph.

Here is a one paragraph summary of the key points from the paper:The paper proposes a new method called coCondenser for pre-training language models to make them more effective for dense passage retrieval tasks. The coCondenser method has two main components. First, it uses the Condenser architecture which conditions language model pre-training on learning good CLS representations that summarize the input text. Second, it adds an unsupervised corpus-level contrastive loss that brings passages from the same document closer together in the embedding space. Experiments on passage ranking for MS MARCO, Natural Questions, and TriviaQA show that models pre-trained with coCondenser achieve similar or better performance compared to state-of-the-art systems like RocketQA, with greatly reduced computational requirements during fine-tuning. The key advantages of coCondenser are that it makes models more robust to noise and learns a better structured embedding space in a completely unsupervised way, avoiding complex fine-tuning pipelines. Overall, the work demonstrates the effectiveness of designing specialized pre-training methods for improving dense retrievers.
