# [Qibo: A Large Language Model for Traditional Chinese Medicine](https://arxiv.org/abs/2403.16056)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Existing large language models (LLMs) perform poorly in the unique domain of traditional Chinese medicine (TCM) due to lack of specialized training data and incorporation of core TCM concepts like "same treatment for different diseases" and "different treatments for same disease". 

- Other Chinese medical LLMs focus only on modern medicine and do not account for the different theoretical foundations between TCM and modern medicine in areas like diagnosis and treatment.

Solution:
- The authors propose Qibo, the first LLaMA-based large language model specialized for the TCM domain that is trained end-to-end from pre-training to supervised fine-tuning (SFT). 

- Qibo is pre-trained on a variety of TCM corpora spanning textbooks, prescriptions, comprehension quizzes etc. to build strong TCM capabilities.

- The model is then fine-tuned using TCM dialog data, NLP tasks data and general medical dialog data to enhance its conversational and multi-task abilities while preventing catastrophic forgetting.

- A specialized multi-stage data processing workflow with custom rules handles the intricacies of TCM text data.

Key Contributions:
- First large language model for TCM that covers complete pipeline from pre-training to supervised fine-tuning using the LLaMA architecture.

- Introduction of a multi-stage TCM data processing flow with custom cleaning rules tuned for ancient Chinese medical texts.

- Compilation of specialized TCM benchmark called Qibo-benchmark for evaluating capabilities of models in the TCM domain across various axes like safety, accuracy etc.

- Experiments demonstrate strong performance of Qibo across subjective and objective TCM assessments, outperforming baseline generalized & medical LLMs.

The model aims to fill the gap in developing performant TCM focused large language models while acknowledging limitations w.r.t safety and accuracy over reliance.


## Summarize the paper in one sentence.

 This paper presents Qibo, the first Traditional Chinese Medicine large language model developed on LLaMA that implements the full pipeline from pre-training to supervised fine-tuning, outperforming other medical models in TCM domains.


## What is the main contribution of this paper?

 Based on the content of the paper, the main contributions are:

1. The authors trained a new large language model specifically for the domain of traditional Chinese medicine (TCM). This is the first LLaMA-based implementation of a TCM model that goes through the full pipeline from pre-training to supervised fine-tuning.

2. The authors provide a data cleaning scheme with different granularity rules, including special rules for processing ancient Chinese medicine texts. 

3. The authors constructed an assessment benchmark specialized for evaluating TCM language models, covering aspects like basic knowledge competence, ability to recognize Chinese medicines, ability to comprehend TCM texts, etc.

4. The authors conducted experiments to demonstrate that their proposed model, Qibo, achieves excellent performance on tasks in the TCM domain, outperforming previous Chinese medical language models.

In summary, the main contributions are proposing a new TCM-specialized language model, a data cleaning methodology, a specialized evaluation benchmark, and empirical experiments showing the strong performance of their model. The key highlight is developing the first complete LLaMA-based TCM language model with customized data processing and evaluation.


## What are the keywords or key terms associated with this paper?

 Based on my review of the paper, some of the key terms and concepts related to this work include:

- Traditional Chinese medicine (TCM)
- Large language models (LLMs) 
- Pre-training
- Supervised fine-tuning (SFT)
- Qibo - the name of the TCM LLM model developed in this paper
- LLaMA - the base LLM model that Qibo is built on 
- Data cleaning/processing 
- TCM knowledge graphs
- TCM textbooks and ancient books  
- TCM corpora
- TCM NLP tasks (e.g. entity recognition, reading comprehension)
- Evaluation benchmark (Qibo-benchmark)
- Safety, professionalism, fluency - evaluation metrics
- Limitations of Qibo model

In summary, the key focus of this paper is on developing a large language model specially adapted for the domain of traditional Chinese medicine, called Qibo. It highlights the unique challenges in TCM and differences from modern medicine. The methods involve pre-training the model on TCM datasets, fine-tuning with supervision, and constructing specialized benchmarks for evaluation.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in the paper:

1. The paper mentions that traditional Chinese medicine (TCM) uses more abstract concepts like Yin-Yang and Hot-Cold to diagnose patients. How does the pre-training data help Qibo learn these abstract TCM concepts that are different from modern medicine?

2. The paper uses a hybrid training strategy with both full-precision and mixed-precision training. What are the tradeoffs with this approach? Why was mixed-precision alone not sufficient?  

3. What additional challenges did the ancient texts in the pre-training data pose for data cleaning? How were special rules devised to handle these texts?

4. The paper employs data from multiple sources for fine-tuning, including general medical dialogues. Why is it important to include both medical domain data as well as general domain data?

5. The Qibo model relies primarily on textual data. What are some key limitations of not incorporating other modalities like medical images? How can future work address this?

6. The paper introduces a new benchmark called Qibo-benchmark for evaluating TCM domain competence. What are some key differences in how this benchmark evaluates models compared to existing methods? 

7. What are some potential safety risks with the information Qibo generates? How does the subjective evaluation methodology used in the paper help mitigate these risks?

8. The paper finds scale effects in model performance, with larger models doing better. What are some ways model scale could be further exploited in future iterations of Qibo?

9. Reinforcement learning from human feedback (RLHF) is noted to improve model performance. Why was this not implemented for the Qibo model and how could it be incorporated in the future?

10. What ethical considerations should be kept in mind as conversational models like Qibo are developed for the medical domain? How were ethical factors addressed in the design and training methodology used for Qibo?
