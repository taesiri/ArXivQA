# [An Emulator for Fine-Tuning Large Language Models using Small Language   Models](https://arxiv.org/abs/2310.12962)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question seems to be:

What capabilities arise from independently scaling pre-training versus fine-tuning of language models?

The authors introduce a method called "emulated fine-tuning" (EFT) which allows them to combine the knowledge learned during pre-training of a model at one scale, with the knowledge learned during fine-tuning of a model at a different scale. 

Using EFT, they can directly study the impact of scaling up or down either the pre-training or fine-tuning stages. The main hypothesis appears to be that scaling up pre-training will lead to improved factuality, while scaling up fine-tuning will lead to improved task performance and helpfulness.

In summary, the central research question is: how do capabilities change when scaling pre-training versus fine-tuning independently? And the key hypothesis is that pre-training scale impacts factuality while fine-tuning scale impacts helpfulness/task performance. The paper introduces EFT to test this hypothesis through controlled experiments.


## What is the main contribution of this paper?

 The main contribution of this paper is the introduction of a technique called "emulated fine-tuning" (EFT). EFT provides a way to decouple the scale of pre-training from the scale of fine-tuning in large language models. 

Specifically, EFT enables:

- Directly studying the capabilities that emerge from scaling up pre-training vs fine-tuning independently. Experiments using EFT provide evidence that scaling up pre-training improves factuality, while scaling up fine-tuning improves helpfulness.

- Approximating fine-tuning a large model using a small fine-tuned model and large pre-trained model, without actually having to fine-tune the large model. This is called "up-scaling" and can be practically useful when fine-tuning a small model is feasible but a large one is not. 

- Modifying the fine-tuning objective at test time without additional training, such as adjusting the tradeoff between helpfulness and harmlessness.

The key idea behind EFT is to factor a fine-tuned model's logits into a) base log probabilities from the pre-trained model and b) a "behavior delta" capturing the changes from fine-tuning. By substituting different scale base models and behavior deltas, the result of pre-training and fine-tuning at different scales can be emulated.

Overall, EFT provides a principled way to study and make use of the different knowledge gained in pre-training vs fine-tuning language models. The techniques enable new analyses and applications around scaling these different stages.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 The paper introduces a method called emulated fine-tuning (EFT) that allows decoupling the scale of pre-training and fine-tuning of language models. The key idea is to represent a fine-tuned model as a pre-trained base model reweighted by the ratio of the fine-tuned and base model probabilities. This allows combining a large pre-trained base model with adjustments learned from fine-tuning a small model, enabling study of their separate effects and approximating fine-tuning a large model without doing the expensive computation.


## How does this paper compare to other research in the same field?

 Here is a brief analysis of how this paper compares to other research in the field of language model training:

This paper introduces a novel technique called "emulated fine-tuning" (EFT) for decoupling the scale of pre-training from the scale of fine-tuning in large language models. The key idea is to decompose a fine-tuned model into a pre-trained base model and a "behavior delta" capturing the changes from fine-tuning. This allows mixing and matching different scales of pre-training and fine-tuning. 

The EFT technique relates to other work on understanding scale effects in LM pre-training vs fine-tuning:

- It formalizes and generalizes some ideas from prior work like "contrastive decoding" which also subtracted log probabilities of different scaled models. However, EFT provides a more principled basis in reinforcement learning for this operation.

- Experiments on scaling pre-training vs fine-tuning align with findings in some prior studies, though EFT enables more direct attribution of capabilities to the training stages.

- The technique of "up-scaling" small fine-tuned models relates to other work on distilling or compressing large LM knowledge into smaller models. But EFT up-scaling doesn't require separate training.

So in summary, EFT contributes a novel technique for flexibly combining scale, enables clearer analysis of scale effects, and introduces efficient up-scaling without training a separate student model. The experiments confirm and strengthen some findings from prior work. Overall it represents an advance in methodology for studying scale in LM training.
