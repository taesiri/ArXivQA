# [Learning Planning-based Reasoning by Trajectories Collection and Process   Reward Synthesizing](https://arxiv.org/abs/2402.00658)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- Large language models (LLMs) have shown promise in complex reasoning tasks by generating step-by-step rationales. However, they are susceptible to hallucination and flaws in the reasoning process.
- Existing approaches that model reasoning as planning suffer from high latency due to frequent assessment of intermediate states and a large exploration space. Supervising the reasoning process with human annotation is costly and difficult to scale.

Proposed Solution: 
- The paper proposes a framework to learn planning-based reasoning by collecting trajectories and optimizing based on synthesized process rewards. 
- It first collects seed trajectories from an LLM policy. Then it samples intermediate states from these trajectories and asks the LLM to complete them multiple times. 
- Based on how often completions reach the correct outcome, it assigns "raw rewards" to intermediate states. A process reward model is trained on these rewards.
- More trajectories are collected and ranked by the process reward model. The policy LLM is optimized via direct preference optimization on good vs bad trajectories.

Main Contributions:
- A novel framework to transform reasoning-as-planning to a learning problem using offline simulation and trajectory collection.
- An approach to synthesize coarse-grained process rewards without teacher models or human annotation.
- Evaluation on LogiQA and ReClor datasets shows significant improvements over strong baselines. GPT-4 assessment indicates improved reasonableness and conciseness of rationales.
- The key idea is to utilize partial trajectory exploration and outcome supervision to guide policy optimization, avoiding online search.

In summary, the paper presents an effective framework to improve reliability of LLM-generated rationales for reasoning tasks in an efficient, scalable manner. The core innovation is in synthesizing process supervision without additional annotation.


## Summarize the paper in one sentence.

 This paper proposes a framework to learn planning-based reasoning for language models by collecting trajectories, synthesizing process rewards without human annotation, and optimizing the policy model using direct preference learning.


## What is the main contribution of this paper?

 According to the paper, the main contributions are:

1) Formulating a novel offline training framework by collecting trajectories and direct preference optimization based on outcome supervision. 

2) Proposing an approach to synthesize coarse-grained process rewards without the help of teacher models or human annotation.

3) Evaluating the proposed methods on two challenging logical reasoning benchmarks. The results demonstrate significant improvements compared to strong baseline models, verifying the effectiveness of the proposed method. The quality of generated rationales is also evaluated by GPT-4, showing particular improvement on reasonableness and conciseness.

So in summary, the key contributions are: the offline training framework, the process reward synthesis approach, and the evaluations demonstrating effectiveness on logical reasoning tasks and rationale quality.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, I would suggest the following key terms and keywords associated with this paper:

- Large Language Models (LLMs)
- Logical reasoning
- Process supervision
- Planning-based reasoning
- Trajectories collection
- Process reward synthesizing  
- Direct preference optimization (DPO)
- Markov Decision Process (MDP)
- Monte Carlo Tree Search (MCTS)
- Reinforcement learning from human feedback (RLHF)

The paper proposes a framework to learn planning-based reasoning for LLMs by collecting reasoning trajectories and synthesizing process rewards without direct human supervision. It models the reasoning process as an MDP and explores offline simulation and trajectory collection to estimate rewards at intermediate reasoning steps. The LLMs are then optimized via direct preference optimization on trajectory pairs ranked by the synthesized process rewards. The approach is evaluated on logical reasoning benchmarks like LogiQA and ReClor.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in this paper:

1. The paper proposes a novel framework to learn planning-based reasoning by collecting and optimizing trajectories. Could you explain in more detail the motivation behind formulating reasoning as a planning problem and how this is implemented in the paper?

2. The paper mentions using "offline simulation" to synthesize process supervision signals. Could you expand on what offline simulation refers to in this context and how it differs from online planning methods like Monte Carlo Tree Search? 

3. One key contribution is using "explore-and-play" to estimate rewards for intermediate reasoning states. What is the intuition behind this approach? And what are some ways the noise in the reward estimates could be reduced?

4. The process reward model is intended to alleviate issues with the raw reward estimates. What are some ways this model could be improved or expanded on to provide better training signals?

5. The paper employs Direct Preference Optimization (DPO) to optimize the policy model. Why was DPO chosen over other RL algorithms like PPO? And are there any modifications made to the DPO algorithm for this framework?

6. How does the paper evaluate whether the synthesized process rewards actually lead to more reasonable and reliable rationales compared to just using outcome supervision? Could additional analysis be done?

7. One interesting result is that the LogiQA dataset led to better generalization than the ReClor dataset. What differences between these datasets could explain this result?

8. How dependent is the overall framework on the availability of ground truth outcome labels for backpropagation? Could the method be adapted for settings without explicit outcome supervision?  

9. The paper focuses specifically on logical reasoning tasks. What modifications would need to be made to apply this framework to other complex reasoning domains?

10. What are some promising future directions for improving or building upon this trajectory collection and preference learning approach? For example, incorporating model-based simulations or self-supervised objectives.
