# [Weight subcloning: direct initialization of transformers using larger   pretrained ones](https://arxiv.org/abs/2312.09299)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem Statement
- Training large transformer models from scratch is computationally expensive and requires large datasets. Usually models are initialized with weights from a pretrained model of the same size to enable faster convergence.  
- However, what if no pretrained model of the required smaller size is available for a target task? How can we transfer knowledge from a larger pretrained model to initialize a smaller model?

Proposed Solution: Weight Subcloning
- Introduce a technique called "weight subcloning" to initialize a smaller "destination" transformer model using weights derived from a larger "parent" pretrained model. 
- Allows destination model to have fewer layers and/or smaller embedding dimensions per layer compared to parent.
- Aims to significantly speed up training convergence of destination model.

Key Insights Enabling Weight Subcloning:
- Residual transformer blocks only induce small changes to hidden representations between layers (additive residual property)
- This allows removing or duplicating certain blocks without significantly altering network function
- Enables creating destination model with fewer/more layers than parent
- Consistent neuron importance ranking patterns exist across layers 
- Enables methodically reducing embedding dimensions by preserving most important neurons

Main Steps of Weight Subcloning:
1) Remove middle transformer blocks from parent to match number of layers in destination
2) Reorder neuron importance consistently across residual-connected layers 
3) Sample portion of weights/biases from parent to initialize destination
4) Add weight scaling to match key stats between models

Benefits:
- Significantly faster convergence and reduced training time e.g. 4x speedup
- Higher accuracy in limited training epochs
- Simple technique with negligible additional cost

Main Contributions:
- Concept of weight subcloning from larger pretrained transformers
- Techniques to change depth and width of transformers 
- Neuron reordering method based on importance ranking
- Demonstrating accelerated convergence across modalities
