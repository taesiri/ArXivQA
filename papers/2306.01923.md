# [The Surprising Effectiveness of Diffusion Models for Optical Flow and   Monocular Depth Estimation](https://arxiv.org/abs/2306.01923)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be:Can diffusion models be effective for optical flow and monocular depth estimation, despite not having specialized architectures or losses designed for these tasks?The key hypotheses appear to be:1) Diffusion models can achieve competitive or state-of-the-art results on optical flow and monocular depth estimation benchmarks, using only a generic image-to-image translation framework.2) Diffusion models can capture uncertainty and multimodality in optical flow and depth estimation, unlike typical regression-based approaches. 3) Diffusion models enable useful applications like iterative 3D scene generation, by allowing sampling of conditional distributions and imputation of missing values.So in summary, the paper aims to demonstrate the surprising effectiveness of generic diffusion models on optical flow and monocular depth tasks, highlighting their ability to model uncertainty and enable new applications compared to specialized regression-based approaches. The experiments and results focus on validating these hypotheses.


## What is the main contribution of this paper?

The main contributions of this paper seem to be:- Formulating optical flow and monocular depth estimation as image-to-image translation using generic diffusion models, without specialized architectures or loss functions.- Identifying key challenges with training diffusion models on real-world data with noisy/incomplete labels, and proposing solutions like infilling missing values, step-unrolled training, and using an L1 loss.- Showing strong results on optical flow and depth benchmarks, including state-of-the-art performance on KITTI optical flow. The diffusion model gets an Fl-all outlier rate of 3.26% on KITTI, about 25% better than prior work.- Demonstrating additional benefits of the diffusion modeling approach, like capturing uncertainty and multi-modality in the predictions, and enabling zero-shot applications like coarse-to-fine refinement and imputation of missing values.Overall, the key contribution seems to be showing that with proper training techniques, generic diffusion models can achieve excellent performance on specialized vision tasks like optical flow and depth estimation, without task-specific model components. The results also highlight the benefits of probabilistic modeling and sampling for these tasks.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper proposes using generic denoising diffusion probabilistic models for monocular depth and optical flow estimation, achieving strong results without specialized model architectures or losses; the diffusion framework also enables representing uncertainty and multimodality.


## How does this paper compare to other research in the same field?

Here are a few key thoughts on how this paper compares to other research in the field of optical flow and monocular depth estimation:- The main novelty of this paper is applying diffusion models to optical flow and depth estimation, framing them as image-to-image translation problems. This is a departure from most prior work which uses specialized model architectures and losses optimized for these tasks. - For optical flow, this paper shows diffusion models can achieve competitive or state-of-the-art results compared to specialized models like RAFT. This is impressive given the simplicity of the diffusion framework. The ability to capture uncertainty and multimodality is also unique.- For monocular depth, this paper achieves very strong results on NYU using a simple U-Net architecture. They don't quite match the accuracy of recent specialized models like BinsFormer, but still perform very well. The flexibility to handle missing data is notable.- A key contribution is the training procedure and data used. The paper highlights the importance of diverse synthetic pre-training data, and proposes techniques to handle sparse/noisy real data. The results validate these methods.- One limitation is that diffusion models are slower at inference time compared to specialized models. But the paper suggests distillation could help.Overall, I'd say the main value of this paper is demonstrating the surprising effectiveness of diffusion models on fundamental vision tasks using simple models and training techniques. It shows these generic models can compete with highly engineered ones. The ability to capture uncertainty is also important, though more work is needed to improve efficiency.


## What future research directions do the authors suggest?

Based on my reading of the paper, here are some of the main future research directions suggested by the authors:- Improving the efficiency and speed of diffusion models for vision tasks. The authors note that the iterative sampling process makes these models much slower than classical approaches like regression networks. They suggest research into techniques like progressive distillation to compress the models while preserving quality.- Developing better techniques for coarse-to-fine refinement with diffusion models. The simple patch-based approach they used provided some gains but there is room for more sophisticated approaches to be developed.- Exploring different mixtures of real and synthetic datasets for pre-training. The authors used a simple greedy strategy but suggest an optimal mixing strategy could further improve results.- Applying diffusion models to other dense prediction vision tasks beyond optical flow and depth estimation. The authors propose these models could be a generic framework applicable to many dense vision problems.- Studying uncertainty modeling and multimodality more rigorously. The authors provide some qualitative results showing diffusion can capture uncertainty but suggest more analysis is needed.- Developing better techniques for handling sparse/noisy data during training, beyond infilling and step-unrolling. Other approaches like curricula or consistency regularization may help further.- Optimizing the fine-tuning procedures, especially for domain gaps between pre-training and target datasets. The suboptimal Sintel fine-tuning results indicate this needs more work.- Leveraging diffusion models for applications like novel view synthesis, 3D reconstruction, etc by building on the depth completion capabilities.In summary, the authors propose many promising research directions to build on their work applying diffusion models to dense vision tasks like optical flow and depth estimation. There are many open problems related to efficiency, datasets, applications, etc.
