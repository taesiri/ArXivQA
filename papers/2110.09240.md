# [Value alignment: a formal approach](https://arxiv.org/abs/2110.09240)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem Addressed:
The paper addresses the value alignment problem in AI systems - how to ensure autonomous AI systems behave according to human values. Specifically, it looks at formally defining what it means for an AI system's goals and behaviors to be aligned with human values. 

Proposed Solution:
The paper proposes a formal model to represent human values as preferences over states of the world. It defines value alignment through quantifying the increase/decrease in preferences that result from an AI system's actions. The key components are:

- Values are defined as cognitive preferences over states of the world with respect to desired goals that transcend specific actions. A value-based preference function defines quantitatively how much one state is preferred over another.

- Value aggregation functions allow combining multiple value-based preferences - either multiple values for one agent or preferences of multiple agents for one value.

- Norms are defined as rules that govern behavior by modifying outcomes of actions using incentives. Value alignment of a norm is then formalized as the degree to which the norm moves the system to more preferred states.

- An computational model is presented to numerically calculate value alignment over sampled paths in a system by aggregating the changes in value-based preferences.


Main Contributions:

- A formal cognitive model of human values as preferences over system states
- Quantitative definitions of value alignment grounded in changes in value-based preferences
- Computational model and algorithms to numerically estimate value alignment in an AI system
- Framework to study research questions around optimal norms, agent groups, and preference aggregations for value alignment

The model provides a foundation to make concepts like values and value alignment in AI systems concrete and quantifiable for engineering purposes.


## Summarize the paper in one sentence.

 Here is a one-sentence summary of the paper:

The paper provides a formal model to represent values as preferences over states of the world and defines value alignment through the increase/decrease of these preferences resulting from the application of norms that govern behavior.


## What is the main contribution of this paper?

 According to the paper, the main contribution is providing a formal model to represent values through preferences over states of the world, and defining value alignment through the increase/decrease of these preferences when norms are applied. Specifically, the paper:

- Formally defines values as preferences of agents over pairs of states of the world with respect to a value.

- Defines different aggregation functions to combine values across multiple values for a single agent, across multiple agents for a single value, and across both multiple agents and multiple values. 

- Formalizes the concept of a normative world where norms modify the transitions and resulting states. 

- Defines the degree of alignment of a norm with a value as the average change in value-based preferences over possible paths in the normative world resulting from applying that norm.

- Defines the relative alignment of two norms with respect to a value.

- Provides an example instantiating the definitions and calculating alignment degrees for different norms and preference functions representing a value in the context of the Prisoner's Dilemma game.

So in summary, the main contribution is the formal modeling of values, preferences and value alignment to reason about normative systems.


## What are the keywords or key terms associated with this paper?

 Based on my analysis, some of the key terms and concepts associated with this paper include:

- Value alignment - The paper focuses on formally defining and quantifying the concept of value alignment in AI systems, specifically the alignment between an AI system's goals/behavior and human values.

- Preferences - Values are formally modeled as preferences over states of the world. Different preference functions represent different interpretations of values.  

- Norms - Since behavior is governed by norms, value alignment is defined in terms of the alignment between norms and values. The paper analyzes how different norms impact value alignment.

- Consequentialism - The paper takes a consequentialist perspective on values, judging their worth based on the outcomes of actions aligned with them. 

- Aggregation functions - Functions for aggregating preferences over multiple values for a single agent, or multiple agents' preferences for a single value. Ensuring coherence between different aggregation orders.

- Socio-cognitive technical systems - The hybrid systems comprising autonomous agents interacting in a norm-regulated, open environment. The space where value alignment is analyzed.

- Quantifying alignment - Formally defining mathematical functions to quantify the degree of alignment between a norm and a value.

So in summary, the key focus is on formally modeling and quantifying value alignment in sociotechnical AI systems in terms of preferences, norms, and outcomes.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in this paper:

1. The paper proposes a formal model to represent values as preferences over states of the world. How does this definition of values allow assessing the "goodness" of states and deciding which action is better? What are the limitations of this consequentialist view of values?

2. The paper discusses different ways to aggregate preferences - one's preference with respect to a set of values, a group's preference with respect to a value, and a group's preference with respect to a set of values. What are the challenges in defining aggregation functions that are coherent across these different types of preferences? 

3. The definition of alignment of a norm with a value quantifies the accumulated preferences in the resulting normative world. How does this capture whether a norm fosters behavior in accordance with a value? What are limitations of using accumulated preferences to measure alignment?

4. The paper suggests using Monte Carlo sampling to estimate alignment since calculating alignment over all possible paths in a normative world is inefficient. What are the trade-offs in using Monte Carlo sampling? How does the choice of number of sampled paths and path length impact the accuracy of alignment measurement?

5. How does the assumption that all transitions are equiprobable impact the calculation of alignment? What additional knowledge about the world could be incorporated to improve alignment measurement?

6. The paper defines relative alignment to compare how aligned two different norms are with respect to a value. What other metrics could be useful in comparing different candidate norms?

7. What open questions remain in defining the different preference aggregation functions described in the paper? What research directions could help better understand emergence of social values from individual values?  

8. The example applies the method to quantify alignment of different tax norms with an equality value in an iterated Prisoner's dilemma. What other game theory examples could serve as good testbeds for this method?

9. The paper formulates some formal questions that can be studied given information about degree of alignment between norms, values and groups of agents. What other analyses could this enable about agent societies and norms? 

10. How could the method proposed be expanded to account for values that are complex and multidimensional rather than being defined as a preference order over states?
