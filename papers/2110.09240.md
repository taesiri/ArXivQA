# [Value alignment: a formal approach](https://arxiv.org/abs/2110.09240)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem Addressed:
The paper addresses the value alignment problem in AI systems - how to ensure autonomous AI systems behave according to human values. Specifically, it looks at formally defining what it means for an AI system's goals and behaviors to be aligned with human values. 

Proposed Solution:
The paper proposes a formal model to represent human values as preferences over states of the world. It defines value alignment through quantifying the increase/decrease in preferences that result from an AI system's actions. The key components are:

- Values are defined as cognitive preferences over states of the world with respect to desired goals that transcend specific actions. A value-based preference function defines quantitatively how much one state is preferred over another.

- Value aggregation functions allow combining multiple value-based preferences - either multiple values for one agent or preferences of multiple agents for one value.

- Norms are defined as rules that govern behavior by modifying outcomes of actions using incentives. Value alignment of a norm is then formalized as the degree to which the norm moves the system to more preferred states.

- An computational model is presented to numerically calculate value alignment over sampled paths in a system by aggregating the changes in value-based preferences.


Main Contributions:

- A formal cognitive model of human values as preferences over system states
- Quantitative definitions of value alignment grounded in changes in value-based preferences
- Computational model and algorithms to numerically estimate value alignment in an AI system
- Framework to study research questions around optimal norms, agent groups, and preference aggregations for value alignment

The model provides a foundation to make concepts like values and value alignment in AI systems concrete and quantifiable for engineering purposes.
