# [ProtoFL: Unsupervised Federated Learning via Prototypical Distillation](https://arxiv.org/abs/2307.12450)

## What is the central research question or hypothesis that this paper addresses?

This paper proposes a new federated learning framework called "ProtoFL" to address key challenges in federated learning for one-class classification, including high communication costs, limited representation power, and unstable learning processes. The central hypothesis is that by using prototype representation distillation and normalizing flows for local classifier learning, the proposed ProtoFL method can:1) Enhance the representation power of the global model and reduce communication costs by distilling prototype representations instead of directly transferring parameters between clients and server.2) Improve one-class classification performance with limited local data by using normalizing flows to estimate the density of the target class distribution.In summary, the central hypothesis is that ProtoFL with prototype distillation and normalizing flows can enable more efficient and effective federated learning for one-class classification compared to existing approaches. The experiments aim to validate whether ProtoFL achieves superior performance over previous methods on both image and tabular datasets.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contributions are:- Proposing ProtoFL, a novel unsupervised federated learning framework that addresses challenges like insufficient local training data, high communication costs, and limited representation power. This is done through prototypical representation distillation and using normalizing flows for local classifier learning.- Introducing a prototype-based representation learning method that distills normal data representation from an off-the-shelf model and dataset. This allows scalability of the global model when adding new clients in federated learning based one-class classification. - Proposing new federated and centralized learning methods for one-class classification, which are evaluated on five widely used benchmarks. The experiments demonstrate superior performance over previous methods.In summary, the key contribution is developing ProtoFL, a new approach to unsupervised federated learning that enables efficient and effective global model updates through prototypical representation distillation and flow-based local classifier learning. This provides a novel solution to challenges in federated learning-based one-class classification.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper proposes a new federated learning framework called ProtoFL that uses prototypical representation distillation and normalizing flows to improve representation learning and reduce communication costs for one-class classification tasks.


## How does this paper compare to other research in the same field?

Here are a few key points comparing this paper to other research in federated learning for one-class classification:- This paper proposes a new method called ProtoFL that distills prototypical representations to enhance the global model in federated learning settings with non-i.i.d. data. This is a novel approach not explored in prior federated learning papers for one-class classification. - Previous federated learning methods like FedUV, FedMetric, and FedAwS rely on error-correcting codes or geometric regularization techniques to train a one-class model. In contrast, ProtoFL uses an off-the-shelf model and dataset to distill prototypical representations for each client. This helps compensate for limited local data.- The paper demonstrates superior performance of ProtoFL over previous federated learning methods on image classification benchmarks like MNIST, CIFAR-10/100, and ImageNet-30. The gains are substantial both in terms of accuracy and communication efficiency.- This is the first work to investigate using federated learning for one-class classification. Prior work focused on centralized learning. The paper shows federated learning can outperform centralized methods by collaboratively training a global model.- The paper also proposes a new flow-based one-class classifier to estimate density on each client. This approach is more suitable for federated learning compared to prior centralized density estimation methods.- Analyses demonstrate the scalability of ProtoFL's representations to new clients. This property is not examined by previous federated learning papers.In summary, this paper introduces a novel federated learning approach via prototypical distillation and flow-based classification that advances the state-of-the-art for one-class classification under decentralized data settings. The gains over prior methods are significant.


## What future research directions do the authors suggest?

Based on my reading of the paper, some of the main future research directions suggested by the authors are:- Investigating the use of FL for one-class classification in other domains beyond computer vision, such as audio, natural language processing, etc. The authors suggest that their approach could be promising for other types of data.- Exploring different model architectures and training techniques for the local one-class classifiers. The authors used normalizing flows in their framework but suggest other types of density estimators could be explored.- Evaluating the performance when allowing some limited data sharing between clients, instead of the extreme non-IID setting used in the paper. The authors suggest this could help further improve performance.- Considering personalization techniques to better adapt the global model to each client's local data distribution. This could help improve client-level performance.- Validating the approach on larger-scale and more complex datasets. The authors suggest experiments on larger benchmarks and real-world datasets to further demonstrate the effectiveness.- Investigating privacy-preserving techniques for the prototype representations to avoid leaking any private information. This could be important for real-world deployment.- Exploring the use of self-supervised or semi-supervised learning to further improve the learned representations in the federated setting.In summary, the main future directions are around applying the approach to new domains and data types, evaluating on larger/real-world datasets, improving the individual components like the local classifiers, and enhancing privacy protections. The authors lay out a promising research direction for advancing federated learning for one-class classification.


## Summarize the paper in one paragraph.

The paper proposes ProtoFL, an unsupervised federated learning framework for one-class classification that utilizes prototypical representation distillation and normalizing flows. The key ideas are:- Propose ProtoFL, a two-phase unsupervised learning method combining federated learning via prototypical representation distillation (ProtoFL) and local classifier learning via normalizing flows (OC-NF). - In ProtoFL phase, distribute prototype representations from an off-the-shelf model to clients to compensate for limited local data. Clients train local models using cosine similarity and KL divergence losses to match prototypes. Aggregate models into an expressive global model.- In OC-NF phase, clients train invertible normalizing flow models as local one-class classifiers by maximizing likelihood and minimizing divergence between augmented examples.- Evaluated on image and tabular benchmarks. ProtoFL outperforms previous federated and centralized methods, achieves lower communication cost, and shows scalability to new clients. First investigation of federated learning for one-class classification.In summary, the key contribution is an unsupervised federated learning approach for one-class classification that distills prototypical representations to improve model expressiveness and leverages normalizing flows for effective local density estimation. Evaluations demonstrate superior performance over existing methods.


## Summarize the paper in two paragraphs.

Here is a two paragraph summary of the paper:This paper proposes ProtoFL, a novel federated learning framework for one-class classification that aims to enhance the representation power of a global model and reduce communication costs. The key ideas are 1) prototypical representation distillation, where representation prototypes generated from an off-the-shelf model are distilled into each client's local model to compensate for limited local data, and 2) a local one-class classifier based on normalizing flows, which estimates the density of the target class. Specifically, the framework has two phases - federated representation learning via prototypical distillation, and local classifier learning via normalizing flows. In the first phase, clients collaboratively learn a shared global model by distilling prototypical representations from an off-the-shelf model, while preserving privacy. This allows the global model to represent all classes effectively despite limited local data. In the second phase, each client trains a normalizing flow-based one-class classifier locally using the global model's features. Extensive experiments on image and tabular benchmarks demonstrate superior performance over previous methods, with significantly lower communication costs. Key benefits are scalability to new clients and compatibility with various one-class classifiers.


## Summarize the main method used in the paper in one paragraph.

The paper proposes ProtoFL, a novel federated learning framework for unsupervised learning that improves representation learning and reduces communication costs for one-class classification. The key ideas are:- ProtoFL has two phases - prototypical representation distillation based federated learning (ProtoFL) and local one-class classifier learning via normalizing flows (OC-NF). - In ProtoFL, an off-the-shelf model generates prototype representations for each client using an off-the-shelf dataset. Clients then train local models to match these prototypes via distillation and similarity losses. A global model aggregates these local models.- In OC-NF, clients use normalizing flows to estimate density of local target data in the latent space of the global model. This is done by maximizing log-likelihood and adding a similarity regularization loss.- This two-phase approach allows learning an expressive global model with limited local data and communication rounds. The global representation transfers well to OC-NF for anomaly detection without further communication.- Experiments on image and tabular benchmarks demonstrate superior performance over previous federated and centralized methods, with significantly reduced communication costs. The global representation also shows good scalability when adding new clients.In summary, ProtoFL uses prototype distillation and normalizing flows in a novel two-phase federated framework to achieve efficient and effective anomaly detection with limited local data. The global model transfers well to local anomaly detectors without further communication.


## What problem or question is the paper addressing?

Based on my reading, the key points of this paper are:- The paper proposes a new federated learning framework called "ProtoFL" for unsupervised learning in an extreme non-IID setting. The goal is to enhance the representation power of a global model and reduce communication costs.- The method combines unsupervised federated learning via prototypical representation distillation with local classifier learning using normalizing flows.- For federated learning, the proposed "ProtoFL" distills representations from an off-the-shelf model/dataset into "prototype representations" that are shared with clients. This avoids sending model parameters directly and reduces communication. - For the local classifier, they use normalizing flows to estimate the density of the target class in a distributed way. This handles complex non-IID data distributions across clients.- They evaluate on image classification (MNIST, CIFAR-10/100, ImageNet-30) and keystroke authentication datasets. Results show ProtoFL outperforms previous federated and centralized learning methods for one-class classification.In summary, the key innovation is using prototype representations and normalizing flows to enable effective unsupervised federated learning for one-class classification, with superior performance and efficiency compared to prior art.
