# [ProtoFL: Unsupervised Federated Learning via Prototypical Distillation](https://arxiv.org/abs/2307.12450)

## What is the central research question or hypothesis that this paper addresses?

This paper proposes a new federated learning framework called "ProtoFL" to address key challenges in federated learning for one-class classification, including high communication costs, limited representation power, and unstable learning processes. The central hypothesis is that by using prototype representation distillation and normalizing flows for local classifier learning, the proposed ProtoFL method can:1) Enhance the representation power of the global model and reduce communication costs by distilling prototype representations instead of directly transferring parameters between clients and server.2) Improve one-class classification performance with limited local data by using normalizing flows to estimate the density of the target class distribution.In summary, the central hypothesis is that ProtoFL with prototype distillation and normalizing flows can enable more efficient and effective federated learning for one-class classification compared to existing approaches. The experiments aim to validate whether ProtoFL achieves superior performance over previous methods on both image and tabular datasets.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contributions are:- Proposing ProtoFL, a novel unsupervised federated learning framework that addresses challenges like insufficient local training data, high communication costs, and limited representation power. This is done through prototypical representation distillation and using normalizing flows for local classifier learning.- Introducing a prototype-based representation learning method that distills normal data representation from an off-the-shelf model and dataset. This allows scalability of the global model when adding new clients in federated learning based one-class classification. - Proposing new federated and centralized learning methods for one-class classification, which are evaluated on five widely used benchmarks. The experiments demonstrate superior performance over previous methods.In summary, the key contribution is developing ProtoFL, a new approach to unsupervised federated learning that enables efficient and effective global model updates through prototypical representation distillation and flow-based local classifier learning. This provides a novel solution to challenges in federated learning-based one-class classification.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper proposes a new federated learning framework called ProtoFL that uses prototypical representation distillation and normalizing flows to improve representation learning and reduce communication costs for one-class classification tasks.


## How does this paper compare to other research in the same field?

Here are a few key points comparing this paper to other research in federated learning for one-class classification:- This paper proposes a new method called ProtoFL that distills prototypical representations to enhance the global model in federated learning settings with non-i.i.d. data. This is a novel approach not explored in prior federated learning papers for one-class classification. - Previous federated learning methods like FedUV, FedMetric, and FedAwS rely on error-correcting codes or geometric regularization techniques to train a one-class model. In contrast, ProtoFL uses an off-the-shelf model and dataset to distill prototypical representations for each client. This helps compensate for limited local data.- The paper demonstrates superior performance of ProtoFL over previous federated learning methods on image classification benchmarks like MNIST, CIFAR-10/100, and ImageNet-30. The gains are substantial both in terms of accuracy and communication efficiency.- This is the first work to investigate using federated learning for one-class classification. Prior work focused on centralized learning. The paper shows federated learning can outperform centralized methods by collaboratively training a global model.- The paper also proposes a new flow-based one-class classifier to estimate density on each client. This approach is more suitable for federated learning compared to prior centralized density estimation methods.- Analyses demonstrate the scalability of ProtoFL's representations to new clients. This property is not examined by previous federated learning papers.In summary, this paper introduces a novel federated learning approach via prototypical distillation and flow-based classification that advances the state-of-the-art for one-class classification under decentralized data settings. The gains over prior methods are significant.


## What future research directions do the authors suggest?

Based on my reading of the paper, some of the main future research directions suggested by the authors are:- Investigating the use of FL for one-class classification in other domains beyond computer vision, such as audio, natural language processing, etc. The authors suggest that their approach could be promising for other types of data.- Exploring different model architectures and training techniques for the local one-class classifiers. The authors used normalizing flows in their framework but suggest other types of density estimators could be explored.- Evaluating the performance when allowing some limited data sharing between clients, instead of the extreme non-IID setting used in the paper. The authors suggest this could help further improve performance.- Considering personalization techniques to better adapt the global model to each client's local data distribution. This could help improve client-level performance.- Validating the approach on larger-scale and more complex datasets. The authors suggest experiments on larger benchmarks and real-world datasets to further demonstrate the effectiveness.- Investigating privacy-preserving techniques for the prototype representations to avoid leaking any private information. This could be important for real-world deployment.- Exploring the use of self-supervised or semi-supervised learning to further improve the learned representations in the federated setting.In summary, the main future directions are around applying the approach to new domains and data types, evaluating on larger/real-world datasets, improving the individual components like the local classifiers, and enhancing privacy protections. The authors lay out a promising research direction for advancing federated learning for one-class classification.


## Summarize the paper in one paragraph.

The paper proposes ProtoFL, an unsupervised federated learning framework for one-class classification that utilizes prototypical representation distillation and normalizing flows. The key ideas are:- Propose ProtoFL, a two-phase unsupervised learning method combining federated learning via prototypical representation distillation (ProtoFL) and local classifier learning via normalizing flows (OC-NF). - In ProtoFL phase, distribute prototype representations from an off-the-shelf model to clients to compensate for limited local data. Clients train local models using cosine similarity and KL divergence losses to match prototypes. Aggregate models into an expressive global model.- In OC-NF phase, clients train invertible normalizing flow models as local one-class classifiers by maximizing likelihood and minimizing divergence between augmented examples.- Evaluated on image and tabular benchmarks. ProtoFL outperforms previous federated and centralized methods, achieves lower communication cost, and shows scalability to new clients. First investigation of federated learning for one-class classification.In summary, the key contribution is an unsupervised federated learning approach for one-class classification that distills prototypical representations to improve model expressiveness and leverages normalizing flows for effective local density estimation. Evaluations demonstrate superior performance over existing methods.
