# [Acquiring Clean Language Models from Backdoor Poisoned Datasets by   Downscaling Frequency Space](https://arxiv.org/abs/2402.12026)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
Language models (LMs) have shown remarkable success in various NLP tasks. However, they are susceptible to backdoor attacks, where attackers poison the training data by implanting triggers (e.g. specific words/sentences/syntax). When trained on such poisoned data, the victim LM performs maliciously on inputs containing triggers while behaving normally on clean inputs. Prior defense methods that train LMs on poisoned data struggle against complex triggers. 

Key Idea and Methodology:
- The paper investigates the learning mechanisms of backdoored LMs using Fourier analysis. It finds that the backdoor input-output mapping exhibits stronger bias towards low frequencies compared to clean mapping. This explains why backdoor mapping converges faster.  
- Inspired by this, the paper proposes MuScleLoRA method. It uses multiple radial scalings in frequency space to downscale the clean mapping. This makes model prioritize learning of high freq clean mapping over low freq backdoor mapping. 
- MuScleLoRA also reduces model capacity using low-rank adaptation and aligns gradients to a small set of clean data. This further mitigates backdoor learning.

Key Contributions:
- Conducts Fourier analysis to reveal learning bias of backdoor mapping, explaining its faster convergence
- Proposes MuScleLoRA defense method that downscales clean mapping in frequency space to prioritize its learning over backdoor mapping
- Shows MuScleLoRA successfully defends against diverse triggers on multiple datasets and architectures (BERT, RoBERTa, Llama2)
- Reduces attack success rate below 15% on average while maintaining accuracy on clean data

In summary, the paper provides useful insights into backdoor learning using Fourier analysis and proposes an effective defense method called MuScleLoRA that leverages these insights. Experiments demonstrate MuScleLoRA's ability to acquire clean LMs from poisoned data across diverse settings.
