# [Are We Ready for Vision-Centric Driving Streaming Perception? The ASAP   Benchmark](https://arxiv.org/abs/2212.08914)

## What is the central research question or hypothesis that this paper addresses?

 The central research question this paper addresses is:

How can we effectively evaluate the online performance of vision-centric perception algorithms in autonomous driving under realistic deployment constraints like inference latency and limited computational resources? 

The key hypotheses are:

1) Existing autonomous driving benchmarks mainly focus on offline performance and do not account for inference latency, which is a critical factor in real-time deployment. 

2) The streaming performance of vision algorithms varies significantly across different computational platforms and resource constraints. The offline rankings may not hold in online streaming settings.

3) Compensating for inference delay by forecasting future states can improve streaming performance of detectors.

To summarize, this paper proposes a new streaming benchmark called ASAP to evaluate the accuracy vs latency tradeoff of vision algorithms under varied computational constraints. It also provides baselines to alleviate the impact of inference delays. The goal is to bridge the gap between ideal academic research and real-world autonomous driving deployment.


## What is the main contribution of this paper?

 Based on my reading of the paper, the main contributions appear to be:

1. Proposing the ASAP (Autonomous-driving Streaming Perception) benchmark, which is the first benchmark to evaluate the online performance of vision-centric perception approaches for autonomous driving. It focuses on evaluating the accuracy vs latency tradeoff.

2. Creating the nuScenes-H dataset, an extension of the nuScenes dataset with 12Hz annotations to enable evaluation of streaming perception for camera-based 3D object detection. This is done through an annotation extending pipeline involving object interpolation and a temporal database. 

3. Introducing the SPUR (Streaming Perception Under constrained-computation) evaluation protocol that allows assessing streaming performance under different computational constraints like varying GPUs and resource sharing.

4. Providing simple baselines for camera-based streaming 3D detection that aim to compensate for inference delay by estimating future object states. Experiments show these baselines improve streaming performance.

5. Comprehensive experiments analyzing streaming performance of modern 3D detectors under different computation constraints using the ASAP benchmark. This reveals how streaming performance depends on model latency and computation budget.

In summary, the key contribution is creating a novel benchmark and protocol to quantify the accuracy vs latency tradeoff for vision-based perception algorithms in autonomous driving, which helps address the gap between research and practical deployment. The nuScenes-H dataset and baselines also facilitate further research in this direction.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the key points from the paper:

The paper proposes a new benchmark called ASAP to evaluate the accuracy-latency trade-off of vision-centric perception methods for autonomous driving, introduces an annotation pipeline to generate high frame-rate labels on nuScenes data, and presents simple baselines that improve streaming performance by predicting future object states to compensate for inference delays.
