# [Are We Ready for Vision-Centric Driving Streaming Perception? The ASAP   Benchmark](https://arxiv.org/abs/2212.08914)

## What is the central research question or hypothesis that this paper addresses?

 The central research question this paper addresses is:

How can we effectively evaluate the online performance of vision-centric perception algorithms in autonomous driving under realistic deployment constraints like inference latency and limited computational resources? 

The key hypotheses are:

1) Existing autonomous driving benchmarks mainly focus on offline performance and do not account for inference latency, which is a critical factor in real-time deployment. 

2) The streaming performance of vision algorithms varies significantly across different computational platforms and resource constraints. The offline rankings may not hold in online streaming settings.

3) Compensating for inference delay by forecasting future states can improve streaming performance of detectors.

To summarize, this paper proposes a new streaming benchmark called ASAP to evaluate the accuracy vs latency tradeoff of vision algorithms under varied computational constraints. It also provides baselines to alleviate the impact of inference delays. The goal is to bridge the gap between ideal academic research and real-world autonomous driving deployment.


## What is the main contribution of this paper?

 Based on my reading of the paper, the main contributions appear to be:

1. Proposing the ASAP (Autonomous-driving Streaming Perception) benchmark, which is the first benchmark to evaluate the online performance of vision-centric perception approaches for autonomous driving. It focuses on evaluating the accuracy vs latency tradeoff.

2. Creating the nuScenes-H dataset, an extension of the nuScenes dataset with 12Hz annotations to enable evaluation of streaming perception for camera-based 3D object detection. This is done through an annotation extending pipeline involving object interpolation and a temporal database. 

3. Introducing the SPUR (Streaming Perception Under constrained-computation) evaluation protocol that allows assessing streaming performance under different computational constraints like varying GPUs and resource sharing.

4. Providing simple baselines for camera-based streaming 3D detection that aim to compensate for inference delay by estimating future object states. Experiments show these baselines improve streaming performance.

5. Comprehensive experiments analyzing streaming performance of modern 3D detectors under different computation constraints using the ASAP benchmark. This reveals how streaming performance depends on model latency and computation budget.

In summary, the key contribution is creating a novel benchmark and protocol to quantify the accuracy vs latency tradeoff for vision-based perception algorithms in autonomous driving, which helps address the gap between research and practical deployment. The nuScenes-H dataset and baselines also facilitate further research in this direction.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the key points from the paper:

The paper proposes a new benchmark called ASAP to evaluate the accuracy-latency trade-off of vision-centric perception methods for autonomous driving, introduces an annotation pipeline to generate high frame-rate labels on nuScenes data, and presents simple baselines that improve streaming performance by predicting future object states to compensate for inference delays.


## How does this paper compare to other research in the same field?

 Here are my thoughts on how this paper compares to other research in vision-centric autonomous driving perception:

- This paper proposes a new benchmark called ASAP (Autonomous-driving Streaming Perception) to evaluate the online performance of vision-based perception methods for autonomous driving. This is novel compared to existing benchmarks like nuScenes, Argoverse, etc. that focus on offline evaluation. 

- The paper introduces a new dataset called nuScenes-H that provides high frame rate (12Hz) annotations on raw nuScenes data to enable streaming evaluation. This enables benchmarking perception algorithms in a realistic online setting.

- The paper proposes a new evaluation protocol called SPUR that evaluates algorithms under varying computational constraints. This better reflects real-world deployment compared to just measuring offline accuracy.

- Simple baselines are provided that use motion prediction to compensate for inference delay in streaming perception. Experiments show these baselines improve streaming performance across different hardware.

- Experiments are conducted with 7 state-of-the-art vision-based 3D detectors on nuScenes-H using SPUR. This provides insights into streaming performance of methods and how ranks differ from offline settings.

Overall, the key novelty is the focus on benchmarking online performance of vision-based perception, which is crucial for real-world deployment but lacking in prior work. The new dataset, evaluation protocol and experiments provide a comprehensive analysis of streaming performance that was not studied previously. This could help guide development of practical vision-based perception algorithms for autonomous driving.
