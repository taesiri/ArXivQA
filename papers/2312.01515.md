# [Bigger is not Always Better: The Effect of Context Size on Speech   Pre-Training](https://arxiv.org/abs/2312.01515)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Speech recognition systems typically assume more context is better, using self-attention to incorporate unlimited context. 
- However, it's unclear if this holds for self-supervised pre-training objectives, which are more nebulous than supervised objectives. 
- Too much irrelevant context could be detrimental to learning useful representations for downstream tasks like phoneme discrimination.

Proposed Solution:
- Adapt contrastive predictive coding (CPC) model to precisely control context using causal, chunked self-attention.
- Train CPC models with context widths from 40ms to 1300ms on phoneme discrimination task.
- Evaluate phoneme discriminability and ASR performance for different context widths.

Key Findings:
- Phoneme discrimination peaks at around 40ms context, degrading substantially beyond 320ms.
- Shorter context windows also work better for ASR with frozen pre-trained features.
- Results are robust to modifications like more layers, conv layers, longer training, etc.

Main Contributions:  
- First speech study to systematically test effect of context in self-supervised pre-training
- Find evidence that too much context hurts for phone discrimination and ASR with frozen features 
- Results suggest changes to design of upstream architectures to better facilitate downstream tasks

In summary, the paper challenges the assumption that more context is universally better in self-supervised speech pre-training, and provides evidence on the ideal amount of context for phoneme discrimination and related downstream tasks. The results point to potential architecture tweaks for better transfer learning.
