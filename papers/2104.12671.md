# [Multimodal Clustering Networks for Self-supervised Learning from   Unlabeled Videos](https://arxiv.org/abs/2104.12671)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question appears to be: How can we learn an effective joint multimodal embedding space from unlabeled videos that have natural narrations?Specifically, the authors propose an approach to learn a joint embedding space across video, audio and text modalities using a large corpus of narrated instructional videos (the HowTo100M dataset). The key ideas explored in this paper are:- Learning joint multimodal representations using both contrastive loss at the instance level and clustering loss at the semantic level. This combines the benefits of bringing aligned instances together while grouping semantically similar instances.- Performing multimodal clustering across features from different modalities (video, audio, text) rather than clustering within each modality separately. This enables clustering in the joint embedding space.- Evaluating the learned representations on challenging multimodal downstream tasks like video retrieval and temporal action localization in a zero-shot setting. This tests how well the model can generalize to new datasets without any fine-tuning.So in summary, the central hypothesis is that combining contrastive loss and clustering loss in a multimodal setting can learn an effective joint embedding space from unlabeled narrated videos. This is validated through strong performance on zero-shot cross-modal retrieval and action localization tasks.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contributions appear to be:1. Proposing a novel method called Multimodal Clustering Network (MCN) that combines contrastive learning with clustering to learn joint representations across video, audio, and text modalities from unlabeled narrated videos. 2. Showing that creating a joint video-audio-text embedding space using a clustering loss is beneficial for self-supervised learning of video representations. This allows semantically similar instances to be close together in the embedding space.3. Demonstrating that the representations learned by MCN lead to significant improvements in downstream tasks like video retrieval and temporal action localization, even in a zero-shot setting without any fine-tuning on the target datasets.In summary, the key ideas are using contrastive learning across modalities, clustering for semantic consistency, and learning a joint embedding space that works well for multimodal downstream tasks without needing dataset-specific training. The proposed MCN framework combines these ideas in a novel way to advance multimodal self-supervised representation learning.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper proposes a novel self-supervised multimodal clustering network called MCN that learns joint embeddings across video, audio, and text modalities by combining a contrastive loss to compare instances with a clustering loss to group semantically similar samples, allowing for improved performance on downstream tasks like video retrieval and action localization without needing dataset-specific finetuning.
