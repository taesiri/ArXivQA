# [PEARL: Prompting Large Language Models to Plan and Execute Actions Over   Long Documents](https://arxiv.org/abs/2305.14564)

## What is the central research question or hypothesis that this paper addresses?

This paper introduces PEARL, a framework for reasoning over long documents with large language models via planning and executing compositional actions. The central hypothesis is that decomposing complex reasoning tasks over long documents into a sequence of simpler executable actions can improve the performance of large language models on these tasks compared to standard prompting techniques like zero-shot prompting or chain-of-thought. Specifically, the paper proposes that prompting the LLM to generate a plan of actions for answering a question, and then executing that plan step-by-step can enable more accurate and comprehensive reasoning compared to directly prompting the LLM for an answer. The key research questions addressed are:1) Can large language models effectively generate reasonable action plans for answering questions that require complex reasoning over long documents? 2) Does executing the generated plans lead to more accurate answers compared to standard prompting baselines?3) Does each component of the PEARL framework (action mining, plan generation, plan execution) contribute positively to the overall performance?4) How does PEARL compare to other related prompting techniques like chain-of-thought when applied to complex reasoning over long documents?The central hypothesis is tested through experiments on a subset of the QuALITY dataset requiring reasoning over long narrative texts. The results show PEARL outperforms the baselines, supporting the hypothesis that decomposing via planning and execution is beneficial for this task. Ablations and comparisons to other techniques provide evidence that each component contributes positively.


## What is the main contribution of this paper?

Here are the main contributions of this paper:- The paper proposes PEARL, a new framework for improving reasoning over long documents with large language models (LLMs). PEARL has three main stages:1. Action mining: An LLM is prompted to generate a set of "actions" (functions for reasoning, like SUMMARIZE or FIND_EVENT) from training data. This allows the action set to be customized for the task.2. Plan formulation: Given a question, the LLM generates an executable plan consisting of a sequence of actions. This decomposes the complex reasoning task into smaller steps. 3. Plan execution: The LLM executes the plan step-by-step, using a prompt template filled with information from previous steps. This stage includes the long document.- PEARL is evaluated on a challenging subset of the QuALITY reading comprehension dataset requiring reasoning over long articles. It outperforms both zero-shot prompting and chain-of-thought prompting baselines.- Ablation experiments demonstrate the importance of each stage of PEARL - removing plan execution, reducing the number of actions, or removing plan refinement hurts performance.- Human evaluation of a sample of PEARL's plans finds them to be reasonable, though some contain unnecessary steps or miss key information.In summary, the main contribution is proposing and evaluating PEARL, a new prompting framework to improve complex reasoning over long documents. PEARL demonstrates the promise of strategically decomposing the task and leveraging LLMs for each stage.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper proposes a framework called PEARL that improves the ability of large language models like GPT-4 to reason over long documents by decomposing questions into plans consisting of executable actions which are then carried out over multiple passes through the document to arrive at more accurate and comprehensive answers compared to directly prompting the model.
