# [Heterogeneous Forgetting Compensation for Class-Incremental Learning](https://arxiv.org/abs/2308.03374)

## What is the central research question or hypothesis that this paper addresses?

 This paper focuses on addressing the problem of catastrophic forgetting in class-incremental learning (CIL) systems. The main hypothesis is that different old classes can exhibit heterogeneous forgetting speeds, with some classes being "easy-to-forget" while others are "hard-to-forget". The key research questions are:1) How to model and alleviate the heterogeneous forgetting of different old classes in CIL? 2) How to compensate for the forgetting heterogeneity from both the representation and gradient/optimization perspectives?Specifically, the paper hypothesizes that:1) Exploring task-shared representations can help alleviate heterogeneous forgetting among old classes from the representation perspective.2) Performing gradient-balanced optimization and distilling heterogeneous class relations can compensate for the forgetting heterogeneity from the gradient perspective. The central goal is to develop a model that can overcome catastrophic forgetting in CIL by explicitly accounting for and handling the heterogeneous forgetting speeds of different old classes. The key novelty lies in tackling this problem from both representation and gradient aspects within a unified framework.In summary, the central hypothesis is that modeling and compensating for heterogeneous forgetting is crucial for effective catastrophic forgetting mitigation in class-incremental learning systems. The research aims to validate this hypothesis through both model design and empirical evaluations.


## What is the main contribution of this paper?

 Based on my reading of the paper, the main contributions appear to be:- The development of a novel Heterogeneous Forgetting Compensation (HFC) model to address the problem of different forgetting speeds for "easy-to-forget" and "hard-to-forget" old classes in class-incremental learning. - A task-semantic aggregation (TSA) block that aggregates local category information to learn robust task-shared representations, helping to alleviate heterogeneous forgetting in the representation.- Two novel plug-and-play losses - a gradient-balanced forgetting compensation (GFC) loss and a gradient-balanced relation distillation (GRD) loss - that help compensate for heterogeneous forgetting from the gradient perspective.- Experiments showing improved performance of the HFC model compared to baseline methods on CIFAR-100, ImageNet-100, and ImageNet-1000 datasets. The plug-and-play losses also improve performance when applied to existing incremental learning methods.In summary, the main contribution appears to be the proposal of a new model and techniques to specifically address the problem of differing forgetting speeds between easy-to-forget and hard-to-forget classes in class-incremental learning. The model tackles this heterogeneous forgetting from both the representation and gradient perspectives.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence TL;DR summary of the paper:The paper proposes a novel Heterogeneous Forgetting Compensation (HFC) model to address the issue of different forgetting speeds among easy-to-forget and hard-to-forget old classes in continual learning, by exploring task-shared representations and performing gradient-balanced compensation.


## How does this paper compare to other research in the same field?

 Here are a few key points comparing this paper to other research in class-incremental learning:- This paper proposes a new model called Heterogeneous Forgetting Compensation (HFC) to address the issue of different forgetting speeds for "easy-to-forget" vs "hard-to-forget" old classes. Most prior CIL methods assume all old classes forget at the same rate. Tackling heterogeneous forgetting is a novel contribution.- The paper introduces two main technical components to address heterogeneous forgetting: 1) A task-semantic aggregation (TSA) block to extract shared representations and reduce forgetting from a representation perspective. 2) Novel gradient-balanced loss functions (GFC and GRD) to balance gradients and compensate for heterogeneous forgetting. - The HFC model achieves state-of-the-art results on CIFAR-100, ImageNet-100, and ImageNet-1000 compared to prior CIL methods like iCaRL, BiC, PODNet, etc. The gains are especially notable as the number of incremental steps increases.- The paper also shows the plug-and-play losses (GFC and GRD) can be added to existing CIL methods like iCaRL, BiC, PODNet to improve their performance. This demonstrates the losses have broad applicability.- Compared to concurrent work like DyTox and FOSTER which also use vision transformers, the HFC model with TSA block and gradient-balanced losses provides complimentary advancements to further boost CIL performance.In summary, the HFC model's ability to address heterogeneous forgetting and the novel technical approach distinguish this work from prior art and contribute new state-of-the-art techniques to the CIL field. The gains on standard benchmarks and plug-and-play nature of the losses demonstrate the impact.


## What future research directions do the authors suggest?

 Based on my reading of the paper, some of the main future research directions suggested by the authors include:- Exploring other transformer architectures besides ViT for continual learning. The authors use ViT in their proposed method but suggest exploring other transformer variants as well.- Applying the proposed TSA block to other continual learning methods besides their own to improve task-shared representations. The TSA block helps aggregate semantic information across tasks.- Developing more advanced plug-and-play losses like GFC and GRD to better address catastrophic forgetting from a gradients perspective. The authors propose these losses but suggest further improvements.- Evaluating on larger-scale datasets beyond CIFAR and ImageNet subsets. The authors use CIFAR and ImageNet but note the need to validate on larger benchmark datasets.- Extending to more complex incremental learning scenarios like class-incremental learning with noisy data labels. The current setting is class-incremental learning on clean labeled data.- Considering computational efficiency and memory overhead when scaling up. The authors' method adds some overhead which may limit scalability.- Investigating forgetting heterogeneity in more depth, such as analyzing the causes and developing better measures. The authors propose to address it but more investigation is needed.In summary, the main future directions are developing more advanced transformer architectures, losses, and evaluations for class-incremental learning, with a focus on improving efficiency and better understanding forgetting heterogeneity. The authors lay the groundwork but highlight many opportunities for extension.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the key points from the paper:The paper proposes a novel Heterogeneous Forgetting Compensation (HFC) model for class-incremental learning that addresses the issue of different old classes having heterogeneous forgetting speeds. The model has two main components to tackle forgetting heterogeneity from representation and gradient aspects. First, a task-semantic aggregation block explores task-shared global representations to alleviate forgetting heterogeneity in the representation. Second, two novel plug-and-play losses are proposed: a gradient-balanced forgetting compensation loss rectifies forgetting speeds of old classes and normalizes learning paces of new classes for balanced gradient propagation, and a gradient-balanced relation distillation loss distills heterogeneous relation consistency between old and new models. Experiments on CIFAR-100, ImageNet-100, and ImageNet-1000 show the HFC model significantly outperforms existing class-incremental learning methods. The plug-and-play losses can also be applied to other methods for improved performance. The model provides an effective approach to overcoming heterogeneous forgetting in incremental learning.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the paper:This paper proposes a novel Heterogeneous Forgetting Compensation (HFC) model to address the issue of heterogeneous forgetting speeds among different old classes in class-incremental learning. Most existing methods make the unrealistic assumption that all old classes forget at the same rate and compensate for forgetting equally. However, in practice, some old classes are easier to forget while others are harder. To tackle this issue, the HFC model resolves heterogeneous forgetting from both the representation and gradient aspects. First, a task-semantic aggregation block is introduced to extract robust, shared representations and alleviate forgetting heterogeneity in the representation space. Second, two novel plug-and-play losses - a gradient-balanced forgetting compensation loss and a gradient-balanced relation distillation loss - are proposed to compensate for forgetting heterogeneity from the gradient perspective. These losses help balance gradients and distill relations among categories to minimize imbalance caused by heterogeneous forgetting speeds. Experiments on CIFAR-100, ImageNet-100, and ImageNet-1000 show significant improvement over state-of-the-art methods. Applying the plug-and-play losses to existing methods also substantially boosts performance. The HFC model demonstrates the importance of accounting for heterogeneous forgetting in class-incremental learning.


## Summarize the main method used in the paper in one paragraph.

 Here is a one paragraph summary of the main method used in the paper:The paper proposes a novel Heterogeneous Forgetting Compensation (HFC) model to address the issue of different forgetting speeds across old categories in class-incremental learning. The key idea is to tackle heterogeneous forgetting from both representation and gradient aspects. From the representation perspective, the model uses a task-semantic aggregation block to extract task-shared global representations by aggregating local category information within each task, which helps alleviate forgetting heterogeneity. To address the gradient aspect, the model introduces two novel losses - a gradient-balanced forgetting compensation loss and a gradient-balanced relation distillation loss. The former helps balance gradients across easy-to-forget and hard-to-forget classes to compensate for heterogeneous forgetting. The latter distills inter-class relationships while reweighting with class-specific gradients to enforce consistent relations between old and new models. Together, representation learning via aggregation and gradient balancing through customized losses allow the model to effectively handle heterogeneous forgetting speeds across old categories.
