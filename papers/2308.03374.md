# [Heterogeneous Forgetting Compensation for Class-Incremental Learning](https://arxiv.org/abs/2308.03374)

## What is the central research question or hypothesis that this paper addresses?

This paper focuses on addressing the problem of catastrophic forgetting in class-incremental learning (CIL) systems. The main hypothesis is that different old classes can exhibit heterogeneous forgetting speeds, with some classes being "easy-to-forget" while others are "hard-to-forget". The key research questions are:1) How to model and alleviate the heterogeneous forgetting of different old classes in CIL? 2) How to compensate for the forgetting heterogeneity from both the representation and gradient/optimization perspectives?Specifically, the paper hypothesizes that:1) Exploring task-shared representations can help alleviate heterogeneous forgetting among old classes from the representation perspective.2) Performing gradient-balanced optimization and distilling heterogeneous class relations can compensate for the forgetting heterogeneity from the gradient perspective. The central goal is to develop a model that can overcome catastrophic forgetting in CIL by explicitly accounting for and handling the heterogeneous forgetting speeds of different old classes. The key novelty lies in tackling this problem from both representation and gradient aspects within a unified framework.In summary, the central hypothesis is that modeling and compensating for heterogeneous forgetting is crucial for effective catastrophic forgetting mitigation in class-incremental learning systems. The research aims to validate this hypothesis through both model design and empirical evaluations.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contributions appear to be:- The development of a novel Heterogeneous Forgetting Compensation (HFC) model to address the problem of different forgetting speeds for "easy-to-forget" and "hard-to-forget" old classes in class-incremental learning. - A task-semantic aggregation (TSA) block that aggregates local category information to learn robust task-shared representations, helping to alleviate heterogeneous forgetting in the representation.- Two novel plug-and-play losses - a gradient-balanced forgetting compensation (GFC) loss and a gradient-balanced relation distillation (GRD) loss - that help compensate for heterogeneous forgetting from the gradient perspective.- Experiments showing improved performance of the HFC model compared to baseline methods on CIFAR-100, ImageNet-100, and ImageNet-1000 datasets. The plug-and-play losses also improve performance when applied to existing incremental learning methods.In summary, the main contribution appears to be the proposal of a new model and techniques to specifically address the problem of differing forgetting speeds between easy-to-forget and hard-to-forget classes in class-incremental learning. The model tackles this heterogeneous forgetting from both the representation and gradient perspectives.
