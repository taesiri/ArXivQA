# A Survey of Evaluation Metrics Used for NLG Systems

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, it appears the central research question is: How can we develop an accurate automatic evaluation metric for natural language generation (NLG) systems that captures the nuances required for assessing different NLG tasks?The paper discusses the challenges in evaluating NLG systems due to the wide variety of criteria that need to be checked across different NLG tasks. It argues that existing metrics like BLEU and ROUGE are inadequate as they were designed mainly for machine translation and do not correlate well with human judgments for many other NLG tasks. The authors survey the large number of evaluation metrics proposed since 2014 and organize them into a taxonomy of context-free versus context-dependent metrics. They highlight the need for more task-specific context-dependent metrics, as well as more interpretable metrics that provide fine-grained scores corresponding to different evaluation criteria.The central hypothesis appears to be that progress in automatic NLG evaluation requires metrics that are tailored for specific tasks, leverage contextual information, and provide nuanced multi-dimensional judgments akin to human evaluation. The paper recommends directions like developing common frameworks for metrics, collecting human judgment datasets, and critically analyzing proposed metrics.In summary, the key research question is how to develop accurate automatic NLG evaluation metrics that can act as a good proxy for multi-faceted human evaluation across diverse NLG tasks. The paper surveys existing work and provides recommendations towards addressing this question.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contribution appears to be presenting a taxonomy of automatic evaluation metrics for natural language generation (NLG) systems. The paper provides a comprehensive survey of existing metrics, organizes them into a coherent taxonomy, highlights shortcomings of current metrics, and makes recommendations for future research directions. Specifically, some of the key contributions are:- Proposing a taxonomy to categorize existing automatic NLG evaluation metrics into context-free vs context-dependent metrics. The context-free metrics are further divided into trained vs untrained metrics. - Providing details on a wide variety of existing metrics ranging from classics like BLEU, ROUGE, METEOR to more recent metrics like BERTScore, BLEURT, etc. The metrics are organized based on the taxonomy.- Summarizing studies that have criticized automatic metrics due to poor correlation with human judgements, lack of interpretability, biases, poor adaptability across tasks, etc.- Discussing different methodologies used for evaluating evaluation metrics - correlation measures, significance tests, adversarial evaluations, etc.- Providing recommendations on developing common codebases and datasets, building context-dependent and interpretable metrics, critical benchmarking of metrics, etc.In summary, the key contribution is providing a structured taxonomy and comprehensive overview of automatic NLG evaluation metrics, summarizing their limitations, and giving recommendations for advancing research in this area. The paper helps quickly get up to speed on the developments in NLG evaluation metrics over the past few years.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Based on my review of the paper, here is a one sentence summary:This LaTeX template provides a starting point for writing an ACM conference paper, including predefined document classes, bibliography styles, and formatting guidance.
