# [Logit Standardization in Knowledge Distillation](https://arxiv.org/abs/2403.01427)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper "Logit Standardization in Knowledge Distillation":

Problem:
- Knowledge distillation (KD) transfers knowledge from a large "teacher" model to a smaller "student" model by minimizing the divergence between their softmax predictions. 
- The softmax predictions depend on a temperature parameter. Conventional KD methods assume teacher and student share the same global temperature.
- However, this forces an exact match between teacher and student logits, even though preserving logit relationships is sufficient for good student performance. This is challenging for smaller students.
- Shared temperatures can also lead to misleading student evaluations, if a student predicts incorrect classes but with logits numerically closer to the teacher's.

Proposed Solution:
- Derive the softmax temperature from information theory principles, showing it depends on a Lagrangian multiplier that can be set differently for teacher vs student.
- Propose logit standardization as a pre-processing step before softmax. Standardize logits to zero mean and unit variance using the z-score.
- This allows student to focus on mimicking teacher logit relationships rather than exact values. The student logits can then take any values suitable for its capacity.

Main Contributions:
- Theoretical derivation showing flexible temperature assignment is valid 
- Identify issues with conventional logit distillation pipelines 
- Propose logit standardization pre-process to enable students to mimic only essential logit relationships
- Extensive experiments showing consistent performance gains when combined with existing logit KD methods

The key insight is that exact logit value mimicry is an unnecessary constraint for students. The logit pre-processing frees students to mimic only the essential knowledge - the logit relationships that drive model performance. This improves student learning and evaluation.
