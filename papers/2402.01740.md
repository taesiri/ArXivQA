# [Compensatory Biases Under Cognitive Load: Reducing Selection Bias in   Large Language Models](https://arxiv.org/abs/2402.01740)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Large language models (LLMs) like GPT-3 exhibit biases that adversely impact their performance, especially for list selection tasks which are important for digital navigation and decision-making. However, there has been little research into quantifying and mitigating these biases. 

Methods:
- The authors conduct controlled experiments with GPT-3 and Claude selecting 3 items from lists of numbers/letters of varying lengths. They systematically measure selection probabilities to analyze the effects of temperature, list length, object type, position, etc on bias.

- Two prompting methods are compared: (1) Separate sampling and guard rail steps (2) Combined sampling + guard rail. Guard rails structurally ensure the response format.

Key Findings:  
- There are significant primacy biases causing the first few list items to be disproportionately selected. The extent and nature of biases strongly depends on the LLM.

- Object type (letters vs numbers) modulates the bias magnitudes. Identity and position of objects in the list greatly alter selection probabilities.

- Using guard rails increases bias and reduces adherence to instructions. This effect is hypothesized to be because of increased cognitive load on the LLM, causing compensatory bias behaviors. Separating the sampling and guard rail steps reduces this.

Implications:
- Bias structure is not consistent even across similar LLMs and tasks. Applications require empirical analysis.

- Complex structures like guard rails impose cognitive load on LLMs, causing compensatory increases in biases. Simplifying prompts by separating concerns reduces biases.

- Provides guidance for designing unbiased LLM systems and suggests LLMs may experience a form of cognitive load.


## Summarize the paper in one sentence.

 This paper experimentally analyzes selection bias in large language models when presented with list sampling tasks, finding primacy effects that vary by model and increase under higher cognitive load from prompts using guard rails.


## What is the main contribution of this paper?

 Based on my reading of the paper, the main contribution is:

The paper presents a systematic study analyzing biases in large language models (LLMs) when performing the task of selecting objects from lists. Specifically, it isolates and quantifies the effects of various factors like temperature, list length, object identity, object type, prompt complexity, and model type on the selection probabilities and biases. Key findings include:

1) There are significant primacy biases causing the first objects in a list to be disproportionately represented in LLM outputs. However, the exact bias structure depends strongly on the LLM model. 

2) The object type (e.g. numbers vs letters) modulates the magnitude of biases, with numbers showing smaller effects in some cases.

3) The use of guard rails, a prompt engineering method to ensure valid output structure, can greatly increase bias and reduce adherence to instructions when combined with a selection task. Separating the selection and guard rail steps reduces this effect.

4) The results suggest LLMs may experience a form of "cognitive load" that leads to compensatory increases in bias, analogous to human behavior. This has implications for designing unbiased LLM applications.

In summary, the key contribution is a rigorous analysis quantifying how different factors affect selection bias in LLMs, providing guidance on mitigating bias and suggesting LLMs have cognitive limitations leading to compensatory biases.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, the key terms and keywords associated with this paper are:

Large Language Models, Cognitive load, List Selection, Bias, Guard rails


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in this paper:

1. The paper hypothesizes that guard rails increase cognitive load for LLMs, leading to compensatory biases. What experiments could be designed to directly test whether guard rails increase cognitive load? For example, does adding more complex validation logic increase primacy bias more than simpler validation?

2. Are the positional and object biases observed independent, or do they interact? Could certain objects have positional preferences that explain the non-uniformities observed? How would you test this?

3. This paper focuses exclusively on sampling letters and numbers. How might the semantic content of objects change the observed biases? What real-world tasks could be studied to analyze how semantic content impacts selection biases? 

4. The authors use a uniform random distribution as the baseline for comparing selection probabilities. However, should an unbiased agent necessarily have a uniform distribution? What statistical tests could determine if a distribution is sufficiently unbiased?

5. How does the length of context provided to the LLM impact the selection biases observed? Does providing more or less context exacerbate or reduce biases?

6. This paper analyzes two proprietary LLMs. How might selection biases evolve during training for an open source LLM? Does the phase transition to higher performance also impact bias profiles?

7. The sampling temperature clearly impacts biases, but how does it relate to model uncertainty? Can the temperature dependence of bias help develop methods to reduce bias by targeting high uncertainty responses?

8. The mutual information quantifies dependence between variables. Are there other statistical dependency measures that could provide further insight into how object and positional variables interact?

9. The authors use bootstrap resampling to estimate probabilities and errors. Would a Bayesian treatment allowing the calculation of full posterior distributions provide any additional benefits?

10. The paper focuses exclusively on sampling 3 items from a list. How do the biases observed change if sampling larger or smaller subsets? Does the ratio of sample size to list length matter more than absolute numbers?
