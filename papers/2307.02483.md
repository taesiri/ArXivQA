# [Jailbroken: How Does LLM Safety Training Fail?](https://arxiv.org/abs/2307.02483)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be: Why do "jailbreak" attacks succeed in eliciting unsafe behavior from safety-trained large language models (LLMs), and how can new attacks be systematically created?The key hypotheses proposed are:1) Jailbreak attacks succeed due to two failure modes of safety training: - Competing objectives, where the model's capabilities and instruction following conflict with safety goals.- Mismatched generalization, where safety training fails to generalize to domains where capabilities exist. 2) These failure modes can be leveraged to systematically construct new jailbreak attacks.The authors then conduct experiments on attacks constructed using these principles and find they outperform existing ad hoc jailbreaks. This provides evidence for the hypothesized failure modes and their usefulness in constructing attacks.In summary, the central question is why jailbreaks work and how to construct them based on hypothesized limitations of safety training, which is then empirically validated.
