# [M3P: Learning Universal Representations via Multitask Multilingual   Multimodal Pre-training](https://arxiv.org/abs/2006.02635)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central hypothesis of this paper is that combining multilingual pre-training and multimodal pre-training into a unified framework via multitask learning can learn universal representations that map objects in different modalities (text, images) and languages into a common semantic space. Specifically, the paper proposes a model called M3P that aims to:- Learn multilingual representations from a large corpus of text covering 100 languages (via multilingual pre-training). - Learn multimodal representations from image-caption pairs labeled in English (via multimodal pre-training).- Use a novel "multimodal code-switched training" method to align images with non-English text to compensate for the lack of non-English labeled multimodal data.- Combine the above via multitask learning to create universal multilingual multimodal representations.The central hypothesis is that this unified model can learn improved representations that work across modalities (text, images) and languages, as evaluated on multilingual image-text retrieval tasks. The key novelty is using code-switching to align non-English text and images to overcome the lack of labeled non-English multimodal data.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contributions are:1. The authors propose M^3P, the first effort on combining multilingual pre-training and multimodal pre-training into a unified framework. M^3P aims to learn universal representations that map objects in different modalities (text, images) and languages into a common semantic space.2. A novel Multimodal Code-switched Training (MCT) method is introduced to enhance the multilingual transfer ability of M^3P, especially in low-resource settings. MCT helps enforce explicit alignments between images and non-English languages during pre-training.3. The authors achieve new state-of-the-art results on the multilingual image-text retrieval task for non-English languages on Multi30K and MSCOCO datasets, outperforming existing multilingual methods by a large margin. M^3P also obtains comparable performance to monolingual models on English.4. Extensive experiments and analysis are provided to give insights into the effectiveness of using MCT and different pre-training tasks in M^3P.In summary, the key innovation is the unified multilingual-multimodal pre-training framework M^3P and the proposed MCT method to improve multilingual transfer ability. This enables the model to achieve much better performance on non-English languages for multimodal tasks compared to previous methods.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper presents M3P, a novel pre-trained model that combines multilingual pre-training and multimodal pre-training into a unified framework via multitask learning, and proposes Multimodal Code-switched Training (MCT) to improve alignment between images and low-resource languages.


## How does this paper compare to other research in the same field?

Based on my review, here are some key ways this paper compares to other research in the field:- This is the first work attempting to combine multilingual pre-training and multimodal pre-training in a unified framework, while most existing works focus on either multilingual or multimodal pre-training separately. - The proposed Multimodal Code-switched Training (MCT) method is novel and effectively improves multilingual transfer ability, especially in low-resource settings. Other works on code-switching have focused only on text, not multimodal data.- The model achieves new state-of-the-art results on multilingual image-text retrieval, significantly outperforming prior multilingual baselines. For English, it obtains comparable results to current monolingual multimodal models.- Extensive ablation studies are provided to analyze the impact of different model components. The analyses on the number of MCT languages and different pre-training tasks give useful insights.- The qualitative analysis on MCT gives an intuitive explanation of why it is effective for non-English languages. The discussion of its potential limitation is also informative.Overall, this paper pushes forward multimodal pre-training to multilingual scenarios for the first time. The proposed approaches and thorough experiments demonstrate strong improvements over existing multilingual works. The analyses also provide valuable insights to guide future research in this direction. This paper makes significant contributions to the field of multilingual multimodal pre-training and retrieval.
