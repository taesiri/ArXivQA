# [M3P: Learning Universal Representations via Multitask Multilingual   Multimodal Pre-training](https://arxiv.org/abs/2006.02635)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading, the central hypothesis of this paper is that combining multilingual pre-training and multimodal pre-training into a unified framework via multitask learning can learn universal representations that map objects in different modalities (text, images) and languages into a common semantic space. 

Specifically, the paper proposes a model called M3P that aims to:

- Learn multilingual representations from a large corpus of text covering 100 languages (via multilingual pre-training). 

- Learn multimodal representations from image-caption pairs labeled in English (via multimodal pre-training).

- Use a novel "multimodal code-switched training" method to align images with non-English text to compensate for the lack of non-English labeled multimodal data.

- Combine the above via multitask learning to create universal multilingual multimodal representations.

The central hypothesis is that this unified model can learn improved representations that work across modalities (text, images) and languages, as evaluated on multilingual image-text retrieval tasks. The key novelty is using code-switching to align non-English text and images to overcome the lack of labeled non-English multimodal data.


## What is the main contribution of this paper?

 Based on my reading of the paper, the main contributions are:

1. The authors propose M^3P, the first effort on combining multilingual pre-training and multimodal pre-training into a unified framework. M^3P aims to learn universal representations that map objects in different modalities (text, images) and languages into a common semantic space.

2. A novel Multimodal Code-switched Training (MCT) method is introduced to enhance the multilingual transfer ability of M^3P, especially in low-resource settings. MCT helps enforce explicit alignments between images and non-English languages during pre-training.

3. The authors achieve new state-of-the-art results on the multilingual image-text retrieval task for non-English languages on Multi30K and MSCOCO datasets, outperforming existing multilingual methods by a large margin. M^3P also obtains comparable performance to monolingual models on English.

4. Extensive experiments and analysis are provided to give insights into the effectiveness of using MCT and different pre-training tasks in M^3P.

In summary, the key innovation is the unified multilingual-multimodal pre-training framework M^3P and the proposed MCT method to improve multilingual transfer ability. This enables the model to achieve much better performance on non-English languages for multimodal tasks compared to previous methods.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the key points from the paper:

The paper presents M3P, a novel pre-trained model that combines multilingual pre-training and multimodal pre-training into a unified framework via multitask learning, and proposes Multimodal Code-switched Training (MCT) to improve alignment between images and low-resource languages.


## How does this paper compare to other research in the same field?

 Based on my review, here are some key ways this paper compares to other research in the field:

- This is the first work attempting to combine multilingual pre-training and multimodal pre-training in a unified framework, while most existing works focus on either multilingual or multimodal pre-training separately. 

- The proposed Multimodal Code-switched Training (MCT) method is novel and effectively improves multilingual transfer ability, especially in low-resource settings. Other works on code-switching have focused only on text, not multimodal data.

- The model achieves new state-of-the-art results on multilingual image-text retrieval, significantly outperforming prior multilingual baselines. For English, it obtains comparable results to current monolingual multimodal models.

- Extensive ablation studies are provided to analyze the impact of different model components. The analyses on the number of MCT languages and different pre-training tasks give useful insights.

- The qualitative analysis on MCT gives an intuitive explanation of why it is effective for non-English languages. The discussion of its potential limitation is also informative.

Overall, this paper pushes forward multimodal pre-training to multilingual scenarios for the first time. The proposed approaches and thorough experiments demonstrate strong improvements over existing multilingual works. The analyses also provide valuable insights to guide future research in this direction. This paper makes significant contributions to the field of multilingual multimodal pre-training and retrieval.


## What future research directions do the authors suggest?

 Based on my reading of the paper, some of the main future research directions suggested by the authors include:

- Expanding M3P to support even more languages, beyond the 100 it currently handles. They suggest continuing to expand the language coverage.

- Improving the multilingual transfer ability of M3P, especially in low-resource settings. They propose ideas like enhancing the Multimodal Code-switched Training (MCT) method.

- Evaluating M3P on more multimodal downstream tasks beyond just image-text retrieval. The authors suggest testing it on tasks like multimodal machine translation.

- Addressing limitations of the current Multimodal Code-switched Training approach, like handling inaccurate translations or grammatical/syntactic errors in the code-switched sentences.

- Incorporating higher quality machine translation and multilingual datasets into the training data when they become available in the future, to further improve performance.

- Expanding the qualitative and quantitative analysis of the model to better understand the impact of different components like MCT and the various pre-training tasks.

- Investigating how to best fine-tune and adapt the model for optimal performance on specific multilingual-multimodal tasks.

So in summary, the main directions are around expanding language support, improving multilingual transfer, testing on more downstream tasks, addressing current limitations of the MCT approach, incorporating better datasets when available, and further analysis/adaptation of the model. The authors seem to propose an extensive research agenda to build on their proposed M3P model.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the paper:

The paper presents M$^3$P, a Multitask Multilingual Multimodal Pre-trained model that combines multilingual pre-training and multimodal pre-training into a unified framework via multitask pre-training. The goal is to learn universal representations that can map objects in different modalities (image, video, etc.) or different languages into a common semantic space. To help with the lack of labeled non-English multimodal data, the authors propose Multimodal Code-switched Training (MCT) which randomly replaces some English words with their translations to encourage alignment between images and non-English text. Experiments on multilingual image retrieval across MSCOCO and Multi30K datasets show M$^3$P achieves state-of-the-art results for non-English languages and comparable results to English-only models for English. The authors also conduct ablation studies demonstrating the benefits of MCT and each pre-training task.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the paper:

The paper presents a new pre-trained model called M3P, which combines multilingual pre-training and multimodal pre-training into a unified framework via multitask pre-training. The goal is to learn universal representations that can map objects in different modalities (text, images) and different languages into a common semantic space. The challenges are that existing multilingual models cannot handle vision data, while existing multimodal models are mostly trained on English data. The paper introduces Multimodal Code-switched Training (MCT) to address the lack of non-English multimodal data by generating code-switched text-image pairs during pre-training. Experiments on multilingual image retrieval demonstrate that M3P achieves state-of-the-art results for non-English languages and comparable performance to monolingual models for English. Ablation studies validate the benefits of using MCT and employing multiple pre-training objectives.

In summary, the key ideas presented are:

1) M3P combines multilingual and multimodal pre-training via multitask learning to create universal multilingual multimodal representations. 

2) MCT generates code-switched text-image pairs to provide more non-English alignment and improve multilingual transfer ability.

3) Experiments show M3P achieves excellent results on multilingual image retrieval, outperforming prior multilingual and monolingual models. Ablations verify the contributions of the proposed model components.


## Summarize the main method used in the paper in one paragraph.

 Here is a one paragraph summary of the main method presented in the paper:

The paper proposes M$^3$P, a novel pre-trained model that combines multilingual pre-training and multimodal pre-training into a unified framework via multitask learning. M$^3$P is pre-trained on a multilingual text corpus (sentences from Wikipedia covering 100 languages) using masked language modeling, and on an English image-caption dataset using tasks like masked language modeling, masked region modeling, and visual-linguistic matching. To allow M$^3$P to learn alignments between images and non-English text, the authors propose a Multimodal Code-switched Training (MCT) method, where some English words in the image captions are randomly replaced by their translations in other languages using bilingual dictionaries. The combination of multitask learning over diverse pre-training tasks and the code-switching strategy allows M$^3$P to learn universal multilingual multimodal representations that can effectively handle vision-language tasks involving non-English text. Extensive experiments on multilingual image-text retrieval demonstrate M$^3$P's superior cross-lingual transfer capabilities.


## What problem or question is the paper addressing?

 The paper is addressing the challenge of extending pre-trained language models to multilingual-multimodal scenarios. The key problems it identifies are:

1. Existing multilingual pre-trained models like BERT can handle text in multiple languages but cannot process visual data like images. 

2. Existing multimodal models like ViLBERT are pre-trained on English image-text data so do not work well for non-English languages.

3. There is a lack of high-quality multilingual multimodal training data with aligned image-text pairs in diverse languages. Relying on machine translation would be expensive.

4. There is a lack of techniques to learn explicit alignments between images and non-English text during pre-training.

To address these limitations, the paper proposes a new pre-trained model called M3P that combines multilingual pre-training and multimodal pre-training into a unified framework using multitask learning. The key idea is to learn universal multilingual-multimodal representations that map both textual and visual inputs in different languages to a common semantic space.

In summary, the key problem is extending monolingual or unimodal pre-trained models to the multilingual and multimodal setting given the lack of aligned non-English training data. M3P aims to address this through its model architecture and training techniques.
