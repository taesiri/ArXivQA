# [Vision Transformer with Super Token Sampling](https://arxiv.org/abs/2211.11167)

## What is the central research question or hypothesis that this paper addresses?

Based on my review, the central research question this paper addresses is: How can we design an efficient vision transformer that can capture both global context and local features effectively, especially in the early layers?Specifically, the authors identify that standard vision transformers suffer from high redundancy when capturing local features in early layers, since the global self-attention concentrates on just a few neighboring tokens. Methods like local attention and early convolution give up global context modeling to reduce this redundancy. To address this, the paper proposes a "super token attention" mechanism that efficiently captures global context in early layers by performing self-attention on learned "super tokens" that provide a compact summary of the image content. This allows global modeling while reducing redundancy.The overall contribution is a new vision transformer backbone called Super Token Vision Transformer (SViT) that combines super token attention, convolutional feature enhancement, and a hierarchical architecture to capture both global context and local features efficiently.In summary, the central hypothesis is that super token attention can enable efficient global modeling in early transformer layers for vision, overcoming limitations of prior work. The paper proposes and evaluates the SViT architecture to validate this idea.


## What is the main contribution of this paper?

Based on my reading, the main contributions of this paper are:1. It proposes a new vision transformer architecture called Super Token Vision Transformer (SViT) that aims to learn efficient global representations in the early stages. 2. It introduces a Super Token Attention (STA) mechanism that efficiently captures global dependencies by performing self-attention on "super tokens" - compact aggregated representations of visual content. 3. The STA module has three main steps:   - Super token sampling to aggregate visual tokens into semantically meaningful super tokens.   - Self-attention on super tokens to model global dependencies.   - Mapping super tokens back to original token space.4. This allows the model to preserve global modeling capacity while reducing redundancy and computation compared to standard self-attention.5. Based on STA, the paper develops a hierarchical vision transformer backbone with convolutional layers called SViT.6. Experiments on image classification, object detection, instance segmentation and semantic segmentation demonstrate SViT's effectiveness and superiority over previous SOTA vision transformers.In summary, the core idea is using super tokens to reduce redundancy and enable efficient global modeling in early layers of vision transformers. The proposed SViT architecture with STA achieves strong performance across multiple vision tasks.
