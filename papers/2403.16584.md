# [Can Large Language Models (or Humans) Distill Text?](https://arxiv.org/abs/2403.16584)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
The paper investigates whether large language models (LLMs) can "distill" text - remove traces of an unwanted variable while preserving other relevant signals. For example, removing demographic information from text while keeping topical content. This is an important capability for using text in social science research or for making models invariant to certain variables.

Method: 
The authors test LLMs' ability to distill the sentiment (positive/negative) from Amazon reviews while retaining signals about the topic (book, music etc). They use Mistral 7B and GPT-4 models in a few-shot prompting setup, where the model is given 3 examples of how to distill the text. As baselines, they test a representation method called mean projection, as well as human annotators. 

Results:
The results show current LLMs struggle with this distillation task. The best LLM (GPT-4 with prompt chaining) reduced sentiment classification accuracy to 75.7% (chance is 50%) while keeping topic accuracy at 94.5%. The human annotators achieved similar performance. However, mean projection operating on representations could reduce sentiment accuracy to 52.4% while perfectly retaining topic accuracy.

Implications: 
The results suggest there are limitations in separating some variables from raw text, even for humans. This highlights difficulties for methods relying on text transformations. It also raises questions about robustness of representation methods if they achieve independence not feasible at the text level.

Contributions:
- Formalizes the problem of "text distillation" 
- Provides benchmark of LLM performance on distilling sentiment from text
- Compares LLM prompting strategies 
- Analyzes tradeoffs faced even by human annotators
- Discusses implications for text processing methods in social sciences
