# [Human-compatible driving partners through data-regularized self-play   reinforcement learning](https://arxiv.org/abs/2403.19648)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem: Developing autonomous vehicles (AVs) that can smoothly interact and coordinate with human drivers is very challenging, yet crucial for safe real-world deployment. Existing simulation agents typically used to model human drivers in simulators either behave unrealistically (e.g. rule-based), do not react to changes (e.g. log replay), or are not effective in closed-loop multi-agent settings (e.g. imitation learning). The key challenge is to have driving agents that are both realistic and compatible with human behavior, while still able to achieve driving goals effectively without collisions or getting stuck.

Proposed Solution: The paper proposes Human-Regularized PPO (HR-PPO), a multi-agent reinforcement learning algorithm where agents are trained using self-play but penalized for deviating too much from a reference human policy. The human policy is obtained using simple behavioral cloning on just 30 minutes of imperfect driving demonstration data from a single human driver. It serves as a soft constraint to nudge the RL agents towards human-like behavior.  

Main Contributions:
- Demonstrate that human-compatible and effective driving agents can be achieved simultaneously using the proposed HR-PPO approach, with collision rates under 3\%, off-road rates under 4\%, and 93\% goal rate.  
- Show benefits over both pure imitation learning and pure self-play multi-agent RL: HR-PPO agents outperform PPO agents on proxy measures of human compatibility (trajectory similarity, action matching) often with no cost of effectiveness.
- Establish that training jointly in multi-agent closed-loop settings is better than single-agent training - even when tested against human logs rather than trained partners.
- Provide insights on coordination capability: HR-PPO has lower rate of collisions against human logs compared to PPO, especially in highly interactive traffic scenarios, suggesting improved compatibility.

The open-sourced code, trained policies and videos showing agent driving behavior are provided to serve as improved simulation agents and to facilitate future research.
