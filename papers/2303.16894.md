# ViewRefer: Grasp the Multi-view Knowledge for 3D Visual Grounding with
  GPT and Prototype Guidance

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is:How can we explicitly grasp the view knowledge from both text and 3D modalities to further boost the 3D grounding performance?The key hypothesis is that by capturing sufficient view cues from both the text and 3D data, the model can better understand the spatial inter-object relations and achieve superior performance on 3D visual grounding. Specifically, the paper proposes to:- Leverage large language models to expand the input text with view-related descriptions, providing more view cues in the text modality.- Introduce a fusion transformer with inter-view attention to enhance cross-view interaction and aggregation of multi-view 3D information.- Present multi-view prototypes to provide high-level guidance on grasping view knowledge, through view-guided textual context and scoring strategy.By explicitly modeling the view dependence in both text and 3D data, the paper hypothesizes that the model can alleviate the view discrepancy issue and achieve state-of-the-art results on 3D visual grounding benchmarks. The experiments verify this hypothesis and demonstrate the effectiveness of the proposed ViewRefer framework.In summary, the central question is how to leverage multi-view knowledge to boost 3D visual grounding, which is addressed through capturing view cues in both text and 3D modalities via the proposed techniques. The core hypothesis is that explicit view modeling will improve grounding performance by alleviating view discrepancy.
