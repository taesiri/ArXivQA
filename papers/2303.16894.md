# ViewRefer: Grasp the Multi-view Knowledge for 3D Visual Grounding with   GPT and Prototype Guidance

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question is:How can we explicitly grasp the view knowledge from both text and 3D modalities to further boost the 3D grounding performance?The key hypothesis is that by capturing sufficient view cues from both the text and 3D data, the model can better understand the spatial inter-object relations and achieve superior performance on 3D visual grounding. Specifically, the paper proposes to:- Leverage large language models to expand the input text with view-related descriptions, providing more view cues in the text modality.- Introduce a fusion transformer with inter-view attention to enhance cross-view interaction and aggregation of multi-view 3D information.- Present multi-view prototypes to provide high-level guidance on grasping view knowledge, through view-guided textual context and scoring strategy.By explicitly modeling the view dependence in both text and 3D data, the paper hypothesizes that the model can alleviate the view discrepancy issue and achieve state-of-the-art results on 3D visual grounding benchmarks. The experiments verify this hypothesis and demonstrate the effectiveness of the proposed ViewRefer framework.In summary, the central question is how to leverage multi-view knowledge to boost 3D visual grounding, which is addressed through capturing view cues in both text and 3D modalities via the proposed techniques. The core hypothesis is that explicit view modeling will improve grounding performance by alleviating view discrepancy.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contributions appear to be:1. Proposing ViewRefer, a multi-view framework for 3D visual grounding that grasps view knowledge from both text and 3D modalities to address the view discrepancy issue. 2. Introducing LLM-expanded grounding texts to leverage large language models' linguistic knowledge to expand a single text into multiple view-related descriptions. This enriches the text modality with more view cues.3. Designing a fusion transformer with inter-view attention to allow interaction and information flow between different views of the 3D modality. This helps capture multi-view knowledge.  4. Presenting multi-view prototypes that provide high-level guidance to the framework in two ways - generating view-guided context for text features and weighting views through a scoring strategy. This further enhances multi-modal fusion for grounding.5. Conducting extensive experiments on 3 benchmark datasets where the proposed ViewRefer framework outperforms prior state-of-the-art methods, demonstrating its effectiveness in grasping view knowledge for superior 3D visual grounding performance.In summary, the main contribution appears to be the novel ViewRefer framework that grasps view knowledge from both text and 3D modalities through designed components like LLM-expanded texts, fusion transformer, and multi-view prototypes. This provides an effective solution to handle the challenging view discrepancy issue in 3D visual grounding.
