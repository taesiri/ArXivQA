# [Perception Test: A Diagnostic Benchmark for Multimodal Video Models](https://arxiv.org/abs/2305.13786)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper abstract, it seems the main goal of this paper is to propose a new multimodal video benchmark called the "Perception Test" to evaluate the perception and reasoning skills of pre-trained multimodal models across different modalities (video, audio, text). The key aspects that set this benchmark apart are:- It focuses on evaluating skills and reasoning abilities rather than just performance on computational tasks like classification or detection. The skills tested include memory, abstraction, physics, semantics. - The reasoning abilities tested include descriptive, explanatory, predictive and counterfactual reasoning.- It uses purposefully designed real-world videos rather than just publicly available videos to systematically probe the skills and reasoning. - The videos are densely annotated with multiple types of labels (object tracks, point tracks, action/sound segments, multiple choice QA, grounded QA) to enable thorough evaluation.- The emphasis is on assessing transfer capabilities in a zero-shot or limited fine-tuning setting rather than in-domain performance.So in summary, the key hypothesis seems to be that this new benchmark can more robustly and efficiently assess the general perception abilities of multimodal models compared to prior datasets focused on individual tasks. The paper aims to demonstrate this through the careful construction and annotation of the dataset, definition of suitable tasks, and evaluation of multiple strong baselines.
