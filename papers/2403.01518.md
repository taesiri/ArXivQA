# [Revisiting Dynamic Evaluation: Online Adaptation for Large Language   Models](https://arxiv.org/abs/2403.01518)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem Statement:
- Language models like transformers rely heavily on the context window, but the context size is limited due to computational constraints. This is an issue when adapting the model at test time to longer sequences or distributional shifts. 
- Online adaptation (dynamic evaluation) updates model parameters on the fly and can help address this, but it has computational overhead.

Proposed Solution:
- Investigate online adaptation by updating parameters with gradient descent as the model processes a long text sequence.
- Use overlapping sequences and Transformer-XL style caching to enable processing longer sequences.
- Explore tradeoffs between performance and compute by varying update frequency and using LoRA for lower-rank adaptation.

Key Contributions:
- Show online adaptation blurs the distinction between in-context learning and finetuning as ways to condition the model on previously seen tokens.
- Find online adaptation is especially beneficial with distribution shift and smaller context, suggesting parameters capture longer-term information than context tokens.  
- Observe smaller models with online adaptation can match/exceed larger model performance, trading off memory for compute.
- LoRA adaptation achieves good performance with lower overhead.
- Simpler resetting of weights between books is a competitive strategy, avoiding overspecialization.
- Overall, extensive study provides insights on the sample efficiency, sensitivity to distribution drift, and computational tradeoffs of online adaptation.

In summary, the key message is that online adaptation offers complementary memory in weights to the memory in activations from context, with tradeoffs to leverage between them. The work provides an analysis to guide when and how to effectively apply online adaptation.
