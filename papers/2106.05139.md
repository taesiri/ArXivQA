# [Pretrained Encoders are All You Need](https://arxiv.org/abs/2106.05139)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the main research question seems to be: Can pretrained image representations and attention models be effectively leveraged for state representation learning in reinforcement learning, without requiring training on large domain-specific datasets?In particular, the paper investigates:1) Using pretrained image encoders like CLIP for generating state representations.2) Utilizing spatial and temporal attention models like optical flow and image differencing to focus on relevant aspects. 3) Fine-tuning the pretrained representations in a self-supervised manner using techniques like contrastive predictive coding.The overarching goal is to develop an approach for sample-efficient and generalizable state representation learning in RL by leveraging pretrained self-supervised models, without needing domain-specific training data.The hypotheses seem to be:- Pretrained encoders can generate useful state representations for RL without domain-specific training.- Attention can help focus on salient spatial/temporal elements to improve representations. - Self-supervised fine-tuning can further enhance representations, even with limited in-domain data.The experiments aim to test these hypotheses for state representation learning on Atari game frames.
