# [Is One Epoch All You Need For Multi-Fidelity Hyperparameter   Optimization?](https://arxiv.org/abs/2307.15422)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the key research question this paper seeks to address is:Can a very simple baseline approach perform comparably to more complex multi-fidelity hyperparameter optimization methods on common benchmarks? The authors hypothesize that a simple approach of selecting the top-K models after only 1 training epoch, and then fully training just those models, can achieve competitive performance compared to more sophisticated methods. This is quite counterintuitive, as one would expect more complex methods that leverage intermediate fidelities to outperform such a simple technique. To test this, the authors implement a 1-Epoch baseline alongside other multi-fidelity HPO methods like Successive Halving, Hyperband, and Learning Curve Extrapolation. They evaluate these on popular benchmarks like HPOBench, LCBench, YAHPO-Gym, and JAHS-Bench. The central hypothesis is that despite their complexity, the advanced HPO methods will not significantly outperform the surprisingly simple 1-Epoch baseline that just trains each configuration for 1 epoch initially. The paper seems aimed at testing this surprising hypothesis on common benchmark datasets.


## What is the main contribution of this paper?

Based on my reading, the main contributions of this paper appear to be:- Introducing a simple 1-Epoch baseline for multi-fidelity hyperparameter optimization (MF-HPO). This involves selecting the top-K candidates after only 1 training epoch, and then training those candidates fully to pick the best one.- Comparing this 1-Epoch baseline to other more complex MF-HPO methods like Successive Halving, Hyperband, and learning curve extrapolation on several benchmarks. - Showing that the simple 1-Epoch baseline actually performs competitively (often comparably) to the more complex methods, while requiring significantly less computation.- Analyzing the learning curves on the benchmarks to explain why the 1-Epoch method works well - there are a few dominant good learning curves that can be identified after only 1 epoch.- Arguing that the 1-Epoch baseline should be routinely used in future MF-HPO benchmarks, and that more complex benchmarks need to be developed where it would not work as well.So in summary, the main contribution is introducing and analyzing the surprising effectiveness of a very simple 1-Epoch baseline for multi-fidelity HPO, and discussing the implications this has for developing better benchmarks going forward. The simplicity yet strong performance of 1-Epoch provides a new baseline that future MF-HPO methods should compare against.
