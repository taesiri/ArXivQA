# [Is One Epoch All You Need For Multi-Fidelity Hyperparameter   Optimization?](https://arxiv.org/abs/2307.15422)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the key research question this paper seeks to address is:Can a very simple baseline approach perform comparably to more complex multi-fidelity hyperparameter optimization methods on common benchmarks? The authors hypothesize that a simple approach of selecting the top-K models after only 1 training epoch, and then fully training just those models, can achieve competitive performance compared to more sophisticated methods. This is quite counterintuitive, as one would expect more complex methods that leverage intermediate fidelities to outperform such a simple technique. To test this, the authors implement a 1-Epoch baseline alongside other multi-fidelity HPO methods like Successive Halving, Hyperband, and Learning Curve Extrapolation. They evaluate these on popular benchmarks like HPOBench, LCBench, YAHPO-Gym, and JAHS-Bench. The central hypothesis is that despite their complexity, the advanced HPO methods will not significantly outperform the surprisingly simple 1-Epoch baseline that just trains each configuration for 1 epoch initially. The paper seems aimed at testing this surprising hypothesis on common benchmark datasets.
