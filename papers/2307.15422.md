# [Is One Epoch All You Need For Multi-Fidelity Hyperparameter   Optimization?](https://arxiv.org/abs/2307.15422)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the key research question this paper seeks to address is:Can a very simple baseline approach perform comparably to more complex multi-fidelity hyperparameter optimization methods on common benchmarks? The authors hypothesize that a simple approach of selecting the top-K models after only 1 training epoch, and then fully training just those models, can achieve competitive performance compared to more sophisticated methods. This is quite counterintuitive, as one would expect more complex methods that leverage intermediate fidelities to outperform such a simple technique. To test this, the authors implement a 1-Epoch baseline alongside other multi-fidelity HPO methods like Successive Halving, Hyperband, and Learning Curve Extrapolation. They evaluate these on popular benchmarks like HPOBench, LCBench, YAHPO-Gym, and JAHS-Bench. The central hypothesis is that despite their complexity, the advanced HPO methods will not significantly outperform the surprisingly simple 1-Epoch baseline that just trains each configuration for 1 epoch initially. The paper seems aimed at testing this surprising hypothesis on common benchmark datasets.


## What is the main contribution of this paper?

Based on my reading, the main contributions of this paper appear to be:- Introducing a simple 1-Epoch baseline for multi-fidelity hyperparameter optimization (MF-HPO). This involves selecting the top-K candidates after only 1 training epoch, and then training those candidates fully to pick the best one.- Comparing this 1-Epoch baseline to other more complex MF-HPO methods like Successive Halving, Hyperband, and learning curve extrapolation on several benchmarks. - Showing that the simple 1-Epoch baseline actually performs competitively (often comparably) to the more complex methods, while requiring significantly less computation.- Analyzing the learning curves on the benchmarks to explain why the 1-Epoch method works well - there are a few dominant good learning curves that can be identified after only 1 epoch.- Arguing that the 1-Epoch baseline should be routinely used in future MF-HPO benchmarks, and that more complex benchmarks need to be developed where it would not work as well.So in summary, the main contribution is introducing and analyzing the surprising effectiveness of a very simple 1-Epoch baseline for multi-fidelity HPO, and discussing the implications this has for developing better benchmarks going forward. The simplicity yet strong performance of 1-Epoch provides a new baseline that future MF-HPO methods should compare against.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper evaluates multi-fidelity hyperparameter optimization methods and finds that a simple baseline of selecting the top models after only one epoch of training performs similarly to more complex methods while requiring significantly less computation.


## How does this paper compare to other research in the same field?

This paper makes several interesting contributions to the field of multi-fidelity hyperparameter optimization (MF-HPO). Here is a summary of how it compares to related work:- It introduces a simple but surprisingly effective 1-Epoch baseline for early stopping/discarding low-performing configurations. Using just 1 training epoch to assess candidates allows drastically reducing compute, yet it performs comparably to more complex methods like SHA, HB, and LCE on several benchmarks. This simple idea was not explored in prior work.- It conducts an extensive empirical study across several MF-HPO benchmarks like HPOBench, LCBench, YAHPO-Gym, and JAHS-Bench. Many prior papers evaluate on only 1 or 2 benchmarks, so the breadth here is useful.- It provides visualizations of learning curves on these benchmarks to explain when and why the 1-Epoch approach works well - namely when good and bad configurations separate quickly. This analysis is valuable for characterizing when the approach may or may not be effective.- It argues for inclusion of the 1-Epoch baseline in future benchmark studies, to prevent claims of gains over strawman approaches. At least one other recent paper (Pasha) has also adopted this baseline.- It does not involve meta-learning or assumptions about curve shapes. Some methods like Freeze-Thaw BO or LCE with SVR/Bayesian NN try to learn predictive models, which may limit generality. This work has a simple approach.- It focuses on the inner loop early stopping part of MF-HPO, rather than the outer loop optimization algorithm. So it complements papers that propose better outer loop sampling.Overall, by combining empirical breadth, visual analysis, and introduction of a strong but previously overlooked baseline, this paper makes a useful contribution compared to prior MF-HPO work. The insights on quickly separating curves are applicable across many problems.


## What future research directions do the authors suggest?

Based on my reading of the paper, some of the main future research directions suggested by the authors are:- Developing new and more challenging benchmarks for multi-fidelity hyperparameter optimization. The authors found that a simple 1-epoch training baseline performed surprisingly well on many existing benchmarks due to characteristics of the learning curves like the presence of dominant "good" learning curves. They suggest the need for benchmarks with more diversity and complexity to better evaluate MF-HPO methods.- Considering different units of fidelity beyond just epochs of training. The authors note their work focused on epochs as the fidelity unit, but suggest exploring other options like wall-clock time or number of examples that may be more reflective of real-world costs. - Incorporating more sophisticated model selection schemes. The authors highlight that many MF-HPO methods simply return the best validation performance observed during search, but suggest integrating proper methods like cross-validation could lead to better final selected models.- Developing MF-HPO methods that are robust to noise and do not assume learning curves will not cross in the future if they are currently ranked. The "short-horizon bias" of some methods like Hyperband is a limitation.- Exploring ways to transfer knowledge about learning curves across problems. The authors' 1-epoch baseline worked without any prior curve knowledge, but meta-learning across tasks could further improve efficiency.- Considering additional aspects of the full pipeline like data preprocessing as part of the hyperparameter optimization process. The authors focused just on model hyperparameters.- Scaling up MF-HPO approaches to handle settings like large language models where training even at low fidelity may be very costly. Special methods may be needed in such cases.In summary, the authors advocate for more complex benchmarks, evaluating different fidelity types, improved model selection, and developing methods robust to noise and short-horizon bias. Transfer learning and expanding beyond just model hyperparameters are also mentioned as interesting future directions.
