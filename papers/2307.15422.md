# [Is One Epoch All You Need For Multi-Fidelity Hyperparameter   Optimization?](https://arxiv.org/abs/2307.15422)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the key research question this paper seeks to address is:Can a very simple baseline approach perform comparably to more complex multi-fidelity hyperparameter optimization methods on common benchmarks? The authors hypothesize that a simple approach of selecting the top-K models after only 1 training epoch, and then fully training just those models, can achieve competitive performance compared to more sophisticated methods. This is quite counterintuitive, as one would expect more complex methods that leverage intermediate fidelities to outperform such a simple technique. To test this, the authors implement a 1-Epoch baseline alongside other multi-fidelity HPO methods like Successive Halving, Hyperband, and Learning Curve Extrapolation. They evaluate these on popular benchmarks like HPOBench, LCBench, YAHPO-Gym, and JAHS-Bench. The central hypothesis is that despite their complexity, the advanced HPO methods will not significantly outperform the surprisingly simple 1-Epoch baseline that just trains each configuration for 1 epoch initially. The paper seems aimed at testing this surprising hypothesis on common benchmark datasets.


## What is the main contribution of this paper?

Based on my reading, the main contributions of this paper appear to be:- Introducing a simple 1-Epoch baseline for multi-fidelity hyperparameter optimization (MF-HPO). This involves selecting the top-K candidates after only 1 training epoch, and then training those candidates fully to pick the best one.- Comparing this 1-Epoch baseline to other more complex MF-HPO methods like Successive Halving, Hyperband, and learning curve extrapolation on several benchmarks. - Showing that the simple 1-Epoch baseline actually performs competitively (often comparably) to the more complex methods, while requiring significantly less computation.- Analyzing the learning curves on the benchmarks to explain why the 1-Epoch method works well - there are a few dominant good learning curves that can be identified after only 1 epoch.- Arguing that the 1-Epoch baseline should be routinely used in future MF-HPO benchmarks, and that more complex benchmarks need to be developed where it would not work as well.So in summary, the main contribution is introducing and analyzing the surprising effectiveness of a very simple 1-Epoch baseline for multi-fidelity HPO, and discussing the implications this has for developing better benchmarks going forward. The simplicity yet strong performance of 1-Epoch provides a new baseline that future MF-HPO methods should compare against.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper evaluates multi-fidelity hyperparameter optimization methods and finds that a simple baseline of selecting the top models after only one epoch of training performs similarly to more complex methods while requiring significantly less computation.


## How does this paper compare to other research in the same field?

This paper makes several interesting contributions to the field of multi-fidelity hyperparameter optimization (MF-HPO). Here is a summary of how it compares to related work:- It introduces a simple but surprisingly effective 1-Epoch baseline for early stopping/discarding low-performing configurations. Using just 1 training epoch to assess candidates allows drastically reducing compute, yet it performs comparably to more complex methods like SHA, HB, and LCE on several benchmarks. This simple idea was not explored in prior work.- It conducts an extensive empirical study across several MF-HPO benchmarks like HPOBench, LCBench, YAHPO-Gym, and JAHS-Bench. Many prior papers evaluate on only 1 or 2 benchmarks, so the breadth here is useful.- It provides visualizations of learning curves on these benchmarks to explain when and why the 1-Epoch approach works well - namely when good and bad configurations separate quickly. This analysis is valuable for characterizing when the approach may or may not be effective.- It argues for inclusion of the 1-Epoch baseline in future benchmark studies, to prevent claims of gains over strawman approaches. At least one other recent paper (Pasha) has also adopted this baseline.- It does not involve meta-learning or assumptions about curve shapes. Some methods like Freeze-Thaw BO or LCE with SVR/Bayesian NN try to learn predictive models, which may limit generality. This work has a simple approach.- It focuses on the inner loop early stopping part of MF-HPO, rather than the outer loop optimization algorithm. So it complements papers that propose better outer loop sampling.Overall, by combining empirical breadth, visual analysis, and introduction of a strong but previously overlooked baseline, this paper makes a useful contribution compared to prior MF-HPO work. The insights on quickly separating curves are applicable across many problems.


## What future research directions do the authors suggest?

Based on my reading of the paper, some of the main future research directions suggested by the authors are:- Developing new and more challenging benchmarks for multi-fidelity hyperparameter optimization. The authors found that a simple 1-epoch training baseline performed surprisingly well on many existing benchmarks due to characteristics of the learning curves like the presence of dominant "good" learning curves. They suggest the need for benchmarks with more diversity and complexity to better evaluate MF-HPO methods.- Considering different units of fidelity beyond just epochs of training. The authors note their work focused on epochs as the fidelity unit, but suggest exploring other options like wall-clock time or number of examples that may be more reflective of real-world costs. - Incorporating more sophisticated model selection schemes. The authors highlight that many MF-HPO methods simply return the best validation performance observed during search, but suggest integrating proper methods like cross-validation could lead to better final selected models.- Developing MF-HPO methods that are robust to noise and do not assume learning curves will not cross in the future if they are currently ranked. The "short-horizon bias" of some methods like Hyperband is a limitation.- Exploring ways to transfer knowledge about learning curves across problems. The authors' 1-epoch baseline worked without any prior curve knowledge, but meta-learning across tasks could further improve efficiency.- Considering additional aspects of the full pipeline like data preprocessing as part of the hyperparameter optimization process. The authors focused just on model hyperparameters.- Scaling up MF-HPO approaches to handle settings like large language models where training even at low fidelity may be very costly. Special methods may be needed in such cases.In summary, the authors advocate for more complex benchmarks, evaluating different fidelity types, improved model selection, and developing methods robust to noise and short-horizon bias. Transfer learning and expanding beyond just model hyperparameters are also mentioned as interesting future directions.


## Summarize the paper in one paragraph.

The paper presents a comparison of multi-fidelity hyperparameter optimization methods against a simple 1-epoch baseline on several benchmarks. The key findings are:The 1-epoch baseline, which trains each configuration for only 1 epoch during the search and then fully trains the top K candidates, performs similarly to more complex methods like Successive Halving, Hyperband, and Learning Curve Extrapolation. This is surprising since the 1-epoch method uses much less compute. By visualizing the learning curves, the authors find dominant good and bad curves that can be identified after only 1 epoch of training. This explains why the 1-epoch baseline works well. The authors argue that the 1-epoch baseline should be included in future benchmark studies on multi-fidelity HPO. They also suggest creating new harder benchmarks where good configurations can't be identified so quickly.In summary, this paper demonstrates the effectiveness of a simple 1-epoch training baseline for multi-fidelity HPO on current benchmarks, and suggests improvements for developing more challenging benchmarks in this area. The key insight is that dominant learning curves allow good configurations to be identified early.


## Summarize the paper in two paragraphs.

Here is a two paragraph summary of the key points from the paper:The paper presents an empirical study evaluating various multi-fidelity hyperparameter optimization (MF-HPO) methods on benchmark datasets. MF-HPO methods leverage intermediate accuracy levels during model training to reduce computational costs. The authors compare several representative MF-HPO methods against two simple baselines - one trains models for the maximum number of epochs, while the other trains models for just one epoch before selection. Surprisingly, the 1-epoch baseline achieves competitive performance to more complex methods, while requiring significantly less computation. The authors analyze the learning curves on the benchmarks and find that good and bad models can often be identified after only 1 epoch of training. This indicates a lack of diversity and complexity in many existing MF-HPO benchmarks. Based on these findings, the authors argue for always including the 1-epoch baseline in future benchmarks, and developing new benchmarks with more complex learning curves that better represent real-world scenarios. Overall, this study provides new insights into the limitations of current MF-HPO methods and benchmarks, and suggestions for developing more effective techniques going forward.
