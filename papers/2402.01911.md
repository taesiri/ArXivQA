# [From PEFT to DEFT: Parameter Efficient Finetuning for Reducing   Activation Density in Transformers](https://arxiv.org/abs/2402.01911)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper "From PEFT to DEFT: Parameter Efficient Finetuning for Reducing Activation Density in Transformers":

Problem:
- Pre-trained large language models (LLMs) like BERT have become standard for fine-tuning on downstream NLP tasks. However, training and inference with these large models is computationally expensive.
- Recent studies have shown activation sparsity exists in the intermediate outputs of MLP blocks in transformers, meaning only a fraction of neurons are activated per input. This enables efficient inference on hardware leveraging zero-skip operations.

Proposed Solution:
- The paper proposes a density loss function to encourage higher activation sparsity (lower density) during fine-tuning of LLMs. 
- The density loss is combined with parameter-efficient fine-tuning (PEFT) methods like adapters, LoRA, prefix tuning etc. which adapt models with fewer trainable parameters.
- This joint training approach is called DEFT - Density Efficient Fine-Tuning. It induces sparser activations while preserving performance.

Main Contributions:
- DEFT reduces activation density consistently across tasks, models and PEFT methods, with drops up to 50.72% for RoBERTa and over 50% for 11B parameter Flan-T5 model.
- DEFT shows greater density reduction for larger model sizes, confirming its scalability.
- The sparse activation patterns from DEFT lead to energy savings on hardware leveraging zero-skip operations for inference. Energy drop is up to 15% on Flan-T5.
- Ablation studies demonstrate the density loss weight and approximation hyperparameters can balance between density and performance.
- DEFT works compatibly with model compression methods like quantization and pruning, enabling compound efficiency.

In summary, the paper proposes an effective method DEFT to induce activation sparsity in transformers during fine-tuning while maintaining performance. This facilitates more efficient inference, especially on sparse-aware hardware.
