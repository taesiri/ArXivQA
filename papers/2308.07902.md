# [Through the Lens of Core Competency: Survey on Evaluation of Large   Language Models](https://arxiv.org/abs/2308.07902)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question seems to be: How to provide a comprehensive overview and clarification of the numerous evaluation benchmarks and tasks proposed for large language models (LLMs) through the lens of core competencies?The authors note that many evaluation benchmarks and hundreds of tasks have been proposed to evaluate the capabilities of LLMs. However, these tasks are disparate and overwhelming. To address this, the authors propose summarizing the evaluation of LLMs around four core competencies - knowledge, reasoning, reliability, and safety. They aim to synthesize similar evaluation tasks under these competencies to provide a coherent framework for understanding LLM evaluations.The central hypothesis seems to be that organizing LLM evaluation benchmarks and tasks under these four competencies will provide a clearer, more systematic way to understand the current landscape of LLM evaluation research. The competency framework can assimilate similar tasks, highlight key abilities, and also allow new tasks to be integrated.In summary, the key research question is how to bring coherence and structure to the numerous proposed LLM evaluation benchmarks and tasks through the lens of core competencies. The hypothesis is that this will enable better understanding and synthesis of work in this area.


## What is the main contribution of this paper?

 Based on my reading of the paper, the main contribution is proposing a framework to evaluate large language models (LLMs) through the lens of core competencies. The key points are:- The paper summarizes 4 core competencies of LLMs: knowledge, reasoning, reliability, and safety. For each competency, the authors provide a definition, taxonomy, and evaluation metrics. - The competency framework helps integrate the numerous evaluation benchmarks and tasks proposed for LLMs. By mapping tasks to competencies, the evaluation results can be presented in a more structured and focused way.- The competency view also allows new tasks to be comprehensively added. The authors have created an extensible project to display the relationships between competencies and tasks.- The paper surveys over 540 tasks used in various papers to evaluate LLMs. Tasks are aggregated under the competency they aim to evaluate. This provides a systematic organization of the diverse evaluation landscape.- Future competencies like sentiment, planning, and code competency are also discussed. The competency paradigm gives a direction to continue expanding LLM evaluations.In summary, the key contribution is using the competency concept to bring structure and clarity to the complex evaluation landscape for LLMs. The competency framework helps integrate disparate efforts and benchmark progress in a more meaningful way.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the paper:The paper surveys evaluation benchmarks for large language models, organizing them into 4 core competencies - knowledge, reasoning, reliability, and safety - and providing definitions, taxonomies, example datasets, and evaluation metrics for each competency.


## How does this paper compare to other research in the same field?

 Based on my reading of the paper, here is a summary of how it compares to other research on evaluating large language models:- Focuses on conceptualizing evaluation through the lens of "core competencies" - This provides a more structured and methodical way of evaluating LLMs compared to some other works that propose a wide array of tasks/benchmarks. The competency framework allows for combining similar tasks and adding new ones in an organized manner.- Surveys a large number of evaluation tasks (540+) - The paper aimed to be comprehensive in covering existing evaluation research on LLMs, more so than many other survey papers. The large task survey provides a good landscape of the evaluation work done so far.- Categorizes tasks into competencies like reasoning, knowledge, reliability, and safety - This is a unique taxonomy compared to papers that group tasks by capabilities like language understanding, generation, reasoning, etc. The competency view provides a new and meaningful way to structure evaluation.- Discusses evaluation for emerging LLMs like ChatGPT - With the explosion in popularity of ChatGPT, some recent works have focused specifically on evaluating these new LLMs. This paper aims to provide a broad survey covering both existing and emerging models.- Provides suggestions for future evaluation - In addition to surveying existing work, the paper also looks ahead at areas like sentiment, planning, and coding competencies that need more development for LLM evaluation.Overall, the competency-based structured approach, comprehensive task survey targeting both existing and new LLMs, and coverage of future evaluation directions distinguish this paper from other reviews and analysis in the field. The conceptual framing using core competencies is a unique contribution.


## What future research directions do the authors suggest?

 Based on my reading of the paper, some of the main future research directions suggested by the authors include:- Developing additional competencies for evaluating LLMs, such as sentiment, planning, coding competency, etc. The authors mention these as important capabilities for LLMs that need further exploration.- Advancing the evaluation of reasoning competencies, including causal reasoning, deduction, induction, abduction, etc. The authors suggest establishing more systematic and comprehensive benchmarks to thoroughly evaluate these reasoning skills.- Exploring commonsense reasoning more deeply as it is critical for human-like understanding. The authors recommend developing more complex benchmarks to push the limits of models' commonsense reasoning capabilities.- Enhancing mathematical reasoning evaluation by incorporating external knowledge, multilingual data, and multi-modal information to test broader reasoning skills.- Expanding tasks and datasets for evaluating reliability, especially quantifying uncertainty and hallucination potential in free-form text generation. New techniques are needed here.- Developing more comprehensive and authoritative methods for evaluating safety. The authors suggest aligning models with human ethics and morals.- Regularly updating test sets to avoid training data leakage and keep up with evolving LLMs. The competency framework allows new tasks to be added systematically.- Continuing to improve evaluation efficiency, rigor, and coverage. The competency approach helps aggregate similar tasks and present results concisely.In summary, the authors recommend developing additional competencies, advancing the measurement of existing ones, updating test sets continuously, and improving the evaluation framework to thoroughly assess evolving LLMs. The competency lens provides a systematic way to integrate new benchmarks as LLMs progress.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the paper:The paper surveys existing works on evaluating large language models (LLMs). It introduces the concept of competency tests to integrate multiple evaluation benchmarks and tasks. Four core competencies of LLMs are identified: knowledge, reasoning, reliability, and safety. Knowledge competency covers linguistic knowledge like grammar and semantics as well as world knowledge like commonsense and domain knowledge. Reasoning competency includes causal, deductive, inductive, abductive, analogical, and multi-hop reasoning capabilities. Reliability competency focuses on evaluating hallucination, uncertainty, and calibration. Safety competency looks at harmfulness, unfairness, social bias, and other ethical considerations. For each competency, the paper discusses the definition, taxonomy, example benchmarks and datasets, and evaluation metrics. The competency test architecture allows similar evaluation tasks to be grouped under relevant competencies while also accommodating new tasks. The paper aims to provide a systematic organization and review of the extensive research on LLM evaluation.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the paper:The paper surveys existing works on evaluating large language models (LLMs). It introduces the concept of using core competencies to categorize and integrate the numerous evaluation benchmarks and tasks proposed for LLMs. Four core competencies are identified: knowledge, reasoning, reliability, and safety. The paper first defines each competency and provides an overview of relevant benchmarks and datasets. For example, the knowledge competency encompasses linguistic knowledge and world knowledge, which can be tested using tasks like BLiMP, WikiFact, and TruthfulQA. The reasoning competency covers abilities like causal, deductive, inductive, and analogical reasoning, with benchmarks like COPA, HotpotQA, and SAT Analogies. Reliability focuses on metrics like hallucination and calibration, while safety examines issues like harmfulness and social bias. Example datasets are provided for each competency. The paper argues that presenting evaluation results organized by competencies provides a more focused and interpretable view of LLM abilities. It also allows new tasks to be easily integrated. Overall, the paper provides a novel perspective on benchmarking LLMs through the lens of core competencies.


## Summarize the main method used in the paper in one paragraph.

 Here is a one paragraph summary of the main method used in the paper:This paper surveys existing works on evaluating large language models (LLMs) and proposes organizing the evaluation using the framework of core competencies. The authors investigated over 540 evaluation tasks and benchmarks and categorized them into 4 core competencies - knowledge, reasoning, reliability, and safety. Each competency is further broken down into sub-categories. For example, knowledge competency includes linguistic knowledge and world knowledge, while reasoning includes causal, deductive, inductive, abductive, analogical, and multi-hop reasoning. The authors argue that organizing evaluation tasks by competencies provides a systematic way to assess LLMs across different skills and abilities. New evaluation tasks can also be easily incorporated within this framework. For each competency and sub-competency, the authors introduce the definition, taxonomy of tasks, representative datasets, and evaluation metrics. This competency-based evaluation provides a way to comprehensively yet concisely examine the capabilities and limitations of LLMs.
