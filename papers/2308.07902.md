# [Through the Lens of Core Competency: Survey on Evaluation of Large   Language Models](https://arxiv.org/abs/2308.07902)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question seems to be: 

How to provide a comprehensive overview and clarification of the numerous evaluation benchmarks and tasks proposed for large language models (LLMs) through the lens of core competencies?

The authors note that many evaluation benchmarks and hundreds of tasks have been proposed to evaluate the capabilities of LLMs. However, these tasks are disparate and overwhelming. 

To address this, the authors propose summarizing the evaluation of LLMs around four core competencies - knowledge, reasoning, reliability, and safety. They aim to synthesize similar evaluation tasks under these competencies to provide a coherent framework for understanding LLM evaluations.

The central hypothesis seems to be that organizing LLM evaluation benchmarks and tasks under these four competencies will provide a clearer, more systematic way to understand the current landscape of LLM evaluation research. The competency framework can assimilate similar tasks, highlight key abilities, and also allow new tasks to be integrated.

In summary, the key research question is how to bring coherence and structure to the numerous proposed LLM evaluation benchmarks and tasks through the lens of core competencies. The hypothesis is that this will enable better understanding and synthesis of work in this area.


## What is the main contribution of this paper?

 Based on my reading of the paper, the main contribution is proposing a framework to evaluate large language models (LLMs) through the lens of core competencies. The key points are:

- The paper summarizes 4 core competencies of LLMs: knowledge, reasoning, reliability, and safety. For each competency, the authors provide a definition, taxonomy, and evaluation metrics. 

- The competency framework helps integrate the numerous evaluation benchmarks and tasks proposed for LLMs. By mapping tasks to competencies, the evaluation results can be presented in a more structured and focused way.

- The competency view also allows new tasks to be comprehensively added. The authors have created an extensible project to display the relationships between competencies and tasks.

- The paper surveys over 540 tasks used in various papers to evaluate LLMs. Tasks are aggregated under the competency they aim to evaluate. This provides a systematic organization of the diverse evaluation landscape.

- Future competencies like sentiment, planning, and code competency are also discussed. The competency paradigm gives a direction to continue expanding LLM evaluations.

In summary, the key contribution is using the competency concept to bring structure and clarity to the complex evaluation landscape for LLMs. The competency framework helps integrate disparate efforts and benchmark progress in a more meaningful way.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the paper:

The paper surveys evaluation benchmarks for large language models, organizing them into 4 core competencies - knowledge, reasoning, reliability, and safety - and providing definitions, taxonomies, example datasets, and evaluation metrics for each competency.


## How does this paper compare to other research in the same field?

 Based on my reading of the paper, here is a summary of how it compares to other research on evaluating large language models:

- Focuses on conceptualizing evaluation through the lens of "core competencies" - This provides a more structured and methodical way of evaluating LLMs compared to some other works that propose a wide array of tasks/benchmarks. The competency framework allows for combining similar tasks and adding new ones in an organized manner.

- Surveys a large number of evaluation tasks (540+) - The paper aimed to be comprehensive in covering existing evaluation research on LLMs, more so than many other survey papers. The large task survey provides a good landscape of the evaluation work done so far.

- Categorizes tasks into competencies like reasoning, knowledge, reliability, and safety - This is a unique taxonomy compared to papers that group tasks by capabilities like language understanding, generation, reasoning, etc. The competency view provides a new and meaningful way to structure evaluation.

- Discusses evaluation for emerging LLMs like ChatGPT - With the explosion in popularity of ChatGPT, some recent works have focused specifically on evaluating these new LLMs. This paper aims to provide a broad survey covering both existing and emerging models.

- Provides suggestions for future evaluation - In addition to surveying existing work, the paper also looks ahead at areas like sentiment, planning, and coding competencies that need more development for LLM evaluation.

Overall, the competency-based structured approach, comprehensive task survey targeting both existing and new LLMs, and coverage of future evaluation directions distinguish this paper from other reviews and analysis in the field. The conceptual framing using core competencies is a unique contribution.


## What future research directions do the authors suggest?

 Based on my reading of the paper, some of the main future research directions suggested by the authors include:

- Developing additional competencies for evaluating LLMs, such as sentiment, planning, coding competency, etc. The authors mention these as important capabilities for LLMs that need further exploration.

- Advancing the evaluation of reasoning competencies, including causal reasoning, deduction, induction, abduction, etc. The authors suggest establishing more systematic and comprehensive benchmarks to thoroughly evaluate these reasoning skills.

- Exploring commonsense reasoning more deeply as it is critical for human-like understanding. The authors recommend developing more complex benchmarks to push the limits of models' commonsense reasoning capabilities.

- Enhancing mathematical reasoning evaluation by incorporating external knowledge, multilingual data, and multi-modal information to test broader reasoning skills.

- Expanding tasks and datasets for evaluating reliability, especially quantifying uncertainty and hallucination potential in free-form text generation. New techniques are needed here.

- Developing more comprehensive and authoritative methods for evaluating safety. The authors suggest aligning models with human ethics and morals.

- Regularly updating test sets to avoid training data leakage and keep up with evolving LLMs. The competency framework allows new tasks to be added systematically.

- Continuing to improve evaluation efficiency, rigor, and coverage. The competency approach helps aggregate similar tasks and present results concisely.

In summary, the authors recommend developing additional competencies, advancing the measurement of existing ones, updating test sets continuously, and improving the evaluation framework to thoroughly assess evolving LLMs. The competency lens provides a systematic way to integrate new benchmarks as LLMs progress.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the paper:

The paper surveys existing works on evaluating large language models (LLMs). It introduces the concept of competency tests to integrate multiple evaluation benchmarks and tasks. Four core competencies of LLMs are identified: knowledge, reasoning, reliability, and safety. Knowledge competency covers linguistic knowledge like grammar and semantics as well as world knowledge like commonsense and domain knowledge. Reasoning competency includes causal, deductive, inductive, abductive, analogical, and multi-hop reasoning capabilities. Reliability competency focuses on evaluating hallucination, uncertainty, and calibration. Safety competency looks at harmfulness, unfairness, social bias, and other ethical considerations. For each competency, the paper discusses the definition, taxonomy, example benchmarks and datasets, and evaluation metrics. The competency test architecture allows similar evaluation tasks to be grouped under relevant competencies while also accommodating new tasks. The paper aims to provide a systematic organization and review of the extensive research on LLM evaluation.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the paper:

The paper surveys existing works on evaluating large language models (LLMs). It introduces the concept of using core competencies to categorize and integrate the numerous evaluation benchmarks and tasks proposed for LLMs. Four core competencies are identified: knowledge, reasoning, reliability, and safety. 

The paper first defines each competency and provides an overview of relevant benchmarks and datasets. For example, the knowledge competency encompasses linguistic knowledge and world knowledge, which can be tested using tasks like BLiMP, WikiFact, and TruthfulQA. The reasoning competency covers abilities like causal, deductive, inductive, and analogical reasoning, with benchmarks like COPA, HotpotQA, and SAT Analogies. Reliability focuses on metrics like hallucination and calibration, while safety examines issues like harmfulness and social bias. Example datasets are provided for each competency. The paper argues that presenting evaluation results organized by competencies provides a more focused and interpretable view of LLM abilities. It also allows new tasks to be easily integrated. Overall, the paper provides a novel perspective on benchmarking LLMs through the lens of core competencies.


## Summarize the main method used in the paper in one paragraph.

 Here is a one paragraph summary of the main method used in the paper:

This paper surveys existing works on evaluating large language models (LLMs) and proposes organizing the evaluation using the framework of core competencies. The authors investigated over 540 evaluation tasks and benchmarks and categorized them into 4 core competencies - knowledge, reasoning, reliability, and safety. Each competency is further broken down into sub-categories. For example, knowledge competency includes linguistic knowledge and world knowledge, while reasoning includes causal, deductive, inductive, abductive, analogical, and multi-hop reasoning. The authors argue that organizing evaluation tasks by competencies provides a systematic way to assess LLMs across different skills and abilities. New evaluation tasks can also be easily incorporated within this framework. For each competency and sub-competency, the authors introduce the definition, taxonomy of tasks, representative datasets, and evaluation metrics. This competency-based evaluation provides a way to comprehensively yet concisely examine the capabilities and limitations of LLMs.


## What problem or question is the paper addressing?

 Based on my reading of the paper, it appears the main problem the paper is addressing is how to better evaluate large language models (LLMs) through the lens of core competencies. 

Specifically, the paper notes that as LLMs have grown in scale and capability, evaluating them has become increasingly challenging for two reasons:

1) Traditional NLP benchmarks are becoming inadequate as LLMs achieve very high performance, resulting in ceiling effects on many tasks.

2) Existing evaluation tasks struggle to keep up with the wide range of real-world applications LLMs are now being used for. 

To address these issues, the paper surveys numerous existing works on LLM evaluation and proposes summarizing them into 4 core competencies:

- Knowledge 
- Reasoning
- Reliability  
- Safety

For each competency, the paper introduces its definition, provides example tasks and datasets, and discusses evaluation metrics. 

The goal is to consolidate the many disparate evaluation tasks/benchmarks into a coherent competency framework, to better understand LLMs' capabilities and limitations. This competency view also aims to help incorporate new evaluation tasks as they emerge.

In summary, the key problem is the need for better, more systematic approaches to evaluate LLMs as they rapidly evolve. The paper tackles this by proposing an evaluation paradigm based on core competencies.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and concepts include:

- Large language models (LLMs): The paper focuses on evaluating large language models, which are pretrained language models with massive numbers of parameters. LLMs like GPT-3 have shown impressive performance on many NLP tasks.

- Competencies: The authors propose evaluating LLMs through the lens of core competencies like reasoning, knowledge, reliability, and safety. This provides a systematic way to assess different capabilities of LLMs.

- Reasoning: The paper discusses various types of reasoning competencies like causal, deductive, inductive, abductive, etc. Assessing reasoning abilities is crucial for complex problem-solving.

- Knowledge: Knowledge competency includes linguistic knowledge and world knowledge. Tests of linguistic knowledge evaluate grammar, semantics, pragmatics while world knowledge tests assess common sense and domain knowledge. 

- Reliability: Reliability competencies deal with model calibration, uncertainty, and potential for hallucination or falsehoods. These impact trustworthiness.

- Safety: Safety evaluates potential for generating harmful, biased or unsafe content. It looks at harmfulness, unfairness, social bias and other ethical risks.

- Benchmarks: The paper surveys benchmarks and datasets for testing the different competencies of LLMs in a standardized way.

- Emergence: Scaling up models leads to the emergence of new abilities not seen in smaller models. The competencies aim to test these emergent skills.

- Systematic evaluation: The goal is to provide a systematic and comprehensive evaluation of LLMs through a skill-based competency framework. This can better guide progress.


## Based on the paper, what questions should we ask to create a comprehensive summary of it? Please list at least 10 questions.

 Here are 10 suggested questions to help create a comprehensive summary of the paper:

1. What is the main focus or purpose of the paper?

2. What problem is the paper trying to address or solve? 

3. What methods, models, or approaches does the paper propose or examine?

4. What are the key contributions or main findings of the paper?

5. What datasets, benchmarks, or experiments were used to evaluate the methods? 

6. What were the quantitative results or main performance metrics?

7. What conclusions or implications does the paper draw based on the results?

8. How does this paper relate to or build upon previous work in the field? 

9. What are the limitations of the methods proposed in the paper?

10. What directions for future work does the paper suggest?


## Design 10 in-depth questions about method proposed in this paper and avoid simple question.

 Here are 10 in-depth questions about the method proposed in the paper:

1. The paper proposes categorizing the evaluation of large language models into 4 core competencies - knowledge, reasoning, reliability, and safety. What are the strengths and limitations of evaluating LLMs through the lens of core competencies? Does this allow for a sufficiently comprehensive evaluation?

2. Under the knowledge competency, the authors divide knowledge into linguistic knowledge and world knowledge. What are other potential ways to categorize the knowledge competency? Could knowledge be divided into narrower or broader categories?

3. For reasoning, the paper decomposes reasoning ability into 6 sub-competencies. Are there important aspects of reasoning that are not covered by this decomposition? Could the categorization be refined further?

4. How suitable are the existing benchmarks used for each competency evaluation? Do they sufficiently cover the full scope of each competency? What are limitations of the benchmarks?

5. The reliability competency focuses on hallucination and uncertainty/calibration. What other aspects of reliability could be evaluated as sub-competencies? How could intrinsic vs extrinsic hallucination be better distinguished?  

6. For the safety competency, only harmful content, unfairness, and social bias are discussed. What other dimensions of safety should be considered for LLMs? How can the subjective nature of safety judgments be addressed?

7. What are the relationships and potential overlaps between the proposed competencies? How can the competencies be clearly delineated? Could some tasks fall under multiple competencies?

8. How can the competency framework remain extensible as new capabilities of LLMs emerge? What is the process for updating competencies and incorporating new benchmarks over time?

9. How can prompts and few-shot learning be standardized when evaluating competencies? How sensitive are current benchmarks to prompt engineering? 

10. Beyond accuracy, what other evaluation metrics should be considered under the competency framework? How can human evaluations play a role? How can tradeoffs between competencies be assessed?
