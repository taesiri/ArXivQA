# [RTQ: Rethinking Video-language Understanding Based on Image-text Model](https://arxiv.org/abs/2312.00347)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a detailed summary of the key points in the paper:

This paper proposes the RTQ framework to address three key challenges in video-language understanding: information redundancy, temporal dependency, and scene complexity. The authors first conduct a clustering analysis, which reveals that existing methods tend to focus on limited aspects of video understanding but can complement each other. Their framework integrates three components to tackle these challenges jointly - a refinement module to reduce redundant patches across frames, a temporal modeling module using message tokens to capture dependencies, and a query module to accumulate task-relevant information from the encoded video. Experiments across text-to-video retrieval, video captioning and QA demonstrate state-of-the-art performance without requiring additional pretraining. Core strengths are the ability to handle untrimmed and complex videos and model temporal relations despite the lack of consistency between frames. The simplicity of the methods also showcase the efficacy of their joint modeling approach. Future work includes incorporating pretraining for open-ended QA and developing more advanced components tailored to each challenge. Overall, this is a novel framework that pushes the boundaries of video understanding.
