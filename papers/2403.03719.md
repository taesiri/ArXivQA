# [Multimodal Transformer for Comics Text-Cloze](https://arxiv.org/abs/2403.03719)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- The paper explores the "text-cloze" task in comics, where models must predict missing text in a comic panel given the panel with obscured text (question panel) and previous context panels. 
- This is a challenging task as it requires integrating visual and textual modalities to understand the narrative flow.  
- Prior methods using RNNs have struggled, limited by OCR accuracy and model architecture.

Proposed Solution:
- The paper introduces a novel Multimodal Large Language Model (Multimodal-LLM) architecture specifically designed for the text-cloze task. 
- The model is based on VL-T5, a multimodal transformer, with customizations for the task.
- A key component is a Domain-Adapted ResNet-50 visual encoder, finetuned on comics using SimCLR. This provides comparable results to more complex encoders with 5x fewer parameters.  
- The model also uses updated OCR from Amazon Textract to enhance text representations.

Main Contributions:
- A new Multimodal-LLM model tailored for the comics text-cloze task, improving over state-of-the-art by 10% on both easy and hard variants.
- Analysis showing comics-finetuned ResNet encoders can match bigger models. 
- Release of new OCR annotations for the text-cloze dataset.
- Formulation of a new generative version of the task, establishing baselines.

The paper makes important advances in modeling the relationship between visual and textual modalities in comics through a specialized Multimodal-LLM architecture. Key innovations include domain-adapted visual encoders and higher quality text representations. By tackling generative dialogues, it also expands the scope for future research.
