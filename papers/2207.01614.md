# [Beyond mAP: Towards better evaluation of instance segmentation](https://arxiv.org/abs/2207.01614)

## What is the central research question or hypothesis that this paper addresses?

This paper focuses on improving the evaluation of instance segmentation methods. The main research questions it addresses are:1. How can we better measure the amount of duplicate (hedged) predictions made by instance segmentation models, both spatially and categorically? 2. Can we design improved evaluation metrics and methods to quantify and reduce the amount of spatial and categorical hedging in existing models?3. Is average precision (AP) alone sufficient to evaluate instance segmentation models, or do we need additional metrics to capture other desirable properties like lower duplicates/hedging, better localization and classification, etc?Specifically, the paper argues that optimizing only for AP can lead instance segmentation models to produce a lot of low-confidence duplicate predictions which artificially boost AP but are undesirable from a practical standpoint. It refers to this issue as "hedging" and proposes new metrics like Duplicate Confusion and Naming Error to explicitly quantify spatial and categorical hedging respectively. The central hypothesis is that augmenting AP with these proposed metrics for hedging, localization, etc. will lead to more robust evaluation and eventually to models that produce fewer duplicate predictions. The paper also proposes a Semantic NMS method to reduce hedging in existing models without compromising AP much. Experiments validate that their method can substantially reduce hedging while preserving mask quality.In summary, the main research contribution is in analyzing the deficiencies of relying solely on AP for evaluating instance segmentation, proposing better complementary metrics to quantify hedging, and developing techniques to mitigate hedging and improve model localization and classification. The overall goal is more reliable and well-rounded evaluation of instance segmentation.


## What is the main contribution of this paper?

The main contributions of this paper are:- It highlights a weakness in the popular Average Precision (AP) metric for evaluating instance segmentation models. Specifically, AP does not penalize duplicate (hedged) predictions, which has led to design choices that improve AP but introduce many false positives. - It proposes two new metrics to explicitly measure the amount of spatial hedging (duplicate confusion error) and categorical hedging (naming error). These metrics successfully capture the duplicate predictions that are not penalized by AP.- It proposes a semantic sorting and NMS module that uses semantic segmentation predictions to resolve both spatial and categorical hedging. This module removes duplicate predictions and improves localization without compromising AP much.- It analyzes several state-of-the-art instance segmentation models using the proposed metrics and module. The results show that modern networks have substantial spatial and categorical hedging, which is not reflected in their high AP scores. Adding the proposed module removes many of these duplicate predictions and gives better qualitative results.In summary, the main contribution is an analysis of the deficiencies of AP for instance segmentation, new metrics to capture duplicate predictions, and a module to remove such hedged predictions while preserving high AP. This provides a better way to evaluate and improve instance segmentation models.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points made in this paper:The paper proposes new metrics to quantify spatial and categorical duplicate predictions (hedging) in instance segmentation, and introduces a Semantic Sorting and NMS module to remove these duplicates while preserving mAP.


## How does this paper compare to other research in the same field?

This paper presents novel methods for evaluating and improving instance segmentation models. Here are some key ways it compares to related work:- It highlights weaknesses in the commonly used mAP metric for evaluating instance segmentation, showing mAP can be "gamed" by adding low-confidence duplicate predictions. This is an important finding, as mAP is the dominant metric used to benchmark progress. - To address mAP's shortcomings, the paper proposes new metrics like Duplicate Confusion and Naming Error to explicitly quantify spatial and categorical hedging (duplicate predictions). This is a novel contribution not explored in prior work.- The authors propose a Semantic NMS module that leverages semantic segmentation to help resolve hedging/duplicates. Using semantics for NMS is novel, as most prior NMS methods operate only on masks and confidence scores.- Experiments show the proposed metrics and Semantic NMS effectively reduce hedging on modern networks like SOLOv2 and DETR, without compromising mask quality. This demonstrates the usefulness of the techniques.- The work builds on related ideas like model calibration, long-tail detection issues, and problems with mAP. But the specific analysis of hedging behavior and solutions for instance segmentation are new.- Compared to prior analysis works like TIDE and LRP which diagnose mAP errors, this paper takes the next step to propose targeted metrics and methods to address key shortcomings identified.In summary, the paper makes multiple novel contributions in analyzing, evaluating, and improving instance segmentation models beyond the commonly used mAP metric. The findings are supported through extensive experiments on standard benchmarks like COCO.
