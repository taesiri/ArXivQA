# [Beyond mAP: Towards better evaluation of instance segmentation](https://arxiv.org/abs/2207.01614)

## What is the central research question or hypothesis that this paper addresses?

 This paper focuses on improving the evaluation of instance segmentation methods. The main research questions it addresses are:1. How can we better measure the amount of duplicate (hedged) predictions made by instance segmentation models, both spatially and categorically? 2. Can we design improved evaluation metrics and methods to quantify and reduce the amount of spatial and categorical hedging in existing models?3. Is average precision (AP) alone sufficient to evaluate instance segmentation models, or do we need additional metrics to capture other desirable properties like lower duplicates/hedging, better localization and classification, etc?Specifically, the paper argues that optimizing only for AP can lead instance segmentation models to produce a lot of low-confidence duplicate predictions which artificially boost AP but are undesirable from a practical standpoint. It refers to this issue as "hedging" and proposes new metrics like Duplicate Confusion and Naming Error to explicitly quantify spatial and categorical hedging respectively. The central hypothesis is that augmenting AP with these proposed metrics for hedging, localization, etc. will lead to more robust evaluation and eventually to models that produce fewer duplicate predictions. The paper also proposes a Semantic NMS method to reduce hedging in existing models without compromising AP much. Experiments validate that their method can substantially reduce hedging while preserving mask quality.In summary, the main research contribution is in analyzing the deficiencies of relying solely on AP for evaluating instance segmentation, proposing better complementary metrics to quantify hedging, and developing techniques to mitigate hedging and improve model localization and classification. The overall goal is more reliable and well-rounded evaluation of instance segmentation.


## What is the main contribution of this paper?

 The main contributions of this paper are:- It highlights a weakness in the popular Average Precision (AP) metric for evaluating instance segmentation models. Specifically, AP does not penalize duplicate (hedged) predictions, which has led to design choices that improve AP but introduce many false positives. - It proposes two new metrics to explicitly measure the amount of spatial hedging (duplicate confusion error) and categorical hedging (naming error). These metrics successfully capture the duplicate predictions that are not penalized by AP.- It proposes a semantic sorting and NMS module that uses semantic segmentation predictions to resolve both spatial and categorical hedging. This module removes duplicate predictions and improves localization without compromising AP much.- It analyzes several state-of-the-art instance segmentation models using the proposed metrics and module. The results show that modern networks have substantial spatial and categorical hedging, which is not reflected in their high AP scores. Adding the proposed module removes many of these duplicate predictions and gives better qualitative results.In summary, the main contribution is an analysis of the deficiencies of AP for instance segmentation, new metrics to capture duplicate predictions, and a module to remove such hedged predictions while preserving high AP. This provides a better way to evaluate and improve instance segmentation models.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the key points made in this paper:The paper proposes new metrics to quantify spatial and categorical duplicate predictions (hedging) in instance segmentation, and introduces a Semantic Sorting and NMS module to remove these duplicates while preserving mAP.


## How does this paper compare to other research in the same field?

 This paper presents novel methods for evaluating and improving instance segmentation models. Here are some key ways it compares to related work:- It highlights weaknesses in the commonly used mAP metric for evaluating instance segmentation, showing mAP can be "gamed" by adding low-confidence duplicate predictions. This is an important finding, as mAP is the dominant metric used to benchmark progress. - To address mAP's shortcomings, the paper proposes new metrics like Duplicate Confusion and Naming Error to explicitly quantify spatial and categorical hedging (duplicate predictions). This is a novel contribution not explored in prior work.- The authors propose a Semantic NMS module that leverages semantic segmentation to help resolve hedging/duplicates. Using semantics for NMS is novel, as most prior NMS methods operate only on masks and confidence scores.- Experiments show the proposed metrics and Semantic NMS effectively reduce hedging on modern networks like SOLOv2 and DETR, without compromising mask quality. This demonstrates the usefulness of the techniques.- The work builds on related ideas like model calibration, long-tail detection issues, and problems with mAP. But the specific analysis of hedging behavior and solutions for instance segmentation are new.- Compared to prior analysis works like TIDE and LRP which diagnose mAP errors, this paper takes the next step to propose targeted metrics and methods to address key shortcomings identified.In summary, the paper makes multiple novel contributions in analyzing, evaluating, and improving instance segmentation models beyond the commonly used mAP metric. The findings are supported through extensive experiments on standard benchmarks like COCO.


## What future research directions do the authors suggest?

 Based on my reading of the paper, here are some of the main future research directions suggested by the authors:- Developing more advanced metrics beyond mAP to better evaluate instance segmentation performance. The authors propose two new metrics - Duplicate Confusion and Naming Error - to quantify spatial and categorical hedging respectively. They suggest more work can be done to design metrics that directly measure desired properties like spatial precision, duplicate prediction, etc.- Improving bottom-up instance segmentation approaches. The authors note that bottom-up methods have comparable qualitative performance to top-down methods, but lag behind in mAP. They suggest exploring ways to improve the localization and aggregation abilities of bottom-up approaches. - Architectures that jointly optimize for semantic segmentation and instance segmentation. The authors' Semantic NMS module relies on predicted semantic segmentation masks. They suggest joint training and inference for both tasks could be beneficial.- Methods to equalize performance across object categories, especially for rare classes. The authors note mAP can be high despite poor performance on rare classes. They suggest techniques like re-weighting and hallucination to improve rare category performance.- Exploring the speed vs performance tradeoff with different NMS algorithms. The authors show their Semantic NMS is faster than traditional NMS. More work can be done to develop fast and effective NMS variants.- Applying the ideas beyond COCO to more diverse and challenging datasets. Evaluating on datasets with more classes, crowded scenes, small objects etc. would better demonstrate the generalizability of techniques.- Improving calibration of confidence scores predicted by instance segmentation networks. The authors note poor calibration contributes to hedging. Techniques to improve calibration could help resolve duplicates.In summary, the main suggestions are around developing better evaluation metrics, improving bottom-up and joint segmentation methods, equalizing performance across categories, designing fast and effective NMS algorithms, and improving overall network calibration.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the key points in the paper:The paper proposes new metrics and methods to improve the evaluation of instance segmentation models beyond mean Average Precision (mAP). It points out limitations of mAP in that it does not penalize duplicate (hedged) predictions, which has led to network designs that improve mAP through excessive false positives rather than better localization and categorization. The authors define spatial and categorical hedging and propose new metrics like Duplicate Confusion and Naming Error to quantify hedging. They also propose a Semantic Sorting and NMS module to remove duplicates based on semantic mask agreement while preserving mAP. Experiments demonstrate that modern segmentation networks have significant gains in mAP but with many duplicates, while older methods like Mask R-CNN are more robust. The proposed modules can remove duplicates and improve localization and classification while maintaining mAP. Overall, the paper provides useful new metrics and methods to mitigate issues with optimizing instance segmentation only for mAP, which can lead to unwanted network behaviors.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the key points from the paper:This paper proposes new evaluation metrics and methods to address weaknesses in using Average Precision (AP) for evaluating instance segmentation models. AP does not penalize duplicate predictions, allowing models to hedge predictions and increase AP at the cost of more false positives. The authors show this occurs in modern instance segmentation frameworks, increasing AP 1-2 points but with many more low-confidence duplicates. To address this, the authors first propose new metrics like Duplicate Confusion and Naming Error to explicitly measure spatial and categorical hedging (duplicates). They also propose a Semantic Sorting and NMS module that uses predicted semantics to remove duplicates, decreasing hedging while preserving AP. Experiments on COCO and a synthetic dataset show their method reduces duplicate predictions up to 33x on modern networks like SOLOv2. The metrics and module allow improved evaluation and optimization of the tradeoff between AP and false positives.


## Summarize the main method used in the paper in one paragraph.

 Here is a one paragraph summary of the main method used in this paper:This paper proposes a new method to improve instance segmentation evaluation by addressing shortcomings in the common evaluation metric of Average Precision (AP). The authors observe that AP does not penalize duplicate (hedged) predictions, allowing models to achieve higher AP through spatial and categorical hedging. To address this, they first propose new metrics called Duplicate Confusion (DC) and Naming Error (NE) to explicitly quantify spatial and categorical hedging respectively. They then propose a Semantic Sorting and NMS module that uses predicted semantic masks to re-score instances and perform non-maximum suppression. This allows duplicate instances to be removed based on mask occupancy rather than just confidence scores. The proposed module can be added to existing frameworks as a plug-and-play component to reduce hedging and improve evaluation metrics related to spatial/categorical duplicates and localization quality, while preserving AP. Experiments on COCO datasets demonstrate the prevalence of hedging in modern architectures and show improvements from adding the proposed module.


## What problem or question is the paper addressing?

 The paper is addressing issues with evaluating instance segmentation models using the standard metric of mean Average Precision (mAP). The key problems and questions it raises are:- mAP does not adequately penalize duplicate predictions, especially low-confidence false positives. This allows models to "hedge" their predictions and artificially boost mAP.- mAP cannot distinguish between instances that are localized correctly but classified incorrectly. It treats all errors equally. - These weaknesses have led to network designs that improve mAP through hedged predictions, creating a dilemma where higher mAP may not correspond to better instance segmentation quality. - How can we better quantify the amount of spatial and categorical duplicate predictions made by models?- How can we modify instance segmentation frameworks to reduce hedging and improve localization and classification, while preserving mAP?In summary, the paper argues that mAP has deficiencies when evaluating instance segmentation models, and proposes new metrics and modifications to address these issues and achieve better tradeoffs between high recall and false positives. The overall goal is improving the interpretability and reliability of instance segmentation evaluation.
