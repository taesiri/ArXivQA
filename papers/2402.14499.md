# ["My Answer is C": First-Token Probabilities Do Not Match Text Answers in   Instruction-Tuned Language Models](https://arxiv.org/abs/2402.14499)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Multiple choice questions (MCQs) are commonly used to evaluate large language models (LLMs). The standard approach is to evaluate based on the model's first token prediction and the log probabilities assigned to answer options.  
- However, recent LLMs tuned to follow instructions have more diverse and natural response styles - they may start with conversational preambles ("Sure") or refuse to answer sensitive questions.
- This makes the reliability of first-token evaluation questionable as it may not match the actual text response.

Proposed Solution:
- Compare first-token evaluation to text output along dimensions of final option choice, refusal rate, choice distribution and robustness to prompt changes. 
- Use six major instruction-tuned LLMs: Llama2-Chat, Mistral-Instruct, Mixtral-Instruct.
- Manually annotate text responses to train classifier for mapping text to options.

Main Contributions:
- First token evaluation is severely misaligned with text output on all dimensions, with over 60% mismatch rates.  
- Refusal is a major contributor to mismatch. Models with safety mechanisms have higher refusal and mismatch rates.
- Text output is more robust to prompt changes than first token.
- Constraint prompts reduce but don't eliminate the mismatch.

Conclusion: 
- First token evaluation alone is unreliable for instruction-tuned models. It is critical to inspect text output as well.
- More direct, realistic evaluation approaches needed to understand model behaviors.


## Summarize the paper in one sentence.

 The paper studies the mismatch between multiple-choice question evaluation methods for language models based on first token probability versus generating full text responses, finding severe misalignment on dimensions like final option choice, refusal rate, and robustness to prompt variations.


## What is the main contribution of this paper?

 The main contribution of this paper is:

It evaluates the reliability of using first-token probabilities for evaluating multiple-choice question (MCQ) accuracy in large language models (LLMs), by comparing them to the actual text responses generated by the models. Specifically, it measures the mismatch rate between the answer predicted by first-token evaluation vs the answer in the text output on several dimensions - final option choice, refusal rate, choice distribution, and robustness to prompt variations. 

The key findings are:

1) There is a high mismatch rate (>60% for some models) between first-token evaluation and text output, suggesting that sole reliance on first-token probabilities is not indicative of actual model behavior when interacting with users.

2) Models fine-tuned to be more conversational/safe tend to have higher mismatch rates.

3) Even with increasingly constrained prompts, significant mismatch remains between the two evaluation methods.

4) First-token evaluation is more prone to prompt bias/perturbations and has higher choice instability than text output.

The paper underscores the importance of looking beyond first-token methods and inspecting text responses as well for more reliable LLM evaluation, especially for open-ended tasks like surveys.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and keywords associated with it are:

- Multiple-choice questions (MCQ)
- Large language models (LLMs) 
- First-token evaluation
- Text output evaluation
- Mismatch rate
- Refusal rate
- Instruction tuning
- Prompt engineering
- Selection bias
- Conversational AI

The paper examines the reliability of using first-token evaluation for measuring the accuracy of large language models on multiple-choice questions. It compares this method to directly evaluating the text output generated by the models. The key findings are that there is a high mismatch rate between these two evaluation methods, especially for models that have been tuned to follow instructions and converese more naturally. Issues like refusal rate, sensitivity to prompt format, and selection bias are analyzed. The paper calls for more inspection of text output rather than just relying on first-token evaluation.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 potential in-depth questions about the method proposed in the paper:

1. The paper compares first-token evaluation to text output evaluation. What are some limitations of relying solely on first-token evaluation that the paper highlights? How does inspecting the full text output provide additional insights?

2. The paper finds over 60% mismatch rates between first-token and text output evaluations for some models. What factors contribute most to this high mismatch rate? How do refusal rates and constraints on prompt formatting impact the mismatch?

3. What annotation process was used to create the text output classifier? What approaches were compared to assess the classifier's performance? How accurate was the final classifier?

4. How consistent were model outputs between the first-token and text output evaluations when answer options were shuffled? Which models showed the greatest variability in response to shuffling?

5. The paper examines both conversational and safety-focused LLMs. Did tuning objectives like safety and conversational ability impact evaluation alignment? If so, how?  

6. For models like Llama2-70b-Chat, what impact did the example template have on distribution of chosen options? How did tweaking the example change selections?

7. Outside of mismatch rates, what other dimensions were compared between first-token and text output evaluations? How did evaluations differ along these dimensions?

8. What trends were observed in refusal rates between different model sizes and prompt constraint levels? Which models were most likely to refuse questions?

9. The paper focuses solely on log probabilities of the first token. How might findings differ for other probability-based MCQ evaluations methods? What approaches should future work explore?

10. When findings suggest evaluations are severely misaligned from model behaviors, what are the implications for relies on evaluations without inspection of text outputs? How could alternative frameworks better capture realistic behaviors?
