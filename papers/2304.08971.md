# [SurfelNeRF: Neural Surfel Radiance Fields for Online Photorealistic   Reconstruction of Indoor Scenes](https://arxiv.org/abs/2304.08971)

## What is the central research question or hypothesis that this paper addresses?

 The central research question this paper addresses is how to achieve online photorealistic reconstruction and rendering of large-scale indoor scenes. 

Specifically, the paper proposes a framework called SurfelNeRF that can incrementally build a neural representation of a previously unseen scene on the fly as images arrive in an online fashion. This enables high-quality photorealistic rendering of novel views of the scene in real time.

The key ideas and contributions are:

- Introducing a neural surfel representation that attaches neural features to classical geometric surfels to encode a neural radiance field representation. This provides a compact and flexible scene representation.

- An efficient neural surfel fusion approach to incrementally integrate new observations into the global scene representation by merging overlapping neural surfels.

- A fast and differentiable rasterization-based rendering algorithm tailored for the neural surfel representation that enables efficient high-quality rendering.

The framework is demonstrated on the large-scale indoor ScanNet dataset, where it outperforms prior state-of-the-art online reconstruction methods in terms of rendering quality while maintaining high efficiency. The ability to perform online photorealistic reconstruction and rendering could enable many applications that require real-time feedback.

In summary, the central hypothesis is that by combining classical surfel representations with neural features and volumetric rendering, one can achieve efficient, scalable and high-quality online photorealistic reconstruction of large indoor scenes from raw image streams. The experiments validate this hypothesis and demonstrate state-of-the-art performance.


## What is the central research question or hypothesis that this paper addresses?

 The central research question is how to perform online photorealistic reconstruction of indoor scenes. 

Specifically, the paper aims to tackle the problem of incrementally building and updating a 3D scene representation from an online stream of input images, such that the representation allows rendering high-quality novel views in real time.

The key hypothesis is that by combining a neural radiance field representation with a surfel-based fusion approach, it is possible to achieve efficient online scene reconstruction and high-quality novel view synthesis for large-scale indoor environments.

The key components of the proposed method include:

- Neural surfel representation to compactly store scene geometry and view-dependent effects.

- Efficient online fusion of per-frame surfel representations into a global model, integrating both geometry and neural features. 

- Fast differentiable rendering by rasterizing and volume rendering only the relevant surfels.

The experiments aim to validate that this surfel-based neural scene representation coupled with the proposed online fusion approach can enable real-time high-quality novel view synthesis on complex indoor scenes. The method is evaluated on the ScanNet dataset and compared to other recent online reconstruction techniques.

In summary, the central hypothesis is that the combination of a neural surfel representation and an efficient online surfel fusion scheme can unlock real-time photorealistic novel view synthesis for online reconstruction of large indoor scenes. The key innovation is in the neural surfel representation and differentiation rendering that allows efficiently integrating and rendering incremental scene observations.


## What is the main contribution of this paper?

 The main contribution of this paper is the proposal of SurfelNeRF, a novel neural radiance field method for online photorealistic reconstruction of indoor scenes. The key ideas are:

- Using a neural surfel representation to represent the scene geometry and appearance. Neural surfels extend classical surfels by associating them with learned neural features encoding the radiance field. 

- An efficient neural surfel fusion scheme to incrementally integrate local neural surfel fields reconstructed from input views into a global model, enabling online scene reconstruction.

- A fast and differentiable rasterization-based rendering algorithm for the neural surfel radiance fields, which is more efficient than standard volume rendering.

- End-to-end training of the entire framework, including local neural surfel field construction, fusion, and rendering, on large-scale indoor scene datasets. This allows the model to generalize to novel scenes.

- Optional per-scene fine-tuning to further improve rendering quality by optimizing the surfel radiance field for the specific scene.

In summary, the key contribution is a novel neural representation and framework for online photorealistic reconstruction of indoor scenes, which combines the benefits of classical surfel fusion techniques and modern neural rendering. The experiments demonstrate state-of-the-art performance for this challenging task.


## What is the main contribution of this paper?

 Based on my reading, the main contributions of this paper are:

1. It proposes SurfelNeRF, a novel neural radiance field representation based on neural surfels for online photorealistic reconstruction and rendering of large-scale indoor scenes. 

2. It introduces an efficient neural surfel fusion scheme to progressively integrate local neural surfel representations into a global model in an online manner. This allows incremental scene reconstruction as new images arrive.

3. It presents a fast and differentiable rasterization-based rendering algorithm tailored for the neural surfel representation. This enables efficient high-quality rendering without needing extra per-scene optimization.

4. It demonstrates state-of-the-art performance on the ScanNet dataset for both feedforward inference and fine-tuning settings. The method achieves better rendering quality than previous approaches while having significantly faster training and inference speed.

In summary, the key novelty is in proposing the neural surfel radiance field representation and the associated techniques for online fusion and efficient rendering. This enables high-quality novel view synthesis for large-scale scenes in an online setting, advancing the state-of-the-art in incremental photorealistic 3D reconstruction.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the key points from the paper:

The paper proposes a neural surfel radiance field method called SurfelNeRF for online photorealistic reconstruction and rendering of large indoor scenes, using an efficient surfel fusion approach and fast rasterization-based rendering that achieves state-of-the-art performance while maintaining high efficiency.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the key points from the paper:

The paper proposes SurfelNeRF, a neural radiance field method that uses a surfel (surface element) representation to achieve efficient online photorealistic reconstruction of indoor scenes from an input stream of images.


## How does this paper compare to other research in the same field?

 This paper presents SurfelNeRF, a neural surfel radiance field approach for online photorealistic reconstruction of indoor scenes. Here are some key comparisons to other related research:

- Representation: SurfelNeRF uses a surfel-based representation to store neural radiance fields. This is more compact and scalable than volumetric grids used in methods like NeRFusion. It also enables more efficient fusion than point-based methods like PointNeRF.

- Rendering: SurfelNeRF proposes a fast differentiable rasterization rendering approach that is much faster than volume rendering used in vanilla NeRF methods. It builds on the strengths of both rasterization and ray tracing.

- Generalization: Unlike regular NeRF methods that require per-scene optimization, SurfelNeRF is trained across scenes to enable direct novel view synthesis. This is similar to other generalized NeRF approaches.

- Online setting: SurfelNeRF focuses on the online setting of incrementally building the scene representation from a stream of images. NeRFusion also tackles the online setting but uses slower volumetric fusion and rendering.

- Performance: Experiments show SurfelNeRF achieves state-of-the-art performance on ScanNet in both feedforward inference and fine-tuning settings. It is also much faster than volumetric methods.

Overall, the key novelty of SurfelNeRF is in combining the strengths of classical surfel fusion techniques with neural radiance fields for efficient online photorealistic reconstruction. The neural surfel representation and rendering scheme help bridge the gap between classical geometric methods and NeRFs.


## How does this paper compare to other research in the same field?

 Here are my thoughts on how this paper compares to other research in online 3D scene reconstruction and novel view synthesis:

Representation:
- The surfel-based neural radiance field representation is more compact and scalable than volumetric methods like NeRFusion. It avoids having to update empty space.
- Compared to point cloud methods like PointNeRF, surfels have some surface information like normals that can aid in feature fusion and rendering.

Fusion:
- The proposed surfel fusion method builds on classical surfel fusion techniques and adapts them to fuse both geometry and neural features. This allows efficient incremental updates.
- The surfel association and update rules are geometrically meaningful, unlike grid or point fusion.

Rendering:
- The proposed rasterization-guided rendering is much faster than volume rendering used in NeRF and NeRFusion. It only evaluates a few surfels per ray.

Generalization:
- Training end-to-end allows SurfelNeRF to generalize to novel scenes without per-scene optimization, unlike NeRF.
- Fine-tuning can further improve quality by adapting the model, but starts from a better initialization than PointNeRF.

So in summary, this paper introduces a representation and techniques tailored for efficient online scene reconstruction and rendering. The surfel representation, fusion scheme, and rendering approach seem to combine the strengths of classical reconstruction and recent neural advances. The comparisons on ScanNet demonstrate improved efficiency and generalization ability compared to recent online NeRF methods.


## What future research directions do the authors suggest?

 Based on my reading of the paper, here are some of the main future research directions suggested by the authors:

- Improving the depth refinement module to handle more challenging cases of noisy or incomplete input depth maps. The authors mention that higher quality depth leads to better reconstruction and rendering results. So exploring more advanced depth estimation or refinement techniques could be beneficial.

- Scaling up the method to handle even larger scenes and longer input sequences. The surfel-based representation helps with scalability but evaluating the method on very large-scale environments or extremely long video sequences could reveal opportunities for further optimization.

- Exploring alternative neural surfel fusion schemes beyond the GRU approach. While the GRU performs well, designing specialized update rules tailored for this fusion task may improve quality and efficiency.

- Generalizing the model to handle different scene types beyond indoor environments, such as outdoor scenes. The radiance field learned on ScanNet may not transfer perfectly to outdoor conditions with different lighting and materials.

- Enabling interactive reconstruction and rendering during live capture by optimizing the runtime. Right now rendering takes 0.2 seconds per frame which could limit real-time applications.

- Improving rendering of complicated transparency and reflection effects. The paper shows some limitations in these areas.

- Combining the advantages of this method with classical SLAM or SfM systems. The explicit surfel representation could complement traditional geometry processing pipelines.

- Exploring self-supervised techniques to avoid reliance on ground truth images for training. Alternate supervision signals may help expand the scope of applicable training data.

So in summary, advancing the depth refinement, scaling up the approach, designing better fusion schemes, improving generalization, reducing runtime, handling complex appearance effects, integrating geometry systems, and leveraging self-supervision seem like promising directions for future work based on this paper.


## What future research directions do the authors suggest?

 The paper suggests several future research directions:

- Improving the depth refinement module: The authors note their method currently relies on initial depth maps from MVS systems or depth sensors. If the input depth is very coarse or noisy, the depth refinement module may fail to produce good depth for neural surfel generation. They suggest improving the depth refinement module, especially when input depth is incomplete or very noisy.

- Handling view-dependent effects: The paper notes their method does not explicitly model view-dependent effects like reflections and refractions. Incorporating an explicit view-dependent appearance modeling could further improve rendering quality.

- Scaling to larger scenes: The experiments are on relatively small indoor scenes. Testing the scalability and generalizability of the method on much larger scenes (e.g. entire buildings) is an important direction. This may require modifications to efficiently store and render the increased number of surfels.

- Online model adaptation: The paper shows fine-tuning the model per-scene improves results. Developing online adaptation techniques to continuously update the model as new data comes in could be useful.

- Combining with classic SLAM systems: The neural surfel fusion could potentially be combined with existing real-time SLAM systems for improved geometry and tracking. Exploring how to tightly couple them is an interesting direction.

- Applications to VR/AR: The online nature of the method makes it suitable for real-time reconstruction for VR/AR applications. Exploring modifications and optimizations for real-time performance in these settings is a promising direction.

So in summary, the main future directions are improving depth inputs, handling view-dependent effects better, scaling to larger scenes, online adaptation, integration with SLAM, and real-time VR/AR applications. The neural surfel representation shows promise, but there are still many opportunities to improve the overall pipeline and apply it to new settings.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the key points from the paper:

The paper proposes SurfelNeRF, a novel neural representation and rendering framework for online photorealistic reconstruction of large-scale indoor scenes. It represents scenes using neural surfels - an extension of traditional surfels with added neural features to encode radiance fields. An efficient neural surfel fusion method is introduced to progressively integrate per-view representations into a global model on the fly. A fast and differentiable rasterization-based rendering algorithm is also proposed for rendering the neural surfel radiance fields. Experiments on the ScanNet dataset demonstrate state-of-the-art performance both with feedforward inference on novel scenes and with additional short per-scene fine-tuning. The method enables high-quality photorealistic novel view synthesis while maintaining efficiency for online usage. Key advantages include the compact and flexible surfel representation, scalability to large scenes, and fast incremental updates and rendering.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the key points from the paper:

The paper proposes SurfelNeRF, a novel neural scene representation based on neural surfels for online photorealistic reconstruction and rendering of indoor scenes. Surfels are point primitives containing geometric attributes and additional neural features encoding radiance fields. For rendering, a fast differentiable rasterization process is proposed that renders pixels based on intersecting surfels. For online scene reconstruction, a neural surfel fusion approach is introduced to incrementally integrate per-view neural surfel representations into a global model, aggregating geometric and neural attributes in a geometry-guided manner. Experiments on ScanNet show state-of-the-art results compared to previous online reconstruction methods. The framework enables feedforward inference on new scenes as well as fine-tuning for further improvements. Key benefits include efficiency, scalability, and high rendering quality of the neural surfel representation.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the key points from the paper:

The paper presents SurfelNeRF, a novel neural representation for online photorealistic reconstruction and rendering of indoor scenes. SurfelNeRF represents scenes using neural surfels, which are point primitives containing geometric attributes like position, normal, and radius, as well as learned neural features encoding the radiance field. This representation is more compact and flexible compared to volumetric approaches like voxels or hash grids. The method performs incremental scene reconstruction by fusing new observed neural surfels into the global model, merging both geometry and neural features, inspired by classical surfel fusion techniques. A key contribution is a fast and differentiable rasterization-based rendering scheme, which only needs to query the neural features of the few surfels along each camera ray. This enables efficient optimization and fast rendering.

Experiments demonstrate state-of-the-art performance on the large-scale ScanNet indoor dataset. The method can generalize to new scenes with direct feedforward inference after training across scenes. Additional short per-scene fine-tuning can further improve rendering quality. The compact surfel representation and efficient rendering scheme allow real-time performance not achieved by other recent neural scene representations. The online updating also enables novel interactive reconstruction applications. Limitations include reliance on estimated depth maps as input. But overall, SurfelNeRF advances the state-of-the-art in real-time photorealistic reconstruction and rendering of indoor scenes.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the key points from the paper:

The paper proposes SurfelNeRF, a method for online photorealistic reconstruction and rendering of large-scale indoor scenes. The key idea is to represent the scene using a neural surfel radiance field. Surfels are point primitives containing geometric attributes like position, normal, color, and radius. The method extends surfels by associating neural features that encode the radiance field. Compared to voxel grids, this representation is more compact and flexible. An efficient neural surfel fusion method is proposed to progressively integrate the per-view surfel representations into a global model in an online manner. This allows aggregating geometric and neural attributes from different views. The method also introduces a fast and differentiable rasterization algorithm to render the neural surfel representation. This only requires a few MLP evaluations per pixel based on the rasterized surfels. 

The method is trained end-to-end on the large-scale ScanNet indoor dataset. Once trained, it can generalize to novel scenes in a feedforward manner for online photorealistic reconstruction. The representation also allows further per-scene fine-tuning if needed. Experiments demonstrate state-of-the-art performance compared to previous online reconstruction methods like NeRFusion and generalizable radiance field methods like PointNeRF. The compact surfel representation and efficient integration and rendering algorithms enable real-time performance not achieved by other methods. The online photorealistic reconstruction capability could enable various interactive applications.


## Summarize the main method used in the paper in one paragraph.

 Here is a one paragraph summary of the main method used in the paper:

The paper proposes SurfelNeRF, a neural surfel radiance field representation for online photorealistic reconstruction and rendering of indoor scenes. The key idea is to extend the classical surfel representation by associating each surfel with neural features encoding 3D geometry and visual appearance. The neural surfel radiance fields can be efficiently rendered using a differentiable rasterization scheme. To enable online scene reconstruction, the paper presents a neural surfel fusion approach that integrates per-view surfel representations into a global model in a streaming fashion. Specifically, they determine correspondences between input and global surfels, and merge overlapping ones through a GRU-based feature fusion while adding non-overlapping ones directly. The framework is trained end-to-end on large indoor scene datasets and can generalize to novel scenes. The compact surfel representation and efficient rasterization allow SurfelNeRF to perform neural rendering and online fusion with low computational overhead. Experiments demonstrate state-of-the-art performance on the ScanNet dataset.


## Summarize the main method used in the paper in one paragraph.

 Here is a one paragraph summary of the main method used in the paper:

The paper proposes SurfelNeRF, a novel method for online photorealistic reconstruction and rendering of large-scale indoor scenes. SurfelNeRF represents scenes using neural surfels, which are an extension of classical surfels (surface elements) that incorporate additional neural features encoding the scene's radiance field. To enable online scene capture, SurfelNeRF performs neural surfel fusion, integrating local neural surfel representations from each input view into a global model in a progressive, incremental fashion. This is inspired by classical real-time surfel fusion techniques from simultaneous localization and mapping (SLAM). SurfelNeRF also introduces a fast, differentiable rasterization-based rendering algorithm to generate photorealistic novel views by only evaluating the MLP network at surfel intersection points. The method can be trained end-to-end on large datasets and generalizes to novel scenes. The combination of the neural surfel representation, efficient fusion scheme, and fast rendering enables online high-quality photorealistic reconstruction.


## What problem or question is the paper addressing?

 The paper appears to address the problem of online photorealistic 3D reconstruction and rendering of indoor scenes. Specifically, it aims to progressively build a neural scene representation and render high-quality novel views as input images stream in, without requiring offline batch processing.

The key questions seem to be:

- How to represent large-scale indoor scenes for efficient online updating and high-quality rendering?

- How to incrementally fuse new information from input views into the global scene representation? 

- How to render the representation efficiently with view-dependent effects?

To tackle these challenges, the paper proposes SurfelNeRF, a framework based on neural surfel radiance fields that can be progressively built and rendered efficiently.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and concepts are:

- Neural radiance fields (NeRF): The paper proposes a novel neural representation called neural surfel radiance fields (SurfelNeRF) which combines explicit geometry with a neural radiance field for efficient novel view synthesis. 

- Neural surfels: TheSurfelNeRF representation uses neural surfels, which are an extension of classical surfels (surface elements) that include additional neural features encoding appearance and view-dependent effects. 

- Online reconstruction: The proposed method aims to tackle online photorealistic reconstruction, where the goal is to incrementally build the scene representation and render high-quality views as images stream in, without requiring preprocessing of all images.

- Surfel fusion: The method integrates information from incremental views into the global scene representation using an efficient surfel fusion scheme adapted from classical surfel fusion techniques. 

- Differentiable rasterization: A key component is a fast and differentiable rasterization algorithm to render the neural surfel radiance fields by evaluating shading only at surfel intersections along each ray.

- Generalizability: The SurfelNeRF framework can be trained end-to-end on large indoor scene datasets and generalizes to novel scenes for feedforward inference without requiring per-scene optimization.

In summary, the key ideas are using neural surfels and surfel fusion for efficient online scene reconstruction, and differentiable rasterization of the neural surfels to enable fast rendering of high-quality novel views.


## Based on the paper, what questions should we ask to create a comprehensive summary of it? Please list at least 10 questions.

 Here are 10 potential questions to ask to create a comprehensive summary of the paper:

1. What is the main purpose or goal of the paper? 

2. What problem is the paper trying to solve? What are the limitations of existing methods that the paper aims to address?

3. What is the proposed method or approach? Please provide a high-level overview of the key ideas, components, and pipeline.

4. What is the neural surfel radiance field representation proposed in the paper and how is it constructed? 

5. How does the paper perform online integration of neural surfels into a global model? How does it fuse information across views?

6. What is the rendering algorithm proposed for neural surfel radiance fields? How does it work? What are its advantages?

7. What datasets were used for training and evaluation? What metrics were used to evaluate the method?

8. What were the main experimental results? How does the proposed method compare to prior state-of-the-art methods quantitatively and qualitatively? 

9. What are the limitations of the proposed method? Are there any failure cases or scenarios where it does not perform well?

10. What are the major takeaways? What impact might this work have on related problems and fields? What future work does it enable?


## Design 10 in-depth questions about method proposed in this paper and avoid simple question.

 Here are 10 potential in-depth questions about the method proposed in this paper:

1. The paper proposes using a neural surfel representation for modeling radiance fields. How does this representation compare to other representations like voxels or implicit fields? What are the advantages and disadvantages?

2. The neural surfel fusion approach is inspired by classical surfel fusion techniques. How is the neural surfel fusion approach adapted to handle feature integration in addition to geometry? What modifications were made to the traditional pipeline?

3. The paper mentions employing a GRU for surfel feature fusion. What is the motivation for using a recurrent network here? How does it help with incremental feature aggregation compared to a simple weighted average?

4. The fast differentiable rasterization-based renderer is a key contribution. Walk through the details of how it works and how it achieves faster rendering compared to volumetric approaches. What approximations or simplifications are made?

5. How is the per-surfel appearance modeled in this work? What network is used to produce view-dependent effects based on surfel features and geometry? How is occluded vs. visible surfel handling performed?

6. What advantages does the explicit surfel representation provide for end-to-end training? How does it simplify optimization compared to implicit representations?

7. The method performs per-scene fine-tuning to further improve rendering quality. What aspects of the model get fine-tuned during this stage? What prevents overfitting to the small fine-tuning dataset?

8. How does the method handle scalability and memory usage for large scenes? Analyze the growth in computation and storage costs as the number of surfels increases.

9. What are some limitations of the current approach? How robust is it to input noise and depth errors? How could the method be improved to handle these cases better?

10. The paper focuses on indoor scenes. What changes would be needed to apply this method to large outdoor scenes? Would the representation and rendering approach still be suitable?


## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a high-quality summary paragraph of the paper:

This paper proposes SurfelNeRF, a method for online photorealistic reconstruction and rendering of large-scale indoor scenes. The key idea is to represent scenes using neural surfels, which are point primitives containing geometric attributes like position and radius as well as neural features encoding radiance. The neural surfels allow efficient integration of sequential view observations into a global scene representation. Specifically, they adopt surfel fusion to accumulate and merge neural surfels based on geometric consistency between views. A differentiable rasterization renderer is proposed to synthesize novel views from the fused global neural surfel representation, which is more efficient than volume rendering in NeRF. The method can be trained end-to-end on scene datasets to obtain a generalizable model. Experiments on ScanNet scenes demonstrate state-of-the-art performance in terms of rendering quality and efficiency. The online nature of SurfelNeRF enables applications like progressive scene capture for VR/AR and interactive visualization. Overall, it effectively combines the representation compactness and fusion efficiency of surfels with the rendering quality of neural radiance fields.


## Summarize the paper in one sentence.

 This paper proposes SurfelNeRF, a neural representation using surfels equipped with learned features for online photorealistic reconstruction and rendering of large-scale indoor scenes.


## Summarize the paper in one paragraphs.

 Here is a one paragraph summary of the paper:

This paper proposes SurfelNeRF, a novel method for online photorealistic reconstruction and rendering of large-scale indoor scenes. It represents scenes using neural surfels, which compactly store geometric and neural radiance field features. An efficient neural surfel fusion approach is used to progressively integrate per-frame surfel representations into a global model in an online manner. To achieve fast rendering, a differentiable rasterization process is proposed to render the neural surfel radiance field. Experiments on ScanNet scenes demonstrate state-of-the-art photorealistic novel view synthesis results, with significantly faster training and rendering compared to prior work like NeRFusion and PointNeRF. The method enables high-quality neural rendering for interactive online capture of indoor environments.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in this paper:

1. What is the motivation of SurfelNeRF for proposing a surfel-based neural representation compared to other representations like voxel grids or point clouds? What are the advantages and disadvantages?

2. How does SurfelNeRF construction work? How are the geometric and neural attributes associated with each surfel extracted from the input frames?

3. Explain the online integration process of neural surfels in detail. How does it associate and fuse local and global surfels? What update rules are used?

4. What is the intuition behind using a GRU for fusing features of corresponding global and local surfels? How does it help compared to a simple weighted feature summation?

5. Explain the proposed differentiable rasterization-based rendering scheme. How does it leverage surfel representation for efficient rendering compared to volume rendering?

6. What are the various losses used for end-to-end training of SurfelNeRF? How is the framework trained with sequential ScanNet data?

7. How does SurfelNeRF scale to large and complex indoor scenes compared to other online reconstruction methods? Analyze its time and memory complexity.

8. What is per-scene fine-tuning in SurfelNeRF? Why can it be done efficiently and how does it help improve rendering quality?

9. What are some of the limitations of SurfelNeRF's representation, fusion scheme and rendering process? When might it struggle?

10. How suitable is SurfelNeRF for interactive or augmented reality applications? What changes would be needed to deploy it on mobile devices?
