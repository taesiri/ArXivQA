# Improving Language Model Negotiation with Self-Play and In-Context   Learning from AI Feedback

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be: Can multiple large language models autonomously improve each other's negotiation skills through self-play and learning from AI feedback, with minimal human intervention?The key points are:- The authors aim to study if LLMs can improve themselves in a negotiation game setting by playing against each other, reflecting on their strategies, and providing constructive feedback to their peer through another LLM acting as a "critic." - This is inspired by AlphaGo Zero, where AI systems learned to master Go through self-play with minimal human knowledge. The authors want to explore if a similar self-improvement is possible for LLMs in a natural language game.- The negotiation game involves a "seller" LLM trying to sell a product at a higher price, and a "buyer" LLM trying to buy it at a lower price. After each round, a "critic" LLM gives feedback to help one player improve their strategy. - The goal is to see if the LLMs can continuously improve their negotiation performance over multiple rounds by incorporating history, feedback, and self-reflection with minimal human input.- This tests the models' ability to understand game rules, strategize, incorporate natural language feedback, and exhibit long-term improvement, which are non-trivial capabilities.- The central hypothesis seems to be that sufficiently capable and well-aligned LLMs can demonstrate autonomous self-improvement in a multi-agent negotiation game setting through self-play and AI feedback. The experiments aim to test this hypothesis.In summary, the key research question is whether LLMs can autonomously improve each other in a negotiation game with minimal human intervention, by leveraging self-play and AI feedback. The paper seeks to experimentally explore this possibility.


## What is the main contribution of this paper?

The paper proposes a method for improving language models' negotiating skills by having them engage in a bargaining game and provide iterative AI feedback to each other. The key ideas and contributions are:- They have multiple LLMs play a negotiation game, with one model acting as the "seller" trying to sell a product at a higher price, another model acting as the "buyer" trying to buy it at a lower price, and a third model acting as a "critic" that provides feedback to the seller or buyer to help improve their negotiating strategy. - They use an "in-context learning from AI feedback" (ICL-AIF) approach where the feedback from the critic model and negotiation dialog history are provided as demonstrations to prompt the next round of negotiation. This allows the models to iteratively improve without needing explicit fine-tuning.- They find that only the most capable models like GPT-3.5, GPT-4, and Claude can actually continuously improve over multiple rounds of negotiation and feedback. Weaker models either don't understand the rules of the game or can't effectively incorporate the feedback.- The buyer role seems to be more difficult to improve than the seller role. Models playing as buyers are less able to benefit from the feedback compared to playing as sellers.- There is a tradeoff between improving the deal price and the likelihood of reaching a deal. As models get better at negotiating, they are also more likely to fail to reach a deal.- The negotiation responses become more verbose and strategic after multiple rounds of feedback, evidencing an improvement in language complexity and bargaining tactics.Overall, the key contribution is showing the promise of using self-play and AI feedback for iteratively improving LLMs' skills in a strategic dialog setting like negotiation, while also revealing the limitations of current models. The results have implications for developing autonomous agents and improving models with minimal human oversight.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from the paper:The paper proposes using multiple large language models to autonomously improve each other in a textual negotiation game through self-play and learning from AI feedback, showing the possibility of creating strong AI agents with minimal human intervention but also highlighting risks around oversight and alignment.


## How does this paper compare to other research in the same field?

Here are some thoughts on how this paper compares to other related work:- The idea of having AI agents learn and improve through self-play in games has been explored in prior work like AlphaGo Zero. However, this paper focuses specifically on using natural language for the game and feedback, rather than something like the game of Go. So it aims to explore these ideas more in the context of language models and dialog.- The use of in-context learning from demonstrations is related to prior work on few-shot prompting and in-context learning for language models. This paper applies those techniques to the setting of iteratively improving agents through self-play. It provides evidence that in-context learning can work for this purpose.- There has been growing interest recently in understanding and improving the capabilities of large language models through multi-turn dialog tasks. This work contributes by proposing a new dialog-based game setting that requires strategic negotiation skills. The game provides a testbed for analyzing model capabilities.- The idea of having one model provide feedback to improve another model relates to work on recursive self-improvement and constitutional AI. This paper explores a simplified version of that, with one "critic" model giving feedback to a "player" model.- Compared to standard reinforcement learning approaches, this work relies purely on natural language feedback rather than numeric rewards. The finding that language feedback can work is an interesting contribution.In summary, this paper builds on several existing ideas like self-play, in-context learning, dialog agents, and learning from feedback. But it combines these in a novel way for the task of improving negotiation skills of language models, while also providing analysis of model capabilities and limitations. The simple game setting is a strength that provides a controlled way to study these ideas.
