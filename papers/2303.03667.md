# [Run, Don't Walk: Chasing Higher FLOPS for Faster Neural Networks](https://arxiv.org/abs/2303.03667)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading, the key points of this paper are:

- It points out that simply reducing FLOPs (floating point operations) does not necessarily lead to faster neural networks, as it can result in inefficiently low FLOPS (floating point operations per second). 

- The paper identifies that frequent memory access, especially in depthwise convolutions, is a major cause of the low FLOPS issue.

- It proposes a new operator called partial convolution (PConv) that reduces both redundant computation and memory access to improve efficiency.

- PConv applies regular convolution to only a subset of input channels, while leaving others unchanged. This achieves lower FLOPs than regular conv and higher FLOPS than depthwise/group conv.

- The paper introduces FasterNet, built using PConv, as a new family of fast and accurate networks for vision tasks. 

- Experiments show FasterNet achieves state-of-the-art speed/accuracy trade-offs on ImageNet classification and COCO detection/segmentation.

So in summary, the central hypothesis is that designing networks to achieve higher FLOPS, by reducing redundant computation and memory access, is key to building truly fast neural networks beyond just reducing FLOPs. PConv and FasterNet are proposed to validate this hypothesis.


## What is the main contribution of this paper?

 The main contributions of this paper are:

- It points out that simply reducing FLOPs (floating point operations) does not necessarily lead to lower latency, since many networks suffer from low FLOPS (floating point operations per second). The paper argues that to truly achieve faster neural networks, both FLOPs and FLOPS need to be optimized.

- It proposes a novel partial convolution (PConv) operator that reduces both redundant computation and memory access simultaneously. PConv applies regular convolution to only a subset of input channels, while leaving the remaining channels untouched. This allows it to achieve higher FLOPS than depthwise/group convolution for a given amount of FLOPs.

- It introduces FasterNet, a new family of convolutional neural networks built using the proposed PConv operator. FasterNet achieves state-of-the-art tradeoffs between accuracy and latency/throughput across GPU, CPU, and ARM processors on image classification, object detection, and segmentation tasks.

- It demonstrates the effectiveness of PConv experimentally. When combined with a pointwise convolution, PConv better approximates full convolution while requiring less computation and memory access.

- Extensive experiments show FasterNet consistently outperforms prior CNN, ViT, and MLP models in terms of speed and efficiency on various devices and vision tasks. For example, FasterNet-T0 is 2.4-3.3x faster than MobileViT-XXS while being more accurate on ImageNet.

In summary, the key innovation is the PConv operator and FasterNet architecture that effectively optimize for both FLOPs and FLOPS to achieve truly fast neural network inference across devices and tasks. The paper makes a strong case that FLOPS needs more attention beyond just FLOPs reduction for fast networks.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence TL;DR summary of the paper:

The paper proposes a novel partial convolution operator and FasterNet architecture that achieves higher computational speed and lower latency compared to existing models by reducing redundant computations and memory access.


## How does this paper compare to other research in the same field?

 This paper introduces a new partial convolution operator and FasterNet architecture for efficient neural networks. Here are some key ways it compares to related prior work:

- It targets improving computational speed (FLOPS) beyond just reducing FLOPs like many other works. This is an important but understudied issue that it helps advance.

- The proposed partial convolution is a simple yet novel operator. It reduces both computational redundancy and memory access simultaneously, unlike existing operators like depthwise convolution that suffer from memory inefficiency when reducing FLOPs.

- FasterNet built on partial convolution consistently achieves state-of-the-art speed on various devices with no compromise on accuracy. Many prior networks are not actually fast in practice due to issues like low FLOPS.

- The paper focuses on designing simple and hardware-friendly networks. This differs from many recent works that incorporate complex mechanisms like attention, dynamic convolutions, advanced normalization layers etc. which may limit speed.

- It does not use techniques like neural architecture search or knowledge distillation. This helps demonstrate the direct gains of the proposed ideas.

In summary, this paper makes important contributions in designing fast neural networks by analyzing the FLOPS issue, proposing efficient partial convolution, and introducing hardware-friendly FasterNet. The simplicity yet strong performance of partial convolution and FasterNet help advance this field meaningfully.


## What future research directions do the authors suggest?

 Based on my reading of the paper, here are some of the future research directions suggested by the authors:

- Exploring different partial convolution designs and ratios to further improve efficiency and effectiveness. The authors mention that the current design applies convolution on the first few channels, but other designs like applying it on random or evenly spaced channels could be tried. The partial ratio could also be further tuned.

- Enlarging the receptive field of FasterNet with techniques like dilated convolution. This may help improve accuracy for tasks requiring larger context. 

- Combining FasterNet with other operators like attention to create hybrid architectures. The authors suggest FasterNet has potential to be combined with other operators for better performance.

- Applying FasterNet to other domains beyond computer vision, such as natural language processing. The authors designed FasterNet mainly for computer vision tasks, but its efficiency may transfer to other domains as well.

- Using techniques like neural architecture search and knowledge distillation to further optimize FasterNet. The authors did not use these techniques in their current work but suggest they could be applied to potentially improve performance.

- Deploying and benchmarking FasterNet on more types of hardware devices, like mobile phones, to validate its efficiency. The authors currently evaluated FasterNet on GPU, CPU and ARM processors but could expand to more hardware.

In summary, the main future direction is exploring variants of the proposed partial convolution and FasterNet architecture to further advance efficiency and accuracy on both vision and non-vision tasks. The authors also suggest combining FasterNet with other techniques for performance optimization.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the paper:

This paper introduces a new partial convolution (PConv) operator and a family of fast neural networks called FasterNet built using PConv. Many prior works focus on reducing floating-point operations (FLOPs) to achieve faster networks, but the authors find this does not directly translate to lower latency due to inefficiently low FLOPS caused by frequent memory access, especially in depthwise convolution. To address this, they propose PConv which applies a regular convolution to only a subset of input channels, reducing redundancy and memory access simultaneously. PConv extracts spatial features efficiently with lower FLOPs than regular convolution but higher FLOPS than depthwise convolution. The paper further introduces FasterNet models built using PConv and pointwise convolution. Experiments on ImageNet classification and COCO detection/segmentation show FasterNet variants outperform prior CNN, ViT, and MLP models in accuracy-latency/throughput tradeoffs on GPU, CPU, and ARM processors. For example, FasterNet-T0 is much faster than MobileViT-XXS while being more accurate, and FasterNet-L matches Swin-B accuracy but with higher throughput.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the paper:

This paper proposes a new partial convolution (PConv) operator and a FasterNet architecture built using PConv to achieve faster neural networks. Many prior works focus on reducing FLOPs (floating point operations) to improve speed, but the authors find this does not always correspond to lower latency due to inefficiently low FLOPS (floating point operations per second). PConv reduces both redundant computation and memory access by only applying regular convolution to a subset of input channels. This provides higher FLOPS than depthwise convolution with lower FLOPs than regular convolution. 

Based on PConv, the authors propose FasterNet architectures with four hierarchical stages using FasterNet blocks consisting of PConv followed by pointwise convolution. On ImageNet-1k classification, tiny FasterNet-T0 is 2.8-3.3x faster than MobileViT-XXS on GPU/CPU with 2.9% higher accuracy. Large FasterNet-L achieves similar 83.5% top-1 accuracy as Swin-B but with 36% higher GPU throughput and 37% lower CPU latency. For detection and segmentation on COCO, FasterNet consistently outperforms ResNet and ResNeXt backbones. The results demonstrate PConv and FasterNet achieve state-of-the-art speed/accuracy trade-offs on various devices and vision tasks.


## Summarize the main method used in the paper in one paragraph.

 Here is a one paragraph summary of the main method used in the paper:

The paper proposes a novel partial convolution (PConv) operator to build efficient neural networks with high computational speed. PConv applies a regular convolution on only a subset of input channels, selecting them as spatial feature representatives, while leaving the remaining channels untouched. This reduces both computational redundancy and memory access compared to regular and depthwise convolutions. PConv extracts spatial features efficiently and is shown to approximate a regular convolution when combined with a pointwise convolution. Based on PConv, the paper introduces FasterNet, a fast yet accurate neural network family. FasterNet consists of four stages with PConv and pointwise convolution stacked as inverted residual blocks. It achieves state-of-the-art tradeoffs between accuracy and latency/throughput across various vision tasks and hardware platforms like GPUs, CPUs and ARM processors. Experiments demonstrate that FasterNet runs faster than prior CNN, MLP and transformer models under similar accuracy constraints. The simplicity and effectiveness of PConv and FasterNet showcase the feasibility of designing high-speed networks.
