# [Run, Don't Walk: Chasing Higher FLOPS for Faster Neural Networks](https://arxiv.org/abs/2303.03667)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the key points of this paper are:- It points out that simply reducing FLOPs (floating point operations) does not necessarily lead to faster neural networks, as it can result in inefficiently low FLOPS (floating point operations per second). - The paper identifies that frequent memory access, especially in depthwise convolutions, is a major cause of the low FLOPS issue.- It proposes a new operator called partial convolution (PConv) that reduces both redundant computation and memory access to improve efficiency.- PConv applies regular convolution to only a subset of input channels, while leaving others unchanged. This achieves lower FLOPs than regular conv and higher FLOPS than depthwise/group conv.- The paper introduces FasterNet, built using PConv, as a new family of fast and accurate networks for vision tasks. - Experiments show FasterNet achieves state-of-the-art speed/accuracy trade-offs on ImageNet classification and COCO detection/segmentation.So in summary, the central hypothesis is that designing networks to achieve higher FLOPS, by reducing redundant computation and memory access, is key to building truly fast neural networks beyond just reducing FLOPs. PConv and FasterNet are proposed to validate this hypothesis.


## What is the main contribution of this paper?

The main contributions of this paper are:- It points out that simply reducing FLOPs (floating point operations) does not necessarily lead to lower latency, since many networks suffer from low FLOPS (floating point operations per second). The paper argues that to truly achieve faster neural networks, both FLOPs and FLOPS need to be optimized.- It proposes a novel partial convolution (PConv) operator that reduces both redundant computation and memory access simultaneously. PConv applies regular convolution to only a subset of input channels, while leaving the remaining channels untouched. This allows it to achieve higher FLOPS than depthwise/group convolution for a given amount of FLOPs.- It introduces FasterNet, a new family of convolutional neural networks built using the proposed PConv operator. FasterNet achieves state-of-the-art tradeoffs between accuracy and latency/throughput across GPU, CPU, and ARM processors on image classification, object detection, and segmentation tasks.- It demonstrates the effectiveness of PConv experimentally. When combined with a pointwise convolution, PConv better approximates full convolution while requiring less computation and memory access.- Extensive experiments show FasterNet consistently outperforms prior CNN, ViT, and MLP models in terms of speed and efficiency on various devices and vision tasks. For example, FasterNet-T0 is 2.4-3.3x faster than MobileViT-XXS while being more accurate on ImageNet.In summary, the key innovation is the PConv operator and FasterNet architecture that effectively optimize for both FLOPs and FLOPS to achieve truly fast neural network inference across devices and tasks. The paper makes a strong case that FLOPS needs more attention beyond just FLOPs reduction for fast networks.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence TL;DR summary of the paper:The paper proposes a novel partial convolution operator and FasterNet architecture that achieves higher computational speed and lower latency compared to existing models by reducing redundant computations and memory access.
