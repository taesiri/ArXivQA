# [Run, Don't Walk: Chasing Higher FLOPS for Faster Neural Networks](https://arxiv.org/abs/2303.03667)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the key points of this paper are:- It points out that simply reducing FLOPs (floating point operations) does not necessarily lead to faster neural networks, as it can result in inefficiently low FLOPS (floating point operations per second). - The paper identifies that frequent memory access, especially in depthwise convolutions, is a major cause of the low FLOPS issue.- It proposes a new operator called partial convolution (PConv) that reduces both redundant computation and memory access to improve efficiency.- PConv applies regular convolution to only a subset of input channels, while leaving others unchanged. This achieves lower FLOPs than regular conv and higher FLOPS than depthwise/group conv.- The paper introduces FasterNet, built using PConv, as a new family of fast and accurate networks for vision tasks. - Experiments show FasterNet achieves state-of-the-art speed/accuracy trade-offs on ImageNet classification and COCO detection/segmentation.So in summary, the central hypothesis is that designing networks to achieve higher FLOPS, by reducing redundant computation and memory access, is key to building truly fast neural networks beyond just reducing FLOPs. PConv and FasterNet are proposed to validate this hypothesis.
