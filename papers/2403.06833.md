# [Can LLMs Separate Instructions From Data? And What Do We Even Mean By   That?](https://arxiv.org/abs/2403.06833)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem Statement
- Large language models (LLMs) lack a formal separation between "instructions" that are meant to be executed vs "data" that is meant to be passively processed. This causes issues like models failing to translate text that says "Don't translate this."
- There is no established definition or measure of instruction-data separation. This gap makes it hard to evaluate models and progress in this area.

Proposed Solution
- The authors introduce a formal measure called the "separation score" to quantify how differently a model responds to the same text being placed in either the instruction or data input.
- They propose an "empirical separation score" that can be estimated from model outputs without needing internal representations.
- They create a dataset called SEP for estimating the separation score. It has diverse tasks and probe/witness pairs to test if probes get executed or processed.

Key Contributions
- Formalizing instruction-data separation for the first time.
- Introducing a separation score measure and its empirical estimate. 
- Releasing the SEP dataset for evaluating separation.
- Finding that current major LLMs fail to adequately separate instructions from data, with scores as low as 0.225 for GPT-4.
- Showing separation does not improve with scale and may get worse, needing new architectures.

In summary, this paper identifies and formalizes the lack of instruction-data separation in LLMs as a key problem. It proposes quantitative solutions for defining and measuring this phenomenon, as well as demonstrating that more progress is needed by evaluating current models.
