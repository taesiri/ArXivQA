# [FlyKD: Graph Knowledge Distillation on the Fly with Curriculum Learning](https://arxiv.org/abs/2403.10807)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Graph neural networks (GNNs) are powerful for graph data but have high memory usage, making deployment difficult. 
- Knowledge distillation (KD) can compress GNNs but optimizing student models on noisy pseudo-labels from teachers is tricky.
- Existing KD methods focus on pseudo-label generation, not noisy optimization.

Proposed Solution:
- FlyKD - Generates unlimited pseudo-labels on-the-fly using random graphs. Avoids GPU memory limits.  
- Incorporates curriculum learning to handle noisy pseudo-labels. Gradually transitions from clean to noisy labels.
- Uses 3 loss terms: original labels, pseudo-labels on original graph, pseudo-labels on random graphs. Weights change over time.

Contributions:
- First KD method to generate unlimited pseudo-labels, enabled by on-the-fly computation.
- Shows curriculum learning is key to handle noisy optimization in KD. Boosts even vanilla KD. 
- Empirically outperforms baselines like vanilla KD and LSPGCN on disease-drug prediction.
- Opens new direction focusing on student optimization, not just pseudo-label generation.
- Analysis shows need for gradual curriculum changes, not step-wise.
- Limitation: Evaluation on single dataset. More validation needed.

In summary, FlyKD with curriculum learning enables noisy KD optimization at scale, significantly improving performance over baselines. The focus on stable optimization over noisy labels is a new direction for KD research.
