# [Distribution Shift Matters for Knowledge Distillation with Webly   Collected Images](https://arxiv.org/abs/2307.11469)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the central research question is: 

How can we effectively perform knowledge distillation to learn a lightweight student network from a pretrained teacher network, when the original training data is unavailable due to privacy or data management issues?

The paper proposes a new method called "Knowledge Distillation between Different Distributions (KD3)" to address this question. The key ideas are:

1) Dynamically select useful training instances from web data that have a similar distribution to the original data, by combining predictions from the teacher and student. 

2) Align features between teacher and student networks by sharing classifier parameters and performing weighted feature alignment on selected web data.

3) Use a new contrastive learning block called MixDistribution to generate perturbed data with different distributions, and train the student network to produce consistent representations for unperturbed and perturbed data.

Overall, the paper hypothesizes that by carefully handling the distribution shift between web data and original data, their proposed KD3 method can effectively perform knowledge distillation to learn an accurate and lightweight student network without needing the original training data.


## What is the main contribution of this paper?

 This paper proposes a new data-free knowledge distillation method called KD^3 to train a student network using web-collected images without access to the original training data. The key contributions are:

- It addresses the issue of distribution shift between web-collected data and original data, which is commonly overlooked in existing data-free knowledge distillation methods. 

- It selects useful web images that have similar distribution to the original data by dynamically combining predictions from the teacher and student networks.

- It aligns the feature distributions of teacher and student networks via weighted feature alignment and sharing the classifier parameters.

- It uses a novel MixDistribution contrastive learning to make the student network robust to distribution shifts.

- Experiments show the proposed KD^3 method outperforms state-of-the-art data-free distillation methods on image classification benchmarks.

In summary, the main contribution is proposing a data-free knowledge distillation approach that explicitly handles the distribution shift issue when using web data, through selective instance sampling, feature alignment, and contrastive learning. This allows training a reliable student network without access to the original training data.
