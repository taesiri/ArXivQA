# [Indirect Gradient Matching for Adversarial Robust Distillation](https://arxiv.org/abs/2312.03286)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a summary paragraph of the key points from the paper:

This paper proposes a novel adversarial distillation method called Indirect Gradient Distillation Module (IGDM) that transfers gradient information from a robust teacher model to a student model. Unlike most adversarial distillation techniques that focus on distilling the teacher's logits, IGDM leverages the inherent local linearity of adversarially trained models to indirectly align input gradients between the teacher and student. Through Taylor expansion approximations, IGDM matches differences in model outputs for perturbed inputs to implicitly match gradients, without needing to directly compute them. As gradient alignment improves, the student model becomes better at emulating the teacher's behavior pointwise. Experiments demonstrate IGDM's compatibility with existing adversarial training strategies, significantly boosting robustness across CIFAR datasets and model architectures. Key results include 30.32% AutoAttack accuracy on ResNet-18 and 29.52% on MobileNetV2 when combined with state-of-the-art techniques. The modular design also enables performance gains without expensive teacher model involvement during adversarial example generation. Overall, distilling gradient cues proves an effective way to transfer topological robustness knowledge.
