# [CLIP2Protect: Protecting Facial Privacy using Text-Guided Makeup via   Adversarial Latent Search](https://arxiv.org/abs/2306.10008)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question seems to be: How to protect facial privacy against unauthorized/black-box face recognition systems in a natural way that preserves user experience?The key hypothesis appears to be:By searching for adversarial latent codes in the low-dimensional manifold of a generative model trained on human faces, it is possible to generate protected face images that fool black-box face recognition systems while looking natural and preserving identity as perceived by humans. The paper proposes a framework to find such adversarial latent codes guided by user-defined makeup text prompts. The main contributions seem to be:1) A two-step facial privacy protection approach using adversarial latent code search in generative models.2) Leveraging textual makeup prompts for more flexible and natural adversarial transformations. 3) An identity-preserving regularization technique to maintain human-perceived identity.In summary, the core research focus is on developing a facial privacy protection method that strikes a balance between deceiving unauthorized face recognition systems (privacy) and preserving natural appearance and identity based on human perception (user experience). The key hypothesis is that optimizing over the latent space of generative models guided by textual prompts can achieve this goal.


## What is the main contribution of this paper?

The main contribution of this paper is developing a facial privacy protection method using text-guided adversarial latent search. Specifically, the key contributions are:1. Proposing a framework to search for adversarial latent codes in the latent space of a pretrained generative model (StyleGAN) to generate natural looking and identity preserving adversarial faces. 2. Leveraging user-defined textual makeup prompts and cross-modal vision-language model (CLIP) to guide the adversarial latent code search. This provides flexibility compared to using reference makeup images.3. Introducing an identity preserving regularization to optimize only over identity-related latent codes. This maintains the visual identity while fooling face recognition systems.4. Demonstrating the effectiveness of the approach under both face verification and identification settings for dodging and impersonation attacks. The method achieves higher protection success rates compared to prior arts like AMT-GAN and TIP-IM.5. Validating the practical applicability by attacking real-world commercial face recognition APIs like Face++ and Tencent.In summary, the key novelty is performing adversarial latent code search guided by user-defined text prompts to generate inconspicuous adversarial faces that protect facial privacy without compromising user experience. The identity preserving regularization and effectiveness against black-box APIs are other notable contributions.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the main contributions of this paper:This paper proposes a new facial privacy protection approach that searches for adversarial latent codes in the low-dimensional manifold of a pretrained generative model, guided by user-defined makeup text prompts and an identity-preserving regularization, to generate natural-looking adversarial faces that can fool black-box face recognition systems.


## How does this paper compare to other research in the same field?

Here are a few key points comparing this paper to other related work on protecting facial privacy:- The paper focuses on generating naturalistic and inconspicuous adversarial faces to fool unauthorized face recognition systems, while preserving the user's identity and online experience. This goal of balancing privacy protection and visual quality differentiates it from other works like obfuscation methods that degrade image quality.- Unlike noise-based adversarial attacks, this paper aims to generate unrestricted adversarial examples by searching over the latent space of a generative model. This allows larger but more natural perturbations compared to norm-bounded noise. The key novelty is the use of textual makeup prompts to guide this latent code search.- Compared to recent makeup-based adversarial attacks, this paper does not require a reference makeup image or retraining the networks for each new target identity. The text prompts provide more flexibility. Also, existing makeup attack methods can have artifacts due to interference between makeup transfer and adversarial objectives. This paper addresses that by initialization and identity regularization.- The method only requires a single input image, unlike some prior works that need multiple photos of a person. And it focuses on black-box attacks suitable for privacy protection against unauthorized systems with no access to the face models.- For evaluation, the paper considers both face verification and identification scenarios. It reports superiority over prior arts in black-box settings on both tasks. The ability to protect privacy under identification is novel compared to makeup attacks that are designed only for impersonation under verification.Overall, the use of text-guided latent code search to generate inconspicuous and identity-preserving adversarial faces appears to be a unique contribution compared to existing facial privacy protection techniques. The results demonstrate improved naturalness, flexibility and black-box transferability.


## What future research directions do the authors suggest?

Based on my reading of the paper, some of the key future research directions suggested by the authors include:- Developing techniques to automatically select the best text prompt and target identity for a given input face image. The current approach relies on manually specified text prompts and target identities. Automating this could improve the usability of the approach.- Reducing the computational cost of generating protected images. The adversarial optimization process is computationally expensive, so research into accelerating or approximating this optimization could allow for real-time use.- Enhancing robustness against advanced FR systems. The authors suggest evaluating robustness against more sophisticated FR models. Extending the approach to fool advanced systems is an important direction.- Exploring alternative generative models beyond StyleGAN. The framework relies on StyleGAN's disentangled latent space. Applying similar ideas to other generative models could be useful.- Evaluating the framework on more diverse and unconstrained face images. Testing on truly in-the-wild social media images could reveal limitations to address.- Incorporating mechanisms for selective sharing. Allowing users to define different privacy levels for different images or applications is suggested as a worthwhile capability.- Comparing human versus machine perceptual quality. More rigorous evaluation of the human perception of protected images could guide improvements to naturalness.In summary, key future directions revolve around improving usability, robustness, efficiency, and evaluation to progress the framework toward real-world viability as a privacy enhancement technology. The paper lays out promising research avenues along these lines.
