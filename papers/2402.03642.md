# [Stanceosaurus 2.0: Classifying Stance Towards Russian and Spanish   Misinformation](https://arxiv.org/abs/2402.03642)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Misinformation spreads rapidly on social media and is a highly multicultural phenomenon, needing cross-cultural and cross-lingual analysis tools. 
- Russian and Spanish misinformation pose significant threats but have limited research and datasets available.
- Existing stance detection datasets are limited in size, language coverage, and fail to provide tweet context.

Proposed Solution:
- The authors present Stanceosaurus 2.0, an expanded 5-way stance classification dataset for analyzing misinformation.
- It contains 1,907 Russian tweets over 18 claims and 1,966 Spanish tweets over 23 claims. 
- Tweets are annotated as "supporting", "refuting", "querying", "discussing" or "irrelevant" towards each claim.
- Context tweets are included in chains to provide additional perspective.

Key Contributions:
- First large-scale 5-way stance corpus for detecting Russian and Spanish misinformation with rich contextual data.
- Demonstrates viability of stance classification for identifying multicultural misinformation via zero-shot cross-lingual transfer tests.  
- Spanish and Russian models achieved macro F1 scores around 43, on par with prior Hindi and Arabic results.
- Analysis of key properties of misinformation discourse in each language to inform future research.
- Public dataset to support advances in multilingual stance detection and misinformation analysis.

In summary, the paper introduces a valuable new dataset and experiments which highlight the potential of fine-tuning language-specific models to leverage stance signals for analyzing misinformation in diverse linguistic communities.


## Summarize the paper in one sentence.

 Here is a one-sentence summary of the paper:

The paper introduces Stanceosaurus 2.0, an extension of the 5-way stance classification dataset Stanceosaurus, which includes 1,907 Russian tweets over 18 claims and 1,966 Spanish tweets over 23 claims to support research on identifying multicultural misinformation; experiments demonstrate viability through 43 F1 score in zero-shot cross-lingual transfer.


## What is the main contribution of this paper?

 Based on my reading of the paper, the main contribution is the creation and release of the Stanceosaurus 2.0 dataset, which expands the existing Stanceosaurus stance classification dataset to include Russian and Spanish tweets related to misinformation claims. Specifically, the authors:

1) Collected and annotated 1907 Russian tweets over 18 misinformation claims and 1966 Spanish tweets over 23 misinformation claims using a 5-way stance labeling scheme. 

2) Showed the feasibility of using this data for cross-lingual stance detection by fine-tuning a multilingual BERT model in a zero-shot transfer setting, achieving macro F1 scores of around 43 for both Russian and Spanish.

3) Discussed characteristics and challenges of working with Russian and Spanish Twitter data for misinformation research, including issues like code-switching, use of obscenities, and circumvention of misinformation filters.

So in summary, the key contribution is creating and releasing a new multilingual stance detection dataset focused on misinformation in Russian and Spanish, along with benchmark cross-lingual experiments demonstrating its utility.


## What are the keywords or key terms associated with this paper?

 Based on my review of the paper, some of the key terms and keywords associated with it include:

- Stance detection/classification
- Misinformation
- Propaganda
- Russian language
- Spanish language 
- Twitter
- Social media
- Cross-lingual transfer
- Multilingual BERT (mBERT)
- Zero-shot learning
- Stanceosaurus dataset
- Fact checking

The paper introduces an extension of the Stanceosaurus dataset called Stanceosaurus 2.0, which includes new Russian and Spanish language Twitter data annotated for stance towards misinformation claims. The goal is to support cross-cultural and cross-lingual stance detection research, especially for identifying Russian and Spanish language misinformation. The paper demonstrates the potential of the dataset via zero-shot cross-lingual transfer experiments on mBERT.

Some other potentially relevant terms based on the content are code-switching, loss functions like cross-entropy and focal loss, propaganda analysis, and limitations around fact checking and using Twitter data. But the core focus seems to be on stance detection, misinformation, multilingual modeling, and the new Russian and Spanish stance datasets.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in this paper:

1. The paper mentions that there is little to no stance-based training data available for Russian and Spanish. Why did the authors still choose to conduct zero-shot cross-lingual transfer instead of collecting labeled data in those languages for supervised learning? What are the tradeoffs?

2. The paper uses a multilingual BERT model for zero-shot cross-lingual transfer. How does this model work under the hood to transfer knowledge from English to other languages? What are its limitations? 

3. The paper experiments with different loss functions like weighted cross-entropy loss and focal loss. Why are these needed instead of just using regular cross-entropy? How do they help mitigate class imbalance?

4. The paper uses Western-leaning sources for Russian misinformation claims due to lack of Russian fact-checking sites. Could this bias the stance distribution? How could the authors account for potential biases?

5. For tweet collection, the queries had to be iteratively refined to obtain a balanced stance distribution. What metrics did the authors use to determine if the queries were effective? 

6. The paper annotates the stance based on the misinformation claim being discussed. What guidelines were given to annotators on how to judge the stance? How consistent were the annotators in applying these guidelines?

7. The paper demonstrates macro F1 scores of around 43 for both Russian and Spanish. How does this compare to state-of-the-art stance detection methods? Is there still room for improvement?

8. The paper points out prevalence of code-switching in the tweets. How could the authors analyze the impact of code-switching on stance detection performance?

9. For future work, the paper suggests analyzing misinformation on platforms like Facebook and WhatsApp. What additional challenges might come up in adapting the approach to those platforms?

10. The stance detection is done on a per-tweet basis. How could the authors incorporate conversational context from reply chains more effectively into the model?
