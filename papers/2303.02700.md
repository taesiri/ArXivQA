# [HairStep: Transfer Synthetic to Real Using Strand and Depth Maps for   Single-View 3D Hair Modeling](https://arxiv.org/abs/2303.02700)

## What is the central research question or hypothesis that this paper addresses?

This paper addresses the problem of single-view 3D hair modeling from real portrait images. The central hypothesis is that using an appropriate intermediate representation can help bridge the gap between synthetic training data and real images, leading to better 3D hair reconstruction results. Specifically, the paper proposes a novel intermediate representation called "HairStep" that consists of a strand map and a depth map. The strand map captures the directed 2D orientation of hair strands, while the depth map provides 3D shape information. The key claims are:- Existing methods use ambiguous and noisy 2D orientation maps as the intermediate representation, which limits reconstruction quality. - The proposed HairStep representation provides clearer strand orientation and 3D shape cues compared to orientation maps.- Learning to predict HairStep from real images helps narrow the gap between synthetic and real domains.- HairStep enables higher fidelity 3D hair reconstruction from a single image compared to using orientation maps.So in summary, the main hypothesis is that a representation like HairStep can bridge the synthetic-to-real domain gap more effectively for single-view 3D hair modeling. The paper aims to demonstrate the advantages of this representation via quantitative and qualitative experiments.
