# [HairStep: Transfer Synthetic to Real Using Strand and Depth Maps for   Single-View 3D Hair Modeling](https://arxiv.org/abs/2303.02700)

## What is the central research question or hypothesis that this paper addresses?

This paper addresses the problem of single-view 3D hair modeling from real portrait images. The central hypothesis is that using an appropriate intermediate representation can help bridge the gap between synthetic training data and real images, leading to better 3D hair reconstruction results. Specifically, the paper proposes a novel intermediate representation called "HairStep" that consists of a strand map and a depth map. The strand map captures the directed 2D orientation of hair strands, while the depth map provides 3D shape information. The key claims are:- Existing methods use ambiguous and noisy 2D orientation maps as the intermediate representation, which limits reconstruction quality. - The proposed HairStep representation provides clearer strand orientation and 3D shape cues compared to orientation maps.- Learning to predict HairStep from real images helps narrow the gap between synthetic and real domains.- HairStep enables higher fidelity 3D hair reconstruction from a single image compared to using orientation maps.So in summary, the main hypothesis is that a representation like HairStep can bridge the synthetic-to-real domain gap more effectively for single-view 3D hair modeling. The paper aims to demonstrate the advantages of this representation via quantitative and qualitative experiments.


## What is the main contribution of this paper?

This paper proposes a new intermediate representation called HairStep for single-view 3D hair modeling. The key contributions are:- HairStep consists of a strand map and a depth map, which provides clearer and more complete information than traditional orientation maps for reconstructing 3D hair models. - Two new datasets are collected - HiSa for strand map annotation and HiDa for relative depth annotation on real images. This enables training and evaluating hair modeling on real data.- A framework is designed to generate HairStep from real images, including strand map prediction and a weakly supervised domain adaptive method for depth estimation. - Novel metrics HairSale and HairRida are introduced to quantitatively evaluate hair modeling results on real images based on the new datasets.- Experiments show HairStep effectively narrows the domain gap between synthetic and real data and achieves state-of-the-art hair modeling performance. The ablation studies validate the contribution of each component.In summary, the key contribution is proposing the HairStep representation and a data-driven framework to extract it from real images, which bridges the gap between synthetic and real domains for single-view 3D hair modeling. The new datasets and metrics are also valuable contributions.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points in the paper:The paper proposes a new hybrid intermediate representation called HairStep, consisting of a strand map and a depth map, for bridging the gap between synthetic and real data and enabling high-fidelity 3D hair modeling from a single image; it contributes new datasets with manual annotations to train and evaluate the approach and introduces novel metrics to quantify modeling accuracy.
