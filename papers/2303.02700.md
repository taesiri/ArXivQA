# [HairStep: Transfer Synthetic to Real Using Strand and Depth Maps for   Single-View 3D Hair Modeling](https://arxiv.org/abs/2303.02700)

## What is the central research question or hypothesis that this paper addresses?

This paper addresses the problem of single-view 3D hair modeling from real portrait images. The central hypothesis is that using an appropriate intermediate representation can help bridge the gap between synthetic training data and real images, leading to better 3D hair reconstruction results. Specifically, the paper proposes a novel intermediate representation called "HairStep" that consists of a strand map and a depth map. The strand map captures the directed 2D orientation of hair strands, while the depth map provides 3D shape information. The key claims are:- Existing methods use ambiguous and noisy 2D orientation maps as the intermediate representation, which limits reconstruction quality. - The proposed HairStep representation provides clearer strand orientation and 3D shape cues compared to orientation maps.- Learning to predict HairStep from real images helps narrow the gap between synthetic and real domains.- HairStep enables higher fidelity 3D hair reconstruction from a single image compared to using orientation maps.So in summary, the main hypothesis is that a representation like HairStep can bridge the synthetic-to-real domain gap more effectively for single-view 3D hair modeling. The paper aims to demonstrate the advantages of this representation via quantitative and qualitative experiments.


## What is the main contribution of this paper?

This paper proposes a new intermediate representation called HairStep for single-view 3D hair modeling. The key contributions are:- HairStep consists of a strand map and a depth map, which provides clearer and more complete information than traditional orientation maps for reconstructing 3D hair models. - Two new datasets are collected - HiSa for strand map annotation and HiDa for relative depth annotation on real images. This enables training and evaluating hair modeling on real data.- A framework is designed to generate HairStep from real images, including strand map prediction and a weakly supervised domain adaptive method for depth estimation. - Novel metrics HairSale and HairRida are introduced to quantitatively evaluate hair modeling results on real images based on the new datasets.- Experiments show HairStep effectively narrows the domain gap between synthetic and real data and achieves state-of-the-art hair modeling performance. The ablation studies validate the contribution of each component.In summary, the key contribution is proposing the HairStep representation and a data-driven framework to extract it from real images, which bridges the gap between synthetic and real domains for single-view 3D hair modeling. The new datasets and metrics are also valuable contributions.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points in the paper:The paper proposes a new hybrid intermediate representation called HairStep, consisting of a strand map and a depth map, for bridging the gap between synthetic and real data and enabling high-fidelity 3D hair modeling from a single image; it contributes new datasets with manual annotations to train and evaluate the approach and introduces novel metrics to quantify modeling accuracy.


## How does this paper compare to other research in the same field?

Here are some key ways this paper compares to other research on single-view 3D hair modeling:- It proposes a new representation called HairStep consisting of a strand map and depth map to bridge the gap between synthetic and real data. This is different from most prior work that uses orientation maps, which can be noisy. The HairStep representation provides more directed and higher quality information.- It contributes two new datasets, HiSa and HiDa, with manual annotations of strand maps and depth for real portrait photos. This provides valuable ground truth data for training and evaluation, which has been lacking in this field. - It presents a learning-based framework to generate the HairStep representation from real images in a fully automatic way. This enables end-to-end hair modeling without extra user input.- It introduces two new quantitative evaluation metrics, HairSale and HairRida, to measure accuracy of predicted strand and depth maps compared to ground truth. This enables more objective evaluation than prior work.- Experiments show the HairStep representation helps achieve state-of-the-art performance in reconstructing 3D hair models from single images. Both visual quality and quantitative metrics are improved over prior art like HairNet and NeuralHDHair.In summary, this paper makes several notable contributions - a new representation, datasets, learning framework, evaluation metrics, and improved results. It moves the state-of-the-art forward for this challenging task of reconstructing 3D hair from images. The representation and data could also enable future work on related hair analysis tasks.
