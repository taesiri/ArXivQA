# [Mixture-of-Prompt-Experts for Multi-modal Semantic Understanding](https://arxiv.org/abs/2403.11311)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key aspects of the paper:

Problem:
- Multi-modal semantic understanding (MSU) tasks like multi-modal sarcasm detection (MSD) and multi-modal sentiment analysis (MSA) require modeling complex relationships between modalities.  
- However, most existing methods rely on sufficient training data which is difficult to collect for MSU tasks, especially sarcasm detection. 
- Vision-language models (VLMs) can alleviate this issue but adapting them for few-shot MSU via prompt learning remains an open challenge.

Proposed Solution:
- Propose Mixture-of-Prompt-Experts (MoPE) containing specialized prompts for images, text and their fusion to enhance both uni-modal representation and cross-modal interaction in VLMs.
- Introduce Block-Aware Prompt Fusion (BAF) to enable deeper prompt interactions and smooth transition from single-modal specialization to multi-modal fusion.  
- Specifically, prompt experts are divided into blocks and cross-attention fusion is applied between blocks to balance twin objectives of specialization and fusion.

Main Contributions:
- Novel specialized prompt experts in MoPE to assist uni-modal encoding and cross-modal interaction for few-shot MSU.
- New BAF mechanism for deeper prompt interactions through block-wise cross-modal attention.
- Experiments show the model with 150M parameters outperforms 8.2B parameter InstructBLIP on MSD task.
- Consistent improvements over previous MSU methods and wide-used prompt techniques on both MSD and MSA tasks.

In summary, the paper introduces an effective specialized prompt design and fusion method to enhance VLMs for complex few-shot multi-modal understanding tasks.
