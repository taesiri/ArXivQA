# Strike (with) a Pose: Neural Networks Are Easily Fooled by Strange Poses   of Familiar Objects

## What is the central research question or hypothesis that this paper addresses?

Based on my reading, the central hypothesis of this paper is that deep neural networks are vulnerable to misclassifying familiar objects when those objects are presented in unusual or "out-of-distribution" poses, even though the networks classify canonical poses of those objects correctly. The authors test this hypothesis by using 3D rendering to generate poses of objects that differ from the typical poses seen during training. They show that networks make errors on the vast majority of these out-of-distribution poses, even when the poses are still recognizable to humans. Furthermore, they find that the errors transfer across models and even to object detectors, suggesting a systematic vulnerability of current networks.In summary, the main hypothesis is that despite good performance on familiar poses, deep neural networks do not achieve true viewpoint-invariant object recognition and can fail on natural out-of-distribution poses of known objects. The paper aims to demonstrate and characterize this vulnerability using 3D rendering.


## What is the main contribution of this paper?

Based on my reading, the main contributions of this paper are:1. The authors propose a framework for finding out-of-distribution errors in computer vision models. Their framework uses iterative optimization in the parameter space of a 3D renderer to estimate changes (e.g. in object geometry, appearance, lighting, etc) that cause a target deep neural network (DNN) to misbehave. 2. Using this framework and a dataset of 3D objects, the authors investigate the vulnerability of state-of-the-art ImageNet classifiers to out-of-distribution poses of objects. They find that the classifiers only correctly label 3.09% of the entire 6D pose space of objects on average. They also show that adversarial poses transfer between models, with 99.9% of poses misclassified by one model also misclassified by another.3. The authors show that adversarial training on poses of known objects does not help the model generalize to new objects in the same class. They hypothesize that future models may need to incorporate more 3D reasoning to be robust against out-of-distribution poses.4. The authors introduce a new promising testing methodology for DNNs using 3D graphics and rendering. Their framework allows enumeration of test cases and provides insights into model failures.In summary, the key contribution is revealing the vulnerability of DNNs to natural out-of-distribution poses of objects, proposing a framework to systematically test models using 3D graphics, and providing insights into potential ways to improve robustness. The paper shows that state-of-the-art DNNs have limited pose understanding and do not perform true object recognition currently.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the paper:The paper proposes a framework for finding failures in computer vision models by optimizing the parameters of a 3D renderer to generate images that cause a target deep neural network to misbehave.


## How does this paper compare to other research in the same field?

Here are some key ways this paper compares to other related research:- Most prior work on adversarial examples has focused on perturbations to the pixel space of natural images. This paper explores a new type of adversarial example - adversarial poses of 3D objects rendered with a graphics engine. - The paper introduces a novel framework for discovering adversarial examples that involves optimizing in the parameter space of a graphics renderer, rather than directly manipulating pixels. This allows the authors to explore how factors like 3D object pose, lighting, and camera viewpoint impact classifier behavior.- The paper provides evidence that state-of-the-art image classifiers like Inception-v3 are highly vulnerable to out-of-distribution object poses, even for familiar objects like school buses. Adversarial poses transfer across models and even object detectors like YOLOv3.- The findings support the view that deep neural networks, while excelling at classifying common poses, do not perform true viewpoint-invariant object recognition. The results align with concurrent work questioning generalization in vision models.- Unlike some prior adversarial example work, the perturbations explored are not constrained to be imperceptible. The adversarial poses are unambiguous to humans but still fool classifiers.- Attempted defenses like adversarial training on 3D object poses are shown to be incomplete solutions. The paper hypothesizes 3D reasoning may be needed alongside efforts to address data/model biases.In summary, this paper explores a practical, physically-realizable method to generate adversarial examples and provides new insights into the limitations of state-of-the-art vision models when encountering out-of-distribution object poses. The frameworks and findings open interesting new research directions.
