# [Strike (with) a Pose: Neural Networks Are Easily Fooled by Strange Poses   of Familiar Objects](https://arxiv.org/abs/1811.11553)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading, the central hypothesis of this paper is that deep neural networks are vulnerable to misclassifying familiar objects when those objects are presented in unusual or "out-of-distribution" poses, even though the networks classify canonical poses of those objects correctly. The authors test this hypothesis by using 3D rendering to generate poses of objects that differ from the typical poses seen during training. They show that networks make errors on the vast majority of these out-of-distribution poses, even when the poses are still recognizable to humans. Furthermore, they find that the errors transfer across models and even to object detectors, suggesting a systematic vulnerability of current networks.In summary, the main hypothesis is that despite good performance on familiar poses, deep neural networks do not achieve true viewpoint-invariant object recognition and can fail on natural out-of-distribution poses of known objects. The paper aims to demonstrate and characterize this vulnerability using 3D rendering.


## What is the main contribution of this paper?

 Based on my reading, the main contributions of this paper are:1. The authors propose a framework for finding out-of-distribution errors in computer vision models. Their framework uses iterative optimization in the parameter space of a 3D renderer to estimate changes (e.g. in object geometry, appearance, lighting, etc) that cause a target deep neural network (DNN) to misbehave. 2. Using this framework and a dataset of 3D objects, the authors investigate the vulnerability of state-of-the-art ImageNet classifiers to out-of-distribution poses of objects. They find that the classifiers only correctly label 3.09% of the entire 6D pose space of objects on average. They also show that adversarial poses transfer between models, with 99.9% of poses misclassified by one model also misclassified by another.3. The authors show that adversarial training on poses of known objects does not help the model generalize to new objects in the same class. They hypothesize that future models may need to incorporate more 3D reasoning to be robust against out-of-distribution poses.4. The authors introduce a new promising testing methodology for DNNs using 3D graphics and rendering. Their framework allows enumeration of test cases and provides insights into model failures.In summary, the key contribution is revealing the vulnerability of DNNs to natural out-of-distribution poses of objects, proposing a framework to systematically test models using 3D graphics, and providing insights into potential ways to improve robustness. The paper shows that state-of-the-art DNNs have limited pose understanding and do not perform true object recognition currently.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the paper:The paper proposes a framework for finding failures in computer vision models by optimizing the parameters of a 3D renderer to generate images that cause a target deep neural network to misbehave.


## How does this paper compare to other research in the same field?

 Here are some key ways this paper compares to other related research:- Most prior work on adversarial examples has focused on perturbations to the pixel space of natural images. This paper explores a new type of adversarial example - adversarial poses of 3D objects rendered with a graphics engine. - The paper introduces a novel framework for discovering adversarial examples that involves optimizing in the parameter space of a graphics renderer, rather than directly manipulating pixels. This allows the authors to explore how factors like 3D object pose, lighting, and camera viewpoint impact classifier behavior.- The paper provides evidence that state-of-the-art image classifiers like Inception-v3 are highly vulnerable to out-of-distribution object poses, even for familiar objects like school buses. Adversarial poses transfer across models and even object detectors like YOLOv3.- The findings support the view that deep neural networks, while excelling at classifying common poses, do not perform true viewpoint-invariant object recognition. The results align with concurrent work questioning generalization in vision models.- Unlike some prior adversarial example work, the perturbations explored are not constrained to be imperceptible. The adversarial poses are unambiguous to humans but still fool classifiers.- Attempted defenses like adversarial training on 3D object poses are shown to be incomplete solutions. The paper hypothesizes 3D reasoning may be needed alongside efforts to address data/model biases.In summary, this paper explores a practical, physically-realizable method to generate adversarial examples and provides new insights into the limitations of state-of-the-art vision models when encountering out-of-distribution object poses. The frameworks and findings open interesting new research directions.


## What future research directions do the authors suggest?

 Based on my reading of the paper, some of the key future research directions the authors suggest are:- Developing better methods for incorporating 3D information into machine learning models to improve object recognition capabilities. The authors hypothesize that future models capable of visual reasoning will benefit from better use of 3D data.- Addressing biases in training datasets like ImageNet and COCO. The authors suggest these datasets reflect biases of their creators (e.g. tending to capture canonical views of objects) and that data augmentation with synthetic 3D rendered images could help alleviate these biases.- Continuing to explore the use of differentiable rendering and 3D scenes for generating adversarial examples and testing computer vision models. The authors propose their framework as a promising new method for evaluating model robustness.- Expanding the framework to jointly optimize lighting, background, and multiple objects in a full "adversarial world" to better approximate complex real-world imagery.- Acquiring larger-scale, high-quality 3D object datasets to use for more extensive adversarial training experiments. The authors were limited by the lack of such datasets but suggest this could be a promising direction.- Further investigation into the factors that make some objects harder or easier for models to recognize across poses (e.g. shape factors, variability in the training data, etc.). The findings on simple balls vs. more complex objects suggest further study here.Overall, the authors highlight the need for continued research into improving model robustness to out-of-distribution poses, incorporating 3D reasoning, addressing dataset biases, and leveraging synthetic data generation as key directions to build towards more capable real-world computer vision systems.


## Summarize the paper in one paragraph.

 The paper investigates the vulnerability of deep neural networks (DNNs) to out-of-distribution (OoD) poses of familiar objects. The authors propose a framework to find such errors in DNNs by optimizing the parameters of a 3D renderer to generate images that cause misclassification. Using this framework and a dataset of 3D objects, they study the robustness of image classifiers on the ImageNet dataset to translations and rotations of objects. The key findings are:- DNNs correctly classify only 3.09% of the entire pose space of objects and are sensitive to small disturbances, with changes as little as 10 degrees in rotation or a few pixels in translation causing misclassification. - Adversarial poses transfer between models, with 99.9% and 99.4% of poses misclassified by Inception-v3 also fooling AlexNet and ResNet-50 respectively. 75.5% of poses also fooled the YOLOv3 object detector.- Adversarial training on poses of known objects improves robustness on those objects but does not generalize well to new objects in the same class.The results suggest DNNs still cannot recognize objects invariant to pose and perform "true object recognition". The authors propose future models may benefit from better incorporation of 3D reasoning and information.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the paper:The paper proposes a framework for finding out-of-distribution (OoD) errors in computer vision models by optimizing the parameters of a 3D renderer to generate images that cause a target deep neural network (DNN) to misbehave. The framework allows exploring changes in object geometry, appearance, lighting, background, and camera parameters to find adversarial poses that fool DNNs. Using this framework and a dataset of 30 3D objects related to self-driving scenarios, the authors investigate the vulnerability of ImageNet classifiers to OoD poses of known objects. They find that DNNs correctly classify only 3.09% of the pose space of objects, even though they readily recognize the objects in canonical poses. Adversarial poses are found throughout the pose parameter space, transfer across DNN models trained on ImageNet, and many transfer to a YOLO object detector trained on MS COCO images. The results suggest state-of-the-art DNNs have not achieved true object recognition and are easily fooled by OoD poses of familiar objects.In more detail, the paper introduces a framework to test DNNs by optimizing renderer parameters to generate scenes that cause misclassifications. The framework allows exploring various scene changes like object pose, lighting, and background. Using 30 traffic-related 3D models, the authors study how ImageNet classifiers respond to object rotations and translations. Through random sampling and optimization, they find adversarial poses throughout the pose space, with DNNs correctly classifying only 3.09% of poses on average. The false poses transfer between DNN architectures and partially to YOLO object detection. These results reveal brittleness to OoD poses for objects that are readily recognized in normal poses. The findings suggest that despite high accuracy on test sets, state-of-the-art DNNs are still far from true object recognition. The paper proposes that future progress may come from better incorporation of 3D information beyond just image classifiers.


## Summarize the main method used in the paper in one paragraph.

 Here is a one paragraph summary of the main method used in the paper:The paper proposes a framework for finding out-of-distribution errors in computer vision models by optimizing the parameters of a 3D renderer to cause a target DNN to misbehave. Specifically, the authors use a 3D scene consisting of 3D objects, lighting, a background, and camera parameters that are fed into a renderer to produce a 2D image. This image is then classified by the target DNN. To find parameters that cause misclassification, the authors either (1) approximate gradients via finite differences and perform gradient descent, or (2) backpropagate through a differentiable renderer to directly obtain gradients and optimize to minimize the loss for a target misclassification label. Using this framework with purchased 3D objects, the authors study how DNNs respond to 3D translations and rotations of objects from known ImageNet classes.


## What problem or question is the paper addressing?

 The key points about the paper are:- The paper investigates the vulnerability of deep neural networks (DNNs) like image classifiers and object detectors to natural, out-of-distribution (OoD) inputs. Specifically, it looks at how DNNs respond to out-of-distribution poses of familiar 3D objects.- The authors propose a framework to generate adversarial examples by optimizing the pose parameters of a 3D renderer to cause a target DNN to misclassify rendered images.- Using this framework on a dataset of 3D models, they find that DNNs fail to correctly classify most poses of familiar objects. Small perturbations to pose like rotation and translation easily fool the DNNs.  - The adversarial poses transfer well across models like Inception-v3, AlexNet, ResNet-50, and to object detector YOLOv3. This suggests that different DNNs share similar weaknesses to OoD poses.- Training on adversarial poses helps for known objects, but does not generalize well to new objects of the same class. This implies pose robustness may require modeling 3D shape and viewpoint variations better.In summary, the key problem addressed is evaluating and understanding DNN failures on OoD poses of known objects using a framework with 3D graphics and optimization. The results reveal limitations of current DNNs for true object recognition.
