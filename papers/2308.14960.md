# [Read-only Prompt Optimization for Vision-Language Few-shot Learning](https://arxiv.org/abs/2308.14960)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading, the central research question this paper addresses is: 

How can we effectively leverage perceptual features from sensors like cameras and LiDAR for end-to-end autonomous vehicle trajectory prediction, in order to improve driving performance and reduce accidents?

The key hypothesis appears to be that using Transformer's attention mechanism to directly interact with high-dimensional perception features will enable smoother, more accurate trajectory predictions compared to existing methods like conditional imitation learning (CIL) or GRU-based networks. 

The limitations of CIL (needing multiple networks for different maneuvers) and GRU approaches (losing important perceptual information during feature compression) motivate exploring Transformer architecture for this trajectory prediction task. The central goal is to develop a waypoint prediction network that makes better driving decisions by fully exploiting available sensor data.


## What is the main contribution of this paper?

 The main contributions of this paper are:

1. The paper proposes a novel Target-point Attention Transformer (TAT) network for trajectory prediction in end-to-end autonomous driving. This is the first work to apply Transformer architecture for trajectory prediction in end-to-end driving models. 

2. The TAT network utilizes Transformer's attention mechanism to directly interact with high-dimensional perception features from the encoder, instead of compressing them like in previous GRU-based approaches. This allows the network to leverage more perceptual information for trajectory forecasting.

3. The proposed TAT network significantly outperforms prior works like conditional imitation learning and GRU-based models on the CARLA driving simulator. Quantitative experiments show reductions in collisions and blocked vehicles, and improvements in route completion.

4. The paper provides an ablation study analyzing different design choices like target-point attention, activation functions, and positional encodings. It concludes that the target-point attention and proper activation function are critical for good performance.

5. Overall, the TAT network sets a new state-of-the-art for end-to-end driving models on the complex CARLA benchmark. The results demonstrate the potential of Transformer architecture and attention mechanisms for trajectory prediction in autonomous driving systems.

In summary, the key innovation is the design of the Target-point Attention Transformer to effectively leverage perceptual features for trajectory forecasting in end-to-end autonomous driving models. Both quantitative and qualitative results on CARLA highlight the advantages of this approach.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the key points in the paper:

The paper proposes a novel Transformer-based trajectory prediction network called Target-point Attention Transformer (TAT) for end-to-end autonomous driving that utilizes attention mechanisms to interact directly with high-dimensional perception features and target points, achieving state-of-the-art performance on the CARLA simulator by significantly reducing accidents and improving route completion compared to prior methods.


## How does this paper compare to other research in the same field?

 Here are my thoughts on how this paper compares to other research in the field of end-to-end autonomous driving:

- This paper proposes a novel Transformer-based network architecture for predicting future waypoints/trajectories. Most prior work has relied on RNN/LSTM or CNN architectures. Using Transformer allows the model to better capture long-range dependencies and context.

- The idea of using attention between predicted waypoints and target points is novel and helps the model learn to follow the navigation goals more accurately, especially at intersections. Prior work did not explicitly model this interaction.

- Evaluating trajectory prediction methods for end-to-end driving in complex urban environments is still relatively new. Many prior papers only tested in simple scenarios or highway settings. The experiments here using the challenging CARLA benchmark push the state-of-the-art in terms of driving performance.

- The comparison between auto-regressive and classification formulations for trajectory prediction is an interesting analysis. For end-to-end driving, they find classification works better, whereas auto-regression is often better for pedestrian trajectory forecasting.

- The overall results achieved surpass prior published methods on the CARLA leaderboard, demonstrating this is a true advance to the state-of-the-art for this problem setting.

- One limitation is that they rely on a pretrained perception model (TransFuser) rather than training the perception and prediction jointly end-to-end. Some recent work has looked at joint training.

Overall, I think the proposed method and experiments demonstrate an important advancement in learning-based models for self-driving vehicles by effectively applying Transformer networks to the trajectory forecasting problem. The results show significant promise in handling complex urban driving scenarios.


## What future research directions do the authors suggest?

 Based on my reading of the paper, the authors suggest the following future research directions:

- Exploring better perception networks, especially explanatory networks such as those proposed in other recent works (e.g. Shao et al. 2022, Chitta et al. 2022, Chen et al. 2022). The authors note that their method struggled with traffic light recognition, likely due to limitations of the Transfuser perception model they used. They suggest investigating perceptual networks that can provide better traffic light detection.

- Incorporating additional constraints into the trajectory planning, such as safety and efficiency requirements. The authors note that their trajectory-based approach can more easily incorporate constraints by adjusting the trajectory planning algorithm. 

- Further exploration of classification vs auto-regression for trajectory prediction in autonomous driving. The authors found classification worked better than auto-regression in their experiments, contrasting with findings in pedestrian trajectory prediction. More research could elucidate when each approach is most suitable.

- Ablation studies and architecture improvements for the Transformer-based decoder. The authors propose a novel decoder architecture but suggest there are likely further enhancements possible through ablation studies and architectural tweaks.

- Testing the approach on more complex datasets and generalized driving scenarios. The authors currently only evaluate on the CARLA simulator with fixed weather conditions. Expanding the diversity of environments could reveal areas for improvement.

In summary, the main future directions are: improving perception, adding constraints, understanding tradeoffs between classification and auto-regression, refining the Transformer decoder, and evaluation on more diverse and complex scenarios. The authors lay out promising paths for advancing their Transformer-based end-to-end driving approach.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary of the key points from the paper:

The paper proposes a novel Transformer-based trajectory prediction network called Target-point Attention Transformer (TAT) for end-to-end autonomous driving. Unlike prior approaches that use RNNs like GRU and compress the perceptual features, TAT leverages the Transformer's attention mechanism to directly interact with high-dimensional 2D perceptual features from the perception backbone. This allows it to better utilize the available perceptual information for trajectory prediction. TAT employs self-attention to build dependencies between the predicted waypoints and navigation target points, and cross-attention to build dependencies between waypoints and perceptual features. Experiments on complex urban CARLA scenarios demonstrate that TAT significantly outperforms prior methods like conditional imitation learning and GRU-based approaches in driving performance metrics. Specifically, it achieves much higher driving scores and route completion rates, with fewer collisions, off-road infractions, and agent blockages. The results highlight the advantages of Transformer attention for end-to-end driving trajectory prediction and establish a new state-of-the-art on the CARLA benchmark.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the key points from the paper:

The paper proposes a novel Transformer-based network called Target-point Attention Transformer (TAT) for end-to-end autonomous vehicle trajectory prediction. Existing methods like conditional imitation learning (CIL) and GRU-based networks have limitations - CIL requires training multiple models and struggles on curves, while GRU-based methods lose important perceptual details through dimensionality reduction. 

TAT uses Transformer attention mechanisms to directly interact with high-dimensional perception features from the encoder and target points for navigation. This allows it to leverage more perceptual details and dependencies between waypoints for smoother, more accurate trajectory prediction. Experiments in complex urban CARLA scenarios demonstrate state-of-the-art performance - TAT significantly reduces collisions and route completion failures compared to CIL, AIM, and Transfuser baselines. Ablation studies validate the importance of the target-point attention structure and positional encodings. Key innovations are the Transformer decoder for trajectory prediction, and use of attention to incorporate perceptual features and navigate via sparse target points.


## Summarize the main method used in the paper in one paragraph.

 Here is a one paragraph summary of the main method used in the paper:

The paper proposes a novel trajectory prediction network called Target-point Attention Transformer (TAT) for end-to-end autonomous driving. TAT uses the Transformer architecture and its attention mechanism to directly interact with high-dimensional perception features from the sensor inputs. It takes as input RGB camera images and LiDAR data which are encoded by a CNN-Transformer backbone (Transfuser). The output is a predicted trajectory in the form of future waypoints. TAT employs self-attention to build dependency between the predicted waypoints and navigation target points. It also uses cross-attention to enable interaction between the waypoints and perception features. This allows TAT to leverage the full perception input for trajectory forecasting. Experiments in complex urban driving scenarios in CARLA show that TAT outperforms prior GRU-based and conditional imitation learning methods by reducing collisions and improving route completion. The results demonstrate the effectiveness of using Transformer attention for trajectory prediction in end-to-end autonomous driving.


## What problem or question is the paper addressing?

 Based on my reading, the key points of this paper are:

- The paper proposes a novel trajectory prediction network called Target-point Attention Transformer (TAT) for end-to-end autonomous driving. 

- Existing methods like conditional imitation learning (CIL) and GRU-based networks have limitations in leveraging perceptual features for trajectory prediction, which can lead to accidents or failure to complete routes. 

- TAT uses Transformer's attention mechanism to directly interact with high-dimensional perceptual features from the perception model and predict smoother, more accurate trajectories.

- The attention mechanism in TAT builds dependencies between predicted waypoints, target points from the route, and perceptual features. This allows it to better navigate intersections and curves compared to CIL or GRU methods.

- Experiments in complex urban driving scenarios in CARLA simulator show TAT significantly reduces collisions and improves route completion over baselines. It achieves state-of-the-art performance on the CARLA leaderboard metrics.

- The paper also analyzes classification vs auto-regressive trajectory prediction, finding classification outperforms auto-regression in this autonomous driving application, unlike in pedestrian trajectory prediction.

In summary, the key contribution is proposing TAT, a Transformer-based network for end-to-end driving that leverages attention to perceptual features and route points to improve trajectory prediction and driving performance. The experiments demonstrate clear benefits over prior CIL and GRU approaches.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and keywords are:

- End-to-end autonomous driving - The paper focuses on end-to-end models for autonomous driving that directly map perceptions to control outputs.

- Trajectory prediction - The main task is predicting the future trajectory of the autonomous vehicle.

- Transformer - The proposed model utilizes Transformer architecture and its attention mechanisms.

- Target-point attention - A novel component of the model that attends to target points for navigation. 

- CARLA simulator - The method is evaluated in the CARLA autonomous driving simulator.

- Driving metrics - Driving Score, Route Completion and Infraction Score are used to quantitatively evaluate driving performance.

- Classification vs auto-regression - The paper compares classification and auto-regressive approaches for trajectory prediction.

- Attention visualization - Attention weights are visualized to provide insights into the model.

So in summary, the key terms cover the task (trajectory prediction), the model (Transformer, target-point attention), the simulation platform (CARLA), the evaluation metrics, and some of the analyses done like the comparison of different modeling approaches.


## Based on the paper, what questions should we ask to create a comprehensive summary of it? Please list at least 10 questions.

 Here are 10 potential questions to ask to create a comprehensive summary of this paper:

1. What is the problem that this paper aims to solve in the field of autonomous driving?

2. What are the limitations of existing end-to-end autonomous driving methods according to the paper? 

3. What is the key idea and novelty of the proposed Target-point Attention Transformer (TAT) model?

4. How does the TAT model utilize Transformer's attention mechanism for trajectory prediction? 

5. What are the components and architecture of the TAT model?

6. How is the TAT model evaluated and what metrics are used?

7. What datasets and simulation environments are used for training and evaluation? 

8. What are the main results of the TAT model compared to other baselines?

9. What are the ablation studies and their findings regarding model components and design choices?

10. What are the main conclusions of the paper and potential future work suggested?


## Design 10 in-depth questions about method proposed in this paper and avoid simple question.

 Here are 10 potential in-depth questions about the method proposed in this paper:

1. The paper proposes a novel Transformer-based network called Target-point Attention Transformer (TAT) for trajectory prediction in end-to-end autonomous driving. What are the key limitations of existing methods like conditional imitation learning (CIL) and GRU-based networks that TAT aims to address?

2. How does TAT leverage the Transformer architecture, especially the attention mechanism, to build dependencies between predicted waypoints, target points, and perception features? What are the benefits of this approach over existing methods?

3. The paper evaluates TAT on the CARLA simulator. What specifics of the CARLA environment and evaluation metrics make it a suitable benchmark for testing end-to-end driving models? What are its limitations?

4. The paper concludes that modeling trajectory prediction as classification outperforms auto-regression for end-to-end driving, unlike in pedestrian trajectory forecasting. What factors may account for this difference?

5. Ablation studies are performed to validate design choices like target-point attention and positional encodings. What do these results reveal about important architectural considerations in TAT?

6. How does the model handle navigating intersections and curves? What role does the target-point attention play in those scenarios?

7. The model struggles with traffic light infractions. How could the perception backbone be improved to address this? What changes would be needed in the waypoint prediction network?

8. What other constraints and objectives besides collision avoidance could be incorporated in TAT's trajectory planning, such as efficiency, comfort or safety considerations?

9. How suitable is the Transformer architecture for online deployment in autonomous vehicles? What modifications may be needed to reduce its computational complexity?

10. What future work could be done to improve TAT? For instance, exploring different perception backbones, adding recurrent connections to handle long term dependencies, or using reinforcement learning to improve generalization.


## What is the central research question or hypothesis that this paper addresses?

 This paper proposes a novel method called Read-only Prompt Optimization (RPO) for adapting large-scale pre-trained vision-language models like CLIP to downstream tasks in a robust and efficient way, especially for few-shot learning settings. 

The central hypothesis is that existing prompt learning methods can negatively impact the internal representations of pre-trained models by allowing bi-directional interactions between the learnable prompts and original features through attention. This representation shift may hurt performance and generalization. 

To address this, RPO introduces read-only prompts that can read from but not influence the original feature representations. This prevents representation shift during adaptation and leads to better generalization and robustness according to the authors.

The key research questions examined are:

- Can preventing representation shift by using read-only prompts improve model generalization and robustness compared to prior prompt learning methods?

- Can read-only prompts provide parameter-efficient adaptation without heavy fine-tuning? 

- Does initializing prompts based on special tokens improve optimization and performance?

- Can read-only prompts enhance performance on few-shot learning compared to other methods?

Through experiments on various recognition benchmarks and model analysis, the authors aim to demonstrate the advantages of RPO in enabling robust and efficient adaptation of vision-language models.


## What is the main contribution of this paper?

 Based on my reading, the main contributions of this paper are:

1. It proposes Read-only Prompt Optimization (RPO), a novel method to adapt pre-trained vision-language models to downstream tasks. RPO uses read-only prompts and masked attention to prevent the internal representation shift of the pre-trained model during adaptation.

2. It develops an effective initialization method for the read-only prompts based on the special tokens (e.g. [CLS], [EOS]) of the pre-trained model. 

3. Through extensive experiments, it demonstrates that RPO achieves better generalization performance in few-shot learning settings, compared to prior methods like CLIP and CoCoOp. Specifically, RPO shows improved base-to-new generalization, domain generalization, lower variance, and computational efficiency.

In summary, the key contribution is the proposed RPO method that adapts vision-language models in a robust and generalizable way for downstream tasks, without shifting the internal representations of the pre-trained model. The read-only prompts and special token initialization enable effective and efficient adaptation in low-data regimes.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 The TL;DR of this paper is: It proposes a Read-only Prompt Optimization (RPO) method to adapt pre-trained vision-language models to downstream tasks by introducing read-only prompts that do not shift the internal representations, improving generalization performance especially in few-shot settings.


## How does this paper compare to other research in the same field?

 Here are a few key points of comparison between this paper and other research in prompt tuning for vision-language models:

- The key contribution of this paper is the proposed Read-only Prompt Optimization (RPO) method, which uses masked attention to prevent the internal representation shift of a pre-trained vision-language model when adapting it to new tasks/datasets. This differs from prior prompt tuning methods like CoOp and CoCoOp which allow the prompts to shift the model's representations.

- The authors argue that preventing internal representation shift leads to better generalization and lower variance, especially in low-data regimes. They provide experiments on few-shot learning settings to demonstrate this. Other prompt tuning papers like CoOp/CoCoOp focused more on standard training set sizes.

- This paper proposes initializing the read-only prompts based on the [CLS] and [EOS] special tokens of CLIP, finding it leads to better convergence. Most prior prompt tuning papers randomized or learned the prompt initialization.

- For evaluation, this paper looks at base-to-new generalization and domain generalization. Other prompt tuning papers have looked at a wider range of tasks and datasets. However, base-to-new generalization is a useful testbed for assessing generalization.

- Compared to adaptive methods like VPT and DualPrompt which also adapt both the text and image side of CLIP, this paper is focused specifically on prompt tuning and does not update any trainable parameters of CLIP itself.

Overall, the proposal of read-only prompts via masked attention is a novel contribution and the paper provides useful experimental analysis on generalization and variance in low-data regimes. The tradeoffs between updatable vs read-only prompts for CLIP tuning is an interesting research question worth further exploration.


## What future research directions do the authors suggest?

 Based on my review, the authors propose the following future research directions in this paper:

- Further investigate the efficiency and effectiveness of the read-only prompt optimization (RPO) approach compared to other adaptation strategies for pre-trained models. The paper suggests RPO is promising but under-explored. More research is needed to fully understand its advantages.

- Explore different techniques to initialize the read-only prompts rather than just using the special tokens. The authors use the [CLS] and [EOS] tokens but other initialization strategies may further improve performance. 

- Apply RPO to other types of models beyond just vision-language models like CLIP. The authors suggest RPO has significant potential but only demonstrate it on CLIP. Testing on other model architectures could reveal new insights.

- Evaluate RPO on a wider range of downstream tasks beyond image classification. The current experiments focus on image recognition benchmarks. Applying RPO in other domains like VQA, captioning etc. could better showcase its versatility.

- Analyze how the number of read-only prompts affects model adaptation with RPO. The paper uses a fixed number of prompts but studying the impact of this hyperparameter could be useful.

- Develop theoretical understandings to explain why read-only prompts generalize better compared to methods that shift representations. The empirical results show RPO generalizes better but more analysis into the theory behind this would be valuable.

In summary, the authors propose further exploring RPO's efficiency, applicability to other models and tasks, the impact of various design choices, and developing more theoretical grounding of why the approach works well as promising future directions.


## Summarize the paper in one paragraph.

 Here is a one paragraph summary:

The paper proposes a novel approach called Read-only Prompt Optimization (RPO) for adapting pre-trained vision-language models like CLIP to downstream tasks. RPO introduces learnable prompts concatenated to the input embeddings of CLIP's visual and text encoders. To prevent these prompts from shifting the internal representations of CLIP, RPO employs masked attention so the prompts can only read from but not affect the original embeddings. The prompts are initialized based on the special tokens like [CLS] and [EOS] for better optimization. Experiments show RPO outperforms methods like CoOp and CoCoOp in few-shot classification, having better generalization to novel classes and robustness to domain shifts. Compared to CoCoOp, RPO also reduces variance and computational overhead. The key benefit is that by restricting internal representation shift, RPO enables more stable adaptation and better generalization in data-deficient settings.


## Summarize the paper in two paragraphs.

 Here is a two paragraph summary of the paper:

The paper proposes a novel approach called Read-only Prompt Optimization (RPO) for adapting pre-trained vision-language models like CLIP to downstream tasks. Existing methods like CoOp and CoCoOp optimize prompts by concatenating them with the input embeddings. However, this changes the internal representations of the model through the attention mechanism, which can negatively impact performance in few-shot learning settings. 

To address this, RPO uses read-only prompts with masked attention to prevent shifting of the internal representations while still allowing the prompts to read from them. The prompts are initialized based on the special tokens like [CLS] to facilitate optimization. Experiments show RPO outperforms CoCoOp in base-to-new generalization and domain generalization across several benchmarks while reducing variance, improving computational efficiency, and using fewer parameters. The ability of RPO to adapt CLIP without altering internal representations makes it suitable for few-shot learning where training data is scarce.


## Summarize the main method used in the paper in one paragraph.

 Here is a one paragraph summary of the main method used in the paper:

The paper proposes a novel approach called Read-only Prompt Optimization (RPO) for adapting pre-trained vision-language models like CLIP to downstream tasks. RPO introduces additional learnable prompt vectors that are concatenated to the input embeddings of the visual and text encoders. To prevent the prompts from shifting the internal representations of the pre-trained model, RPO employs masked attention such that the prompts can read from but not influence the original feature tokens. The prompts are initialized based on the special tokens like [CLS] and [EOS] of the pre-trained model for better convergence. By optimizing the read-only prompts while keeping the pre-trained weights frozen, RPO allows efficient adaptation without damaging the representations learned during pre-training. Experiments show RPO achieves better generalization and robustness compared to existing prompt tuning methods, especially in few-shot settings, while reducing computational overhead.


## What problem or question is the paper addressing?

 Based on my review, the key points of this paper are:

- The paper addresses the problem of efficiently adapting large pre-trained vision-language models like CLIP to downstream tasks. Fine-tuning the entire models can be inefficient and damaging to the pre-trained representations. 

- The paper proposes a method called Read-only Prompt Optimization (RPO) to adapt pre-trained models by introducing additional learnable prompts while preventing shift in the internal representations of the pre-trained model.

- RPO uses masked attention and initializes the prompts based on special tokens like [CLS] and [EOS] to facilitate optimization and improve convergence.

- Through experiments, the paper shows RPO achieves better generalization performance in few-shot learning settings compared to prior prompt tuning methods like CoOp and CoCoOp. RPO also shows lower variance and is more robust to domain shifts.

- The main contributions are developing the RPO method to adapt pre-trained vision-language models efficiently without damaging internal representations, and demonstrating its effectiveness for generalization and robustness in few-shot learning scenarios.

In summary, the key focus is on efficiently adapting large pre-trained vision-language models to new tasks through prompt optimization while preventing negative impacts on the model's internal representations, especially in low-data regimes. The proposed RPO method is shown to improve generalization and robustness compared to prior arts.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the main keywords and key terms are:

- Vision-language models: The paper discusses adapting large-scale pre-trained vision-language models like CLIP, ALIGN, and FILIP to downstream tasks. These models are trained on image-text pairs to learn joint representations.

- Prompt learning/tuning: The paper focuses on using prompt learning to adapt the pre-trained models, where extra tokens or prompts are added to provide contextual clues for the downstream task.

- Read-only prompts: The key contribution of the paper is proposing read-only prompts that can read from but not modify the internal representations of the pre-trained model. This prevents representation shift.

- Masked attention: The read-only prompts are implemented using masked attention, which restricts the attention flow from prompts to existing feature tokens.

- Few-shot learning: The paper evaluates the approach on few-shot learning benchmarks with limited training data. Read-only prompts help improve generalization in data-deficient regimes. 

- Base-to-new generalization: Evaluating generalization from base classes seen during training to novel classes.

- Domain generalization: Evaluating how robust the representations are to domain shift, like from natural images to sketches.

- Parameter efficiency: Read-only prompts add minimal extra parameters compared to full fine-tuning or conditional prompts.

In summary, the key focus is on using read-only prompts for efficient and generalizable adaptation of vision-language models, especially for few-shot learning scenarios.


## Based on the paper, what questions should we ask to create a comprehensive summary of it? Please list at least 10 questions.

 Here are 10 potential questions to ask to create a comprehensive summary of the paper:

1. What is the motivation behind this work? Why is adapting large pre-trained vision-language models an important problem to solve?

2. What are some of the existing methods for adapting pre-trained models like CLIP to downstream tasks? What are some limitations of these methods?

3. What is prompt learning/engineering? How has it been used to adapt pre-trained language and vision-language models? 

4. What is the key idea behind the proposed Read-only Prompt Optimization (RPO) method? How does it differ from prior prompt learning techniques?

5. How does RPO prevent the internal representation shift of the pre-trained model? Why is preventing this shift beneficial?

6. How are the read-only prompts in RPO initialized? Why is this initialization strategy effective? 

7. What experiments were conducted to evaluate RPO? What metrics were used? How did it compare to baselines like CLIP, CoOp, and CoCoOp?

8. What analysis was done to demonstrate the benefits of RPO in terms of model variance, generalization, and computational efficiency?

9. What are the limitations of RPO? Are there any potential negative societal impacts of this work?

10. What are the key takeaways and contributions of this work? What directions for future work does it suggest?


## Design 10 in-depth questions about method proposed in this paper and avoid simple question.

 Here are 10 potential in-depth questions about the method proposed in this paper:

1. The paper proposes using "read-only" prompts to prevent the internal representation of the pre-trained CLIP model from shifting during adaptation. Why is preventing this shift important, especially in few-shot learning settings? How does it improve model robustness and generalization?

2. The proposed Read-only Prompt Optimization (RPO) method uses masked attention to prevent the prompts from impacting the original feature tokens in CLIP. Can you explain in more detail how the attention masking works? Why is this important?

3. The paper finds that special token-based initialization of the prompts leads to better performance. Why do you think initializing prompts based on [CLS] and [EOS] tokens works well compared to random initialization?

4. How does RPO differ from prior prompt tuning methods like CoOp and CoCoOp? What are the key advantages of using read-only prompts and masked attention compared to these prior methods?

5. The results show RPO achieves strong performance in few-shot base-to-new generalization. Why does RPO generalize better to novel classes compared to other methods? Does preventing representation shift play a role?

6. For domain generalization, what factors make RPO more robust to distribution shift compared to other prompt tuning methods?

7. RPO achieves lower variance in performance compared to CoCoOp, especially in extreme few-shot settings. Why does RPO exhibit lower variance? How does this relate to representation shift?

8. What are the computational benefits of RPO compared to conditional prompt methods like CoCoOp? Why does RPO have lower computational overhead?

9. The paper analyzes RPO with uni-modal (text-only) prompts. How does using read-only prompts in just one modality impact performance compared to bi-modal RPO?

10. What limitations does the RPO method have? What future work could be done to build upon or improve RPO for vision-language model tuning?
