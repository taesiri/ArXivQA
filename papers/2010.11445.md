# [MAM: Masked Acoustic Modeling for End-to-End Speech-to-Text Translation](https://arxiv.org/abs/2010.11445)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading of the paper, the main research question seems to be:

How can we develop an end-to-end speech translation model that does not rely on source language transcriptions, unlike previous approaches? 

The key hypothesis appears to be:

By training a model to reconstruct masked portions of the speech signal in a self-supervised manner, we can learn robust speech representations without relying on source language transcriptions.

In summary, the paper aims to develop an end-to-end speech translation technique that does not depend on source language transcriptions, which have been a core requirement of prior methods. The proposed approach, Masked Acoustic Modeling (MAM), attempts to address this by using a self-supervised reconstruction task on the speech input as an auxiliary training signal.


## What is the main contribution of this paper?

 Based on my reading of the paper, the main contributions seem to be:

- Proposing a novel Masked Acoustic Modeling (MAM) technique for end-to-end speech-to-text translation (E2E-ST) that learns robust speech representations in a self-supervised manner without relying on source language transcriptions. 

- Demonstrating that MAM can be used as an extra training module for E2E-ST to improve performance, even without any pre-training. Using span masking, MAM achieves average +1.09 BLEU improvements over 8 languages compared to vanilla E2E-ST.

- Showing that MAM can also be used as a stand-alone pre-training framework on arbitrary acoustic signals, not just speech. To my knowledge, this is the first technique able to pre-train on any audio.

- When pre-trained on English speech and fine-tuned for E2E-ST, MAM (without transcriptions) achieves average +2.26 BLEU over vanilla E2E-ST across 8 languages. This is close to the +2.41 BLEU gained by multi-task learning E2E-ST with ASR using transcriptions.

- Demonstrating that pre-training MAM on non-speech acoustic data (Free Music Archive dataset) still improves E2E-ST by +1.55 BLEU on average, showing the potential of pre-training on the vast amount of audio data available.

- The success of MAM does not rely on intensive computation like BERT or GPT-3, with only 6.5% more parameters than the E2E-ST baseline.

In summary, the key contribution is proposing the MAM technique and showing it can improve E2E-ST in various settings, without relying on source language transcriptions like previous work. MAM provides an effective way to pre-train on arbitrary acoustic data, which has great potential for speech-related tasks.
