# [Elastic Decision Transformer](https://arxiv.org/abs/2307.02484)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question appears to be: How can the limitations of Decision Transformer (DT) in offline reinforcement learning be addressed to improve its ability to perform trajectory stitching?The paper proposes a novel model called Elastic Decision Transformer (EDT) to address this question. The key limitations of DT that EDT aims to resolve are:- DT struggles with trajectory stitching, which involves generating an optimal trajectory by combining parts of sub-optimal trajectories. This limits its performance when learning from imperfect demonstrations. - DT uses a fixed context length when making decisions, which can lead to sub-optimal actions if the current trajectory is poor but DT focuses too much on matching the past context.The main hypothesis of the paper seems to be:- By using a variable context length that adapts based on trajectory quality, EDT can overcome the trajectory stitching limitations of DT and achieve better performance on offline RL benchmarks.Specifically, EDT can use shorter history when the current trajectory is poor, allowing it to "reset" and consider more possibilities. And it can use longer history when the trajectory is good, maintaining optimal trajectories. This flexible history should improve trajectory stitching. The paper evaluates this hypothesis through extensive experiments on D4RL and Atari games.In summary, the core research question is how to improve DT's trajectory stitching, and the proposed hypothesis is that a variable/elastic history mechanism can achieve this. The EDT model embodies this hypothesis and is evaluated versus DT and other baselines.
