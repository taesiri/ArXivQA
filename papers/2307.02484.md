# [Elastic Decision Transformer](https://arxiv.org/abs/2307.02484)

## What is the central research question or hypothesis that this paper addresses?

Based on my reading of the paper, the central research question appears to be: How can the limitations of Decision Transformer (DT) in offline reinforcement learning be addressed to improve its ability to perform trajectory stitching?The paper proposes a novel model called Elastic Decision Transformer (EDT) to address this question. The key limitations of DT that EDT aims to resolve are:- DT struggles with trajectory stitching, which involves generating an optimal trajectory by combining parts of sub-optimal trajectories. This limits its performance when learning from imperfect demonstrations. - DT uses a fixed context length when making decisions, which can lead to sub-optimal actions if the current trajectory is poor but DT focuses too much on matching the past context.The main hypothesis of the paper seems to be:- By using a variable context length that adapts based on trajectory quality, EDT can overcome the trajectory stitching limitations of DT and achieve better performance on offline RL benchmarks.Specifically, EDT can use shorter history when the current trajectory is poor, allowing it to "reset" and consider more possibilities. And it can use longer history when the trajectory is good, maintaining optimal trajectories. This flexible history should improve trajectory stitching. The paper evaluates this hypothesis through extensive experiments on D4RL and Atari games.In summary, the core research question is how to improve DT's trajectory stitching, and the proposed hypothesis is that a variable/elastic history mechanism can achieve this. The EDT model embodies this hypothesis and is evaluated versus DT and other baselines.


## What is the main contribution of this paper?

Based on my reading of the paper, the main contribution seems to be the proposal of a new reinforcement learning method called Elastic Decision Transformer (EDT). The key innovation of EDT compared to prior work like Decision Transformer is its ability to perform "trajectory stitching" more effectively. Specifically, EDT takes a variable length history of the trajectory as input, rather than a fixed length history. It tries to determine the optimal history length to use at each timestep based on estimating which length will lead to the highest future return. Using a shorter history allows EDT to "stitch" together parts of different suboptimal trajectories into an improved trajectory. The authors show empirically that EDT outperforms Decision Transformer and other offline RL algorithms, especially in multi-task regimes like locomotion and Atari games. They argue EDT is able to successfully leverage and combine suboptimal trajectories in the offline dataset to learn better policies.In summary, the main contribution seems to be proposing Elastic Decision Transformer, a modification to Decision Transformer that facilitates more effective trajectory stitching in offline RL by using variable length histories. The experiments aim to demonstrate the benefits of EDT over prior methods.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence summary of the key points from this paper:The paper proposes a novel Elastic Decision Transformer (EDT) for offline reinforcement learning that achieves improved performance and trajectory stitching capabilities by adaptively adjusting the history length used for action predictions based on estimating the maximal achievable return for different lengths.
