# [Multi-Fidelity Reinforcement Learning for Time-Optimal Quadrotor   Re-planning](https://arxiv.org/abs/2403.08152)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem: 
- High-speed online trajectory planning for drones is challenging due to the need to precisely model complex dynamics while being constrained by real-time computational requirements. 
- Traditional methods rely on simplified models or approximations, leading to conservative trajectories. Reinforcement learning requires extensive training data and struggles to adapt policies trained in simulation to the real world.

Proposed Solution:
- A multi-fidelity reinforcement learning (MFRL) framework that integrates reinforcement learning and Bayesian optimization to develop a planning policy optimized for real-world, real-time scenarios. 
- Co-trains a planning policy to minimize trajectory time and a reward estimator to predict trajectory feasibility. The reward estimator uses multi-fidelity Bayesian optimization to efficiently construct a high-fidelity feasibility boundary model, incorporating complex simulations and real-world tests.
- Further extends framework to include real-world flight experiments in training, enabling precise modeling of real-world constraints.

Key Contributions:
- Directly models feasibility boundary of time-optimal trajectories using Bayesian optimization instead of simplified constraints. Significantly enhances planning policy performance.
- Uses multi-fidelity Bayesian optimization to efficiently construct high-fidelity feasibility boundary model using a low-fidelity foundation. Reduces high-fidelity data requirements.  
- Incorporates real-world flight experiments into reinforcement learning training. Ensures planning policy precisely reflects real-world constraints. 
- Planning policy generates faster, more reliable trajectories in simulated and real-world tests compared to baseline method, with average 4.7% time reduction.
- Computation time for trajectory updates reduced to 2ms on average, compared to minutes for baseline. Enables real-time replanning.
