# [What to Hide from Your Students: Attention-Guided Masked Image Modeling](https://arxiv.org/abs/2203.12719)

## What is the central research question or hypothesis that this paper addresses?

 The central research question of this paper is:

Can an attention-guided masking strategy for masked image modeling improve vision transformer self-supervised pre-training compared to random masking?

The key hypotheses are:

1) Randomly masking image patches is less effective for masked image modeling pretext tasks compared to natural language because of the greater redundancy of visual information across patches.

2) Masking the most highly attended patches according to the transformer's own attention map will create a more challenging and useful pretext task by hiding more discriminative/salient visual information. 

3) This proposed attention-guided masking strategy called "Attentive Masking" will improve self-supervised pre-training for vision transformers compared to random masking, as measured by downstream task performance.

In summary, the paper introduces and advocates for attention-guided masking in masked image modeling to improve self-supervised vision transformer pre-training, instead of just using random masking like in previous work. The central hypothesis is that this will create a more useful pretext task and lead to better learned visual representations.


## What is the main contribution of this paper?

 The main contribution of this paper is the introduction of a novel masking strategy called "attention-guided masking" for self-supervised learning with vision transformers. 

Specifically, the key ideas are:

- Current masked image modeling (MIM) methods that are adapted from masked language modeling use random masking of image patches. However, random masking of image patches is less effective than random masking of words in text, because images contain more redundant information.

- To address this, the authors propose to use the intrinsic self-attention mechanism of vision transformers to guide the masking. An attention map is first generated by forwarding an image through a teacher transformer. Then, the most highly attended patches according to the attention map are masked, creating a more challenging pretext task. 

- This attention-guided masking strategy, called AttMask, is incorporated into distillation-based MIM frameworks like iBOT. It accelerates learning, improves performance on downstream tasks, and increases robustness to background changes compared to random masking baselines.

- AttMask incurs no additional cost when incorporated into distillation-based methods, since the teacher transformer generates the attention map anyway.

In summary, the key contribution is the idea of using the intrinsic self-attention of vision transformers to guide masking in a principled way instead of random masking, creating a more challenging and constructive pretext task for self-supervised learning. This demonstrably improves vision transformer pre-training.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

 Here is a one sentence summary of the paper:

The paper proposes a novel masking strategy called attention-guided masking that leverages a vision transformer's intrinsic self-attention maps to hide the most salient image regions during masked image modeling for more effective self-supervised pre-training.
