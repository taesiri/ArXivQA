# [When hard negative sampling meets supervised contrastive learning](https://arxiv.org/abs/2308.14893)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
- State-of-the-art image models follow a two-stage strategy - pre-training on large datasets and fine-tuning with cross-entropy loss. 
- Cross-entropy loss has limitations such as poor generalization, sensitivity to noisy labels and adversarial examples.
- Although solutions like knowledge distillation, Mixup, and label smoothing address some limitations, cross-entropy still limits performance especially in few-shot learning.
- Existing contrastive losses also have limitations - supervised contrastive losses ignore hard negative mining while unsupervised ones lack label information.

Proposed Solution:
- Propose a new fine-tuning loss called SCHaNe that integrates supervised contrastive learning with hard negative sampling.
- Uses labels to identify true positives and negatives. Reweights negatives based on similarity to positives to emphasize hard negatives.
- Loss function has two main parts - numerator minimizes distance between embeddings of positive pairs, denominator maximizes distance between anchor and negatives (especially hard negatives).

Main Contributions:
- Introduction of SCHaNe loss that improves model performance by generating more distinct embeddings via hard negative sampling.
- Significant gains over strong BEiT-3 baseline in few-shot learning settings (up to 3.32% on CIFAR-FS) and full dataset fine-tuning (3.41% on iNaturalist 2017).   
- Sets new SOTA for base models on ImageNet-1K with 86.14% accuracy.
- First work combining explicit hard negative sampling with supervised contrastive learning for image model fine-tuning.
- Analysis showing SCHaNe produces better embeddings and explains improved effectiveness.
