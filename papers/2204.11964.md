# [SceneTrilogy: On Human Scene-Sketch and its Complementarity with Photo   and Text](https://arxiv.org/abs/2204.11964)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading, the central research question this paper addresses is:

How can we learn a flexible joint embedding for scene understanding that fully supports the "optionality" brought by the complementary information across sketch, photo, and text modalities?

More specifically, the paper focuses on learning an embedding that provides:

1) Optionality across modalities - enables using any combination of modalities (e.g. only sketch, only text, or both sketch+text) as query for downstream tasks like retrieval.

2) Optionality across tasks - supports utilizing the embedding for both discriminative (e.g. retrieval) and generative (e.g. captioning) tasks. 

To achieve this, the paper proposes:

- Disentangling each modality into a modality-specific and a shared modality-agnostic component. The modality-agnostic components are aligned across modalities.

- Modeling the interaction between modality-agnostic components of sketch and text using a cross-attention mechanism. This allows flexibly combining sketch, text, or both as queries.

- Using the modality-agnostic components for retrieval, and combining them with modality-specific components for generative tasks like captioning.

In summary, the central hypothesis is that disentangling and flexibly combining information across modalities in this way enables a joint embedding that provides "optionality" in how sketch, photo, and text are utilized for diverse scene understanding tasks.
