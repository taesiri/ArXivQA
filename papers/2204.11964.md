# [SceneTrilogy: On Human Scene-Sketch and its Complementarity with Photo   and Text](https://arxiv.org/abs/2204.11964)

## What is the central research question or hypothesis that this paper addresses?

 Based on my reading, the central research question this paper addresses is:

How can we learn a flexible joint embedding for scene understanding that fully supports the "optionality" brought by the complementary information across sketch, photo, and text modalities?

More specifically, the paper focuses on learning an embedding that provides:

1) Optionality across modalities - enables using any combination of modalities (e.g. only sketch, only text, or both sketch+text) as query for downstream tasks like retrieval.

2) Optionality across tasks - supports utilizing the embedding for both discriminative (e.g. retrieval) and generative (e.g. captioning) tasks. 

To achieve this, the paper proposes:

- Disentangling each modality into a modality-specific and a shared modality-agnostic component. The modality-agnostic components are aligned across modalities.

- Modeling the interaction between modality-agnostic components of sketch and text using a cross-attention mechanism. This allows flexibly combining sketch, text, or both as queries.

- Using the modality-agnostic components for retrieval, and combining them with modality-specific components for generative tasks like captioning.

In summary, the central hypothesis is that disentangling and flexibly combining information across modalities in this way enables a joint embedding that provides "optionality" in how sketch, photo, and text are utilized for diverse scene understanding tasks.


## What is the main contribution of this paper?

 The main contribution of this paper is proposing the use of human scene sketches and exploring its complementarity with photos and text for multi-modal scene understanding. The key ideas are:

- Extending scene understanding to include human scene sketches, forming a complete "trilogy" of modalities - sketch, photo, and text.

- Conducting pilot studies showing sketch vs text tradeoffs for retrieval and subjective captioning tasks. This motivates combining sketch, photo, and text for multi-modal scene understanding. 

- Proposing a flexible joint embedding to support "optionality" across modalities (use any combinations as query) and tasks (retrieval and captioning). This is enabled by disentangling modality-specific and modality-agnostic components.

- Modeling the interaction between sketch, photo, and text modality-agnostic components using modified cross-attention and pooling. This allows flexible fusion of the three modalities.

- The unified embedding, once learned, supports various downstream tasks like retrieval, captioning, and subjective captioning without task-specific modifications.

In summary, the key contribution is exploring sketch and its complementarity with photo/text for multi-modal scene understanding via a flexible embedding, and demonstrating its effectiveness on various tasks. The inclusion of sketch brings new capabilities and understanding of human scene perception.
