# [Recurrent Vision Transformers for Object Detection with Event Cameras](https://arxiv.org/abs/2212.05598)

## What is the central research question or hypothesis that this paper addresses?

 Based on my understanding, the main research question this paper addresses is: How can we design an object detection framework for event cameras that achieves both high accuracy and low latency inference without specialized hardware?

The key points are:

- Event cameras have great potential for low-latency object detection due to their unique properties of sub-millisecond latency, high dynamic range, and robustness to motion blur. 

- Prior work has shown good detection performance but with high inference times beyond 40ms, which does not fully leverage the low latency aspect.

- The authors identify some suboptimal design choices in prior work, like expensive Conv-LSTM cells and heavy backbones, that lead to this tradeoff. 

- Their goal is to fundamentally redesign the vision backbone to get a better balance between performance and efficiency on conventional GPU hardware.

- The main hypothesis seems to be that through a hierarchical multi-stage design utilizing transformers, convolutions, and LSTMs in a novel way, they can achieve state-of-the-art accuracy with much lower latency than prior art.

In summary, by revisiting backbone design for event-based detection, the paper aims to enable high performance low-latency inference on conventional hardware, unlocking the potential of event cameras. The key novelty is in the model architecture itself.
