# [Adaptive Learning of Tensor Network Structures](https://arxiv.org/abs/2008.05437)

## What is the central research question or hypothesis that this paper addresses?

The central research question addressed in this paper is how to jointly optimize the structure and parameters of a tensor network to minimize a loss function in an efficient manner. Specifically, the paper considers the problem of minimizing a loss function over arbitrary tensor network structures, subject to a constraint on the total number of parameters. This is formalized as:\begin{equation}\min_{R_{i,j}} \min_{\mathbf{G}^{(1)},\dots,\mathbf{G}^{(p)}} \mathcal{L}(\text{TN}(\mathbf{G}^{(1)},\dots,\mathbf{G}^{(p)})) \quad \text{s.t. } \texttt{size}(\mathbf{G}^{(1)},\dots,\mathbf{G}^{(p)}) \leq C\end{equation}where $\mathcal{L}$ is the loss function, $\mathbf{G}^{(i)}$ are the core tensors of the tensor network, $R_{i,j}$ define the tensor network structure, and $C$ is a bound on the number of parameters. This formulation poses a bilevel optimization problem, with a discrete optimization over tensor network structures at the upper level, and a continuous optimization over tensor parameters at the lower level. The key research question is how to efficiently solve this problem in order to identify the best tensor network structure and parameters for minimizing a given loss function under a parameter budget constraint.To address this, the paper proposes a greedy algorithm that starts from a simple rank-1 tensor network and iteratively adds edges and increases ranks in a greedy fashion to improve the loss function. The greedy search is combined with continuous optimization techniques to solve the lower level problem.In summary, the core research contribution is an efficient joint optimization algorithm for adaptively learning the structure and parameters of tensor networks tailored to a specific loss function and parameter budget. Experiments demonstrate that the proposed method can find better tensor network structures and decompositions compared to existing approaches.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence TL;DR summary of the paper:The paper proposes a greedy algorithm to efficiently and adaptively learn the structure and parameters of a tensor network to optimize an arbitrary differentiable loss function.


## How does this paper compare to other research in the same field?

This paper introduces a novel greedy algorithm for adaptively learning tensor network (TN) structures to optimize an arbitrary differentiable loss function. The key contributions and comparisons to related work are:- Formulates the novel optimization problem of minimizing a loss function over the space of TN structures under a constraint on number of parameters. Prior work has focused on learning the parameters/ranks of a fixed TN structure. - Proposes a greedy algorithm to tackle this bi-level (discrete + continuous) optimization problem. At each iteration, it identifies the most promising TN edge to increment, transferring weights from previous iterations. This is different from prior greedy approaches that adaptively learn ranks of a fixed TN.- Can learn TN structures with internal nodes by using SVD to split nodes. Most prior work has focused only on structures without internal nodes like TT or TR.- Evaluated on tensor decomposition, completion and neural network compression tasks. Outperforms recent evolutionary algorithm for TN decomposition in [1], and finds better structures than TT for network compression [2].Key differences from related works:- [1] uses evolutionary algorithms to find TN structures to exactly decompose a tensor. Limited to decomposition and uniform ranks.- Prior greedy methods like [3,4] only learn ranks of a fixed TN structure. - [2] uses fixed TT structure for network compression. Proposed method finds better structures.- [5] explores TN structures for compression but uses a rounding algorithm.So in summary, key novelty is the problem formulation and greedy algorithm for jointly learning both TN structure and parameters for any differentiable loss function. Demonstrates state-of-the-art performance across multiple tasks.


## What future research directions do the authors suggest?

The paper suggests a few promising future research directions:- Exploring more efficient discrete optimization techniques to solve the upper-level discrete optimization problem of finding the best tensor network structure. The greedy algorithm proposed in the paper can get stuck in local optima. Other techniques like evolutionary algorithms, Bayesian optimization, etc. could be explored. - Scaling up the method to discover efficient tensor network structures for compressing larger neural network models. The experiments in the paper are limited to a small MNIST model. Applying the method to compress large modern convolutional neural networks could be an interesting research direction.- Incorporating structural constraints into the search space, for example to restrict it to tree-like tensor network structures. This could improve efficiency and scalability.- Studying whether the method can be adapted for specific application domains like spatio-temporal data or computer vision, where certain tensor network structures are known to be more effective. The search procedure could be biased based on domain knowledge.- Analyzing the notion of rank induced by unconstrained tensor network structures compared to traditional formats like Tucker or TT. This could provide theoretical insights into the expressiveness of different tensor network architectures.- Exploring whether the concepts proposed could be extended to graph neural networks or related graph-based models in addition to tensors.Overall, the paper provides a nice proof-of-concept for adaptively learning tensor network structures. But there is significant scope for future work to scale up and improve the approach as well as apply it to various domains. The combination of discrete structure search and continuous optimization seems promising.


## What is the main contribution of this paper?

The main contribution of this paper is proposing a greedy algorithm for adaptively learning tensor network (TN) structures to optimize an arbitrary differentiable loss function. The key ideas are:- Formulating the problem of jointly optimizing a loss function over TN structures and parameters as a bi-level optimization problem. The upper level problem is a discrete optimization over TN structures and the lower level is a continuous optimization over TN parameters.- Proposing a greedy approach to tackle the upper level problem. Starting from a rank-1 TN, the algorithm successively identifies the most promising edge to increment the rank and optimize the loss over the new TN structure. - Using a weight transfer mechanism to initialize the new TN parameters when incrementing the rank of an edge. This avoids optimizing from scratch at each greedy iteration.- Adding a method to split TN nodes using truncated SVD, enabling the discovery of TN structures with internal nodes. The effectiveness of the proposed greedy algorithm is demonstrated on tensor decomposition, completion, and neural network compression tasks. Key results include:- Outperforming specialized algorithms like CP, Tucker, TT/TR on tensor decomposition.- Outperforming TT/TR algorithms for image completion. - Finding more efficient TN structures than TT-based methods for neural network compression.In summary, the main contribution is a generic greedy algorithm to efficiently and adaptively learn TN structures directly from data/tasks rather than using predefined structures like TT or TR. This provides a way to automate and improve the design of TN models.
