# [Adaptive Learning of Tensor Network Structures](https://arxiv.org/abs/2008.05437)

## What is the central research question or hypothesis that this paper addresses?

The central research question addressed in this paper is how to jointly optimize the structure and parameters of a tensor network to minimize a loss function in an efficient manner. Specifically, the paper considers the problem of minimizing a loss function over arbitrary tensor network structures, subject to a constraint on the total number of parameters. This is formalized as:\begin{equation}\min_{R_{i,j}} \min_{\mathbf{G}^{(1)},\dots,\mathbf{G}^{(p)}} \mathcal{L}(\text{TN}(\mathbf{G}^{(1)},\dots,\mathbf{G}^{(p)})) \quad \text{s.t. } \texttt{size}(\mathbf{G}^{(1)},\dots,\mathbf{G}^{(p)}) \leq C\end{equation}where $\mathcal{L}$ is the loss function, $\mathbf{G}^{(i)}$ are the core tensors of the tensor network, $R_{i,j}$ define the tensor network structure, and $C$ is a bound on the number of parameters. This formulation poses a bilevel optimization problem, with a discrete optimization over tensor network structures at the upper level, and a continuous optimization over tensor parameters at the lower level. The key research question is how to efficiently solve this problem in order to identify the best tensor network structure and parameters for minimizing a given loss function under a parameter budget constraint.To address this, the paper proposes a greedy algorithm that starts from a simple rank-1 tensor network and iteratively adds edges and increases ranks in a greedy fashion to improve the loss function. The greedy search is combined with continuous optimization techniques to solve the lower level problem.In summary, the core research contribution is an efficient joint optimization algorithm for adaptively learning the structure and parameters of tensor networks tailored to a specific loss function and parameter budget. Experiments demonstrate that the proposed method can find better tensor network structures and decompositions compared to existing approaches.


## What is the TL;DR (Too Long, Didn't Read) version of the paper? If you had to summarize it in one sentence, what would that be?

Here is a one sentence TL;DR summary of the paper:The paper proposes a greedy algorithm to efficiently and adaptively learn the structure and parameters of a tensor network to optimize an arbitrary differentiable loss function.


## How does this paper compare to other research in the same field?

This paper introduces a novel greedy algorithm for adaptively learning tensor network (TN) structures to optimize an arbitrary differentiable loss function. The key contributions and comparisons to related work are:- Formulates the novel optimization problem of minimizing a loss function over the space of TN structures under a constraint on number of parameters. Prior work has focused on learning the parameters/ranks of a fixed TN structure. - Proposes a greedy algorithm to tackle this bi-level (discrete + continuous) optimization problem. At each iteration, it identifies the most promising TN edge to increment, transferring weights from previous iterations. This is different from prior greedy approaches that adaptively learn ranks of a fixed TN.- Can learn TN structures with internal nodes by using SVD to split nodes. Most prior work has focused only on structures without internal nodes like TT or TR.- Evaluated on tensor decomposition, completion and neural network compression tasks. Outperforms recent evolutionary algorithm for TN decomposition in [1], and finds better structures than TT for network compression [2].Key differences from related works:- [1] uses evolutionary algorithms to find TN structures to exactly decompose a tensor. Limited to decomposition and uniform ranks.- Prior greedy methods like [3,4] only learn ranks of a fixed TN structure. - [2] uses fixed TT structure for network compression. Proposed method finds better structures.- [5] explores TN structures for compression but uses a rounding algorithm.So in summary, key novelty is the problem formulation and greedy algorithm for jointly learning both TN structure and parameters for any differentiable loss function. Demonstrates state-of-the-art performance across multiple tasks.
