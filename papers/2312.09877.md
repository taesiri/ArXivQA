# [Distributed Learning of Mixtures of Experts](https://arxiv.org/abs/2312.09877)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem:
The paper considers the problem of learning mixture of experts (MoE) models in a distributed setting with large amounts of data. Specifically, the data is decentralized across multiple local machines and it is expensive or infeasible to train a MoE model centrally on the full dataset. The goal is to develop a distributed approach to learn an accurate global MoE model by training models locally on each machine and then aggregating them. 

Proposed Solution:
The paper proposes an aggregation strategy based on optimal transport to obtain a global MoE model called the "reduction estimator". The key ideas are:

1) Train a local MoE model on each machine using its local dataset. This gives local conditional density estimates. 

2) View the weighted average of the local conditional densities as an "ambient" MoE model. This ambient model has too many components (number of local models x number of experts per model).

3) Use an optimal transport divergence to measure the similarity between this ambient model and candidate global models. Find the global model with minimum divergence. This is the reduction estimator.

4) Derive an Majorization-Minimization (MM) algorithm to efficiently compute the reduction estimator. This alternates between computing the optimal transport plan and updating the global model parameters.

5) Show that the reduction estimator is consistent, as long as the local estimators are consistent. Also, establish convergence guarantees for the MM algorithm.

The key novelty is the use of optimal transport for principled aggregation in MoE, along with the theoretical analysis. This addresses limitations of naive averaging approaches.


Main Contributions:

- First distributed learning algorithm for MoE models
- Optimal transport based aggregation strategy 
- Consistency result for reduction estimator
- Custom MM algorithm to compute transport and MoE parameters
- Strong empirical performance over baselines in experiments
- 10x speed up over centralized approach without loss of accuracy

The paper makes both theoretical and practical contributions for distributed learning of conditional density models like MoEs. The ideas could be extended to other mixture models and deep networks.
