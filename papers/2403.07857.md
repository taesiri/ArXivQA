# [Fairness Feedback Loops: Training on Synthetic Data Amplifies Bias](https://arxiv.org/abs/2403.07857)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the key points from the paper:

Problem:
- Model-induced distribution shifts (MIDS) occur when model outputs pollute new training data over generations of models, causing issues like model collapse in generative models or unfairness feedback loops in classifiers.
- MIDS cause models to encode mistakes, biases, and unfairness into the data ecosystem.
- Practitioners lack understanding of how MIDS function and their impacts on fairness and representation. 

Proposed Solution:
- The paper introduces a unified framework to track multiple co-occurring MIDS over generations of models. 
- They evaluate performance, fairness metrics, and representation of models in two settings: chains of classifiers with labels from prior models (performative prediction) and chains of generative models training on prior synthetic outputs (model collapse).
- They simulate algorithmic reparation (AR), an intentional MIDS aimed at promoting equity, through controlled sampling of training batches.

Key Contributions:
- Unified different MIDS into one framework and empirically evaluated their impacts, showing loss of accuracy, fairness, and representation over generations.
- Found that unawareness of MIDS leads to overstated accuracy and fairness. 
- Identified interactions between co-occurring MIDS - e.g. classifiers can adapt to input distribution changes from model collapse.
- Simulated AR interventions with stratified sampling to improve representation and fairness, mitigating negative MIDS effects.
- Highlighted the ability for models to cause both harm and good through intentional and unintentional impacts on the broader data ecosystem.

The paper makes an important contribution toward accountability and maintenance of model impacts over time through the introduced framework and AR perspective.


## Summarize the paper in one sentence.

 Here is a one sentence summary of the paper:

The paper introduces the concept of model-induced distribution shifts (MIDS) to unify phenomena where models change data ecosystems over generations, evaluates harms including performance declines and unfairness amplifications from MIDS, and demonstrates algorithmic reparation interventions to promote equity.


## What is the main contribution of this paper?

 Based on my understanding, the main contributions of this paper are:

1) It defines a new term "model-induced distribution shift" (MIDS) to unify several types of distribution shifts caused by machine learning models, such as fairness feedback loops, model collapse, and disparity amplification. This allows analyzing their common causes and impacts even when multiple MIDS occur together.

2) It empirically evaluates the impacts of MIDS like fairness feedback loops and model collapse over multiple generations of models on various datasets. It finds that MIDS can lead to poor performance, lack of representation of marginalized groups, and unfairness over time.

3) It proposes a framework called "algorithmic reparation" (AR) as an intentional intervention to provide redress for historical discrimination by using models. It simulates AR interventions through a stratified sampling algorithm and shows they can alleviate harms and unfairness caused by other MIDS.

4) Overall, it highlights the need for accountability regarding impacts of models on data ecosystems, especially regarding fairness and representation of marginalized groups. It advocates for more awareness of MIDS and intentional interventions like AR to promote equity.

In summary, the main contribution is defining MIDS, empirically demonstrating their negative impacts, and proposing algorithmic reparation as a potential mitigation.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and concepts include:

- Model-induced distribution shift (MIDS): Changes to the data distribution caused by models' outputs and behaviors over generations of models. Includes phenomena like fairness feedback loops, model collapse, and disparity amplification.

- Fairness feedback loops: When a model's predictions influence real-world outcomes which then get encoded as future training data, entrenching the model's biases. Also called performative prediction. 

- Model collapse: When generative models are trained on synthetic samples created by previous models, leading to loss of information and convergence to a restricted part of the original data distribution.

- Disparity amplification: When poor model performance for marginalized groups causes those groups to disengage, shifting the data distribution and worsening representational disparity.

- Algorithmic reparation (AR): Using models to provide redress for historical discrimination against marginalized groups. Seen as an intentional, equity-promoting form of distribution shift.

- Minoritized/majoritized groups: Groups that are disadvantaged/advantaged in society. Used instead of "minority/majority" since population size doesn't always correspond to degree of marginalization.

- Representational disparity: Imbalance in how different groups are represented in a dataset or modeling pipeline. Can compound historical discrimination.

The key focus is on how successive generations of models can induce unfair distribution shifts, and possible interventions to promote equity.


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 potential in-depth questions about the method proposed in this paper:

1) How does the proposed framework for modeling multiple model-induced distribution shifts (MIDS) help elucidate their common causes and enable analysis even when they co-occur? What are some of the key benefits of this unified perspective?

2) The paper proposes an algorithm called STratified AR (STAR) to simulate algorithmic reparation interventions. What are some ways this algorithm could be extended or improved to better address intersectional fairness and equity? How might the "reparation budget" parameter be adjusted based on context?

3) In the sequential classifier experiments, what accounts for the high variance in metrics like accuracy and demographic parity difference when reparation is not used? And why does the variance decrease substantially when using classifier-STAR?

4) The paper finds generator-STAR leads to better preservation of group/label balance than classifier-STAR in most datasets. What factors might explain this? Are there any cases or data contexts where classifier-STAR might outperform?

5) How suitable are the FML fairness metrics used in this paper for evaluating and comparing reparative interventions? What additional or alternative metrics aligned with algorithmic reparation goals could complement the analysis? 

6) What types of model architectures could help improve the representational capabilities of generators to better support algorithmic reparation interventions addressing disparities for undersampled intersectional groups?

7) In what ways could the stochastic gradient descent training procedure be adapted on a per-batch basis to better align with reparative principles when using STAR batch curation?

8) The paper simulates a "uniform" reparative ideal for STAR, but notes that this is likely inappropriate without careful context-specific analysis. What are some ways a more nuanced, equitable target distribution could be derived for reparative interventions?

9) What kinds of additional experiments could better evaluate the real-world efficacy of using algorithmic reparation to counteract model unfairness and data ecosystem discrimination? What are limitations of the empirical results?

10) How might the concepts explored in this paper inform responsible and ethical practices around publishing and consuming synthetic data to mitigate problematic impacts on training data ecosystems?
