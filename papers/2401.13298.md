# [Towards Explainable Harmful Meme Detection through Multimodal Debate   between Large Language Models](https://arxiv.org/abs/2401.13298)

## Write a very high-quality and detailed summary of the paper that describes the paper in a way that a human can fully understand. The summary should cover the problem, the proposed solution and highlight the main contributions of the paper.

 Here is a detailed summary of the paper:

Problem: 
The spread of harmful memes, consisting of images with overlaying text, is a growing issue online. Detecting such memes is challenging because their harmfulness arises from the implicit interplay between the text and image, which current models fail to properly explain.

Proposed Solution:
This paper proposes a novel approach called \textsc{ExplainHM} for explainable detection of harmful memes. The key insight is to conduct a "debate" between large language models (LLMs) to generate conflicting rationales explaining the meme's harmfulness or lack thereof from different viewpoints. These rationales provide reasoning knowledge when fine-tuning a smaller judge model to classify memes, enabling it to better capture nuanced harm-indicative patterns.

Specifically, two LLMs are prompted to give arguments supporting or opposing the meme's harmfulness. This facilitates dialectical thinking to uncover harmful implications overlooked by directly prompting an LLM alone. The rationales act as thought chains for a LLM judge to determine which one is more reasonable. For better reliability, a small LM judge is fine-tuned using the rationales to make the final prediction. Cross-attention mechanisms in this judge align the multimodal features between the meme and rationales for enhanced understanding. 

Main Contributions:
- Novel idea of harnessing LLM knowledge via conflicting rationales from a "debate" to empower models for nuanced reasoning on meme harmfulness
- Tunable small LM judge architecture that leverages LLM outputs as prior knowledge while avoiding expensive LLM fine-tuning  
- Superior performance over previous state-of-the-art methods on three public datasets
- Helpful natural language rationales explaining predictions to facilitate human evaluation

In summary, this work advances multimodal reasoning for meme analysis through an innovative debate-based paradigm using LLMs. By generating insightful explanations, it represents an important step towards interpretable AI systems that can justify their decisions.


## Summarize the paper in one sentence.

 Here is a one-sentence summary of the paper:

The paper proposes an explainable approach for harmful meme detection that conducts a multimodal debate between large language models to generate conflicting rationales and then trains a small language model judge to make harmfulness predictions based on fusing the meme content with the rationales.


## What is the main contribution of this paper?

 The main contributions of this paper are summarized in three folds:

1. It proposes a novel approach for explainable harmful meme detection by conducting a multimodal debate between large language models (LLMs) to generate conflicting rationales from harmless and harmful perspectives. This facilitates dialectical reasoning and complex understanding of implicit harm-indicative patterns in memes.

2. It designs a multimodal judge module with a small LM to perform harmfulness inference. By fusing the generated multimodal debate rationales with intrinsic multimodal information of memes, the model is empowered for deeper contextual reasoning.  

3. Extensive experiments conducted on three public meme datasets show that the proposed approach achieves superior performance over state-of-the-art methods for harmful meme detection. It also demonstrates a strong capacity to provide informative natural language explanations to unveil the reasoning behind predicted harms in memes.

In summary, the key contribution is an explainable framework for harmful meme detection, which elicits and harnesses knowledge from debates between LLMs to empower multimodal reasoning and harmfulness explanations. The effectiveness is validated through comprehensive experiments and analysis.


## What are the keywords or key terms associated with this paper?

 Based on my reading of the paper, some of the key terms and keywords associated with this paper include:

- Harmful meme detection - The main focus of the paper is on detecting harmful memes, which are defined as multimodal units consisting of an image and text that can potentially cause harm. 

- Explainability - A key goal of the paper is to provide explanations for why a meme is deemed harmful by the model, in order to support content moderation.

- Multimodal debate - The paper proposes conducting a debate between large language models from harmless and harmful perspectives to generate explanations.

- Large language models (LLMs) - The paper leverages the text generation capacities of LLMs to provide explanations through chain-of-thought prompting. Models like LLaVA and ChatGPT are used.

- Multimodal fusion - A small language model judge is fine-tuned to integrate the multimodal information in memes along with the rationales from the LLM debate.

- Dialectical reasoning - The paper aims to perform reasoning from conflicting viewpoints in order to better identify harmfulness indicators in memes.

- Commonsense knowledge - Understanding and explaining memes requires commonsense and cultural knowledge, which the paper aims to elicit from LLMs.

So in summary, the key terms cover harmful meme detection, explainability, multimodal debate, large language models, multimodal fusion, dialectical reasoning, and commonsense knowledge. Let me know if you need any clarification or have additional questions!


## Design 10 in-depth questions about the method proposed in this paper and avoid simple questions.

 Here are 10 in-depth questions about the method proposed in this paper:

1. The paper proposes conducting a "multimodal debate" between large language models (LLMs) to generate conflicting rationales explaining why a meme is harmless or harmful. What are the key advantages of using a debate format compared to directly prompting a single LLM? How does it help with bias and accuracy?

2. The paper fine-tunes a small LM as a "debate judge" to make the final harmfulness prediction. Why is a small LM used instead of the large debaters themselves? What challenges would fine-tuning the large LM debaters present?  

3. The paper feeds the rationales from the LLM debate as input to help train the small LM judge. How does providing these rationales help the model understand and learn? Does order matter when concatenating the rationales?

4. What multimodal fusion techniques does the paper use to align the features between the meme, text, and generated rationales? Why is cross-modal interaction important for this task? How are the image features incorporated?

5. The paper demonstrates superior performance over state-of-the-art baselines. What factors contribute most to this improvement? Is performance very dependent on model size? How does the approach perform with small, base, and large LM sizes?

6. The paper emphasizes explainability as a key advantage of the approach. How is explainability achieved? What metrics and evaluations are used to measure the quality of explanations? What are limitations?

7. What common types of errors does the approach make in detecting harmful memes? What are some underlying reasons and limitations? How can the approach be improved?

8. How well does the approach generalize across diverse meme datasets like Harm-C, Harm-P, and FHM? Where does it struggle? Why might this be the case?

9. The approach relies heavily on commonsense knowledge and reasoning from LLMs. What inherent limitations around bias, hallucination, and generalization exist? How can these be addressed?

10. What opportunities exist to extend this work? For example, evaluating on additional tasks and datasets? Using different LLMs as debaters or judges? Adding more turns to the debate? Exploring debate formats to minimize bias and improve generalization?
