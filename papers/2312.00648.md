# [SPOT: Self-Training with Patch-Order Permutation for Object-Centric   Learning with Autoregressive Transformers](https://arxiv.org/abs/2312.00648)

## Write a very high-quality paragraph summarizing the paper that effectively covers the paper.

 Here is a high-quality paragraph summarizing the key points of the paper:

This paper introduces SPOT, a novel framework to enhance unsupervised object-centric learning in slot-based autoencoders. SPOT makes two key technical contributions: (1) A self-training approach that distills superior slot-based attention masks from the decoder to the encoder, enhancing object segmentation captured in the slots. (2) An innovative patch-order permutation strategy for the autoregressive transformer decoder that amplifies the role of slot vectors during reconstruction. Experiments demonstrate the synergistic effects of these strategies. SPOT significantly outperforms prior slot-based autoencoder methods, especially on complex real-world images from the COCO dataset. The approach improves both the precision of the encoder in generating object-specific slots as well as the decoder's ability to leverage these slots for reconstruction. By advancing unsupervised object-centric learning on real-world data, SPOT takes an important step towards achieving human-like scene decomposition abilities using only visual cues.
