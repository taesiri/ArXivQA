{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1779,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import shelve\n",
    "import os\n",
    "import git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1780,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_paper_ids_from_repo(repo_url):\n",
    "    # Get the content of the repository's papers folder\n",
    "    response = requests.get(repo_url)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    # Use BeautifulSoup to parse the HTML content\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    # Extract all CSV filenames\n",
    "    csv_files = [\n",
    "        link.get(\"href\")\n",
    "        for link in soup.find_all(\"a\")\n",
    "        if link.get(\"href\", \"\").endswith(\".csv\")\n",
    "    ]\n",
    "\n",
    "    # Extract paper IDs from filenames\n",
    "    paper_ids = [filename.split(\"/\")[-1].replace(\".csv\", \"\") for filename in csv_files]\n",
    "\n",
    "    return paper_ids\n",
    "\n",
    "\n",
    "# Get the list of paper IDs from the repository\n",
    "REPO_URL = \"https://huggingface.co/datasets/taesiri/arxiv_qa\"\n",
    "paper_ids = get_paper_ids_from_repo(REPO_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def csv_to_markdown(paper_id, repo_path, cache, fetch_title_online=True):\n",
    "    # Read CSV from the cloned repository\n",
    "    try:\n",
    "        df = pd.read_csv(os.path.join(repo_path, \"papers\", f\"{paper_id}.csv\"))\n",
    "    except FileNotFoundError:\n",
    "        print(f\"No CSV found for paper ID: {paper_id}\")\n",
    "        return\n",
    "    \n",
    "    # Check if the paper title is in the cache\n",
    "    if paper_id in cache:\n",
    "        paper_title = cache[paper_id]\n",
    "    elif fetch_title_online:\n",
    "        # Fetch the paper title using the arXiv API\n",
    "        ARXIV_API_ENDPOINT = \"http://export.arxiv.org/api/query?id_list={}\"\n",
    "        try:\n",
    "            title_response = requests.get(ARXIV_API_ENDPOINT.format(paper_id))\n",
    "            title_response.raise_for_status()\n",
    "            xml_content = title_response.content.decode(\"utf-8\")\n",
    "            title_start = xml_content.find(\"<title>\") + 7\n",
    "            title_end = xml_content.find(\"</title>\", title_start)\n",
    "            paper_title = xml_content[title_start:title_end].strip()\n",
    "            paper_title = paper_title.replace(\"\\n\", \" \").replace(\"\\r\", \"\")\n",
    "        except requests.RequestException as e:\n",
    "            print(f\"Error fetching title for {paper_id}: {str(e)}\")\n",
    "            return\n",
    "        # Cache the paper title\n",
    "        cache[paper_id] = paper_title\n",
    "    else:\n",
    "        paper_title = paper_id  # or another default/fallback title\n",
    "    \n",
    "    # Convert DataFrame to markdown\n",
    "    markdown_lines = []\n",
    "    paper_title_link = f\"[{paper_title}](https://arxiv.org/abs/{paper_id})\"\n",
    "    markdown_lines.append(\"# \" + paper_title_link)\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        markdown_lines.append(\"\\n## \" + row[\"question\"])\n",
    "        markdown_lines.append(\"\\n\" + str(row[\"answer\"]) + \"\\n\")\n",
    "    \n",
    "    markdown_content = \"\\n\".join(markdown_lines)\n",
    "    \n",
    "    # Ensure the directory exists\n",
    "    os.makedirs(os.path.join(\"./papers\"), exist_ok=True)\n",
    "    \n",
    "    # Save to .md file named using the paper_id\n",
    "    with open(f\"./papers/{paper_id}.md\", \"w\") as md_file:\n",
    "        md_file.write(markdown_content)\n",
    "\n",
    "\n",
    "def get_paper_ids_from_repo(repo_path):\n",
    "    # Get the list of CSV files in the papers directory of the cloned repository\n",
    "    csv_files = [\n",
    "        filename\n",
    "        for filename in os.listdir(os.path.join(repo_path, \"papers\"))\n",
    "        if filename.endswith(\".csv\")\n",
    "    ]\n",
    "\n",
    "    # Extract paper IDs from filenames\n",
    "    paper_ids = [filename.replace(\".csv\", \"\") for filename in csv_files]\n",
    "\n",
    "    # Extract base IDs and ensure only one version of each paper is included\n",
    "    base_ids = {paper_id.split('v')[0] for paper_id in paper_ids}\n",
    "    unique_paper_ids = []\n",
    "    for base_id in base_ids:\n",
    "        versions = [pid for pid in paper_ids if pid.startswith(base_id)]\n",
    "        unique_paper_ids.append(sorted(versions)[0])  # Add the earliest version\n",
    "\n",
    "    return unique_paper_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6158/6158 [00:52<00:00, 118.36it/s]\n"
     ]
    }
   ],
   "source": [
    "# Clone the repository using GitPython or pull the latest changes if it exists\n",
    "REPO_URL = \"https://huggingface.co/datasets/taesiri/arxiv_qa.git\"\n",
    "REPO_PATH = \"./arxiv_qa_repo\"\n",
    "if not os.path.exists(REPO_PATH):\n",
    "    git.Repo.clone_from(REPO_URL, REPO_PATH)\n",
    "else:\n",
    "    repo = git.Repo(REPO_PATH)\n",
    "    origin = repo.remotes.origin\n",
    "    origin.pull()\n",
    "\n",
    "# Get the list of paper IDs from the cloned repository\n",
    "paper_ids = get_paper_ids_from_repo(REPO_PATH)\n",
    "\n",
    "# Open a shelve cache\n",
    "with shelve.open(\"arxiv_cache\") as cache:\n",
    "    # Convert each paper's CSV to markdown\n",
    "    for paper_id in tqdm(paper_ids):\n",
    "        csv_to_markdown(paper_id, REPO_PATH, cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_ids = list(set(paper_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_paper_details(paper_id, cache_file=\"paper_details_cache\"):\n",
    "    \"\"\"Retrieve the paper title and publication date from the arXiv API for a given paper_id.\"\"\"\n",
    "    with shelve.open(cache_file) as cache:\n",
    "        # Check if the result is already in the cache\n",
    "        if paper_id in cache:\n",
    "            return cache[paper_id]\n",
    "\n",
    "        ARXIV_API_ENDPOINT = \"http://export.arxiv.org/api/query?id_list={}\"\n",
    "        response = requests.get(ARXIV_API_ENDPOINT.format(paper_id))\n",
    "        response.raise_for_status()\n",
    "        xml_content = response.content.decode(\"utf-8\")\n",
    "\n",
    "        # Extract title\n",
    "        title_start = xml_content.find(\"<title>\") + 7\n",
    "        title_end = xml_content.find(\"</title>\", title_start)\n",
    "        title = xml_content[title_start:title_end].strip()\n",
    "\n",
    "        # Extract publication date\n",
    "        date_start = xml_content.find(\"<published>\") + 11\n",
    "        date_end = xml_content.find(\"</published>\", date_start)\n",
    "        date_str = xml_content[date_start:date_end].strip()\n",
    "        pub_date = datetime.strptime(date_str, \"%Y-%m-%dT%H:%M:%SZ\")\n",
    "\n",
    "        # Store the result in the cache\n",
    "        cache[paper_id] = (title, pub_date)\n",
    "\n",
    "    return title, pub_date\n",
    "\n",
    "\n",
    "def create_parent_md(paper_ids, output_file=\"./README.md\"):\n",
    "    \"\"\"Create a parent Markdown file listing all papers, sorted by publication date.\"\"\"\n",
    "    paper_details = []\n",
    "\n",
    "    # Fetch titles and dates for all papers\n",
    "    for paper_id in tqdm(paper_ids, desc=\"Fetching paper details\", ncols=100):\n",
    "        title, pub_date = get_paper_details(paper_id)\n",
    "        paper_details.append((paper_id, title, pub_date))\n",
    "\n",
    "    # Sort papers by publication date\n",
    "    paper_details.sort(key=lambda x: x[2], reverse=True)\n",
    "\n",
    "    # Generate Markdown content\n",
    "    lines = [\"# List of Papers\\n\"]\n",
    "    prev_year, prev_month = None, None\n",
    "\n",
    "    for paper_id, title, pub_date in paper_details:\n",
    "        year = pub_date.strftime(\"%Y\")\n",
    "        month_name = pub_date.strftime(\"%B\")\n",
    "\n",
    "        # Add Year header if the year changes\n",
    "        if year != prev_year:\n",
    "            lines.append(f\"\\n## {year}\\n\")\n",
    "            prev_year = year\n",
    "            prev_month = None  # Reset month whenever year changes\n",
    "\n",
    "        # Add Month header if the month changes\n",
    "        if month_name != prev_month:\n",
    "            lines.append(f\"\\n### {month_name} {year}\\n\")\n",
    "            prev_month = month_name\n",
    "\n",
    "        date_str = pub_date.strftime(\"%Y/%m\")\n",
    "        arxiv_link = f\"https://arxiv.org/abs/{paper_id}\"\n",
    "        md_link = f\"https://github.com/taesiri/ArXivQA/blob/main/papers/{paper_id}.md\"\n",
    "\n",
    "        title = title.replace(\"\\n\", \" \").replace(\"\\r\", \"\")\n",
    "        lines.append(f\"- {title} - [[ArXiv]({arxiv_link})] [[QA]({md_link})].\\n\")\n",
    "\n",
    "    with open(output_file, \"w\") as md_file:\n",
    "        md_file.writelines(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_parent_md(paper_ids, output_file=\"./README.md\"):\n",
    "    \"\"\"Create a parent Markdown file listing all papers, sorted by publication date.\"\"\"\n",
    "    paper_details = []\n",
    "\n",
    "    # Fetch titles and dates for all papers\n",
    "    for paper_id in tqdm(paper_ids, desc=\"Fetching paper details\", ncols=100):\n",
    "        title, pub_date = get_paper_details(paper_id)\n",
    "        paper_details.append((paper_id, title, pub_date))\n",
    "\n",
    "    # Sort papers by publication date\n",
    "    paper_details.sort(key=lambda x: x[2], reverse=True)\n",
    "\n",
    "    # Generate Markdown content\n",
    "    lines = [\"# List of Papers\\n\"]\n",
    "    prev_year, prev_month = None, None\n",
    "\n",
    "    for paper_id, title, pub_date in paper_details:\n",
    "        year = pub_date.strftime(\"%Y\")\n",
    "        month_name = pub_date.strftime(\"%B\")\n",
    "\n",
    "        # Add Year header and accordion structure if the year changes\n",
    "        if year != prev_year:\n",
    "            if prev_year is not None:\n",
    "                lines.append(\"</div></details>\")  # Close div for previous year\n",
    "            lines.extend(\n",
    "                [\n",
    "                    f\"\\n<details open>\",  # The \"open\" attribute makes the year visible by default\n",
    "                    f\"<summary><strong>{year}</strong></summary>\",\n",
    "                    f\"<div>\\n\",\n",
    "                ]\n",
    "            )\n",
    "            prev_year = year\n",
    "            prev_month = None  # Reset month whenever year changes\n",
    "\n",
    "        # Add Month header if the month changes\n",
    "        if month_name != prev_month:\n",
    "            lines.append(f\"\\n### {month_name} {year}\\n\")\n",
    "            prev_month = month_name\n",
    "\n",
    "        date_str = pub_date.strftime(\"%Y/%m\")\n",
    "        arxiv_link = f\"https://arxiv.org/abs/{paper_id}\"\n",
    "        md_link = f\"https://github.com/taesiri/ArXivQA/blob/main/papers/{paper_id}.md\"\n",
    "\n",
    "        # Use the paper_id as the link to the paper\n",
    "        title = title.replace(\"\\n\", \" \").replace(\"\\r\", \"\")\n",
    "        lines.append(f\"- {title} - [[{paper_id}]({arxiv_link})] [[QA]({md_link})].\\n\")\n",
    "\n",
    "    lines.append(\"</div></details>\")  # Close last year's div\n",
    "\n",
    "    with open(output_file, \"w\") as md_file:\n",
    "        md_file.writelines(lines)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching paper details: 100%|█████████████████████████████████| 6158/6158 [00:01<00:00, 4825.10it/s]\n"
     ]
    }
   ],
   "source": [
    "create_parent_md(paper_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6158"
      ]
     },
     "execution_count": 1771,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(paper_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"paper_ids.txt\", \"w\") as f:\n",
    "    for paper_id in paper_ids:\n",
    "        f.write(paper_id + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HF README"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_parent_md_hf(paper_ids, output_file=\"./HF/README.md\"):\n",
    "    \"\"\"Create a parent Markdown file listing all papers, sorted by publication date.\"\"\"\n",
    "    paper_details = []\n",
    "\n",
    "    # open header and add header to the beginning of the file\n",
    "    with open('./HF/HEADER.md', 'r') as file:\n",
    "        header = file.read()\n",
    "    \n",
    "    lines = [header] + [\"\\n# List of Papers\\n\"]\n",
    "\n",
    "    # Fetch titles and dates for all papers\n",
    "    for paper_id in tqdm(paper_ids, desc=\"Fetching paper details\", ncols=100):\n",
    "        title, pub_date = get_paper_details(paper_id)\n",
    "        paper_details.append((paper_id, title, pub_date))\n",
    "\n",
    "    # Sort papers by publication date\n",
    "    paper_details.sort(key=lambda x: x[2], reverse=True)\n",
    "\n",
    "    # Generate Markdown content\n",
    "    prev_year, prev_month = None, None\n",
    "\n",
    "    for paper_id, title, pub_date in paper_details:\n",
    "        year = pub_date.strftime(\"%Y\")\n",
    "        month_name = pub_date.strftime(\"%B\")\n",
    "\n",
    "        # Add Year header and accordion structure if the year changes\n",
    "        if year != prev_year:\n",
    "            if prev_year is not None:\n",
    "                lines.append(\"</div></details>\")  # Close div for previous year\n",
    "            lines.extend(\n",
    "                [\n",
    "                    f\"\\n<details open>\",  # The \"open\" attribute makes the year visible by default\n",
    "                    f\"<summary><strong>{year}</strong></summary>\",\n",
    "                    f\"<div>\\n\",\n",
    "                ]\n",
    "            )\n",
    "            prev_year = year\n",
    "            prev_month = None  # Reset month whenever year changes\n",
    "\n",
    "        # Add Month header if the month changes\n",
    "        if month_name != prev_month:\n",
    "            lines.append(f\"\\n### {month_name} {year}\\n\")\n",
    "            prev_month = month_name\n",
    "\n",
    "        date_str = pub_date.strftime(\"%Y/%m\")\n",
    "        arxiv_link = f\"https://arxiv.org/abs/{paper_id}\"\n",
    "        md_link = f\"https://github.com/taesiri/ArXivQA/blob/main/papers/{paper_id}.md\"\n",
    "\n",
    "        # Use the paper_id as the link to the paper\n",
    "        title = title.replace(\"\\n\", \" \").replace(\"\\r\", \"\")\n",
    "        lines.append(f\"- {title} - [[{paper_id}]({arxiv_link})] [[QA]({md_link})].\\n\")\n",
    "\n",
    "    lines.append(\"</div></details>\")  # Close last year's div\n",
    "\n",
    "    with open(output_file, \"w\") as md_file:\n",
    "        md_file.writelines(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching paper details: 100%|█████████████████████████████████| 6158/6158 [00:01<00:00, 4886.98it/s]\n"
     ]
    }
   ],
   "source": [
    "create_parent_md_hf(paper_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 files contain the phrase 'bad phrases'\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "# Get a list of all markdown files in the directory\n",
    "md_files = glob.glob(\"./papers/*.md\")\n",
    "bad_files = []\n",
    "# Loop through each file and check if the phrase is present\n",
    "count = 0\n",
    "for file in md_files:\n",
    "    with open(file, \"r\") as f:\n",
    "        content = f.read()\n",
    "        if \"without having access to the full paper\" in content.lower():\n",
    "            bad_files.append(file)\n",
    "            count += 1\n",
    "        elif \"access to the full paper\" in content.lower():\n",
    "            bad_files.append(file)\n",
    "            count += 1\n",
    "        elif \"without access to the full paper\" in content.lower():\n",
    "            bad_files.append(file)\n",
    "            count += 1\n",
    "\n",
    "print(f\"{count} files contain the phrase 'bad phrases'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./papers/2107.13586.md',\n",
       " './papers/2306.04031.md',\n",
       " './papers/2211.17257.md',\n",
       " './papers/2307.01848.md',\n",
       " './papers/2205.11916.md',\n",
       " './papers/2309.08637.md',\n",
       " './papers/2305.10855.md',\n",
       " './papers/2304.10970.md']"
      ]
     },
     "execution_count": 1776,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bad_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# # Define the path to the papers folder\n",
    "# papers_folder = \"/Github/arxiv_qa/papers\"\n",
    "\n",
    "# # Loop through each file in bad_files and delete it\n",
    "# for file in bad_files:\n",
    "#     file = file.replace(\"./papers/\", \"\")\n",
    "#     file = file.replace(\".md\", \".csv\")\n",
    "\n",
    "#     file_path = os.path.join(papers_folder, file)\n",
    "#     print(f\"Deleting {file_path}\")\n",
    "#     os.remove(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# # Define the path to the papers folder\n",
    "\n",
    "# # Loop through each file in bad_files and delete it\n",
    "# for file in bad_files:\n",
    "#     file_path = os.path.join(file)\n",
    "#     print(f\"Deleting {file_path}\")\n",
    "#     os.remove(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
