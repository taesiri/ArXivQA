{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import shelve\n",
    "\n",
    "\n",
    "def csv_to_markdown(paper_id):\n",
    "    # Fetch CSV from Hugging Face website\n",
    "    CSV_URL = f\"https://huggingface.co/datasets/taesiri/arxiv_qa/raw/main/papers/{paper_id}.csv\"\n",
    "    csv_response = requests.get(CSV_URL)\n",
    "    csv_response.raise_for_status()\n",
    "    csv_content = csv_response.text\n",
    "\n",
    "    # Fetch the paper title using the arXiv API\n",
    "    ARXIV_API_ENDPOINT = \"http://export.arxiv.org/api/query?id_list={}\"\n",
    "    title_response = requests.get(ARXIV_API_ENDPOINT.format(paper_id))\n",
    "    title_response.raise_for_status()\n",
    "    xml_content = title_response.content.decode(\"utf-8\")\n",
    "    title_start = xml_content.find(\"<title>\") + 7\n",
    "    title_end = xml_content.find(\"</title>\", title_start)\n",
    "    paper_title = xml_content[title_start:title_end].strip()\n",
    "    paper_title = paper_title.replace(\"\\n\", \" \").replace(\"\\r\", \"\")\n",
    "\n",
    "    # Convert CSV to markdown\n",
    "    markdown_lines = []\n",
    "    markdown_lines.append(\"# \" + paper_title)\n",
    "\n",
    "    csv_reader = csv.DictReader(csv_content.splitlines())\n",
    "    for row in csv_reader:\n",
    "        markdown_lines.append(\"\\n## \" + row[\"question\"])\n",
    "        markdown_lines.append(\"\\n\" + row[\"answer\"].strip() + \"\\n\")\n",
    "\n",
    "    markdown_content = \"\\n\".join(markdown_lines)\n",
    "\n",
    "    # Save to .md file named using the paper_id\n",
    "    with open(f\"./papers/{paper_id}.md\", \"w\") as md_file:\n",
    "        md_file.write(markdown_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_paper_ids_from_repo(repo_url):\n",
    "    # Get the content of the repository's papers folder\n",
    "    response = requests.get(repo_url)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    # Use BeautifulSoup to parse the HTML content\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    # Extract all CSV filenames\n",
    "    csv_files = [\n",
    "        link.get(\"href\")\n",
    "        for link in soup.find_all(\"a\")\n",
    "        if link.get(\"href\", \"\").endswith(\".csv\")\n",
    "    ]\n",
    "\n",
    "    # Extract paper IDs from filenames\n",
    "    paper_ids = [filename.split(\"/\")[-1].replace(\".csv\", \"\") for filename in csv_files]\n",
    "\n",
    "    return paper_ids\n",
    "\n",
    "\n",
    "# Get the list of paper IDs from the repository\n",
    "REPO_URL = \"https://huggingface.co/datasets/taesiri/arxiv_qa/tree/main/papers\"\n",
    "paper_ids = get_paper_ids_from_repo(REPO_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import shelve\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import git\n",
    "import requests\n",
    "import shelve\n",
    "\n",
    "\n",
    "def csv_to_markdown(paper_id, repo_path, cache):\n",
    "    # Read CSV from the cloned repository\n",
    "    with open(os.path.join(repo_path, \"papers\", f\"{paper_id}.csv\"), \"r\") as file:\n",
    "        csv_content = file.read()\n",
    "\n",
    "    # Check if the paper title is in the cache\n",
    "    if paper_id in cache:\n",
    "        paper_title = cache[paper_id]\n",
    "    else:\n",
    "        # Fetch the paper title using the arXiv API\n",
    "        ARXIV_API_ENDPOINT = \"http://export.arxiv.org/api/query?id_list={}\"\n",
    "        title_response = requests.get(ARXIV_API_ENDPOINT.format(paper_id))\n",
    "        title_response.raise_for_status()\n",
    "        xml_content = title_response.content.decode(\"utf-8\")\n",
    "        title_start = xml_content.find(\"<title>\") + 7\n",
    "        title_end = xml_content.find(\"</title>\", title_start)\n",
    "        paper_title = xml_content[title_start:title_end].strip()\n",
    "        paper_title = paper_title.replace(\"\\n\", \" \").replace(\"\\r\", \"\")\n",
    "\n",
    "        # Cache the paper title\n",
    "        cache[paper_id] = paper_title\n",
    "\n",
    "    # Convert CSV to markdown\n",
    "    markdown_lines = []\n",
    "    markdown_lines.append(\"# \" + paper_title)\n",
    "\n",
    "    csv_reader = csv.DictReader(csv_content.splitlines())\n",
    "    for row in csv_reader:\n",
    "        markdown_lines.append(\"\\n## \" + row[\"question\"])\n",
    "        markdown_lines.append(\"\\n\" + row[\"answer\"].strip() + \"\\n\")\n",
    "\n",
    "    markdown_content = \"\\n\".join(markdown_lines)\n",
    "\n",
    "    # Save to .md file named using the paper_id\n",
    "    with open(f\"./papers/{paper_id}.md\", \"w\") as md_file:\n",
    "        md_file.write(markdown_content)\n",
    "\n",
    "\n",
    "def get_paper_ids_from_repo(repo_path):\n",
    "    # Get the list of CSV files in the papers directory of the cloned repository\n",
    "    csv_files = [\n",
    "        filename\n",
    "        for filename in os.listdir(os.path.join(repo_path, \"papers\"))\n",
    "        if filename.endswith(\".csv\")\n",
    "    ]\n",
    "\n",
    "    # Extract paper IDs from filenames\n",
    "    paper_ids = [filename.replace(\".csv\", \"\") for filename in csv_files]\n",
    "\n",
    "    return paper_ids\n",
    "\n",
    "\n",
    "# # Clone the repository using GitPython\n",
    "# REPO_URL = \"https://huggingface.co/datasets/taesiri/arxiv_qa.git\"\n",
    "# REPO_PATH = \"./arxiv_qa_repo\"\n",
    "# if not os.path.exists(REPO_PATH):\n",
    "#     git.Repo.clone_from(REPO_URL, REPO_PATH)\n",
    "\n",
    "# # Get the list of paper IDs from the cloned repository\n",
    "# paper_ids = get_paper_ids_from_repo(REPO_PATH)\n",
    "\n",
    "# # Open a shelve cache\n",
    "# with shelve.open(\"arxiv_cache\") as cache:\n",
    "#     # Convert each paper's CSV to markdown\n",
    "#     for paper_id in tqdm(paper_ids):\n",
    "#         csv_to_markdown(paper_id, REPO_PATH, cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 648/648 [00:00<00:00, 2249.74it/s]\n"
     ]
    }
   ],
   "source": [
    "# Clone the repository using GitPython or pull the latest changes if it exists\n",
    "REPO_URL = \"https://huggingface.co/datasets/taesiri/arxiv_qa.git\"\n",
    "REPO_PATH = \"./arxiv_qa_repo\"\n",
    "if not os.path.exists(REPO_PATH):\n",
    "    git.Repo.clone_from(REPO_URL, REPO_PATH)\n",
    "else:\n",
    "    repo = git.Repo(REPO_PATH)\n",
    "    origin = repo.remotes.origin\n",
    "    origin.pull()\n",
    "\n",
    "# Get the list of paper IDs from the cloned repository\n",
    "paper_ids = get_paper_ids_from_repo(REPO_PATH)\n",
    "\n",
    "# Open a shelve cache\n",
    "with shelve.open(\"arxiv_cache\") as cache:\n",
    "    # Convert each paper's CSV to markdown\n",
    "    for paper_id in tqdm(paper_ids):\n",
    "        csv_to_markdown(paper_id, REPO_PATH, cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# from git import Repo\n",
    "\n",
    "\n",
    "# def get_paper_ids_from_repo(repo_url, local_dir=\"./temp_repo\"):\n",
    "#     # Clone the repository\n",
    "#     Repo.clone_from(repo_url, local_dir)\n",
    "#     repo = Repo(local_dir)\n",
    "\n",
    "#     # Get list of CSV filenames in the 'papers' directory\n",
    "#     csv_files = [\n",
    "#         item.path\n",
    "#         for item in repo.tree().traverse()\n",
    "#         if item.path.endswith(\".csv\") and item.path.startswith(\"papers/\")\n",
    "#     ]\n",
    "\n",
    "#     # Extract paper IDs from filenames\n",
    "#     paper_ids = [\n",
    "#         os.path.basename(filename).replace(\".csv\", \"\") for filename in csv_files\n",
    "#     ]\n",
    "\n",
    "#     # Cleanup: Remove the cloned repository from local storage\n",
    "#     for root, dirs, files in os.walk(local_dir, topdown=False):\n",
    "#         for name in files:\n",
    "#             os.remove(os.path.join(root, name))\n",
    "#         for name in dirs:\n",
    "#             os.rmdir(os.path.join(root, name))\n",
    "#     os.rmdir(local_dir)\n",
    "\n",
    "#     return paper_ids\n",
    "\n",
    "\n",
    "# # Get the list of paper IDs from the repository\n",
    "# REPO_URL = \"https://huggingface.co/datasets/taesiri/arxiv_qa\"\n",
    "# paper_ids = get_paper_ids_from_repo(REPO_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(paper_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Convert each CSV to Markdown\n",
    "# for paper_id in tqdm(paper_ids):\n",
    "#     csv_to_markdown(paper_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_paper_title(paper_id):\n",
    "#     \"\"\"Retrieve the paper title from the arXiv API for a given paper_id.\"\"\"\n",
    "#     ARXIV_API_ENDPOINT = \"http://export.arxiv.org/api/query?id_list={}\"\n",
    "#     response = requests.get(ARXIV_API_ENDPOINT.format(paper_id))\n",
    "#     response.raise_for_status()\n",
    "#     xml_content = response.content.decode(\"utf-8\")\n",
    "#     title_start = xml_content.find(\"<title>\") + 7\n",
    "#     title_end = xml_content.find(\"</title>\", title_start)\n",
    "#     return xml_content[title_start:title_end].strip()\n",
    "\n",
    "\n",
    "# def create_parent_md(paper_ids, output_file=\"./papers.md\"):\n",
    "#     \"\"\"Create a parent Markdown file listing all papers.\"\"\"\n",
    "#     lines = [\"# List of Papers\\n\"]\n",
    "#     for paper_id in paper_ids:\n",
    "#         title = get_paper_title(paper_id)\n",
    "#         arxiv_link = f\"https://arxiv.org/abs/{paper_id}\"\n",
    "#         md_link = f\"./papers/{paper_id}.md\"\n",
    "#         lines.append(f\"- {title} - [[Arxiv]({arxiv_link})] [[QA]({md_link})].\\n\")\n",
    "\n",
    "#     with open(output_file, \"w\") as md_file:\n",
    "#         md_file.writelines(lines)\n",
    "\n",
    "\n",
    "# # Assuming paper_ids is the list of paper IDs you've fetched before\n",
    "# # create_parent_md(paper_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from glob import glob\n",
    "# paper_ids = [x.split(\"/\")[-1].split(\".md\")[0] for x in glob(\"./papers/*.md\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_ids = list(set(paper_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_paper_details(paper_id, cache_file=\"paper_details_cache\"):\n",
    "    \"\"\"Retrieve the paper title and publication date from the arXiv API for a given paper_id.\"\"\"\n",
    "    with shelve.open(cache_file) as cache:\n",
    "        # Check if the result is already in the cache\n",
    "        if paper_id in cache:\n",
    "            return cache[paper_id]\n",
    "\n",
    "        ARXIV_API_ENDPOINT = \"http://export.arxiv.org/api/query?id_list={}\"\n",
    "        response = requests.get(ARXIV_API_ENDPOINT.format(paper_id))\n",
    "        response.raise_for_status()\n",
    "        xml_content = response.content.decode(\"utf-8\")\n",
    "\n",
    "        # Extract title\n",
    "        title_start = xml_content.find(\"<title>\") + 7\n",
    "        title_end = xml_content.find(\"</title>\", title_start)\n",
    "        title = xml_content[title_start:title_end].strip()\n",
    "\n",
    "        # Extract publication date\n",
    "        date_start = xml_content.find(\"<published>\") + 11\n",
    "        date_end = xml_content.find(\"</published>\", date_start)\n",
    "        date_str = xml_content[date_start:date_end].strip()\n",
    "        pub_date = datetime.strptime(date_str, \"%Y-%m-%dT%H:%M:%SZ\")\n",
    "\n",
    "        # Store the result in the cache\n",
    "        cache[paper_id] = (title, pub_date)\n",
    "\n",
    "    return title, pub_date\n",
    "\n",
    "\n",
    "def create_parent_md(paper_ids, output_file=\"./README.md\"):\n",
    "    \"\"\"Create a parent Markdown file listing all papers, sorted by publication date.\"\"\"\n",
    "    paper_details = []\n",
    "\n",
    "    # Fetch titles and dates for all papers\n",
    "    for paper_id in tqdm(paper_ids, desc=\"Fetching paper details\", ncols=100):\n",
    "        title, pub_date = get_paper_details(paper_id)\n",
    "        paper_details.append((paper_id, title, pub_date))\n",
    "\n",
    "    # Sort papers by publication date\n",
    "    paper_details.sort(key=lambda x: x[2], reverse=True)\n",
    "\n",
    "    # Generate Markdown content\n",
    "    lines = [\"# List of Papers\\n\"]\n",
    "    prev_year, prev_month = None, None\n",
    "\n",
    "    for paper_id, title, pub_date in paper_details:\n",
    "        year = pub_date.strftime(\"%Y\")\n",
    "        month_name = pub_date.strftime(\"%B\")\n",
    "\n",
    "        # Add Year header if the year changes\n",
    "        if year != prev_year:\n",
    "            lines.append(f\"\\n## {year}\\n\")\n",
    "            prev_year = year\n",
    "            prev_month = None  # Reset month whenever year changes\n",
    "\n",
    "        # Add Month header if the month changes\n",
    "        if month_name != prev_month:\n",
    "            lines.append(f\"\\n### {month_name} {year}\\n\")\n",
    "            prev_month = month_name\n",
    "\n",
    "        date_str = pub_date.strftime(\"%Y/%m\")\n",
    "        arxiv_link = f\"https://arxiv.org/abs/{paper_id}\"\n",
    "        md_link = f\"./papers/{paper_id}.md\"\n",
    "\n",
    "        title = title.replace(\"\\n\", \" \").replace(\"\\r\", \"\")\n",
    "        lines.append(f\"- {title} - [[ArXiv]({arxiv_link})] [[QA]({md_link})].\\n\")\n",
    "\n",
    "    with open(output_file, \"w\") as md_file:\n",
    "        md_file.writelines(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching paper details: 100%|██████████████████████████████████| 648/648 [00:00<00:00, 20639.63it/s]\n"
     ]
    }
   ],
   "source": [
    "create_parent_md(paper_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "648"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(paper_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
