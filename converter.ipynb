{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import shelve\n",
    "import os\n",
    "import git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_paper_ids_from_repo(repo_url):\n",
    "    # Get the content of the repository's papers folder\n",
    "    response = requests.get(repo_url)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    # Use BeautifulSoup to parse the HTML content\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    # Extract all CSV filenames\n",
    "    csv_files = [\n",
    "        link.get(\"href\")\n",
    "        for link in soup.find_all(\"a\")\n",
    "        if link.get(\"href\", \"\").endswith(\".csv\")\n",
    "    ]\n",
    "\n",
    "    # Extract paper IDs from filenames\n",
    "    paper_ids = [filename.split(\"/\")[-1].replace(\".csv\", \"\") for filename in csv_files]\n",
    "\n",
    "    return paper_ids\n",
    "\n",
    "\n",
    "# Get the list of paper IDs from the repository\n",
    "REPO_URL = \"https://huggingface.co/datasets/taesiri/arxiv_qa\"\n",
    "paper_ids = get_paper_ids_from_repo(REPO_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def csv_to_markdown(paper_id, repo_path, cache, fetch_title_online=True):\n",
    "    # Read CSV from the cloned repository\n",
    "    try:\n",
    "        df = pd.read_csv(os.path.join(repo_path, \"papers\", f\"{paper_id}.csv\"))\n",
    "    except FileNotFoundError:\n",
    "        print(f\"No CSV found for paper ID: {paper_id}\")\n",
    "        return\n",
    "    \n",
    "    # Check if the paper title is in the cache\n",
    "    if paper_id in cache:\n",
    "        paper_title = cache[paper_id]\n",
    "    elif fetch_title_online:\n",
    "        # Fetch the paper title using the arXiv API\n",
    "        ARXIV_API_ENDPOINT = \"http://export.arxiv.org/api/query?id_list={}\"\n",
    "        try:\n",
    "            title_response = requests.get(ARXIV_API_ENDPOINT.format(paper_id))\n",
    "            title_response.raise_for_status()\n",
    "            xml_content = title_response.content.decode(\"utf-8\")\n",
    "            title_start = xml_content.find(\"<title>\") + 7\n",
    "            title_end = xml_content.find(\"</title>\", title_start)\n",
    "            paper_title = xml_content[title_start:title_end].strip()\n",
    "            paper_title = paper_title.replace(\"\\n\", \" \").replace(\"\\r\", \"\")\n",
    "        except requests.RequestException as e:\n",
    "            print(f\"Error fetching title for {paper_id}: {str(e)}\")\n",
    "            return\n",
    "        # Cache the paper title\n",
    "        cache[paper_id] = paper_title\n",
    "    else:\n",
    "        paper_title = paper_id  # or another default/fallback title\n",
    "    \n",
    "    # Convert DataFrame to markdown\n",
    "    markdown_lines = []\n",
    "    paper_title_link = f\"[{paper_title}](https://arxiv.org/abs/{paper_id})\"\n",
    "    markdown_lines.append(\"# \" + paper_title_link)\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        markdown_lines.append(\"\\n## \" + row[\"question\"])\n",
    "        markdown_lines.append(\"\\n\" + str(row[\"answer\"]) + \"\\n\")\n",
    "    \n",
    "    markdown_content = \"\\n\".join(markdown_lines)\n",
    "    \n",
    "    # Ensure the directory exists\n",
    "    os.makedirs(os.path.join(\"./papers\"), exist_ok=True)\n",
    "    \n",
    "    # Save to .md file named using the paper_id\n",
    "    with open(f\"./papers/{paper_id}.md\", \"w\") as md_file:\n",
    "        md_file.write(markdown_content)\n",
    "\n",
    "\n",
    "def get_paper_ids_from_repo(repo_path):\n",
    "    # Get the list of CSV files in the papers directory of the cloned repository\n",
    "    csv_files = [\n",
    "        filename\n",
    "        for filename in os.listdir(os.path.join(repo_path, \"papers\"))\n",
    "        if filename.endswith(\".csv\")\n",
    "    ]\n",
    "\n",
    "    # Extract paper IDs from filenames\n",
    "    paper_ids = [filename.replace(\".csv\", \"\") for filename in csv_files]\n",
    "\n",
    "    # Extract base IDs and ensure only one version of each paper is included\n",
    "    base_ids = {paper_id.split('v')[0] for paper_id in paper_ids}\n",
    "    unique_paper_ids = []\n",
    "    for base_id in base_ids:\n",
    "        versions = [pid for pid in paper_ids if pid.startswith(base_id)]\n",
    "        unique_paper_ids.append(sorted(versions)[0])  # Add the earliest version\n",
    "\n",
    "    return unique_paper_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 935/6019 [00:11<03:12, 26.41it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching title for 1910.11480: ('Connection aborted.', ConnectionResetError(54, 'Connection reset by peer'))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 1147/6019 [00:16<02:55, 27.70it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching title for 2212.12645: ('Connection aborted.', ConnectionResetError(54, 'Connection reset by peer'))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 1303/6019 [00:21<03:52, 20.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching title for 2303.02375: ('Connection aborted.', ConnectionResetError(54, 'Connection reset by peer'))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 1346/6019 [00:25<04:59, 15.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching title for 2303.07622: ('Connection aborted.', ConnectionResetError(54, 'Connection reset by peer'))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 1452/6019 [00:30<03:05, 24.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching title for 2111.13196: ('Connection aborted.', ConnectionResetError(54, 'Connection reset by peer'))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 1635/6019 [00:35<02:35, 28.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching title for 2303.15083: ('Connection aborted.', ConnectionResetError(54, 'Connection reset by peer'))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 2033/6019 [00:41<02:18, 28.75it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching title for 2303.14435: ('Connection aborted.', ConnectionResetError(54, 'Connection reset by peer'))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 2267/6019 [00:46<02:11, 28.62it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching title for 2202.08335: ('Connection aborted.', ConnectionResetError(54, 'Connection reset by peer'))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 2519/6019 [00:51<01:52, 31.17it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching title for 2211.07273: ('Connection aborted.', ConnectionResetError(54, 'Connection reset by peer'))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 2749/6019 [00:56<01:59, 27.44it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching title for 2005.00558: ('Connection aborted.', ConnectionResetError(54, 'Connection reset by peer'))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▋     | 2787/6019 [01:00<03:19, 16.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching title for 2211.14086: ('Connection aborted.', ConnectionResetError(54, 'Connection reset by peer'))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 2889/6019 [01:05<02:35, 20.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching title for 2304.02950: ('Connection aborted.', ConnectionResetError(54, 'Connection reset by peer'))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 3410/6019 [01:11<01:18, 33.39it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching title for 2307.13702: ('Connection aborted.', ConnectionResetError(54, 'Connection reset by peer'))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 3704/6019 [01:16<01:12, 31.93it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching title for 2303.11366: ('Connection aborted.', ConnectionResetError(54, 'Connection reset by peer'))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 3801/6019 [01:21<01:28, 25.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching title for 2304.01663: ('Connection aborted.', ConnectionResetError(54, 'Connection reset by peer'))\n",
      "Error fetching title for 2212.12249: ('Connection aborted.', ConnectionResetError(54, 'Connection reset by peer'))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 3855/6019 [01:29<02:56, 12.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching title for 2304.03283: ('Connection aborted.', ConnectionResetError(54, 'Connection reset by peer'))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 3941/6019 [01:34<02:10, 15.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching title for 2003.01964: ('Connection aborted.', ConnectionResetError(54, 'Connection reset by peer'))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 4216/6019 [01:39<01:00, 29.66it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching title for 2303.11681: ('Connection aborted.', ConnectionResetError(54, 'Connection reset by peer'))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 4358/6019 [01:44<01:08, 24.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching title for 2111.07832: ('Connection aborted.', ConnectionResetError(54, 'Connection reset by peer'))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 4571/6019 [01:49<00:52, 27.63it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching title for 2309.01523: ('Connection aborted.', ConnectionResetError(54, 'Connection reset by peer'))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 4780/6019 [01:54<00:43, 28.51it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching title for 2309.11009: ('Connection aborted.', ConnectionResetError(54, 'Connection reset by peer'))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 4834/6019 [01:58<00:59, 19.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching title for 2310.09118: ('Connection aborted.', ConnectionResetError(54, 'Connection reset by peer'))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 4958/6019 [02:03<00:45, 23.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching title for 2307.03109: ('Connection aborted.', ConnectionResetError(54, 'Connection reset by peer'))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 4981/6019 [02:07<01:21, 12.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching title for 2212.09072: ('Connection aborted.', ConnectionResetError(54, 'Connection reset by peer'))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▎ | 5026/6019 [02:12<01:20, 12.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching title for 2210.06284: ('Connection aborted.', ConnectionResetError(54, 'Connection reset by peer'))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 5067/6019 [02:16<01:18, 12.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching title for 2308.12351: ('Connection aborted.', ConnectionResetError(54, 'Connection reset by peer'))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 5166/6019 [02:20<00:48, 17.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching title for 2010.07492: ('Connection aborted.', ConnectionResetError(54, 'Connection reset by peer'))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 5255/6019 [02:25<00:43, 17.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching title for 2308.01263: ('Connection aborted.', ConnectionResetError(54, 'Connection reset by peer'))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 5355/6019 [02:30<00:35, 18.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching title for 2309.11081: ('Connection aborted.', ConnectionResetError(54, 'Connection reset by peer'))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 5626/6019 [02:35<00:14, 26.55it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching title for 2108.05540: ('Connection aborted.', ConnectionResetError(54, 'Connection reset by peer'))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 5915/6019 [02:41<00:03, 28.95it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching title for 1902.11038: ('Connection aborted.', ConnectionResetError(54, 'Connection reset by peer'))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6019/6019 [02:45<00:00, 36.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching title for 2303.10475: ('Connection aborted.', ConnectionResetError(54, 'Connection reset by peer'))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Clone the repository using GitPython or pull the latest changes if it exists\n",
    "REPO_URL = \"https://huggingface.co/datasets/taesiri/arxiv_qa.git\"\n",
    "REPO_PATH = \"./arxiv_qa_repo\"\n",
    "if not os.path.exists(REPO_PATH):\n",
    "    git.Repo.clone_from(REPO_URL, REPO_PATH)\n",
    "else:\n",
    "    repo = git.Repo(REPO_PATH)\n",
    "    origin = repo.remotes.origin\n",
    "    origin.pull()\n",
    "\n",
    "# Get the list of paper IDs from the cloned repository\n",
    "paper_ids = get_paper_ids_from_repo(REPO_PATH)\n",
    "\n",
    "# Open a shelve cache\n",
    "with shelve.open(\"arxiv_cache\") as cache:\n",
    "    # Convert each paper's CSV to markdown\n",
    "    for paper_id in tqdm(paper_ids):\n",
    "        csv_to_markdown(paper_id, REPO_PATH, cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_ids = list(set(paper_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_paper_details(paper_id, cache_file=\"paper_details_cache\"):\n",
    "    \"\"\"Retrieve the paper title and publication date from the arXiv API for a given paper_id.\"\"\"\n",
    "    with shelve.open(cache_file) as cache:\n",
    "        # Check if the result is already in the cache\n",
    "        if paper_id in cache:\n",
    "            return cache[paper_id]\n",
    "\n",
    "        ARXIV_API_ENDPOINT = \"http://export.arxiv.org/api/query?id_list={}\"\n",
    "        response = requests.get(ARXIV_API_ENDPOINT.format(paper_id))\n",
    "        response.raise_for_status()\n",
    "        xml_content = response.content.decode(\"utf-8\")\n",
    "\n",
    "        # Extract title\n",
    "        title_start = xml_content.find(\"<title>\") + 7\n",
    "        title_end = xml_content.find(\"</title>\", title_start)\n",
    "        title = xml_content[title_start:title_end].strip()\n",
    "\n",
    "        # Extract publication date\n",
    "        date_start = xml_content.find(\"<published>\") + 11\n",
    "        date_end = xml_content.find(\"</published>\", date_start)\n",
    "        date_str = xml_content[date_start:date_end].strip()\n",
    "        pub_date = datetime.strptime(date_str, \"%Y-%m-%dT%H:%M:%SZ\")\n",
    "\n",
    "        # Store the result in the cache\n",
    "        cache[paper_id] = (title, pub_date)\n",
    "\n",
    "    return title, pub_date\n",
    "\n",
    "\n",
    "def create_parent_md(paper_ids, output_file=\"./README.md\"):\n",
    "    \"\"\"Create a parent Markdown file listing all papers, sorted by publication date.\"\"\"\n",
    "    paper_details = []\n",
    "\n",
    "    # Fetch titles and dates for all papers\n",
    "    for paper_id in tqdm(paper_ids, desc=\"Fetching paper details\", ncols=100):\n",
    "        title, pub_date = get_paper_details(paper_id)\n",
    "        paper_details.append((paper_id, title, pub_date))\n",
    "\n",
    "    # Sort papers by publication date\n",
    "    paper_details.sort(key=lambda x: x[2], reverse=True)\n",
    "\n",
    "    # Generate Markdown content\n",
    "    lines = [\"# List of Papers\\n\"]\n",
    "    prev_year, prev_month = None, None\n",
    "\n",
    "    for paper_id, title, pub_date in paper_details:\n",
    "        year = pub_date.strftime(\"%Y\")\n",
    "        month_name = pub_date.strftime(\"%B\")\n",
    "\n",
    "        # Add Year header if the year changes\n",
    "        if year != prev_year:\n",
    "            lines.append(f\"\\n## {year}\\n\")\n",
    "            prev_year = year\n",
    "            prev_month = None  # Reset month whenever year changes\n",
    "\n",
    "        # Add Month header if the month changes\n",
    "        if month_name != prev_month:\n",
    "            lines.append(f\"\\n### {month_name} {year}\\n\")\n",
    "            prev_month = month_name\n",
    "\n",
    "        date_str = pub_date.strftime(\"%Y/%m\")\n",
    "        arxiv_link = f\"https://arxiv.org/abs/{paper_id}\"\n",
    "        md_link = f\"https://github.com/taesiri/ArXivQA/blob/main/papers/{paper_id}.md\"\n",
    "\n",
    "        title = title.replace(\"\\n\", \" \").replace(\"\\r\", \"\")\n",
    "        lines.append(f\"- {title} - [[ArXiv]({arxiv_link})] [[QA]({md_link})].\\n\")\n",
    "\n",
    "    with open(output_file, \"w\") as md_file:\n",
    "        md_file.writelines(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_parent_md(paper_ids, output_file=\"./README.md\"):\n",
    "    \"\"\"Create a parent Markdown file listing all papers, sorted by publication date.\"\"\"\n",
    "    paper_details = []\n",
    "\n",
    "    # Fetch titles and dates for all papers\n",
    "    for paper_id in tqdm(paper_ids, desc=\"Fetching paper details\", ncols=100):\n",
    "        title, pub_date = get_paper_details(paper_id)\n",
    "        paper_details.append((paper_id, title, pub_date))\n",
    "\n",
    "    # Sort papers by publication date\n",
    "    paper_details.sort(key=lambda x: x[2], reverse=True)\n",
    "\n",
    "    # Generate Markdown content\n",
    "    lines = [\"# List of Papers\\n\"]\n",
    "    prev_year, prev_month = None, None\n",
    "\n",
    "    for paper_id, title, pub_date in paper_details:\n",
    "        year = pub_date.strftime(\"%Y\")\n",
    "        month_name = pub_date.strftime(\"%B\")\n",
    "\n",
    "        # Add Year header and accordion structure if the year changes\n",
    "        if year != prev_year:\n",
    "            if prev_year is not None:\n",
    "                lines.append(\"</div></details>\")  # Close div for previous year\n",
    "            lines.extend(\n",
    "                [\n",
    "                    f\"\\n<details open>\",  # The \"open\" attribute makes the year visible by default\n",
    "                    f\"<summary><strong>{year}</strong></summary>\",\n",
    "                    f\"<div>\\n\",\n",
    "                ]\n",
    "            )\n",
    "            prev_year = year\n",
    "            prev_month = None  # Reset month whenever year changes\n",
    "\n",
    "        # Add Month header if the month changes\n",
    "        if month_name != prev_month:\n",
    "            lines.append(f\"\\n### {month_name} {year}\\n\")\n",
    "            prev_month = month_name\n",
    "\n",
    "        date_str = pub_date.strftime(\"%Y/%m\")\n",
    "        arxiv_link = f\"https://arxiv.org/abs/{paper_id}\"\n",
    "        md_link = f\"https://github.com/taesiri/ArXivQA/blob/main/papers/{paper_id}.md\"\n",
    "\n",
    "        # Use the paper_id as the link to the paper\n",
    "        title = title.replace(\"\\n\", \" \").replace(\"\\r\", \"\")\n",
    "        lines.append(f\"- {title} - [[{paper_id}]({arxiv_link})] [[QA]({md_link})].\\n\")\n",
    "\n",
    "    lines.append(\"</div></details>\")  # Close last year's div\n",
    "\n",
    "    with open(output_file, \"w\") as md_file:\n",
    "        md_file.writelines(lines)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching paper details: 100%|█████████████████████████████████| 6019/6019 [00:01<00:00, 4378.11it/s]\n"
     ]
    }
   ],
   "source": [
    "create_parent_md(paper_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6019"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(paper_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"paper_ids.txt\", \"w\") as f:\n",
    "    for paper_id in paper_ids:\n",
    "        f.write(paper_id + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HF README"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_parent_md_hf(paper_ids, output_file=\"./HF/README.md\"):\n",
    "    \"\"\"Create a parent Markdown file listing all papers, sorted by publication date.\"\"\"\n",
    "    paper_details = []\n",
    "\n",
    "    # open header and add header to the beginning of the file\n",
    "    with open('./HF/HEADER.md', 'r') as file:\n",
    "        header = file.read()\n",
    "    \n",
    "    lines = [header] + [\"\\n# List of Papers\\n\"]\n",
    "\n",
    "    # Fetch titles and dates for all papers\n",
    "    for paper_id in tqdm(paper_ids, desc=\"Fetching paper details\", ncols=100):\n",
    "        title, pub_date = get_paper_details(paper_id)\n",
    "        paper_details.append((paper_id, title, pub_date))\n",
    "\n",
    "    # Sort papers by publication date\n",
    "    paper_details.sort(key=lambda x: x[2], reverse=True)\n",
    "\n",
    "    # Generate Markdown content\n",
    "    prev_year, prev_month = None, None\n",
    "\n",
    "    for paper_id, title, pub_date in paper_details:\n",
    "        year = pub_date.strftime(\"%Y\")\n",
    "        month_name = pub_date.strftime(\"%B\")\n",
    "\n",
    "        # Add Year header and accordion structure if the year changes\n",
    "        if year != prev_year:\n",
    "            if prev_year is not None:\n",
    "                lines.append(\"</div></details>\")  # Close div for previous year\n",
    "            lines.extend(\n",
    "                [\n",
    "                    f\"\\n<details open>\",  # The \"open\" attribute makes the year visible by default\n",
    "                    f\"<summary><strong>{year}</strong></summary>\",\n",
    "                    f\"<div>\\n\",\n",
    "                ]\n",
    "            )\n",
    "            prev_year = year\n",
    "            prev_month = None  # Reset month whenever year changes\n",
    "\n",
    "        # Add Month header if the month changes\n",
    "        if month_name != prev_month:\n",
    "            lines.append(f\"\\n### {month_name} {year}\\n\")\n",
    "            prev_month = month_name\n",
    "\n",
    "        date_str = pub_date.strftime(\"%Y/%m\")\n",
    "        arxiv_link = f\"https://arxiv.org/abs/{paper_id}\"\n",
    "        md_link = f\"https://github.com/taesiri/ArXivQA/blob/main/papers/{paper_id}.md\"\n",
    "\n",
    "        # Use the paper_id as the link to the paper\n",
    "        title = title.replace(\"\\n\", \" \").replace(\"\\r\", \"\")\n",
    "        lines.append(f\"- {title} - [[{paper_id}]({arxiv_link})] [[QA]({md_link})].\\n\")\n",
    "\n",
    "    lines.append(\"</div></details>\")  # Close last year's div\n",
    "\n",
    "    with open(output_file, \"w\") as md_file:\n",
    "        md_file.writelines(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching paper details: 100%|█████████████████████████████████| 6019/6019 [00:01<00:00, 5179.15it/s]\n"
     ]
    }
   ],
   "source": [
    "create_parent_md_hf(paper_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 files contain the phrase 'bad phrases'\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "# Get a list of all markdown files in the directory\n",
    "md_files = glob.glob(\"./papers/*.md\")\n",
    "bad_files = []\n",
    "# Loop through each file and check if the phrase is present\n",
    "count = 0\n",
    "for file in md_files:\n",
    "    with open(file, \"r\") as f:\n",
    "        content = f.read()\n",
    "        if \"without having access to the full paper\" in content.lower():\n",
    "            bad_files.append(file)\n",
    "            count += 1\n",
    "        elif \"access to the full paper\" in content.lower():\n",
    "            bad_files.append(file)\n",
    "            count += 1\n",
    "        elif \"without access to the full paper\" in content.lower():\n",
    "            bad_files.append(file)\n",
    "            count += 1\n",
    "\n",
    "print(f\"{count} files contain the phrase 'bad phrases'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./papers/2307.01848.md',\n",
       " './papers/2205.11916.md',\n",
       " './papers/2309.08637.md',\n",
       " './papers/2305.10855.md']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bad_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# # Define the path to the papers folder\n",
    "# papers_folder = \"/Github/arxiv_qa/papers\"\n",
    "\n",
    "# # Loop through each file in bad_files and delete it\n",
    "# for file in bad_files:\n",
    "#     file = file.replace(\"./papers/\", \"\")\n",
    "#     file = file.replace(\".md\", \".csv\")\n",
    "\n",
    "#     file_path = os.path.join(papers_folder, file)\n",
    "#     print(f\"Deleting {file_path}\")\n",
    "#     os.remove(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# # Define the path to the papers folder\n",
    "\n",
    "# # Loop through each file in bad_files and delete it\n",
    "# for file in bad_files:\n",
    "#     file_path = os.path.join(file)\n",
    "#     print(f\"Deleting {file_path}\")\n",
    "#     os.remove(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
